INFO:__main__:Loaded model_type: /mnt/file2/changye/model/NLP/Qwen2.5-1.5B-Instruct
INFO:__main__:Dataset loaded successfully.
INFO:__main__:Dataset tokenized successfully.
/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ðŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
[2025-01-03 20:11:48,754] [INFO] [real_accelerator.py:222:get_accelerator] Setting ds_accelerator to cuda (auto detect)
INFO:root:gcc -pthread -B /home/changye/miniconda3/envs/ML/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/changye/miniconda3/envs/ML/include -fPIC -O2 -isystem /home/changye/miniconda3/envs/ML/include -fPIC -c /tmp/tmpp7hs49ej/test.c -o /tmp/tmpp7hs49ej/test.o
INFO:root:gcc -pthread -B /home/changye/miniconda3/envs/ML/compiler_compat /tmp/tmpp7hs49ej/test.o -laio -o /tmp/tmpp7hs49ej/a.out
INFO:root:gcc -pthread -B /home/changye/miniconda3/envs/ML/compiler_compat -Wno-unused-result -Wsign-compare -DNDEBUG -fwrapv -O2 -Wall -fPIC -O2 -isystem /home/changye/miniconda3/envs/ML/include -fPIC -O2 -isystem /home/changye/miniconda3/envs/ML/include -fPIC -c /tmp/tmppfsjtzig/test.c -o /tmp/tmppfsjtzig/test.o
INFO:root:gcc -pthread -B /home/changye/miniconda3/envs/ML/compiler_compat /tmp/tmppfsjtzig/test.o -L/usr/local/cuda -L/usr/local/cuda/lib64 -lcufile -o /tmp/tmppfsjtzig/a.out
[2025-01-03 20:11:49,928] [INFO] [comm.py:652:init_distributed] cdb=None
/mnt/file2/changye/NLPFINAL/fine-tuning_deepspeed/train.py:117: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
INFO:__main__:Starting training with DeepSpeed & W&B logging...
[rank1]: Traceback (most recent call last):
[rank1]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning_deepspeed/train.py", line 142, in <module>
[rank1]:     main()
[rank1]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning_deepspeed/train.py", line 128, in main
[rank1]:     train_result = trainer.train()
[rank1]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2164, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2262, in _inner_training_loop
[rank1]:     self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)
[rank1]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/integrations/deepspeed.py", line 398, in deepspeed_init
[rank1]:     hf_deepspeed_config.trainer_config_finalize(args, model, num_training_steps)
[rank1]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/integrations/deepspeed.py", line 270, in trainer_config_finalize
[rank1]:     raise ValueError(
[rank1]: ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:
[rank1]: - ds train_batch_size=32 vs hf train_batch_size (calculated)=16
[rank1]: The easiest method is to set these DeepSpeed config values to 'auto'.
[rank1]: Traceback (most recent call last):
[rank1]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning_deepspeed/train.py", line 142, in <module>
[rank1]:     main()
[rank1]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning_deepspeed/train.py", line 128, in main
[rank1]:     train_result = trainer.train()
[rank1]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2164, in train
[rank1]:     return inner_training_loop(
[rank1]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2262, in _inner_training_loop
[rank1]:     self.optimizer, self.lr_scheduler = deepspeed_init(self, num_training_steps=max_steps)
[rank1]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/integrations/deepspeed.py", line 398, in deepspeed_init
[rank1]:     hf_deepspeed_config.trainer_config_finalize(args, model, num_training_steps)
[rank1]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/integrations/deepspeed.py", line 270, in trainer_config_finalize
[rank1]:     raise ValueError(
[rank1]: ValueError: Please correct the following DeepSpeed config values that mismatch TrainingArguments values:
[rank1]: - ds train_batch_size=32 vs hf train_batch_size (calculated)=16
[rank1]: The easiest method is to set these DeepSpeed config values to 'auto'.
