You are using the default legacy behaviour of the <class 'transformers.models.t5.tokenization_t5.T5Tokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565
Using 8 GPUs!
Epoch 1/3:   0%|                                                                                                    | 0/1810 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/parallel/_functions.py:71: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.
  warnings.warn(
Epoch 1/3:   2%|█▎                                                                             | 29/1810 [02:19<2:22:44,  4.81s/it, loss=6.2]
Traceback (most recent call last):
  File "/mnt/file2/changye/NLPFINAL/pre-train.py", line 193, in <module>
    main()
  File "/mnt/file2/changye/NLPFINAL/pre-train.py", line 133, in main
    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 118, in parallel_apply
    thread.join()
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/file2/changye/NLPFINAL/pre-train.py", line 193, in <module>
    main()
  File "/mnt/file2/changye/NLPFINAL/pre-train.py", line 133, in main
    outputs = model(input_ids=input_ids, attention_mask=attention_mask, labels=labels)
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1736, in _wrapped_call_impl
    return self._call_impl(*args, **kwargs)
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/modules/module.py", line 1747, in _call_impl
    return forward_call(*args, **kwargs)
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 193, in forward
    outputs = self.parallel_apply(replicas, inputs, module_kwargs)
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/parallel/data_parallel.py", line 212, in parallel_apply
    return parallel_apply(
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/site-packages/torch/nn/parallel/parallel_apply.py", line 118, in parallel_apply
    thread.join()
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/threading.py", line 1096, in join
    self._wait_for_tstate_lock()
  File "/home/changye/miniconda3/envs/sae/lib/python3.10/threading.py", line 1116, in _wait_for_tstate_lock
    if lock.acquire(block, timeout):
KeyboardInterrupt
