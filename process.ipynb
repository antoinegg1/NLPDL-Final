{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "# 加载多个数据集\n",
    "dataset1 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part1\")\n",
    "dataset2 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part2\")\n",
    "dataset3 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part3\")\n",
    "dataset4 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part4\")\n",
    "dataset5 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part5\")\n",
    "dataset6 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part6\")\n",
    "\n",
    "\n",
    "# 合并数据集\n",
    "combined_dataset = concatenate_datasets([dataset1, dataset2,dataset3,dataset4,dataset5,dataset6])\n",
    "\n",
    "print(combined_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_test_split = combined_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 再将临时集拆分为验证集和测试集（各占 50%，即总数据的 10%）\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# 最终数据集\n",
    "final_dataset = {\n",
    "    'train': train_test_split['train'],\n",
    "    'val': val_test_split['train'],\n",
    "    'test': val_test_split['test']\n",
    "}\n",
    "\n",
    "# 打印拆分后的数据集信息\n",
    "print(\"Train dataset size:\", len(final_dataset['train']))\n",
    "print(\"Validation dataset size:\", len(final_dataset['val']))\n",
    "print(\"Test dataset size:\", len(final_dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_dir = \"/mnt/file2/changye/dataset/casual_formal_sentence_pair_ACL170k\"  # 设置保存目录\n",
    "final_dataset['train'].save_to_disk(f\"{output_dir}/train\")\n",
    "final_dataset['val'].save_to_disk(f\"{output_dir}/val\")\n",
    "final_dataset['test'].save_to_disk(f\"{output_dir}/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_from_disk\n",
    "test_dataset=load_from_disk(\"/mnt/file2/changye/dataset/ACL_clear/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def generate_text_hash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for the given text using SHA-256.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Unique hash for the text.\n",
    "    \"\"\"\n",
    "    hash_object = hashlib.sha256(text.encode('utf-8'))\n",
    "    return hash_object.hexdigest()\n",
    "test_list=[]\n",
    "for item in test_dataset:\n",
    "    for test in item['texts']:\n",
    "        test_list.append({\n",
    "            'directory': item['directory'],\n",
    "            'filename': item['filename'],\n",
    "            'text': test,\n",
    "            'hash': generate_text_hash(item['directory']+item['filename']+test)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((test_list[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_english(text):\n",
    "    # 移除多余的换行符，将多行文本合并为单段落\n",
    "    text = re.sub(r'\\s*\\n\\s*', ' ', text)\n",
    "    # 替换多个连续空格为单个空格\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # 确保标点符号和单词之间的空格规范化\n",
    "    text = re.sub(r'\\s*([.,!?;:])\\s*', r'\\1 ', text)\n",
    "    # 修复句号后需要的空格\n",
    "    text = re.sub(r'([.!?])([A-Za-z])', r'\\1 \\2', text)\n",
    "    # 去除括号及其内容\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)  # 去除圆括号及其中的内容\n",
    "    text = re.sub(r'（.*?）', '', text)  # 去除中文圆括号及其中的内容\n",
    "    \n",
    "    # 去除 URL（标准格式）\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    # 去除 URL（带有空格的格式）\n",
    "    text = re.sub(r'https?\\s*:\\s*//(?:\\S+\\s*)*', '', text)\n",
    "    \n",
    "    # 再次替换多个连续空格为单个空格，以处理移除 URL 后可能出现的多余空格\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # 去除首尾空格\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def is_valid_sentence(sentence):\n",
    "    \"\"\"\n",
    "    判断一个句子是否有效： \n",
    "    - 句子不应仅包含数字或URL\n",
    "    - 句子应包含至少一个字母字符\n",
    "    \"\"\"\n",
    "    if not sentence:\n",
    "        return False\n",
    "    # 排除仅包含 URL 或数字的句子\n",
    "    if re.match(r'^https?\\s*:\\s*//', sentence) or re.match(r'^\\d+$', sentence):\n",
    "        return False\n",
    "    # 需要包含至少一个字母字符\n",
    "    if not re.search(r'[a-zA-Z]', sentence):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def split_and_process_texts(text_list, max_length=200):\n",
    "    updated_text_list = []\n",
    "\n",
    "    for text in text_list:\n",
    "        text['text'] = clean_text_english(text['text'])\n",
    "        \n",
    "        if len(text['text']) > max_length:\n",
    "            # 按标点分割文本，并只选择有效的句子\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text['text'])\n",
    "            current_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if is_valid_sentence(sentence):  # 仅添加有效的句子\n",
    "                    # 如果当前段落加上句子长度超出限制，将当前段落加入结果，并重新开始\n",
    "                    if len(current_chunk) + len(sentence) > max_length:\n",
    "                        updated_text_list.append({\n",
    "                            'directory': text['directory'],\n",
    "                            'filename': text['filename'],\n",
    "                            'text': current_chunk.strip(),\n",
    "                            'hash': generate_text_hash(text['directory'] + text['filename'] + current_chunk.strip())\n",
    "                        })\n",
    "                        current_chunk = sentence\n",
    "                    else:\n",
    "                        current_chunk += \" \" + sentence\n",
    "\n",
    "            # 添加最后的剩余部分\n",
    "            if current_chunk:\n",
    "                updated_text_list.append({\n",
    "                    'directory': text['directory'],\n",
    "                    'filename': text['filename'],\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'hash': generate_text_hash(text['directory'] + text['filename'] + current_chunk.strip())\n",
    "                })\n",
    "        else:\n",
    "            # 如果文本长度小于等于限制，直接加入结果\n",
    "            updated_text_list.append({\n",
    "                'directory': text['directory'],\n",
    "                'filename': text['filename'],\n",
    "                'text': text['text'].strip(),\n",
    "                'hash': generate_text_hash(text['directory'] + text['filename'] + text['text'].strip())\n",
    "            })\n",
    "\n",
    "    return updated_text_list\n",
    "\n",
    "# 筛选和拆分\n",
    "filtered_list = split_and_process_texts(test_list, max_length=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count=0\n",
    "for text in test_list:\n",
    "    if len(text['text'])>50 and len(text['text'])<400:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(len(filtered_list))\n",
    "formal_list=[]\n",
    "for item in filtered_list:\n",
    "    if len(item['text'])>300 or len(item['text'])<75:\n",
    "        continue\n",
    "    formal_list.append(item)\n",
    "print(len(formal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(formal_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# 将数据转换为 Hugging Face 数据集格式\n",
    "dataset = Dataset.from_dict({\n",
    "    'directory': [entry['directory'] for entry in formal_list],\n",
    "    'filename': [entry['filename'] for entry in formal_list],\n",
    "    'text': [entry['text'] for entry in formal_list],\n",
    "    'hash': [entry['hash'] for entry in formal_list]\n",
    "})\n",
    "\n",
    "# 显示数据集\n",
    "print(\"Original Hugging Face dataset:\")\n",
    "print(dataset)\n",
    "\n",
    "# 保存数据集到磁盘（使用 Arrow 格式）\n",
    "dataset.save_to_disk(\"clear_ACL_sentence170k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['evaluation', 'directory', 'filename', 'formal_text', 'casual_text', 'Model_formal_text'],\n",
      "    num_rows: 300\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "new_data=load_from_disk(\"/mnt/file2/changye/NLPFINAL/AI_eval_result/Qwen2.5-1.5B-Instruct-finetune_sentence_part1\")\n",
    "print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "```json\n",
      "{\n",
      "    \"Style Transfer Strength\": 0.2,\n",
      "    \"Content Preservation\": 0.1,\n",
      "    \"Fluency\": 0.3\n",
      "}\n",
      "```\n"
     ]
    }
   ],
   "source": [
    "print(new_data[0]['evaluation'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300\n"
     ]
    }
   ],
   "source": [
    "pattern = r'\\d\\.\\d'\n",
    "import re\n",
    "Style_Transfer_Strength=[]\n",
    "Content_Preservation=[]\n",
    "Fluency=[]\n",
    "valid=0\n",
    "for item in new_data:\n",
    "    text=item['evaluation']\n",
    "    matches = re.findall(pattern, text)\n",
    "    if len(matches)==3:\n",
    "        valid+=1\n",
    "        Style_Transfer_Strength.append(float(matches[0]))\n",
    "        Content_Preservation.append(float(matches[1]))\n",
    "        Fluency.append(float(matches[2]))\n",
    "    else:\n",
    "        continue\n",
    "print(valid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Style Transfer Strength:  0.163\n",
      "Content Preservation:  0.263\n",
      "Fluency:  0.2886666666666666\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "print(f\"Style Transfer Strength: \",np.array(Style_Transfer_Strength).mean())\n",
    "print(f\"Content Preservation: \",np.array(Content_Preservation).mean())\n",
    "print(f\"Fluency: \",np.array(Fluency).mean())\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
