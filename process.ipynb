{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['directory', 'filename', 'formal_text', 'casual_text'],\n",
      "    num_rows: 60000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "# 加载多个数据集\n",
    "dataset1 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part1\")\n",
    "dataset2 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part2\")\n",
    "dataset3 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part3\")\n",
    "dataset4 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part4\")\n",
    "dataset5 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part5\")\n",
    "dataset6 = load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_sentence_pair_ACL170k_part6\")\n",
    "\n",
    "\n",
    "# 合并数据集\n",
    "combined_dataset = concatenate_datasets([dataset1, dataset2,dataset3,dataset4,dataset5,dataset6])\n",
    "\n",
    "print(combined_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 48000\n",
      "Validation dataset size: 6000\n",
      "Test dataset size: 6000\n"
     ]
    }
   ],
   "source": [
    "train_test_split = combined_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 再将临时集拆分为验证集和测试集（各占 50%，即总数据的 10%）\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# 最终数据集\n",
    "final_dataset = {\n",
    "    'train': train_test_split['train'],\n",
    "    'val': val_test_split['train'],\n",
    "    'test': val_test_split['test']\n",
    "}\n",
    "\n",
    "# 打印拆分后的数据集信息\n",
    "print(\"Train dataset size:\", len(final_dataset['train']))\n",
    "print(\"Validation dataset size:\", len(final_dataset['val']))\n",
    "print(\"Test dataset size:\", len(final_dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 48000/48000 [00:00<00:00, 188624.79 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6000/6000 [00:00<00:00, 153354.77 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 6000/6000 [00:00<00:00, 157314.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/mnt/file2/changye/dataset/casual_formal_sentence_pair_ACL170k\"  # 设置保存目录\n",
    "final_dataset['train'].save_to_disk(f\"{output_dir}/train\")\n",
    "final_dataset['val'].save_to_disk(f\"{output_dir}/val\")\n",
    "final_dataset['test'].save_to_disk(f\"{output_dir}/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['directory', 'filename', 'formal_text', 'casual_text'],\n",
       "     num_rows: 32000\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['directory', 'filename', 'formal_text', 'casual_text'],\n",
       "     num_rows: 4000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['directory', 'filename', 'formal_text', 'casual_text'],\n",
       "     num_rows: 4000\n",
       " })}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "test_dataset=load_from_disk(\"/mnt/file2/changye/dataset/ACL_clear/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['directory', 'filename', 'texts'],\n",
      "    num_rows: 1811\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def generate_text_hash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for the given text using SHA-256.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Unique hash for the text.\n",
    "    \"\"\"\n",
    "    hash_object = hashlib.sha256(text.encode('utf-8'))\n",
    "    return hash_object.hexdigest()\n",
    "test_list=[]\n",
    "for item in test_dataset:\n",
    "    for test in item['texts']:\n",
    "        test_list.append({\n",
    "            'directory': item['directory'],\n",
    "            'filename': item['filename'],\n",
    "            'text': test,\n",
    "            'hash': generate_text_hash(item['directory']+item['filename']+test)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['directory', 'filename', 'text', 'hash'])\n"
     ]
    }
   ],
   "source": [
    "print((test_list[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text_english(text):\n",
    "    # 移除多余的换行符，将多行文本合并为单段落\n",
    "    text = re.sub(r'\\s*\\n\\s*', ' ', text)\n",
    "    # 替换多个连续空格为单个空格\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # 确保标点符号和单词之间的空格规范化\n",
    "    text = re.sub(r'\\s*([.,!?;:])\\s*', r'\\1 ', text)\n",
    "    # 修复句号后需要的空格\n",
    "    text = re.sub(r'([.!?])([A-Za-z])', r'\\1 \\2', text)\n",
    "    # 去除括号及其内容\n",
    "    text = re.sub(r'\\(.*?\\)', '', text)  # 去除圆括号及其中的内容\n",
    "    text = re.sub(r'（.*?）', '', text)  # 去除中文圆括号及其中的内容\n",
    "    \n",
    "    # 去除 URL（标准格式）\n",
    "    text = re.sub(r'https?://\\S+', '', text)\n",
    "    # 去除 URL（带有空格的格式）\n",
    "    text = re.sub(r'https?\\s*:\\s*//(?:\\S+\\s*)*', '', text)\n",
    "    \n",
    "    # 再次替换多个连续空格为单个空格，以处理移除 URL 后可能出现的多余空格\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # 去除首尾空格\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "def is_valid_sentence(sentence):\n",
    "    \"\"\"\n",
    "    判断一个句子是否有效： \n",
    "    - 句子不应仅包含数字或URL\n",
    "    - 句子应包含至少一个字母字符\n",
    "    \"\"\"\n",
    "    if not sentence:\n",
    "        return False\n",
    "    # 排除仅包含 URL 或数字的句子\n",
    "    if re.match(r'^https?\\s*:\\s*//', sentence) or re.match(r'^\\d+$', sentence):\n",
    "        return False\n",
    "    # 需要包含至少一个字母字符\n",
    "    if not re.search(r'[a-zA-Z]', sentence):\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def split_and_process_texts(text_list, max_length=200):\n",
    "    updated_text_list = []\n",
    "\n",
    "    for text in text_list:\n",
    "        text['text'] = clean_text_english(text['text'])\n",
    "        \n",
    "        if len(text['text']) > max_length:\n",
    "            # 按标点分割文本，并只选择有效的句子\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text['text'])\n",
    "            current_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                sentence = sentence.strip()\n",
    "                if is_valid_sentence(sentence):  # 仅添加有效的句子\n",
    "                    # 如果当前段落加上句子长度超出限制，将当前段落加入结果，并重新开始\n",
    "                    if len(current_chunk) + len(sentence) > max_length:\n",
    "                        updated_text_list.append({\n",
    "                            'directory': text['directory'],\n",
    "                            'filename': text['filename'],\n",
    "                            'text': current_chunk.strip(),\n",
    "                            'hash': generate_text_hash(text['directory'] + text['filename'] + current_chunk.strip())\n",
    "                        })\n",
    "                        current_chunk = sentence\n",
    "                    else:\n",
    "                        current_chunk += \" \" + sentence\n",
    "\n",
    "            # 添加最后的剩余部分\n",
    "            if current_chunk:\n",
    "                updated_text_list.append({\n",
    "                    'directory': text['directory'],\n",
    "                    'filename': text['filename'],\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'hash': generate_text_hash(text['directory'] + text['filename'] + current_chunk.strip())\n",
    "                })\n",
    "        else:\n",
    "            # 如果文本长度小于等于限制，直接加入结果\n",
    "            updated_text_list.append({\n",
    "                'directory': text['directory'],\n",
    "                'filename': text['filename'],\n",
    "                'text': text['text'].strip(),\n",
    "                'hash': generate_text_hash(text['directory'] + text['filename'] + text['text'].strip())\n",
    "            })\n",
    "\n",
    "    return updated_text_list\n",
    "\n",
    "# 筛选和拆分\n",
    "filtered_list = split_and_process_texts(test_list, max_length=300)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43183\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for text in test_list:\n",
    "    if len(text['text'])>50 and len(text['text'])<400:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "200794\n",
      "171460\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_list))\n",
    "formal_list=[]\n",
    "for item in filtered_list:\n",
    "    if len(item['text'])>300 or len(item['text'])<75:\n",
    "        continue\n",
    "    formal_list.append(item)\n",
    "print(len(formal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'directory': 'eamt', 'filename': '2020.eamt-1.30.json', 'text': 'Finally, future work will also include the replication of these experiments with data from real diagnostic interviews and with data from other diagnostic domains.', 'hash': '4e6628d3b96cd4b01cc5788bceb2d25be5eb05df9b20bfa538b437137246a4e4'}\n"
     ]
    }
   ],
   "source": [
    "print(formal_list[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hugging Face dataset:\n",
      "Dataset({\n",
      "    features: ['directory', 'filename', 'text', 'hash'],\n",
      "    num_rows: 171460\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 171460/171460 [00:00<00:00, 1148021.91 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# 将数据转换为 Hugging Face 数据集格式\n",
    "dataset = Dataset.from_dict({\n",
    "    'directory': [entry['directory'] for entry in formal_list],\n",
    "    'filename': [entry['filename'] for entry in formal_list],\n",
    "    'text': [entry['text'] for entry in formal_list],\n",
    "    'hash': [entry['hash'] for entry in formal_list]\n",
    "})\n",
    "\n",
    "# 显示数据集\n",
    "print(\"Original Hugging Face dataset:\")\n",
    "print(dataset)\n",
    "\n",
    "# 保存数据集到磁盘（使用 Arrow 格式）\n",
    "dataset.save_to_disk(\"clear_ACL_sentence170k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'load_from_disk' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m new_data\u001b[38;5;241m=\u001b[39m\u001b[43mload_from_disk\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/mnt/file2/changye/dataset/ACL_clear/train\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(new_data)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'load_from_disk' is not defined"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "new_data=load_from_disk(\"/mnt/file2/changye/dataset/ACL_clear/train\")\n",
    "print(new_data)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
