{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['directory', 'filename', 'formal_text', 'casual_text'],\n",
      "    num_rows: 40000\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk, concatenate_datasets\n",
    "\n",
    "# 加载多个数据集\n",
    "dataset1 = load_from_disk(\"/mnt/file2/changye/dataset/casual_formal_pair_ACL80k/casual_formal_pair_ACL80k_part1\")\n",
    "dataset2 = load_from_disk(\"/mnt/file2/changye/dataset/casual_formal_pair_ACL80k/casual_formal_pair_ACL80k_part2\")\n",
    "dataset3 = load_from_disk(\"/mnt/file2/changye/dataset/casual_formal_pair_ACL80k/casual_formal_pair_ACL80k_part3\")\n",
    "dataset4 = load_from_disk(\"/mnt/file2/changye/dataset/casual_formal_pair_ACL80k/casual_formal_pair_ACL80k_part4\")\n",
    "\n",
    "\n",
    "# 合并数据集\n",
    "combined_dataset = concatenate_datasets([dataset1, dataset2,dataset3,dataset4])\n",
    "\n",
    "print(combined_dataset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train dataset size: 32000\n",
      "Validation dataset size: 4000\n",
      "Test dataset size: 4000\n"
     ]
    }
   ],
   "source": [
    "train_test_split = combined_dataset.train_test_split(test_size=0.2, seed=42)\n",
    "\n",
    "# 再将临时集拆分为验证集和测试集（各占 50%，即总数据的 10%）\n",
    "val_test_split = train_test_split['test'].train_test_split(test_size=0.5, seed=42)\n",
    "\n",
    "# 最终数据集\n",
    "final_dataset = {\n",
    "    'train': train_test_split['train'],\n",
    "    'val': val_test_split['train'],\n",
    "    'test': val_test_split['test']\n",
    "}\n",
    "\n",
    "# 打印拆分后的数据集信息\n",
    "print(\"Train dataset size:\", len(final_dataset['train']))\n",
    "print(\"Validation dataset size:\", len(final_dataset['val']))\n",
    "print(\"Test dataset size:\", len(final_dataset['test']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 32000/32000 [00:00<00:00, 126576.67 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4000/4000 [00:00<00:00, 107768.01 examples/s]\n",
      "Saving the dataset (1/1 shards): 100%|██████████| 4000/4000 [00:00<00:00, 116918.47 examples/s]\n"
     ]
    }
   ],
   "source": [
    "output_dir = \"/mnt/file2/changye/dataset/casual_formal_pair_ACL80k\"  # 设置保存目录\n",
    "final_dataset['train'].save_to_disk(f\"{output_dir}/train\")\n",
    "final_dataset['val'].save_to_disk(f\"{output_dir}/val\")\n",
    "final_dataset['test'].save_to_disk(f\"{output_dir}/test\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': Dataset({\n",
       "     features: ['directory', 'filename', 'formal_text', 'casual_text'],\n",
       "     num_rows: 32000\n",
       " }),\n",
       " 'val': Dataset({\n",
       "     features: ['directory', 'filename', 'formal_text', 'casual_text'],\n",
       "     num_rows: 4000\n",
       " }),\n",
       " 'test': Dataset({\n",
       "     features: ['directory', 'filename', 'formal_text', 'casual_text'],\n",
       "     num_rows: 4000\n",
       " })}"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_from_disk\n",
    "test_dataset=load_from_disk(\"/mnt/file2/changye/dataset/ACL_clear/test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['directory', 'filename', 'texts'],\n",
      "    num_rows: 1811\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "print(test_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import hashlib\n",
    "def generate_text_hash(text: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a unique identifier for the given text using SHA-256.\n",
    "\n",
    "    Args:\n",
    "        text (str): Input text.\n",
    "\n",
    "    Returns:\n",
    "        str: Unique hash for the text.\n",
    "    \"\"\"\n",
    "    hash_object = hashlib.sha256(text.encode('utf-8'))\n",
    "    return hash_object.hexdigest()\n",
    "test_list=[]\n",
    "for item in test_dataset:\n",
    "    for test in item['texts']:\n",
    "        test_list.append({\n",
    "            'directory': item['directory'],\n",
    "            'filename': item['filename'],\n",
    "            'text': test,\n",
    "            'hash': generate_text_hash(item['directory']+item['filename']+test)\n",
    "        })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['directory', 'filename', 'text', 'hash'])\n"
     ]
    }
   ],
   "source": [
    "print((test_list[0].keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text_english(text):\n",
    "    import re\n",
    "    # 移除多余的换行符，将多行文本合并为单段落\n",
    "    text = re.sub(r'\\s*\\n\\s*', ' ', text)\n",
    "    # 替换多个连续空格为单个空格\n",
    "    text = re.sub(r'\\s{2,}', ' ', text)\n",
    "    # 确保标点符号和单词之间的空格规范化\n",
    "    text = re.sub(r'\\s*([.,!?;:])\\s*', r'\\1 ', text)\n",
    "    # 修复句号后需要的空格\n",
    "    text = re.sub(r'([.!?])([A-Za-z])', r'\\1 \\2', text)\n",
    "    # 去除首尾空格\n",
    "    text = text.strip()\n",
    "    return text\n",
    "\n",
    "# 示例\n",
    "text = \"\"\"  This is   an example   text.  \n",
    "It contains   unnecessary   spaces and  line breaks.  Here is a new line. \n",
    "Another sentence.  \"\"\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "def split_and_process_texts(text_list, max_length=200):\n",
    "    updated_text_list = []\n",
    "\n",
    "    for text in text_list:\n",
    "        text['text']=clean_text_english(text['text'])\n",
    "        if len(text['text']) > max_length:\n",
    "            # 按标点分割文本\n",
    "            sentences = re.split(r'(?<=[.!?])\\s+', text['text'])\n",
    "            current_chunk = \"\"\n",
    "            for sentence in sentences:\n",
    "                # 如果当前段落加上句子长度超出限制，将当前段落加入结果，并重新开始\n",
    "                if len(current_chunk) + len(sentence) > max_length:\n",
    "                    updated_text_list.append({\n",
    "                        'directory': text['directory'],\n",
    "                        'filename': text['filename'],\n",
    "                        'text': current_chunk.strip(),\n",
    "                        'hash': generate_text_hash(text['directory'] + text['filename'] + current_chunk.strip())\n",
    "                    })\n",
    "                    current_chunk = sentence\n",
    "                else:\n",
    "                    current_chunk += \" \" + sentence\n",
    "\n",
    "            # 添加最后的剩余部分\n",
    "            if current_chunk:\n",
    "                updated_text_list.append({\n",
    "                    'directory': text['directory'],\n",
    "                    'filename': text['filename'],\n",
    "                    'text': current_chunk.strip(),\n",
    "                    'hash': generate_text_hash(text['directory'] + text['filename'] + current_chunk.strip())\n",
    "                })\n",
    "        else:\n",
    "            # 如果文本长度小于等于限制，直接加入结果\n",
    "            updated_text_list.append({\n",
    "                'directory': text['directory'],\n",
    "                'filename': text['filename'],\n",
    "                'text': text['text'].strip(),\n",
    "                'hash': generate_text_hash(text['directory'] + text['filename'] + text['text'].strip())\n",
    "            })\n",
    "\n",
    "    return updated_text_list\n",
    "\n",
    "\n",
    "\n",
    "# 筛选和拆分\n",
    "filtered_list = split_and_process_texts(test_list, max_length=1500)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "42257\n"
     ]
    }
   ],
   "source": [
    "count=0\n",
    "for text in test_list:\n",
    "    if len(text['text'])>50 and len(text['text'])<400:\n",
    "        count+=1\n",
    "print(count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97974\n",
      "81294\n"
     ]
    }
   ],
   "source": [
    "print(len(filtered_list))\n",
    "formal_list=[]\n",
    "for item in filtered_list:\n",
    "    if len(item['text'])>2000 or len(item['text'])<75:\n",
    "        continue\n",
    "    formal_list.append(item)\n",
    "print(len(formal_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original Hugging Face dataset:\n",
      "Dataset({\n",
      "    features: ['directory', 'filename', 'text', 'hash'],\n",
      "    num_rows: 81294\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 81294/81294 [00:00<00:00, 864396.95 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from datasets import Dataset\n",
    "import hashlib\n",
    "\n",
    "\n",
    "# 将数据转换为 Hugging Face 数据集格式\n",
    "dataset = Dataset.from_dict({\n",
    "    'directory': [entry['directory'] for entry in formal_list],\n",
    "    'filename': [entry['filename'] for entry in formal_list],\n",
    "    'text': [entry['text'] for entry in formal_list],\n",
    "    'hash': [entry['hash'] for entry in formal_list]\n",
    "})\n",
    "\n",
    "# 显示数据集\n",
    "print(\"Original Hugging Face dataset:\")\n",
    "print(dataset)\n",
    "\n",
    "# 保存数据集到磁盘（使用 Arrow 格式）\n",
    "dataset.save_to_disk(\"clear_ACL_sentences80k\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After achieving remarkable successes in Machine Translation (Sutskever et al. , 2014; Cho et al. , 2014), neural networks with the encoder-decoder architectures (a. k. a sequence-to-sequence models, Seq2Seq) have been proven to be a functioning method to model short-text conversations (Vinyals and Le, 2015; Shang et al. , 2015), where the corresponding task is often called Neural Response Generation. The advantage of applying Seq2Seq models to conversation generation is that the training procedure can be performed end-to-end in an unsupervised manner, based on human-generated conversational utterances (typically query-response pairs mined from social networks). One of the potential applications of such neural response generators is to improve the capability of existing conversational interfaces (informally also known as chatbots) by enabling them to go beyond predefined tasks and chat with human users in an open domain.\n"
     ]
    }
   ],
   "source": [
    "new_data=load_from_disk(\"/mnt/file2/changye/NLPFINAL/casual_formal_pair_ACL80k\")\n",
    "print(new_data[0]['formal_text'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "ML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
