{
  "train_micro_batch_size_per_gpu": 4,
  "gradient_accumulation_steps": 1,
  "fp16": {
      "enabled": true,
      "initial_scale_power": 12
  },
  "zero_optimization": {
      "stage": 2,
      "reduce_scatter": true,
      "allgather_partitions": true,
      "allgather_bucket_size": 2e8,
      "overlap_comm": true,
      "contiguous_gradients": true
  },
  "scheduler": {
      "type": "WarmupLR",  
      "params": {
          "warmup_min_lr": 0,
          "warmup_max_lr": 1e-5,
          "warmup_num_steps": 1000
      }
  },
  "gradient_clipping": 1.0
}