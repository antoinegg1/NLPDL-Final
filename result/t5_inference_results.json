[
    {
        "casual_text": "When fine-tuning using the MLM objective, the BASE version of models usually shows better performance improvements on the General Corpus dataset. This is likely because models with bigger parameter sizes are better at understanding the connections between different parts of similes, especially when dealing with similes that have more varied contexts.",
        "formal_text": "Convert casual text to formal text: When fine-tuning using the MLM objective, the BASE version of models usually shows better performance improvements on the General Corpus dataset. This is likely because models with bigger parameter sizes are better"
    },
    {
        "casual_text": "Position-Aware Graphs. PacSum is a new graph-based method that picks out important sentences from a bunch of documents (Zheng and Lapata, 2019). In PacSum, a sentence is more likely to be chosen if it’s more similar to the sentences that come after it and less similar to the ones that come before it. This approach helps PacSum focus on selecting sentences that are early in the text or \"semantically central.\" We took PacSum and made it even better by using SBERT to measure how similar sentences are (we call this new version SPS) and looked at both the individual and global-graph versions of SPS.",
        "formal_text": "Convert casual text to formal text: Position-Aware Graphs. PacSum is a new graph-based method that picks out important sentences from a bunch of documents (Zheng and Lapata, 2019"
    },
    {
        "casual_text": "We're suggesting that we can use subtle hints from users to cut down on the need for manual labeling, which will help dialog systems handle more unusual or less common phrases. In this paper, we’re looking at situations where the system messes up a user’s input, and the user gets frustrated and rephrases what they said (check out Figure 1 for an example). This happens a lot in real-world systems, where people often try multiple times to get the right response. When the system finally gets it right, we can use that successful input to figure out what the earlier, failed attempts were supposed to mean. Figure 1 shows an interaction where the first and third user inputs, which are kind of tricky or unusual, are misunderstood by the system. But the final, simpler version works. Our method, MARUPA, uses something called PD and FD to spot two pairs of paraphrased sentences—one that caused trouble and one that worked (like 1+4 and 3+4). For both pairs, we take the successful version and apply it to the failed one (we call this LP). The second utterance isn’t used here. After collecting this new data from multiple interactions, we use it to retrain the model and make it smarter.",
        "formal_text": "Convert casual text to formal text: We're suggesting that we can use subtle hints from users to cut down on the need for manual labeling, which will help dialog systems handle more unusual or less common phrases. In this paper,"
    },
    {
        "casual_text": "Just like in 2010, the conference has two main parts: one is the research track, where researchers talk about their big findings in machine translation and similar fields, including some serious testing. The other is the user track, where people share their real-world experiences with MT, whether they're from businesses, governments, or non-profits.",
        "formal_text": "Convert casual text to formal text: Just like in 2010, the conference has two main parts: one is the research track, where researchers talk about their big findings in machine translation and similar fields, including some serious testing. The other is the user"
    },
    {
        "casual_text": "We use dependency parsing to pull out syntactic features like uni-grams and tri-grams that show how words are connected. Dependency parsing helps us see the relationships between words, which are linked by these binary, asymmetrical connections called dependencies. Figure 2 shows a graph of this parsing with the syntactic sequences. We’re using Spacy’s dependency parser, which was introduced by Honnibal and Johnson in 2015. After looking at some random samples from our data and analyzing the dependency graphs, we decided to focus on POS tags for uni-gram features, like pronouns, proper nouns, direct objects, indirect objects, coordinating conjunctions, and interjections. For tri-gram features, we looked at (subject-object-verb) tuples and dependency graphs involving auxiliary verbs and their two closest neighbors on the right. In dialogues, each utterance can have one or more functions, like giving or asking for information from the person you're talking to. These functions are called dialogue control functions (Bunt et al., 2019). For example, in the sentence \"Hi John, Please get ready for some exercise,\" \"Hi John\" is a greeting, and \"Please get ready for some exercise\" is a request. Each part with its function is called a functional segment. The list of dialogue control functions we used is in Table 2.",
        "formal_text": "Convert casual text to formal text: We use dependency parsing to pull out syntactic features like uni-grams and tri-grams that show how words are connected. Dependency parsing helps us see the"
    },
    {
        "casual_text": "For compounds of different lengths, we also check their error rates based on the minimum atom frequency. As you can see in Figures 8, 9, and 10, the error rate doesn’t really match up with atom frequency no matter the length of the compound. Figure 10 specifically shows what happens when the compound length is set to 5. Tables 7 and 8 give some stats on a few monolingual data sources, comparing them to the ROC-Filter data we used to build the CoGnition dataset. From these tables, we can tell that our dataset has shorter sentences and a vocabulary made up of more common words.",
        "formal_text": "Convert casual text to formal text: For compounds of different lengths, we also check their error rates based on the minimum atom frequency. As you can see in Figures 8, 9, and 10, the error rate doesn’t really match"
    },
    {
        "casual_text": "In this part, we're talking about LinkYelp, which is the method we created to tackle the new problem we're proposing for Yelp-EL. LinkYelp has two main parts: candidate generation and candidate ranking. First, the candidate generation step looks for a bunch of businesses that could be the one being talked about, based on the mention string. Then, the candidate ranking step sorts through all those businesses and picks the one that seems most likely to be the correct target.",
        "formal_text": "Convert casual text to formal text: In this part, we're talking about LinkYelp, which is the method we created to tackle the new problem we're proposing for Yelp-EL. LinkYe"
    },
    {
        "casual_text": "The player can play back audio tracks that were recorded on mix-mode CDs, CD-Gs, CD-Extras, and CD Text.",
        "formal_text": "Convert casual text to formal text: The player can play back audio tracks that were recorded on mix-mode CDs, CD-Gs, and CD Text. Convert casual text to formal text: The player can play back audio tracks"
    },
    {
        "casual_text": "Let’s take a look at how different design choices affect how well our model works.",
        "formal_text": "Convert casual text to formal text: Let’s take a look at how different design choices affect the well our model works. Convert casual text to formal text: Let’s take a look at how different design choices affect the"
    },
    {
        "casual_text": "[41] Check if the system's knowledge base is on board with, disagrees with, or doesn't really care about adding this component to the overall plan.",
        "formal_text": "Convert casual text to formal text: [41] Check if the system's knowledge base is on board with, disagrees with, or really care about adding this component to the overall plan."
    },
    {
        "casual_text": "Okay, let's say f is part of the linear family, and we've got this joint feature mapping  that takes pairs from X and Y and maps them to some space Rd. So, basically:",
        "formal_text": "Convert casual text to formal text: Okay, let's say f is part of the linear family, and we've got this joint feature mapping  that takes pairs from X and Y and maps to some space R"
    },
    {
        "casual_text": "We went with the IOB2 tagging system for all our tests. In each experiment, we calculated both micro and macro-averaged precision, recall, and F1 scores based on chunks, plus micro and macro scores for new classes, just like in (Qu et al., 2016). Because of space issues, we're only showing the micro-averaged scores in this paper, but we'll have all our full results—including scores for individual labels—posted online.",
        "formal_text": "Convert casual text to formal text: We went with the IOB2 tagging system for all our tests. In each experiment, we calculated both micro and macro-averaged precision, recall, and F1 scores based on chunk"
    },
    {
        "casual_text": "Alright, so to create a single real-valued vector that represents a question in the collection language (LexCL), we begin with a probabilistic structure that shows how the question is translated (like in Figure 1, which is an example of a grammar-based translation method). For every word in the collection-language vocabulary, we figure out its weight by taking the average of its probability across all the terms in that probabilistic structure.",
        "formal_text": "Convert casual text to formal text: Alright, so to create a single real-valued vector that represents a question in the collection language (LexCL), we begin with a probabilistic structure that shows how the question"
    },
    {
        "casual_text": "So, the MLP and OffsetNet have the same number of parameters. The ResNet, though, has 50% more because it has an extra weight matrix in the output layer. All these networks use the SELU activation function. We used the Adam optimizer for all our training runs.",
        "formal_text": "Convert casual text to formal text: So, the MLP and OffsetNet have the same number of parameters. The ResNet, though, has 50% more because it has an extra weight matrix in the output layer. All these networks use"
    },
    {
        "casual_text": "You can find the Tacotron 2 project here: https://github.com/NVIDIA/tacotron2 And the WaveGlow project here: https://github.com/NVIDIA/waveglow",
        "formal_text": "You can find the Tacotron 2 project here: https://github.com/NVIDIA/tacotron2 And the WaveGlow project here: https://github.com/NVIDIA/waveglow"
    },
    {
        "casual_text": "To build the graph, we use a fully connected one, with each node representing a unique word from X. For the connections (edges), we create two adjacency matrices:   A X ij for the backward direction and   A X ij for the forward direction. Both are based on how close the words are in terms of their positions.",
        "formal_text": "Convert casual text to formal text: To build the graph, we use a fully connected one, with each node representing a unique word from X. For the connections (edges), we create two adjacency"
    },
    {
        "casual_text": "The models are consistent, but they don't give us a direct way to estimate costs. So, to help with picking the right model for real-world applications, especially when you're working outside of academia, we’re keeping an eye on how long things take and how much they actually cost. We’re using cloud machines for this because they’re easy to get, and they always have the same hardware, price, and performance. In the next parts, we’ll just call \"time\" the time it takes and \"cost\" the actual money spent during pretraining, training, and when doing predictions.",
        "formal_text": "Convert casual text to formal text: The models are consistent, but they don't give us a direct way to estimate costs. So, to help with picking the right model for real-world applications, especially when you're working"
    },
    {
        "casual_text": "In the QA generation interface, workers see a sentence with all the target words highlighted in bold. They're asked to come up with one or more questions that connect two of these bold words. To make the question, they start by picking a question prefix, then, if needed, add an auxiliary, and finally, choose parts of the sentence to build the full question. They might tweak it a bit to make it sound right. After creating the question, the next step is to answer it by picking parts of the sentence. Just like with the question, they can adjust the answer to make it grammatically correct.",
        "formal_text": "Convert casual text to formal text: In the QA generation interface, workers see a sentence with all the target words highlighted in bold. They're asked to come up with one or more questions that connect two of these bold words."
    },
    {
        "casual_text": "The idea of \"universal schema\" (from Riedel et al. in 2013) also tries to put KB entities and noun phrases in the same space, but they go with noun phrases instead of the KB entities that Gardner et al. used. This approach has its issues because it needs some sort of entity linking system to work beforehand, and it struggles with common nouns that refer to proper entities without losing important info. Our method, along with Lao et al.'s, avoids this problem altogether by not even trying to mix KB entities with noun phrases.",
        "formal_text": "Convert casual text to formal text: The idea of \"universal schema\" (from Riedel et al. in 2013) also tries to put KB entities and noun phrases in the same space, but they go with"
    },
    {
        "casual_text": "Back in 2006, McDonald came up with a discriminative compression model, and then Clarke and Lapata (2008) made it better by using ILP for decoding. Since our method is built on this model, let's quickly go over it first. For more details, you can check out Clarke and Lapata's work from 2008. In this model, they use a score function to evaluate each possible compression option.",
        "formal_text": "Convert casual text to formal text: Back in 2006, McDonald came up with a discriminative compression model, and then Clarke and Lapata (2008) made it better by using ILP for decoding. Since our method is built on"
    },
    {
        "casual_text": "Okay, so let's break this down in a simpler way: 1. **The special effects with the mom's ghost**: This part is a bit messed up, but it seems like it's talking about some kind of visual effect involving a ghost mom. Maybe in a movie or something? 2. **Best readily available food coloring**: This one's pretty clear. It's saying that this is the best, easy-to-find food coloring you can get. 3. **They fit easily under my snow pants and they don't show through**: This is about something that fits nicely under snow pants and doesn't make it obvious that there's something underneath. 4. **They concord easily under my snow pants and not they go through them**: This one's a bit confusing, but it seems like it's trying to say the same thing as the previous point—something fits well under snow pants and doesn't stick out or show through. 5. **Almost correct but discarded**: This part is saying that something was close to being right, but it didn't make the cut or was thrown out. So, overall, it's a mix of talking about special effects, food coloring, and something that fits under snow pants without being noticeable.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in a simpler way: 1. **The special effects with the mom's ghost**: This part is a bit messed up, but it"
    },
    {
        "casual_text": "Sure! Here's the informal version: You can check out these links for more info on DB2: 1. [IBM Developer Answers for DB2](https://developer.ibm.com/answers/topics/db2/) 2. [IBM Support for DB2 on Linux, Unix, and Windows](https://www.ibm.com/mysupport/s/topic/0TO500000001fUNGAY/db2-linux-unix-and-windows) 3. [IBM Knowledge Center](https://www.ibm.com/support/knowledgecenter/) Hope that helps!",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: You can check out these links for more info on DB2: 1. [IBM Developer Answers for DB2](https://developer"
    },
    {
        "casual_text": "We took the top n automatically extracted relations and compared them to the gold standard to see how well our method worked. We plotted a precision-recall curve to show how precise and complete our method was. Precision tells us how many of the extracted relations were actually useful (not redundant), while recall shows how many of the gold standard relations we managed to extract. For each point on the curve, which is a pair of precision (p) and recall (r) values, we calculated the F-measure using the formula F = 2pr/(p + r). This F-measure is like a balance between precision and recall. The precision-recall curve shows precision values for different recall levels, from 0 to 1, with steps of 0.1. To simplify things, we picked one number to represent the quality of our ranked output compared to the gold standard. We did this by finding F_max, which is the highest F-measure we got from the curve. F_max gives us the best mix of precision and recall for our ranked output. Finally, we looked at all possible values of n (the number of top-ranked relations we considered) and chose the one that gave us the highest F_max value. This n is the sweet spot where our method works best in terms of balancing precision and recall.",
        "formal_text": "Convert casual text to formal text: We took the top n automatically extracted relations and compared them to the gold standard to see how well our method worked. We plotted a precision-recall curve to show how precise and complete"
    },
    {
        "casual_text": "Our research lays the groundwork for tackling more complex syntax issues. Take the sentence \"the girl will put the orange on the tray in the bowl,\" for instance. It can be broken down in two different ways: [the girl will put][the orange on the tray][in the bowl] or [the girl will put the orange][on the tray in the bowl] (Coco and Keller, 2015). Both ways are grammatically fine, but only one makes sense when you see a picture of the situation. Future models that deal with understanding language based on visual clues might be able to figure out which one is right by using more detailed visual info.",
        "formal_text": "Convert casual text to formal text: Our research lays the groundwork for tackling more complex syntax issues. Take the sentence \"the girl will put the orange on the tray in the bowl,\" for instance. It can be broken down in"
    },
    {
        "casual_text": "If there were no out-of-vocabulary subwords in the bilingual lexicon, the recall of the oracle method would be 100%.",
        "formal_text": "Convert casual text to formal text: If there were no out-vocabulary subwords in the bilingual lexicon in the bilingual lexicon would be 100%. Convert casual text to formal text: If there were no"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. Sometimes, basic patterns or simple variables don't really carry much meaning or information. They’re just too basic and don’t tell us much. So, just grouping these simple patterns together or trying to unify them isn’t enough to capture the real, complex relationships and processes we’re interested in. It’s not efficient. Let’s look at an example to make this clearer. Imagine we’re talking about a mathematical category. This category is made up of different features. Each feature has a name and a value, and we can represent it like a term, say term(name, value). Does that help?",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. Sometimes, basic patterns or simple variables don't really carry much meaning or information. They’re just too basic and don’t tell"
    },
    {
        "casual_text": "The way this stuff is put together could give us some insights into how meaning works, especially for tasks related to TE.",
        "formal_text": "Convert casual text to formal text: The way this stuff is put together could give us some insights how meaning works, especially for tasks related to TE."
    },
    {
        "casual_text": "Okay, so here's the deal: we're talking about some properties that help us figure out how to make the LMG (whatever that is) simpler, so that we can solve a specific problem faster. Specifically, we're looking at cases where the problem can be solved in polynomial time, which is a fancy way of saying \"not too slow.\" Now, let's break it down: 1. **Non-combinatorial LMG**: This is a type of LMG where, in the rules, every argument of a nonterminal (on the right-hand side) is just a single variable. In other words, we don't allow anything fancy or complex inside the predicates. If we have a non-combinatorial LMG, any terminal string that shows up during the process of deriving something will just be a part of the final string. It won't be something extra or complicated. 2. **Examples**: - The grammar in Example 2.8 is non-combinatorial. It follows the rules we just talked about. - But the grammar in Example 2.9? Not so much. It breaks the rules because of the first VP production. That's the one causing the trouble. 3. **Left Binding**: - For example, the rule DO/y E(u, z) is left binding, which means it follows the rules. - But these other rules? Nope, they don't follow the rules, so they're not left binding. So, in short, we're trying to simplify things by sticking to certain rules, and these examples show how some grammars follow those rules while others don't.",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: we're talking about some properties that help us figure out how to make the LMG (whatever that is) simpler, so that we can solve"
    },
    {
        "casual_text": "Phoneme embeddings do a good job of capturing the distribution of phonemes and also pick up on their articulatory features without being told to (Silfverberg et al., 2018; Kolachina and Magyar, 2019). So, we used word2vec (Mikolov et al., 2013) to create these phoneme vectors, based on the transcription of the NEGRA corpus (Skut et al., 1997). The transcription was done with a grapheme-to-phoneme tool from the Bavarian Archive for Speech Signals (BAS) (Reichel, 2012, 2014). We used the cbow model with negative sampling and a window size of 1 to get 30-dimensional phoneme embeddings. For word meanings, we rely on word embeddings. We used pre-trained German fastText embeddings (Grave et al., 2018) as the output for our models. This approach of using word embeddings for semantic representation is also seen in other studies (Baayen et al., 2019; Chuang et al., 2020; Hendrix and Sun, 2020).",
        "formal_text": "Convert casual text to formal text: Phoneme embeddings do a good job of capturing the distribution of phonemes and also pick up on their articulatory features without being told to (Silfverberg e"
    },
    {
        "casual_text": "If we use a loss function L(y, y_i) that's basically the negative BLEU score, it’s like doing minimum-risk training on the imputed data, which is something Smith and Eisner (2006) and Li and Eisner (2009) talked about. The cool thing is that the objective function in equation (8) becomes differentiable because each coefficient p_(y | x) is a differentiable function of . This means we can optimize it using gradient-based methods, and in our experiments, we used the L-BFGS algorithm (Liu et al., 1989). We tested this out with the syntax-based MT system called Joshua (Li et al., 2009a), which uses dynamic programming algorithms for second-order expectation semirings (Li and Eisner, 2009) to efficiently calculate the gradients needed for optimizing equation (8).",
        "formal_text": "Convert casual text to formal text: If we use a loss function L(y, y_i) that's basically the negative BLEU score, it’s like doing minimum-risk training on the imputed"
    },
    {
        "casual_text": "Here's how the paper is laid out: Section 2 talks about some language stuff where you need to figure out an implicit predicate based on the meaning of an explicit argument. Section 3 gives a quick rundown of related work. Section 4 covers the SALSA corpus. Section 5 explains our method. Lastly, Section 6 tests our method with experiments, and Section 7 wraps things up.",
        "formal_text": "Convert casual text to formal text: Here's how the paper is laid out: Section 2 talks about some language stuff where you need to figure out an implicit predicate based on the meaning of an explicit argument. Section 3 gives"
    },
    {
        "casual_text": "1. uMQE (Etchegoyhen et al., 2018): This is a method that uses lexical translation tables and statistical language models.",
        "formal_text": "Convert casual text to formal text: 1. uMQE (Etchegoyhen et al., 2018): This is a method that uses lexical translation tables and statistical language models. 2."
    },
    {
        "casual_text": "Off-topic examples in human-rated data are usually too rare to teach an automated system how to spot and reject off-topic responses. As a result, these automated systems tend to struggle more than human raters when it comes to scoring off-topic stuff accurately (Lochbaum et al., 2013; Higgins and Heilman, 2014). To make sure speaking test scores are reliable, there needs to be a way to catch off-topic responses before the scores are finalized (Wang et al., 2019). In our educational app, we use an automated speaking assessment system to help non-native English learners get ready for the IELTS speaking test. We’ve noticed that freemium features tend to attract more off-topic responses because some users just mess around with the system. In this case, being able to detect off-topic stuff accurately is super important. It helps build trust and encourages trial users to upgrade to paid customers.",
        "formal_text": "Convert casual text to formal text: Off-topic examples in human-rated data are usually too rare to teach an automated system how to spot and reject off-topic responses. As a result, these automated systems tend to struggle more than"
    },
    {
        "casual_text": "You can confirm a segment by hitting the ACCEPT button. After you do that, it'll automatically move on to the next segment.",
        "formal_text": "Convert casual text to formal text: You can confirm a segment by hitting the ACCEPT button. After you that, it'll automatically move on to the next segment. Convert casual text to formal text: You can confirm a"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. For the general level-M case, the matrix-matrix product is calculated as: Y = AV = Y(0) + P(0) * ((1) + P(1) * ((2) + P(2) ... and so on. Basically, it's a step-by-step process where you keep adding and multiplying matrices in a specific order.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. For the general level-M case, the matrix-matrix product is calculated as: Y = AV ="
    },
    {
        "casual_text": "We created FunLines as a funny competition to encourage players to do their best. They get points for editing and rating, and their rankings show up on the game's leaderboard.",
        "formal_text": "Convert casual text to formal text: We created FunLines as a funny competition to encourage players to do their best. They get points for editing and rating, and their rankings show their rankings on the game's leaderboard. Con"
    },
    {
        "casual_text": "On the flip side, using events as a big-picture concept has been pretty helpful in picking content for MDS, as shown by studies like Filatova and Hatzivassiloglou (2004) and Li et al. (2006). But, the full potential of events in summarization hasn’t really been explored yet, and not many papers talk about using events for ordering information in MDS. We’re gonna make the case that events are super important for organizing information in MDS, especially when it comes to summarizing multiple news articles. Algorithms that use both event and entity info do a better job than those that only rely on entity info.",
        "formal_text": "Convert casual text to formal text: On the flip side, using events as a big-picture concept has been pretty helpful in picking content for MDS, as shown by studies like Filatova and Hatzivassilogl"
    },
    {
        "casual_text": "Alright, so we took a closer look at the translation results to figure out why the BNNJM with TPD didn't work as well for the Japanese-English (JE) translation task, even though it did better in other tasks. We discovered that switching to BNNJM instead of NNJM for the JE task actually made a big difference in how well infrequent words were translated, but it didn't help much with the more common words.",
        "formal_text": "Convert casual text to formal text: Alright, so we took a closer look at the translation results to figure out why the BNNJM with TPD didn't work as well for the Japanese-English (JE) translation"
    },
    {
        "casual_text": "The system we're talking about here really cuts down on how much space things take up, no matter if we throw in some randomness or not. You can see this pretty clearly by looking at the maximum possible space used by the regular CKY method (check out equation 5) and comparing it to the space used by the smarter, more limited CKY method (equation 6). It's a big difference!",
        "formal_text": "Convert casual text to formal text: The system we're talking about here really cuts down on how much space things take up, no matter if we throw in some randomness or not. You can see this pretty clearly by looking at"
    },
    {
        "casual_text": "We tweaked this ordering to better show how the numbers are arranged, as you can see in Figure 1 below.",
        "formal_text": "Convert casual text to formal text: We tweaked this ordering to better show how the numbers arranged, as you can see in Figure 1 below. Convert casual text to formal text: We tweaked this ordering to better show"
    },
    {
        "casual_text": "If there are no word guesses in a certain time period that score higher than a set limit, we add some edges labeled \"GAP\" with a score that matches that limit.",
        "formal_text": "Convert casual text to formal text: If there are no word guesses in a certain time period that score higher than a set limit, we add some edges labeled \"GAP\" with a score matching that limit"
    },
    {
        "casual_text": "Even with smaller families, we can't really see a big range of differences because there just isn't enough data. But they still show pretty much the same results.",
        "formal_text": "Convert casual text to formal text: Even with smaller families, we can't really see a big range of differences because there just isn't enough data. But they still show pretty much the same results."
    },
    {
        "casual_text": "Figuring out how MeSH terms are connected is a big deal in MeSH indexing. This paper comes up with a way to map out these relationships using GCN and introduces a fresh, all-in-one model for MeSH indexing.",
        "formal_text": "Convert casual text to formal text: Figuring out how MeSH terms are connected is a big deal in MeSH indexing. This paper comes up with a way to map out these relationships using GCN and introduces a"
    },
    {
        "casual_text": "To make this happen, we use various types of knowledge to create a special kind of information storage. This storage works with a constraint-based system for understanding and processing natural language. We've put this approach into practice and tested it in the Advanced Language Engineering Platform (ALEP) environment. ALEP is a big tool for developing natural language processing stuff, built around an object-centered design and a fancy way of describing language using typed feature logic. The European Commission (EC) supports this platform as part of their Linguistic Research and Engineering (LRE) efforts and it's also tied to their upcoming Fourth Framework Programme.",
        "formal_text": "Convert casual text to formal text: To make this happen, we use various types of knowledge to create a special kind of information storage. This storage works with a constraint-based system for understanding and processing natural language. We'"
    },
    {
        "casual_text": "Right before breaking those rules, we pick the center of gravity of the group as a potential spot to focus on.",
        "formal_text": "Convert casual text to formal text: Right before breaking those rules, we pick the center of gravity of the group as a potential spot to focus on. Convert casual text to formal text: Right before breaking those rules, we pick the"
    },
    {
        "casual_text": "Besides the ranking stuff, in section 5, we suggest a training method for each criterion. We also think that with this kind of training in situations with not much data, models can easily overfit and not generalize well. Using a regularizer is super important to stop the model from getting too confident about the training data. We show that when we pair our training ideas with L2-norm regularization, it boosts performance a lot—sometimes by over 30% in accuracy.",
        "formal_text": "Convert casual text to formal text: Besides the ranking stuff, in section 5, we suggest a training method for each criterion. We also think that with this kind of training in situations with not much data, models can easily"
    },
    {
        "casual_text": "The author considers the global embedding structure and suggests a more effective optimization strategy for FLL compared to the greedy algorithm.",
        "formal_text": "Convert casual text to formal text: The author considers the global embedding structure and suggests more effective optimization strategy for FLL compared to the greedy algorithm. Convert casual text to formal text: The author considers the global"
    },
    {
        "casual_text": "Okay, let's break this down in a more casual way: - Is \"wi/wi+1\" acting like a discourse marker? (Like, is it connecting ideas or something?) - Is \"wi/wi+1\" just punctuation? (Like a comma, period, etc.) - Is \"wi/wi+1\" a player's name? - What's the part of speech for \"wi/wi+1\"? (Like, is it a noun, verb, etc.?) - Is one of \"wi/wi+1\" dependent on the other? (Like, does one word rely on the other?) - Are \"wi\" and \"wi+1\" both dependent on the same thing? (Like, do they both rely on the same word?) - What are the dependency relations across the split point? (How do the words connect?) - How high up is \"wi/wi+1\" in the dependency tree? (Where are they in the structure?) - What's the difference in height between \"wi\" and \"wi+1\" in the tree? - What's the (wi, ej) for all the words on the left and right of the split point? (This is a bit technical, but it's about how words relate on either side of the split.) - What's the difference between the best affinity scores for \"wi/wi+1\"? (Like, how similar or different are their scores?) - Do the best affinity scores come from the same event? (Are they related in that way?)",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a more casual way: - Is \"wi/wi+1\" acting like a discourse marker? (Like, is it connecting"
    },
    {
        "casual_text": "If it's the first sentence in the text or if searching through all the active frames doesn't work, the theme gets linked to a \"base\" frame.",
        "formal_text": "Convert casual text to formal text: If it's the first sentence in the text or if searching through all the active frames doesn't work, the theme gets linked to a \"base\" frame. Convert casual text to"
    },
    {
        "casual_text": "We'll use these ranked argument candidates to train the ranking SVM (as introduced by Joachims in 2002) and figure out the model's parameters. When we're testing the model, we just pick the candidate it ranks highest as the final output. One thing to mention is that we don't need any eye gaze data during the testing phase.",
        "formal_text": "Convert casual text to formal text: We'll use these ranked argument candidates to train the ranking SVM (as introduced by Joachims in 2002) and figure out the model's parameters. When we're testing"
    },
    {
        "casual_text": "We should mention that our model was trained on Urban Dictionary data, but the older English dataset from SemEval is from way back in the 1800s, specifically 1810-1860.",
        "formal_text": "Convert casual text to formal text: We should mention that our model was trained on Urban Dictionary data, but the older English dataset from SemEval is from way back the 1800s, specifically 1810-1860. Convert casual text"
    },
    {
        "casual_text": "A bunch of studies have pointed out that using a system dictionary can really help with Chinese word segmentation (Low et al., 2005; Wang et al., 2011). So, we’re using a tool that does Chinese word segmentation and POS tagging based on a system dictionary. We’ve added some extra lexicons we’ve pulled out into that dictionary. These extra lexicons don’t just help with figuring out unknown words, but they also make it easier to handle the issue of how detailed the word segmentation should be.",
        "formal_text": "Convert casual text to formal text: A bunch of studies have pointed out that using a system dictionary can really help with Chinese word segmentation (Low et al., 2005; Wang et al., 2011)."
    },
    {
        "casual_text": "The other three metrics perform really well on this task, with accuracy over 0.95, showing they can easily spot pairs with high similarity. But, just because they're good at identifying these pairs doesn't mean the neighborhoods they create will be totally clean or free of noise. In real life, highly similar characters probably don't show up that often.",
        "formal_text": "Convert casual text to formal text: The other three metrics perform really well on this task, with accuracy over 0.95, showing they can easily spot pairs with high similarity. But, just because they're good at identifying these pairs"
    },
    {
        "casual_text": "One important thing we noticed when adding surface relations was the edge sequence ALIAS, ALIAS INVERSE>. This basically shows how languages often share a name with the group of people who speak them (like Maori or French). Since PRA can create more complex features, we also came across this other edge sequence for the same idea: /people/ethnicity/included in group, ALIAS, ALIAS INVERSE>. This one still points out that languages get their names from groups of people but focuses on subgroups within an ethnicity. Now, these features are super tricky—maybe even impossible—to include in systems that don’t make a clear difference between noun phrases and knowledge base entities. For example, the graphs made by Gardner et al. (2013) or most typical relation extraction systems, which usually just deal with noun phrases after doing some basic entity linking, wouldn’t be able to handle these features.",
        "formal_text": "Convert casual text to formal text: One important thing we noticed when adding surface relations was the edge sequence ALIAS, ALIAS INVERSE>. This basically shows how languages often share a name with the group of people"
    },
    {
        "casual_text": "Basically, this rule tells us to stick a high unrounded back vowel [W] in between consonants or at the end of a word. When we turn this rule into a thing called an FST, it can create stuff like the example below. It changes how we say \"ice cream\" in English to how it sounds in Japanese, ignoring other changes that might happen along the way.",
        "formal_text": "Convert casual text to formal text: Basically, this rule tells us to stick a high unrounded back vowel [W] in between consonants or at the end a word. When we turn this rule into"
    },
    {
        "casual_text": "At any stage of a hypothesis, you might have either none or all 26 OPEN actions to choose from, but only up to one CLOSE action and one SHIFT action. So, when OPEN actions are available, they usually make up most or all of the possible next moves. To make things easier and reduce the number of options to consider, it makes sense to think about whether we can skip some of these actions by using a simpler model to weed them out early.",
        "formal_text": "Convert casual text to formal text: At any stage of a hypothesis, you might have either none or all 26 OPEN actions to choose from, but only up to one CLOSE action and one SHIFT action. So, when"
    },
    {
        "casual_text": "Sure, the cross-project approach makes sense for testing how well a model works on different projects. But when it comes to using it in batch mode across projects, it does rely on some pretty big assumptions—like there being no documentation at all for any of the methods in the projects you're looking at.",
        "formal_text": "Convert casual text to formal text: Sure, the cross-project approach makes sense for testing how well a model works on different projects. But when it comes to using it in batch mode across projects, it does rely on some pretty"
    },
    {
        "casual_text": "In the paper, we noticed that depth doesn't do better than the RGB model for certain classes of objects. This happens when the depth map doesn't change much across those objects.",
        "formal_text": "Convert casual text to formal text: In the paper, we noticed that depth doesn't do better than the RGB model for certain classes of objects. This happens when the depth map doesn't change much across those objects. Convert casual"
    },
    {
        "casual_text": "So, Topic CVaR helps reduce the differences between topics by minimizing something called KL divergences. This idea works well with the overall goal of training p  to match the test data distribution. Unlike in the MLE method, where minimizing KL is the same as minimizing log loss, here it's different. In MLE, if you minimize KL(p train",
        "formal_text": "Convert casual text to formal text: So, Topic CVaR helps reduce the differences between topics by minimizing something called KL divergences. This idea works well with the overall goal of training p  to match the test"
    },
    {
        "casual_text": "Training an ED system usually needs a ton of labeled data. But labeling stuff is complicated and expensive, so there aren’t many good resources, and they’re not evenly spread across languages (Hsi et al., 2016). This makes it tough to build an ED system for languages with not enough training data. Cross-lingual ED (Ji, 2009; Chen and * Equal contribution. Ji, 2009; Zhu et al., 2014; Hsi et al., 2016; Liu et al., 2018a) tries to solve this by sharing knowledge between languages to improve performance. The problem is, older cross-lingual ED methods depend on super good machine translation (MT) systems or manually aligned documents to work well. But parallel resources, like large sets of matching sentences in different languages, are only available for a few language pairs (Koehn et al., 2007), which makes these methods pretty limited in how useful they can be.",
        "formal_text": "Convert casual text to formal text: Training an ED system usually needs a ton of labeled data. But labeling stuff is complicated and expensive, so there aren’t many good resources, and they’re not"
    },
    {
        "casual_text": "When we talk about pronouns in language, it’s like trying to figure out who or what a pronoun is pointing to. This is called coreference, where the pronoun and its \"antecedent\" (the thing it’s referring to) both point to the same thing. The main challenge in figuring this out is called pronoun resolution. A lot of research has been done on this, trying to figure out the right object by looking at the grammar and meaning of words in a sentence. People like Sidner, Brennan, and Kameyama have worked on this for years. The tricky part is when the thing the pronoun is referring to isn’t directly mentioned in the text. In those cases, you need to use common sense and really think hard to figure it out, like in those classic examples Charniak talked about. The problem is, this whole process relies a lot on getting the grammar and meaning of the sentence exactly right. Because of that, it’s not something we can easily use in real-world systems just yet.",
        "formal_text": "Convert casual text to formal text: When we talk about pronouns in language, it’s like trying to figure out who or what a pronoun is pointing to. This is called coreference, where the pro"
    },
    {
        "casual_text": "Rankings should work consistently within the same dataset (like on a dev set) and also be able to apply to similar datasets (like a test set). To check the first part, we look at how stable the rankings are when we split the development data into separate groups (Buckley and Voorhees, 2000). For the second part, we compare the rankings from the development set to those from the test set to see how well they match up (Voorhees, 1998).",
        "formal_text": "Convert casual text to formal text: Rankings should work consistently within the same dataset (like on a dev set) and also be able to apply to similar datasets (like a test set). To check the first part"
    },
    {
        "casual_text": "Our model, as explained earlier, figures out how French words, phrases, and alignments fit together, along with some parameters, based on an English sentence and some extra settings we call hyperparameters.",
        "formal_text": "Convert casual text to formal text: Our model, as explained earlier figures out how French words, phrases, and alignments fit together, along with some parameters, based on an English sentence and some extra settings we call hyperparameters"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way. So, our rule system gives some words a secondary stress, especially if they're single-morpheme words or the first part of a compound word. Then, another rule kicks in to bump that secondary stress up to primary stress. The numbers you see are like a checklist, showing how many times something needs to happen. For example, \"2\" means it has to happen at least twice. Now, let's look at some specific rules: - If you have a word that starts with a vowel and is followed by a consonant, it gets stressed. - If a word starts with two consonants and then a vowel, it also gets stressed. - And if a word has two consonants in a row, followed by a vowel, it gets stressed too. So, in short, these rules help figure out where the stress should go in a word, depending on how it's spelled.",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in a simpler way. So, our rule system gives some words a secondary stress, especially if they're single-morpheme words or the"
    },
    {
        "casual_text": "We’re working on a basic version of local word discovery using an FST (Finite State Transducer) to connect known morphemes found in a noisy phone sequence to more complex word forms. Our method relies on two main assumptions: first, that the morphosyntactic description is detailed enough to be turned into an FST, and second, that we have a decent phone recognizer, which could be something simple, like training one on a few hours of transcribed audio from similar languages, or tweaking a bigger pretrained model. For speech representation, we’re using Allosaurus to give us a low-dimensional way to represent speech, which helps with matching it to phone sequences predicted by our morphological transducer. Allosaurus comes with a pretrained model that lets us limit the output to a specific set of phones, which is pretty handy (Li et al., 2020). It supports phone inventories for over 2,000 languages out of the box, including Kunwinjku. However, we noticed that the default inventory for Kunwinjku was missing some stuff, so we made our own based on (Evans, 2003).",
        "formal_text": "Convert casual text to formal text: We’re working on a basic version of local word discovery using an FST (Finite State Transducer) to connect known morphemes found in a noisy phone sequence to more"
    },
    {
        "casual_text": "The results on the real test set are way worse than those on Ori. and Avg., showing that real user evaluations are way tougher. This is because real cases can have multiple robustness problems all at once, while each augmentation method in LAUG looks at them one by one. Even with this difference, the model's performance on real data gets a lot better after being fine-tuned on the augmented data, proving that LAUG really helps boost the model's real-world toughness. Table 13 checks which kind of errors the model made on the real test set by going through all the mistakes BERT Ori. made. \"Others\" are errors that aren't due to robustness issues, like when the model just doesn't perform well. It turns out the model struggles a lot with LU robustness (over 70%), and almost half the errors are because of different language varieties. This happens because real user evaluations have way more diverse ways of saying things than the original data. After training with the augmented data, we see fewer errors related to Speech Characteristics and Noise Perturbation, meaning BERT Aug. handles these issues better. Note that the total percentage of errors is over 100% because 25% of the cases have more than one robustness issue. This just shows again that real user evaluations are harder than the original test set.",
        "formal_text": "Convert casual text to formal text: The results on the real test set are way worse than those on Ori. and Avg., showing that real user evaluations are way tougher. This is because real cases can have multiple"
    },
    {
        "casual_text": "There’s no straightforward, step-by-step way to figure out someone’s competence based on how they perform, just like there’s no simple formula to turn a bunch of observations into a solid scientific theory. But just because it’s not easy doesn’t mean scientists—like physicists—can skip coming up with theories. The same goes for linguists; they still need to come up with theories about how language works. Testing these theories isn’t usually a direct process, though. It’s more like how other sciences work, where you use indirect methods to check if something holds up. For example, if we want to know if John can understand a super long sentence (like ten billion words long), we’re not going to actually hand him that sentence to read. Instead, we’d use other, more roundabout ways to figure it out. For instance, we might see if John understands simpler sentences like “Paul is hungry” or “David is thirsty.” And we’d also check if he gets the rule that if you have two sentences (let’s call them  and ), putting “and” between them makes a new sentence. Now, figuring this out might not be super easy or totally foolproof, but hey, we often claim to know stuff like this anyway.",
        "formal_text": "Convert casual text to formal text: There’s no straightforward, step-by-step way to figure out someone’s competence based on how they perform, just like there’s no simple formula to turn a bunch of observations"
    },
    {
        "casual_text": "The weights for the features \"functional words translation strength\" and \"alignment obliqueness\" depend on the language pair because of how the source and target languages handle word order. On top of that, these weights need to be tweaked based on the translation dictionary being used, since the word alignments are built using that dictionary. Plus, since this process isn't symmetrical, we'll need to figure out different weights when translating from the source language to the target and back again.",
        "formal_text": "Convert casual text to formal text: The weights for the features \"functional words translation strength\" and \"alignment obliqueness\" depend on the language pair because of how the source and target languages handle word order. On"
    },
    {
        "casual_text": "Using the CUI model, the generator creates the final interface that the device will run and connects it to the Papillon database. For our project, we end up with:",
        "formal_text": "Convert casual text to formal text: Using the CUI model, the generator creates the final interface that the device will run and connects it to the Papillon database. For our project, we end up with: Convert"
    },
    {
        "casual_text": "One big reason for sparsity is how we put words together to make new meanings. With a small set of words, the number of ways you can combine them grows really fast as the combinations get longer. Being able to understand and create all kinds of new mixes from existing words—what we call compositional generalization—has been shown to be a weak point for a lot of machine learning methods.",
        "formal_text": "Convert casual text to formal text: One big reason for sparsity is how we put words together to make new meanings. With a small set of words, the number of ways you can combine them grows really fast as the combinations"
    },
    {
        "casual_text": "According to research by Joshi et al. (2020), there have been bigger projects that have created datasets in various languages, like TyDi QA (Clark et al. , 2020), XQuAD (Artetxe et al. , 2020), and MLQA (Lewis et al. , 2020). But even with these efforts, the variety of languages and their structures in question answering datasets is still way behind the actual diversity of the world's languages. For instance, TyDi QA covers 11 languages, which is less than 0.2% of the roughly 6,500 languages spoken worldwide (Hammarström, 2015). These 11 languages come from 9 different language families, but their typological diversity score is only 0.41, calculated on a scale from 0 to 1 using the method defined by Ponti et al. (2020). MLQA, on the other hand, has data for 7 languages from 4 families, with a typological diversity score of 0.32. The total population covered by TyDi QA, based on estimates from Glottolog (Nordhoff and Hammarström, 2012), is less than 20% of the world's population (the languages in TyDi QA are spoken by around 1.45 billion people).",
        "formal_text": "Convert casual text to formal text: According to research by Joshi et al. (2020), there have been bigger projects that have created datasets in various languages, like TyDi QA (Clark et"
    },
    {
        "casual_text": "To check how well our models performed, we used the BLEU and SARI scores from the EASSE library (Alva-Manchego et al., 2019) and the Flesch-Kincaid Grade Level score, tweaked for Russian. We tested our models on two types of datasets: one with paragraphs from books and novels they hadn’t seen during training, and another smaller, hand-picked dataset mostly made up of kids' literature, which we used to evaluate alignment. For the dataset aligned with Bleualign, both the test and development sets had 1000 paragraphs each. The ones aligned with CATS had 1500 paragraphs. The best results for each system on the bigger test sets are shown in Table 5.",
        "formal_text": "Convert casual text to formal text: To check how well our models performed, we used the BLEU and SARI scores from the EASSE library (Alva-Manchego et al., 2019) and"
    },
    {
        "casual_text": "The idea behind word sense discrimination is to group the different ways a word is used in sentences based on its actual meaning. This is usually tackled as an unsupervised learning problem, where all you have is a big pile of text (like in studies by Pedersen and Bruce, 1997, Schütze, 1998, and Purandare and Pedersen, 2004). These methods typically need you to know beforehand how many groups (or clusters) you're looking for, which is often labeled as \"k.\" But in real-world situations, you usually don't know what k should be.",
        "formal_text": "Convert casual text to formal text: The idea behind word sense discrimination is to group the different ways a word is used in sentences based on its actual meaning. This is usually tackled as an unsupervised learning problem, where all"
    },
    {
        "casual_text": "So, there's this issue with \"unknown words\"—basically, words that weren't in the training data—which is a big deal in part-of-speech tagging. In our case, it's even trickier because some words that actually are in the training data might still be treated as unknown due to weird spelling variations. In the experiments we're talking about, we let unknown words be any part of speech (which makes sense in this context), but we give different parts of speech different levels of importance. Specifically, if a word isn't in our word list, we tweak the calculation in equation (2) (check out equation 1 above for reference).",
        "formal_text": "Convert casual text to formal text: So, there's this issue with \"unknown words\"—basically, words that weren't in the training data—which is a big deal in part-of-speech"
    },
    {
        "casual_text": "Using a PVP (P, v) helps us tackle task A like this: We start with an input x, and we use P to turn it into a representation called P(x). Then, M takes over and figures out the label y from the set L that fits best, where v(y) is the most likely replacement for the mask. For instance, let's say we're trying to figure out if two sentences, a and b, contradict each other (label y0) or agree with each other (y1). For this task, we might use the pattern P(a, b) = a?",
        "formal_text": "Convert casual text to formal text: Using a PVP (P, v) helps us tackle task A like this: We start with an input x, and we use P to turn it into a representation called P"
    },
    {
        "casual_text": "• GCAE (Xue and Li, 2018): This cutting-edge method uses a convolutional neural network with gating mechanisms, designed for both aspect-category and aspect-term sentiment analysis. We’re focusing on comparing it with its aspect-category sentiment analysis part.",
        "formal_text": "Convert casual text to formal text: • GCAE (Xue and Li, 2018): This cutting-edge method uses a convolutional neural network with gating mechanisms, designed for both aspect-category"
    },
    {
        "casual_text": "On the flip side, the syntactic details from the input sentences can help clear up any confusion in the initial skeleton step. For instance, as you can see in Figure 6, models without the GAT-encoder might mess up the relationship between k1 and k4, which is actually the discourse relation between the two short sentences. The main issue here could be the word \"if\" in the second sentence, which is a clue for the After relation. But when the syntactic info is processed by the GAT encoder, the GAT-enc+dec model can pick up on the finer details, like how \"if\" changes things, and correctly figure out the accurate relation between the two sentences (which is Conti.).",
        "formal_text": "Convert casual text to formal text: On the flip side, the syntactic details from the input sentences can help clear up any confusion in the initial skeleton step. For instance, as you can see in Figure 6, models without"
    },
    {
        "casual_text": "Plus, we're using the B3 F1 score (from Bagga and Baldwin, 1998) as our evaluation metric. This is something that's been used a lot in earlier studies (like Marcheggiani and Titov, 2016; Elsahar et al., 2017; Wu et al., 2019; Hu et al., 2020). The F1 score is calculated as the harmonic mean of precision and recall, and it's more influenced by whichever one is lower. This makes it a pretty fair way to show how well the model is performing.",
        "formal_text": "Convert casual text to formal text: Plus, we're using the B3 F1 score (from Bagga and Baldwin, 1998) as our evaluation metric. This is something that's been used a lot in earlier"
    },
    {
        "casual_text": "Alright, so s/e represents the probabilities for the start and end of each token, and y_s/y_e are the actual start and end targets. Now, when it comes to predicting the answer, some tests have questions that don’t really have an answer. So, we start by calculating the score for the span between the i-th and j-th tokens like this:",
        "formal_text": "Convert casual text to formal text: Alright, so s/e represents the probabilities for the start and end of each token, and y_s/y_e are the actual start and end targets. Now, when"
    },
    {
        "casual_text": "So, we noticed that even after practicing a lot with different forms, people still don't do well on the \"wug test.\"",
        "formal_text": "Convert casual text: So, we noticed that even after practicing a lot with different forms with different forms still ... Convert casual text: So, we noticed that even after practicing a lot with different forms still ... Convert"
    },
    {
        "casual_text": "I'm going to expand on some ideas from recent years about underspecified semantic formalisms, like the ones mentioned by Reyle in 1995. The goal is to create an underspecified representation for discourse structure. To do this, I'll use a first-order tree logic developed by Kallmeyer in 1996 to define an underspecified version of SDRT. I'll explain this in more detail in the next sections.",
        "formal_text": "Convert casual text to formal text: I'm going to expand on some ideas from recent years about underspecified semantic formalisms, like the ones mentioned by Reyle in 1995. The goal is to create an underspecified"
    },
    {
        "casual_text": "This method ensures that the translation system always hits the exact length requirement (which is a strict rule), but it comes with a big downside. The system doesn't know how many more words it can add until it gets to the required length. So, it can't adjust by making the start of the sentence shorter to meet the limit.",
        "formal_text": "Convert casual text to formal text: This method ensures that the translation system always hits the exact length requirement (which is a strict rule), but it comes with a big downside. The system doesn't know how many more words"
    },
    {
        "casual_text": "Using edit distance for stuff like spell checking is a bit different from how it's used in EBMT and TM. Here's why: 1. Normally, you compare character sequences (like strings). But for our tests, we're looking at sequences of symbols or tokens (basically words and all the punctuation marks). 2. If you have a big dictionary for checking words, the distance between an input word and a word in the dictionary is usually pretty small. But for matching sentences, things are different. The alphabet size (all the possible symbols) is way bigger (think different characters vs. different words), and the minimum edit distance can be much larger. So, there's no one-size-fits-all threshold for edit distance in practice. 3. Having an upper limit for edit distance can really speed things up. But for what we're doing, we don't know an upper limit for mismatches beforehand. 4. We want to find all the matches with the highest similarity, not just one close match or the best single match. In the experiments we talked about in this paper, we didn't use shortcuts like bucketing or assuming an upper limit for edit distance to speed things up. While these methods could help reduce runtime for our approach, they might also make the results less accurate.",
        "formal_text": "Convert casual text to formal text: Using edit distance for stuff like spell checking is a bit different from how it's used in EBMT and TM. Here's why: 1. Normally, you compare character sequence"
    },
    {
        "casual_text": "After deep learning started doing really well in a bunch of different tasks, it's become important to create a single model that can handle various areas without needing extra adjustments for new, unrelated data. This is crucial for real-world use because, in practice, a model needs to work well with data it hasn't seen before.",
        "formal_text": "Convert casual text to formal text: After deep learning started doing really well in a bunch of different tasks, it's become important to create a single model that can handle various areas without needing extra adjustments for new, unrelated"
    },
    {
        "casual_text": "Alright, let's break this down into something more readable and casual. Here's the informal version: \"Hey, so you're up for some fun, right? We've got this plan where we can chill and do some cool stuff together. We'll grab some snacks, hang out, and maybe play some games. It's all about having a good time with friends. We've got a bunch of ideas lined up, like going to that new spot downtown, or maybe checking out that event happening this weekend. Whatever we choose, it's gonna be a blast. We'll figure out the details later, but for now, just know that we're all in this together. Oh, and don't forget, we're thinking of doing a potluck-style thing, so bring your favorite dish if you can. It'll be a great way to share some good food and laughs. So, yeah, just keep your schedule open and be ready for some fun. We'll touch base soon and make it happen. Can't wait to see you there!\" This version keeps the same general idea but makes it way more conversational and easy to understand.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down into something more readable and casual. Here's the informal version: \"Hey, so you're up for some fun, right? We'"
    },
    {
        "casual_text": "The appendix has a detailed look at how accurate each model is for different dietary restrictions.",
        "formal_text": "Convert casual text to formal text: The appendix has a detailed dietary restriction. The appendix has a detailed dietary. Convert casual text to formal text: Convert casual text to formal text: Convert"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. First, we tested how well our method works with and without using semantic information (check out Equation 2 for the details). Table 5 shows that including semantic info really helps improve the results, especially when it comes to distinguishing between different classes. Next, we wanted to figure out which words are the most important when predicting whether a review is positive or negative. To do this, we used the word-level sentiment labels from Liu’s Opinion Lexicon. We looked at the top 10 words that had the biggest impact on predicting sentiment in the IMDB dataset. We tested this on 1000 movie reviews. Table 6 shows that our method does a better job at picking out the most important words for sentiment classification. In other words, it’s better at finding the right sentiment-related words compared to LIME.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. First, we tested how well our method works with and without using semantic information (check out Equation 2 for the details). Table"
    },
    {
        "casual_text": "Okay, so let's say y a i, m can be either \"Good\" or \"Bad\", y b i can be \"Related\" or \"Not-related\", and y c i, m can be \"Relevant\" or \"Irrelevant\". These are the labels for subtasks A, B, and C, respectively. As we mentioned earlier, subtask C relies on the other two subtasks. Basically, if c i m is a good comment related to the existing question q i, and q i is connected to the new question q (subtask A), then c i m is probably a relevant answer to q. Similarly, subtask B can also get some help from subtask C: if a comment c i m in the thread answering q i is relevant to q, then it's likely that q i is related to q.",
        "formal_text": "Convert casual text to formal text: Okay, so let's say y a i, m can be either \"Good\" or \"Bad\", y b i can be \"Related\""
    },
    {
        "casual_text": "We set up the generation process as conditional text generation and fine-tune GPT-2 (from Radford et al., 2019) to create a generator called Polyjuice, using pairs of (x, x). To make it easier to focus on specific counterfactuals, we also add control codes, like negation or delete (check out Figure 1B), and use fill-in-the-blank structures (inspired by Donahue et al., 2020) to clearly mark where the changes happen and what kind of changes to make. Our internal evaluation shows that Polyjuice creates outputs that are fluent, varied, and pretty close to the original x. Plus, the control mechanisms help it generate changes that you probably wouldn’t get from standard language models.",
        "formal_text": "Convert casual text to formal text: We set up the generation process as conditional text generation and fine-tune GPT-2 (from Radford et al., 2019) to create a generator called Polyjuice,"
    },
    {
        "casual_text": "We tested and looked at the proposed model using the monotonicity subset from Semantic Fragments (Richardson et al., 2020), as well as HELP (Yanaka et al., 2019b) and MED (Yanaka et al., 2019a). We also expanded MED to create a dataset for testing 2-hop inference. The model can successfully learn natural logic operations through end-to-end training. 1 2 Related Work",
        "formal_text": "Convert casual text to formal text: We tested and looked at the proposed model using the monotonicity subset from Semantic Fragments (Richardson et al., 2020), as well as HELP"
    },
    {
        "casual_text": "\"Her (film)\" is a movie, but the word \"her\" is usually just a possessive pronoun. Since \"her\" doesn't often show up as a link to \"Her (film)\" in Wikipedia articles, it's usually not considered a mention that connects to the movie.",
        "formal_text": "Convert casual text to formal text: \"Her (film)\" is a movie, but the word \"her\" is usually just a possessive pronoun. Since \"her\" doesn't often show up as a"
    },
    {
        "casual_text": "Indexed grammars are like a simpler version of monadic predicate grammar, where a nonterminal can only have one argument at a time.",
        "formal_text": "Convert casual text to formal text: Indexed grammars are like a simpler version of monadic predicate grammar, where a nonterminal can only have one argument at a time. Convert casual text to"
    },
    {
        "casual_text": "Alright, let me break this down in simpler terms. So, we've got these individual features we talked about earlier, and when we put them all together, we get one big feature for a specific instance, which we call F x. Now, when we compare this instance x (let's say it's our test instance) with another instance x (this time, our training instance), we're looking at how many features they don't have in common. Specifically, we count the number of features that are filled in (not empty) in one instance but not in the other. That count is the distance between these two instances.",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in simpler terms. So, we've got these individual features we talked about earlier, and when we put them all together, we get one big feature for"
    },
    {
        "casual_text": "[Problem 6] (Setting Layers of 'Understanding') As mentioned in Sections 6 and 7, we can identify at least two main layers of understanding and knowledge that are important for MT. The big questions are: Can these two types of understanding and knowledge be combined into one system? How should they work together with the language processing stuff (like analysis, transfer, and generation)? And, how much of this knowledge can actually be put into MT systems? These are the things we need to figure out.",
        "formal_text": "Convert casual text to formal text: [Problem 6] (Setting Layers of 'Understanding') As mentioned in Sections 6 and 7, we can identify at least two main layers of understanding and knowledge that are"
    },
    {
        "casual_text": "So, we looked at the tricky parts of figuring out all kinds of product problems from customer feedback. To deal with these challenges, we came up with the idea of treating the problem like a seq2seq modeling thing and used a text-to-text transfer learning setup. We made things more efficient by using one model to handle different levels of product issues, which saves on costs. The results show that our model is pretty much on par with humans when it comes to identifying issues. We also noticed that our method doesn’t need a ton of labels and works well even for issues it hasn’t seen before. Next up, we’re planning to find ways to make the model even better and more reliable for predicting bigger-picture issues.",
        "formal_text": "Convert casual text to formal text: So, we looked at the tricky parts of figuring out all kinds of product problems from customer feedback. To deal with these challenges, we came up with the idea of treating the problem like a se"
    },
    {
        "casual_text": "Okay, so here's the deal: we're dealing with spanning cells as boundaries for checking how similar things are. Let's use Table 7 as an example. We start checking from the bottom-right cell, which is 360, and we look at each row and column within these boundaries. The cell \"1999. 04. 01-2000. 03. 31\" is a spanning cell, so the 2nd row becomes a boundary. Similarly, \"Price\" is a spanning cell, so the 2nd column is a boundary too. This means we can look at the table tags both row by row and column by column. After that, we move on to the next round. We shift our starting points to new bottom-right positions, which are (3, 5) and (9, 3). In this round, we reset the boundaries. The cells \"DP9LAX01AB\" and \"Adult\" (\"Child\") are spanning cells, so the 1st row and 1st column become our new boundaries. This time, we focus on \"row-wise\" checking.",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: we're dealing with spanning cells as boundaries for checking how similar things are. Let's use Table 7 as an example. We start checking from the"
    },
    {
        "casual_text": "Our method works really well on standard evaluation metrics. It consistently beats the baseline models, especially on SPICE and CIDEr. For example, our model boosts the AoANet baseline from 118.4 to 119.1 on CIDEr and from 21.5 to 21.7 on SPICE during the MLE phase. It also improves the ATTN baseline on CIDEr from 117.4 to 120.1 and on SPICE from 20.5 to 21.0 during the RL phase. Since CIDEr uses tf-idf weighting, it helps to highlight methods that generate more specific details about images that aren’t common across the dataset. Our method is designed to encourage models to create sentences with more objects, attributes, or relationships, which is why we also see improvements on SPICE. When it comes to descriptiveness-related metrics, our method consistently performs better on R@1 and R@5 in both the MLE and RL phases. The model also improves retrieval performance on the DISC baseline, boosting R@1 from 46.5 to 48.1 and R@5 from 83.6 to 87.9. Our weighted CIDEr reward works well alongside the discriminative loss in DISC, further improving retrieval performance. We also used an externally trained NLI model (from Section 3.1) to analyze the relationships between captions generated by our method and the baselines (AoA and DISC) on the test set. Figure 2 shows that our model generates more descriptive sentences.",
        "formal_text": "Convert casual text to formal text: Our method works really well on standard evaluation metrics. It consistently beats the baseline models, especially on SPICE and CIDEr. For example, our model boosts the AoANet baseline from"
    },
    {
        "casual_text": "Metrology is like a big umbrella for making sure measurements are consistent across all kinds of science, so we can compare things properly. Computer science has taken some ideas from metrology, especially the concept of reproducibility, but it doesn’t always use the same definitions (as we talked about in Section 2). Here, we’re going to explain quantified reproducibility assessment (QRA), which is basically a method that takes the ideas and definitions from metrology and uses them exactly as they are. This helps us figure out how similar numerical results are and how similar the studies that produced them are. First, we’ll go over the concepts and definitions that QRA is built on, then we’ll give you an idea of the framework (in Section 3.2) and how to actually use it in real life (in Section 3.3).",
        "formal_text": "Convert casual text to formal text: Metrology is like a big umbrella for making sure measurements are consistent across all kinds of science, so we can compare things properly. Computer science has taken some ideas from metrology, especially the concept of"
    },
    {
        "casual_text": "We average the final precision, recall, and F-measure across different communities.",
        "formal_text": "Convert casual text to formal text: We average the final precision, recall, and Fmeasure across different communities. Convert casual text to formal text: We average the final precision, recall, and Fmeasure across different communities."
    },
    {
        "casual_text": "Hey, check this out: It's from the 20th China National Conference on Computational Linguistics. The pages are 895 to 905, and it happened in Hohhot, China, from August 13th to 15th, 2021. It's all about computational linguistics stuff, and it's brought to you by the Technical Committee on Computational Linguistics under the Chinese Information Processing Society of China.",
        "formal_text": "Convert casual text to formal text: Hey, check this out: It's from the 20th China National Conference on Computational Linguistics. The pages are 895 to 905, and it happened in Hohhot,"
    },
    {
        "casual_text": "Using FastSeq, we’ve seen a big boost in performance—like a 4x to 9x speedup, depending on the case. Check out Table 1 for more info. In the original setup, when working with the CNN/DailyMail summarization dataset, models like BART, DistilBART, ProphetNet, GPT2, and UniLM were processing between 1.7 and 3.4 samples per second. But once we turned on FastSeq, all those models shot up to over 10 samples per second. The BART model, for example, hit 18.4 samples per second, which is a 7.7x speedup. On the WMT16 translation datasets, FastSeq also helped increase throughput by 4.3 times.",
        "formal_text": "Convert casual text to formal text: Using FastSeq, we’ve seen a big boost in performance—like a 4x to 9x speedup, depending on the case. Check out Table 1 for more info"
    },
    {
        "casual_text": "For a long time, psycholinguistic studies on reading and comprehension were all about latent semantic analysis (LSA), as introduced by Deerwester et al. in 1990. But then, Pynte et al. (2008) came along and showed that a document-level approach to understanding long-range semantics could predict gaze duration better than earlier eye movement measures. They used the Dundee corpus, which focuses on discourse rather than just single sentences. On the other hand, Griffiths et al. (2007) found that topic models might actually do a better job than LSA in psycholinguistic experiments, especially when it comes to predicting gaze durations for ambiguous words, as proposed by Blei et al. in 2003. Meanwhile, McDonald and Shillcock (2003) suggested that a word 2-gram model might be more about low-level contextual properties since it’s better at explaining first fixation duration rather than later eye movements. Smith and Levy (2013) took it a step further and showed that a Kneser-Ney smoothed 3-gram model could predict gaze duration too, likely because it uses a bigger contextual window when dealing with discourse comprehension, similar to what Pynte et al. did with the Dundee data. Lastly, Frank (2009) demonstrated that a simple recurrent neural network is more effective at handling gaze duration data compared to a probabilistic context-free grammar, as suggested by Demberg and Keller in 2008.",
        "formal_text": "Convert casual text to formal text: For a long time, psycholinguistic studies on reading and comprehension were all about latent semantic analysis (LSA), as introduced by Deerwester et al. in 1990. But then"
    },
    {
        "casual_text": "Sure! Here's a more casual version: - \"ve\" (they), \"vh\" (he/she/it), \"tum\" (you), \"p\" (you-formal) - These pronouns combine with postpositions, but they’re not written separately. For example: - \"lke ke lie\" (for the boy), - \"kmre m\" (in the room), - \"mez pr\" (on the table) - Also, you can add a particle or discourse marker before a noun, like: - \"lk h\" (only the girl), - \"lk bh\" (the girl too), - \"pn tk\" (even water)",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: - \"ve\" (they), \"vh\" (he/she/it), \"tum\" (you), \""
    },
    {
        "casual_text": "TAC, also known as TripWire, is a new platform that the DIA is using in its analytical departments. It lets users take a deep, ongoing look at different issues together, all in real-time. TAC uses SysTran language tools to translate foreign language RSS alerts and is working on adding Language Weaver to handle translations for Arabic and Chinese.",
        "formal_text": "Convert casual text to formal text: TAC, also known as TripWire, is a new platform that the DIA is using in its analytical departments. It lets users take a deep, ongoing look at different issues together,"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. We're dealing with simple sentences, where each sentence has a bunch of words (called \"case fillers\") followed by a verb. Our job is to figure out the meaning, or \"sense,\" of each verb in the sentence. To do this, we use a list of verb senses from a dictionary called \"IPAL\" (from 1987), which also includes examples of these case fillers. We also use a method by Kurohashi to measure how similar two case fillers are, or more specifically, how similar the main nouns in those case fillers are. The similarity is based on something called the \"length of the path\" between two nouns in a structure called \"HPSG\" (don't worry too much about that). This similarity is calculated using a formula, and we use a tool called \"Hyokiyo\" (from the National Language Research Institute, 1964) to help with this. Following Kurohashi's method, we define a similarity score between two words, X and Y, as shown in Table 1. It's important to note that this method works independently of the resources we use. To show how the whole process works, we'll use some examples mentioned earlier and compare them to a more general case, as shown in Figure 4. The input we're working with is a set of case fillers and a verb, like nc, 'mc), nc: 'm. ce, v. The possible meanings (or \"senses\") for the verb v are s1, s2, and s3, and we get these from a database.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way. We're dealing with simple sentences, where each sentence has a bunch of words (called \"case fillers\") followed by"
    },
    {
        "casual_text": "So far, we've been talking about semantic spaces that are built using how often words appear together. But, the way words are spread out across different documents can also give us some pretty useful information about their meanings. Latent Semantic Analysis (LSA), which was introduced by Landauer and Dumais in 1997, uses exactly this kind of distribution information to figure out hidden semantic factors by simplifying the data (a process called dimensionality reduction). To do this, LSA uses a method called Singular Value Decomposition (SVD), developed by Berry and his team in 1994. SVD takes a big table (matrix) that shows how often words and documents appear together and breaks it down into smaller, simpler pieces. One of these pieces shows words in terms of these hidden factors, and another shows documents in the same way. This setup allows us to see that any document can be thought of as a mix (linear combination) of the words it contains. So, in this way of looking at things, it makes sense to treat groups of words as if they were a mini-document and represent them by mixing the vectors of the individual words.",
        "formal_text": "Convert casual text to formal text: So far, we've been talking about semantic spaces that are built using how often words appear together. But, the way words are spread out across different documents can also give us some pretty useful information about"
    },
    {
        "casual_text": "The framework by Coecke and his colleagues from 2010 is pretty abstract. It doesn't give you specific instructions on how to build tensors for words with special roles, like verbs or adjectives. And even more importantly, it doesn't tell you exactly what the sentence space S should look like, so you're kind of left to figure that out yourself.",
        "formal_text": "Convert casual text to formal text: The framework by Coecke and his colleagues from 2010 is pretty abstract. It doesn't give you specific instructions on how to build tensors for words with special roles, like verbs"
    },
    {
        "casual_text": "This paper isn’t just about speeding up single-threaded decoding; it’s also about fixing the issues with current decoding setups on multicore servers. Unlike what Fernández et al. (2016) did, we’re going to dig deeper into the \"black box\" to boost decoding speed. We’ll compare how our implementation performs on multiple cores against the top-performing phrase-table from Junczys-Dowmunt (2012). We’ll use the cube-pruning algorithm, but the standard phrase-based decoding is also an option, and there’s a framework in place to add other algorithms later on. We’re using KenLM (Heafield, 2011) because it’s widely used and reliable, but like with Moses, we can swap in other language models if needed.",
        "formal_text": "Convert casual text to formal text: This paper isn’t just about speeding up single-threaded decoding; it’s also about fixing the issues with current decoding setups on multicore servers. Unlike what"
    },
    {
        "casual_text": "Sure! Here's a more casual version: The words  (jià) meaning \"marry,\"  (q) also meaning \"marry,\" and  (hn) meaning \"marriage\" belong to Subfamily #2. Then there's  (y) for \"aunt,\"  (ji) for \"older sister,\" and  (mèi) for \"younger sister.\" In Table 1, each group of characters is linked to a specific meaning or idea. The members of each group are all the characters that share that meaning. Some characters in the same group are really similar in meaning (that's the \"Character sets\" column), but others might not seem as close (that's the \"Other characters\" column).",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: The words  (jià) meaning \"marry,\"  (q) also meaning \"marry,\" and  ("
    },
    {
        "casual_text": "Basically, in most cases, the closest words to \"vehicals vehical,\" \"vehicles,\" \"vehicels,\" \"vehicular,\" \"cars,\" \"vehicle,\" and \"automobiles\" are kind of like saying the same thing in different ways. The PARAGRAM-PHRASE model, not surprisingly, can't handle stuff like \"not.\" So, in every case, the nearest neighbor to \"not\" is just \"not\" itself, because it’s way more important than the word it’s modifying. The other close words are either the word being modified or just \"stalled.\"",
        "formal_text": "Convert casual text to formal text: Basically, in most cases, the closest words to \"vehicals vehical,\" \"vehicles,\" \"vehicels,\" \"vehicular,\" \"car"
    },
    {
        "casual_text": "First off, some expressions can be subjective without really leaning positive or negative. Like, take this example from (Wilson et al., 2005a): Jerome says the hospital feels just like a hospital in the states. An NLP system might want to dig into all kinds of personal stuff—like someone’s motivations, thoughts, or guesses—not just whether they’re feeling happy or sad. Second, sentiment analysis can work better if you break it down. You can start by figuring out if something is neutral or has some kind of polarity (like positive or negative), and then figure out the specific polarity. This idea has been talked about by folks like Yu and Hatzivassiloglou (2003), Pang and Lee (2004), Wilson et al. (2005a), and Kim and Hovy (2006). We’ll see more about this in Section 4.2.3 of this paper.",
        "formal_text": "Convert casual text to formal text: First off, some expressions can be subjective without really leaning positive or negative. Like, take this example from (Wilson et al., 2005a): Jerome says"
    },
    {
        "casual_text": "2. Explicit and Implicit Senses: One sense is directly linked to the explicit conjunction, and the other senses come from reading between the lines.",
        "formal_text": "Convert casual text to formal text: 2. Explicit and Implicit Senses: One sense is directly linked to the explicit conjunction, and the other senses are from reading between the lines."
    },
    {
        "casual_text": "We talked about how MT research is different from other frameworks in NLU, and we pointed out that one unique thing about MT as an NLP application is that we can't easily define a specific task-oriented level of 'understanding' like we can with other applications.",
        "formal_text": "Convert casual text to formal text: We talked about how MT research is different from other frameworks in NLU, and we pointed out that one unique thing about MT as an NLP application is that we can't easily define"
    },
    {
        "casual_text": "To check how accurate ACT is when using word alignment, we tested it on a new batch of 200 sentences from the UN EN/FR corpus (not the same ones as before), which had 207 instances of seven different discourse connectives. Just like we did for the first version of ACT (before we added the disambiguation module), we looked at each of the six cases and counted how many times the scoring was right or wrong. The results were: case 1: 64 correct, 0 wrong; case 2: 64 correct, 3 wrong; case 3: 33 correct, 4 wrong; case 4: 1 correct, 0 wrong; and case 6: 0 correct, 0 wrong. Out of the 38 sentences in case 5, 21 were actually correct translations. So, the ACTa score was about 10% off in the first version of ACT, but now it's pretty much spot on. ACTa5 and ACTm were both about 2% off before and are now only 0.5% off. This shows that word alignment makes the ACT metric more accurate.",
        "formal_text": "Convert casual text to formal text: To check how accurate ACT is when using word alignment, we tested it on a new batch of 200 sentences from the UN EN/FR corpus (not the same ones as before), which had"
    },
    {
        "casual_text": "Remember the example from Section 1? When it comes to \"Dennis Rodman,\" there's not much helpful info around him to show he's an athlete. But in that sentence, he's mentioned alongside \"Pippen,\" who's obviously an athlete. That little detail helps us figure out that Dennis Rodman is an athlete too.",
        "formal_text": "Convert casual text to formal text: Remember the example from Section 1? When it comes to \"Dennis Rodman,\" there's not much helpful info around him to show he's an athlete. But in that sentence"
    },
    {
        "casual_text": "While training, we tweak the probability P  (|S) for the tuple y (p i, a j, r) based on a sentence S.",
        "formal_text": "Convert casual text to formal text: While training, we tweak the probability P  (|S for the tuple y (p i, a j, r) based on a sentence"
    },
    {
        "casual_text": "Both use a bunch of statistical models. These models help figure out \"how likely is B if we have A?\" and typically spit out a bunch of probabilities. The decoder then looks at these probabilities and assigns a weight to each model when making a decision. The service has some pretty straightforward methods: Detect() for language detection, Translate() for translation, AddTranslation() to submit an edit or vote, and GetTranslations() to grab a bunch of translations for a given source.",
        "formal_text": "Convert casual text to formal text: Both use a bunch of statistical models. These models help figure out \"how likely is B if we have A?\" and typically spit out a bunch of probabilities. The decode"
    },
    {
        "casual_text": "No surprise here, the accuracy is pretty much the same as L'19's parser. It's a bit lower on DM, EDS, and AMR, though. Maybe that's because these graphbanks need non-projective AM dependency trees to be parsed accurately.",
        "formal_text": "Convert casual text to formal text: No surprise here, the accuracy is pretty much the same as L'19's parser. It's a bit lower on DM, EDS, and AMR, though."
    },
    {
        "casual_text": "To explain this difference, let's call the syntactic idea of a head the \"grammatical head\" and introduce another concept called the \"logical head\" of a phrase. Of course, to make this work, we need to define it in a way that ensures the logical head of a phrase is also uniquely determined. Take this example: () John persuaded an American to win hwin(y). The thing is, (7) doesn't necessarily mean there's an m American, while (6) does. This difference comes from the distinction between Equi- and Raising-verbs.",
        "formal_text": "Convert casual text to formal text: To explain this difference, let's call the syntactic idea of a head the \"grammatical head\" and introduce another concept called the \"logical head\" of a phrase."
    },
    {
        "casual_text": "Okay, let’s break this down in a simpler way: 1. **Purely Morphologically**: These are words that we can easily recognize as proforms just by looking at their form. Examples include personal pronouns (like *he*, *she*, *it*) and some adverbs that act like proforms. 2. **Morphologically/Syntactically**: These are words that could be proforms based on their form, but we need more context (syntax) to be sure. For example: - Possessive pronouns (like *his*, *her*, *their*) only act as proforms when they’re not being used to describe something (non-attributive). - Some adverbs can also act like conjunctions, so we need to check how they’re used. - Certain question words (like *who*, *which*) look the same as relative pronouns, so we need to figure out their role in the sentence. 3. **Borderline Cases**: These are tricky. They’re not clearly proforms based on form or syntax, but we know from experience that they can act like proforms sometimes. Examples include words like *do*, *happen*, *for this reason*, etc. 4. **Semantic Proforms**: This is a group of proforms based on meaning, not form. It’s mostly about how they function in a sentence.",
        "formal_text": "Convert casual text to formal text: Okay, let’s break this down in a simpler way: 1. **Purely Morphologically**: These are words that we can easily recognize as proforms just by looking at"
    },
    {
        "casual_text": "vp(X, P) means that for a verb phrase with position X and predicate P, it can either be broken down into a transitive event (tranev) with positions X and Y and a plural form Pl, followed by a noun phrase (np) with position Y, plural Pl, and predicate P. Or, it can be a transitive non-core argument verb phrase (tncransv) with position X and predicate P. Figure 2 shows an example sentence in the language this grammar recognizes, along with its surface structure and the semantic structure created by the grammar.",
        "formal_text": "Convert casual text to formal text: vp(X, P) means that for a verb phrase with position X and predicate P, it can either be broken down into a transitive event (tranev)"
    },
    {
        "casual_text": "When you map an NDA onto this network, pairs of NDA states (q) and input symbols (x) where (q, x)   get turned into activity patterns. The network's temporal relationships then handle the NDA transitions. Each NDA transition corresponds to a single network transition. This setup creates complex representations of both NDA states and input symbols. An NDA state is represented by all the activity patterns that include that state, and input patterns are represented by combining (OR-ing) all the activity patterns that contain that input symbol. One cool thing about this network is that it naturally handles mixed temporal images, which are kind of like the subthreshold version of mixture states. The temporal image of an active overlap includes at least all the activity patterns representing the next state. But that's not all. The network also acts like it's running the deterministic version of the NDA, meaning it explores all possible paths through the state space that the input allows, all at once. The representations of the states in this deterministic finite-state automaton (FSA) are built dynamically as the network runs. These are called mixed temporal images. The idea of \"dynamically constructed representations\" comes from Touretzky [9], who actually argued that they couldn't exist in today's neural networks, like Hopfield models.",
        "formal_text": "Convert casual text to formal text: When you map an NDA onto this network, pairs of NDA states (q) and input symbols (x) where (q, x)   get turned into activity"
    },
    {
        "casual_text": "In this case, the way alignment links depend on each other, which is called Markovian dependency, is represented by a distribution that describes how these alignment jumps happen.",
        "formal_text": "Convert casual text to formal text: In this case, the way alignment links depend each other, which is called Markovian dependency, is represented by a distribution that describes how these alignment jumps happen. In this case, the way"
    },
    {
        "casual_text": "Lastly, we should mention that acyclic feature structures without any re-entrancy can be seen as trees. In these trees, branches are labeled with feature names, and atomic values only show up at the leaf nodes, while the interior nodes remain unlabeled.",
        "formal_text": "Convert casual text to formal text: Lastly, we should mention that acyclic feature structures without any re-entrancy can be seen as trees. In these trees, branches are labeled with feature names, and"
    },
    {
        "casual_text": "We're suggesting a new, unsupervised way to evaluate controlled text generation called CTRLEval. Instead of relying on references, this method focuses on three key areas: coherence, consistency, and attribute relevance. It breaks these down into different text infilling tasks and uses the combined generation probabilities from a pre-trained language model to score the results.",
        "formal_text": "Convert casual text to formal text: We're suggesting a new, unsupervised way to evaluate controlled text generation called CTRLEval. Instead of relying on references, this method focuses on three key areas: coher"
    },
    {
        "casual_text": "We start by using this simulated single-reference set as our training data (1H = 1 Human reference) and test our system on a separate validation set made up of the NIST MT04 and MT05 datasets (2,870 sentences in total), which we’ll call MT04+05 for short. Next, we tweak the simulated set by paraphrasing it, pick the best paraphrase, and add it as a second reference. Then, we retune the MT system using this new set with two references (1H+1P = 1 Human reference + 1 Paraphrase).",
        "formal_text": "Convert casual text to formal text: We start by using this simulated single-reference set as our training data (1H = 1 Human reference) and test our system on a separate validation set made up of the NIST MT"
    },
    {
        "casual_text": "In section 4.2, they talked about the (ei) thing, which is like an atom in [7. This atom has a special connection, called an axiom link, between e and BIe.",
        "formal_text": "Convert casual text, to formal text: In section 4.2, they talked about the (ei) thing, which is like an atom in [7. This atom has a special connection, called an axiom"
    },
    {
        "casual_text": "score() is a function that checks how well a given label matches with a particular span representation.",
        "formal_text": "Convert casual text to formal text: score() is a function checks how well a given label matches with a particular span representation. Convert casual text to formal text: score() is"
    },
    {
        "casual_text": "On the flip side, the backward mode kicks in when someone asks the system a question. Usually, for the sake of efficiency, it’s better not to dump all the inferred info into the knowledge base right away. S-rules in backward mode only activate when a question comes up that can’t be answered straight off. They then start digging through the knowledge base to see if they can figure out the answer based on what’s already there. For instance, the Present S-rule in backward mode doesn’t trigger when sentence (27) is processed and T-expression (28) is generated by START. It only triggers when question (29) is asked, because that’s something the system can’t answer directly.",
        "formal_text": "Convert casual text to formal text: On the flip side, the backward mode kicks in when someone asks the system a question. Usually, for the sake of efficiency, it’s better not to dump all the in"
    },
    {
        "casual_text": "For the extended version of Lin-EBMT, called Lin-EBMT REC+, they added some stuff from the template-based EBMT approach during the recombination step. The first two steps are still the same as before.",
        "formal_text": "Convert casual text to formal text: For the extended version of Lin-EBMT, called Lin-EBMT REC+, they added some stuff from the template-based EBMT approach during the recombination step."
    },
    {
        "casual_text": "The approach is pretty straightforward, but it needs a good amount of know-how to work well across different situations.",
        "formal_text": "Convert casual text to formal text: The approach is pretty straightforward, but it needs a good amount of know-how to work well across different situations. Convert casual text to formal text: The approach is pretty straightforward, but it needs"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: M i = M i + x 14: L e = UpdatePredictedClusters(, x, L e) 15: C x = ComputeCandidates(M i, G e, M v)",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: M i = M i + x 14: L e = UpdatePredictedClusters(, x"
    },
    {
        "casual_text": "Okay, so here's how a finite-horizon MDP works. The system is in some state, let's call it s, and it picks an action, a. After that, it moves to a new state, s, which is chosen based on some probability T(•|s, a), and it gets a reward, r, which also depends on the state and action, R(s, a). This keeps happening over and over until it finally hits a terminal state and the process stops.",
        "formal_text": "Convert casual text to formal text: Okay, so here's how a finite-horizon MDP works. The system is in some state, let's call it s, and it picks an action, a"
    },
    {
        "casual_text": "Thompson and Koehn (2019) came up with a method that uses bilingual sentence embeddings. They measure how similar these embeddings are to figure out the best alignment.",
        "formal_text": "Convert casual text to formal text: Thompson and Koehn (2019) came up with a method that uses bilingual sentence embeddings. They measure how similar these embeddings are to figure out the best alignment."
    },
    {
        "casual_text": "The ML training process can get more stable and improve optimization by using lots of regularizations across all the exits. But during our tests, we noticed that the weight of the KD loss, called , really affects how well the model performs. Turns out, setting  to a smaller value, like 0.1 or 0.2, gives better results. This matches what Yang et al. (2020) and Sun et al. (2019b) found in their experiments. It feels like the KD objective and the CE loss are kind of fighting each other a bit.",
        "formal_text": "Convert casual text to formal text: The ML training process can get more stable and improve optimization by using lots of regularizations across all the exits. But during our tests, we noticed that the weight of the KD loss,"
    },
    {
        "casual_text": "Verbs needed a lot more work after the initial clustering. The VIIs, though, had the best results without any extra processing. For instance, V II 6 was all about taste and smell words, V II 7 was mostly weather-related, V II 8 had verbs that only work with plural subjects, V II 9 was filled with words about sound and sight, and V II 10 had verbs that acted like nouns (like mîsiyâpiskâw, which means '(it is) rust(y)'). Even though these clusters were pretty well-organized, V II 1 through V II 5 weren’t as tight and needed some manual grouping. In the end, we ended up with separate classes like II-natural-land, II-weather-time, II-sensory-attitude, II-plural, II-move, II-time, and II-named. 8 While some postprocessing was needed, it wasn’t too big of a deal in terms of time or effort.",
        "formal_text": "Convert casual text to formal text: Verbs needed a lot more work after the initial clustering. The VIIs, though, had the best results without any extra processing. For instance, V II 6 was all about taste and smell"
    },
    {
        "casual_text": "Wow, this looks like a bunch of random symbols and characters! It seems like some kind of code or encrypted message. I can't really make sense of it in its current form, but if you have a specific question or need help decoding it, let me know! Otherwise, it's a bit tough to work with as is.",
        "formal_text": "Convert casual text to formal text: Wow, this looks like a bunch of random symbols and characters! It seems like some kind of code or encrypted message. I can't really make sense of it in its current form, but"
    },
    {
        "casual_text": "Each part gets translated or edited individually, and there are four different ways to move around the document:",
        "formal_text": "Convert casual text to formal text: Each part gets translated or edited individually, and there are four different ways to move around the document: Convert casual text to formal text: Each part gets translated or edited individually, and there are four different"
    },
    {
        "casual_text": "In this paper, we introduce LP-MERT, a cool new algorithm for finding the best N translation options that works really well with the usual assumptions made in MERT, like how the error metric can be broken down by sentence. Now, there's no magic bullet for optimizing those tricky non-convex functions, but here's the thing: the error surface in SMT systems is special because the number of possible translations for a given input is limited. This means the error surface is made up of a bunch of flat spots, and there's only a finite number of them. Back in Och's 2003 work, you could think of going through all these flat spots and picking the best one—Och did this pretty efficiently for each dimension. But when you try to do this across all dimensions at once, it gets messy. That's where LP-MERT comes in. It uses smart tricks like lazy enumeration, divide-and-conquer, and linear programming to quickly rule out options that won't work, making the whole process way more efficient. We tested LP-MERT on thousands of searches and it never did worse than Och's algorithm, which is a big deal. It strongly suggests that our algorithm is spot on. We even prove in the appendix that this search algorithm is the best it can be. We show it's fast, growing with the size of N and the model, but it does get slower with more tuning sentences, which makes sense.",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce LP-MERT, a cool new algorithm for finding the best N translation options that works really well with the usual assumptions made in MERT, like how the error"
    },
    {
        "casual_text": "Using the pre-trained BERT (base-uncased) model (Devlin et al., 2019) as an example, we show that our method can do the same kind of analysis as other existing approaches. Plus, we uncover some new insights into how and what parts of language each neuron picks up on.",
        "formal_text": "Convert casual text to formal text: Using the pre-trained BERT (base-uncased) model (Devlin et al., 2019) as an example, we show that our method can do"
    },
    {
        "casual_text": "To make the most of the training dictionary, we tried a straightforward post-processing trick that kind of overfits the dictionary. Here's what we did: we first trained projection-based CLWE and then tweaked it by retrofitting to the training dictionary (you can see this in the pink parts of Figure 1). Retrofitting was originally used to refine regular word embeddings by adding synonym info from a lexical database (Faruqui et al., 2015). For CLWE, we used the training dictionary D as our database for retrofitting.",
        "formal_text": "Convert casual text to formal text: To make the most of the training dictionary, we tried a straightforward post-processing trick that kind of overfits the dictionary. Here's what we did: we first trained projection-based"
    },
    {
        "casual_text": "Graph building. One big difference between our work and the earlier methods mentioned is how we build the graph for the PRA algorithm. We’ll explain how our graph-building method compares to those earlier approaches in a bit more detail. Lao et al. (2012) made a graph where every word in every sentence of the corpus is a node, and the edges show the dependency relationships between those words. They linked this graph to the KB graph using a basic entity linking system (along with coreference resolution). The result was a huge graph, so they had to do some fancy indexing and use a cluster of 500 machines to run the PRA calculations. The problem is, the edges only represent dependency labels, not the actual words. This means the PRA algorithm doesn’t have access to the verbs or other important words in the sentences, which often show the relationships. PRA only looks at edge types, not node types, so all the useful information in the words gets lost. Plus, this graph setup wouldn’t work with the vector space similarity we added, since dependency labels aren’t great for that kind of representation. On the other hand, Gardner et al. (2013) used an approach pretty similar to what we talked about in Section 2—they preprocessed the corpus to pull out surface relations.",
        "formal_text": "Convert casual text to formal text: Graph building. One big difference between our work and the earlier methods mentioned is how we build the graph for the PRA algorithm. We’ll explain how our graph-building method compares to those"
    },
    {
        "casual_text": "Here’s the informal version: **Task**, **Learning Rate**, **Warm-up Steps (/epoch)**, **Batch Size**, **** - **CoLA**: 1e-5, 1.0, 32, 0.2 - **SST-2**: 2e-5, 0.8, 128, 0.1 **Table 6**: This table shows the hyperparameter settings we used for the initial experiments. We mainly played around with the learning rate, batch size, warm-up steps, and the weight for the knowledge distillation loss term. Oh, and just to be clear, we did all this tuning on the training set using cross-validation, so we didn’t peek at the GLUE benchmarks' dev set during training. **Table 8**: This table has the results of GAML-BERT when we tried it out on image classification tasks.",
        "formal_text": "Convert casual text to formal text: Here’s the informal version: **Task**, **Learning Rate**, **Warm-up Steps (/epoch)**, **Batch Size**, **"
    },
    {
        "casual_text": "We mostly compare our results with the latest end-to-end SRL models and also with the pipeline methods that work on argument role labeling using gold predicates. The results for the Chinese CoNLL09 dataset are in Table 1. From what we see, doing joint detection for both predicates and arguments seems to work better.",
        "formal_text": "Convert casual text to formal text: We mostly compare our results with the latest end--end SRL models and also with the pipeline methods that work on argument role labeling using gold predicates. The results for the Chinese CoNLL"
    },
    {
        "casual_text": "Soo (2018) and Blinov and his crew (2019) looked into whether a text snippet is just a one-liner. Meanwhile, Zhang and Liu (2014), Ortega-Bueno and friends (2018), and Chiruzzo et al. (2019) dove into the task of classifying humor in tweets. Castro and his team (2018) gathered humor ratings and funniness scores for Spanish tweets using crowdsourcing. Chiruzzo et al. (2019) even came up with a regression task to predict a tweet's humor score. Li and his colleagues (2020) collected Chinese internet slang and mixed it with a humor detection method to analyze the sentiment of Weibo posts. It's worth mentioning that all the examples in the datasets used or created in these studies are separate from each other. Since our dataset is based on full scripts, the annotated lines and text spans could be useful for researchers working on algorithms that understand humor in longer contexts. Besides studies on short text snippets, Bertero (2019) and Hasan and his team (2019) made datasets from TV sitcoms like The Big Bang Theory and TED talks, respectively. Their aim was to predict if a sequence of texts would trigger immediate laughter. Yang and his team (2015) extracted key words like \"sing,\" \"sign language,\" and \"pretty handy\" from jokes, which is kind of like what we did with our information extraction annotations.",
        "formal_text": "Convert casual text to formal text: Soo (2018) and Blinov and his crew (2019) looked into whether a text snippet is just a one-liner. Meanwhile, Zhang and Liu (2014), Ort"
    },
    {
        "casual_text": "We calculated inter-annotator agreement (IAA) separately for the second and third stages since they have different goals. In the second stage (cluster quality), we got an average Spearman correlation of r_s = 0.73, which is similar to what other studies in topic modeling have reported (Newman et al., 2010, with r_s = 0.73/0.78, and Aletras and Stevenson, 2013, with r_s = 0.70/0.64/0.54). We also got an average Cohen's Kappa of  = 0.48, which is considered moderate agreement. For the third stage (issue identification), we found  = 0.36, which is fair agreement. Looking at disagreements in stage 2, only 2% came from differing opinions on Good-Bad clusters. Most disagreements were about Good-Intermediate (37%) and Intermediate-Bad (61%) cases. This is a good sign because it shows that annotators rarely have completely opposite views on cluster quality. They mostly agree on what makes a cluster coherent, which is the main point of this task. For issue identification, most disagreements (49%) were about distinguishing Intermediate-Chained cases. This makes sense because it can be tricky to identify subclusters in the first stage. For the final decision-making process, we found that there was always a majority, so we went with the label that at least two out of three annotators agreed on. Table 1 gives an overview of the corpus size, coherence quality, and issues identified for COVID-19 and Election. Check out Appendix C for more details.",
        "formal_text": "Convert casual text to formal text: We calculated inter-annotator agreement (IAA) separately for the second and third stages since they have different goals. In the second stage (cluster quality), we got an average Spearman"
    },
    {
        "casual_text": "Since both \"two-level SoftTFIDF\" and agglomerative clustering need some parameters set, for each language, we picked two tricky names from the Boulder Name corpus to use as practice data (like John Smith, Michael Johnson, Li Gang, and Zhang Yong). The rest of the data—Bagga's corpus and the other names from the Boulder Name corpus (like Robert Smith, James Jones, Li Hai, and Liu Bo)—are kept for testing.",
        "formal_text": "Convert casual text to formal text: Since both \"two-level SoftTFIDF\" and agglomerative clustering need some parameters set, for each language, we picked two tricky names from the Boulder Name corpus to use"
    },
    {
        "casual_text": "BERT, developed by Devlin and team in 2018, is a super powerful pre-trained language model that’s used a lot in different areas. A bunch of NLP models that use BERT have actually hit the top when it comes to performance. To see how well our model works after we added BERT into it, we ran four different experiments: the Baseline Model without BERT, the Baseline Model with BERT, the Semantic-aware Model without BERT, and the Semantic-aware Model with BERT. Just a heads-up, the way we integrated BERT in our model is the same way we integrated the semantic dependency graph. The results for the Semantic-aware Model without BERT are what we’re looking at here.",
        "formal_text": "Convert casual text to formal text: BERT, developed by Devlin and team in 2018, is a super powerful pre-trained language model that’s used a lot in different areas. A bunch of NLP models"
    },
    {
        "casual_text": "Do we always need to break down sentences into their parts before figuring out their deep structure? And what’s the deal with semantics in all of this?",
        "formal_text": "Convert casual text to formal text: Do we always need to break down sentences into their parts before figuring out their deep structure? And what’s the deal with semantics in all of this? Convert casual text to formal text:"
    },
    {
        "casual_text": "In Table 3, you can see that the MUC scores for all the models are way lower than those for B 3 and CEAF e. This happens because when the cross-document coreference links are removed, most events end up as single, isolated events (as you can see in Table 1, the average length of WD coreference chains is just 1.24). B 3 and CEAF e are really affected by these lone events. In this situation, MUC is a better way to compare how well the models are doing. We noticed that our model did 8.3% better on MUC F1 and 4.2% better on CoNLL F1 compared to the Joint baseline. These results show that using paraphrase features and combining argument label embeddings with event embeddings works way better for event coreference resolution than just using fixed, manual features and argument slots to create event embeddings.",
        "formal_text": "Convert casual text to formal text: In Table 3, you can see that the MUC scores for all the models are way lower than those for B 3 and CEAF e. This happens because when the cross-document coreference links are"
    },
    {
        "casual_text": "(2) Just so you know, for any value of Acc, the function linking E(Acc) and K(Acc) is its own inverse.",
        "formal_text": "(2) Just so you know, for any value of Acc, the function linking E(Acc) and K(Acc) is its own inverse. So, for any value of Acc, the function linking E(Acc) and K"
    },
    {
        "casual_text": "Hey, at the JSTV Chinese New Year Gala in 2019, Shen Teng said to the lady: \"Ma'am, can you really think back carefully? I definitely didn't bump into you.\" The skit was called \"Help Her Up or Not.\"",
        "formal_text": "Convert casual text to formal text: Hey, at the JSTV Chinese New Year Gala in 2019, Shen Teng said to the lady: \"Ma'am, can you really think back carefully? I definitely didn't bump"
    },
    {
        "casual_text": "As a serial processor, the learner can only focus on one thing at a time.",
        "formal_text": "Convert casual text to formal text: As a serial processor, the learner can only focus on one thing at a time. Convert casual text to formal text: As a serial processor, the learner can only focus on"
    },
    {
        "casual_text": "This assessment doesn't consider punctuation and is based on the updated .prm parameter setup in evalb. 5. Check out the link here: http://www.cis.upenn.edu/dbikel/software.html",
        "formal_text": "Convert casual text to formal text: This assessment doesn't consider punctuation and is based on the updated .prm parameter setup in evalb 5. Check out the link here: http://www.cis"
    },
    {
        "casual_text": "From the table, it's clear that the model's ability to fix different types of errors varies a lot. For instance, a model without synthetic data can correct 53.3% of 'Noun number' errors, but only manages to fix 9.41% of 'Wrong Collocation/Idiom' errors. Data augmentation methods help to tackle this issue to some extent. Rule-based methods improve the recall for local errors like 'Spelling, Punctuation, etc.' and 'Noun number'. On the other hand, representation editing methods boost the model's performance on other error types, such as 'Verb tense' and 'verb form'. With the help of pre-defined rules, our method achieves the best recall for most error types. Our approach not only scores well in overall evaluation but also significantly improves performance across most error types. The two data augmentation methods work well together, and using both of them covers most error types, producing high-quality and diverse samples. This allows the model to detect and correct a wide range of errors, which is exactly what's needed in real-world applications.",
        "formal_text": "Convert casual text to formal text: From the table, it's clear that the model's ability to fix different types of errors varies a lot. For instance, a model without synthetic data can correct 53.3% of"
    },
    {
        "casual_text": "Psychometrics says that construct validity is super important for making sure the results of psychological experiments make sense. Messick (1995) breaks it down into six parts, which you can check out in Table 2.",
        "formal_text": "Convert casual text to formal text: Psychometrics says that construct validity is super important for making sure the results of psychological experiments make sense. Messick (1995) breaks it down into six parts, which you check out in Table 2. Mess"
    },
    {
        "casual_text": "We built a searchable index for book sentences, where we swap out every mention of a character—including pronouns—with their unique ID. When someone asks a question, we do the same thing with the character mentions in the question and then use BM25F (a method from Zaragoza et al., 2004) to rank the relevant passages from the book.",
        "formal_text": "Convert casual text to formal text: We built a searchable index for book sentences, where we swap out every mention of a character—including pronouns—with their unique ID. When someone asks a question,"
    },
    {
        "casual_text": "Our tests show that our metric keeps doing a good job and stays consistent even when the model or data quality changes over time.",
        "formal_text": "Convert casual text to formal text: Our tests show that our metric keeps a good job and stays consistent even when the model or data quality changes. Convert casual text to formal text: Our tests show that our metric keeps"
    },
    {
        "casual_text": "In Figure 2, you can see how the different academic subjects are spread out in S2ORC, based on Microsoft Academic's fields of study. Not every paper in S2ORC is listed in Microsoft Academic, though—the ones that aren't are labeled as \"Unclassified.\" Around 677,000 papers have more than one main field of study in Microsoft Academic, but Figure 2 only shows the top field for each paper.",
        "formal_text": "Convert casual text to formal text: In Figure 2, you can see how the different academic subjects are spread out in S2ORC, based on Microsoft Academic's fields of study. Not every paper in S2ORC is listed"
    },
    {
        "casual_text": "Second, you can get to your saved Shortcuts super fast, and you don’t really need to type much, if at all. There are two things that make this quick access possible:",
        "formal_text": "Convert casual text to formal text: Second, you can get to your saved Shortcuts super fast, and you don’t really need to type much, if at all. There are two things that make this quick access possible:"
    },
    {
        "casual_text": "Alright, so the idea here is to close the big gap between how we talk (natural language), how computers talk (computer language), and logic. We want to do this for a good reason: when you're building a computer system for a specific area (like healthcare, finance, etc.), you need to really understand that area inside and out. But usually, the people building the system (the implementors) don’t know much about it, so they have to learn from the experts in that field. The problem is, those experts usually don’t know much about computers. So, having a way to clearly and efficiently explain this knowledge would be super helpful. Right now, there’s no tool like that, so we’re trying to create one. LESK is kind of like a first step toward making that happen.",
        "formal_text": "Convert casual text to formal text: Alright, so the idea here is to close the big gap between how we talk (natural language), how computers talk (computer language), and logic. We want to do this for a good reason"
    },
    {
        "casual_text": "Piecewise Convolutional Neural Network (PCNN). In distantly supervised relation extraction, it's pretty standard to use the PCNN (Zeng et al., 2015) to create contextualized representations from a sequence of word embeddings. Compared to the usual 1D-CNN with max-pooling (Zeng et al., 2014), the PCNN's piecewise max-pooling can better capture the structure between two entities by paying attention to their positions. Here's how it works: first, a 1D-CNN (Kim, 2014) processes the input sequence to get contextualized representations. Then, piecewise max-pooling is applied to the output sequence to create a sentence-level embedding. That's basically the process in a nutshell.",
        "formal_text": "Convert casual text to formal text: Piecewise Convolutional Neural Network (PCNN). In distantly supervised relation extraction, it's pretty standard to use the PCNN (Zeng et al., 2015)"
    },
    {
        "casual_text": "k is the count of words linked to topic k in the dth tweet. After that, each hashtag t dm gets tagged based on the topic-specific translation probability P.",
        "formal_text": "Convert casual text to formal text: k is the count of words linked to topic k in the dth tweet. After that, each hashtag t dm gets tagged based on the topic-specific translation probability"
    },
    {
        "casual_text": "The key thing we’re looking at in our algorithm is the size of the post-training subset, which we write as ||S post ||. We tested different sizes, ranging from 1000 to 4000, to see how it affects how well the model works. As you can see in Figure 8, our method stays pretty consistent no matter what size we use for ||S post ||. This shows that the SetConv layer has figured out how to pick up on the general patterns that are shared across different data samples. So, as long as ||S post || is big enough, like 1000, changing its size doesn’t really mess with the model’s performance.",
        "formal_text": "Convert casual text to formal text: The key thing we’re looking at in our algorithm is the size of the post-training subset, which we write as ||S post ||. We tested different sizes, ranging from"
    },
    {
        "casual_text": "The \"affinity score\" measures how well a fragment ( s ) matches the event ( e_j ) it's aligned with. We call it \"affinity\" because it tells us how good the alignment is. Basically, if fragment ( s ) does a good job describing event ( e_j ), it will have a higher affinity score. To break it down, the affinity between ( s ) and ( e_j ) is calculated by multiplying the affinity of each word ( w_i ) in ( s ) with ( e_j ). Since ( e_j ) is made up of different attributes, we figure out how well each word ( w_i ) matches each attribute ( a ) of ( e_j ) and combine those scores to get the overall affinity.",
        "formal_text": "Convert casual text to formal text: The \"affinity score\" measures how well a fragment ( s ) matches the event ( e_j ) it's aligned with."
    },
    {
        "casual_text": "\"Media annotations\" usually means adding notes to videos or audio, but in this paper, we're using the term \"media entity annotations\" to talk about annotating texts made by big media companies. These texts are about things like movies, books, or other creative works.",
        "formal_text": "Convert casual text to formal text: \"Media annotations\" usually means adding notes to videos or audio, but in this paper, we're using the term \"media entity annotations\" to talk about annotating texts made by big"
    },
    {
        "casual_text": "We used PanPhon feature vectors to make a set of features that the CRF could work with. We did this because we realized that the way speech sounds are put together—called phonotactic patterns—usually follows the same kinds of groups that phonological features are meant to describe. For our experiments, we split these features into six groups, which mostly match the categories that phonologists often use.",
        "formal_text": "Convert casual text to formal text: We used PanPhon feature vectors to make a set of features that the CRF could work with. We did this because we realized that the way speech sounds are put together—called"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: We're trying to find values for w, b, and  (these are variables, don't worry too much about what they mean). We want to minimize this expression: 1/2 * wT * w + C * sum(_i) But we have some conditions: For each i, y_i * (wT * (x_i) + b) should be at least 1 - _i, and _i should be greater than or equal to 0. In simpler terms, we're trying to make this expression as small as possible while making sure these conditions are met.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: We're trying to find values for w, b, and  (these are variables, don't worry too"
    },
    {
        "casual_text": "For the MultiDoc2Dial dataset evaluations, there are two main tasks. Task 1 is about predicting the grounding span for the next agent response. For this task, we use: (1) the current user turn, (2) the dialogue history, and (3) all the documents from every domain as input. The goal is to find the most relevant text span from one document to use in the next agent response. Task 2 is about generating the agent's response in natural language. For this task, we also use: (1) the current user turn, (2) the dialogue history, and (3) all the documents from every domain as input.",
        "formal_text": "Convert casual text to formal text: For the MultiDoc2Dial dataset evaluations, there are two main tasks. Task 1 is about predicting the grounding span for the next agent response. For this task, we use:"
    },
    {
        "casual_text": "To figure out how entities in the original text match up with those in the translated version, we need to do two things: first, identify the parts of the translated sentence that are entities, and second, connect those parts to the corresponding ones in the original sentence. We've built an alignment tool that uses the attention weights between the encoder and decoder in the Marian model to link the input and output sentences. These weights tell us how much focus is put on each input word when the model is generating an output word. There's a heatmap in Figure 2 that shows these attention weights for an English sentence and its Italian translation. The cross-attention score for each decoder word is calculated using multi-head attention (Vaswani et al., 2017) across all the encoder words. In the example from Figure 2, each attention vector is represented by a column in the heatmap.",
        "formal_text": "Convert casual text to formal text: To figure out how entities in the original text match up with those in the translated version, we need to do two things: first, identify the parts of the translated sentence that are entities, and second,"
    },
    {
        "casual_text": "All the examples are treated the same (like how elements N P 1 are the same as nodes IP 1, CP 1, and N P 2).",
        "formal_text": "Convert casual text to formal text: All the examples are treated the same (like how elements N P 1 are the same as nodes IP 1, CP 1, and N P 2). Convert casual text to formal text: All the examples"
    },
    {
        "casual_text": "In the dynamic completion (DC) approach, we add some extra actions to every state to give the agent more options and make it easier to deal with the sparsity in knowledge graphs (KGs). But the question is, will the agent actually pick these extra actions? In other words, do these additional actions even make a difference?",
        "formal_text": "Convert casual text to formal text: In the dynamic completion (DC) approach, we add some extra actions to every state to give the agent more options and make it easier to deal with the sparsity in knowledge graphs (KGs"
    },
    {
        "casual_text": "The Manipuri language is usually written in two different scripts: Bengali and Meitei Mayek. Right now, most Manipuri documents are written in Bengali script, so this paper is mainly looking at Manipuri words written in that script. Working with Bengali script is trickier than Meitei Mayek because of how the sounds are represented. Bengali has 55 symbols to cover 38 phonemes in Manipuri (according to Singh et al., 2007).",
        "formal_text": "Convert casual text to formal text: The Manipuri language is usually written in two different scripts: Bengali and Meitei Mayek. Right now, most Manipuri documents are written in Bengali script"
    },
    {
        "casual_text": "Conversational agents (CAs) that use deep language models (DLMs) have shown a lot of potential in mental health care. For example, they’ve been used to offer informational or therapeutic services, like cognitive behavioral therapy, to patients. But one area that hasn’t been looked at much is how CAs can help with mental health triaging. This is tricky because it involves carefully crafting follow-up questions (FQs), which are usually handled by mental health professionals (MHPs) in real clinical settings. When it comes to depression, our experiments found that DLMs combined with process knowledge from a mental health questionnaire perform better at generating FQs. Specifically, they were 12.54% and 9.37% better in terms of similarity and longest common subsequence matches compared to questions in the PHQ-9 dataset, when compared to DLMs without this process knowledge. However, even with this extra knowledge, DLMs can still mess up by generating questions that are redundant, irrelevant, or even unsafe. This shows that training DLMs to create FQs that follow clinical guidelines is pretty challenging with existing datasets. To tackle this issue, we worked with MHPs to create an extended PHQ-9 dataset called PRIMATE. This dataset includes annotations that help determine if a question in the PHQ-9 has already been answered based on the user’s initial description of their mental health condition.",
        "formal_text": "Convert casual text to formal text: Conversational agents (CAs) that use deep language models (DLMs) have shown a lot of potential in mental health care. For example, they’ve been used to offer informational"
    },
    {
        "casual_text": "Okay, so L_s and L_t represent all the hidden layers of models S and T, respectively. MSE() is just the mean-square error function, and l_is is the i-th hidden layer of S. In PKD, f_it is the teacher's i-th layer, but in our setup, f_it is the outcome of combining some layers from T using a function called F(). This combination is determined by a mapper function M(), which takes an index (like a pointer to a layer in the student model) and returns a group of indices from the teacher model. Using these indices, specific layers from the teacher model are combined and then fed into the distillation process. For example, if M(2) = 1, 3, that means F() takes the first (l_1t) and third (l_3t) layers of T, and the distillation happens between l_2s and f_2t (which is the result of that fusion).",
        "formal_text": "Convert casual text to formal text: Okay, so L_s and L_t represent all the hidden layers of models S and T, respectively. MSE() is just the mean-square error function, and l_i"
    },
    {
        "casual_text": "We built JoGANIC using a Transformer-based encoder-decoder setup, kind of like Tell, but with our own twist—template guidance. We also came up with two upgraded versions: JoGANIC+NEE, which adds fancy named entity embeddings (check out Section 4.3.1), and JoGANIC+MSTR, which uses a multispan text reading technique (see Section 4.3.2). To see how well JoGANIC handles template guidance, we created two special versions: JoGANIC (oracle) and JoGANIC+MSTR+NEE (oracle), where we fed in the actual template components using . To test how well our model uses both text and image inputs, we did a couple of experiments. First, we ran our multimodal model but turned off the text features during testing (so X_T and X_E were just zero vectors)—we called this JoGANIC (zero-out text). We also tried turning off the image features—JoGANIC (zero-out image). Additionally, we trained models that only used either the image encoder (JoGANIC image only) or the text encoder (JoGANIC text only). For comparison, we looked at two types of baselines: (i) Two-step generation methods: These use traditional image captioning models (like Xu et al., 2015; Rennie et al., 2017; Anderson et al., 2018; Lu et al., 2017; Biten et al., 2019) to first create captions with placeholders and then fill in the named entities. (ii) End-to-end models: Like VGG+LSTM (Ramisa et al.).",
        "formal_text": "Convert casual text to formal text: We built JoGANIC using a Transformer-based encoder-decoder setup, kind of like Tell, but with our own twist—template guidance. We also came up with two upgraded"
    },
    {
        "casual_text": "Next, we add the thread title to the comment and send the whole thing to an outside company for manual translations. When we got the translations back, we saw that the quality wasn't always consistent. Turns out, translating stuff from social media, with all its tricky little details, is pretty tough even for people. To make sure we get the best translations, we go through the data by hand, break the comments into sentences, and toss out the ones that aren't up to par for our test data. This way, we end up with about 1,000 sentence pairs in each direction for our final test set.",
        "formal_text": "Convert casual text to formal text: Next, we add the thread title to the comment and send the whole thing to an outside company for manual translations. When we got the translations back, we saw that the quality wasn't always"
    },
    {
        "casual_text": "Okay, so this is a bit of a mess, but let me try to break it down in simpler terms. Basically, it's talking about a relationship that only works under certain conditions. The text is saying that this relationship, let's call it \"L,\" only holds true when a specific property of something, like a \"thing\" or \"object,\" is involved, along with some other stuff. Now, this \"L\" thing isn't really useful unless we know exactly what conditions need to be met for it to work. So, the text is trying to explain those conditions with something called a \"meaning postulate.\" It's saying that VPVQ(o, dy(l', (2), . , , d, , sig (, X, , ', '(4) is the way to spell out those conditions. In short, it's just trying to make clear what needs to happen for this relationship to make sense.",
        "formal_text": "Convert casual text to formal text: Okay, so this is a bit of a mess, but let me try to break it down in simpler terms. Basically, it's talking about a relationship that only works under certain"
    },
    {
        "casual_text": "We're starting with two things: (1) a sentence-aligned pair of texts (bitext); and (2) a function w that tells us how strong the translation connection is between any pair of words in the source and target languages. There are different ways to define w, but it makes sense to figure it out based on how often words appear together in the bitext. For now, we'll use the scores from Anymalign's output. Later on, we'll see that these scores work better than the ones from other common methods.",
        "formal_text": "Convert casual text to formal text: We're starting with two things: (1) a sentence-aligned pair of texts (bitext); and (2) a function w that tells us how strong the translation connection is between"
    },
    {
        "casual_text": "We used a random selection of 550 sentences from the CMU data and 36 sentences from translated SMS messages as our evaluation data. For training, we had a lot more translated SMS sentences—around 1,500 at the time of writing. We're also doing a human evaluation for Creole to English and English to Creole translations, but we didn't have the results yet when this was written. Check out http://www.microsofttranslator.com/Tools/ for all the Microsoft Translator tools and info.",
        "formal_text": "Convert casual text to formal text: We used a random selection of 550 sentences from the CMU data and 36 sentences from translated SMS messages as our evaluation data. For training, we had a lot more translated SMS sentences—around"
    },
    {
        "casual_text": "In this model, we treat the task of guessing the part of speech for unknown words as a sequence labeling problem. We got this idea from watching how humans figure out what a word means. Typically, when people come across an unknown word, they see it as a string of characters or smaller parts (morphemes) that can be broken down into segments. Each of these segments tends to have a more or less clear meaning on its own.",
        "formal_text": "Convert casual text to formal text: In this model, we treat the task of guessing the part of speech for unknown words as a sequence labeling problem. We got this idea from watching how humans figure out what a word means"
    },
    {
        "casual_text": "Grounding Document: Hey, there's this awesome no-kill shelter near my place where I volunteer every Wednesday! Cool, right? Yeah, so, is a no-kill shelter different from a regular animal shelter?",
        "formal_text": "Grounding Document: Hey, there's this awesome no-kill shelter near my place where I volunteer every Wednesday! Cool, right? Yeah, so, is a no-kill shelter different a regular animal shelter? Yeah, so"
    },
    {
        "casual_text": "Alright, so here's the deal: when we're looking at a syntactic category, we can break it down into a graph. This graph is almost like a proof structure, except it's missing those special \"axiom links.\" The whole point of parsing in this setup is to take the syntactic categories and their order, then add these non-crossing axiom links to make it into a proper proof structure, or \"proof net.\" This means there's a way to prove something, let's say \"b',\" using certain types in a specific order. Now, here's a little twist: for some technical reasons, the order of the conclusions (which are basically the types we're using) in the proof net to prove \"S\" is actually the reverse of the order of the words tied to those types.",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal: when we're looking at a syntactic category, we can break it down into a graph. This graph is almost like a"
    },
    {
        "casual_text": "Lastly, other projects in natural language processing have tried to pull together databases of social behavior from news articles, like tracking protests (Hanna, 2017), gun violence incidents (Pavlick et al., 2016), and international relations (Schrodt and Gerner, 1994; Schrodt, 2012; Boschee et al., 2013; O'Connor et al., 2013; Gerrish, 2013). These can be seen as tasks to build event databases, but they vary in how specific they are about what counts as an \"event.\"",
        "formal_text": "Convert casual text to formal text: Lastly, other projects in natural language processing have tried to pull together databases of social behavior from news articles, like tracking protests (Hanna, 2017), gun violence incidents (Pavlick"
    },
    {
        "casual_text": "So, in this paper, we take a look at two language models that work with syntax, but in a kind of basic translation setup. We separate the target language's structure from the translation rules, which gives the language model more room to do its thing. This way, we can really see what the parser is good at and where it might fall short when picking the right translation output.",
        "formal_text": "Convert casual text to formal text: So, in this paper, we take a look at two language models that work with syntax, but in a kind of basic translation setup. We separate the target language's structure from the translation"
    },
    {
        "casual_text": "[Question] You can use the subway to get to and from work mask> days a week.",
        "formal_text": "[Question] You can use the subway to get and from work mask> days a week. Convert casual text to formal text: [Question] You can use the subway to get and from work"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: 1. First, we increase the value of t by 1. 2. Then, we take a sample from D_t-1_o (which is the data from the previous step) and call it D_t_o. The amount of data we take is based on the amount we specified. 3. Next, we combine D_i (the initial data) with D_t_o (the sampled data) to create D_t_train. 4. Finally, we use D_t_train to train the model M_t, starting from the previous model M_t-1.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: 1. First, we increase the value of t by 1. 2. Then, we take a sample from D"
    },
    {
        "casual_text": "When training NMT models, we usually aim to make the model as good as possible at predicting the training data, which is a bunch of sentence pairs, D  (x (n), y (n) ) N n=1. Each pair (x (n), y (n) ) represents two sentences that are supposed to be translations of each other. Based on this idea, we tweak the model's settings to improve how well it can predict each word in the sentences, using something called token-level crossentropy loss.",
        "formal_text": "Convert casual text to formal text: When training NMT models, we usually aim to make the model as good as possible at predicting the training data, which is a bunch of sentence pairs, D  (x ("
    },
    {
        "casual_text": "Our HULK benchmark, shown in Figure 1, uses a bunch of classic datasets that are super popular in the community for testing energy efficiency. It evaluates pretrained models across multiple tasks. The tasks include the natural language inference task MNLI (Williams et al., 2017), the sentiment analysis task SST-2 (Socher et al., 2013), and the Named Entity Recognition task CoNLL-2003 (Sang and De Meulder, 2003). We picked these tasks to give a solid comparison of energy efficiency from pretraining to fine-tuning and inference.",
        "formal_text": "Convert casual text to formal text: Our HULK benchmark, shown in Figure 1, uses a bunch of classic datasets that are super popular in the community for testing energy efficiency. It evaluates pretrained models across multiple tasks. The"
    },
    {
        "casual_text": "When doing sentiment classification, we only tweak the parameters of the models that were fine-tuned using two different methods: (2) fine-tuned with the MLM objective for the probing task, and (3) fine-tuned with the knowledge-enhanced objective for the probing task. The results are in Table 6. First off, fine-tuning with the MLM objective boosts the performance of all models in the sentiment classification task. This shows that making models better at figuring out the properties of similes helps them understand sentiment better. Plus, using our knowledge-enhanced objective improves performance even more, especially for RoBERTa, which gets most of its gains from our extra knowledge embedding. This proves that our knowledge-enhanced approach works well for sentiment analysis tasks.",
        "formal_text": "Convert casual text to formal text: When doing sentiment classification, we only tweak the parameters of the models that were fine-tuned using two different methods: (2) fine-tuned with the MLM objective for the probing task"
    },
    {
        "casual_text": "The model we end up with, which does a great job of fitting the original training data, can be used in two cool ways: first, as a parser, we might expect it to do a better job than models that just rely on training data with extra rules added by hand. Second, as a tree-augmenter, it should be more in tune with the actual data, making it better at adding stuff to trees compared to those old, hand-written rules.",
        "formal_text": "Convert casual text to formal text: The model we end up with, which does a great job of fitting the original training data, can be used in two cool ways: first, as a parser, we might expect it"
    },
    {
        "casual_text": "Homographs can be tricky for parsers. The way a word's argument structure is used can change a lot based on its grammatical role, but lots of words have the same form and can play different grammatical parts.",
        "formal_text": "Convert casual text to formal text: Homographs can be tricky for parsers. The way a word's argument structure is used can change a lot based on its grammatical role, but lots of"
    },
    {
        "casual_text": "Semantic parsing is all about turning sentences in plain language into something a computer can understand and work with. This means converting natural language (NL) phrases into structured meaning representations (MRs) that are written in formal languages like Prolog, SQL, or Python. These formal languages have their own rules, which are laid out in a formal grammar—basically, a set of instructions that define how the language works. In these formal languages, MRs are often called logical forms or programs. The cool thing is that these MRs can be run in a programming environment to get results, like pulling data from a database using an SQL query. This process helps with automated reasoning, which is a fancy way of saying the computer can figure things out on its own (Kamath and Das, 2018).",
        "formal_text": "Convert casual text to formal text: Semantic parsing is all about turning sentences in plain language into something a computer can understand and work with. This means converting natural language (NL) phrases into structured meaning representations ("
    },
    {
        "casual_text": "In the middle of the table, you'll find the results broken down by SBA. Noun phrases that didn't get a semantic analysis result, basically because the analysis didn't work, are grouped together and labeled as other>, which you can see at the end of the table.",
        "formal_text": "Convert casual text to formal text: In the middle of the table, you'll find the results broken down by SBA. Noun phrases that didn't get a semantic analysis result, basically because the analysis didn't work"
    },
    {
        "casual_text": "We tried combining this model with the training process of the edit model by doing joint training, but it ended up making things worse.",
        "formal_text": "Convert casual text to formal text: We tried combining this model with the training process of the edit model by doing joint training, but ended up making things worse. Convert casual text to formal text: We tried combining this model with"
    },
    {
        "casual_text": "The linear constraint i=1  i y i = 0 that SVM uses isn't applied in Preference Learning because SVM needs this constraint to find the optimal b, but Preference Learning doesn't have a b in g(x). Even though SVM light (from Joachims, 1999) has a version for Preference Learning, we decided to build our own because SVM light doesn't handle non-linear kernels, and our version works faster.",
        "formal_text": "Convert casual text to formal text: The linear constraint i=1  i y i i = 0 that SVM uses isn't applied in Preference Learning because SVM needs this constraint"
    },
    {
        "casual_text": "3. Use the V2W map to grab the smaller list of words that match each vector in the k-nearest neighbors.",
        "formal_text": "Convert casual text to formal text: 3. Use the V2W map grab the smaller list of words that match each vector in k-nearest neighbors. 4. Use the V2W map grab the smaller list of words that match"
    },
    {
        "casual_text": "CoNLL 2003, Ritter, and i2b2 2014 already come with their own train/test/dev splits. But we wanted to have more flexibility with the sizes of the training and development sets, especially to keep the same train/dev ratio as we worked with bigger training sets. So, we decided not to use the usual train/dev splits.",
        "formal_text": "Convert casual text to formal text: CoNLL 2003, Ritter, and i2b2 2014 already come with their own train/test/dev splits. But we wanted to have more flexibility with the sizes of the training"
    },
    {
        "casual_text": "You can find the code in this repository: https://gitlab.inria.fr/bemuller/bert_normalizer",
        "formal_text": "Convert casual text to formal text: You find the code in this repository: https://gitlab.inria.fr/bemuller/bert_normalizer"
    },
    {
        "casual_text": "So, we can get the sentence's representation like this.",
        "formal_text": "Convert casual text to formal text: So, we can get the sentence's representation like this. Convert casual text to formal text: So, we can get the sentence's representation like this. Convert casual text to formal text"
    },
    {
        "casual_text": "We get a sentence-level representation, let's call it s_h, by adding up all the value vectors in the sentence, but each one is multiplied by a weight first (check out Equation 11). After that, there are two feed-forward layers. The first one is non-linear and shrinks the sentence representation into a smaller feature space. The second one is linear and gives us a single number, o_h, which is the sentence-level score for each head h (Equation 12).",
        "formal_text": "Convert casual text to formal text: We get a sentence-level representation, let's call it s_h, by adding up all the value vectors in the sentence, but each one is multiplied by a"
    },
    {
        "casual_text": "The third row compares our model to NO-SYNTAX, which is a stripped-down version of our model that uses lexical features but doesn’t include the syntactic structure. The results show that it performs better than the SENSORS-ONLY and RELATIONAL-RANDOM baselines but still falls short compared to our full system. This shows that the syntactic features our model uses help create better semantic representations of the text. Features like empty(x1)  freecell(x1) are helpful because they reuse variables to make sure objects have important properties—in this case, making sure a freecell is empty. Other features, like homecell(x1)  value(x2, x3), help narrow down the search to useful combinations of predicates (in Freecell, whether you can play a card on a homecell depends on the card’s value). It’s worth noting that three out of the 15 formulas are basically useless because they’re always false, like card(x1)  tableau(x1). This highlights how important it is to get the term assignment right to create useful features for learning. In the NO-SYNTAX system, which doesn’t consider the relationship between term assignment and syntactic structure, eight out of the top 15 formulas were useless because of term incompatibility.",
        "formal_text": "Convert casual text to formal text: The third row compares our model to NO-SYNTAX, which is a stripped-down version of our model that uses lexical features but doesn’t include the synt"
    },
    {
        "casual_text": "We use precision, recall, and the F-measure to evaluate how well the DB field is classified. For checking how well we identify the evidence spans, we look at the F-measure for individual words, as well as BLEU (from Papineni et al., 2002) and ROUGE (from Lin and Hovy, 2003). When evaluating the evidence span identification, we only considered cases where the DB field was classified correctly. Let’s say we have two binary vectors: z = (z1, z2, ..., zm) for the estimated evidence spans and z = (z1, z2, ..., zm) for the true evidence spans. In these vectors, a \"1\" means the word is part of the evidence. Using these vectors, we can calculate the F-measure for words using equation 5.",
        "formal_text": "Convert casual text to formal text: We use precision, recall, and the F-measure to evaluate how well the DB field is classified. For checking how well we identify the evidence spans, we look at the F-measure for"
    },
    {
        "casual_text": "The second method lets us use a \"semanticized\" version of valence theory and a \"semanticized\" dependency grammar.",
        "formal_text": "Convert casual text to formal text: The second method lets us use a \"semanticized\" version of valence theory and a \"semanticized\" dependency grammar."
    },
    {
        "casual_text": "We showed that the Patient-KD method can shrink BERT 12 down to BERT 6 without losing any performance. Now, we’re digging deeper into how well Patient-KD works for saving storage space and speeding up inference time. Table 4 has all the details on parameter counts and inference times. All the models use the same embedding layer, which has 24 million parameters and turns a 30,000-word vocabulary into a 768-dimensional vector. This setup helps save 1.64 times the machine memory compared to BERT 6 and 2.4 times compared to BERT 3.",
        "formal_text": "Convert casual text to formal text: We showed that the Patient-KD method can shrink BERT 12 down to BERT 6 without losing any performance. Now, we’re digging deeper into how well Patient-KD works for saving"
    },
    {
        "casual_text": "In this paper, we introduce a new approach called a diversified multiple instance learning network to tackle DMSC using only document-level supervision. We frame this problem as multiple instance learning to better understand the connection between aspect-level sentiment and document-level sentiment. To make sure the document-level supervision effectively translates to aspect-level predictions, we also introduce two new ideas: diversified textual regularization and diversified sentimental regularization. Our experiments on two popular datasets show that our D-MILN can effectively capture the relationship between aspect-level and document-level sentiment, setting a new state-of-the-art for weakly supervised DMSC. We also did some comparisons that highlight how important and effective our new regularizations are. Looking ahead, we plan to enhance D-MILN by incorporating aspect-level annotations and explore how to integrate it with pre-training methods (like the ones mentioned by Tian et al., 2020).",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce a new approach called a diversified multiple instance learning network to tackle DMSC using only document-level supervision. We frame this problem as multiple instance learning to better"
    },
    {
        "casual_text": "Basically, we figure out how words work together in sentences and come up with a quick way to make it happen. Then, we show you how it works with an example.",
        "formal_text": "Convert casual text to formal text: Basically, we figure out how words work together in sentences and come up with a quick way to make it happen. Then, we show you it works with an example."
    },
    {
        "casual_text": "We tweak the parameters, like V (l), b (l)  for each GCN encoder and the threshold , in two steps, which is kind of like how supervised contrastive learning works (shoutout to Khosla et al., 2020). First, we adjust the scoring function f s by trying to minimize the contrastive loss we see in Eq. 3.",
        "formal_text": "Convert casual text to formal text: We tweak the parameters, like V (l), b (l)  for each GCN encoder and the threshold , in two steps, which is kind of like how"
    },
    {
        "casual_text": "The HAPS segmenter uses something called factor graphs, which are a general way to represent things like Markov or Bayesian networks. A factor graph is like a two-part graph with two types of nodes: function nodes and variable nodes. Each function node is connected to the variable nodes that are its inputs. When you run the Max-Sum algorithm (a method described by Bishop in 2006) on a factor graph, it figures out the best set of variable values that gives the highest total of all the functions. This algorithm works by passing messages between nodes. Variable nodes send messages to the function nodes they're connected to, and function nodes send messages back to the variable nodes that are their inputs. A message from a variable node x to a function node f is calculated by adding up all the messages coming into x, except for the one from f itself.",
        "formal_text": "Convert casual text to formal text: The HAPS segmenter uses something called factor graphs, which are a general way to represent things like Markov or Bayesian networks. A factor graph is like a two-part graph"
    },
    {
        "casual_text": "We took a RoBERTa-Large model that was fine-tuned on a stained version of the MultiNLI training set. The whole MultiNLI dataset (train, dev-matched, dev-mismatched) was stained during the experiment. We also masked the stain for 10% of our training data to make sure those examples were part of the model's training, which allowed us to use these masked-stain examples later in our analysis. We ran the experiment three times, each time treating one of the three NLI classes as the stain. In all three runs, the models performed really well on the mismatched dev-set of the stained MultiNLI, with accuracy over 97%. This high performance was expected and shows that the models were definitely using the stain features to make predictions.",
        "formal_text": "Convert casual text to formal text: We took a RoBERTa-Large model that was fine-tuned on a stained version of the MultiNLI training set. The whole MultiNLI dataset (train"
    },
    {
        "casual_text": "If we make sure everyone gets their regular, super-effective vaccines, we can stop diseases from spreading and keep our kids safe from getting sick.",
        "formal_text": "Convert casual text to formal text: If we make sure everyone gets their regular, super-effective vaccines, we can stop diseases from spreading and keep our kids safe to get sick.. Convert casual text to formal text: If we"
    },
    {
        "casual_text": "For hyperparameter tuning, we started by running one epoch on the lexical part of PPDB XXL, which has 770,007 word pairs. We used either WS353 or SL999 for picking the best model settings (more on that below). After choosing the best hyperparameters, we trained the models for 50 epochs to make sure they had enough time to fully converge. You can find all the nitty-gritty details about our tuning process in the supplementary material. Basically, we tuned everything thoroughly, including the activation functions for CHARAGRAM and charCNN, as well as things like regularization strength, mini-batch size, and sampling type for all models. For charCNN, we tried two different sets of filters: one set uses 175 filters for each ngram size (2, 3, 4), and the other set is from Kim et al. (2015), which has 25 filters of size 1, 50 of size 2, 75 of size 3, 100 of size 4, 125 of size 5, and 150 of size 6. We also played around with using dropout (from Srivastava et al., 2014) on the inputs to the final layer of charCNN instead of L2 regularization, and we even tried removing the last feedforward layer. Turns out, neither of these tweaks made a big difference in performance on our word or sentence similarity tasks. But, using more filters does help, and it seems to improve performance in a way that’s proportional to the square of the number of filters.",
        "formal_text": "Convert casual text to formal text: For hyperparameter tuning, we started by running one epoch on the lexical part of PPDB XXL, which has 770,007 word pairs. We used"
    },
    {
        "casual_text": "You can check out an alignment example from lexnorm15 in Table 2. Let’s quickly talk about some obvious upsides and downsides of this alignment method. On the plus side, using the same tokenization as BERT’s pre-training means the token sequence BERT saw during training should be modeled well. This should help with normalization. On the flip side, learning normalization this way means dealing with a lot of [MASK] tokens, which requires focusing more on the surrounding context rather than the raw tokens. This could make normalization a bit trickier. But, as we’ll see in Section 5, even though this alignment is simple, it still lets our model perform pretty well.",
        "formal_text": "Convert casual text to formal text: You can check out an alignment example from lexnorm15 in Table 2. Let’s quickly talk about some obvious upsides and downsides of this alignment method. On the plus side, using"
    },
    {
        "casual_text": "Another way to look at it is to think of each set as representing a probability distribution, with the items in the set being like samples we've observed from that distribution. This idea lets us turn a kernel that works on distributions into a kernel that works on sets. If the items in the set are strings, we can use something called a string embedding  String to figure out a probability distribution for all possible smaller strings (subsequences) in the set. We do this by adding up the feature mappings for each string in the set and then normalizing the result.",
        "formal_text": "Convert casual text to formal text: Another way to look at it is to think of each set as representing a probability distribution, with the items in the set being like samples we've observed from that distribution. This idea lets us turn"
    },
    {
        "casual_text": "To check how good our annotated datasets are and how tough the task is, we look at how well the annotators agree with each other. We're using the same approach as Pakhomov and his team did back in 2011. This includes: 1) seeing how much each pair of annotators agrees, which we measure with coefficients, and 2) checking the overall reliability by looking at summary stats for all the annotators combined.",
        "formal_text": "Convert casual text to formal text: To check how good our annotated datasets are and how tough the task is, we look at how well the annotators agree with each other. We're using the same approach as Pa"
    },
    {
        "casual_text": "To check out the specifics of these strategies, head over to Appendix F. Table 5 gives us the results for Transformer-IF on DailyDialog. It shows that Transformer-IF did better on the \"Other\" set compared to the \"Uninformative\" set. This backs up our idea that how informative the future conversation is really matters for this approach.",
        "formal_text": "Convert casual text to formal text: To check out the specifics of these strategies, head over to Appendix F. Table 5 gives us the results for Transformer-IF on DailyDialog. It shows that Transformer-IF did better"
    },
    {
        "casual_text": "Not all unsupervised taggers actually create word categories. A lot of systems, called part-of-speech disambiguators (like the one Merialdo talked about in 1994), use external dictionaries with possible tags instead. Our project is based on two older methods for part-of-speech tagging—the word clustering algorithms from Clark (2000) and Brown et al. (1992). These methods have been found to be more reliable than other popular fully unsupervised techniques, according to Christodoulopoulos and his team in 2010.",
        "formal_text": "Convert casual text to formal text: Not all unsupervised taggers actually create word categories. A lot of systems, called part-of-speech disambiguators (like the one Merialdo talked about in 1994),"
    },
    {
        "casual_text": "Being chosen as a positive example. This probability is like the average importance of each word in shaping the final vector. That's how we use it when adding everything up in each model.",
        "formal_text": "Convert casual text to formal text: Being chosen as a positive example. This probability is like the average importance of each word in shaping the final vector. That's how we use it when adding everything up in each model."
    },
    {
        "casual_text": "When trying to create a target sentence, figuring out the sentence structure gets trickier, especially if the interlingua (a kind of middle language) doesn’t have any grammar details. So, you can’t just copy the structure from the interlingua directly. In this paper, we introduce a method to handle this issue for task-based interlingua-based dialogue systems. We take the interlingua representation, called Interchange Format (IF), and turn it into something called an FS that matches the grammar of the target Arabic sentence. This method tackles the challenge of figuring out Arabic sentence structure when using the interlingua approach. We built this mapper mainly for the NESPOLE! project, which is all about translating spoken language for e-commerce across different languages. (For more on NESPOLE’s interlingua specifics, check out this link: http://www.is.cs.cmu.edu/nespole/db/specification.html).",
        "formal_text": "Convert casual text to formal text: When trying to create a target sentence, figuring out the sentence structure gets trickier, especially if the interlingua (a kind of middle language) doesn’t have any grammar details"
    },
    {
        "casual_text": "Basically, e k1 i is the vector for the i-th node from the (k 1)-th layer. N(i) is just the set of neighbors for node i. e k1 j is the vector for the j-th neighbor of node i. W 1 and W 2 are matrices of weights that the model can learn, and Aggr is the function that combines all this stuff together.",
        "formal_text": "Convert casual text to formal text: Basically, e k1 i is the vector for the i-th node from the (k 1)-th layer. N(i) is just the set"
    },
    {
        "casual_text": "So, for these queries, we take the AM query (let's call it q_am) and stick it together with the document D_a to create a single input sequence for the AM.",
        "formal_text": "Convert casual text to formal text: So, for these queries, we take the AM query (let's call it q_am) and stick it together with the document D_a to create a single input sequence for the"
    },
    {
        "casual_text": "Things like the situation you're talking about, how familiar you are with something, and how new or important the information is can affect which articles you use. For instance, if you're introducing a guy into the conversation, you might start with \"a man\" and then later call him \"the man.\" But sometimes, other stuff like grammar rules or the meaning of the word stops you from using an article. Like, words like \"this,\" \"that,\" or \"no\" don't need articles, and neither do things like \"money\" that are just general ideas.",
        "formal_text": "Convert casual text to formal text: Things like the situation you're talking about, how familiar you are with something, and how new or important the information is can affect which articles you use. For instance, if you're"
    },
    {
        "casual_text": "Alright, let’s break down the results. Figure 5 gives us the big picture. Most of the knowledge picked out is grammatically sound and actually related to the question. Plus, 83% of it is factually accurate. According to the human evaluators, 72% of this knowledge is useful for answering the questions, but 13% actually does more harm than good. Now, when the knowledge actually corrects the model’s predictions, 93% of it is seen as helpful by the evaluators. On the flip side, if the knowledge leads the model astray, only 21% is considered helpful, and 39% is labeled as harmful. Here’s the interesting part: of the knowledge that’s both helpful and corrects the model’s mistakes, 95% is factual. But for the knowledge that’s harmful and misleads the model, 86% is non-factual. This tells us that making sure the knowledge is factually correct could really improve how helpful it is. We also took a look at the knowledge that wasn’t selected and found it’s a bit less factual and helpful compared to the stuff that was picked. Table 5 shows some examples where the generated knowledge fixed the model’s predictions. We’ve only included the selected knowledge for each question because of space limits. In all these examples, the model without the extra knowledge gives a higher score to the wrong answer instead of the right one. But when we add the knowledge, the correct answer gets a much higher score.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break down the results. Figure 5 gives us the big picture. Most of the knowledge picked out is grammatically sound and actually related to the question. Plus, 8"
    },
    {
        "casual_text": "The hidden state vector   h t in the forward LSTM layer takes into account the stuff that comes before, or the past tokens, of w t in the sentence. On the flip side, we can also figure out the hidden state vector   h t in the backward LSTM layer, which looks at the stuff that comes after, or the future tokens, of w t in the sentence. Then, we just mash these two vectors together.",
        "formal_text": "Convert casual text to formal text: The hidden state vector   h t in the forward LSTM layer takes into account the stuff that comes before, or the past tokens, of w t in the"
    },
    {
        "casual_text": "For instance, they take inputs (a, b) for recognizing textual entailment (RTE) and turn them into something like \"rte sentence1: a sentence2: b\". Then, the PLM is supposed to guess stuff like \"not entailment\".",
        "formal_text": "Convert casual text to formal text: For instance, they take inputs (a, b) for recognizing textual entailment (RTE) and turn them into something like \"rte sentence1: a"
    },
    {
        "casual_text": "Okay, so basically, all this stuff follows some rules that help us figure out things like how similar two groups of words are, or how important a word is in a sentence. For example, if you have a sentence with the word \"follow\" in it, and it's marked by a special rule, we can give it a score based on where it is in the sentence and how it relates to other words. This method can help us compare two sets of words, even if they’re different lengths. First, we check how similar two words are by looking at exact matches, parts of words that match, and how close they are in a thesaurus. Then, we kind of combine all those similarities to see how similar the whole groups of words are. The tricky part is that people often use slightly different words or phrases for the same idea, and it’s hard to catch all of those differences. Using a thesaurus and rules like the ones in Table 2 can help, but there’s still a lot of variety in how different those words or phrases can be. Figuring out how to handle all that will be something we work on in the future. For rules about topic words, we give a score to the word marked by a square. And for matching words, \"X\" and \"x\" mean the words are the same or synonyms from this Japanese thesaurus. Same goes for \"Bulu'ui\" and \"Nagao\".",
        "formal_text": "Convert casual text to formal text: Okay, so basically, all this stuff follows some rules that help us figure out things like how similar two groups of words are, or how important a word is in a sentence. For example,"
    },
    {
        "casual_text": "So, like we mentioned earlier, double propagation works because there's this natural connection between opinion words and features. It's like, opinion words are often used to describe or tweak features. Plus, opinion words and features themselves are linked in phrases that express opinions (Qiu et al., 2009). We can spot these connections using a dependency parser (Lin, 1998), which is based on dependency grammar. Finding these connections is super important for pulling out the features.",
        "formal_text": "Convert casual text to formal text: So, like we mentioned earlier, double propagation works because there's this natural connection between opinion words and features. It's like, opinion words are often used to describe or tweak features. Plus"
    },
    {
        "casual_text": "Some other popular topics cover stuff like immigration rules, money matters, and travel options like flights and trains, all tailored to different countries. This info could be handy for making quick summaries by topic, which would be super useful for people planning trips to multiple places or even deciding where to go in the first place. For instance, if you're into music shopping, you might want to check out the UK, but if electronics are your thing, Singapore could be the spot (based on one of the topics we found).",
        "formal_text": "Convert casual text to formal text: Some other popular topics cover stuff like immigration rules, money matters, and travel options like flights and trains, all tailored to different countries. This info could be handy for making quick summaries by topic,"
    },
    {
        "casual_text": "So, M  m is just the number of the transformer layer we're dealing with, and t  i is the position of the word, counting backward from the last word in the context. In this paper, we keep the W  H size at 33. To match the size of the original hidden state h M ct, we use a linear layer L h along with a GELU activation function (Hendrycks and Gimpel, 2016). After that, we stick it together with the original hidden state to create a fresh input hidden state.",
        "formal_text": "Convert casual text to formal text: So, M  m is just the number of the transformer layer we're dealing with, and t  i is the position of the word, counting backward from the last"
    },
    {
        "casual_text": "We used a re-implementation of Tacotron2 (version 3) from Shen et al. (2018) for our U2S models. For single-speaker models trained on LJSpeech, we kept everything the same—same hyperparameters and model architecture as in the original paper by Shen et al. For multi-speaker models trained on VCTK, we added a speaker embedding table with 256 dimensions for all speakers. This table helps control the speaker identity by injecting the embeddings in two spots in the decoder: first, combined with the original input to the decoder LSTM, and second, combined with the output of the decoder LSTM just before it predicts the stop token and the frame's spectra. For all U2S models, we used a pre-trained WaveGlow vocoder (Prenger et al., 2019), showing how versatile vocoder models can be. Here’s a breakdown of some metrics and decoding approaches for the VQ3 SAT-FT model: - **Sampling with Temperature** - t = 1.0 - t = 0.7 - t = 0.4 - t = 0.1 - **Top-K Sampling** - k = 10 - k = 5 - k = 3 Table A4 shows the vocabulary size of the VQ3 SAT-FT model based on different decoding methods. The numbers in the table correspond to the curves you see in Figure 4.",
        "formal_text": "Convert casual text to formal text: We used a re-implementation of Tacotron2 (version 3) from Shen et al. (2018) for our U2S models. For single-speaker"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. Imagine you have a bunch of protocols, and each one has different words in it. The variable x_ji is just a way to count how many times the word w_i shows up in the jth protocol. Now, TF-IDF is a method that adjusts the importance of each word in these protocols. Specifically, it tweaks the ith element of these word count vectors to give more weight to words that are important but not too common.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in simpler terms. Imagine you have a bunch of protocols, and each one has different words in it. The variable x_ji is just"
    },
    {
        "casual_text": "Plus, we checked out how our top models (with RD and AT) did compared to older studies, like Farasa's, on the test sets of both datasets. The results are in Table 2, and our model with the BiLSTM encoder totally crushed it, beating all the previous models and setting the new standard for performance on both datasets.",
        "formal_text": "Convert casual text to formal text: Plus, we checked out how our top models (with RD and AT) did compared to older studies, like Farasa's, on the test sets of both datasets. The"
    },
    {
        "casual_text": "Compared to the imbalanced version, the balanced one scored higher in BLEU for DE-EN but performed way worse in RO-EN for both the original label smoothing (LS) and MLS. Training on RO-EN examples seems to mess with the model's ability to generalize in RO-EN translation but doesn’t affect DE-EN quality. This might be because the RO-EN data introduces some bias that impacts DE-EN training. Even in an imbalanced setup, MLS still performs better (37.53) than the original LS in a balanced setup (37.44). This shows that MLS can help with the imbalance issue in multilingual translation. But the improvement in the high-resource direction (RO-EN) isn’t as big as in the balanced condition. We think label smoothing has a more complicated effect on multilingual models because of the extra languages and their relationships. We’ll leave that for future research. Based on the results, two things stand out: 1) Using WLS generally improves translation quality compared to the original LS. 2) Only WLS with  t,  c,  s set to 1/2-1/2-0 beats the original LS across all tasks, making it the most reliable setup.",
        "formal_text": "Convert casual text to formal text: Compared to the imbalanced version, the balanced one scored higher in BLEU for DE-EN but performed way worse in RO-EN for both the original label smoothing (LS) and"
    },
    {
        "casual_text": "From Lord et al. (1968, p. 70), we can understand this better. Basically, if we don’t know someone’s true skill level  j yet, we should choose questions where we’re most unsure about how they’ll answer. Our uncertainty, or entropy, is highest when there’s an equal chance of getting the answer right or wrong, which happens when the value of I i ( j ) is at its peak. It also makes sense that this value gets bigger as the question’s discriminability  i increases.",
        "formal_text": "Convert casual text to formal text: From Lord et al. (1968, p. 70), we can understand this better. Basically, if we don’t know someone’s true skill level  j"
    },
    {
        "casual_text": "Phrase-based translation systems depend a lot on the target language model to make sure the translated sentences sound natural and follow the right order. But just using a target n-gram language model isn't enough. So, translation systems need to consider how the original sentence likes to be rearranged too. Unfortunately, older systems usually used pretty basic models for this reordering process. They might only check how far apart neighboring phrases are or just look at single phrases without context. The decoders also rely on limiting how much they can reorder things at once, both to save time and to improve translation quality.",
        "formal_text": "Convert casual text to formal text: Phrase-based translation systems depend a lot on the target language model to make sure the translated sentences sound natural and follow the right order. But just using a target n-gram language model"
    },
    {
        "casual_text": "Our CM-Net is killing it on two big SLU benchmarks—ATIS and SNIPS—and scoring top marks in most categories.",
        "formal_text": "Convert casual text to formal text: Our CM-Net is killing on two big SLU benchmarks—ATIS and SNIPS—and scoring top marks in most categories. Convert casual text to formal text: Our CM"
    },
    {
        "casual_text": "So, researchers dealing with non-English languages in situations where resources are limited haven’t had a tool to help them pick the right models by giving them useful linguistic insights—until now. We’ve introduced LINSPECTOR WEB, an open-source, web-based evaluation tool that comes with 16 probing tasks for 28 languages. It can work with pretrained static word embeddings and different layers of a bunch of AllenNLP models. Plus, it’s super easy to add more languages, tasks, models, or even new AllenNLP models if needed. You can check out LINSPECTOR WEB here: https://linspector.ukp.informatik.tu-darmstadt.de, and the source code for the server is available on GitHub: https://github.com/UKPLab/linspector-web, along with instructions on how to set it up. Right now, we’re working on expanding it to handle contextualized word embeddings and contextualized probing tasks using Universal Dependency Treebanks (Nivre et al., 2019).",
        "formal_text": "Convert casual text to formal text: So, researchers dealing with non-English languages in situations where resources are limited haven’t had a tool to help them pick the right models by giving them useful linguistic insights—until"
    },
    {
        "casual_text": "So, the target word t_i is connected to the source word s_a_i. This connection, called a_i, comes from word alignments and some rules we follow (heuristics 1). To figure out these probabilities, the NNJM looks at m source words around the target and n-1 target words that came before it. It uses a neural network to guess the unnormalized probabilities p(t_i | C) and then adjusts them to fit all the words in the target vocabulary V.",
        "formal_text": "Convert casual text to formal text: So, the target word t_i is connected to the source word s_a_i. This connection, called a_i, comes from word alignments and some rules we follow"
    },
    {
        "casual_text": "We tried out a bunch of different ways to measure entailment, like SAPinc and SBalAPinc from Kartsaklis and Sadrzadeh (2016), their word-level versions from Kotlerman et al. (2010), KL-divergence on smoothed vectors (Chen and Goodman, 1996), -skew with  set to 0.99 (Kotlerman et al., 2010), WeedsPrec from Weeds et al. (2004), and ClarkeDE from Clarke (2009). As a simple baseline, we used strict feature inclusion, where entailment only happens if F(phrase 1) is completely inside F(phrase 2). For combining words into phrases, we tested things like multiplying or taking the minimum of vectors, adding them or taking the maximum, and some tensor models we talked about in Section 6. From messing around with these, we found that sticking the distributional info from verb vectors directly into the tensors (Section 6.4) worked way better than the basic versions. So, that’s the approach we’re reporting results for. We also tried a least squares fitting model, which kind of mimics how whole phrases or sentences behave, like Baroni and Zamparelli (2010) did. Basically, for each verb, we made a predictor that guesses the ith element of the resulting vector.",
        "formal_text": "Convert casual text to formal text: We tried out a bunch of different ways to measure entailment, like SAPinc and SBalAPinc from Kartsaklis and Sadrzadeh (2016),"
    },
    {
        "casual_text": "To make things more thorough, we’re also including METEOR scores to go along with the BLEU evaluation from section 4.3. BLEU is the main evaluation method for WMT biomedical translation tasks (Jimeno Yepes et al., 2017; Neves et al., 2018; Bawden et al., 2019, 2020). The improvements of DIV-FACTORIZED over EQUIVALENTS and DIV-AGNOSTIC are smaller on average when looking at METEOR compared to what BLEU shows. But here’s the thing: METEOR results can be a bit tricky when it comes to medical translations. In this field, we might not want to consider synonyms when comparing the reference to the hypothesis, so METEOR might not give the most accurate picture.",
        "formal_text": "Convert casual text to formal text: To make things more thorough, we’re also including METEOR scores to go along with the BLEU evaluation from section 4.3. BLEU is the main evaluation method for WMT bio"
    },
    {
        "casual_text": "To sum it up, we’ve come up with LMLM and rLMLM, which focus on telling good hypotheses apart from bad ones based on specific tasks, like WER or BLEU, instead of just trying to lower PPL.",
        "formal_text": "Convert casual text to formal text: To sum it, we’ve come up with LMLM and rLMLM, which focus on telling good hypotheses apart from bad ones based on specific tasks, like WER or"
    },
    {
        "casual_text": "We also looked at some gradient-based methods for comparison. Sensitivity Analysis (from Simonyan et al., 2013) just takes the absolute value of the gradient. Gradient*Input (by Shrikumar et al., 2016) is a straightforward multiplication of the input by its gradient, element by element. Integrated Gradients (Sundararajan et al., 2017) works by integrating gradients from a baseline input up to the current input. Since these gradient-based methods give relevance scores for each part of the word vector, we added up all the parts that belong to the same word. Keep in mind, these methods don’t take likelihood into account in NLP (check out Section 1 for more on that), so they’re just for comparison—not the ultimate benchmark.",
        "formal_text": "Convert casual text to formal text: We also looked at some gradient-based methods for comparison. Sensitivity Analysis (from Simonyan et al., 2013) just takes the absolute value of the gradient. Gradient"
    },
    {
        "casual_text": "In TESLA-M, there are two ways they handle weights. First, they focus more on content words by reducing the weight of an n-gram by 0.1 for each function word it has. Second, they measure how similar two n-grams are based on their lemmas, WordNet synsets (from Fellbaum, 1998), and the part of speech tags of each word in the n-grams.",
        "formal_text": "Convert casual text to formal text: In TESLA-M, there are two ways they handle weights. First, they focus more on content words by reducing the weight of an n-gram by 0.1 for each function"
    },
    {
        "casual_text": "How can we make the composition functions flexible enough to handle all kinds of different combinations? Like, adjective-noun pairs are totally different from verb-prepositional phrase combos, so the functions need to work well with both.",
        "formal_text": "Convert casual text to formal text: How can we make the composition functions flexible enough to handle all kinds of different combinations? Like, adjective-noun pairs are totally different from verb-prepositional phrase combos, so the functions need"
    },
    {
        "casual_text": "So, we take all the attention vectors we calculated for each row in T and combine them with weights to get the relation-oriented representation l_i for sentence i.",
        "formal_text": "Convert casual text to formal text: So, we take all attention vectors we calculated for each row in T and combine them with weights to get the relation-oriented representation l_i for sentence i. So, we take"
    },
    {
        "casual_text": "Step 6: Group the sentences you picked for the summary by where they came from, then arrange them in the order they appear in the original text.",
        "formal_text": "Convert casual text to formal text: Step 6: Group the sentences you picked for the summary by they came from, then arrange them in the order they appear in the original text. Step 7: Group the sentences you picked for the summary by"
    },
    {
        "casual_text": "For the benchmarks, we’re using FairSeq and HuggingFace-Transformers as our starting points to check how well everything performs. We’ve got a mix of models here, including encoder-decoder types (like BART, DistilBART, T5, and ProphetNet), decoder-only models (like GPT2), and encoder-only models (like UniLM). The datasets we’re testing on are the CNN / Daily Mail dataset (from Hermann et al., 2015) and WMT'16 (from Bojar et al., 2016). The experiments are divided into two main groups: 1) HuggingFace-Transformers with and without FastSeq, and 2) FairSeq with and without FastSeq. If both FairSeq and HuggingFace-Transformers have the same model, we pick the faster one as our baseline.",
        "formal_text": "Convert casual text to formal text: For the benchmarks, we’re using FairSeq and HuggingFace-Transformers as our starting points to check how well everything performs. We’ve got a mix"
    },
    {
        "casual_text": "It drops down to 856 sequences, which probably isn't enough for the models to learn and make good predictions.",
        "formal_text": "Convert casual text to formal text: It drops down to 856 sequences, which probably ... more likely to be enough for the models learn and good predictions. Convert casual text to formal text: It drops down to 856 sequence"
    },
    {
        "casual_text": "We've got a cool thing going on: we’re sharing the full text of 8.1 million papers that are open access, and we’ve made it super easy for machines to read. The S2ORC full text keeps all the important stuff intact, like paragraph breaks, section titles, mentions of citations within the text, and even links to tables, figures, and other papers. Plus, we’ve got 1.5 million papers where we’ve pulled out the full text in LaTeX format, so you also get the actual content of tables and math formulas, along with citations and references. Check out Table 1—it shows that S2ORC has way more structured full text papers and covers a wider range of subjects than other resources out there.",
        "formal_text": "Convert casual text to formal text: We've got a cool thing going on: we’re sharing the full text of 8.1 million papers that are open access, and we’ve made it super easy for machines to read."
    },
    {
        "casual_text": "Revere mentioned they got an offer from a group of investors who want to buy the company for $16 per share, which adds up to around $127 million.",
        "formal_text": "Convert casual text to formal text: Revere mentioned they got an offer from a group of investors who want to buy the company for $16 per share, which adds to around $127 million."
    },
    {
        "casual_text": "We'll talk about this in the next part. For an example of an abstract, you can check out these links: http://ieee.rkbexplorer.com/description/publication-00534618 and http://www.ncbi.nlm.nih.gov/pubmed.",
        "formal_text": "Convert casual text to formal text: We'll talk about this in the next part. For an example of an abstract, you can check out these links: http://ieee.rkbexplorer.com/description"
    },
    {
        "casual_text": "A lot of research has been done on figuring out what attention heads actually learn when language models are being trained. For example, Clark et al. (2019) and Kovaleva et al. (2019) have looked into this. Even Liu et al. (2016) found that giving attention heads some direction can help improve how well neural machine translation works.",
        "formal_text": "Convert casual text to formal text: A lot of research has been done on figuring out what attention heads actually learn when language models are being trained. For example, Clark et al. (2019) and Kovaleva e"
    },
    {
        "casual_text": "3. Comments in another language: We use a tool called langid.py (created by Lui and Baldwin) to check the language of comments. If the probability of a comment being in any language other than the one we're focusing on is more than 0.5, we toss it out.",
        "formal_text": "Convert casual text to formal text: 3. Comments in another language: We use a tool called langid.py (created by Lui and Baldwin to check the language of comments. If the probability of a comment being"
    },
    {
        "casual_text": "• The beam-search coefficient B is used to cut out hypotheses that have a score lower than the best score times this coefficient.",
        "formal_text": "• The beam-search coefficient B is used to cut out hypotheses that have a score lower than the best score times this coefficient. Convert casual text to formal text: • The beam-search coefficient B is used to cut out"
    },
    {
        "casual_text": "There are a bunch of ways to make unsupervised methods better. For example, if we train bigger word embedding models, we can cover a larger vocabulary. Looking into common n-grams in these models might help us identify phrases or multi-word expressions more effectively. Another idea is to keep refining the process by picking the best-scoring sentence pairs from bitexts after training and alignment, and adding them to the training set of the SMT system. This would give us more accurate training data, which could lead to better translations with each iteration. As a result, we’d probably feel more confident about choosing the best alignments.",
        "formal_text": "Convert casual text to formal text: There are a bunch of ways to make unsupervised methods better. For example, if we train bigger word embedding models, we can cover a larger vocabulary. Looking into common n"
    },
    {
        "casual_text": "We take sentences from Wikipedia dumps in the languages we're working with, along with some monolingual data from WAT-ILMPC and a few news articles we found online. We use these to do backtranslation and get more training samples for different languages. We only backtranslate to Hindi and English from other low-resource languages because the BLEU scores for the other directions weren't great. For more details on the data we used and how it compares across languages on other test sets, you can check out Philip et al. (2019).",
        "formal_text": "Convert casual text to formal text: We take sentences from Wikipedia dumps in the languages we're working with, along with some monolingual data from WAT-ILMPC and a few news articles we found online. We use"
    },
    {
        "casual_text": "For the topic segmentation task, we want to create a way to measure how similar words are, even if they're synonyms or have some kind of related meaning. This similarity measure will help us check how well different parts of the text fit together in terms of meaning. If the meaning starts to feel disconnected, that's when we'll know there's a shift in the story. To make this work, we'll first need to build a way to represent the document that supports this similarity measure.",
        "formal_text": "Convert casual text to formal text: For the topic segmentation task, we want to create a way to measure how similar words are, even if they're synonyms or have some kind of related meaning. This similarity measure"
    },
    {
        "casual_text": "The first experiment gave us a bunch of useful facts for travelers, like a destination's weather, laws, and how things work there. But it didn’t really cover the cultural stuff. Now, we’re curious to see if we can dig deeper into that by looking at text written by people who actually live in those countries. This way, we can see what they talk about and how they talk about it, which might give us a better idea of the culture.",
        "formal_text": "Convert casual text to formal text: The first experiment gave us a bunch of useful facts for travelers, like a destination's weather, laws, and how things work there. But it didn’t really cover the cultural stuff."
    },
    {
        "casual_text": "When talking about Machine Translation (MT), \"noise\" usually means issues in parallel texts gathered from the web that aren't always high-quality. Let's look at five common types of noise in the German-English Paracrawl dataset: sentences that don't align properly, awkward or broken text, the wrong language showing up, really short segments, and sentences that haven't been translated. They looked into how this noise affects translation quality and noticed that when training data includes untranslated sentences, the MT model tends to just repeat the input during translation. This discovery led to a shared task at WMT since 2018, where the goal is to clean up noisy samples from web-crawled data. This research goes beyond just dealing with these obvious mismatches and looks at more subtle differences that only affect a few words in otherwise similar sentence pairs. These fine-grained divergences can be found even in high-quality parallel corpora.",
        "formal_text": "Convert casual text to formal text: When talking about Machine Translation (MT), \"noise\" usually means issues in parallel texts gathered from the web that aren't always high-quality. Let's look at five common"
    },
    {
        "casual_text": "Basically, if you have a whole tree for one sentence in a language, and you're looking at the root category R, you need to make sure all the features fi of R are filled in based on their possible values in D(fi). This is because the tree could represent different possible ways to understand the sentence. Once that's done, you check the rules for each tree in this group. Only the trees where FCRc(0) and LPc(0) are set and their evaluation is empty are allowed to be considered correct.",
        "formal_text": "Convert casual text to formal text: Basically, if you have a whole tree for one sentence in a language, and you're looking at the root category R, you need to make sure all the features fi of R"
    },
    {
        "casual_text": "Our ordering algorithm works even better for longer texts because the importance of having things in the right order and making sense grows as the text gets longer. When we look at the 400-word category, the full model's ordering, according to Kendall's measure, is way better than any other method we tried.",
        "formal_text": "Convert casual text to formal text: Our ordering algorithm works even better for longer texts because the importance of having things in the right order and making sense grows as the text gets longer. When we look at the 400-word category, the full"
    },
    {
        "casual_text": "In this paper, we introduce a framework called FIND (Feature Investigation aNd Disabling) that helps humans debug and improve deep text classifiers by turning off hidden features that don't really matter for the classification task. FIND uses a method called layer-wise relevance propagation (LRP) (Arras et al., 2016) to figure out how the classifier behaves when it makes predictions on training data. It then puts all this information together in the form of word clouds to give a clear, overall picture of the model. This makes it easier for people to understand the features the deep classifier has learned and decide which ones to disable if they think they might mess up the predictions during testing. The main things that set our work apart from what's already out there are: (i) FIND focuses on getting human feedback on the model's components, not just on individual predictions, to help with debugging; (ii) FIND is designed specifically for deep text classifiers, which are more complex than the traditional ones used in other work, like Naive Bayes or Support Vector Machines.",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce a framework called FIND (Feature Investigation aNd Disabling) that helps humans debug and improve deep text classifiers by turning off hidden features that"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: We're taking the data from E  2, p train and splitting it into three parts: E  2, p train, E  2, p val, and E  2, p test. This is done using the split function with the ratios r x, r y, and r z. Next, we do the same thing with the data from E  1  2, p train, splitting it into E  1  2, p train, E  1  2, p val, and E  1  2, p test, again using the split function with the same ratios. Finally, we repeat this process with the data from E   1, p train, splitting it into E   1, p train, E   1, p val, and E   1, p test, using the same split function and ratios.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: We're taking the data from E  2, p train and splitting it into three parts: E"
    },
    {
        "casual_text": "In simpler terms, when words in a text have multiple meanings, like in \"bank\" (could mean a financial institution or the side of a river), the system automatically figures out which meaning is being used. This process is called Word-Sense Disambiguation (WSD for short). It's a big deal in Natural Language Processing (NLP) and there have been a bunch of different methods suggested to tackle it, like the ones mentioned by folks like G. Salton in 1991 and D. Yarowsky in 1992.",
        "formal_text": "Convert casual text to formal text: In simpler terms, when words in a text have multiple meanings, like in \"bank\" (could mean a financial institution or the side of a river), the system automatically figures"
    },
    {
        "casual_text": "We look at how different fixed budgets for creating training data impact different languages and offer some tips for people planning to make datasets in the future.",
        "formal_text": "Convert casual text to formal text: We look at how different fixed budgets for creating training data impact different languages and offer some tips people planning to make datasets in the future. Convert casual text to formal text: We look at how"
    },
    {
        "casual_text": "We take a look at some basic stuff for each word in a sentence. For things like SRL and RC, we usually check out the word itself (or its base form if we have it) and its part of speech. For RC, we might also throw in some extra details, like what kind of named entity it is or use WordNet. All these bits of info get turned into small, easy-to-handle numbers, which we call feature embeddings. You can already get word embeddings ready-made by using word2vec on a big pile of text without labels, and they’ve been pretty useful in lots of projects. After that, we mix all these feature embeddings together in a fancy way, so each word can be represented like this:",
        "formal_text": "Convert casual text to formal text: We take a look at some basic stuff for each word in a sentence. For things like SRL and RC, we usually check out the word itself (or its base form if we have"
    },
    {
        "casual_text": "W_M is the score matrix for label M, and e_M is the label embedding for label M.",
        "formal_text": "Convert casual text to formal text: W_M is the score matrix for label M, and e_M is the label embedding for label M. Convert casual text to formal text: W_M is the score matrix"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. Right now, we're looking at a label, which is basically the tag given after processing the last part of a sentence or clause. One important idea in this area is called Focus Background Pair (FBP), which is related to something called \"background.\" This concept of D-Subordination was explained by Asher back in 1996, specifically on page 24 of his work. Now, figuring out how things like narration or elaboration work in discourse—that's handled by a theory called DICE. DICE stands for Discourse in Commonsense Entailment, and it uses a type of logic that isn't the usual, straightforward kind. It takes into account the reader's general knowledge and some principles from Grice, who talked about how people communicate effectively. DICE gives us a formal way to understand how discourse is put together. The key parts of this theory are defaults, which are like rules that help us understand the relationships between different parts of a discourse and how they fit together.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in simpler terms. Right now, we're looking at a label, which is basically the tag given after processing the last part of a sentence or"
    },
    {
        "casual_text": "Alright, so first, we grab a bunch of question clusters from Q&A websites. Each cluster has a bunch of questions that the website thinks are related. For instance, on most Q&A sites, each question page has a section that lists other similar questions.",
        "formal_text": "Convert casual text to formal text: Alright, so first, we grab a bunch of question clusters from Q&A websites. Each cluster has a bunch of questions that the website thinks are related. For instance, on"
    },
    {
        "casual_text": "We turn each structured data example, x, into a text sequence by breaking it down into a list of attribute names and their values, separated by [SEP] tokens. For example, the first attribute-name and attribute-value pair in Figure 1 would look like 'odor | pungent'. This way of organizing the data lets us use pre-trained language models for the classification task. Our method is similar to the one in Yin et al. 2020, but we don’t use the column type. We’ll call this linearized format of structured inputs 'Features-as-Text' or 'FaT' for short. This approach was especially helpful because some of the 9 datasets we worked with had super small sample sizes (like 5 or so). You can check out the list of crowdsourced tasks in Table 7.",
        "formal_text": "Convert casual text to formal text: We turn each structured data example, x, into a text sequence by breaking it down into a list of attribute names and their values, separated by [SEP] tokens. For example"
    },
    {
        "casual_text": "Alright, so with the in-passage and cross-passage edges set up as mentioned earlier, we can build an inter-sentence relation graph (ISRG) for the review V and rebuttal B. In this graph, the nodes represent all the sentences from both the review and the rebuttal. Now, the adjacency matrix A, which is a (m+n)(m+n) matrix, can be created to show these connections.",
        "formal_text": "Convert casual text to formal text: Alright, so with the in-passage and cross-passage edges set up as mentioned earlier, we can build an inter-sentence relation graph (ISRG) for the review V and"
    },
    {
        "casual_text": "The workers tossed big burlap bags of the imported stuff into a giant container, added some cotton and acetate fibers, and then used a machine to mix everything together. This is how they make filters.",
        "formal_text": "Convert casual text to formal text: The workers tossed big burlap bags of the imported stuff into a giant container, added some cotton and acetate fibers, and then used a machine to mix everything together. This is"
    },
    {
        "casual_text": "So, let's break it down: \"a_i\" is the i-th atom in the compound C, N is just how many atoms are in the compound, M is the total number of ways you can pair up two atoms, and the PMI score is calculated using this formula:",
        "formal_text": "Convert casual text to formal text: So, let's break it down: \"a_i\" is the i-th atom in the compound C, N is just how many atoms are in the compound, M"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Here's a list of sounds and some info about them: **Sounds:** p, b, f, v, m, 8, 4, t, d, s, z, c, n, S, Z, C, j, T, 5, k, g, x, N, q, G, X, 7, h, l, L, w, y, r! V **Voiced or Not:** - Voiced: b, v, m, 8, 4, d, z, n, S, Z, C, j, T, 5, g, N, q, G, X, 7, l, L, w, y, r! V - Not Voiced: p, f, t, s, c, x, h **Place of Articulation:** - **Labial:** p, b, f, v, m, 8, 4 - **Dental:** t, d, s, z, c, n - **Alveolar:** t, d, s, z, c, n - **Palatal/Post-alveolar:** S, Z, C, j, T, 5 - **Velar:** k, g, x, N, q, G, X, 7 - **Uvular:** q, G, X, 7 - **Glottal:** h **Manner of Articulation:** - **Stop:** p, b, t, d, k, g, q, G, X, 7 - **Fricative:** f, v, s, z, S, Z, C, x, h, r! V - **Affricate:** C, j, T, 5 - **Nasal:** m, n, N",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. Here's a list of sounds and some info about them: **Sounds:** p, b,"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: \"Open set: A lot of keyphrases from scientific papers (around 20% in our case) haven’t been seen in the training data (meaning the keyphrase pool is kind of endless). Our plan has two steps: First, we build a structured document X+ by adding relevant keyphrases R from the training data to the original document X. After that, the second step is to create structure-aware representations using X+. We use a graph to combine X and R, and the graph can be tweaked depending on whether we’re dealing with a closed or open set situation.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: \"Open set: A lot of keyphrases from scientific papers (around 20% in our case) haven’t been seen in"
    },
    {
        "casual_text": "The algorithm looks for potential acronyms that meet the rules outlined in Section 3.1 and aren't inside parentheses. When it spots a possible acronym, it checks the words right before and after it to see if there's a matching definition. The search for the definition is limited to four words for each letter in the acronym.",
        "formal_text": "Convert casual text to formal text: The algorithm looks for potential acronyms that meet the rules outlined in Section 3.1 and aren't inside parentheses. When it spots a possible acronym, it checks the words right before"
    },
    {
        "casual_text": "Another idea is to add Voice as a feature at the feature level. UD already has features for Modality (like Mood), Tense, and Aspect, but it doesn’t have one for Voice. Voice covers things like Passive, Agentive, Resultative, Causative, and so on. So, it would be cool to create a Voice feature to make the description of auxiliaries in UD morphology more complete. This is especially interesting because Voice is closely tied to the semantic role of the subject, like whether it’s a Patient, Agent, Beneficiary, Cause, etc. Having this feature could also help with other NLP tasks that deal with meaning.",
        "formal_text": "Convert casual text to formal text: Another idea is to add Voice as a feature at the feature level. UD already has features for Modality (like Mood), Tense, and Aspect, but it doesn’t"
    },
    {
        "casual_text": "From the results, it looks like: 1) Even though both CL Trans (1 cand.) and Embedding proj are methods that don't rely on the content, CL Trans does better (+3.2% on F1). This suggests that the embedding projection method might have issues with how the shared embedding space is set up. On the other hand, CL Trans, which uses a word-to-word alignment, seems to fix this problem to some extent. 2) Getting more translation candidates can definitely boost Recall. But if you go overboard (like adding 5 candidates), Precision takes a hit, which messes up the overall F1 score.",
        "formal_text": "Convert casual text to formal text: From the results, it looks like: 1) Even though both CL Trans (1 cand.) and Embedding proj are methods that don't rely on the content, CL Trans does better"
    },
    {
        "casual_text": "Alright, let's dive in with the most important stuff first. We'll follow this order: PERS, CASE_, NLMR, GEI_R. Keep it simple and don't waste space storing extra stuff. Start from the left and work your way through.",
        "formal_text": "Convert casual text to formal text: Alright, let's dive in with the most important stuff first. We'll follow this order: PERS, CASE_, NLMR, GEI_"
    },
    {
        "casual_text": "Also, our analysis doesn't back up the idea that POS might be causing changes in meaning (Dubossarsky et al., 2016). But keep in mind, since half of our words are slang, the results might not match up with studies using regular words.",
        "formal_text": "Convert casual text to formal text: Also, our analysis doesn't back up the idea that POS might be causing changes in meaning (Dubossarsky et al., 2016). But keep in mind"
    },
    {
        "casual_text": "Basically, it tries to make the difference between a uniform distribution across K classes (let's call it U(l)) and what the discriminator predicts as shown in equation (3) as small as possible. The final loss for the QA model is a combination of the regular loss (L QA) and an extra loss (L adv) that depends on how important we think the adversarial part is, with  being the knob we turn to adjust that importance. During our tests, we switch back and forth between improving the QA model and the discriminator.",
        "formal_text": "Convert casual text to formal text: Basically, it tries to make the difference between a uniform distribution across K classes (let's call it U(l)) and what the discriminator predicts as shown in equation (3)"
    },
    {
        "casual_text": "For all our experiments, we keep the weights (from Press and Wolf, 2017) of the encoder embedding, decoder embedding, and decoder output layers tied together. This really helps cut down on the number of parameters and speeds up training time. We train for up to 20 epochs and pick the checkpoint with the best oracle metric. We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 and momentum parameters 1 = 0.9 and 2 = 0.999. The minibatch size is 64 for question generation and 32 for abstractive summarization. All the models are built using PyTorch (Paszke et al., 2017) and trained on a single Tesla P40 GPU, running on the NAVER Smart Machine Learning (NSML) platform (Kim et al., 2018a).",
        "formal_text": "Convert casual text to formal text: For all our experiments, we keep the weights (from Press and Wolf, 2017) of the encoder embedding, decoder embedding, and decoder output layers tied together"
    },
    {
        "casual_text": "Alright, so we've got a bunch of chat logs where people are talking to each other, and we're pulling out all the URLs they mention. We use Selenium 2 to load both static and dynamic web pages. Our main goal is to grab the text content, so we clean up the extra stuff like menus, search bars, sidebars, headers, and footers. We keep the text and any references to things like embedded videos or other media. Finally, we save the cleaned-up content in two formats: markdown and formatted text.",
        "formal_text": "Convert casual text to formal text: Alright, so we've got a bunch of chat logs where people are talking to each other, and we're pulling out all the URLs they mention. We use Selenium"
    },
    {
        "casual_text": "In life, I'm free to do whatever I want. No worries, no stress, just living it up.",
        "formal_text": "Convert casual text to formal text: In life, I'm free whatever I want. No worries, no stress, just living it up. Convert casual text to formal text: In life, I'm free whatever I want."
    },
    {
        "casual_text": "Okay, so if we have a set 1, 2, ..., n with n elements, this works. This gives us a setup c0 where T c0(i) =  and A c0(i) = .",
        "formal_text": "Convert casual text to formal text: Okay, so if we have a set 1, 2, ..., n with n elements, this works. This gives us a setup c0 where"
    },
    {
        "casual_text": "Okay, so once we've confirmed that a local tree is just a projection of an ID rule, we do a few things: first, we apply all the F1Ps to that tree, and then we apply the FCRs to its mother. We also figure out which FCRs might be relevant for the mother and put those numbers in a set called APP(0). Next, we look at the subtrees of the daughters and apply all the applicable FCRs to them, including to all the other categories in their stthtrecs. After that, we take the leftover FCR constraints from this evaluation, along with the FCR constraint (C, APP(0)) that applies to the mother, and mash them together to create a new set of FCR constraints called FCRc(0). This new set is then passed on to the mother, with C being the main category at the root of the subtree.",
        "formal_text": "Convert casual text to formal text: Okay, so once we've confirmed that a local tree is just a projection of an ID rule, we do a few things: first, we apply all the F1Ps to"
    },
    {
        "casual_text": "Karachi has fewer than five districts. Correct. Karachi has fewer than six districts. Correct. Karachi has fewer than seven districts. Correct. Karachi has five districts. Incorrect. Karachi has six districts. Incorrect. Karachi has seven districts. Incorrect. Karachi has more than five districts. Incorrect. Karachi has more than six districts. Correct. Karachi has more than seven districts. Correct. Table 3: Some test cases based on the problem where the main idea is that Karachi has exactly six districts.",
        "formal_text": "Convert casual text to formal text: Karachi has fewer than five districts. Correct. Karachi has fewer than six districts. Correct. Karachi has fewer than seven districts. Correct. Karachi has five districts. Incorrect"
    },
    {
        "casual_text": "Our setup is like a tree, with one main root and a bunch of end points called leaf nodes.",
        "formal_text": "Convert casual text to formal text: Our setup is like a tree, with one main root and a bunch of end points called leaf nodes."
    },
    {
        "casual_text": "The c-test (Raatz and Klein-Braley, 1981) is a common exercise used to test language skills. In this test, the second half of every other word is missing, and you have to fill in the blanks to complete the text. For instance, the word \"redundancy\" would become \"redun___.\"",
        "formal_text": "Convert casual text to formal text: The c-test (Raatz and Klein-Braley, 1981) is a common exercise used to test language skills. In this test, the second half of every other word is"
    },
    {
        "casual_text": "Deep neural models are kind of famous for being easy to trick with something called adversarial perturbations—basically, tiny changes to the input that can mess with the model's output, even if the model was trained really well (Goodfellow et al., 2015; Ebrahimi et al., 2018b). There's another type of attack where someone messes with the model itself to create fake weaknesses, so the attacker can control how the model reacts to certain changes. For example, they might tweak the model to always say a sentence has a positive sentiment, even if it’s totally negative. Like, \"This is a train wreck of an action film—a ridiculous attempt by the filmmakers to cram James Bond into the dumb XXX BB mold and throw 40 years of movie history out the window for some flashy lights and loud explosions.\"",
        "formal_text": "Convert casual text to formal text: Deep neural models are kind of famous for being easy to trick with something called adversarial perturbations—basically, tiny changes to the input that can mess with the model's output, even"
    },
    {
        "casual_text": "So, we think this testbed and process could be really important for future NLP research, especially when it comes to studying psychometrics and fairness as part of understanding users better.",
        "formal_text": "Convert casual text to formal text: So, we think this testbed and process could really important for future NLP research, especially when it comes to studying psychometrics and fairness as part of understanding users better. Convert casual text"
    },
    {
        "casual_text": "Whether it's computers or humans doing the understanding, a lot of it comes down to how the speaker organizes and presents the information. So, we really focus on using psycholinguistics—basically, the study of how people process language—to build models that help us figure out how speech comprehension works.",
        "formal_text": "Convert casual text to formal text: Whether it's computers or humans doing the understanding, a lot of it comes down to how the speaker organizes and presents the information. So, we really focus on using psycholinguistics—"
    },
    {
        "casual_text": "Looking at Table 8, our non-UGC system is way behind the MoNoise model (from van der Goot and van Noord, 2017) when it comes to the F1 score. To give a better idea, we also included the overall accuracy of our system in Table 6. So, we're lagging behind by 6.7 points in the F1 score and just 0.2 points in overall accuracy when comparing to the lexnorm15 dataset.",
        "formal_text": "Convert casual text to formal text: Looking at Table 8, our non-UGC system is way behind the MoNoise model (from van der Goot and van Noord, 2017) when it comes to the F1"
    },
    {
        "casual_text": "Probing in NLP: What’s it all about? Probing is a way to figure out how much linguistic knowledge is hidden in language representations. Basically, it checks how much useful information is packed into these representations by seeing how well a supervised model can predict certain linguistic features just by looking at them. This idea has been explored by researchers like Köhn (2015), Gupta et al. (2015), Yaghoobzadeh and Schütze (2016), Conneau et al. (2018), Tenney et al. (2019), Hewitt and Manning (2019), Zhao et al. (2020), and Belinkov (2022). In this study, we’re using probing to investigate whether metaphorical knowledge is embedded in PLM (Pre-trained Language Model) representations and if this knowledge can be applied consistently across different languages and datasets.",
        "formal_text": "Convert casual text to formal text: Probing in NLP: What’s it all about? Probing is a way to figure out how much linguistic knowledge is hidden in language representations. Basically, it checks"
    },
    {
        "casual_text": "In our algorithm, the prediction tree T_pr is only needed when we're verifying stuff. So, during substitution and adjunction, we just set it to nil and don't use it.",
        "formal_text": "Convert casual text to formal text: In our algorithm, the prediction tree T_pr is only needed when we're verifying stuff. So, during substitution and adjunction we just set it to nil and don'"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. You can create different versions of a translated sentence using just the words from the original translation. For example, if you take a sentence and shuffle the words around, you might still get a similar score using something called the Bleu score. In the example they gave, there are at least 40,320 different ways to rearrange the words and still get a similar Bleu score. The number of ways you can rearrange the words depends on how long the sentence is and how many pairs of words (bigrams) don't match up. So, if a translation is almost exactly the same as one of the reference translations, there are fewer ways to rearrange it without changing the score much. Basically, as the translation gets better, the number of random changes you can make without messing up the score goes down. But right now, the Bleu score allows for way too much variation. Even if the translation is pretty good, there are still tons of ways to rearrange the words and get the same score. In Figure 1, they show a bunch of translations made by the second-best Bleu system from the 2005 NIST MT Evaluation. Some of these translations have more than 1073 possible ways to rearrange the words and still get the same score. That's a crazy big number!",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. You can create different versions of a translated sentence using just the words from the original translation. For example, if you take"
    },
    {
        "casual_text": "When we include that one weird entry, Figures 2 and 3 show the average human score for each of the seven NIST entries compared to their Bleu score. What stands out is that one entry got a way higher human score than you'd expect based on its low Bleu score. This entry was different because it wasn't a fully automatic machine translation. Instead, it had some help from English speakers who picked the best options from different automatic translations of Arabic sentences and then edited the result (Callison-Burch, 2005). The other six entries were all fully automatic, using phrase-based statistical machine translation. They were trained on the same data and most of them used a Bleu-based method (Och, 2003) to tweak the settings of their translation models (Och and Ney, 2002).",
        "formal_text": "Convert casual text to formal text: When we include that one weird entry, Figures 2 and 3 show the average human score for each of the seven NIST entries compared to their Bleu score. What stands out is that one entry"
    },
    {
        "casual_text": "(a) A single, neat model that works across different sets of semantic graphs (check out section 2), (b) consistent ways to represent and score stuff (sections 4 and 6), (c) comparing how different methods perform (section 5), and (d) more mixing and matching of different parsing techniques (section 7).",
        "formal_text": "Convert casual text to formal text: (a) A single, neat model that works across different sets of semantic graphs (check out section 2), (b) consistent ways to represent and score stuff (sections 4 and 6), ("
    },
    {
        "casual_text": "In 2020, they suggested trying out knowledge-grounded conversations in both low-resource and zero-resource situations. We didn't compare our work with Lin et al. (2020) and Zhao et al. (2020b) because they didn't share their full source code.",
        "formal_text": "Convert casual text to formal text: In 2020, they suggested trying out knowledge-grounded conversations in both low-resource and zero-resource situations. We didn't compare our work with Lin et al. (2020"
    },
    {
        "casual_text": "They used two language models to find keyphrases, comparing them with a set of target documents and a background corpus. They picked pointwise KL divergence to figure out the difference between the two models.",
        "formal_text": "Convert casual text to formal text: They used two language models to find keyphrases, comparing them with a set of target documents and a background corpus. They picked pointwise KL divergence to figure out the"
    },
    {
        "casual_text": "Comforting alone won't fix things... Let me help him/her do something about it and get through this tough time. (Reflection of Feelings) Yeah, it’s really frustrating and stressful.",
        "formal_text": "Convert casual text to formal text: Comforting alone won't fix things... Let me help him/her help him/her do something/her and get through this tough time. (Reflection of Feelings) Yeah,"
    },
    {
        "casual_text": "Alright, here's the deal: grab two eggs and separate them. Then, with a fork, gently crack them into whatever you're working with.",
        "formal_text": "Convert casual text to formal text: Alright, here's the deal: grab two eggs and separate them. Then, with a fork, gently crack them into whatever you're working with. Grab two eggs and separate"
    },
    {
        "casual_text": "The tagger can basically say, \"If something doesn’t match the noun on its right, it’s probably not part of the same noun phrase, so it must be a pronoun.\" But a JUDPPDU FKHFNHU can’t just assume anything is right—it can’t rely on the noun and determiner agreeing with each other. Instead, it needs to spot when something’s missing or wrong and point it out. So, this new, more relaxed tagger allows for more uncertainty. Instead of being super strict, the NGC (New Grammar Checker) has specific rules to catch errors. For example, Rule (03) (out of 700 rules) checks for gender mismatches between a determiner and the noun that follows (like Rule 04). Lingsoft gave us a guideline that says we can have up to 30% \"false alarms\" (meaning 70% of the alarms should actually be real errors). The NGC does pretty well—it has a precision of 75% (Hagen, Johannessen, and Lane, 2001), compared to 70% for the SGC (Birn, 2000). We haven’t calculated the recall rate for the NGC yet, though.",
        "formal_text": "Convert casual text to formal text: The tagger can basically say, \"If something doesn’t match the noun on its right, it’s probably not part of the same noun phrase, so it must be a pro"
    },
    {
        "casual_text": "We keep the rank 2 the same for all the diagonal blocks in the attention matrix, so the rank-2 approximation for the ij-th block in the original attention matrix A at level-l stays consistent.",
        "formal_text": "Convert casual text to formal text: We keep the rank 2 the same for all the diagonal blocks in the attention matrix, so the rank-2 approximation for the ij-th block in the original attention matrix A at level"
    },
    {
        "casual_text": "As an alternative to the second jump rules, we're testing out a method based on (Huck et al., 2011) that limits reorderings to a pretty strict level. In this version, you can only jump over one continuous chunk of a sentence at a time.",
        "formal_text": "Convert casual text to formal text: As an alternative to the second jump rules, we're testing out a method based on (Huck et al., 2011) that limits reorderings to a"
    },
    {
        "casual_text": "In this project, we introduce DISCO, a self-supervised contrastive learning approach that helps in understanding general code patterns and also pinpoints specific traits for detecting vulnerabilities and code clones. Our tests show that even when DISCO is trained on smaller datasets, it can still beat the performance of bigger models, which proves how effective our design is. Just to clarify, these two tokens are keywords, but by looking at their parent node type, we can quickly tell they're part of the same if-statement and are siblings in the AST.",
        "formal_text": "Convert casual text to formal text: In this project, we introduce DISCO, a self-supervised contrastive learning approach that helps in understanding general code patterns and also pinpoints specific traits for detecting vulnerabilities and code clones."
    },
    {
        "casual_text": "We looked at a bunch of other metrics during BOLT, like NDCG and R-precision, and the results were pretty much the same. We set k to 20 for our evaluation, but we checked and found that changing k doesn’t really affect the overall conclusions.",
        "formal_text": "Convert casual text to formal text: We looked at a bunch of other metrics during BOLT, like NDCG and R-precision, and the results were pretty much the same. We set k to 20 for our evaluation"
    },
    {
        "casual_text": "Inverse selectional preferences are basically about how subjects or objects (like the \"doer\" or \"thing\" in a sentence) tend to pick certain verbs to go with them. It's like saying, \"Hey, I work better with these verbs than others.\" These preferences help us understand patterns in language, like how there are usually more verbs that an agent (like a person or thing acting) can do compared to how many agents can actually do a specific verb. This idea was talked about by Erk and her team back in 2010.",
        "formal_text": "Convert casual text to formal text: Inverse selectional preferences are basically about how subjects or objects (like the \"doer\" or \"thing\" in a sentence) tend to pick certain verbs to go with them. It"
    },
    {
        "casual_text": "Lastly, some other researchers have used probabilistic methods for text segmentation, like hidden Markov models (Yamron et al., 1999) and (Blei and Moreno, 2001). Beeferman (Beeferman et al., 1997) also figured out how to calculate the probability of where segments should split by looking at word usage stats, cue words, and a few other things.",
        "formal_text": "Convert casual text to formal text: Lastly, some other researchers have used probabilistic methods for text segmentation, like hidden Markov models (Yamron et al., 1999) and (Blei and Moren"
    },
    {
        "casual_text": "In this paper, we looked at how well BERT follows the transitivity rule for the IS-A relationship, using a simple setup to test it. We found that while BERT can predict IS-A relationships to some degree, it doesn’t always make predictions that make logical sense. It would be cool to explore in the future how we can get BERT, or other neural network models, to stick to the transitivity rule for IS-A relationships. On top of IS-A, there are other transitive relationships like \"after,\" \"before,\" \"larger than,\" \"smaller than,\" and so on. It would also be interesting to see how well BERT handles these other transitive relationships in future research.",
        "formal_text": "Convert casual text to formal text: In this paper, we looked at how well BERT follows the transitivity rule for the IS-A relationship, using a simple setup to test it. We found that while BERT can predict IS"
    },
    {
        "casual_text": "In this setup, we’ve got two labels: (i) NT means it’s a constituent, and (ii)  means it’s not a constituent. The  label lets the parser work with non-binary trees—more on that in Kitaev and Klein (2018). Most unsupervised parsing models out there don’t use nonterminal categories when working with the development set, so we’re suggesting we train these unlabeled constituency parsing models as a baseline.",
        "formal_text": "Convert casual text to formal text: In this setup, we’ve got two labels: (i) NT means it’s a constituent, and (ii)  means it’s not a constituent. The"
    },
    {
        "casual_text": "Okay, so this symbol [ ] is followed by a number and is used to wrap up a group of morphemes. This wrapped-up part is like a building block in a sentence, like a noun phrase or a clause. The only thing we keep from this wrapped-up part is the part of speech of the last morpheme, and everything else gets swapped out for a variable. In Chinese, the last word in a group usually shows the main grammatical role, so we focus on that. When the same number appears in both the original sentence and the rephrased version, those parts get replaced with the same variable.",
        "formal_text": "Convert casual text to formal text: Okay, so this symbol [ ] is followed by a number and is used to wrap up a group of morphemes. This wrapped-up part is like a building block in"
    },
    {
        "casual_text": "When we tweak our task, we mess with the text by either deleting stuff or shuffling it around. In Table 6, we tested different ways: replacing words with masks instead of deleting them, or just deleting words without shuffling. Turns out, our delete-and-shuffle method gives us the best BERT-iBLEU score compared to the other two options.",
        "formal_text": "Convert casual text to formal text: When we tweak our task, we mess with the text by either deleting stuff or shuffling it around. In Table 6, we tested different ways: replacing words with masks instead of deleting them"
    },
    {
        "casual_text": "Alright, let me show you an example of a messed-up auxiliary verb that the French-B1 model fixes. In French, the phrase for \"go shopping\" is \"faire des achats,\" and the verb \"faire\" basically means \"make\" or \"do.\"",
        "formal_text": "Convert casual text to formal text: Alright, let me show you an example of a messed-up auxiliary verb that the French-B1 model fixes. In French, the phrase for \"go shopping\" is \""
    },
    {
        "casual_text": "Lin and the gang (2019) shared BLEU scores for translating 54 different languages into English. We're looking at how these BLEU scores connect with different ways of measuring how far apart languages are.",
        "formal_text": "Convert casual text to formal text: Lin and the gang (2019) shared BLEU scores for translating 54 different languages into English. We're looking at how these BLEU scores connect with different ways measuring how far apart languages"
    },
    {
        "casual_text": "For every example that has an input x, a gold label y, and an explanation e, we figure out how well a model m aligns with the importance by averaging the Fisher-transformed Pearson correlation (r) between the model's importance scores and the oracle's importance scores for that example. The oracle we use could be the hard (H), soft (S), or expert (E) oracle.",
        "formal_text": "Convert casual text to formal text: For every example that has an input x, a gold label y, and an explanation e, we figure out how well a model m aligns with the importance by a"
    },
    {
        "casual_text": "For French to English translation, we got a 1.0% boost in BLEU scores on both the out-of-domain and in-domain test sets compared to the non-factored baseline. When we used a lexicalized reordering model, the increase was 0.4% and 0.3% for the out-of-domain and in-domain sets, respectively.",
        "formal_text": "Convert casual text to formal text: For French to English translation, we got a 1.0% boost in BLEU scores on both the out-of-domain and in-domain test sets compared to the non-factored"
    },
    {
        "casual_text": "Plus, the results show that bootstrapping works really well for expanding pairs of related items. Like, check out Table 6—it’s spot-on for matching NBA team names. It’s almost perfect for pairing U.S. states/territories with their governors and Taiwanese cities with their mayors too. When it comes to matching acronyms and full names of federal agencies, it’s almost flawless, with 97% precision in the top 100 results. And it does a solid job with car makers and the countries they’re from, hitting 90% precision. For the last two datasets, we think the MAP score could get better if we just ran more bootstrapping rounds. Table 7 gives you a peek at some example wrappers and the instances they pulled out for type 1 wrappers.",
        "formal_text": "Convert casual text to formal text: Plus, the results show that bootstrapping works really well for expanding pairs of related items. Like, check out Table 6—it’s spot-on for matching NBA team names. It’s"
    },
    {
        "casual_text": "Alright, so here's the deal: we're using this frozen autoencoder (shown in gray) to create a mapping  (which is trained and shown in green). This mapping takes the autoencoder's input embedding z x and turns it into the embedding z y for the output sentence y. During training, we have two main goals: L task makes sure the predicted output embedding is close to the real one, and L adv is an extra bit that ensures the output embedding fits nicely within the autoencoder's space. When we're actually using this system (inference time),  works between the autoencoder's encoder and decoder to change the input sentence x into the output sentence . Oh, and there's also an unsupervised version where we only have x sequences for training, not y ones (check out Section 9 for that).",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal: we're using this frozen autoencoder (shown in gray) to create a mapping  (which is trained and shown in green"
    },
    {
        "casual_text": "For every original sentence, we might have come up with a few different ways to say it. To make sure we don’t accidentally let information from one part sneak into another, we split AP H into a training and testing group. This means all the paraphrased versions of a single sentence will either be in the training set or the testing set, but not both. Just a heads-up, AP H isn’t evenly split, as you can see in Table 2. Table 4 breaks down how many M I and non-M I pairs are in the training and testing parts of AP H. And if you check out the 'M I attempts' and 'non-M I attempts' columns in Table 2, you’ll see similar info for other datasets we worked with. When it came to testing, we used AP H whenever AP H -train wasn’t part of the training data, and AP H -test in all other cases.",
        "formal_text": "Convert casual text to formal text: For every original sentence, we might have come up with a few different ways to say it. To make sure we don’t accidentally let information from one part sneak into another, we split AP"
    },
    {
        "casual_text": "Deep learning is now the go-to method for tackling most Natural Language Processing (NLP) tasks, like text classification. When you have enough high-quality training data, these models can work amazingly well. But in real-life situations, perfect datasets are pretty rare. Usually, the data you get is small, packed with regular but useless words, and might have some hidden biases (Wiegand et al., 2019; Gururangan et al., 2018). This can result in models that aren't as good as they could be and might have some unwanted traits. For instance, they might be biased against certain groups or not perform well in actual use because they've just memorized the flawed training data.",
        "formal_text": "Convert casual text to formal text: Deep learning is now the go-to method for tackling most Natural Language Processing (NLP) tasks, like text classification. When you have enough high-quality training data, these models can work"
    },
    {
        "casual_text": "Alright, so, table wrappers don't always bring in tables. To figure out what they're actually doing, we use two filtering rules to sort things out:",
        "formal_text": "Convert casual text to formal text: Alright, so, table wrappers don't always bring in tables. To figure out what they're actually doing, we use two filtering rules to sort things out: filtering rules to"
    },
    {
        "casual_text": "CISP is all about learning a way to turn natural language (NL) sentences into machine-readable representations (MRs). These MRs can then be used in programming environments, like databases or knowledge graphs, to get a result, called a denotation. The structure of an MR can be a tree or a graph, depending on the formal language it's based on. There are three main types of formal languages for MRs: logic-based (like first-order logic), graph-based (like AMR), and programming languages (like Java or Python). Some semantic parsers use a production grammar to create MRs from NL sentences. This grammar has a set of rules that help figure out possible ways to translate each NL sentence into a valid MR.",
        "formal_text": "Convert casual text to formal text: CISP is all about learning a way to turn natural language (NL) sentences into machine-readable representations (MRs). These MRs can then be used in programming environments, like"
    },
    {
        "casual_text": "Like we mentioned earlier, we can only say if the treatments in different studies are similar or the same if the trial was designed to check for non-inferiority or equivalence. So, we use two algorithms to spot this kind of spin.",
        "formal_text": "Convert casual text to formal text: Like we mentioned earlier, we can only say if the treatments in different studies are similar or the same if the trial was designed to check for non-inferiority or equi"
    },
    {
        "casual_text": "We use SABLE format (which is an XML standard for speech synthesis markup) to code the utterances and give them the right prosody.",
        "formal_text": "Convert casual text to formal text: We use SABLE format (which is XML standard for speech synthesis markup) to code the utterances and give them the right prosody."
    },
    {
        "casual_text": "Another cool thing about our CSV classifier is that it can group documents using CSVs without needing fancy techniques like PCA for reducing dimensions. Figures 9 and 10 show how it clusters documents based on what the Transformer model learned for IMDB and AGnews datasets. These clusters help us understand how our classifier works, giving us a big-picture view of its predictions. We also highlight how important it is to use pairwise Euclidean distance in our classification process by grouping sentences into their predicted categories. When it comes to parameter reduction, we compared the number of parameters our nearest neighbor classifier uses versus a black-box approach, using HealthLink data as shown in Table 4. Our compressed classifier uses fewer parameters than the black-box one. Our model only relies on embeddings and CSVs, and we ditch the rest of the layers. The number of parameters in our proposed classifier stays the same across all architectures because the size of the embedding layer and CSVs is consistent for each black-box setup. Plus, our model slashes the inference time from 0.037 to 0.085 seconds down to just 0.007 seconds, as you can see in Table 4. We also compared the inference time to show how much faster it is.",
        "formal_text": "Convert casual text to formal text: Another cool thing about our CSV classifier is that it can group documents using CSVs without needing fancy techniques like PCA for reducing dimensions. Figures 9 and 10 show how it cluster"
    },
    {
        "casual_text": "The folks working on Natural Language Understanding (NLU) keep saying that current Machine Translation (MT) systems just translate text without actually understanding it. Meanwhile, the MT researchers argue that the NLU crowd has been building 'prototype' systems that only work in very limited areas and can't handle the variety of linguistic stuff you find in other fields. But, let's be real, dealing with 'understanding texts' is something we can't ignore if we want to make really good translation systems in the future. In fact, there are a few experimental systems out there, like Carbonel, Lytinen, Ishizaki, and Nomura, that are trying to translate text by actually understanding it.",
        "formal_text": "Convert casual text to formal text: The folks working on Natural Language Understanding (NLU) keep saying that current Machine Translation (MT) systems just translate text without actually understanding it. Meanwhile, the MT researchers argue that the NLU crowd"
    },
    {
        "casual_text": "[LC92] talks about our formal belief model and how we show belief strengths in recipes and our belief model. The main thing here is that Dr. Smith is teaching CS360, which is the last condition we need to check. So, since we have evidence for all the conditions needed for the Express-Doubt action, and because our focusing rules say this action makes sense in the conversation (even if it's not the best option), (14a) is seen as someone expressing doubt about S2's answer, which is that Dr. Smith is teaching CS360.",
        "formal_text": "Convert casual text to formal text: [LC92] talks about our formal belief model and how we show belief strengths in recipes and our belief model. The main thing here is that Dr. Smith is teaching CS360, which is the"
    },
    {
        "casual_text": "In this paper, we focus on paraphrasing between sentences and don’t really get into how our work could be applied to paraphrasing longer pieces of text. 2 The symbols and notations we use in the paper are all listed in Table 1.",
        "formal_text": "Convert casual text to formal text: In this paper, we focus on paraphrasing between sentences and don’t really get into how our work could be applied to paraphrasing longer pieces of text. 2 The symbols and not"
    },
    {
        "casual_text": "Looking at Figure 2, we can see that both MRR and Hit@10 get better and kind of level off around d_s = 20. We did the same thing with the WN18RR dataset and found that the best subembedding dimension there is 4. Check out Table 4 for H@10 results from the FB15-237 validation set, broken down by categories like 1-to-N, N-to-1, and N-to-N.",
        "formal_text": "Convert casual text to formal text: Looking at Figure 2, we can see that both MRR and Hit@10 get better and kind of level off around d_s = 20. We did the same thing with the WN18RR"
    },
    {
        "casual_text": "This approach basically removes collocates that probably aren’t related to the specific meaning of the target word. We’ll call this the hybrid-sense-filtered-counts method (or hybrid-filt for short). Here’s an example of a hybrid-filtered DPWS for the word \"bank\" in the \"financial institution\" sense: DPWS(bank \"fin. inst. \": money, 100); boat, 80; bond, 70; . . .",
        "formal_text": "Convert casual text to formal text: This approach basically removes collocates that probably aren’t related to the specific meaning of the target word. We’ll call this the hybrid-sense-filtered-counts method"
    },
    {
        "casual_text": "Style strength is all about figuring out how formal or informal a sentence sounds. Usually, folks use binary classifiers, like TextCNN (Chen, 2015), to decide if a sentence is formal or not (Lai et al., 2021). Taking that idea, we trained a TextCNN formality classifier on a parallel training dataset (like GYAFC) to tell the difference between informal and formal sentences. For an unlabeled informal sentence *u* and its pseudo target sentence **, we only keep the pair (*c(u)*, **) for unsupervised training.",
        "formal_text": "Convert casual text to formal text: Style strength is all about figuring out how formal or informal a sentence sounds. Usually, folks use binary classifiers, like TextCNN (Chen, 2015), to decide if"
    },
    {
        "casual_text": "When talking about searching through spoken documents, Siegler (1999) talked about how many times we expect certain words to show up and came up with a method to guess those numbers using something called lattices, which are based on how likely different word options are. Chelba and Acero (2005) took it a step further and used a more direct formula to calculate word counts by adding up probabilities from the edges in those lattices. Saraclar and Sproat (2004) looked for specific words in speech lattices, focusing on ones that were expected to appear more than a certain number of times. Meanwhile, Yu and his team (2005) did something similar but for phrases, using a measure called expected word relevance to search through spoken documents.",
        "formal_text": "Convert casual text to formal text: When talking about searching through spoken documents, Siegler (1999) talked about how many times we expect certain words to show up and came up with a method to guess those numbers using something called lat"
    },
    {
        "casual_text": "Most methods like TTOS, GLMP, BossNet, and Mem2Seq don’t do as well on the Nav. sub-domain when it comes to entity F1 scores compared to the Sch. and Wea. sub-domains. This could be because the knowledge base (KB) for Nav. is way more complex than the ones for the other two. Specifically, each entry in the Nav. KB has way more complex attributes. Plus, dialogues in Nav. often require multi-step reasoning, while Sch. and Wea. usually just need single-step reasoning. For instance, if someone asks, \"Give me directions to the nearest grocery store,\" the system has to first figure out the type of place (grocery store) and then find the closest one based on distance. In the future, we’re planning to come up with better techniques to combine KB knowledge and improve multi-step reasoning for TTOS. For a case study, we picked a cool example from the CamRest test set to see how our model and others perform. We put some of the responses from TTOS and the other methods in Table 3. We noticed that Mem2Seq kinda misses the point of the conversation and gives random answers. GLMP does better and gives more readable responses, but it messes up when it comes to pulling the right info from the KB. Also, GLMP’s performance really drops off as the conversation gets longer.",
        "formal_text": "Convert casual text to formal text: Most methods like TTOS, GLMP, BossNet, and Mem2Seq don’t do as well on the Nav. sub-domain when it comes to"
    },
    {
        "casual_text": "Trying to create a single, all-encompassing taxonomy that covers multiple perspectives just doesn’t work because the way these perspectives interact is super complicated. Plus, people tend to struggle with making a neat and tidy taxonomy when they’re dealing with terms from different angles. This problem, along with some other big ideas about how to organize things, is talked about in (Hovy, 2002).",
        "formal_text": "Convert casual text to formal text: Trying to create a single, all-encompassing taxonomy that covers multiple perspectives just doesn’t work because the way these perspectives interact is super complicated. Plus, people tend to struggle with"
    },
    {
        "casual_text": "Our syntax-based language models didn’t do as well as our ngram baselines, which kind of shows that using unadapted parsers with a weak translation model and our search approach isn’t really working for the machine translation task in Table 1 (BLEU-4 scores). The number after the slash tells you how many human references were used to calculate the BLEU score. We didn’t do any post-processing on the MT output, and the parser score weight was set to go no lower than 0.1, which is why the parser + bigram models ended up scoring just a tiny bit lower than the bigram model by itself. Still, digging into the pros and cons of these parsers could give us some good insights for building better language models for machine translation.",
        "formal_text": "Convert casual text to formal text: Our syntax-based language models didn’t do as well as our ngram baselines, which kind of shows that using unadapted parsers with a weak translation model and our search approach"
    },
    {
        "casual_text": "Okay, let’s break this down in a simpler way: 1. We added the Chinese words found using Strategy 2 to the system dictionary and trained the Chinese segmenter using the short unit training data, which was set up in Section 3.3. 2. Table 5 shows how well the Chinese-to-Japanese translation worked using the NICT Chinese Treebank. The short unit method performed the best. The Chinese words we found also boosted the BLEU scores a lot. Strategy 2 did better than Strategy 1, except for test sets 2 and 5. We think this is because Strategy 2 found more words, which helped with the unknown word problem. 3. Table 6 shows the results for Chinese-to-Japanese translation using CTB 7. Strategy 2 got better BLEU scores than the baseline, but the improvement wasn’t as big compared to Strategy 1. We looked into why this happened and found that many of the words found in the parallel training corpus and the annotated training data were the same. For example, \"((protein)\" came from the annotated data and overlapped with \"(protein)\" and \"((quality)\" from the parallel corpus. When the Chinese segmenter tried to break down \"((protein)\", this overlap caused inconsistent results. 4. Table 6: Results of Chinese-to-Japanese translation experiments using CTB 7.",
        "formal_text": "Convert casual text to formal text: Okay, let’s break this down in a simpler way: 1. We added the Chinese words found using Strategy 2 to the system dictionary and trained the Chinese segmenter using the short unit training data,"
    },
    {
        "casual_text": "In Table 5, you can see how much data we filtered out for each neighborhood choice. The global approach ended up losing about twice as much data as the other two methods. The 1k batch neighborhood did just as well as the more detailed document-level neighborhood but made things run way faster—like, more than ten times faster. After digging deeper, we noticed that over 98.5% of the pairs we filtered out in the document-level approach were still there in the batched approach. So, for the rest of our experiments, we stuck with 'Batch Filtering' as our go-to method. We also used batch filtering on each sentence-aligned corpus in section 4.1 to clean them up a bit. Table 4 shows what our training corpus looked like after all the filtering.",
        "formal_text": "Convert casual text to formal text: In Table 5, you can see how much data we filtered out for each neighborhood choice. The global approach ended up losing about twice as much data as the other two methods. The 1k batch neighborhood did"
    },
    {
        "casual_text": "Table 2 gives us the results of how well we could tell apart the names of candidates, party leaders, and key party members. The success rate for our method was 70%, while the basic method only got 50%. Cool thing is, we nailed the bipolarization for the heads of the DPP and KMT—Tsai Ing-wen and Ma Ying-jeou. During the campaign, these leaders kept boosting their party's candidates, so their names popped up together a lot. That made it easier to tell them apart. But we did goof up on two candidates who were super competitive. Take Kun-cheng Lai and Li-chen Kuang, for example. They were fighting hard for the same spot and kept talking about each other, so their names showed up together a bunch. Our method thought they were connected and wrongly put Kun-cheng Lai with the KMT. Check out Table 2 for the full results of our election dataset (with  = 0.7). And in Figure 5, you can see how getting rid of off-topic stuff really helped with bipolarization. The weighted correlation thingy helped a little, but not much.",
        "formal_text": "Convert casual text to formal text: Table 2 gives us the results of how well we could tell apart the names of candidates, party leaders, and key party members. The success rate for our method was 70%, while the basic method only got"
    },
    {
        "casual_text": "To check if coreference-based embeddings actually capture the semantic stuff related to coreference better, we divided our coreference data into two chunks—about 85% and 15%. We trained the embeddings on the bigger chunk and then calculated the cosine similarity for each pair of words that were in the same coreference chain in the smaller chunk. We did this for both text-based and coreference-based embeddings. The numbers in Table 2 show that the coreference-based vectors have higher similarity within the chains compared to the text-based ones.",
        "formal_text": "Convert casual text to formal text: To check if coreference-based embeddings actually capture the semantic stuff related to coreference better, we divided our coreference data into two chunks—about 85% and 15%. We"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: **Greedy Algorithm for Weighted Set-Cover** **Input:** A graph G = (W, D, E) 1. **N:** The total number of documents we need to cover. 2. **Output:** A set of phrases that help distinguish between different topics. 3. **W = w1, w2, ..., wn:** All the possible phrases we can choose from. 4. **W chosen = :** We start with no phrases selected. 5. **num docs covered = 0:** We haven't covered any documents yet. 6. **while num docs covered  N:** Keep going until we've covered all the documents. 7.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: **Greedy Algorithm for Weighted Set-Cover** **Input:** A graph G"
    },
    {
        "casual_text": "We’ve set up NT with a classifier that tells the difference between real (positive) and fake (negative) full forms, using all the feature functions from the original paper. The original paper had some rules for coming up with possible full-form options, but we swapped that out for a function called C(x, y). This way, both the classifier and our alignment model get the same list of full-form candidates to work with. The NT system’s classifier is built using LIB-SVM, which uses something called Radial Basis Function. Here’s how it performed on our data.",
        "formal_text": "Convert casual text to formal text: We’ve set up NT with a classifier that tells the difference between real (positive) and fake (negative) full forms, using all the feature functions from the original paper. The"
    },
    {
        "casual_text": "The second way to think about how similar two words are is based on the idea that if you see both words in the same kind of situation, that situation can give you clues about how the things they represent are connected. For instance, you might often see \"bear\" and \"forest\" together in sentences like \"w1 lives in w2\" or \"in the w2, there's a w1,\" which makes you think there's a connection like \"LOCATED IN\" or \"LIVES IN\" between bears and forests. If \"fish\" and \"reef\" show up in similar kinds of sentences as \"bear\" and \"forest,\" it suggests that fish and reefs have a similar kind of relationship. So, the idea here is that if two pairs of words (like \"bear\" and \"forest\" vs. \"fish\" and \"reef\") appear in similar situations, they probably have similar meanings.",
        "formal_text": "Convert casual text to formal text: The second way to think about how similar two words are is based on the idea that if you see both words in the same kind of situation, that situation can give you clues about how the"
    },
    {
        "casual_text": "Okay, so we have t = t1 t2 ... tn, and we also have the acoustic probabilities like Pr(o1|t1), Pr(o2|t2), and so on, up to Pr(on|tn).",
        "formal_text": "Convert casual text to formal text: Okay, so we have t = t1 t2 ... tn, and we also have the acoustic probabilities like Pr(o1|t1),"
    },
    {
        "casual_text": "So, we're dealing with a sequence _s that's sampled from our model, and r is a baseline reward. We're using this self-critical training approach (shoutout to Rennie et al., 2017), where we get the baseline reward r by applying the same reward function r to a sequence _g that's decoded greedily. Basically, r = r(_g). From what we've seen in practice, using this self-critical baseline reward really helps keep the training of our summarization model stable.",
        "formal_text": "Convert casual text to formal text: So, we're dealing with a sequence _s that's sampled from our model, and r is a baseline reward. We're using this self-critical training"
    },
    {
        "casual_text": "In Table 11, we tested what happens when we bump up the learning rate to 5e-5 for RIPPLES on the SST-2 dataset during fine-tuning. Turns out, cranking up the pre-training learning rate doesn’t really help—it messes with performance on clean data and doesn’t give much of a boost to poisoning performance either. The only weird exception is the IMDb dataset, where the loss goes haywire, and the model’s performance on clean data tanks to random guessing levels.",
        "formal_text": "Convert casual text to formal text: In Table 11, we tested what happens when we bump up the learning rate to 5e-5 for RIPPLES on the SST-2 dataset during fine-tuning. Turns out, crank"
    },
    {
        "casual_text": "Dangling phrases are common in longer texts, where prepositional phrases, infinitives, and other extra bits of info often \"stick to\" the ends of sentences. Usually, these phrases can be connected to different parts of the sentence. The skimming algorithm works by focusing more on the meaning of these phrases rather than just their grammar. Since a lot of these phrases give details about time, place, or other stuff related to events, paying attention to their meaning seems to work better than just following the rules of grammar.",
        "formal_text": "Convert casual text to formal text: Dangling phrases are common in longer texts, where prepositional phrases, infinitives, and other extra bits of info often \"stick to\" the ends of sentences. Usually, these"
    },
    {
        "casual_text": "Any words or bits of words that aren't in the dictionary get labeled as UNK, which stands for UNKnown. After breaking down the words into smaller parts during the morphological analysis, these parts are organized into a chart. This chart is then used for further processing by the (morpho)syntactic rules.",
        "formal_text": "Convert casual text to formal text: Any words or bits of words that aren't in the dictionary get labeled as UNK, which stands for UNKnown. After breaking down the words into smaller parts during the morphological"
    },
    {
        "casual_text": "Let's take a closer look at the first condition of exposure bias, but in a more detailed and unbiased way. We're aiming to figure out",
        "formal_text": "Convert casual text to formal text: Let's take a closer at the first condition of exposure bias, but in a more detailed and unbiased way. We're aiming to figure out how to figure out how to figure"
    },
    {
        "casual_text": "Sure! Here's a more casual version: We pick entity-related features by assuming that if two mentions, A and B, frequently show up together in the same passage, they’re probably from the same category. Before building the feature sets, we can toss in some of A’s local context features into B’s feature set, and do the same for B into A’s. This way, each mention gets a richer set of context info. Here’s how we extract those entity-related features:",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: We pick entity-related features by assuming that if two mentions, A and B, frequently show up together in the same passage, they"
    },
    {
        "casual_text": "In this paper, we did a comparison of two main problems in extractive summarization: ranking and selection. We looked at three different learning-to-rank methods for ranking sentences or concepts: SVR, which is a pointwise ranking algorithm; RankNet, a pairwise learning-to-rank algorithm; and ListNet, a listwise learning-to-rank algorithm. We used an ILP framework to select sentences based on either sentence ranking or concept ranking. We also compared it with other selection methods like MMR and Diversity Penalty. Our experiments were done using the TAC 2008 and TAC 2009 datasets. Our work has two main contributions: First, as far as we know, this is the first time someone has done a systematic and detailed analysis comparing different ranking and selection strategies. Second, this is the first time pairwise and listwise learning-to-rank algorithms have been used for concept (word bigram) ranking in extractive summarization.",
        "formal_text": "Convert casual text to formal text: In this paper, we did a comparison of two main problems in extractive summarization: ranking and selection. We looked at three different learning-to-rank methods for ranking sentences or concepts:"
    },
    {
        "casual_text": "We pulled out a bunch of names using a NER from two datasets, LDC2009T13 and Adige500k. Both datasets have a ton of proper names—160,869 from the English one and 185,508 from the Italian one. It turns out that there are a lot of almost-the-same names, which we think might be misspelled versions of the same name. We show this in Figure 4. The English Cand and Italian Cand are just raw numbers, while English True and Italian True are percentages. For instance, a name with 5 letters probably has about 23 possible misspellings, but only around 17% of those are actual misspellings—the rest are just different names. We got these numbers by looking at samples with sizes between 30 and 50 for each name length. The percentages change a lot depending on how long the name is. For names longer than 11 letters, there's a 98% chance that a misspelling candidate is actually a true misspelling. This makes us think we should start looking for patterns with longer names first and work our way down to shorter ones. The patterns we found using the algorithm in Section 4 show up between 900 and 20 times. There are 12 patterns that appear more than 400 times, and 20 that show up between 20 and 50 times.",
        "formal_text": "Convert casual text to formal text: We pulled out a bunch of names using a NER from two datasets, LDC2009T13 and Adige500k. Both datasets have a ton of proper names—"
    },
    {
        "casual_text": "(3) In Section 3.2, we came up with two ways to combine those four groups of words. In the \"-Weighted pooling\" part, we swapped the weighted pooling method for mean-pooling, and that made things worse. Compared to mean-pooling, the weighting strategy does more than just give different words importance based on how meaningful they are. It also brings in the frequency of each word from the data, which has been shown to be useful.",
        "formal_text": "Convert casual text to formal text: (3) In Section 3.2, we came up with two ways to combine those four groups of words. In the \"-Weighted pooling\" part, we swapped the weighted pool"
    },
    {
        "casual_text": "For the error analysis, we randomly picked 100 wrong predictions. Let’s break down two main types of errors we found: Entity Ambiguity. Even though our entity detection module tags each predicted span with an (entity) type, dealing with entity ambiguity is still the biggest challenge for our system. For example, take the question, \"Who is associated with Jeff Smith?\" Our entity detection module correctly spots \"Jeff Smith\" as an entity and tags it as a \"common name.\" But here’s the catch: the Wikidata knowledge graph has over ten entities with the exact same label and type. This makes it tricky for our entity linking module to figure out the right one. Wikidata entity linking is a relatively new area of research, and it comes with its own set of challenges, like entities having the same labels, non-standard labels created by users, and even multi-word labels (some as long as 62 words) (Mulang et al., 2020b). Adding more context, like entity descriptions and other info from the knowledge graph, could help clear up this ambiguity (Mulang et al., 2020a).",
        "formal_text": "Convert casual text to formal text: For the error analysis, we randomly picked 100 wrong predictions. Let’s break down two main types of errors we found: Entity Ambiguity. Even though our entity detection module tags each predicted span"
    },
    {
        "casual_text": "Sure, here's a simpler way to say that: There's a deal where any company that buys three or more machines gets a discount. Six companies from Switzerland each buy one machine. A company from Germany buys four machines. So, who gets the discount?",
        "formal_text": "Convert casual text to formal text: Sure, here's a simpler way to say that: There's a deal where any company that buys three or more machines gets a discount. Six companies from Switzerland each buy one"
    },
    {
        "casual_text": "This paper suggests creating a stock network using ticker symbols that appear together in tweets. The features of this SSN show some strong connections between the stocks involved, which can be useful for predicting stock movements based on social media sentiment. Our tests show that SSN does a better job than CSN at capturing how stocks are related to each other. Plus, the sentiment and topics from closely connected stocks really help improve stock market predictions. In Table 3, you can see the average and best prediction accuracies (for time periods of 15 to 60) for different scenarios with various factors. The cell for \"dis(0.96)\" means that \"$dis\" has the highest price correlation strength of 0.96 with \"$goog\" (and it's the same for the other entries in the CSN column). The best results are marked in bold.",
        "formal_text": "Convert casual text to formal text: This paper suggests creating a stock network using ticker symbols that appear together in tweets. The features of this SSN show some strong connections between the stocks involved, which can be useful for predicting"
    },
    {
        "casual_text": "In 2016, Arjovsky and team came up with Unitary-Evolution Recurrent Networks to tackle the issue of exploding and vanishing gradients, which happens because of non-linear activation functions. But here's the thing: they still use ReLU activations between time steps, which is different from how URNs work. What we're mostly focused on is the structure of these unitary embeddings. The connection between all this is that if an RNN has trouble with exploding or vanishing gradients, it can't handle long-term dependencies. Arjovsky's embeddings are easier to compute than ours since they can be multiplied in linear time. However, like us, they don't cover the entire space of unitary matrices. Jing et al. (2017) also proposed a representation that's less computationally heavy than ours but has the same number of parameters in the long run. Another approach is to let back-propagation update the unitary matrices freely and then periodically project them back onto the unitary space (Wisdom et al., 2016; Kiani et al., 2022).",
        "formal_text": "Convert casual text to formal text: In 2016, Arjovsky and team came up with Unitary-Evolution Recurrent Networks to tackle the issue of exploding and vanishing gradients, which happens because of non"
    },
    {
        "casual_text": "We used the 6th Transformer layer from the first iteration of the HuBERT BASE model to create target labels. We did this by grouping the outputs into 500 clusters using the k-means clustering method.",
        "formal_text": "Convert casual text to formal text: We used the 6th Transformer layer from the first iteration of the HuBERT BASE model to create target labels. We did this by grouping the outputs into 500 clusters using the"
    },
    {
        "casual_text": "A tool that helps figure out the right word forms and tags by looking at the grammar and context (Karlsson and others 1995, Hagen, Johannessen, and Nklestad 2000a and 2000b).",
        "formal_text": "Convert casual text to formal text: A tool that helps figure out the right word forms and tags by looking at the grammar and context (Karlsson and others 1995, Hagen, Johannessen, and Nklesta"
    },
    {
        "casual_text": "In this project, we're diving into cross-document coreference (CDCR), which naturally involves within-document coreference (WDCR). We've come up with a new model that boosts both coreference accuracy and efficiency. Lately, within-document entity coreference resolution has seen some cool improvements. Sequential prediction—basically making coreference calls from start to finish in a text—has been performing really well (Lee et al., 2017) and uses less computing power. This approach works great in real-world situations where new documents keep coming in daily, as it can easily link up events and entities with previously processed stuff. Most non-sequential models would need to start all over again, which is a pain. In this project, we're showing how this method can be expanded to handle cross-document entity coreference and then tweaked to work with cross-document event coreference too.",
        "formal_text": "Convert casual text to formal text: In this project, we're diving into cross-document coreference (CDCR), which naturally involves within-document coreference (WDCR). We've come up with a new model that"
    },
    {
        "casual_text": "Take it out of the oven and sprinkle some fake bacon bits and potato chips on top. Put it back in the baking dish and bake it without a lid for 30 minutes.",
        "formal_text": "Convert casual text to formal text: Take it out of the oven and sprinkle some fake bacon bits and potato chips on top. Put it back in the baking dish and bake it without a lid for 30 minutes."
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: Basically, we've got this equation: F = e, f D q(D | e, f) log [P  D (D | e, f) / q(D | e, f)] + e, f D q(D | e, f) log P  (e, f) Which can also be written as: F = e, f log P  (e, f) (8)  KL[q(D | e, f) || P  D (D | e, f)] So, it's combining a bunch of terms and using some log stuff, along with something called KL divergence, to express F.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: Basically, we've got this equation: F = e, f D q(D | e,"
    },
    {
        "casual_text": "A function g that takes stuff from R n and spits out stuff in R n is called a \"guarding function\" for X when it comes to Z (and the class H) if the set of all g(x) where x is in X is guarded for Z with respect to H.",
        "formal_text": "Convert casual text to formal text: A function g that takes stuff from R n and spits out stuff in R n is called a \"guarding function\" for X when it comes to Z (and"
    },
    {
        "casual_text": "We ran all the experiments on a Titan X GPU or a Titan RTX GPU when we needed more memory. Once we picked the best set of hyperparameters for each model and each criterion, we let the model train for up to 100 epochs or 96 hours, whichever came first. After each epoch, we checked how well it was doing on the development set, based on each criterion (whether it was fully unsupervised or using just a few labeled examples). The best hyperparameter setups are listed in Table 5.",
        "formal_text": "Convert casual text to formal text: We ran all the experiments on a Titan X GPU or a Titan RTX GPU when we needed more memory. Once we picked the best set of hyperparameters for each model and"
    },
    {
        "casual_text": "Valence and arousal are concepts from a famous emotion theory by Osgood et al. (1957). We only checked how well people agreed on the positive or negative meaning of a complex word. Two people looked at a random selection of 200 words and their agreement was pretty high, with a kappa score of 0.86.",
        "formal_text": "Convert casual text to formal text: Valence and arousal are concepts from a famous emotion theory by Osgood et al. (1957). We only checked how well people agreed on the positive or negative meaning of"
    },
    {
        "casual_text": "We checked the case frames our system made for these nouns against the ones that are considered the best, or the \"gold standard.\" If the system's case frame had the same slots as the gold standard, we called it correct. Table 5 shows the results: the system came up with 70 case frames, and 58 of them matched the gold standard, so they were correct.",
        "formal_text": "Convert casual text to formal text: We checked the case frames our system made for these nouns against the ones that are considered the best, or the \"gold standard.\" If the system's case frame had the same slots as the"
    },
    {
        "casual_text": "Based on the concept of CMCBP, we're introducing a new method called Compositional Approach with Word Embedding Projection (CMWEP). Here's how it works step by step: 1. First, you need two word embedding models—one for the source language and one for the target language—with the same vector size. 2. Next, you learn a transformation matrix using the method we talked about earlier. 3. Then, for each word in a Multiword Expression (MWT), you use a bilingual seed lexicon to translate it. If a word isn't in the dictionary, you get its embedding vector by projecting it using the transformation matrix. If the word is in the lexicon, you average the vectors of all its possible translations. Let's break it down with an example: Say you have an MWT \"ABC\". If \"A\" and \"B\" are in the lexicon with translations like \"a1\", \"a2\", \"a3\" for \"A\" and \"b1\", \"b2\" for \"B\", and \"C\" isn't in the lexicon, here's what happens: - For \"A\" and \"B\", you take the average of their translation vectors from the target language's word embedding system. - For \"C\", since it's not in the lexicon, you use the transformation matrix to get its projected embedding vector. This way, every word in the MWT from the source language ends up as a single vector in the target language's space.",
        "formal_text": "Convert casual text to formal text: Based on the concept of CMCBP, we're introducing a new method called Compositional Approach with Word Embedding Projection (CMWEP). Here's how it"
    },
    {
        "casual_text": "This study could be expanded to other languages, different fields, and more features in translated texts. These features could be analyzed using Natural Language Processing (NLP) and evaluated. For Machine Translation (MT), the findings suggest that non-translated texts are usually simpler than translated ones (check Table 1). So, to improve MT systems, researchers should compare non-translated and translated texts to figure out what makes non-translated texts unique. Then, they can try to make MT outputs match those characteristics. Keep in mind, these features might vary depending on the field, so it’s important to focus on specific genres or registers.",
        "formal_text": "Convert casual text to formal text: This study could be expanded to other languages, different fields, and more features in translated texts. These features could be analyzed using Natural Language Processing (NLP) and evaluated. For Machine Translation (MT"
    },
    {
        "casual_text": "3. Pruning: We should get rid of parts that probably won't help with the final result right from the start.",
        "formal_text": "Convert casual text to formal text: 3. Pruning: We should get rid of parts that probably won't help with the final result right with the final result right. Right now, we have a few things to do. 1."
    },
    {
        "casual_text": "First, let's look at the vector representations used to handle mentions. Earlier research used ELMo and pre-trained GloVE (Pennington et al., 2014) for word and character embeddings, but newer models are using RoBERTa (Cattan et al., 2020; Yu et al., 2020b). We tried swapping out BERT-base for RoBERTa-base and adding GloVE alongside BERT in our models (check out Appendix B for the details), and we saw a big drop in performance. We think the big difference between BERT and RoBERTa might be because BERT was trained with Next Sentence Prediction (NSP), while RoBERTa wasn't. The NSP might make BERT better at understanding whole documents, which is super important for coreference resolution. So, we guess that without specific fine-tuning, adaptive pre-training works best for coreference on ECB+. We also noticed that our entity coreference model is less affected by changes in features compared to the event coreference model. For instance, the event coreference model really depends on argument features.",
        "formal_text": "Convert casual text to formal text: First, let's look at the vector representations used to handle mentions. Earlier research used ELMo and pre-trained GloVE (Pennington et al."
    },
    {
        "casual_text": "where h t1 is the hidden state from the last step, W v stands for the trainable parameters, and",
        "formal_text": ""
    },
    {
        "casual_text": "The Recurrent FGREP module can handle two main tasks: it can take a sequence of inputs and turn them into a single, consistent output, or it can take a fixed input and generate a sequence of outputs. In a network that processes sequential inputs, the input data changes with each step, but the teaching pattern stays the same. The network works on creating a stable representation of the sequence. On the other hand, in a network that produces sequential outputs, the input stays the same, but the teaching pattern changes at each step. Here, the network is interpreting the input in a sequence-like way. After each step, the error is sent back through the network, and the weights are adjusted. Both types of Recurrent FGREP networks end up forming representations in their input layers.",
        "formal_text": "Convert casual text to formal text: The Recurrent FGREP module can handle two main tasks: it can take a sequence of inputs and turn them into a single, consistent output, or it can take a fixed input"
    },
    {
        "casual_text": "At the bottom of the proof frame, you've got the original sequent's categories, complete with labels and polarities. We call these the terminal formulae. Up top, you'll find basic categories, also with labels and polarities, which we refer to as the axiomatic formulae. Oh, and we'll make sure to keep them separate.",
        "formal_text": "Convert casual text to formal text: At the bottom of the proof frame, you've got the original sequent's categories, complete with labels and polarities. We call these the terminal formulae. Up top, you'"
    },
    {
        "casual_text": "APTrans has three main parts: first, a semi-interactive module for analyzing source language (SL) text, second, an automated transfer module, and third, an automated module for generating target language (TL) text. The analysis module, which is the first part, has two main tools: a fully automated tagger and phrase chunker, and a smaller part where you can interactively work on predicate/case-role dependency analysis of the SL text.",
        "formal_text": "Convert casual text to formal text: APTrans has three main parts: first, a semi-interactive module for analyzing source language (SL) text, second, an automated transfer module, and third, an automated module for"
    },
    {
        "casual_text": "We reached out to the relief community to figure out what kinds of translations they needed and if they knew any useful data sources. One key connection we made was with Ushahidi, a group that had set up a system for collecting text messages in Haiti, using the number 4636. Later, Ushahidi handed over the management of 4636 to Crowdflower, who rebranded it as Mission 4636. Crowdflower has been sending us batches of these text messages, both in Creole and English. The Creole messages are translated into English by volunteers from all over the world. We had bilingual speakers go through the messages to clean them up (since SMS messages can be messy, and the translations are done quickly, so the quality can vary). We then added this cleaned-up data to our training set. Table 1 shows some examples of these SMS messages and the different types of noise you might find in them.",
        "formal_text": "Convert casual text to formal text: We reached out to the relief community to figure out what kinds of translations they needed and if they knew any useful data sources. One key connection we made was with Ushahidi, a"
    },
    {
        "casual_text": "In 2018, it was still kinda tricky to figure out if the results we got were actually the best they could be. When we looked at how these tools differed, we thought using lenses would be a good way to dig deeper into finding overlaps and partial matches.",
        "formal_text": "Convert casual text to formal text: In 2018, it was still kinda tricky figure out if the results we got were actually the best they could be. When we looked at how these tools differed, we thought using lenses would"
    },
    {
        "casual_text": "To address this issue, we're introducing a new task called subjectivity word sense disambiguation (SWSD). The goal is to figure out automatically which words in a text are being used in a subjective way and which ones are being used objectively. We think SWSD is more doable than trying to figure out every single word's exact meaning because it’s less detailed—you don’t always need to know the exact sense of a word. Plus, we believe SWSD can help make contextual subjectivity analysis systems work better by using sense-aware classification.",
        "formal_text": "Convert casual text to formal text: To address this issue, we're introducing a new task called subjectivity word sense disambiguation (SWSD). The goal is to figure out automatically which words in a text are being"
    },
    {
        "casual_text": "Okay, so here's the deal: We've got two literal movement grammars, G1 and G2, with their own sets of stuff (like #1, $1, P1 for G1 and #2, $2, P2 for G2). The important thing is that the domains of #1 and #2 don't overlap at all, meaning dom(#1) and dom(#2) don't share any elements. Now, we can make a new grammar, G, by combining the stuff from G1 and G2, plus adding a new rule. So, G = (#1 U #2 U (S, 0), S, P1 U P2 U R), where R is a rule that says \"S, 0\" can turn into \"S, 0 S2()/x\". Basically, G will only recognize sentences that both G1 and G2 would recognize. It's like combining their powers into one super grammar!",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: We've got two literal movement grammars, G1 and G2, with their own sets of stuff (like #1, $1, P1 for G1"
    },
    {
        "casual_text": "We looked at three POS tagging methods: log-linear, CRF, and DNN. All of them were trained on the usual datasets—PTB for English and CTB 9.0 for Chinese. We tested them on their respective test sets to see how they did. For NER (coarse-grained), we wanted to make sure the models would work well in real-world situations, so we mixed a bunch of public datasets for English NER. These include CoNLL2003, BTC, GMB, SEC_FILING, WikiGold, and WNUT17. Since each dataset uses slightly different labels, we simplified things by sticking to just three common ones: Person, Location, and Organization. For Chinese NER, we made our own dataset with around 80,000 sentences labeled with 12 types of entities, following a similar approach to the Ontonotes dataset. We split this into a training set and a test set, with a 3:1 ratio. We tested two NER methods: CRF and DNN. For DNN, we used RoBERTa-CRF and Flair models. We noticed that RoBERTa-CRF worked better for the Chinese dataset, while Flair performed better on the English one. So, in our experiments, we used RoBERTa-CRF for Chinese and Flair for English. Lastly, we did some constituency parsing experiments on both English and Chinese datasets.",
        "formal_text": "Convert casual text to formal text: We looked at three POS tagging methods: log-linear, CRF, and DNN. All of them were trained on the usual datasets—PTB for English and CTB"
    },
    {
        "casual_text": "To speed up training, we group sentences in a document into batches based on the total number of possible meanings (400 for SACE base and SACE mul, 150 for SACE large and SACE large+). Basically, if adding a sentence pushes the total number of possible meanings over 400 or 150, it goes into the next batch. For each batch, we only run the gloss and context encoders once. We also adjust the length of the context and gloss to match the longest sequence in the batch to avoid extra padding and calculations. Plus, we use apex for mixed-precision computing to save time. Check out Appendix A for more info.",
        "formal_text": "Convert casual text to formal text: To speed up training, we group sentences in a document into batches based on the total number of possible meanings (400 for SACE base and SACE mul, 150 for SACE large"
    },
    {
        "casual_text": "You could argue that for an NLI, the grammar of what someone says isn’t really important. But in most human language interactions, there are extra rules beyond just figuring out what someone’s trying to say. There’s no reason why NLIs shouldn’t follow the same pattern. While (13) is understandable, it doesn’t follow typical English grammar rules, and people probably wouldn’t mind if it wasn’t allowed.",
        "formal_text": "Convert casual text to formal text: You could argue that for an NLI, the grammar of what someone says isn’t really important. But in most human language interactions, there are extra rules beyond just figuring out what someone’"
    },
    {
        "casual_text": "Here, p(y * i | X) represents the likelihood of assigning the correct label y * i to the input x i.",
        "formal_text": "Convert casual text to formal text: Here, p(y * i | X) represents the likelihood of assigning the correct label y * i to the input x i. Convert casual text to formal"
    },
    {
        "casual_text": "Most researchers in the field of generation don't really focus on strategies—they just stick to one method to get the job done. But we're more interested in the process and how using a particular grammar can be applied in different ways.",
        "formal_text": "Convert casual text to formal text: Most researchers in the field of generation don't really focus on strategies—they just stick one method to get the job done. But we're more interested in the process and how using a particular"
    },
    {
        "casual_text": "DBPedia (Mendes et al., 2012) can be created automatically by pulling information from web text or other sources, just like how NELL (Carlson et al., 2010) and YAGO (Suchanek et al., 2007) do it. These knowledge bases have tons of real-world entities and the connections between them. But even though they’re huge, they’re still missing a lot—like big gaps in the relationships between common things (West et al., 2014). So, figuring out new connections just by looking at what’s already in these knowledge bases has become a really big deal.",
        "formal_text": "Convert casual text to formal text: DBPedia (Mendes et al., 2012) can be created automatically by pulling information from web text or other sources, just like how NELL (Carlson et"
    },
    {
        "casual_text": "Our QA task is kind of like what Mirsha et al. (2016) did, but instead of asking about the sentiment of a paragraph, we ask random questions. Our multitask method for doing both the QA task and predicting gaze is similar to what Klerke et al. (2016), Berrett et al. (2018), and Mishra et al. (2018) did. Specifically, in Equation 4, we use the same loss term as Barrett et al. (2018), which combines an NLP task loss and a gaze prediction loss. The difference is that Barrett et al. (2018) used gaze predictions as input attention weights for the NLP task, but we just treat gaze as an output. This is kind of like how humans read, where eye movements are just a behavior, not something that affects language processing. Our work is also different from Mishra et al. (2018) because we use a different model and a single auxiliary objective based on gaze. Lastly, Vajjala et al. (2016) collected eye-tracking data from ESL learners for 4 articles from the same OneStopEnglish source we used (Vajjala and Lui, 2018) to study how text difficulty affects fixation measures and reading comprehension. Our work, however, focuses on a different task and a different group of readers.",
        "formal_text": "Convert casual text to formal text: Our QA task is kind of like what Mirsha et al. (2016) did, but instead of asking about the sentiment of a paragraph, we ask random questions. Our multitask method"
    },
    {
        "casual_text": "Alright, so we're looking at how source tokens (let's call them \"source(k)\") end up in the target sequence. The question is: do certain source tokens tend to show up more often in specific spots in the target? To figure this out, we're checking how much each source token position (k) contributes overall to the entire target sequence.",
        "formal_text": "Convert casual text to formal text: Alright, so we're looking at how source tokens (let's call them \"source(k)\") end up in the target sequence. The question is: do certain source tokens"
    },
    {
        "casual_text": "The chance that an N-gram with the target t in spot i and relative r in spot j will have the POS N-gram p is:",
        "formal_text": "Convert casual text to formal text: The chance that an N-gram with the target t in spot i and relative r in spot j will have the POS N-gram p is: The chance that an N"
    },
    {
        "casual_text": "Sure! Let me break it down in a simpler way: For instance, if we have something like  Q, P >, we don’t just need to deal with whole sentences. We also need to handle bits and pieces like \"founder of Microsoft\" or \"founded Microsoft,\" or even stuff like \"who is the founder\" and \"Bill Gates founded.\" Often, this kind of thing can help the model figure out the answer pretty easily. So, we should dig deeper and pull out some extra details from the interaction tensor S. Here’s how we can do it: for i in range(n):",
        "formal_text": "Convert casual text to formal text: Sure! Let me break it down in a simpler way: For instance, if we have something like  Q, P >, we don’t just need to deal with whole sentences."
    },
    {
        "casual_text": "Verifiability (V) basically tells you how easy it is to check if the info in a sentence is true. We added this because we think people are more likely to believe something if they can easily confirm it themselves.",
        "formal_text": "Convert casual text to formal text: Verifiability (V) basically tells you how easy it it check if the info in a sentence is true. We added this because we think people are more likely to believe something"
    },
    {
        "casual_text": "Since EAD assumes everything is perfect and doesn't consider how language is actually used, we wanted to dig deeper into this issue and suggest a more practical version of Expectation-Adjusted Distinct for real-world scenarios. Before using EAD, it's important to look at how the score relates to text length (check out Figure 1) and see how EAD performs on training data. From what we know, if the training data comes from big, open-domain sources like OpenSubtitles or Reddit, EAD works well across different text lengths. So, it can be used directly to evaluate models trained on these datasets. But, we noticed that when we tested it on Twitter data, EAD dropped for longer texts. This might be because Twitter limits the number of words (like 280 characters), so people try to cram as much info as they can into shorter posts. In cases like this, using EAD to evaluate methods that produce longer texts isn't really fair. Li et al. (2016) came up with Distinct, which is calculated by dividing the number of unique words by the total number of words. This metric is meant to measure how diverse a text is and has been used a lot in tasks like dialogue generation (Wu et al., 2021a; Zheng et al., 2019) or story generation (Guan et al., 2021). But, as we showed in Figure 1, it's not a fair measure because it gets influenced by the length of the text.",
        "formal_text": "Convert casual text to formal text: Since EAD assumes everything is perfect and doesn't consider how language is actually used, we wanted to dig deeper into this issue and suggest a more practical version of Expectation-Adjust"
    },
    {
        "casual_text": "The new representations get fed into something called self-matched attention to figure out how words interact over time. Let’s use vector V as an example. The self-matched attention for V, which we can write as V = att(V), works like this:",
        "formal_text": "Convert casual text to formal text: The new representations get fed into something called self-matched attention to figure out how words interact over time. Let’s use vector V as an example. The self-matched attention for V, which"
    },
    {
        "casual_text": "Turning the transliteration decision into an optimization problem lets us easily add other factors to our goal. Specifically, we like matching short words better, so we include that as a tweak to our objective function. To do this, we use a normalization factor. When we look at a pair of words (w s, w t ), we adjust by dividing the weight vector length by the length of the shorter word. Now, our decision model looks like this:",
        "formal_text": "Convert casual text to formal text: Turning the transliteration decision into an optimization problem lets us easily add other factors to our goal. Specifically, we like matching short words better, so we include that as a tweak to our"
    },
    {
        "casual_text": "We looked at how well the BNN model works for predicting suffixes at the part-of-speech (POS) level. Table 5 shows the accuracy of suffix predictions for English-Russian, broken down by POS. For this part, we used Russian data that was tagged by TreeTagger. We also calculated the average number of suffixes per stem for each POS. Our findings match what Chahuneau et al. (2013) found: predicting suffixes for adjectives is trickier than for other POS types, while predicting verbs is relatively easier, even though verbs tend to have more suffixes per stem. These differences highlight how important it is to consider the context—whether it's the source or target language. For example, adjectives need to match the gender of the nouns they describe, but this often has to be figured out from the target context alone.",
        "formal_text": "Convert casual text to formal text: We looked at how well the BNN model works for predicting suffixes at the part-of-speech (POS) level. Table 5 shows the accuracy of suffix predictions for"
    },
    {
        "casual_text": "We compared our SkipBERT with some basic models—specifically, 6-layer and 4-layer models. The results show that SkipBERT beats all the other methods when it comes to the GLUE score. When we look at TinyBERT, since we mostly followed their distillation process, our approach still performs better across all tasks. BERT-EMD takes things a step further by using a more advanced, task-specific distillation method based on TinyBERT, which boosts performance even more. However, SkipBERT still comes out on top when you look at the overall score.",
        "formal_text": "Convert casual text to formal text: We compared our SkipBERT with some basic models—specifically, 6-layer and 4-layer models. The results show that SkipBERT beats all the other methods when it comes to the"
    },
    {
        "casual_text": "(2) Skeleton: To check how well the skeleton part is doing, we use a method similar to how we score sentences in constituent parsing. We treat the DRU nodes as words and compare them to a constituent tree. (3) Tuple: For the DRU performance, we use the F1-score to measure how well the tuple-level matching works. This is because the basic units inside a DRU are tuples, which are like sets of relation-variable functions. We do exact matching here, taking into account the order of the variables.",
        "formal_text": "Convert casual text to formal text: (2) Skeleton: To check how well the skeleton part is doing, we use a method similar to how we score sentences in constituent parsing. We treat the DRU nodes"
    },
    {
        "casual_text": "Basically, if you have a category C[ before a category Oj in a local tree t, they shouldn't mess with the transitive LP-relation (C], and they can't be extensions of two categories C I and Cj where Cj comes before CI. Now, we can explain what makes a tree \"admissible.\"",
        "formal_text": "Convert casual text to formal text: Basically, if you have a category C[ before a category Oj in a local tree t, they shouldn't mess with the transitive LP-relation ("
    },
    {
        "casual_text": "When it comes to how we reward actions in this reinforcement learning setup, each move we make, like choosing an action ai, s, gets a reward r(ai, s, i, s). The goal of RL is to teach the model to rack up as much reward as possible by maximizing the expected total reward J = E(R), where R =  R_s and R_s =  r(ai, s, i, s). Here's how we calculate those rewards. First, let's say #pos is the number of POS (part-of-speech) annotations in the training data, which is also the number of tokens. N is the number of sentences. Then we define r_pos = N / #pos. If the model correctly adds a POS annotation or removes an incorrect one (like when it overwrites something), it gets a reward of r_pos. But if it adds an incorrect POS annotation or removes a correct one, it gets a penalty of -r_pos. The same idea applies to syntactic and semantic dependencies. Syntactic roots and semantic top predicates are treated as virtual syntactic and semantic dependencies, respectively. So, building the full \"gold\" structure (the correct one) gives an average reward of 3 per sentence, spread evenly across the three layers (POS, syntactic, semantic). Finally, the reward for a specific action is the sum of the rewards from its effects, minus a tiny penalty for non-HALT actions (HALT means stopping). This penalty is set at one-tenth of the average reward per token in the training data, to discourage the model from just hanging around without making progress.",
        "formal_text": "Convert casual text to formal text: When it comes to how we reward actions in this reinforcement learning setup, each move we make, like choosing an action ai, s, gets a reward r(ai,"
    },
    {
        "casual_text": "Okay, so Wi represents the weight of noun i. This weight helps determine the importance of the noun. With the article vectors from formula (4), we can calculate how similar two articles, Ai and Aj, are using formula (1). The higher the value of Sim(Ai, Aj), the more similar those two articles are. The clustering algorithm we talked about in Stage Four is applied to each batch of articles, and it groups them into clusters. These clusters are sorted based on how similar they are, with the most similar ones at the top.",
        "formal_text": "Convert casual text to formal text: Okay, so Wi represents the weight of noun i. This weight helps determine the importance of the noun. With the article vectors from formula (4), we can calculate how similar two articles"
    },
    {
        "casual_text": "Comparing how different methods work on individual topics, the citation-context methods did better than all the others in most cases—like, 65% of the time.",
        "formal_text": "Convert casual text to formal text: Comparing how different methods work on individual topics, the citation-context methods did better than all the others—like, 65% of the time. Convert casual text to formal text: Compar"
    },
    {
        "casual_text": "The final client is a big pharmaceutical company, and we're working on translating a huge bunch of e-learning stuff about their internal SAP setup.",
        "formal_text": "Convert casual text to formal text: The final client is a big pharmaceutical company, and we're working on translating a huge bunch of e-learning stuff about their internal SAP setup. Convert casual text to formal text:"
    },
    {
        "casual_text": "In this paper, we're not looking at all proof nets, just a subset of the multiplicative ones—specifically, those from intuitionistic implicative linear logic. Here, sequents have multiple antecedent formulas but only one succedent formula. To handle the intuitionistic aspect with proof nets (since we're dealing with one-sided sequents), we use the idea of polarities: inputs are negative (,) and outputs are positive (o) (Danos, 1990; Lamarche, 1995). This helps us label the formulas—positive ones are succedent formulas, and negative ones are antecedent formulas.",
        "formal_text": "Convert casual text to formal text: In this paper, we're not looking at all proof nets, just a subset of the multiplicative ones—specifically, those from intuitionistic implicative linear logic. Here,"
    },
    {
        "casual_text": "The first step in the process we're talking about is creating a special kind of language representation that fits the topic we're focusing on. This method looks for groups of n-grams—which are just chunks of words—in paragraphs that seem to pop up a lot and connect with other paragraphs. We call these \"useful\" n-grams because they show up often and are spread across different texts in the same area of interest.",
        "formal_text": "Convert casual text to formal text: The first step in the process we're talking about is creating a special kind of language representation that fits the topic we're focusing on. This method looks for groups of n-gram"
    },
    {
        "casual_text": "The factored decoding process can be broken down into several steps to fully translate the input. Basically, this means Equation 4 is split into smaller parts, which are often referred to as translation steps.",
        "formal_text": "Convert casual text to formal text: The factored decoding process can be broken down into several steps to fully translate the input. Basically, this means Equation 4 is split into smaller parts, which are often referred to as"
    },
    {
        "casual_text": "(i) a thing that says whether the sentence is short; (ii) a thing that says whether the sentence is long;",
        "formal_text": "Convert casual text to formal text: (i) a thing that says whether the sentence is short; (ii) a thing that says whether the sentence is long; (iv) a thing that says whether the sentence"
    },
    {
        "casual_text": "We used Dirichlet priors with a symmetric hyperparameter  = 1 for all the algorithms. Early tests showed that the algorithms don’t really care much about hyperparameter settings, so we kept it simple. For batch VB, we ran it until the log probability of the training set changed by less than 0.001%. For stochastic VB, we set  = 0.9,  = 1, and used minibatches of 10,000 sentences. To check for convergence and overfitting, we ran both stochastic and collapsed VB for 15 epochs, shuffling the training corpus each time. For streaming VB, the first minibatch had 10,000 sentences, and the rest had just 1. We did one iteration of VB per minibatch. Klein and Manning (2004) found that how you start (initialization) really affects the grammar you get when training with POS-tagged WSJ10 data. They suggested a harmonic initialization that gives more weight to rules involving terminals that often appear close together in the training data. We tested both uniform initialization, where the only counts are the basic Dirichlet priors (plus random sentence-specific counts for collapsed VB), and harmonic initialization. For streaming VB, we gathered harmonic counts from each minibatch, while for the others, we got them from the whole training set.",
        "formal_text": "Convert casual text to formal text: We used Dirichlet priors with a symmetric hyperparameter  = 1 for all the algorithms. Early tests showed that the algorithms don’t really care much about hyperparameter settings"
    },
    {
        "casual_text": "From Table 3, it’s clear that our framework consistently boosted the base model’s performance when we used different ways of tweaking the text. But back-translation didn’t do so well—it mostly gave worse results than the base model. This is kind of surprising because (Xie et al., 2020) claimed that back-translation is really effective for semi-supervised text classification. We think the reason it didn’t work here is that back-translation changes the whole sentence into something that’s similar in meaning but different in structure. This creates a bigger gap between the tweaked input and the pseudo-target sentence made from the original input, which messes up how well the model keeps the content intact. On the other hand, simpler word-level changes worked way better. Stuff like spelling errors (spell), randomly swapping words (swap), and replacing abbreviations (abbr) did a great job. These methods just tweak the words a bit without losing the meaning, unlike other methods that either delete whole words (drop, mask) or replace them with completely different ones (synonym, tf-idf). Those more drastic changes might create a bigger mismatch between the pseudo input and output.",
        "formal_text": "Convert casual text to formal text: From Table 3, it’s clear that our framework consistently boosted the base model’s performance when we used different ways of tweaking the text. But back-translation didn’t do so well"
    },
    {
        "casual_text": "Language models like BERT, XLNet, and GPT2 have been proven to handle language semantics and context way better than those old-school static embeddings from GloVe and other word count or frequency-based methods (Sun et al., 2020; Howard and Ruder, 2018). Thanks to their extensive pre-training, these models can pick up on long-term dependencies and understand the contextual and hierarchical relationships between words more effectively than those pre-computed static embeddings. For example, XLNet uses a special [CLS] token to summarize the entire text sequence. We focus on the final layer embeddings of this token for our analysis. We then train a softmax output layer that takes these [CLS] token embeddings from XLNet as input and calculates probabilities for each decision class in our setup. For GPT2, we follow a similar approach but use the pooled output from the final layer of the model for a given input text. This output is also linked to a softmax output layer, which, like with XLNet, is trained to predict classes based on the input embeddings.",
        "formal_text": "Convert casual text to formal text: Language models like BERT, XLNet, and GPT2 have been proven to handle language semantics and context way better than those old-school static embeddings from GloVe"
    },
    {
        "casual_text": "We used a learning rate of 0.03 with a batch size of 5 for all the CNN models in our final experiments. For the LSTM models, we went with a learning rate of 0.01 and a batch size of 10 for all of them.",
        "formal_text": "Convert casual text to formal text: We used a learning rate of 0.03 with a batch size of 5 for all the CNN models in our final experiments. For the LSTM models, we went with a learning rate of"
    },
    {
        "casual_text": "We considered the features of the given kernels and the setup of the semantic-annotated data. Based on that, we came up with the tree structures shown in figures 2(a), 2(b), and 3 for STK and PTK, and sequential structures for SK, as explained below (all these structures are based on the same example from Section 3, which is \"Ho un problema col monitor\"). The structures we used for SK are:",
        "formal_text": "Convert casual text to formal text: We considered the features of the given kernels and the setup of the semantic-annotated data. Based on that, we came up with the tree structures shown in figures 2(a), 2("
    },
    {
        "casual_text": "Lastly, there's the \"end with the same punctuation\" thing. This is another yes-or-no feature. It's a 1 if both end with the same kind of punctuation—like a period, exclamation mark, etc.—or if neither has any punctuation at the end. If that's not the case, then it's a 0.",
        "formal_text": "Convert casual text to formal text: Lastly, there's the \"end with the same punctuation\" thing. This is another yes-or-no feature. It's a 1 if both end with the"
    },
    {
        "casual_text": "Sentence compression can really boost the quality of summaries, as shown by studies like Zajic et al (2007) and Peng et al (2011). Since we're not focusing on sentence compression in this paper, we're just using the updated methods from Li et al (2011). Basically, we clean up each sentence by getting rid of extra stuff like adverbs, relative clauses, abbreviations, participles, and infinitive phrases.",
        "formal_text": "Convert casual text to formal text: Sentence compression can really boost the quality of summaries, as shown by studies like Zajic et al (2007) and Peng et al (2011). Since we're"
    },
    {
        "casual_text": "Thanks to the progress in semantic role labeling (Li et al., 2018) and dependency syntactic parsing (Zhou and Zhao, 2019), some researchers are now looking into improving semantic representations. For example, Zhang et al. (2020b) enhance token representation by combining it with semantic role labels. Meanwhile, Zhang et al. (2020c) and Bai et al. (2021) add extra self-attention layers to incorporate syntactic dependency. Additionally, Mihaylov and Frank (2019) use multiple discourse-aware semantic annotations for machine reading comprehension (MRC) on narrative texts.",
        "formal_text": "Convert casual text to formal text: Thanks to the progress in semantic role labeling (Li et al., 2018) and dependency syntactic parsing (Zhou and Zhao, 2019), some researchers are now"
    },
    {
        "casual_text": "For the training part, we picked a bidirectional GRU for the encoder and a regular forward GRU for the decoder. We went with the top 50,000 most common words for our vocabulary, set the word embedding size to 150, and randomly initialized the embeddings with a uniform distribution between -0.1 and 0.1. The hidden layers are set to 300 dimensions. We used Adam for optimizing the model, with an initial learning rate of 10-4, a gradient clipping value of 0.1, and a dropout rate of 0.5. Training stops when the loss on the validation set doesn't improve for a few iterations.",
        "formal_text": "Convert casual text to formal text: For the training part, we picked a bidirectional GRU for the encoder and a regular forward GRU for the decoder. We went with the top 50,000 most common words for"
    },
    {
        "casual_text": "There's a lot of research out there about how annotator bias affects learning. Reidsma and op den Akker (2008) found that differences between annotators aren't just random mistakes—they're actually different biases or ways of thinking. They noticed that a model trained on one annotator's data worked really well on that same annotator's data but didn't perform as well on data from other annotators. To deal with this, they suggested a couple of solutions: 1) Identify parts of the data where annotators agree more and use only those parts for training (like focusing on cases where at least one person is the main subject for speaker address identification). 2) If possible, train separate models on data from different annotators and combine them by voting. However, this can lower recall because they decided to have the model skip cases where there's no clear agreement.",
        "formal_text": "Convert casual text to formal text: There's a lot of research out there about how annotator bias affects learning. Reidsma and op den Akker (2008) found that differences between annotators aren"
    },
    {
        "casual_text": "Non-autoregressive models are way faster than autoregressive ones, which really matters when the system is being used in real-world situations.",
        "formal_text": "Convert casual text to formal text: Non-autoregressive models are way faster than autoregressive ones, which really matters when the system is used in real-time situations. Convert casual text to formal text: Non-"
    },
    {
        "casual_text": "Costa-jussà and Fonollosa (2006) came up with a two-step method to rearrange translations in a decoder that uses n-grams. In the first step, they group source words into categories and reorder the source sentence based on these categories. Then, in the second step, they translate this reordered sequence of categories into the target language in a straightforward, linear way.",
        "formal_text": "Convert casual text to formal text: Costa-jussà and Fonollosa (2006) came up with a two-step method to rearrange translations in a decoder that uses n-grams"
    },
    {
        "casual_text": "Over the last few years, there's been a growing interest in using syntax-based methods for statistical machine translation. A lot of the work has been focused on improving the translation model, not the language model. But, the big differences in syntax between parallel texts make it tricky to learn syntax-based translation rules effectively. Maybe putting more effort into syntax-based approaches within the language model could be a better way to fix the grammar issues in most machine translation results.",
        "formal_text": "Convert casual text to formal text: Over the last few years, there's been a growing interest in using syntax-based methods for statistical machine translation. A lot of the work has been focused on improving the translation model, not the"
    },
    {
        "casual_text": "I think it's a good idea to comfort them as you learn more about what's going on. Have you considered talking to your parents or a close friend about this?",
        "formal_text": "Convert casual text to formal text: I think it's a good idea to comfort them as you learn more about what's going on. Have you considered talking to your parents or a close friend about this? Have you considered"
    },
    {
        "casual_text": "One thing we didn’t do in this project is create the topic hierarchy tree ourselves—we just assumed it was already given to us. We think this is okay for now because there are other methods out there that can make these trees, and the ones we used were pretty basic. In the future, we’re planning to figure out how to make these trees automatically to see if that works.",
        "formal_text": "Convert casual text to formal text: One thing we didn’t do in this project is create the topic hierarchy tree ourselves—we just assumed it was already given to us. We think this is okay for now because there are other methods out"
    },
    {
        "casual_text": "So, FLIP, SWAP, and TOGGLE work together to create a full Gibbs sampler that reliably samples from the posterior P(z|x, ). These operators aren’t just valid Gibbs steps—they can also create a path with a positive chance of moving from any starting alignment to any target alignment in the space of phrase alignments. Technically, this means the Markov chain they create is irreducible. At the very least, you can always unalign all the phrases in the starting state using TOGGLE, adjust the phrase boundaries to match the target using FLIP, and then use TOGGLE again to get the target alignments.",
        "formal_text": "Convert casual text to formal text: So, FLIP, SWAP, and TOGGLE work together to create a full Gibbs sampler that reliably samples from the posterior P(z|x, ). These"
    },
    {
        "casual_text": "du Plessis and his colleagues (2017) and Kato and team (2018) talked about ways to estimate the class prior from PU data, but they made some assumptions about how the data is distributed. Hsieh and others (2018) came up with PUbN, which is another method based on PU data for dealing with biased negatives in learning. PUbN works in two steps: first, it uses a neural model to estimate the probability of a sample being labeled as positive or negative, and then it uses that estimate. In our tests, PUbN kept overfitting to the majority baseline. We think this might be because estimating the labeling probability is really tricky and can lead to noisy results.",
        "formal_text": "Convert casual text to formal text: du Plessis and his colleagues (2017) and Kato and team (2018) talked about ways to estimate the class prior from PU data, but they made some assumptions about how the data is distributed. H"
    },
    {
        "casual_text": "In this paper, we’re looking at tasks like generating questions or summaries, where one input can lead to multiple possible outputs. To break it down, let’s say we have a source sequence x = (x1...xS) from a set X. Our aim is to figure out a way to model the probability of different target sequences, p(y|x), so that we can assign higher probabilities to multiple valid outputs. For example, in Figure 1, you can see three different questions generated from the same passage, showing that there are multiple ways to map the input to outputs.",
        "formal_text": "Convert casual text to formal text: In this paper, we’re looking at tasks like generating questions or summaries, where one input can lead to multiple possible outputs. To break it down, let’s say we have"
    },
    {
        "casual_text": "Besides the stuff we talked about earlier with our main results, the next parts will show more experiments to give us a better understanding of the method we’re proposing.",
        "formal_text": "Convert casual text to formal text: Besides the stuff we talked about earlier with our main results, the next parts will show more experiments to give us a better understanding of the method we’re proposing. Convert casual text to"
    },
    {
        "casual_text": "So, GM (•) is like a gate that decides how much of the nearby information from the topological space and the hidden space should be combined. This gate is shown in equation (8). W is just a weight matrix that helps train this gate along with the whole model. From equation (7), we can see that if the hidden space neighborhood adds too much noise, its weight gets reduced. This helps cut down the noise from the hidden space and makes the model more accurate.",
        "formal_text": "Convert casual text to formal text: So, GM (•) is like a gate that decides how much of the nearby information from the topological space and the hidden space should be combined. This gate is shown in equation"
    },
    {
        "casual_text": "We’re also working with a semi-supervised learning method called self-learning or pseudolabeling. Basically, this involves training models using predictions made by an earlier version of the model on unlabeled data. This approach has been pretty successful across a bunch of language tasks, like named entity recognition (Collins and Singer, 1999), word sense disambiguation (Mihalcea, 2004), and parsing (McClosky et al., 2006). It’s also been applied in computer vision, and there’s a good overview of that. For task-oriented dialog systems, Cho et al. (2019) found that using self-learning to introduce new features led to significant error reduction. While our method is still based on self-learning using MARUPA-labeled data, we’re taking it a step further by incorporating user feedback, like paraphrasing and friction, as extra signals to improve the self-learning process.",
        "formal_text": "Convert casual text to formal text: We’re also working with a semi-supervised learning method called self-learning or pseudolabeling. Basically, this involves training models using predictions made by an earlier version of the model on"
    },
    {
        "casual_text": "To dive deeper into the issues we're looking at, let's quickly go over some background info and basic terms.",
        "formal_text": "Convert casual text to formal text: To dive deeper into the issues we're looking at, let's quickly over some background info and basic terms. Convert casual text to formal text. Convert casual text to formal text. Con"
    },
    {
        "casual_text": "We looked at two ways to break down compound words. Koehn and Knight (2003) came up with a method that relies entirely on data. They gather frequency stats from the original, unsplit text and then split words in a way that maximizes the geometric mean of the frequencies of the parts. Basically, they aim for the split that makes the most sense based on how often the parts appear. On the other hand, Fritzinger and Fraser (2010) took a mixed approach. They also use the same data-driven method to pick the best split from several options. But instead of considering every possible way to split a word, they only go with splits that are approved by a finite-state morphology tool. So, they’re a bit more selective about what they consider valid.",
        "formal_text": "Convert casual text to formal text: We looked at two ways to break down compound words. Koehn and Knight (2003) came up with a method that relies entirely on data. They gather frequency stats from the original,"
    },
    {
        "casual_text": "We tested how well CAN works on two big benchmarks for MDTC. The results were awesome—CAN hit the top scores. Plus, when we tried it on some unsupervised multi-source domain adaptation tasks, it showed it can handle new, unseen domains pretty well.",
        "formal_text": "Convert casual text to formal text: We tested how well CAN works on two big benchmarks for MDTC. The results were awesome—CAN hit the top scores. Plus, when we tried it on some unsupervised multi-source domain"
    },
    {
        "casual_text": "Surface structure is basically the stuff you get from reading words, phrases, and how they're put together in a sentence when you look at text.",
        "formal_text": "Convert casual text to formal text: Surface structure is basically the stuff you get from reading words, phrases, and how they're put together in a sentence when you look at text. Surface structure is basically the stuff you get from reading"
    },
    {
        "casual_text": "Alright, so the issue here is figuring out the best match between nodes in two graphs, based on the scoring system in Equation (2). This can be turned into a problem where we use something called an integer linear program. Let's say we have these binary variables, x_ik, that tell us if node n_i from graph s is matched with node n_k from graph g.",
        "formal_text": "Convert casual text to formal text: Alright, so the issue here is figuring out the best match between nodes in two graphs, based on the scoring system in Equation (2). This can be turned into a problem"
    },
    {
        "casual_text": "The distribution we get from t(x) after applying temperature scaling is what we're dealing with here, and [t 1/ (x)] k works similarly. The temperature parameter lets us adjust how \"soft\" the teacher's predictions are. Basically, the higher the temperature, the less noticeable the gap is between the highest and lowest values in the probability vector. Temperature scaling helps fix the issue where the network's predictions are way too confident—like putting way too much emphasis on the top guess and not enough on the others. The  2 in Eq 2 makes sure that scaling the temperature doesn't mess up the size of the gradients.",
        "formal_text": "Convert casual text to formal text: The distribution we get from t(x) after applying temperature scaling is what we're dealing with here, and [t 1/ (x)] k works similarly. The temperature parameter"
    },
    {
        "casual_text": "Okay, so we have m = m(1) to m(S), which are all elements of M, and M is basically the set of numbers from 1 to N, repeated S times. These m's are paired with feature vectors h 1, m(1) up to h S, m(S).",
        "formal_text": "Convert casual text to formal text: Okay, so we have m = m(1) to m(S), which are all elements of M, and M is basically the set of numbers from 1 to N, repeated S times."
    },
    {
        "casual_text": "In Figure 2: On the inside pass (left side), DIORA combines two nearby vectors. On the outside pass (right side), DIORA calculates the values for a specific span (i, j) by using its sibling inside span (j +1, k) and outside spans (0, i 1) and (k + 1, n 1). The sibling span during the outside pass can be to the left of the target span, so the indexing gets adjusted accordingly.",
        "formal_text": "Convert casual text to formal text: In Figure 2: On the inside pass (left side), DIORA combines two nearby vectors. On the outside pass (right side), DIORA calculates the values for a specific"
    },
    {
        "casual_text": "Sure! Here's a more casual version: - HMGCN's different versions: - HMGCN without \"cat\" just uses GCN co and GCN prop to figure out the type. - HMGCN without \"prop\" uses GCN co and GCN cat to do the same type inference.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: - HMGCN's different versions: - HMGCN without \"cat\" just uses GCN co and GCN prop to figure"
    },
    {
        "casual_text": "John leaving is a smart move. Basically, the \"John\" part is the one doing the leaving, and the \"clever\" part is how we're judging that action.",
        "formal_text": "Convert casual text to formal text: John leaving is a smart move. Basically, the \"John\" part is the one doing the leaving, and the \"clever part is how we're judging that action."
    },
    {
        "casual_text": "Like HypeNet, RNN(sen) needs entity-concept pairs that show up together in sentences. But unlike RNN(sen), RNN(e) only looks at sentences with the entity in them. From these sentences, RNN(e) tries to figure out which concept the entity is part of. We’re following HypeNet and RNN by using pre-trained GloVe embeddings (from Pennington et al., 2014) to get started. Also, we’re comparing BNSL to the traditional support vector machines (SVM) with a linear kernel. Both SVM and BNSL use the same input features—the top K relations for each concept. We’ve set K to 5 for this case.",
        "formal_text": "Convert casual text to formal text: Like HypeNet, RNN(sen) needs entity-concept pairs that show up together in sentences. But unlike RNN(sen), RNN(e) only looks at sentences"
    },
    {
        "casual_text": "We’ve got 10 grad students who write summaries after checking out documents and watching videos about the same thing. For each topic, we save 3 summaries. The main thing for writing these summaries is: (1) making sure the important stuff from the documents and videos is included.",
        "formal_text": "Convert casual text to formal text: We’ve got 10 grad students who write summaries after checking out documents and watching videos about the same thing. For each topic, we save 3 summaries. The main thing for writing"
    },
    {
        "casual_text": "When creating each part of a sentence that's joined together, the same old stuff might not show up at the surface level because the system stops the tiles from making any words for those repeated elements. Our way of combining different things creates what linguists call ellipsis and gapping. In Figure 4, you can see how two ideas are put together to make \"A1 re-stocked tea on Monday and milk on Friday.\" Some of you might spot that PRED and ARG1 in both parts are marked as RECURRING, but only the later ones get cut off at the surface level. We'll explain why in Section 5.4.",
        "formal_text": "Convert casual text to formal text: When creating each part of a sentence that's joined together, the same old stuff might not show up at the surface level because the system stops the tiles from making any words for those repeated elements."
    },
    {
        "casual_text": "Let’s take the task of labeling arguments as an example. Normally, arguments that are in the passive voice, like subjects, are usually labeled A1. But, there are some less common cases where this rule doesn’t apply. For example, passive subjects in certain structures (like the TELL.01 frame) are usually labeled A2 instead. Check out Figure 1 for an example. Other examples of these quirks include different types of diathesis alternation that only happen in specific frames and with certain argument types (Kipper et al., 2008), the irregular way higher-order roles (A2 to A5) are structured across different frames, and how roles are handled in non-agentive frames. These things only show up in specific, often rare situations, but they’re super important for SRL. For instance, “The silver was sold by the man” is an example where the passive subject isn’t labeled A1.",
        "formal_text": "Convert casual text to formal text: Let’s take the task of labeling arguments as an example. Normally, arguments that are in the passive voice, like subjects, are usually labeled A1. But, there are some less"
    },
    {
        "casual_text": "We suggest adding a simple prediction aggregation module that doesn't require much extra processing power. This module tweaks the predictions based on the specific features of the input clauses. Our tests on the dataset we created show that this aggregation module works well and can be applied generally.",
        "formal_text": "Convert casual text to formal text: We suggest adding a simple prediction aggregation module that doesn't require much extra processing power. This module tweaks the predictions based on the specific features of the input clauses. Our"
    },
    {
        "casual_text": "The results for our Farsi-to-English MT system are in Table 2. We compared our approach to two baselines: monotonic translation and a distance-based penalty model, which is pretty well-known. The distance-based model didn’t make much of a difference: the BLEU score went up from 29.1% to 29.4%, and the WER improved by the same amount. When we switched to the run-based penalty model with the 4 features mentioned in Section 3.4, we saw a bigger jump—the BLEU score improved by 1.5% and the WER by 0.7%, compared to the monotonic translation. So, the run-based model clearly did better than the distance-based one. Looking at the scaling factors for the short, medium, and long-range penalties after optimization, we noticed that the short-range factor was small but negative, which basically gave a bonus for local reorderings. On the other hand, the penalty for a long-range jump was 10 times higher than for a medium-range one. Table 3 shows examples of how the run-based model fixed word order and improved translation quality compared to the distance-based model. Next, we tried applying parse-based reordering rules. When we used these rules on the source sentences and then did monotonic SMT (the \"hard\" reordering in Table 2), the MT error measures actually got worse.",
        "formal_text": "Convert casual text to formal text: The results for our Farsi-to-English MT system are in Table 2. We compared our approach to two baselines: monotonic translation and a distance-based penalty model, which"
    },
    {
        "casual_text": "Another way to combine phrases works at the sentence level. This method looks at how different machine translation systems handle the same source sentence. It gathers all the phrase pairs that these systems use to translate that sentence. Basically, it gives more weight to phrase pairs that are picked by lots of different decoders.",
        "formal_text": "Convert casual text to formal text: Another way to combine phrases works at the sentence level. This method looks at how different machine translation systems handle the same source sentence. It gathers all the phrase pairs that these systems use to translate that sentence"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: - j_l is the smallest number j where j is between 1 and J, and c_j equals 0. - j_r is the biggest number j where j is between 1 and J, and c_j equals 1.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down a simpler way: - j_l is the smallest number j where j is between 1 and"
    },
    {
        "casual_text": "We got rid of everything in /user, /common, /type (except for /type/object/type), /base, and /freebase since they weren't relevant to what we're doing. We also tossed out relations related to individual music tracks, book editions, and TV episodes because there were way too many, they were super specific, and they probably wouldn't help with predicting the relations in our test set.",
        "formal_text": "Convert casual text to formal text: We got rid of everything in /user, /common, /type (except for /type/object/type), /base, and /freebase since they weren"
    },
    {
        "casual_text": "Plus, there's a bunch of research out there that talks about what happens when you focus on optimizing just a bound for the log-likelihood (Alemi et al., 2017) and how re-weighting the information-theoretic stuff within that bound affects things (Higgins et al., 2017; Rainforth et al., 2018).",
        "formal_text": "Convert casual text to formal text: Plus, there's a bunch of research out there that talks about what happens when you focus on optimizing just a bound for the log-likelihood (Alemi"
    },
    {
        "casual_text": "Check out Table 3 for the average confidence levels in the three experiments. Turns out, how confident people feel about their guesses lines up almost perfectly with how accurate they actually are. On average, folks feel more confident when they're right and less confident when they're wrong. Specifically, people tend to be more confident when they correctly guess someone is female, but they're least accurate when they incorrectly guess someone is female. One interesting thing is in the Opposite experiment—when users guessed males wrong, they were more confident than when they guessed males right. This wasn't the case with females. This suggests that women use more distinct language on Twitter, while men might be easier to mix up with women.",
        "formal_text": "Convert casual text to formal text: Check out Table 3 for the average confidence levels in the three experiments. Turns out, how confident people feel about their guesses lines up almost perfectly with how accurate they actually are. On average,"
    },
    {
        "casual_text": "We’ve just sketched out some basic steps to try, but they’re still pretty rough. While we’re still figuring out the best methods, the goals are pretty clear. Using word associations based on big datasets might work well in some cases, but in reality, the words linked together can be so loosely connected that they don’t all pop up when someone’s doing a specific language task. That’s why we need task-specific associations to focus on the most relevant words for a given situation. In this project, we’ve used collocation translation as our task and suggested filtering associations based on similarity or closeness, which we got from word embedding. This way, we hope to improve how dictionaries help translators come up with both accurate and smooth translations.",
        "formal_text": "Convert casual text to formal text: We’ve just sketched out some basic steps to try, but they’re still pretty rough. While we’re still figuring out the best methods, the goals are pretty clear."
    },
    {
        "casual_text": "The rest of the paper goes like this: Section 2 talks about related work. Section 3 explains how we built and used the SentTopic-MultiRank model for summarizing multiple documents. Section 4 covers the process of picking sentences to create the summary. And finally, Section 5 shows the experiments we did and the results we got.",
        "formal_text": "Convert casual text to formal text: The rest of the paper goes like this: Section 2 talks about related work. Section 3 explains how we built and used the SentTopic-MultiRank model for summarizing multiple documents."
    },
    {
        "casual_text": "Entity Extraction. In our system, entities come in three flavors: tasks (like \"Question Answering\"), datasets (like \"SQuAD2.0\"), and metrics (like \"F1\"). To handle this, we use two methods: a dictionary-based approach and a learning-based one. Here's how it works. First, we started with manually curated dictionaries from paperswithcode. But since these dictionaries might not cover everything, we built a module to automatically pull out entities. Unlike earlier work that mostly looked at abstracts (Gábor et al., 2018; Luan et al., 2018), we go through the whole paper and grab the three types of entities related to the main findings. We framed this as a textual entailment task: we treat the paper content as text and the Task-Dataset-Metric (TDM) triples as the hypothesis. This approach makes our model focus on spotting patterns between the text and different triples. We trained our module on a dataset of 332 NLP papers and it did pretty well, scoring a macro-F1 of 56.6 and a micro-F1 of 66.0 on a test set of 162 papers (Hou et al., 2019). Overall, our system tagged 872 tasks, 345 datasets, and 62 metrics from the entire collection of papers.",
        "formal_text": "Convert casual text to formal text: Entity Extraction. In our system, entities come in three flavors: tasks (like \"Question Answering\"), datasets (like \"SQuAD2.0\"), and metrics (like"
    },
    {
        "casual_text": "On the other hand, there were a lot of instances where people talked about membership stuff. It was pretty clear that folks weren’t sure what the robot needed to do the spatial task, and that confusion showed up in the different ways they explained it. Take a look at Figure 2 for a straightforward example.",
        "formal_text": "Convert casual text to formal text: On the other hand, there were a lot of instances where people talked about membership stuff. It was pretty clear that folks weren’t sure what the robot needed to do the spatial task, and that"
    },
    {
        "casual_text": "Without considering dependency constraints, PROP did a bit better than BNST. But the difference wasn’t huge, and only the ROUGE 1 score showed a significant difference (according to the Wilcoxon signed-rank test, p  0.05). However, when dependency constraints were taken into account, PROP really pulled ahead of BNST. This time, both the ROUGE 1 and ROUGE 2 scores had significant differences (p  0.01).",
        "formal_text": "Convert casual text to formal text: Without considering dependency constraints, PROP did a bit better than BNST. But the difference wasn’t huge, and only the ROUGE 1 score showed a significant difference (according"
    },
    {
        "casual_text": "One big issue for languages with fewer resources is that there aren’t many reliable evaluation benchmarks that are easy to access. After a lot of digging, we managed to find two decent test sets and even made one ourselves. Here’s what we found: SIPC: Post and his team (2012) put together a collection of parallel texts between English and six Indian languages, including Bengali, by using crowdsourcing.",
        "formal_text": "Convert casual text to formal text: One big issue for languages with fewer resources is that there aren’t many reliable evaluation benchmarks that are easy to access. After a lot of digging, we managed to find two decent test"
    },
    {
        "casual_text": "We're working with the nonanonymized version of the CNN-DM dataset (Hermann et al., 2015; See et al., 2017). This dataset is split into training, validation, and test sets, with 287, 113, 13, 368, and 11, 490 source-target pairs, respectively. On average, the source documents are around 386 tokens long, and the target summaries are about 55 tokens. Following See et al. (2017), we cut the source and target sentences to 400 and 100 tokens during training.",
        "formal_text": "Convert casual text to formal text: We're working with the nonanonymized version of the CNN-DM dataset (Hermann et al., 2015; See et al., 2017). This dataset is split"
    },
    {
        "casual_text": "CLOnE, created by Funk et al. in 2007, is a CNL that works with the GATE 9 natural language processing framework. It's a straightforward language for building ontologies, using eleven sentence patterns that kind of match up with eleven OWL axiom patterns. It's not totally clear if CLOnE can be expanded in a consistent way to handle bigger parts of OWL, though.",
        "formal_text": "Convert casual text to formal text: CLOnE, created by Funk et al. in 2007, is a CNL that works with the GATE 9 natural language processing framework. It's a straightforward language for building on"
    },
    {
        "casual_text": "Alright, let me break it down in simpler terms. For instance, if you have something like 5(list(list(list()))) = 3, it means that in real-world situations, the number of times you can nest these lists should be pretty low—like, no more than 9. But when you add some limits (like in footnote 5), things might not always work out that way.",
        "formal_text": "Convert casual text to formal text: Alright, let me break it down in simpler terms. For instance, if you have something like 5(list(list(list())))) = 3, it means that in"
    },
    {
        "casual_text": "Alright, let’s break this down. We’re going to look at each of the different ways people use \"if\" one by one. The goal is to figure out if these uses mean something different from the usual \"if\" we use in conditionals. We’ll also see if these non-standard uses show up with other sentence types, so we don’t have to invent a whole new meaning for \"if.\" Maybe they’re connected to other non-standard uses, meaning they could share the same unusual meaning. Or maybe each one needs its own special, quirky meaning of \"if.\"",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down. We’re going to look at each of the different ways people use \"if\" one by one. The goal is to figure out if these uses"
    },
    {
        "casual_text": ",,4,ii+1,j(1),_uu,_v,,,,1/(1+v)C_uv,ii+1,1,0u,ii+11 ,:,S,(2)P_ALN_nulls(E)P_LMN_words(E) ,",
        "formal_text": "Convert casual text to formal text: ,,4,ii+1,j(1),_uu,_v,,,,"
    },
    {
        "casual_text": "Geolocation prediction can basically be approached as either a regression or a classification problem. But in reality, since predicting exact coordinates is pretty tricky, people usually mix regression with classification (Eisenstein et al., 2010; Lourentzou et al., 2017; Fornaciari and Hovy, 2019b). Most of the time, though, it's treated as a classification problem, where you need to figure out which geographic area something belongs to and label it accordingly.",
        "formal_text": "Convert casual text to formal text: Geolocation prediction can basically be approached as either a regression or a classification problem. But in reality, since predicting exact coordinates is pretty tricky, people usually mix regression with classification (Eis"
    },
    {
        "casual_text": "Each experiment uses a training set of QA examples from OneStopQA along with gaze data, plus a development set and a test set. For every experiment, we tweak an initial model for 15 rounds (epochs) for each  value in the range [0, 0.2, 0.4, 0.6, 0.8, 1.0]. We choose the epoch and  combo that works best on the development set, based on question-answering accuracy, and then check how well it does on the test set. For the OneStopQA experiments, we do a five-fold cross-validation setup. Each fold has 18 training articles, 6 for development, and 6 for testing. Each article shows up three times in training, once in development, and once in testing across the five folds.",
        "formal_text": "Convert casual text to formal text: Each experiment uses a training set of QA examples from OneStopQA along with gaze data, plus a development set and a test set. For every experiment, we tweak an initial model"
    },
    {
        "casual_text": "After the preprocessing step, the text gets broken down and analyzed at a more detailed level, like separating words and looking at their parts.",
        "formal_text": "Convert casual text to formal text: After the preprocessing step, the text gets broken down and analyzed a more detailed level, like separating words and looking at their parts. Convert casual text to formal text: After the"
    },
    {
        "casual_text": "Alright, so N_H(i) is basically a group of entities in the latent space that are close to or connected to entity i, including i itself. And \"a_i\" is just a number we use to keep things balanced or normalized. W is... well, it's probably some kind of weight or parameter, but the details aren't clear here.",
        "formal_text": "Convert casual text to formal text: Alright, so N_H(i) is basically a group of entities in the latent space that are close to or connected to entity i, including i itself. And \"a"
    },
    {
        "casual_text": "We think the idea of \"signposts\" could be helpful for dealing with some tricky issues in LTA, like data changing over time, managing versions of growing datasets, and moving data around. In this paper, we talked about the theory behind it and showed how it could work with some real-world (and partly made-up) examples. But there are still some questions we need to figure out: - Do signposts need a title or a short, easy-to-read summary to explain where something fits in the dataset? We didn’t include that in our examples because it’s not something you can easily create automatically. - Should we keep info about objects that have been deleted, especially if the thing they were linked to is gone? One argument for keeping it is that it could help with reproducibility, but that only makes sense if there are standard ways to cite these objects, like including file checksums in the signpost. - How well does it work to assume that a presentation layer can handle extra metadata? We suggested that links to newer versions of an object could be managed in the presentation layer to avoid messing with the metadata. But this mixes up data modeling and presentation, which could create new headaches for data repositories. - It might also be worth making the signpost format more compatible with existing metadata standards, like CMDI (Broeder et al.).",
        "formal_text": "Convert casual text to formal text: We think the idea of \"signposts\" could be helpful for dealing with some tricky issues in LTA, like data changing over time, managing versions of growing datasets, and moving data around."
    },
    {
        "casual_text": "We're working on a system that automatically adds new stuff to the Chart and puts it on the Agenda for later, whenever it can. Plus, there's this idea from psychology and language studies that the more context you have from later parts of a sentence, the easier it is to understand the earlier parts (Pollack & Pickett, 1963; Warren & Warren, 1970). So, we're thinking of using a left-to-right approach that kind of mimics the middle-out method used in HWIM and Hearsay II, but without all the hassle of building separate \"islands\" and with less computer power needed. This smarter way of using \"right context\" relies on giving different paths priority scores.",
        "formal_text": "Convert casual text to formal text: We're working on a system that automatically adds new stuff to the Chart and puts it on the Agenda for later, whenever it can. Plus, there's this idea from psychology and language"
    },
    {
        "casual_text": "Okay, so this part is all about how transcription works. Basically, transcribers need to stick to some set rules for dialog acts, slots, and values, but they can get creative with their wording using those guidelines. Dialog acts are like the purpose or goal of what someone says—kind of like their intent. Just remember, the user and the assistant each have their own set of these dialog acts, which you can check out in Table 1. You’re only allowed to use the ones that are already defined. Slots are like the 10 specific traits or details about restaurants that are set in advance.",
        "formal_text": "Convert casual text to formal text: Okay, so this part is all about how transcription works. Basically, transcribers need to stick to some set rules for dialog acts, slots, and values, but they can get creative with"
    },
    {
        "casual_text": "The first one is the basic dot product self-attention model. G  R II represents a favor alignment. Check out Figure 1 for a visual of the approach we're proposing. In this case, we're using a window size of 2 (D = 2).",
        "formal_text": "Convert casual text to formal text: The first one is the basic dot product self-attention model. G  R II represents a favor alignment. Check out Figure 1 for a visual of the approach we're"
    },
    {
        "casual_text": "Okay, so let’s say ( ) is a word, and _ is either the word itself or a phrase that includes the word. Here, ( ) stands for a specific term, and _ represents all the related terms. Now, here’s how the new GPU sampling works:",
        "formal_text": "Convert casual text to formal text: Okay, so let’s say ( ) is a word, and _ is either the word itself or a phrase that includes the word. Here, ( ) stands for a"
    },
    {
        "casual_text": "For our next steps, we're planning to include the application probability of CCG combinatory operators, which are used during glue grammar rule application, into the grammaticality feature. Additionally, we want to incorporate more CCG-based evaluation metrics that focus on syntax. This should help us fine-tune and evaluate translations more accurately, leading to better grammaticality in the translation output.",
        "formal_text": "Convert casual text to formal text: For our next steps, we're planning to include the application probability of CCG combinatory operators, which are used during glue grammar rule application, into the grammaticality feature. Additionally, we"
    },
    {
        "casual_text": "Sure, this basically means that in this model, trying to make the negative log likelihood as small as possible for individual words and for whole sequences gives the same result.",
        "formal_text": "Convert casual text to formal text: Sure, this basically means that in this model, trying to make the negative log likelihood small possible for individual words and for whole sequences gives the same result. Convert casual text to formal text: Sure"
    },
    {
        "casual_text": "Alright, let's talk about how we write things down when it comes to relational morphology rules. Here's how we do it in our system:",
        "formal_text": "Convert casual text to formal text: Alright, let's talk about how we write things down when it comes to relational morphology rules. Here's we write things down in our system: Here's how we write"
    },
    {
        "casual_text": "Alright, let me break this down in simpler terms. FCRc(0) is basically a collection of pairs, where each pair consists of a category (let's call it Ci) and a list of numbers (APP(i)). Ci is a category within a smaller tree structure, with C8 being the main category at the top. APP(i) is a list of FCRs that can still be used on Ci if we decide to assign specific feature values to that category. Now, the interesting part is that the only new APP list in a smaller tree is calculated based on the \"mother\" category. This is because when we check the FCR rules for the smaller trees under the \"daughters\" (which are like the main categories of those smaller trees), we also apply the FCRs that can be used on those daughter categories. As a result, the remaining pairs for the daughters and their smaller trees will be figured out during this evaluation process. Finally, the set of FCR rules that apply to a specific part of a lexical rule is represented as FCRc(0) = (C, APP(O)).",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in simpler terms. FCRc(0) is basically a collection of pairs, where each pair consists of a category (let's call it Ci)"
    },
    {
        "casual_text": "TINA, the language understanding system talked about in (Seneff, 1992), combines important ideas from context-free grammar, augmented transition networks, and unification. It takes the context-free grammar rules of English and uses them to create a parse tree for a given sentence. This parse tree is then connected to a semantic frame, which acts like a kind of interlingua. For example, if you input the sentence \"0819 z uss sterett,\" the system will generate a parse tree that shows both the syntax and the meaning of the sentence. Higher-level categories like 'sentence' or 'subject' are based on syntax, while lower-level ones like 'ship_name' or 'time_expression' are based on meaning. One cool thing about using semantic categories is that it helps us easily define which words can go together. For instance, the system knows that 'ships' can be modified by a small group of words, like 'uss', which we call 'ship_mod'. This reduces the confusion in the sentence. Plus, it makes it super easy to understand special terms. The system directly understands that 'sterett' and 'kirov' are ship names, 'ssn-12' is a submarine name, and 'z' stands for Greenwich Mean Time. There's more work being done on English/Korean text translation too, like in (Choi, 1994). For more details, check out (Lee, 1995).",
        "formal_text": "Convert casual text to formal text: TINA, the language understanding system talked about in (Seneff, 1992), combines important ideas from context-free grammar, augmented transition networks, and unification. It takes the"
    },
    {
        "casual_text": "Here’s the same info, but in a more relaxed style: p@1 p@3 p@5 p@10 - No attention: 0.8897, 0.7978, 0.7235, 0.5531 - No GCN: 0.9145, 0.8250, 0.7417, 0.5773 - No title: 0.9094, 0.8351, 0.7589, 0.5984 - No abstract: 0.8763, 0.7857, 0.7050, 0.5569 - Title & abstract combined: 0.9082, 0.8361, 0.7621, 0.6058 - Our method (HGCN4MeSH): 0.9267, 0.8495, 0.7707, 0.6124 Table 5: Results from the ablation studies. - \"w/o\" means \"without\" - \"atten\" is short for \"attention\" - \"abs\" is \"abstract\" - \"ours\" refers to HGCN4MeSH - \"title&abs\" means the title and abstract are joined together and fed into the GRU as input.",
        "formal_text": "Convert casual text to formal text: Here’s the same info, but in a more relaxed style: p@1 p@3 p@5 p@10 - No attention: 0.8897,"
    },
    {
        "casual_text": "For each part of a word’s meaning, we group together all the characters that include that part. As you can see in Table 1, not all characters in these groups have meanings that are closely related enough to be useful examples in teaching materials. So, for any group, the goal is to pick out a smaller set of characters that have meanings that are really similar. When creating an algorithm for this, we focus on two main areas of research:",
        "formal_text": "Convert casual text to formal text: For each part of a word’s meaning, we group together all the characters that include that part. As you can see in Table 1, not all characters in these groups have meanings that are closely"
    },
    {
        "casual_text": "* Portability: SABLE started with French and English, then they added Spanish and English, and later Korean and English too. They've got the whole process down and even wrote it all up (Melamed, 1996c).",
        "formal_text": "Convert casual text to formal text: * Portability: SABLE started with French and English, then they added Spanish and English, and later Korean and English too. They've got the whole process down and even wrote it all (M"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Basically, we're talking about whether a statement is mostly true or not. Even if there are some rare exceptions or weird edge cases, it can still be considered factually correct as long as those exceptions don't happen often. Here are some examples to help you get it:",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Basically, we're talking about whether a statement is mostly true or not. Even if there are some rare exceptions or weird edge"
    },
    {
        "casual_text": "The word alignment interface shows a standard alignment grid. The rows are for the words in one language, and the columns are for the words in the translation. You can edit the alignments by clicking on the cells in the grid to connect or disconnect words. The interface lets you align parts of the text that don’t line up perfectly. In Figure 2(b), the light grey, dark grey, and black cells represent words that aren’t linked, possible alignments, and definite alignments, respectively.",
        "formal_text": "Convert casual text to formal text: The word alignment interface shows a standard alignment grid. The rows are for the words in one language, and the columns are for the words in the translation. You can edit the alignments by clicking on"
    },
    {
        "casual_text": "- Your HIT approval rate should be over 90% for all the tasks you've done. - You need to be in one of these countries: Australia, Canada, New Zealand, the UK, or the US. - You should have at least 500 approved HITs. - You must have earned the \"Masters\" badge (this means the platform thinks you're really good at annotating). We got 5 people to evaluate each recipe for the questions in Figure 6 (paid $0.30 per response), Figure 7 ($0.25), and Figure 8 ($0.50). For the model comparison, if less than 3 out of the 5 evaluations agreed, we called it a tie between the models. We didn’t ask our human annotators to check the fish-free dietary constraint because Worcestershire sauce, the main ingredient that might cause issues, isn’t widely known to have fish in it. This caused some confusion during a test run. Heat some olive oil in a big pot over medium heat.",
        "formal_text": "Convert casual text to formal text: - Your HIT approval rate should be over 90% for all the tasks you've done. - You need to be in one of these countries: Australia, Canada, New Zealand, the UK"
    },
    {
        "casual_text": "Besides the open-domain approach, a lot of recent work in question answering has been about the reading comprehension part, where the answer to each question is assumed to be somewhere in a single paragraph that the model can read (Hermann et al., 2015; Rajpurkar et al., 2016; Seo et al., 2017). Another area of focus is question answering using structured knowledge bases (Berant et al., 2013; Berant and Liang, 2014; Yao and Van Durme, 2014; Gardner and Krishnamurthy, 2017). While we’re mainly looking at the broader open-domain setup, QBLink can also be tweaked to work in the reading comprehension setup and the knowledge-bases setup.",
        "formal_text": "Convert casual text to formal text: Besides the open-domain approach, a lot of recent work in question answering has been about the reading comprehension part, where the answer to each question is assumed to be somewhere in a single paragraph"
    },
    {
        "casual_text": "In Section 2, we talked about how we add the \"end of expression\" character, $, to S. This $ only shows up at the very end of any expression. Based on this, we can prove:",
        "formal_text": "Convert casual text to formal text: In Section 2, we talked about how we add the \"end of expression\" character, $, to S. This $ only shows at the very end of any expression. Based on this, we can prove"
    },
    {
        "casual_text": "(3) Here, BoW S_k stands for the bag of words for sentence S_k. So, the document is created in two main steps.",
        "formal_text": "Convert casual text to formal text: (3) Here, BoW S_k stands for the bag of words for sentence S_k. So, the document created in two main steps."
    },
    {
        "casual_text": "8. If you type \"J'espère les avocats\" into the test interface, it translates to \"I hope The lawyers.\" Why is \"The\" capitalized?",
        "formal_text": "Convert casual text to formal text: 8. If you type \"J'espère les avocats\" into the test interface, it translates to \"I hope The lawyers.\" Why is \"The\" capitalized? Why is \"The\""
    },
    {
        "casual_text": "We're breaking down the whole challenge into three main parts: 1) finding the right documents, 2) figuring out the important bits in those documents that relate to what we're working on and should be summarized, and 3) creating a final summary that focuses on the specific topic we're dealing with. It's obvious that current NLP tech isn't advanced enough to solve this whole thing in one go, but we think working on each part separately will help us get closer to a complete solution eventually.",
        "formal_text": "Convert casual text to formal text: We're breaking down the whole challenge into three main parts: 1) finding the right documents, 2) figuring out the important bits in those documents that relate to what we're working on and should be"
    },
    {
        "casual_text": "So, FFN(x) is basically a two-layer thing where data goes in, gets processed, and there's a ReLU function in between the layers to spice things up.",
        "formal_text": "Convert casual text to formal text: So, FFN(x) is basically a two-layer thing where data goes in, gets processed, and there's a ReLU function in between the layers to spice things up."
    },
    {
        "casual_text": "Lastly, we should definitely take into account user feedback, whether it's about little things like typos or missing stuff, or bigger stuff like adding new features or supporting new types of apps.",
        "formal_text": "Convert casual text to formal text: Lastly, we should definitely take into account user feedback, whether it's about little things like typos or missing stuff, or bigger things like adding new features or supporting new types of apps."
    },
    {
        "casual_text": "Lexical semantics is super important for organizing the embedding space in the early layers of representation. As we move to later layers, non-lexical compositional features start to show up more, especially when we look at how probes perform on non-typical sentences. This is all based on what we saw in Experiment 1 and Figure 1.",
        "formal_text": "Convert casual text to formal text: Lexical semantics is super important for organizing the embedding space in the early layers of representation. As we move to later layers, non-lexical compositional features start to show up more,"
    },
    {
        "casual_text": "To figure out the chance that the difference in skill is more than two standard errors, which basically means we're looking at a significance level of   0.05.",
        "formal_text": "Convert casual text to formal text: To figure out the chance that the difference in skill is more than two standard errors, which basically means we're looking at a significance level of   0.05.. Convert casual"
    },
    {
        "casual_text": "This paper tackles the challenge of creating a text summary from different types of media—like text, audio, and video—without needing them to be perfectly synced. We frame this multi-modal summarization (MMS) task as an optimization problem, focusing on maximizing certain functions while staying within a set budget. To make the most of audio transcriptions, we came up with some strategies using a graph model to figure out which parts of the text are most important, which helps us create summaries that are both clear and packed with useful info. We also looked at different ways to see how well images and text connect, and found that the image match model works the best. The tests we ran on our MMS dataset, covering both English and Chinese, showed that our system really benefits from using all these different types of information together.",
        "formal_text": "Convert casual text to formal text: This paper tackles the challenge of creating a text summary from different types of media—like text, audio, and video—without needing them to be perfectly synced. We frame this multi"
    },
    {
        "casual_text": "First, we use graph neural networks to encode the knowledge graph structure. Then, we feed the output, which has a bunch of structure info, into a graph embedding model for making predictions. You can either train the graph model and scoring model together end-to-end or just use the graph encoder output to set up the entity embedding (shoutout to Nathani et al., 2019). But in this paper, we go a different route: we throw the graph context right into the distance scoring function.",
        "formal_text": "Convert casual text to formal text: First, we use graph neural networks to encode the knowledge graph structure. Then, we feed the output, which has a bunch of structure info, into a graph embedding model for making"
    },
    {
        "casual_text": "To make our questions more believable, we create three wrong answers for each simile. These wrong answers follow two rules (based on Haladyna et al., 2002, and Ren and Zhu, 2020): they should be true-negative and challenging. True-negative means the wrong answers don’t make sense when used in the question, but they’re still related to the correct answer in a tricky way. Our process for making these wrong answers has three steps: 1) coming up with options, 2) picking the best ones, and 3) double-checking with people to make sure they’re good.",
        "formal_text": "Convert casual text to formal text: To make our questions more believable, we create three wrong answers for each simile. To make our questions more believable, we create three wrong answers for each simile. These wrong answers follow two"
    },
    {
        "casual_text": "Check out Table 4 for the results. Both knowledge distillation and POSPD help lower the repetition rate in NAG models across four datasets. They work especially well on XSUM, which has longer sentences. When it comes to the number of tokens, knowledge distillation really cuts down on the tokens generated by NAG models, especially on XSUM. On the other hand, POSPD keeps the length of the generated sentences pretty close to the reference without making the repetition rate worse. Table 4 breaks it all down: it shows how the NAG models perform. \"Reference\" is the target sentence we're aiming for. \"Repetition\" and \"Tokens\" show the difference in repetition rate and the number of tokens between the reference and what the model actually outputs.",
        "formal_text": "Convert casual text to formal text: Check out Table 4 for the results. Both knowledge distillation and POSPD help lower the repetition rate in NAG models across four datasets. They work especially well on XSUM, which"
    },
    {
        "casual_text": "Okay, so another parsing algorithm for automatically analyzing Turkish words was suggested and used by kiz in his Ph.D. thesis. His algorithm is called \"Identified Maximum Match (IMM) Algorithm.\" Basically, it tries to find the longest substring that exists in a given dictionary to the left of the word. If it finds a match, that part is identified as the root, and the rest of the word is treated as the new search target. This leftover part is then checked in a suffix dictionary to identify the morphemes one by one. The process stops when there's nothing left to analyze. However, sometimes the algorithm might not find a proper match, but a consistency check shows that the solution isn't correct. In those cases, the previous \"almost-solution\" is shortened by one character, and the whole search process starts over again.",
        "formal_text": "Convert casual text to formal text: Okay, so another parsing algorithm for automatically analyzing Turkish words was suggested and used by kiz in his Ph.D. thesis. His algorithm is called \"Identified Maximum Match ("
    },
    {
        "casual_text": "A possible issue with the Collins parser is that it assumes a lot of hidden structure. The Treebank grammar, which is the model used in the Collins parser, isn’t necessarily the best fit for machine translation. In fact, a lot of researchers have pointed out that the Treebank grammar isn’t even the top choice for creating parsers that are judged based on how well they can recreate Treebank structures (Klein and Manning, 2003; Matsuzaki et al., 2005; Petrov and Klein, 2007). It makes sense to think that a different grammar might work better as a language model for MT.",
        "formal_text": "Convert casual text to formal text: A possible issue with the Collins parser is that it assumes a lot of hidden structure. The Treebank grammar, which is the model used in the Collins parser, isn’"
    },
    {
        "casual_text": "It's helpful for dialogue systems to use these kinds of scenarios to make dialogue generation smoother. But the problem is, manually labeling all these scenario contexts is super tough and just too much work. Instead, we noticed that these scenarios are already hiding in existing multi-turn dialogue datasets. In these datasets, the whole conversation—both what’s been said before and what’s coming next—kind of naturally forms a specific dialogue scenario. Check out Figure 1 for an example. In Scenario 1, the phrase \"for a week\" in the future part of the conversation shows that the response is about time. In Scenario 2, \"cut the price\" tells us the response is dealing with price details.",
        "formal_text": "Convert casual text to formal text: It's helpful for dialogue systems to use these kinds of scenarios to make dialogue generation smoother. But the problem is, manually labeling all these scenario contexts is super tough and just too much work"
    },
    {
        "casual_text": "We're introducing a new way to rank answers for MLQA (which stands for Learning to Translate or L2T). This method focuses on finding the best translation for a question and/or its possible answers by figuring out how well it can tell good answers from bad ones. To do this, we've come up with a bunch of features that highlight both the words and the meanings that match between a question and its potential answers, using different translation tricks (check out Section 3.1 for more on that). Then, the model learns how important each of these features is for different translation directions and methods by going through a training process that helps it tell the difference between good and bad answers (Section 3.2 explains this part). Once the model's all trained up, it can be used for MLQA by ranking all the possible answers based on how well they score. Instead of having just one model that tries to handle all languages, it might make more sense to train a separate model for each language. This way, each model can learn what makes a good answer in its specific language, leading to a more detailed ranking (Section 3.4 talks about this). We're calling this new idea Learning to Custom Translate (L2CT).",
        "formal_text": "Convert casual text to formal text: We're introducing a new way to rank answers for MLQA (which stands for Learning to Translate or L2T). This method focuses on finding the best translation for a"
    },
    {
        "casual_text": "Over the past few years, people have really started to see the value in reproducibility in science, especially when it comes to reusing data. This has been tied to the rise of \"open science\" (check out Fecher and Friesike, 2014 for a good summary) and the whole FAIR principles thing (Wilkinson et al., 2016). To make sure a scientific process can be reproduced, you need to keep access to the data. But that’s tricky when data gets deleted. It’s interesting that while people usually think about data formats changing when preserving data (like Conway et al., 2011 show), no one seems to have talked about data changing because of legal issues. One thing that sets growing datasets apart is that, for legal reasons, parts of the data might need to be removed or changed. This isn’t usually considered in long-term archival stuff (see Digital Preservation Coalition, 2015 or Neuroth et al., 2009), where they usually assume data stays the same. To stick as close as possible to the idea of long-term archival, it’s important to still provide some useful info when someone tries to access data that’s been removed. Researchers probably won’t expect data linked to a persistent identifier to have been changed, so this is a big deal. As far as we know, only Caron et al. have really addressed this issue.",
        "formal_text": "Convert casual text to formal text: Over the past few years, people have really started to see the value in reproducibility in science, especially when it comes to reusing data. This has been tied to the rise of \"open science"
    },
    {
        "casual_text": "The Definitional Nonce (DN) task, introduced by Herbelot and Baroni in 2017, gives you one definitional sentence for each word you need to test. These test words are regular words that already have a really good \"gold\" vector because they show up a lot in the training data. The goal here is for a few-shot learning algorithm to guess a vector that’s really close to this gold vector. To check how well it’s doing, we look at how far the guessed vector is from a bunch of background vectors. Ideally, the gold vector should be the closest one, meaning it’s at rank 1. The performance is measured using two things: the Mean Reciprocal Rank (MRR) and the median rank, both calculated across 300 test words. Since the DN task uses definitional sentences instead of how words are naturally used, we use a special development set for the DN to tweak the settings and get the best results for this specific dataset.",
        "formal_text": "Convert casual text to formal text: The Definitional Nonce (DN) task, introduced by Herbelot and Baroni in 2017, gives you one definitional sentence for each word you need to test. These test words are regular words"
    },
    {
        "casual_text": "where m is the length of the question and n is the length of the document.",
        "formal_text": "Convert casual text to formal text: where m is the length of the question and n is the length of the document."
    },
    {
        "casual_text": "AG News. This dataset is for classifying news articles into four different categories. It's split into a balanced training set with 120,000 samples and a test set with 7,600 samples.",
        "formal_text": "Convert casual text to formal text: AG News. This dataset is for classifying news articles into four different categories. It's split into a balanced training set with 120,000 samples and a test set with 7,600 samples. It"
    },
    {
        "casual_text": "Basically, we follow the same steps we've already set up to process the document. After that, for each potential Key Point (KP), we tally up how many times it shows up in all the noun phrases, which includes both regular nouns and proper names. Lastly, we adjust the score we get by dividing it by the total number of times that candidate appears in the whole document.",
        "formal_text": "Convert casual text to formal text: Basically, we follow the same steps we've already set up to process the document. After that, for each potential Key Point (KP), we tally up how many times it shows"
    },
    {
        "casual_text": "Alright, so for future work on this task, we need to tackle a few key things: (a) We should work on picking better context. One idea is to mix traditional methods (IR) with neural ones to clean up the passages we choose, or maybe even try to optimize both context selection and answer extraction at the same time (Das et al., 2019). (b) We need to find better ways to represent questions, sentences, and possible answers. Right now, we're just averaging embeddings, which isn't great because it loses some important info. (c) We should figure out better ways to pretrain models so they're more prepared for the actual BookQA task. (d) Lastly, we should think about adding commonsense knowledge and structure to the mix, which wasn't covered in this paper.",
        "formal_text": "Convert casual text to formal text: Alright, so for future work on this task, we need to tackle a few key things: (a) We should work on picking better context. One idea is to mix traditional methods (IR"
    },
    {
        "casual_text": "Okay, let's break this down into simpler, more conversational language: 1. Start with  1(0) =  2. 2. For each step from e = 0 to E - 1: - Go through each i from 1 to n: - Calculate  A using the formula: exp   (en+i1) A divided by exp  A, where A is part of the set R and  (en+i1) A. - Then, f (en+i) (A  ) is the expected value of f (t, A  ) based on . - Update  using the formula: ( + i) - . - For each A   in the set R: - Update  (en+i) A with this formula: (1 - ) times  (en+i1) A plus  times l (i) A times f (en+i) (A  ). Basically, this is a step-by-step process where you're updating some values based on certain calculations and rules.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down into simpler, more conversational language: 1. Start with  1(0) =  2. 2. For each step from e = 0 to E -"
    },
    {
        "casual_text": "For our analysis, we combined the explanations, and we averaged the accuracies for both the multi-class and binary datasets because the patterns stayed consistent across both.",
        "formal_text": "Convert casual text to formal text: For our analysis, we combined the explanations, and we averaged the accuracies for both the multi-class and binary datasets because the patterns stayed consistent across both. For our"
    },
    {
        "casual_text": "But to do this, we need a context-free grammar to use as the foundation for parsing.",
        "formal_text": "Convert casual text to formal text: But to do this, we need a context-free grammar use as the foundation for parsing. Convert casual text to formal text: But to do this, we need a context-"
    },
    {
        "casual_text": "MULTISEQ is an upgrade to SEQ that lets you predict multiple labels for each token. In some sequence labeling tasks, you don't know ahead of time how many labels each token will need. So, we calculate a probability score for each label using binary cross-entropy as the loss function. Then, we pick out all the labels that score above a certain threshold and output them. You can set this threshold in the dataset's configuration file.",
        "formal_text": "Convert casual text to formal text: MULTISEQ is an upgrade to SEQ that lets you predict multiple labels for each token. In some sequence labeling tasks, you don't know ahead of time how many labels each token"
    },
    {
        "casual_text": "For Indo-European and Mayan, the CNNs actually work better than the baseline system. But for Austronesian, the CNNs don't do better than the baseline. The results for Indo-European and Mayan (check out table 5) are pretty much the same as the ones in table 4. Basically, the CharCNN system is the best for the Mayan language family, and the PhoneticCNN system is the best for the Indo-European family. Interestingly, for the Austronesian family, the baseline system actually does better in terms of F-score compared to the top-performing system from table 4, which is the CharCNN with language features. The baseline system also has a higher Accuracy score, but the difference isn't big enough to be considered statistically significant. One possible reason for this is that there just isn't enough info in those 20 meanings to figure out phonological similarities across 100 languages.",
        "formal_text": "Convert casual text to formal text: For Indo-European and Mayan, the CNNs actually work better than the baseline system. But for Austronesian, the CNNs don't do better than the baseline. The results"
    },
    {
        "casual_text": "Let’s say you have a text with T sentences and a vocabulary of L unique words (like words that aren’t in a stop list, otherwise most sentences would look pretty similar). You can represent this text using a T x L matrix called F. Here’s how it works: for each sentence (t = 1, 2, ..., T) and each word (1 = 1, 2, ..., L), you put a 1 in the matrix if the 1-th word is in the t-th sentence, and a 0 if it’s not.",
        "formal_text": "Convert casual text to formal text: Let’s say you have a text with T sentences and a vocabulary of L unique words (like words that aren’t in a stop list, otherwise most sentences would look pretty similar"
    },
    {
        "casual_text": "Here are some suggestions for future experiments on unsupervised constituency parsing.",
        "formal_text": "Convert casual text to formal text: Here some suggestions on future experiments on unsupervised constituency parsing. Convert casual text to formal text: Here some suggestions on future experiments on unsupervised constituency parsing. Convert casual"
    },
    {
        "casual_text": "A PET for a permutation  is basically a tree where the nodes are labeled with operators (check Figure 1). The leaves of any subtree in this tree form a smaller part of , called a sub-permutation, which is just a continuous chunk of  that looks like a permutation. Take  a = 6, 1, 4, 2, 3, 5 and  b = 6, 1, 5, 2, 3, 4 as examples. Their PETs (the two on the left in Figure 1) are made using only monotone 1, 2 or inverted 2, 1 operators. A couple of local inversions with 2, 1 could make both  a and  b monotone. Now, look at  c = 2, 4, 5, 6, 1, 3 (the one on the right in Figure 1). It needs a 2, 4, 1, 3 at the root to become monotone. On the other hand,  d = 6, 2, 4, 1, 5, 3 can't be broken down because it doesn't have proper sub-permutations. These kinds of permutations that can't be factored are called primal permutations (Albert and Atkinson, 2005)—you can read more about them in Section 3. Basically,  d needs itself to be converted into monotone. In short,  a and  b seem easier to handle than  c, which is simpler than  d.",
        "formal_text": "Convert casual text to formal text: A PET for a permutation  is basically a tree where the nodes are labeled with operators (check Figure 1). The leaves of any subtree in this tree form"
    },
    {
        "casual_text": "At first, we noticed that there wasn’t much of a difference in how well antonyms were classified when we used text-based features versus coreference-based ones. But when we looked closer at the mistakes, we figured out that our idea for using coreference-based embeddings only really works for nouns, not for other types of words. We’ll get into that more later. So, we decided to run our tests in two ways: one where we look at all words (considering every pair) and another where we only focus on nouns (only looking at pairs where the target word is a noun). To figure out if a word is a noun or not, we used the Stanford part-of-speech tagger, which was developed by Toutanova and team back in 2003.",
        "formal_text": "Convert casual text to formal text: At first, we noticed that there wasn’t much of a difference in how well antonyms were classified when we used text-based features versus coreference-based ones. But when"
    },
    {
        "casual_text": "Reporting verbs give different amounts of info about things like time, how something was said, whether it's true, and how reliable it is. The simplest reporting verb is \"say.\" All \"say\" assumes is that someone said something, and it tries to represent that as closely as possible. In this way, \"say\" is even less complicated than \"report,\" which also mentions who the message was for (usually understood from the context). Other reporting verbs are different because of the situations they're used in. Let's take a closer look at \"insist.\" One example can be found in the first part of the first sentence in Figure 1, which I'll repeat here as (1). The idea of opposition, which is mentioned clearly in LDOCE but just hinted at in MWDP, is a big part of what \"insist\" means. After analyzing a big collection of TIME magazine articles from 1963 (called the TIMEcorpus) [Berglerg0a], we found that every sentence with \"insist\" had some kind of opposition in it, and this was usually shown by other things like word order or emphasis. The most common way to show opposition was by using negation, like in (1).",
        "formal_text": "Convert casual text to formal text: Reporting verbs give different amounts of info about things like time, how something was said, whether it's true, and how reliable it is. The simplest reporting verb is \"say.\" All"
    },
    {
        "casual_text": "The dataset is huge—it has over 200 hours of spoken content along with manually corrected transcripts. This makes it super useful for other speech-related tasks that need this kind of data. For example, you could use it for speech-to-text, text-to-speech, or even learning word or sentence embeddings directly from speech (like Chung and Glass did in 2018, or Haque et al. in 2019). Usually, these kinds of tasks rely on big datasets of read speech (like the one Panayotov et al. used in 2015), but our data lets you explore them with real, spontaneous spoken speech.",
        "formal_text": "Convert casual text to formal text: The dataset is huge—it has over 200 hours of spoken content along with manually corrected transcripts. This makes it super useful for other speech-related tasks that need this kind of data. For example,"
    },
    {
        "casual_text": "First, let's talk about full-sentence MT and SiMT, with a special focus on the prefix-to-prefix setup.",
        "formal_text": "Convert casual text to formal text: First, let's talk about full-sentence MT and SiMT, with a special focus on the prefix-to-prefix setup."
    },
    {
        "casual_text": "It’s clear that both aligners did a pretty good job, but CATS-Align worked better with hand-aligned data. When we looked into the mistakes Bleualign made, we noticed that most of the errors happened because it grabbed a few extra sentences from the next paragraph. This only seemed to happen with adapted paragraphs. One reason for this could be that the original and adapted texts don’t match up perfectly in size. But when it comes to aligning longer books or novels, this might actually be helpful. Sometimes, longer paragraphs in the original text are split into smaller ones in the adaptation, so grabbing a few extra adapted paragraphs could help match the longer original ones better.",
        "formal_text": "Convert casual text to formal text: It’s clear that both aligners did a pretty good job, but CATS-Align worked better with hand-aligned data. When we looked into the mistakes Bleualign"
    },
    {
        "casual_text": "[Answer] five [Knowledge] You ride the subway to and from work five days a week.",
        "formal_text": "[Answer] five [Knowledge] You ride the subway to and work five days a week. You have a job that requires you to be on the subway to and work five days a week. You have"
    },
    {
        "casual_text": "We used the LSP natural language processing system, written in FORTRAN, on a Control Data 6600 computer. It needed around 75,000 words of memory to run. The LSP system can also work on CDC CER, VAX, and UNIVAC ii00 machines. When we tested it on hospital discharge summaries, the system found that 49% of the sentences were incomplete—basically sentence fragments. These fragments came in six types, which are like full sentences but missing some parts: 1. Missing the verb and object (or subject and verb), leaving just a noun phrase. 2. Missing the tense and verb \"be.\" 3. Missing the subject, tense, and verb \"be.\" 4. Missing just the subject. 5. Missing the subject, tense, and verb \"be\" in a passive sentence. 6. Missing the subject, tense, and verb \"be\" in an infinitive phrase. For example, \"To be followed in hematology clinic\" is a fragment of type 6.",
        "formal_text": "Convert casual text to formal text: We used the LSP natural language processing system, written in FORTRAN, on a Control Data 6600 computer. It needed around 75,000 words of memory to run. The LSP system can"
    },
    {
        "casual_text": "For the \"wug test,\" we split the inflection tables we got from this dataset into a 7:1:2 train-dev-test ratio. Basically, we're using the same split as the shared task, but instead of dividing by lemma-tag-form triples, we're dividing by inflection tables. This way, the lemmas used for validation and testing don't overlap with those used for training. You can check out the data stats in Appendix A if you're curious.",
        "formal_text": "Convert casual text to formal text: For the \"wug test,\" we split the inflection tables we got from this dataset into a 7:1:2 train-dev-test ratio. Basically, we're"
    },
    {
        "casual_text": "We fine-tuned the hyperparameters for the CRW and DN development sets, sticking to the original evaluation setup. These same settings are applied to the new tasks as well. All the model training we mention below was done using the Westbury Wikipedia Corpus (Shaoul, 2010), which has been used in earlier research (Schick and Schütze, 2018; Khodak et al., 2018).",
        "formal_text": "Convert casual text to formal text: We fine-tuned the hyperparameters for the CRW and DN development sets, sticking to the original evaluation setup. These same settings are applied to the new tasks as well."
    },
    {
        "casual_text": "Alright, so next up, we pick a random inflection for each word in our training examples, but we use this special inflectional distribution instead of just picking randomly. To keep things clean, we only pick inflections that match the original word's part of speech (UPOS). We do this four times for each example, which gives us a training set with a 1:4 ratio of clean to adversarial examples. This whole process is super efficient and can be done in linear time, so it scales really well. Check out Algorithm 2 in Appendix C for the nitty-gritty details, and Figure 2b shows how the inflectional distribution changes before and after we do this.",
        "formal_text": "Convert casual text to formal text: Alright, so next up, we pick a random inflection for each word in our training examples, but we use this special inflectional distribution instead of just picking randomly. To keep things"
    },
    {
        "casual_text": "Another way we use \"if\" is when the second part is already true, and there's no need to make a conclusion. This happens when there's an adjective in the first part. For example: (5) (... ) now that you've finally grown up, if a little late (... ) (121) (... ) a well-known place for quickly, but temporarily, helping drunks get better (... ) This use of \"if\" can't be simplified to the usual \"if\" meaning by saying the first part was once a full sentence. If we try to do that, like:",
        "formal_text": "Convert casual text to formal text: Another way we use \"if\" is when the second part is already true, and there's no need to make a conclusion. This happens when there's an adjective in the first part."
    },
    {
        "casual_text": "The progress in commonsense inference is kind of holding back the development of truly general-purpose AI. You can see its impact in making chatbots and search engines smarter. It helps these systems go beyond just using the context of the question and makes them more like humans. Future improvements could come from using word embeddings, like those made from ConceptNet and other commonsense datasets and graphs (like Conceptnet Numberbatch embeddings). Making sentences more grammatically correct and complex could also boost accuracy. Plus, tweaking the model's hyperparameters and gathering more training data would definitely help advance this field.",
        "formal_text": "Convert casual text to formal text: The progress in commonsense inference is kind of holding back the development of truly general-purpose AI. You can see its impact in making chatbots and search engines smarter. It helps these"
    },
    {
        "casual_text": "Another key rule to stop things from going on forever is the attenuation factor. This is like a discount that lowers the score for each level as you move away from the starting point where the original pair entered the matching process. The farther you get from that starting level (called level 0), the lower the highest possible score becomes. Eventually, the top score drops below a certain limit, and that's when SWESIL says, \"Okay, enough matching for now.\"",
        "formal_text": "Convert casual text to formal text: Another key rule to stop things from going on forever is the attenuation factor. This is like a discount that lowers the score for each level as you move away from the starting point where"
    },
    {
        "casual_text": "Check out https://www.xbrl.org/the-standard/what/an-introduction-to-xbrl/ for a quick intro to xbrl.",
        "formal_text": "Convert casual text to formal text: Check out https://www.xbrl.org/the-standard/what/an-introduction-to-xbrl/ for a quick intro to xb"
    },
    {
        "casual_text": "Psychometrics is all about measuring stuff like attitudes, beliefs, perceptions, and personality traits. A lot of the language resources and tests out there kind of focus on these psychometric aspects, as mentioned by Ahmad et al. in 2020. We’re building on this idea by looking at some less explored areas, like trust, anxiety, and how people feel about their literacy in health-related situations. Instead of just relying on separate annotations, we’re using text that people write themselves, which is paired with answers they give in surveys about these psychometric dimensions. This way, we have their survey responses as a kind of gold standard to compare against what we’re measuring using NLP techniques. This paper connects the worlds of social science and NLP when it comes to building these testbeds. It’s similar to recent work in NLP that’s been focusing on mental health, like predicting psychological health or helping with suicide prevention, as seen in studies by Lynn et al. in 2018, Shing et al. in 2020, and Resnik et al. in 2021. This approach also follows what Buechel et al. did with self-reported survey items.",
        "formal_text": "Convert casual text to formal text: Psychometrics is all about measuring stuff like attitudes, beliefs, perceptions, and personality traits. A lot of the language resources and tests out there kind of focus on these psychometric aspects, as mentioned"
    },
    {
        "casual_text": "It looks like these two types of embeddings focus on different ways of measuring similarity. Unlike the text-based ones, the coreference-based neighbors share the same gender. The text-based ones are just words that can replace each other, but swapping them tends to change the meaning more compared to the coreference-based ones.",
        "formal_text": "Convert casual text to formal text: It looks like these two types of embeddings focus on different ways of measuring similarity. Unlike the text-based ones, the coreference-based neighbors share the same gender. The text"
    },
    {
        "casual_text": "I’m pretty sure that the whole idea of syntactic complexity is, for the most part, full of bias, prejudice, and flat-out wrong assumptions. It’s so messed up that most of what people say about it just isn’t worth much. Specifically, I think that “Wittgensteinian” slogan everyone talks about is way off base. I suspect its popularity comes from people misinterpreting it—they don’t see it as a factual claim, but more as a general, fuzzy suggestion to just say whatever you want in the simplest, clearest way you can. And who could argue with that, right? But even if you take it that way, it’s not always great advice. If you think simplicity means keeping things syntactically simple, you might end up oversimplifying, and the cost of that could be way too high, even if it’s technically “possible.”",
        "formal_text": "Convert casual text to formal text: I’m pretty sure that the whole idea of syntactic complexity is, for the most part, full of bias, prejudice, and flat-out wrong assumptions. It’s so messe"
    },
    {
        "casual_text": "The final training loss during cross-lingual training, L_cross, is just the total of all those losses we talked about earlier.",
        "formal_text": "Convert casual text to formal text: The final training loss during cross-lingual training, L_cross, is just the total of all those losses we earlier. Convert casual text to formal text: The final training loss during cross-lingual"
    },
    {
        "casual_text": "You can definitely bring in senior linguists for the project, but mainly for handling linguistic coordination stuff rather than actual post-editing. They add a lot of experience, credibility, and help keep the team together.",
        "formal_text": "Convert casual text to formal text: You can definitely bring in senior linguists for the project, but mainly for handling linguistic coordination stuff rather actual post-editing. They add a lot of experience, credibility,"
    },
    {
        "casual_text": "Check out Table 1 for some examples of English-French pairs that show the extremes of this spectrum (where mutual information isn’t zero). We think this method, where we weigh alignments using mutual information, works really well with the log-entropy setup we use for X. Both approaches are rooted in information theory, so they just seem to click together.",
        "formal_text": "Convert casual text to formal text: Check out Table 1 for some examples of English-French pairs that show the extremes of this spectrum (where mutual information isn’t zero). We think this method, where we weigh alignment"
    },
    {
        "casual_text": "We offer support for three different acquisition models, depending on the language and the availability of human resources for the task. In all cases, there needs to be a line supervisor on-site who oversees the language acquisition process. This supervisor doesn’t necessarily need to know the language being worked on, but they should be able to communicate with the people doing the acquisition, no matter where they are or who they are. It’s also expected that this supervisor will spend some time getting to know the language, and if possible, learning to speak it. Their main job is to make sure the acquisition happens on time and meets the required quality standards. Additionally, the supervisor should have some background in computational linguistics to help with building, using, and evaluating the morphological analyzer.",
        "formal_text": "Convert casual text to formal text: We offer support for three different acquisition models, depending on the language and the availability of human resources for the task. In all cases, there needs to be a line supervisor on-site who oversees"
    },
    {
        "casual_text": "Following up on Zipf's (1949) ideas, Piantadosi and his colleagues in 2012 argued that, when you look at communication from an information theory perspective, ambiguity is actually a key factor in making a communication system work efficiently. They focused on the idea of using words in a way that saves effort. According to them, having words with multiple meanings can be useful, especially when the context helps clarify what's being said—it lets us reuse simpler words. To back up their claim, they showed a connection between how many meanings a word has in WordNet (Miller, 1995) and how easy it is for speakers to use those words. They looked at things like how easy the word is to pronounce, how long it is, and how common it is in everyday language (based on data from a big collection of text).",
        "formal_text": "Convert casual text to formal text: Following up on Zipf's (1949) ideas, Piantadosi and his colleagues in 2012 argued that, when you look at communication from an information theory perspective, ambiguity is actually"
    },
    {
        "casual_text": "Basically, if the oracle says two terms, i and j, are a match, you’d put a 1 in D1 at spots (i, j) and (j, i). Even if you fill D1 with this kind of term matching info, the matrix B will still be pretty sparse. Without any document-to-document data, D2 could just be an identity matrix or a zero matrix. From what we’ve seen, using D2 = 0 works a little better in real-world situations. Figure 2 shows a block matrix that’s been updated with these term alignments.",
        "formal_text": "Convert casual text to formal text: Basically, if the oracle says two terms, i and j, are a match, you’d put a 1 in D1 at spots (i, j"
    },
    {
        "casual_text": "Alright, so here's the deal: this process focused on tagging individual tweets as potentially risky for suicide, not the whole history of a user's tweets. We managed to get a decent agreement between the people doing the tagging, with a Cohen's Kappa score of 0.72, which was overseen by a clinical psychologist. The dataset we ended up with has 3,984 tweets flagged as suicidal. For each user, we grabbed their Twitter timeline, which covers a decade from 2009 to 2019. On average, a user's history has 748 tweets, but some have way more—up to 3,200—with a standard deviation of 789 tweets. For users with a ton of tweets, we cut it down to just the 100 most recent ones. The time between tweets varies a lot—on average, it's about two days, but the standard deviation is almost 24 days, showing how different people tweet at different rates. Oh, and by the way, 4,070 users didn't have any historical tweets at all.",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal: this process focused on tagging individual tweets as potentially risky for suicide, not the whole history of a user's tweets. We"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. First, we ran a special test called the Tukey HSD test with 5,000 tries to see if there's a real difference in the mean scores, using a 0.05 threshold for significance. The results are in Table 3, parts (a) and (c), which show how consistent the measures are when we use OC tasks. For instance, in part (a), we randomly split 100 topics from Sem16T4C into two groups 1,000 times, and  (kappa) clearly did better than the others, which is marked with a \".\" Now, in Table 3, parts (b) and (d), we did similar tests but with only 10 topics in each group to check how well the measures hold up with smaller groups. If you average the results from (a) and (c), the top performers are the two 's and , while the bottom two are CEM ORD and Accuracy. The same pattern shows up if you average (b) and (d). So, even though Amigó et al. (2020) said CEM ORD was pretty robust, our tests don't back that up.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. First, we ran a special test called the Tukey HSD test with 5,000 tries to see if there'"
    },
    {
        "casual_text": "Event detection (ED) is a big deal in natural language processing. It's all about spotting event triggers in text. For instance, if you have a sentence like, \"A man died when a tank fired on the hotel,\" ED would need a system to pick out the words \"died\" and \"fired\" as event triggers, and also figure out that they belong to the types \"Die\" and \"Attack.\" This has been studied by folks like Ahn in 2006 and Nguyen and Grishman in 2015.",
        "formal_text": "Convert casual text to formal text: Event detection (ED) is a big deal in natural language processing. It's all about spotting event triggers in text. For instance, if you have a sentence like,"
    },
    {
        "casual_text": "We took the model that scored the best F1 score on the Switchboard dev set and tested it on some out-of-domain data without tweaking it. Check out Table 5—our unsupervised model did pretty well across the board, including on the Switchboard set and three other domains. It’s kind of cool because, unlike the results in Table 3 for the Switchboard dev set, our unsupervised model performs almost as well as the ELECTRA-Base fine-tuning model. This shows that our model is pretty solid when it comes to testing in different domains. We think it’s because our method uses a ton of unlabeled news data and ASR outputs, which helps it handle the domain mismatch issue during cross-domain testing. Even when compared to the supervised Patternmatch model (Zayats and Ostendorf, 2018), which is known for being top-notch in cross-domain scenarios, our model holds its own and performs competitively.",
        "formal_text": "Convert casual text to formal text: We took the model that scored the best F1 score on the Switchboard dev set and tested it on some out-of-domain data without tweaking it. Check out Table 5—our un"
    },
    {
        "casual_text": "Okay, so a parser has a few key parts: 1. **A lexicon**: This gives you all the possible ways a word can be used—its syntax and meaning—and also tells you when it’s appropriate to use each one. Plus, it links the word to the typical \"frames\" it’s part of. 2. **A frame dictionary**: This explains the different types of elements that can fit into a frame and how they relate to each other within that frame. 3. **Rules for logic and interpretation**: These help connect different frames together, which is useful for building a system that shows how a text develops over time. 4. **A list of active frames**: This keeps track of the frames currently being used based on themes, interpretations, or inferences in the text.",
        "formal_text": "Convert casual text to formal text: Okay, so a parser has a few key parts: 1. **A lexicon**: This gives you all the possible ways a word can be used—its syntax and meaning"
    },
    {
        "casual_text": "We compare models that were trained on different amounts of similar and different WikiMatrix samples. We start with the examples marked as different in section 3.2. Then, we pick the most detailed differences by using the bicleaner score (Ramrez-Sánchez et al., 2020) set at 0.5, 0.7, and 0.8. Check out section A for more info.",
        "formal_text": "Convert casual text to formal text: We compare models that were trained on different amounts of similar and different WikiMatrix samples. We start with the examples marked as different in section 3.2. Then, we pick the most"
    },
    {
        "casual_text": "It might look like the super simple TURN architecture isn't cut out for anything more than basic tasks. But actually, that's not true at all. Our first experiment used a TURN for a natural language agreement task from Linzen et al. (2016). This task involves predicting whether third person verbs in English text should be singular or plural, with some supervised training. For example, in the sentence \"The keys to the cabinet are on the table\", the RNN is trained to pick the plural \"are\" instead of the singular \"is\".",
        "formal_text": "Convert casual text to formal text: It might look like the super simple TURN architecture isn't cut out for anything more than basic tasks. But actually, that's not true at all. Our first experiment used a"
    },
    {
        "casual_text": "The third type of approach tries to mix internal features of words with context info, like what Lu (2005) and Goh et al. (2006) did. Lu (2005) came up with a hybrid model that blends a rule-based system with two statistical models to guess the part-of-speech (POS) tags of unknown Chinese words. The rule-based part has 35 hand-crafted rules focusing on the type, length, and structure of unknown words. The two statistical models use context clues and the chances of a character appearing in a certain spot within a word of a specific length and POS category. One of these models is based on Wu and Jiang's (2000) work. This setup got a precision of 89.00%, which is a big jump from the previous best result of 69.00%. Meanwhile, Goh et al. (2006) suggested a method to guess POS tags for unknown words by using context and internal features with maximum entropy models. Both Lu (2005) and Goh et al. (2006) stick to local context and don’t consider the bigger picture. When it comes to internal features, Lu (2005) only uses the word category in his rule-based model, while Goh et al. (2006) focus on the first and last characters. From these studies, it seems like methods using internal features are pretty effective, but there’s still a lot of room for improvement and more research in this area.",
        "formal_text": "Convert casual text to formal text: The third type of approach tries to mix internal features of words with context info, like what Lu (2005) and Goh et al. (2006) did. Lu (2005) came up with a"
    },
    {
        "casual_text": "This happened because the verb reordering task is kind of tricky. The main idea was to place the verb between the subject and the object, but figuring out exactly where the subject ends and the object begins is really tough. The parser often struggles with this. Check out Table 4 for how adding reordered Farsi sentences and their Arabic versions to the training data affected things (the reordering was done using run-based penalties during the search).",
        "formal_text": "Convert casual text to formal text: This happened because the verb reordering task is kind of tricky. The main idea was to place the verb between the subject and the object, but figuring out exactly where the subject ends and the"
    },
    {
        "casual_text": "In this case, the person trying to figure out where the fake text came from (let's call them the \"attributor\") trains a special tool called a classifier. This classifier uses a language model (LM) that’s either pre-trained or tweaked for the job. The twist here is that the person creating the fake text (the \"adversary\") might use different settings for generating the text than what the attributor used to train their classifier. To break it down: There are n different pre-trained or fine-tuned LMs (let's call them M1, M2, ..., Mn). The attributor wants to train a classifier that can correctly identify which of these models was used to create a piece of text. The adversary, however, uses one of these models (let’s say Mk, where k is any number from 1 to n) with specific settings (Sk) that the attributor doesn’t know about. The attributor’s job is to guess which model Mk was used to generate the text based on what they’ve trained their classifier to recognize.",
        "formal_text": "Convert casual text to formal text: In this case, the person trying to figure out where the fake text came from (let's call them the \"attributor\") trains a special tool called a classifier. This"
    },
    {
        "casual_text": "• In Task 1, disagreements don't really spread much—it's only around 0.05 to 0.1 for the other tasks.",
        "formal_text": "• In Task 1, disagreements don't really spread much—it's only around 0.05 to 0.1 for the other tasks. • In Task 1, disagreements don't really spread much—it's only around 0.05 to"
    },
    {
        "casual_text": "Basically, the task here is to find pairs of (O, T), where O is a chunk of words that represents an opinion, and T is another chunk of words that represents the thing the opinion is about. So, O could be something like \"really great\" and T could be \"the service.\" The pair (O, T) just means that the opinion \"really great\" is aimed at \"the service.\" Both O and T can be made up of more than one word.",
        "formal_text": "Convert casual text to formal text: Basically, the task here is to find pairs of (O, T), where O is a chunk of words that represents an opinion, and T is another chunk of words that represents the thing the"
    },
    {
        "casual_text": "For this case study, we used this method to check Importance Alignment in eight NLI models—six transformer models, an LSTM, and a BOW model. We based our analysis on explanations from the ANLI dataset made by annotators. What we found was pretty interesting: the basic versions of the transformer models had way better importance alignment compared to their bigger versions. Among them, BERT-base had the highest alignment. But here's the catch—smaller models don't always mean better alignment. BERT-base, for example, had higher alignment than both the LSTM and BOW models. Also, we noticed that importance alignment scores didn't really match up with how accurate the models were on ANLI or the HANS dataset in most cases. This tells us that importance alignment and accuracy are kind of like two different ways to judge how good a model is.",
        "formal_text": "Convert casual text to formal text: For this case study, we used this method to check Importance Alignment in eight NLI models—six transformer models, an LSTM, and a BOW model. We"
    },
    {
        "casual_text": "In the Generation Track, each model creates its own set of KPs, so there isn't a single standard set to follow. Along with the official evaluation metrics, two extra scores are included: the average of the strict and relaxed mAP values, and p@50% for the strict view. Submissions that included descriptions are marked with (*). Figure 2 shows: (a) How KPs are spread across arguments for the pro stance on the topic \"Social media platforms should be regulated by the government.\" This is for the expert ground truth (strict view) and selected models, normalized by the number of arguments that match a KP. The JSd compared to the expert distribution is in brackets. (b) JSd rank vs. AP rank for all models in the Matching Track, per topic and stance. The topic id and topic text mapping isn't relevant for comparing KP distributions, so we can't directly compare them like we did above. But, we can still do a qualitative, anecdotal analysis. Table 8 in the appendix lists the KPs generated for one topic and stance by the expert and different models. The expert made their KPs beforehand without seeing the actual arguments, while the models generated KPs based on the input arguments. Looking at the generated KPs, it seems like the expert might have missed some.",
        "formal_text": "Convert casual text to formal text: In the Generation Track, each model creates its own set of KPs, so there isn't a single standard set to follow. Along with the official evaluation metrics, two extra scores"
    },
    {
        "casual_text": "D is just a way to measure how different two probability distributions are, and we're using something called Kullback-Leibler divergence for that. The main goal, L sup, for supervised learning is basically the combination of two things: L ce and L com.",
        "formal_text": "Convert casual text to formal text: D is just a way to measure how different two probability distributions are, and we're using something called Kullback-Leibler divergence for that. The main goal, L"
    },
    {
        "casual_text": "We didn’t find any German verbal affixoids in our literature review. That’s not really surprising, though, because German doesn’t really do much with verb compounding either. Verbs can only show up as parts of compound words when they’re modifying nouns or adjectives, like in Bratpfanne (“frying pan”) or waschecht (“colorfast”—literally “wash-true”) (Olsen, 2000). We did come across 40 possible nominal prefixoids in the stuff we looked at, though. Just a heads-up: these lists might not be totally complete. The items in the literature are usually found through people thinking about them rather than being pulled from actual data.",
        "formal_text": "Convert casual text to formal text: We didn’t find any German verbal affixoids in our literature review. That’s not really surprising, though, because German doesn’t really do much with verb compounding either."
    },
    {
        "casual_text": "We’re all about figuring out how the plot and the ending connect with each other, so we can pull out useful features from them. This whole idea was inspired by the IIN model (shoutout to Gong et al., 2018). Basically, we mix the plot and ending stuff together to make this fancy word-by-word interaction thing, where each part shows how words from one side relate to the other. We’ve also experimented with different ways of handling this, like...",
        "formal_text": "Convert casual text to formal text: We’re all about figuring out how the plot and the ending connect with each other, so we can pull out useful features from them. This whole idea was inspired by the IIN model (s"
    },
    {
        "casual_text": "For our first experiment, we looked at 3,266 forum discussions from lonelyplanet.com, which is like the biggest travel blog site ever, with forums for pretty much every possible travel spot. We figured out how to use this data to compare and summarize stuff, and we also showed how our method works better than what people have done before with similar data. Then, in the second experiment, we compared blogs written by locals and ran our model on 7,388 English-language blogs from three different countries. This was a cool way to see if we could automatically spot cultural differences.",
        "formal_text": "Convert casual text to formal text: For our first experiment, we looked at 3,266 forum discussions from lonelyplanet.com, which is like the biggest travel blog site ever, with forums for pretty much every possible travel spot."
    },
    {
        "casual_text": "Use a swarm intelligence software routine to fine-tune the translations that made it through the last step. The goal here is to fix the \"infection\" by getting rid of the biggest issues.",
        "formal_text": "Convert casual text to formal text: Use a swarm intelligence software routine to fine-tune the translations that made it through the last step. The goal here is to fix the \"infection\" by getting rid of the"
    },
    {
        "casual_text": "Radford and team (2019) came up with OpenAI GPT-2, a model that uses a unidirectional transformer and is trained to predict the next word in a sentence. We're looking at the \"gpt2\" model that's available on Hugging Face Transformers. GPT-2 was trained using a dataset called Web-Text, which is basically a collection of links from Reddit that have at least 3 karma points. The creators of the model (who also made the dataset) think this is a good way to judge the quality of the links. WebText has over 8 million documents and 40GB of text in total.",
        "formal_text": "Convert casual text to formal text: Radford and team (2019) came up with OpenAI GPT-2, a model that uses a unidirectional transformer and is trained to predict the next word in a sentence. We"
    },
    {
        "casual_text": "From Figure 4, you can see that the curve for personalization looks kind of like a U-shape in both the AOL log and the Sogou Log. As more users join a group, the performance of the best non-personalized rankings starts to drop, levels off, and then improves after a while. Just a heads-up, the left side of the curve looks a lot like what Teevan and his team (2007) found in their research.",
        "formal_text": "Convert casual text to formal text: From Figure 4, you can see that the curve for personalization looks kind of like a U-shape in both the AOL log and the Sogou Log. As more users join"
    },
    {
        "casual_text": "Tables 9 and 10 show how our algorithm boosts the performance of our QuALiM system, like you can see in (Kaisser et al., 2006). In Section 6 of this paper, we explain how answer candidates are ranked using formulas 2 and 3. This ranking is added to the existing QA system's ranking as an extra feature, helping to boost candidates based on their confidence score. The difference between the two tables is that Table 9 uses all 1658 questions in our test sets for evaluation, while Table 10 only looks at the 1122 questions where our system could learn a pattern. So, for Table 10, questions that the system couldn’t answer due to limited training data are left out. As you can see, accuracy@1 goes up by 4.9% on the full test set and by 11.5% on the smaller set. Keep in mind that the baseline QA system we’re comparing to has a couple of advantages: a) It has web-based components, so it can access way more text information. b) The algorithm we’re talking about in this paper is just for answer extraction. For paragraph retrieval, we use the same method as for evaluation set 1, which is explained in Section 7.1. But, in over 20% of cases, this method doesn’t return a single paragraph that has both the answer and at least one question keyword. In those cases, the basic paragraph retrieval makes it really hard for our algorithm to give the right answer.",
        "formal_text": "Convert casual text to formal text: Tables 9 and 10 show how our algorithm boosts the performance of our QuALiM system, like you can see in (Kaisser et al., 2006). In Section 6"
    },
    {
        "casual_text": "DRG is a new way of representing DRS graphs, created specifically for MRP 2020. The goal was to make it as similar as possible to other frameworks (Abzianidze et al., 2020). But there are already a few different ways to encode DRS data. For example, Liu et al. (2018) focused more on labeling edges than nodes in their DRG format. On the other hand, van Noord et al. (2018) worked with DRS in a clausal form, using sets of triples and quadruples. This latter approach is more common among DRS parsers, especially since it was the official format for the DRS parsing shared task (Abzianidze et al., 2019). That shared task led to several DRS parsers, including ones by Evang (2019), van Noord (2019), and Fancellu et al. (2019). The best results (with an F1 score of 0.85) came from a word-level sequence-to-sequence model using a Transformer. Just a heads-up: the F1 score for the DRS shared task was based on clausal forms, which isn't directly comparable to the MRP F1 score calculated over DRGs.",
        "formal_text": "Convert casual text to formal text: DRG is a new way of representing DRS graphs, created specifically for MRP 2020. The goal was to make it as similar as possible to other frameworks (Abzianidze"
    },
    {
        "casual_text": "In the superior temporal gyrus (STG), the effect flips for predictors based on Combinatory Categorial Grammar (CCG). This is the only brain region where Lopopolo et al. (2021) see an effect related to phrase structure processing, as opposed to dependency grammar processing. This might be because our version of CCG is lexicalized. Of course, the CCGbank grammar doesn't just focus on lexical dependencies; it also captures other aspects of sentence structure (as explained by Hockenmaier and Steedman in 2007). Shain et al. (2020) took a different approach, using a noncombinatory categorial grammar to model fMRI data. While their earlier work used the surprisal hypothesis to study predictive processing, our current study looks at the parsing steps needed to recover grammatical descriptions assigned by CCG. This difference can be thought of as the distinction between Marr's computational and algorithmic levels of analysis, which we touched on in Section 2. But beyond just the perspective, there are also conceptual differences that lead to different modeling approaches at both levels. For example, the generalized categorial grammar used by Shain et al. (2020) is very expressive and might go beyond what context-free grammars can handle. However, in their study, they first converted it into a probabilistic context-free grammar (PCFG) to calculate surprisal predictions. In our study, we skip that step and directly derive processing complexity predictions from CCG derivations by counting the number of nodes.",
        "formal_text": "Convert casual text to formal text: In the superior temporal gyrus (STG), the effect flips for predictors based on Combinatory Categorial Grammar (CCG). This is the only brain region where"
    },
    {
        "casual_text": "We're still using the scaling factor  to decide which score gets more importance.",
        "formal_text": "Convert casual text to formal text: We're still using the scaling factor  decide which score gets more importance. Convert casual text to formal text: We're still using the scaling factor  decide which score gets more importance"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: All the methods mentioned earlier can technically be used for any text classification task, but genres come with their own set of challenges. Techniques that automatically translate key words for prediction (like what Bel et al. did in 2003 or Prettenhofer and Stein in 2010) work pretty well for topics and sentiment analysis. That’s because these tasks usually focus on content words—things like nouns, adjectives, and adverbs. For instance, the word \"hospital\" might suggest a text about the medical field, and \"excellent\" could mean a positive review. These kinds of words are generally easier to translate, though there’s still some uncertainty sometimes. But when it comes to genres, things get trickier. They’re often identified using function words (Karlgren and Cutting, 1994; Stamatatos et al., 2000b)—words like \"of,\" \"it,\" or \"in.\" Trying to translate these without any context is basically impossible. And it gets even more complicated if languages have different ways of handling grammar, like how some languages use prefixes or suffixes instead of separate words.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: All the methods mentioned earlier can technically be used for any text classification task, but genres come with their own set of challenges. Technique"
    },
    {
        "casual_text": "We used a setup that's kind of similar to the LDA Gibbs sampler 4 by Phan and Nguyen from 2008.",
        "formal_text": "Convert casual text to formal text: We used a setup that's kind of similar to the LDA Gibbs sampler 4 by Phan and Nguyen from 2008. We used a setup that's kind of"
    },
    {
        "casual_text": "3. 3. 2 AGENDA CONTROL: Adding stuff to the agenda is limited by something called \"control-feature N.\" This N tells us where to check in the feature-trees, which are like the building blocks of our categories, to find the features that really matter when it comes to meaning.",
        "formal_text": "2. 2 AGENDA CONTROL: Adding stuff to the agenda is limited by something called \"control-feature N.\" This N tells us where to check in the feature-trees, which are like the building blocks of"
    },
    {
        "casual_text": "We did the same experiment as in section 3.2, but this time we used equation (3) as the objective function and set  to 1. In Fig. 1b, you can see the training loss going down steadily and getting really close to zero pretty quickly. If you compare it to the learning curve of the basic setup (shown in Fig. 1a), the large margin training method is way more stable. We also looked at the histogram of log values in Fig. 2b.",
        "formal_text": "Convert casual text to formal text: We did the same experiment as in section 3.2, but this time we used equation (3) as the objective function and set  to 1. In Fig. 1b, you can see the training loss"
    },
    {
        "casual_text": "For data augmentation, we used synonym replacement on the original dataset, doubling its size. Table 2 shows the results of using this approach on a generation task. We noticed that applying data augmentation to the generation task actually made things worse. But when we used the augmented data for the grounding task, the model scored a 40.55 F1 score and a 23.49 exact match score. This is better than our baseline model, which was trained only on the original grounding task data, by +0.5 F1 score and +0.64 exact match score. These findings suggest that using synonym data augmentation on the generation task's answers doesn't really help the model learn anything useful for the generation task. So, we decided to only use the augmented data for the grounding task during multi-task learning.",
        "formal_text": "Convert casual text to formal text: For data augmentation, we used synonym replacement on the original dataset, doubling its size. Table 2 shows the results of using this approach on a generation task. We noticed that applying data augmentation"
    },
    {
        "casual_text": "Basically, this means that something is considered recognized if we can get the empty string starting from S(0, n, n + 1, n + 1 + m). According to Theorem 1.2 from Boullier's 2000 paper, figuring out if a bottom-up non-erasing k-RCG can be recognized can be done in time O(|G|n).",
        "formal_text": "Convert casual text to formal text: Basically, this means that something is considered recognized if we can get the empty string starting from S(0, n, n + 1, n + 1 + m). According to"
    },
    {
        "casual_text": "This demo shows how RelationFactory works to come up with its results and points out where there's room for improvement in relation extraction. We think Relation-Factory is a good starting point for anyone interested in relation extraction, and we hope it can be used as a foundation for new progress in building knowledge bases.",
        "formal_text": "Convert casual text to formal text: This demo shows how RelationFactory works to come up with its results and points out where there's room for improvement in relation extraction. We think Relation-Factory is a"
    },
    {
        "casual_text": "In Figure 3a, you can see a t-SNE projection of the hidden states, which are labeled based on their depths. We also tried to pull out the stack elements from the hidden states of the network. For Dyck-2 samples with lengths between 2 and 50 and depths from 1 to 10, we trained an LSTM for the NCP task. At the same time, we trained some extra classifiers to guess each element of the stack up to a depth of 10. Basically, the LSTM hidden state that predicts the next valid characters is also used to predict the stack elements by running it through 10 different linear layers. We found that the model could not only predict the stack elements for a validation set from the same distribution but also for sequences with longer lengths (52 to 150) that it hadn’t been trained on (check out Figure 3b). This shows that LSTMs can handle longer sequences pretty well as long as the depth stays within limits. We also did some extra experiments to make sure the model wasn’t just memorizing the training data. You can find more details about these probing tasks and robustness experiments in the Appendix.",
        "formal_text": "Convert casual text to formal text: In Figure 3a, you can see a t-SNE projection of the hidden states, which are labeled based on their depths. We also tried to pull out the stack"
    },
    {
        "casual_text": "The idea behind how we recognize spoken words, called the abstract theory of spoken word recognition (SWR), suggests that there are two main steps in this process: a prelexical phase and a lexical phase (Scharenborg and Boves, 2010). In the prelexical phase, we deal with things like phonological units, which are basically the sounds we pick up from the raw acoustic signal—the actual sound waves of speech. These units are thought to get activated before we even start connecting them to the meanings of words, which happens in the lexical phase. By creating a computer model that mimics this process, we can test the theory and maybe even improve it by looking at how the model works and behaves.",
        "formal_text": "Convert casual text to formal text: The idea behind how we recognize spoken words, called the abstract theory of spoken word recognition (SWR), suggests that there are two main steps in this process: a prelexical phase and a"
    },
    {
        "casual_text": "With the growth of data-hungry Neural Machine Translation (NMT), projects like the OPUS data portal (Tiedemann, 2012), OpenSubtitles (Lison et al., 2018), and Wikimatrix (Schwenk et al., 2019) have been working to release more and more parallel data, including English-Indonesian pairs, in the millions. But as far as we know, no one has actually used this data for English-Indonesian machine translation yet. So, it's kind of unclear how helpful it really is in this case. Indonesian, or Bahasa Indonesia, is a standardized version of Malay and serves as the national language of the country, which has over 700 different local languages (Riza, 2008). Because of this, the everyday spoken Indonesian is pretty different from the official, standardized version. This difference comes from influences of local languages and also some popular foreign languages, like English or Arabic. This variation affects certain areas—for example, in casual conversations where people usually speak in a more informal way, or in religious contexts where Arabic words or phrases are sometimes used directly without translation. Recent approaches to English-Indonesian machine translation haven't really taken these different domains into account (Shahih and Purwarianti, 2016; Octoviani et al., 2019). Instead, they've mostly focused on the news domain, which tends to use the standardized Indonesian (Hermanto et al., 2015).",
        "formal_text": "Convert casual text to formal text: With the growth of data-hungry Neural Machine Translation (NMT), projects like the OPUS data portal (Tiedemann, 2012), OpenSubtitles (Lison"
    },
    {
        "casual_text": "Following the approach from Lample et al. (2016), we use both a forward and a backward LSTM to create a representation for each word based on its characters (check out the right side of Figure 3). We start with a character lookup table that's randomly set up and then train it along with the Bi-LSTM model we talked about in Section 4.1.",
        "formal_text": "Convert casual text to formal text: Following the approach from Lample et al. (2016), we use both a forward and a backward LSTM to create a representation for each word based on its characters ("
    },
    {
        "casual_text": "Recent developments in conditional generative adversarial networks (CGANs) show that by using class-aware information to condition both the generator and discriminator, we can get better alignment between two different distributions (Mirza and Osindero, 2014). The label predictions give us some useful discriminative info, which helps uncover the structure of the data distribution. This means that conditional adversarial learning can do a better job of handling differences between domains when it comes to shared features and label predictions. Unlike earlier work that focused on adapting marginal distributions (like Liu et al., 2017, and Chen and Cardie, 2018), our CAN framework is all about aligning joint distributions of shared features and label predictions. Our model has two training processes. Because of the nature of adversarial learning, the conditional domain discriminator gets updated using a separate optimizer, while the rest of the CAN components are trained with the main optimizer. These two processes are designed to work together and support each other. Let’s call the loss functions for the classifier C and the conditional domain discriminator D as L_C and L_D, respectively. We use the negative log-likelihood (NLL) loss to define these two loss functions.",
        "formal_text": "Convert casual text to formal text: Recent developments in conditional generative adversarial networks (CGANs) show that by using class-aware information to condition both the generator and discriminator, we can get better alignment between two"
    },
    {
        "casual_text": "1. SyntheticQE-MT: Basically, the MT models are used to create the target side of the synthetic data.",
        "formal_text": "Convert casual text to formal text: 1. SyntheticQE-MT: Basically, the MT models used to create the target side of the synthetic data. 2. Convert casual text to formal text: 1. SyntheticQE-"
    },
    {
        "casual_text": "Okay, so when we looked at the errors in our system, we found that for certain words, it couldn’t make a matching cluster for specific meanings. For example, words like \"yard\" when it refers to a ship, or the \"impound/enclosure\" meaning of \"pound\" didn’t get matched properly. It seems like this happened because these meanings don’t show up much in English Wikipedia. In fact, the \"yard\" meaning as a ship part only appeared twice, and the \"impound/enclosure\" meaning of \"pound\" showed up three times in the top 100 words. That’s why the system didn’t create a cluster for them.",
        "formal_text": "Convert casual text to formal text: Okay, so when we looked at the errors in our system, we found that for certain words, it couldn’t make a matching cluster for specific meanings. For example, words like \"yard"
    },
    {
        "casual_text": "The author did some experiments and found out that editing a machine-translated text isn't as hard as it might seem. In fact, it's pretty easy if you imagine the machine has already taken care of all the grammar stuff and maybe even some of those tricky phrases, and has put the text in the right order for the target language. There's an example of what a German-English machine translation might look like in the Appendix. The author suggests you try picking one option from each column, smooth it out a bit, and then compare your English sentence with someone else's version of the same German text on page 1 of the Appendix. Another fun thing to do would be to translate your English sentence back into German and see how it compares to the original.",
        "formal_text": "Convert casual text to formal text: The author did some experiments and found out that editing a machine-translated text isn't as hard as it might seem. In fact, it's pretty easy if you imagine the"
    },
    {
        "casual_text": "In Table A4, you'll find the numbers that are shown in the graph in Figure 4.",
        "formal_text": "Convert casual text to formal text: In Table A4, you'll find the numbers that are shown in the graph in Figure 4. In Figure 4, you'll find the numbers that are shown in the graph in Figure 4. In Table A4,"
    },
    {
        "casual_text": "Content-based recommendation algorithms work by comparing the content of items to find similarities with things the user has interacted with—like clicking on something or giving it a rating. They then suggest the items that are most similar. These algorithms don't rely on user behavior, so they’re not as affected by a lack of data. However, they might have trouble coming up with fresh recommendations that could spark new interests in the user, which is something recommender systems really want to do (Castells et al., 2011).",
        "formal_text": "Convert casual text to formal text: Content-based recommendation algorithms work by comparing the content of items to find similarities with things the user has interacted with—like clicking on something or giving it a rating. They then suggest the items"
    },
    {
        "casual_text": "Out of the 25 pairs, the judges both rated 10 of them. In 3 of those 10, the judges didn’t agree. But for the other 7, they both gave their ratings in favor of ccLDA.",
        "formal_text": "Convert casual text to formal text: Out of the 25 pairs, the judges both rated 10 of them. In 3 of those 10, the judges didn’t agree. But for the other 7, they both gave their ratings in favor of"
    },
    {
        "casual_text": "Okay, so P M could be either P M oS from equation 2 or P M P from equation 4. The set y 1, . . . , y T  includes the words whose embeddings are closest to f avg ct, and x 1, . . . , x T  is the set from Table 9. Table 9 compares the perplexity of different models when dealing with similar or dissimilar words. These models are based on GPT-2 Small and were trained using OpenWebText.",
        "formal_text": "Convert casual text to formal text: Okay, so P M could be either P M oS from equation 2 or P M P from equation 4. The set y 1, . . . , y T"
    },
    {
        "casual_text": "We did a quick check to see how our fine-tuning worked. Table 4 (check the Appendix for more) shows some examples of how the translations got better after we made those adjustments.",
        "formal_text": "Convert casual text to formal text: We did a quick check to see how our fine-tuning worked. Table 4 (check the Appendix for more) shows some examples how the translations got better after we made those"
    },
    {
        "casual_text": "So, these findings suggest that the window size for rank N across the whole dataset and the 4th and 5th features are pretty important. They seem to stand out and play a significant role in the overall analysis.",
        "formal_text": "Convert casual text to formal text: So, these findings suggest that the window size for rank N across the whole dataset and the 4th and 5th features are pretty important. They seem to stand out and play a significant role in the"
    },
    {
        "casual_text": "Okay, so let’s say we have a pair of sentences, (x, y), where x is a sentence in some input (source) language, and y is its translation in a different output (target) language. X and Y represent the vocabularies for the input and output languages, respectively. The IBM alignment models (from 1 to 5) are basically ways to calculate the probability P(x|y), which tells us how likely it is that x is the original sentence that got translated into y.",
        "formal_text": "Convert casual text to formal text: Okay, so let’s say we have a pair of sentences, (x, y), where x is a sentence in some input (source) language, and y is its"
    },
    {
        "casual_text": "Even though each part of the system plays a role in creating coordinate constructions, most of the coordination work happens in the sentence planner and the lexical chooser. These two parts handle the main tasks of dealing with coordination conjunctions: the sentence planner spots recurring elements in the coordinated sentences, and the lexical chooser decides which of these recurring elements to leave out. This split happens because ellipsis (leaving stuff out) depends on the order of these recurring elements in the final sentence. That order isn’t clear until after the sentence structure and word choices are figured out. For instance, in the sentence \"On Monday, John rearranged cereals in Aisle 2 and cookies in Aisle 4,\" the second \"on Monday\" part is left out. But in \"John rearranged cereals in Aisle 2 and cookies in Aisle 4 on Monday,\" the first \"on Monday\" is the one that gets deleted. CASPER, the system, just marks the recurring elements and lets the lexical chooser handle the actual deletion later. If you want more details, check out Section 5.",
        "formal_text": "Convert casual text to formal text: Even though each part of the system plays a role in creating coordinate constructions, most of the coordination work happens in the sentence planner and the lexical chooser. These two parts handle the main"
    },
    {
        "casual_text": "We’ve come up with a cool and efficient setup for tracking the whole relaxation process of maximum entropy problems. Right now, we’re diving into natural language processing stuff. Specifically, we’re working on homotopy methods for domain adaptation (like Blitzer did in 2008) and language modeling using context tree weighting (as Willems et al. suggested back in 1995). We’re also looking into expanding our approach by swapping the relative entropy objective with a separable Bregman function (thanks to Censor and Zenios in 1997). This tweak could help us connect more with other homotopy methods, like the least angle regression algorithm by Efron et al. (2004) and general homotopy methods for the Lasso (Osborne et al., 2000). On top of that, we’re planning to explore separable Bregman functions to find entire path solutions for less common objectives, such as the Itakura-Saito spectral distance (Rabiner and Juang, 1993) and other distances that are super handy for natural language processing.",
        "formal_text": "Convert casual text to formal text: We’ve come up with a cool and efficient setup for tracking the whole relaxation process of maximum entropy problems. Right now, we’re diving into natural language processing stuff. Specifically"
    },
    {
        "casual_text": "In this paper, we look at how word-order rules can be used in a simple EBMT system. Since we're working with Romanian, which doesn't have a lot of resources, we try to keep things as resource-free as we can. The system mainly uses surface-level patterns and stats from a corpus. That's why we stick to ideas from linear and template-based EBMT methods.",
        "formal_text": "Convert casual text to formal text: In this paper, we look at how word-order rules can be used in a simple EBMT system. Since we're working with Romanian, which doesn't have a lot"
    },
    {
        "casual_text": "Sometimes, you come across spelling mistakes that are pretty obvious, like typing \"ffentlivchendienst\" instead of \"öffentlichen Dienst.\" These are what we call typos, and they happen when someone knows the word but just hits the wrong key by accident. So, if someone makes a typo like this, it should still be counted as correct because they actually know the word. To figure out if a mistake is a typo, we can check if the wrong letter is next to the right one on the keyboard.",
        "formal_text": "Convert casual text to formal text: Sometimes, you come across spelling mistakes that are pretty obvious, like typing \"ffentlivchendienst\" instead of \"öffentlichen Dienst.\" These are what we call typos, and they happen when"
    },
    {
        "casual_text": "Alright, so when we're dealing with discourse and need to figure out the exact way things are put together (syntax), the part that handles meaning (semantics) might need to revisit and tweak the syntactic structure to make sure it matches how the PPs (prepositional phrases) are finally attached.",
        "formal_text": "Convert casual text to formal text: Alright, so when we're dealing with discourse and need to figure out the exact way things are put together (syntax), the part that handles meaning (semantics) might"
    },
    {
        "casual_text": "Here’s what we learned: (1) In areas with a smaller set of facts, like radiology reports, a well-designed IE system can help make neural summarization models more accurate by using reinforcement learning (RL). (2) Even if you don’t have a solid IE system, focusing on improving ROUGE scores with RL can still make the summaries more factually correct.",
        "formal_text": "Convert casual text to formal text: Here’s what we learned: (1) In areas with a smaller set of facts, like radiology reports, a well-designed IE system can help make neural summarization models more accurate"
    },
    {
        "casual_text": "This method assigns a weight to each paragraph, showing how likely it is to be related to other paragraphs. The weight takes into account things like the paragraph's length, how big and how often certain word groups (n-grams) show up in it, and how those n-grams are spread out across the whole text.",
        "formal_text": "Convert casual text to formal text: This method assigns a weight to each paragraph, showing how likely it is to be related to other paragraphs. The weight takes into account things like the paragraph's length, how big and how"
    },
    {
        "casual_text": "Alright, so the second extra dataset we made came from pulling out all the split-antecedent examples from the raw annotations in PD to make sure we got as many as possible. After grabbing all those split-antecedent annotations, we used majority voting to sort things out when people didn’t agree. This gave us 47.7k split-antecedent annotations tied to 6.2k mentions (check Table 1 for the details). We tested how good this method was using the gold part of the PD corpus, and it turned out pretty well—91.7% recall, which is exactly what we wanted. But, as you’d expect, the data’s a bit messy, with only 11.1% precision and an F1 score of 19.7%. We went through the false positives manually and found most of the errors were because of three things: single-antecedent coreference (where the chains were marked as split-antecedents), bridging references (which didn’t need to be annotated), and other annotation mistakes. Now, the first two types of mistakes aren’t a big deal for our task because our third and fourth datasets are built using those kinds of relationships. Oh, and another thing—ARRAU also has bridging references, and one of the bridging relations they annotated, element-of (and its inverse), is super relevant to figuring out split-antecedent plurals.",
        "formal_text": "Convert casual text to formal text: Alright, so the second extra dataset we made came from pulling out all the split-antecedent examples from the raw annotations in PD to make sure we got as many as possible. After"
    },
    {
        "casual_text": "We're looking at how MCTS stacks up against Non-Guided Data Generation (NGDG), which is a method that doesn't use the learning classifier's knowledge when creating new data. In this case, every new example starts with a token that marks the beginning of a sentence, bos>.",
        "formal_text": "Convert casual text to formal text: We're looking at how MCTS stacks against Non-Guided Data Generation (NGDG), which is a method that doesn't use the learning classifier's knowledge when"
    },
    {
        "casual_text": "The idea behind using semantics in computers is to take bits of a natural language and give them meanings that computers can work with. This is how all computational uses of semantics work. For instance, when you're querying a database, you ignore sentences with actual quantifiers and stick to simple sentences or ones that pretend to have quantifiers, like \"for all\" meaning \"for all the individuals in the database.\"",
        "formal_text": "Convert casual text to formal text: The idea behind using semantics in computers is to take bits of a natural language and give them meanings that computers can work with. This is how all computational uses of semantics work. For instance"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: We've got some math stuff here, but I'll try to explain it in plain English. First, there's this equation: y x y x T T D T T 2 2      (7) It looks complicated, but it's just saying that we're dealing with some variables (y, x, T, D) and doing some operations on them, like adding, subtracting, and multiplying. The (7) at the end is just a reference number for this equation. Next, there's this part:   12 ) ( 1 3 3       x n i i i x t t N N T This is saying that for each value of x, we're taking the difference between some terms (t and N), and then summing them up. The 12 at the beginning is another reference number, and the  means we're adding everything together. Finally, there's this:   12 ) ( 1 3 3       y n j j j y t t N N T This is similar to the previous part, but now we're doing the same thing with y instead of x. So, for each y, we're finding the difference between t and N, and then summing those differences. In short, these equations are about calculating differences and sums for x and y, with some reference numbers thrown in for good measure.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: We've got some math stuff here, but I'll try to explain it in plain English. First, there's"
    },
    {
        "casual_text": "The Text+Berg dataset (Bubenhofer et al., 2015) includes old meeting records from the Schweizer Alpenclub (Swiss Alpine Club) from 1864 to 1899. These records are in Swiss German and French. The documents have been digitized, and any mistakes from the OCR process were fixed by hand. The collection has 19,024 pages in total, with 17,186 in Swiss German and 1,838 in French. We found 88,302 unique words that were misread by the OCR and matched them with their correct versions.",
        "formal_text": "Convert casual text to formal text: The Text+Berg dataset (Bubenhofer et al., 2015) includes old meeting records from the Schweizer Alpenclub (Swiss Alpine Club) from 1864 to"
    },
    {
        "casual_text": "Grammatical gender is a thing in some languages, like a way of categorizing nouns (Seifart, 2010). In languages that use it, words like articles and adjectives have to match the noun they’re describing based on its gender (Corbett, 1991, 2001). For example, Swedish has a two-gender system—common and neuter. When you use articles or adjectives, they need to agree with the noun’s gender. Like, in Swedish, you’d say *ett stort äpple* (a big apple) for a neuter noun and *en stor häst* (a big horse) for a common noun.",
        "formal_text": "Convert casual text to formal text: Grammatical gender is a thing in some languages, like a way of categorizing nouns (Seifart, 2010). In languages that use it, words like articles and"
    },
    {
        "casual_text": "Sure! Here's the informal version: - fuzzywuzzy: https://pypi.org/project/fuzzywuzzy/ - gTTS: https://pypi.org/project/gTTS/ - DeepSpeech by PaddlePaddle: https://github.com/PaddlePaddle/DeepSpeech",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: - fuzzywuzzy: https://pypi.org/project/fuzzywuzzy/ - gTTS: https"
    },
    {
        "casual_text": "Wow, this text is completely scrambled and looks like a random mix of symbols and letters. It seems like it's been encrypted or encoded in some way. If you're trying to decode it or make sense of it, you might need to use a specific key or method to unlock the original message. It could be a code, a cipher, or even a mistake in formatting. If you have any additional context or know what this is supposed to be, I can try to help you figure it out! Otherwise, it’s a bit like trying to read a foreign language without a dictionary. Let me know if you need help with anything else!",
        "formal_text": "Convert casual text to formal text: Wow, this text is completely scrambled and looks like a random mix of symbols and letters. It seems like it's been encrypted or encoded in some way. If you're"
    },
    {
        "casual_text": "• Thinking about the ability to get a basic grasp of the language stuff.",
        "formal_text": "Convert casual text to formal text: • Thinking about the ability to get a basic grasp of the language stuff."
    },
    {
        "casual_text": "Sure, no problem! Let me break it down in a more casual way: Even though people are usually cooperative, real-life conversations are full of things like being vague, underplaying stuff, or even telling little white lies. This means we’d need to figure out the difference between what someone actually says and what they really mean, both for the speaker and the listener. But in this study, we’re keeping it simple—we’re only looking at cases where there aren’t any of those tricky parts. We’re assuming that what someone says is exactly what they mean, and the listener gets it just like that. The way we show what’s said in the study looks like this:",
        "formal_text": "Convert casual text to formal text: Sure, no problem! Let me break it down in a more casual way: Even though people are usually cooperative, real-life conversations are full of things like being vague, underplaying stuff,"
    },
    {
        "casual_text": "The MultiWOZ experiments show that our model beats a bunch of top-notch methods, both in automatic tests and when people check it out.",
        "formal_text": "Convert casual text to formal text: The MultiWOZ experiments show that our model beats a bunch of notch methods, both in automatic tests and when people check it out. Convert casual text to formal text: The MultiWO"
    },
    {
        "casual_text": "Next, doctors aren’t just curious about how many times mistakes pop up in a conversation—they also want to know what specific mistakes happen and where. To tackle this, we’ll calculate precision, recall, and the F1 score based on the number of each type of error in every conversation. We’ll call this way of checking things the ERROR level. Figure 1 shows both the UTTERANCE and ERROR level evaluations. Just a heads-up: the utterance level error code [EU] can only show up once per conversation. So, if we predict more than one [EU] code, we’ll just skip the extras.",
        "formal_text": "Convert casual text to formal text: Next, doctors aren’t just curious about how many times mistakes pop up in a conversation—they also want to know what specific mistakes happen and where. To tackle this, we’ll calculate"
    },
    {
        "casual_text": "The \"w/ exit\" option in Table 1, which stands for the early exit mechanism, gives us a handy way to balance speed and accuracy. When we turn on early exit, both SkipBERT 6+6 and SkipBERT 6+4 get a nice boost in how fast they work, with just a tiny drop in accuracy. We'll dive deeper into this in Section 4.5.",
        "formal_text": "Convert casual text to formal text: The \"w/ exit\" option in Table 1, which stands for the early exit mechanism, gives us a handy way to balance speed and accuracy. When we turn on early exit, both SkipBERT"
    },
    {
        "casual_text": "In this part, we're sharing the results of two experiments we did to test out our new method.",
        "formal_text": "Convert casual text to formal text: In this part, we're sharing the results we're sharing two experiments we're sharing out our new method. Convert casual text to formal text: In this part, we're sharing"
    },
    {
        "casual_text": "It's helpful to notice that the highlighted instructions are for the same step because it helps you understand things like ingredient swaps, how the same step can be explained or done differently, and how you can mix up the order of steps without messing up the final result.",
        "formal_text": "Convert casual text to formal text: It's helpful to notice that the highlighted instructions are for the same step because it helps you understand things like ingredient swaps, how the same step can be explained or done differently, and how you can"
    },
    {
        "casual_text": "To check how well our optimization process works, we ran all three steps of it on all 13 datasets from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). Table 1 shows the labeled attachment scores with the default settings and after each of the three optimization phases, along with the difference between the final setup and the default. First off, the optimization boosts parsing accuracy for every language, no exceptions. The improvement ranges from about 1 percentage point for languages like Chinese, Japanese, and Swedish to 8-9 points for Dutch, Czech, and Turkish. For most languages, the biggest jump comes from feature selection in phase 3. However, for languages with lots of non-projective dependencies—like Czech, Dutch, and Slovene—phase 2 also makes a big difference, especially when it comes to choosing the right parsing algorithm. The time it takes to run the optimization depends on the dataset size. It’s about half an hour for smaller sets but can take up to a day for really big ones, like the Czech dataset.",
        "formal_text": "Convert casual text to formal text: To check how well our optimization process works, we ran all three steps of it on all 13 datasets from the CoNLL-X shared task on multilingual dependency parsing (Buchholz"
    },
    {
        "casual_text": "In this paper, we talk about how context plays a big role in figuring out if emotions and their causes are actually connected. To tackle this, we came up with a new task: figuring out if an emotion and its cause are linked in a given context. We made a dataset for this by manually labeling stuff and using negative sampling from the ECPE dataset. Plus, we created a prediction aggregation module (PAM) that’s not too heavy on the computer, so models can tweak their final answer based on the type of emotion-cause pair in a document. The experiments show that our PAM works well and can be used in different setups.",
        "formal_text": "Convert casual text to formal text: In this paper, we talk about how context plays a big role in figuring out if emotions and their causes are actually connected. To tackle this, we came up with a new task:"
    },
    {
        "casual_text": "We take the [class] token embedding to represent the visual region. So, the image I is basically represented like this:",
        "formal_text": "Convert casual text to formal: We take the [class] token embedding to represent the visual region. So, the image I is basically represented like this: [class] token embedding to represent the visual region. So,"
    },
    {
        "casual_text": "In section 3, we talk about the main thing the corpus has going for it. Then, in section 4, we dive into some more stats and mention two related projects. Finally, section 5 wraps things up by highlighting the key wins from the work.",
        "formal_text": "Convert casual text to formal text: In section 3, we talk about the main thing the corpus has going for it. Then, in section 4, we dive into some more stats and mention two related projects. Finally, section 5 wrap"
    },
    {
        "casual_text": "Most of these methods simplify the process so they can use SEQ2SEQ (Yin and Neubig, 2018; Guo et al., 2019b). At each step, the decoder either picks a parsing action or a production rule, and by the end, you get a grammatically correct MR. These methods create different derivations by changing up the grammars. For example, NSM uses a simplified version of Lisp syntax. TRANX (Yin and Neubig, 2018) uses a language called Abstract Syntax Description Language to define its grammars, while IRNET (Guo et al., 2019b) works with a context-free grammar from a language named SemQL.",
        "formal_text": "Convert casual text to formal text: Most of these methods simplify the process so they can use SEQ2SEQ (Yin and Neubig, 2018; Guo et al., 2019b). At each step, the"
    },
    {
        "casual_text": "For the word embeddings in BERTScore and BERTScore++, we went with the contextualized embeddings from the 9th layer of the multilingual BERT (as mentioned by Devlin et al. in 2019). In BERTScore++, we set \"a\" to 0.8 and \"\" to 0.01.",
        "formal_text": "Convert casual text to formal text: For the word embeddings in BERTScore and BERTScore++, we went with the contextualized embeddings from the 9th layer of the multilingual BERT ("
    },
    {
        "casual_text": "Okay, so here's how we handle training: We stick a linear output layer on each intermediate layer of the pre-trained BERT/ALBERT model to act as an internal classifier. To avoid peeking at the GLUE tasks' dev set info, we tune the hyperparameters using cross-validation on the training set. For the grid search, we try out batch sizes of 16, 32, 128, and learning rates of 1e-5, 2e-5, 3e-5, 5e-5 for the model parameters . For the learnable weights , we test learning rates of 1e-5, 1e-4, 1e-3, 5e-3. The cross-level cycle length C will be picked from 2, 4, or 8. We’ll use the Adam optimizer for this. At each epoch, we randomly split the training set into two equal parts, D1 and D2. We also have an early stopping rule with a patience of 5, meaning we’ll stop if the model doesn’t improve for 5 epochs. After each epoch, we check the model’s performance on the dev set. The dev performance of our early exiting setup is calculated as the average performance across all exits. Finally, we’ll pick the model that performs best on average during cross-validation.",
        "formal_text": "Convert casual text to formal text: Okay, so here's how we handle training: We stick a linear output layer on each intermediate layer of the pre-trained BERT/ALBERT model to act as an internal class"
    },
    {
        "casual_text": "So, we’ve got this system that works for two things: intentional searches where you’re looking for similar words, and also for when you accidentally type something wrong. This makes our dictionary way more user-friendly. It’s kind of like the FOKS dictionary interface (Bilac, 2002), which helps fix mistakes in searches, especially when you’re looking up words based on how they’re written. Let’s say, for instance, someone wants to find the word , which means \"festival float,\" but they’re not sure how to pronounce it. With FOKS, they can try to guess the pronunciation by using readings they know for each character in other words. So, they might think,  [yama] means \"mountain\" and  [kuruma] means \"car,\" so they guess the word is pronounced [yamakuruma]. Now, the real pronunciation is [dashi], which you wouldn’t guess just from the parts, but our system would still help them find the word and give them both the right pronunciation and the meaning.",
        "formal_text": "Convert casual text to formal text: So, we’ve got this system that works for two things: intentional searches where you’re looking for similar words, and also for when you accidentally type something wrong. This makes our dictionary way more"
    },
    {
        "casual_text": "So, take sentence (2a), where we use the comparative form \"taller\" to compare how tall two people are. Then there's (2b), which mentions a specific height, like 5 feet. And (2c) uses the basic form of the adjective \"tall,\" which kind of implies a comparison with some hidden standard. In degree-based semantics, these gradable adjectives are seen as two-part things that involve an entity and a degree (Cresswell, 1976). For example, (2b) would be analyzed as \"tall(john, 5 feet).\"",
        "formal_text": "Convert casual text to formal text: So, take sentence (2a), where we use the comparative form \"taller\" to compare how tall two people are. Then there's (2b), which mentions a specific height"
    },
    {
        "casual_text": "Alright, so the experts gave us some tips on how to do this task really well. Here's what they said: 1. **True Counter Speech**: This one: - Uses a line from the speech it's responding to. - Talks about a specific point or argument from that speech. - Covers more stuff from the speech than the other options. - Hits the points in the same order they come up in the speech. - Talks about similar topics. - Focuses on the main issue in the speech. 2. **Elimination**: This means getting rid of the other options because they talked about things that weren't even in the original speech. 3. **Guess and Other**: These are for when you have to just wing it and write something in your own words. So, that's the gist of what the experts said. Pretty straightforward, right?",
        "formal_text": "Convert casual text to formal text: Alright, so the experts gave us some tips on how to do this task really well. Here's what they said: 1. **True Counter Speech**: This one: - Uses"
    },
    {
        "casual_text": "TESLA-M has an advantage over the earlier metrics because it uses lightweight linguistic features like lemmas, synonym dictionaries, and POS tags. Even though these tools are often available for languages other than English, they do make it a bit more challenging to adapt TESLA-M for non-English languages. TESLA-M performed really well in the WMT 2010 evaluation. Based on the system-level correlation with human judgments (check out Tables 1 and 2), it ranked at the top for the out-of-English task and was almost at the top for the into-English task (as mentioned by Callison-Burch et al., 2010).",
        "formal_text": "Convert casual text to formal text: TESLA-M has an advantage over the earlier metrics because it uses lightweight linguistic features like lemmas, synonym dictionaries, and POS tags. Even though these tools are often available"
    },
    {
        "casual_text": "We can also use \"-t maxcov\" with METEOR to reduce the number of words that don’t align properly.",
        "formal_text": "Convert casual text to formal text: We can also use \"-t maxcov\" with METEOR to reduce the number of words that don’t align properly. Convert casual text to formal text: We can also use \"-"
    },
    {
        "casual_text": "Sure, buddy! In the same language, you can find words that are pretty much the same. These pairs or groups of words are often called doublets.",
        "formal_text": "Convert casual text to formal text: Sure, buddy! In the same language, you can find words that are pretty much the same. These pairs or groups of words often called doublets. Convert casual text to formal text: Sure"
    },
    {
        "casual_text": "This project got funding from a Spanish project called TIC2003-08681-C02 and the Ministry of Education and Science.",
        "formal_text": "Convert casual text to formal text: This project got funding from a Spanish project called TIC2003-08681-C02 and the Ministry of Education."
    },
    {
        "casual_text": "Okay, so here's something I really want to point out before we dive into some of the cooler stuff. I hope this isn’t as big of a surprise to you as it was to me when I first figured it out. At first, I thought that when we talk about explaining syntactic complexity, all we’d need to do is compare it to a specific language. (Logicians and some linguists know that the same sentence can actually belong to completely different languages, so it wouldn’t be weird to think that those sentences might have different levels of complexity depending on the language they’re in.) But what really caught me off guard—though it only threw me for a second because, let’s be real, it makes total sense—is that complexity isn’t just relative to the language, but also to the grammar. So, the same sentence in the same language can have one level of complexity when you look at it through one grammar, and a totally different level when you use another grammar. Even two different sentences can be more complex than each other depending on which grammar you’re using.",
        "formal_text": "Convert casual text to formal text: Okay, so here's something I really want to point out before we dive into some of the cooler stuff. I hope this isn’t as big of a surprise to you as it was"
    },
    {
        "casual_text": "To simplify things, we're using just one background model for all the new tasks. This might affect the scores a bit, but it still lets us compare how different models perform on the same task.",
        "formal_text": "Convert casual text to formal text: To simplify things, we're using just one background model for all the new tasks. This might affect the scores a bit, but still lets us compare how different models perform on the same task."
    },
    {
        "casual_text": "We also trained the model on the 960-hour LibriSpeech dataset, as mentioned in Table 11. The tests show that our SpeechT5 model gets a big boost in performance, even without adding a language model (LM) on top. It works just as well, or even better, than wav2vec 2.0 when they do use an LM. To check how good the TTS results are, we used the NISQA-TTS tool for automatic evaluation. It's way easier and cheaper than using MOS or CMOS, which require people to listen and rate the audio. As you can see in Table 13, the version of...",
        "formal_text": "Convert casual text to formal text: We also trained the model on the 960-hour LibriSpeech dataset, as mentioned in Table 11. The tests show that our SpeechT5 model gets a big boost in performance, even"
    },
    {
        "casual_text": "So, entity linking has its limits and can be kind of random. For instance, systems by Ferragina and Scaiella (2010) and Ratinov et al. (2011) both correctly link \"vitamin C\" but mess up with \"pineapple juice,\" linking it to just \"pineapple.\" The thing is, \"pineapple juice\" isn’t recognized as a beverage because it’s not important enough to have its own Wikipedia page. As Table 1 shows, Wikipedia tends to focus on well-known entities but misses out on less common or newer ones. For example, Wang et al. (2012) found there are over 900 active shoe brands, but only 82 are listed on Wikipedia. In areas like intelligence analysis or local search, non-Wikipedia entities are often super important. That’s why we’re looking at the problem of unlinkable noun phrases: when a noun phrase doesn’t link to Wikipedia, we need to figure out if it’s an entity and what its specific type is. It’s tricky because not all noun phrases are entities—like \"Some people,\" \"an addition,\" or \"nearly half.\" Plus, predicting semantic types is hard because there are so many different kinds of entities in regular text. In our experiments, we used the Freebase type system, which has over 1,000 different semantic types.",
        "formal_text": "Convert casual text to formal text: So, entity linking has its limits and can be kind of random. For instance, systems by Ferragina and Scaiella (2010) and Ratinov et al. (2011) both correctly link"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Input: An SCFG Output: A binary SCFG ′ that's equivalent Function ITERATIVECOSTREDUCTION( ) 1: Set ′ to 0 2: Go through each  0 and do the following:",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Input: An SCFG Output: A binary SCFG ′ that's equivalent Function ITERATIVECOSTREDUCTION("
    },
    {
        "casual_text": "In recent years, transfer learning has gotten a lot more practical, allowing us to use huge neural networks even when we don't have much labeled data (Howard and Ruder, 2018; Peters et al., 2018; Devlin et al., 2019). This is especially useful for computer science tasks, which often deal with limited resources. However, earlier research on sequence labeling for code-switching mostly stuck to traditional machine learning (ML) techniques because they worked better than deep learning models trained from scratch with small datasets (Yirmibeşoglu and Eryigit, 2018; Al-Badrashiny and Diab, 2016). But things are changing. Some recent studies have shown great results by using pre-trained monolingual embeddings for tasks like Named Entity Recognition (NER) and Part-of-Speech (POS) tagging (Trivedi et al., 2018; Winata et al., 2018; Ball and Garrette, 2018). Others have used multilingual sub-word embeddings, like fastText (Bojanowski et al., 2017), for Language Identification (LID) (Mave et al., 2018), or cross-lingual sentence embeddings for text classification, like LASER (Schwenk, 2018; Schwenk and Li, 2018; Schwenk and Douze, 2017), which can even handle code-switched sentences. These findings show that pre-trained models have a lot of potential, and they’ve inspired us to dig deeper into transfer learning for code-switching scenarios.",
        "formal_text": "Convert casual text to formal text: In recent years, transfer learning has gotten a lot more practical, allowing us to use huge neural networks even when we don't have much labeled data (Howard and Ruder"
    },
    {
        "casual_text": "In this paper, we came up with the idea of making covariance matrices more sparse to improve bilingual projection directions. We haven’t seen any NLP research that tries to recover the sparseness of these matrices to enhance projection directions. Our work is different from the sparse CCA (Canonical Correlation Analysis) approach in machine learning (Hardoon and Shawe-Taylor, 2011; Rai and Daumé III, 2009). Their goal is to find projection directions that represent the original documents as sparse vectors in a shared subspace. Another related but different area of research is sparse covariance matrix selection (Banerjee et al., 2005). Their focus is on finding matrices where the inverse of the covariance matrix is sparse, which is useful for Gaussian processes.",
        "formal_text": "Convert casual text to formal text: In this paper, we came up with the idea of making covariance matrices more sparse to improve bilingual projection directions. We haven’t seen any NLP research that"
    },
    {
        "casual_text": "(1) and (2) without any extra info to start with, and (ii) figuring out the interpretable dimension later by subtracting the normalized embedding vectors of opposite words, like  gender = ( man   woman ) + ( he   she ). We can compare how well our method works by checking the sign of the cosine similarity between new words and the vector we made. This lets us see if our method is as accurate as the current best one. To get a sense of how uncertain our results are, we calculate binomial confidence intervals using the normal approximation.",
        "formal_text": "Convert casual text to formal text: (1) and (2) without any extra info to start with, and (ii) figuring out the interpretable dimension later by subtracting the normalized embedding vectors of opposite words, like"
    },
    {
        "casual_text": "The reordering model that's commonly used in this kind of research is a distance-based one. Basically, it gives a cost depending on how far the last translated phrase is from the start of the current one. If there's a big jump between them, it gets penalized. The formula for the distortion model feature, called h Dist, looks like this:",
        "formal_text": "Convert casual text to formal text: The reordering model that's commonly used in this kind of research is a distance-based one. Basically, it gives a cost depending on how far the last translated phrase is"
    },
    {
        "casual_text": "Lately, deep learning (Bengio, 2009) has been kicking butt in a bunch of areas like computer vision and speech recognition. It's also been pretty effective in NLP tasks like sentiment analysis (dos Santos and Gatti, 2014), parsing (Socher et al., 2013), summarization (Rush et al., 2015), and machine translation. With all the progress in deep learning, there's been a lot of work on designing neural networks for relation extraction. For example, (Socher et al., 2012) used a recursive neural network for this, and (Xu et al., 2015; Miwa and Bansal, 2016) took it a step further with LSTMs. On the other hand, (Zeng et al., 2014; dos Santos et al., 2015) went with CNNs for the task, while (Zeng et al., 2015; Lin et al., 2016) combined attention-based multiinstance learning, which has shown some cool results. But here's the thing: most of these models only focus on sentences that directly mention both target entities. They miss out on the important relation paths hidden in the text. In this paper, we’re introducing a new path-based neural RE model to tackle this issue. We’re using CNN to test our model, but honestly, other neural models could easily fit into our framework too.",
        "formal_text": "Convert casual text to formal text: Lately, deep learning (Bengio, 2009) has been kicking butt in a bunch of areas like computer vision and speech recognition. It's also been pretty effective in NLP"
    },
    {
        "casual_text": "Our method works with any neural encoder-decoder setup. For this project, we used the big pre-trained BART model (Lewis et al., 2019) and tweaked it with our own technique. Testing it on real news articles showed that our approach improves on what's already out there. When we applied it to a synthetic domain, the BART model, after being fine-tuned with our weak supervision, became way more efficient with data. It even beat the best systems before by a lot, using just 0.4% of the training examples.",
        "formal_text": "Convert casual text to formal text: Our method works with any neural encoder-decoder setup. For this project, we used the big pre-trained BART model (Lewis et al., 2019)"
    },
    {
        "casual_text": "Lately, there's been some cool progress in understanding procedural text, like the work by Gupta and Durrett (2019b), Du et al. (2019), and Das et al. (2019). But, the current methods don't really handle the relationships between entities, actions, and locations in a structured way. Instead, most of them rely on basic rules about how entities change or use extra knowledge to make guesses. For instance, Gupta and Durrett (2019b) came up with a fancy neural network that keeps track of each entity's state and uses a CRF model to sum up how everything changes overall. They also added some common-sense rules to their model to help it out. While Das et al. (2019) did look at how entities and locations connect, there isn't a standard way to deal with these relationships yet. Plus, some key connections, like how entities interact with actions or with each other, are totally overlooked.",
        "formal_text": "Convert casual text to formal text: Lately, there's been some cool progress in understanding procedural text, like the work by Gupta and Durrett (2019b), Du et al. (2019"
    },
    {
        "casual_text": "So, basically, it's about translating the first word of the input from the source language (SL) to the target language (TL). When dealing with these kinds of constraints, sometimes we get info that isn't really needed later on. We could make the algorithm better by focusing only on the TL sequences that actually end up in the output, instead of looking at every possible word. The alignment part is about matching up the TL tokens (basically, chunks of text) with the SL tokens from the input. When we say 'same' in this case, it means the variables or bits of text have the same alignment number.",
        "formal_text": "Convert casual text to formal text: So, basically, it's about translating the first word of the input from the source language (SL) to the target language (TL). When dealing with these kinds of constraints, sometimes we get info"
    },
    {
        "casual_text": "Every SL/TL text fragment and variable gets the same ID number. If there's no alignment info for some variables, they’re labeled as N OALIGN number. For TL variables that aren’t aligned, they’re marked as N OALIGN 0. If SL text fragments aren’t aligned, there’s no matching alignment number on the TL side.",
        "formal_text": "Convert casual text to formal text: Every SL/TL text fragment and variable gets the same ID number. If there's no alignment info for some variables, they’re labeled as N OALIGN number. For"
    },
    {
        "casual_text": "The rest of the documents are in different languages, mostly English, French, or Spanish. To figure out which language each one was, we used a tool called langdetect. We ended up keeping only the ones in those three languages, which gave us a total of 50,380 documents.",
        "formal_text": "Convert casual text to formal text: The rest of the documents are in different languages, mostly English, French, or Spanish. To figure out which language each one was, we used a tool called langdetect. We ended up"
    },
    {
        "casual_text": "Sure! Let’s break this down in a more casual way: We’re focusing on two specific scenarios that are kind of like the continuous-mode use case and the time-segmented methodology. There’s room for more research and experiments with these ideas in the future. **Cross-project continuous-mode use case:** This is similar to the continuous-mode use case, but with a twist. When training the model at time , instead of using all the samples from all projects before , we only use samples from other projects. It’s like mixing the cross-project and time-segmented methodologies together. From the perspective of people using the machine learning (ML) model, this isn’t as practical as the regular continuous-mode use case because using samples from the actual project you’re working on usually makes the model perform better. But for ML researchers, this approach could be useful for testing how well the model works on new, unseen data while also considering how software evolves over time. **Online continuous-mode use case:** This is also similar to the continuous-mode use case, but here’s the difference: when we train a new model at time , instead of throwing away the old model from  1 and starting fresh, we keep building on the old model using the data between  1 and . Think of it like online learning algorithms (Shalev-Shwartz, 2012) where you’re constantly updating the model. This approach is kind of like the time-segmented methodology, but with multiple rounds of training and evaluation. So, in short, these two scenarios are variations of the continuous-mode use case, each with their own tweaks and potential benefits depending on who’s using them—whether it’s ML users or researchers.",
        "formal_text": "Convert casual text to formal text: Sure! Let’s break this down in a more casual way: We’re focusing on two specific scenarios that are kind of like the continuous-mode use case and the time-se"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. So, there's this thing where the verb can either come before or after the indirect object, depending on who's being talked about—like if it's \"you\" or \"me\" (first or second person), the verb comes after the indirect object. But if it's \"they\" or something else (third person), the verb comes before. Now, let's say we've already figured out the direct object right after the subject. In that case, we know what it is, but we're not sure where exactly it fits in the sentence—before or after the indirect object. That placement depends on whether the indirect object is \"you,\" \"me,\" or \"they.\" If it's \"you\" or \"me,\" the direct object goes after the indirect object. If it's \"they,\" the direct object goes before. But here's the tricky part: if we start by figuring out the indirect object first, we might have to hold onto it in our memory while we figure out the direct object. This happens when the indirect object is in the third person and not reflexive. So, whether we start with the direct or indirect object, we end up dealing with some extra mental work to keep track of things.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. So, there's this thing where the verb can either come before or after the indirect object, depending on who's being"
    },
    {
        "casual_text": "We adjusted the settings on a small group of 50 questions from Yahoo! Answers, which we used as a development set. This group wasn’t part of the test set. For the smoothing parameter , we set it to 2000 based on testing for both English and Chinese (after translating to Chinese). For the  parameter in equation 11, we set it to 0.8 after some trial and error. For the  parameter, we tested different values—0.1, 0.2, and so on up to 0.9—on the development set to see which one gave the best results in terms of MAP. After trying them out, we went with  = 0.6 because it worked the best. For the k parameter mentioned in Algorithm 1, we also tested a few values on the development set and ended up choosing k = 30 because it performed better than the others. Table 4 compares the results we got using either just English or both English and Chinese representations with the language model (LM) described in sections 4.2.1 and 4.2.2 for question retrieval. In the table, E stands for the baseline LM using only English, while C represents the LM using Chinese (after translating from English).",
        "formal_text": "Convert casual text to formal text: We adjusted the settings on a small group of 50 questions from Yahoo! Answers, which we used as a development set. This group wasn’t part of the test set. For the smooth"
    },
    {
        "casual_text": "(iii) Using dialogue control functions and looking at syntactic features like tri-grams (which include an auxiliary verb, the word right after it, and the one after that) as (DCF, SS2).",
        "formal_text": "Convert casual text to formal text: (iii) Using dialogue control functions and looking at syntactic features like tri-grams (which include an auxiliary verb, the word right after it, and the one after"
    },
    {
        "casual_text": "Check out our multi-view response selection model in Figure 2. Basically, the context and response are turned into semantic embeddings in two different views. Both views share the same word embeddings, which helps them exchange info. In the utterance sequence view, we use a TCNN to handle the utterance embeddings. For the word sequence view and the utterance sequence view, we have two separate Gated Recurrent Units (GRUs)—one deals with word embeddings and the other with utterance embeddings. The word-level GRU focuses on word dependencies, while the utterance-level GRU looks at semantic and discourse stuff. We calculate confidence scores for picking the response in both views separately. Finally, we tweak the model by minimizing this loss:",
        "formal_text": "Convert casual text to formal text: Check out our multi-view response selection model in Figure 2. Basically, the context and response are turned into semantic embeddings in two different views. Both views share the same word embedding"
    },
    {
        "casual_text": "Sometimes, different words can have the same vector representation, like anagrams, for example. Also, we've decided to include the letter 'y' in the group of vowels.",
        "formal_text": "Convert casual text to formal text: Sometimes, different words can have the same vector representation, like anagrams, for example. Also, we've decided to include the letter 'y' in the group of vowels."
    },
    {
        "casual_text": "So, for the task of generating feedback based on the data, we end up with the matching features from both the speech and the question text.",
        "formal_text": "Convert casual text to formal text: So, for the task of generating feedback based on the data, we end up with the matching features from both the speech and the question text. Convert casual text to formal text: So,"
    },
    {
        "casual_text": "Okay, so you have two sequences:  i+1,  i+2, ...,  j and  j+1,  j+2, ...,  k. The thing you're looking at is called  i, j, k, and it's defined as:  i, j, k = j =i+1 k r=j+1 B[ r,  ]  B[,  r ] (5) Basically, it's the difference between B[ r,  ] and B[,  r ] for some values in those sequences.",
        "formal_text": "Convert casual text to formal text: Okay, so you have two sequences:  i+1,  i+2, ...,  j and  j+1,  j+2,"
    },
    {
        "casual_text": "Got it! I'll chat with the user and recommend the perfect restaurant based on the info you give me.",
        "formal_text": "Convert casual text to formal text: Got it! I'll chat the user and recommend the perfect restaurant based on the info you give me."
    },
    {
        "casual_text": "We approached the problem of automatically figuring out why someone made an edit by treating it as a multi-label classification task. To do this, we came up with four groups of features to help identify the intentions behind edits. **Set I** has two features related to the editor: 1. Whether the editor was registered or just an anonymous user. 2. How long the editor has been around—this is the number of months between the edit and when they first registered. **Set II** focuses on the comment the editor writes to explain their edit. This set includes 16 features, like the length of the comment and some patterns to spot specific intentions, such as \"*pov*\" (point of view), \"*clarify*,\" \"*simplif*,\" or \"*add link*.\" **Set III** looks at the actual changes made in the revision. This set has 198 features based on what’s different between the current version and the previous one. It’s similar to what Daxenberger and Gurevych (2013) did, but we looked at a broader range of things that could be changed. For example, we checked how many characters, uppercase words, numbers, spaces, or special characters (like Chinese/Japanese/Korean characters, HTML entities, URLs, punctuation, etc.) were added or removed. We also considered language-related stuff, like whether the edit used stop words, swear words, or informal language. **Set IV** has two features related to vandalism and reverts. We used the Wikipedia API to figure out if a revision was likely a case of vandalism or if it was reverting a previous edit.",
        "formal_text": "Convert casual text to formal text: We approached the problem of automatically figuring out why someone made an edit by treating it as a multi-label classification task. To do this, we came up with four groups of features to help"
    },
    {
        "casual_text": "The RotatE model, introduced by Sun et al. in 2019, treats each relation as a 2-D rotation from one entity to another. This approach is designed to capture key properties of relations, like symmetry/antisymmetry, inversion, and composition, which have been shown to be useful for predicting links in knowledge graphs. While other methods might handle one or a couple of these patterns, RotatE naturally covers all of them. Plus, RotatE splits entity and relation embeddings into multiple groups—for instance, it uses 1000 2-D rotations in their setup. Each group is modeled and scored separately, and the final score is just the sum of all these individual scores. This setup is kind of like having an ensemble of different models, which helps improve link prediction performance. But there are some limitations. RotatE is stuck with 2-D rotations, so it doesn’t have as much flexibility in modeling. Also, it doesn’t take into account the context of the graph, which could be useful for dealing with tricky relation types like 1-to-N, N-to-1, or N-to-N predictions.",
        "formal_text": "Convert casual text to formal text: The RotatE model, introduced by Sun et al. in 2019, treats each relation as a 2-D rotation from one entity to another. This approach is designed to capture key properties of relations"
    },
    {
        "casual_text": "Alright, here's the deal: the best settings for these parameters are: the longest question can have 15 words, the CNN filter sizes are 2 and 3, there's 1 shared CNN layer, 2 shared Bi-LSTM layers, a hidden dimension of 1000, 4 attention heads, and both image and object feature dimensions are set to 2048. The image level feature has 100 spatial locations, while the object level feature has 36 objects. The bilinear pooling rank is 3, there are 8 bilinear attention maps, the model trains for 100 epochs, and the starting learning rate is 0.002. The Adamax optimizer, which was introduced by Kingma and Ba in 2014, is used to tweak the weights during training.",
        "formal_text": "Convert casual text to formal text: Alright, here's the deal: the best settings for these parameters are: the longest question can have 15 words, the CNN filter sizes are 2 and 3, there's 1 shared CNN layer,"
    },
    {
        "casual_text": "Here's the informal version: \"Figure 4 shows the simulation error rates for each condition (remember, higher is better here). Polyjuice-surprise has the highest error rate, which means showing these counterfactuals would give users the most extra info if they were displayed.\"",
        "formal_text": "Convert casual text to formal text: Here's the informal version: \"Figure 4 shows the simulation error rates for each condition (remember, higher is better here). Polyjuice-surprise has the highest error rate, which"
    },
    {
        "casual_text": "Sure! So, while the online support docs are Adobe talking to users, there's also a ton of back-and-forth where users reach out to Adobe for help.",
        "formal_text": "Adobe is talking to users, but there's also a ton of back-and-forth where users reach out to Adobe for help. So, while the online support docs are Adobe talking to users, there's"
    },
    {
        "casual_text": "GB-Theory, as introduced by Chomsky back in 1981, changed how we think about grammar. Instead of focusing on a bunch of rules, it introduced the idea of grammar being made up of different modules. These modules include things like X-bar, Theta, Case, Bounding, Trace, Control, Binding, and Government. For this paper, though, we’re just going to give a quick overview of X-bar, Theta, Control, and Binding, mainly because of space constraints.",
        "formal_text": "Convert casual text to formal text: GB-Theory, as introduced by Chomsky back in 1981, changed how we think about grammar. Instead of focusing on a bunch of rules, it introduced the idea of grammar"
    },
    {
        "casual_text": "Like Yogatama et al. (2015) and Ren et al. (2016), we put feature representations and labels into the same space so that an object is closer to things of the same type than to things that aren’t. Basically, we’re trying to figure out how to map these features in a way that makes sense.",
        "formal_text": "Convert casual text to formal text: Like Yogatama et al. (2015) and Ren et al. (2016), we put feature representations and labels into the same space so that an object is closer to things of the same type"
    },
    {
        "casual_text": "First off, let's take a look at Table 1 to compare our new pre-training method with other approaches. Overall, ENPAR really shines when it comes to relation performance, beating all other models in two key evaluation areas. To be more specific, in exactly matching mode, our method gets a 4.4-point boost over the LSTM-based GCN joint model (Sun et al., 2019a) and a 3.3-point increase compared to the BERT-based QA model (Li et al., 2019). Even when we compare it to BERT-based multi-task learning models (Wadden et al., 2019), our method still comes out ahead by 2.7 points in relation performance. Now, while our method doesn't do as well on entity performance compared to those multi-task learning models, we think that extra supervision signals like coreference and event information during fine-tuning might be the reason for the gap. Plus, they also consider all spans and cross-sentence context, which usually helps with entity performance. But even with a slightly weaker entity encoder and no extra multi-task training data, our pre-trained entity pair encoder still manages to outperform others in relation performance. This really shows how effective our pre-training method is.",
        "formal_text": "Convert casual text to formal text: First off, let's take a look at Table 1 to compare our new pre-training method with other approaches. Overall, ENPAR really shines when it comes to relation performance, beating"
    },
    {
        "casual_text": "The whole process of an EBMT system, which includes matching, alignment, and recombination, was first explained in (Nagao, 1984) and then more clearly named like this in (Somers, 1999).",
        "formal_text": "Convert casual text to formal text: The whole process an EBMT system, which includes matching, alignment, and recombination was first explained in (Nagao, 1984) and then more clearly named like this in ("
    },
    {
        "casual_text": "Hierarchical clustering can help us see how tags are related to each other. Figure 1 (b) gives an example of how tags might be grouped in this way. But there are still some issues with this approach: First, clusters can combine different types of relationships, like synonyms and hypernyms, which can get messy. Second, they don’t account for the direction of the relationship, like how \"browser\" relates to \"firefox.\" Third, it’s tricky to know if the clustering is accurate or not—it’s hard to say for sure if two tags are really similar. In real-world use, we’d rather have clear, directed relationships between tags that are easy to evaluate, like the ones shown in Figure 1 (c).",
        "formal_text": "Convert casual text to formal text: Hierarchical clustering can help us see how tags are related to each other. Figure 1 (b) gives an example of how tags might be grouped in this way. But there are still some"
    },
    {
        "casual_text": "From Figure 1a, we can see that the block n-gram repeats algorithm takes up a whopping 25% of the generation time. To cut down on this cost, we came up with a new GPU-based kernel (check out Algorithm 1) that uses parallel computing to make things more efficient. Here's what it does: 1. It avoids moving data between the GPU and CPU, which helps get around the bottleneck caused by the PCIe bus interface. 2. It scans n-grams in parallel. Instead of going through tokens one by one to find repeated n-grams, it can do this all at once using threads equal to the number of n-grams generated up to time step t. Plus, each sample in a batch can be handled at the same time using multiple thread-blocks. 3. It uses GPU shared memory for quicker access to data.",
        "formal_text": "Convert casual text to formal text: From Figure 1a, we can see that the block n-gram repeats algorithm takes up a whopping 25% of the generation time. To cut down on this cost, we came up with"
    },
    {
        "casual_text": "Alright, so here's the deal: You've got a question in English, Hindi, or even a mix of both, and there's an image that goes with it. The challenge is to use the image to figure out the right answer. Basically, you need to do some deep thinking about the picture to pick the correct response from all the options. In short:",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal: You've got a question in English, Hindi, or even a mix of both, and there's an image that goes with it."
    },
    {
        "casual_text": "For the i-th event, we also toss in a projection layer to transform the raw feature into the input dimension using an embedding layer called f.",
        "formal_text": "Convert casual text to formal text: For the i-th event, we also toss in a projection layer to transform the raw feature into the input dimension using an embedding layer called f. For the i"
    },
    {
        "casual_text": "In the third line, we're saying there's a graph constant G that belongs to C and has  G equal to [ ]. This is based on Lemma G. 10 and Assumption 1.",
        "formal_text": "Convert casual text to formal text: In the third line, we're saying there's a graph constant G that belongs to C and has  G equal to [ ]. This is based on Lemma G."
    },
    {
        "casual_text": "The English Switchboard (SWBD) dataset, which is the biggest and most commonly used corpus for disfluency detection, has 173,000 sentences for training (Godfrey et al., 1992). We're using the English Switchboard as our main data source. Following the setup from Charniak and Johnson (2001), we divided the Switchboard corpus into training, development (dev), and test sets like this: the training data includes all sw[23]*.dff files, the dev data has all sw4[5-9]*.dff files, and the test data has all sw4[0-1]*.dff files. Also, following Honnibal and Johnson (2014), we converted all the text to lowercase, got rid of punctuation and partial words. We also removed 'um' and 'uh' tokens and combined 'you know' and 'i mean' into single tokens.",
        "formal_text": "Convert casual text to formal text: The English Switchboard (SWBD) dataset, which is the biggest and most commonly used corpus for disfluency detection, has 173,000 sentences for training (Godfrey et"
    },
    {
        "casual_text": "In Figure 3's example, the phrase \"f\" shows up a few times in the training data paired with target phrases like e1, e2, e3, and e5. By counting how often \"f\" is linked with each target phrase, we can use something called relative frequency estimation to figure out the probability that a certain target phrase, like ei, is the translation of \"f.\"",
        "formal_text": "Convert casual text to formal text: In Figure 3's example, the phrase \"f\" shows up a few times in the training data paired with target phrases like e1, e2, e3, and e5."
    },
    {
        "casual_text": "The idea here is that themes aren't just straightforward or basic when it comes to what they refer to.",
        "formal_text": "Convert casual text to formal text: The idea here is that themes aren't just straightforward or basic when it comes to what refers to. Convert casual text to formal text. Convert casual text to formal text. Convert"
    },
    {
        "casual_text": "A text plan is basically a big, organized collection of frames, kind of like a family tree. At the top of this tree is the text structure frame, which has a special slot called \"has-as-part.\" This slot holds an ordered list of plan sentence frames, let's call them S_i. Each plan sentence frame has a slot named \"subtype\" with options like \"simple,\" \"complex,\" or \"and.\" There's another slot that holds an ordered list of plan clause frames, which make up the sentence. Now, a plan clause frame is where the magic happens. It takes the input and turns it into actual words and meanings, both for the main idea and the little details that make it work. It lists the main part of the idea and points to other frames that handle the roles related to this idea. Sometimes, these roles point to other clauses instead. Also included are the feelings or attitudes the speaker has about the clause and little markers that show how the clauses connect, kind of like glue. Plus, it tells you if a clause is part of a bigger sentence or if it stands on its own. The plan role frames work in a similar way, just dealing with the roles instead of the whole clause.",
        "formal_text": "Convert casual text to formal text: A text plan is basically a big, organized collection of frames, kind of like a family tree. At the top of this tree is the text structure frame, which has a special slot called"
    },
    {
        "casual_text": "The graph-enhanced bi-directional attention layer is designed to capture the intricate connections between sentences and relation instances. It creates a more refined representation of relation instances by combining information from within a sentence and across different sentences. This setup includes three main parts: the S2R layer, the GCN layer, and the R2S layer.",
        "formal_text": "Convert casual text to formal text: The graph-enhanced bi-directional attention layer is designed to capture the intricate connections between sentences and relation instances. It creates a more refined representation of relation instances by combining information from"
    },
    {
        "casual_text": "We're using the imSitu dataset to create our image-to-verb model. This dataset is pretty cool—it has a bunch of images, each showing one of 504 different verbs. The images cover all sorts of semantic roles, as described by Yatskar et al. in 2016. Check out Figure 4 for some examples from the dataset. We've split the data into different parts: 379 classes for training, 29 for validation, and 96 for testing.",
        "formal_text": "Convert casual text to formal text: We're using the imSitu dataset to create our image-to-verb model. This dataset is pretty cool—it has a bunch of images, each showing one of 504 different"
    },
    {
        "casual_text": "To tackle the issue of slow training times with neural network language models (NNLMs), Vaswani and his colleagues (2013) came up with a method called noise contrastive estimation (NCE). Unlike maximum likelihood estimation (MLE), NCE doesn’t need to go through the whole vocabulary over and over. Instead, it does some nonlinear logistic regression to tell the difference between real data and fake stuff made up as noise.",
        "formal_text": "Convert casual text to formal text: To tackle the issue of slow training times with neural network language models (NNLMs), Vaswani and his colleagues (2013) came up with a method called noise contrastive estimation (NCE)."
    },
    {
        "casual_text": "Lastly, we tried DP on MTL-5 (let's call it MTL-DP). It turns out MTL-DP has a bit better accuracy than MTL-5, as you can see in Table 1, except for NER when we cut 50% of the attention heads. This could mean that all the tasks are fighting over the same heads, even though about half of them aren't really doing much during multitask training.",
        "formal_text": "Convert casual text to formal text: Lastly, we tried DP on MTL-5 (let's call it MTL-DP). It turns out MTL-DP has a bit better accuracy than MTL-5, as"
    },
    {
        "casual_text": "For the concept ranking thing, we use scores from the Pyramid dataset, which is human-annotated. Each semantic content unit (SCU) gets a weight based on how many reference summaries it shows up in. So, for a bigram, its gold-standard score is the highest weight of all the SCUs that include it. If a bigram doesn’t appear in any SCU, it gets a score of 0. Since there are four human-written summaries for each query, the bigram scores can only be 0, 1, 2, 3, or 4. These scores are plugged directly into ListNet (check out formula (5)). In RankNet, bigram pairs are made based on these gold-standard scores.",
        "formal_text": "Convert casual text to formal text: For the concept ranking thing, we use scores from the Pyramid dataset, which is human-annotated. Each semantic content unit (SCU) gets a weight based on how many reference sum"
    },
    {
        "casual_text": "A sentence might have parts that help connect different ideas, but we won’t get into that here.",
        "formal_text": "Convert casual text to formal text: A sentence might have parts that help connect different ideas, but we won’t get that here. Convert casual text to formal text: A sentence might have parts that help connect different ideas, but we"
    },
    {
        "casual_text": "We ended up with a big collection of 1,472,798 strings after doing this extraction thing. The number of context strings we got for each compound varied a lot: 288 compounds had 1,000 or more sentences linked to them, while 191 had 10 or fewer, and 45 pairs didn't have any sentences at all. The biggest groups of context strings were mostly tied to stuff like political or economic topics (like \"government official\" or \"oil price\"), which makes sense since the Gigaword sentences come from news sources.",
        "formal_text": "Convert casual text to formal text: We ended up with a big collection of 1,472,798 strings after doing this extraction thing. The number of context strings we got for each compound varied a lot: 288 compounds had 1,000"
    },
    {
        "casual_text": "• Pre-training: First, you train a neural network on the source domain. Then, you take the weights from that trained network to set up a new one. Finally, you tweak those weights a bit more to make it work better on the target domain.",
        "formal_text": "Convert casual text to formal text: • Pre-training: First, you train a neural network on the source domain. Then, you take the weights from that trained network to set up a new one. Finally, you"
    },
    {
        "casual_text": "Okay, so, in addition to the usual basic rules and glue rules, we’ve got this swap rule that lets us switch around nearby chunks of words, but not whole phrases that are nested inside each other. Instead of using XH as the starting point for these rules, we went with XP. This choice lets us stick the results of the swap rule into bigger, hierarchical phrases, which you can’t do with the results of the regular hierarchical rules in a shallow grammar. To keep things balanced, we also added a rule that lets us combine stuff in a straightforward way. Now, we can put a limit on how many words these XP and XH chunks can cover. With that length limit in place, you can’t just keep applying the rules from Equation (4) to build super-long sub-derivations.",
        "formal_text": "Convert casual text to formal text: Okay, so, in addition to the usual basic rules and glue rules, we’ve got this swap rule that lets us switch around nearby chunks of words, but not whole phrases that are nes"
    },
    {
        "casual_text": "Every entry has some text attached to it, which is usually the main part of the entry. We think of this text as the way to break down the noun phrase. If the noun phrase has more than one word, those words will show up in the text on their own or as part of other noun phrases.",
        "formal_text": "Convert casual text to formal text: Every entry has some text attached to it, which is usually the main part of the entry. We think of this text as the way to break down the noun phrase. If the noun phrase has"
    },
    {
        "casual_text": "These were made for STK and PTK. They give you trees that get more complex as you go along, like this: The first one (FLAT) is a basic tree showing how words and chunked concepts are connected directly. From this, STK and PTK can grab useful bits (tree fragments).",
        "formal_text": "Convert casual text to formal text: These were made for STK and PTK. They give you trees that get more complex as you go along, like this: The first one (FLAT) is a basic tree showing how words"
    },
    {
        "casual_text": "He really liked that apple.",
        "formal_text": "Convert casual text to formal text: He really liked that apple. Convert casual text to formal text: He really liked that apple. Convert casual text to formal text: He really liked that apple. Convert casual text to formal text"
    },
    {
        "casual_text": "In this paper, we’re taking the solutions we mentioned earlier and creating a new tool called Graph2Tree to tackle the issues with current methods for solving math word problems (MWPs). Here’s what we’re bringing to the table:",
        "formal_text": "Convert casual text to formal text: In this paper, we’re taking the solutions we mentioned earlier and creating a new tool called Graph2Tree to tackle the issues with current methods for solving math word problems (MWP"
    },
    {
        "casual_text": "Turns out, all three models don't work as well when the whole dependency tree is included. Seems like adding more info actually messes things up. Lastly, we noticed that when we add context to the GCN, it becomes less picky about the tree structure. This is probably because the model can use the word sequence info from the LSTM layer to figure out any extra details it needs for getting the relationships right.",
        "formal_text": "Convert casual text to formal text: Turns out, all three models don't work as well when the whole dependency tree is included. Seems like adding more info actually messes things up. Lastly, we noticed that"
    },
    {
        "casual_text": "Dr. Micklesen got two grants from our university's Graduate School to do two different studies. In the first one, he looked at how compounding works in Russian and came up with ideas for breaking down compounds efficiently using machines. The second study turned into a really detailed analysis of Russian MT form classes, which is important for figuring out the intended grammatical and non-grammatical meanings automatically. He also made a full list of all the subclasses of Russian paradigmatic form classes and counted how many unique forms are in each set. These classes are all about the structure—they’re the most efficient way to split things into stems and endings.",
        "formal_text": "Convert casual text to formal text: Dr. Micklesen got two grants from our university's Graduate School to do two different studies. In the first one, he looked at how compounding works in Russian and came up with ideas"
    },
    {
        "casual_text": "A more translation-focused project is OTELO, which has members like SAP (from Germany), Lotus Development (from Ireland), CST (from Denmark), and Logos (also from Germany). The goal here is to create a cool, all-in-one automated translator setup. This would bring together different programs—like machine translation, translation memory, and other tools—into one easy-to-use interface. Plus, it’ll let new or curious users test out machine translation over the internet.",
        "formal_text": "Convert casual text to formal text: A more translation-focused project is OTELO, which has members like SAP (from Germany), Lotus Development (from Ireland), CST (from Denmark), and Logos (also from Germany). The"
    },
    {
        "casual_text": "There are a few ways to handle domain generalization. One approach is to train a separate model for each in-domain dataset. Then, when you're testing on out-of-domain data, you pick the in-domain dataset that’s most similar and use its model for predictions. Other methods, like the ones by Ghifary et al. (2015) and Muandet et al. (2013), focus on training a model to learn features that stay the same across different domains. They do this using stuff like multi-view autoencoders and mean map embedding techniques.",
        "formal_text": "Convert casual text to formal text: There are a few ways to handle domain generalization. One approach is to train a separate model for each in-domain dataset. Then, when you're testing on out-of-"
    },
    {
        "casual_text": "CNBC is gonna create and make these programs, and the folks from U.S. News are helping out with background info and research.",
        "formal_text": "Convert casual text to formal text: CNBC is gonna create and make these programs, and the folks from U.S. News are helping with background info. Convert casual text to formal text: Convert casual text to formal text"
    },
    {
        "casual_text": "We also mentioned this result in the \"punctuation\" paper (Spitkovsky et al., 2011, 7), which came out around the same time. They’re also pretty competitive with Bayesian estimators, especially on bigger datasets, when using cross-validation (Gao and Johnson, 2008).",
        "formal_text": "Convert casual text to formal text: We also mentioned this result in the \"punctuation\" paper (Spitkovsky et al., 2011, 7), which came out around the same time. They’"
    },
    {
        "casual_text": "We're looking at how words are spread out in a model's main group (let's call it N) and want to check how well it matches the real data (GT) and how varied the words it picks are.",
        "formal_text": "Convert casual text to formal text: We're looking at how words are spread out in a model's main group (let's call it N) and want to check how well it matches the real data (GT) and"
    },
    {
        "casual_text": "We use a tree-based method to combine the hidden states from individual EDUs (elementary discourse units) into a single, document-level representation. This approach lets the more important parts of the text, as shown in the discourse tree, have a bigger impact on the final document representation. It’s kind of like giving more weight to the key ideas. There are two main choices we need to make when deciding how to use the discourse tree for this aggregation process.",
        "formal_text": "Convert casual text to formal text: We use a tree-based method to combine the hidden states from individual EDUs (elementary discourse units) into a single, document-level representation. This approach lets the more important"
    },
    {
        "casual_text": "It seems like the text you provided is a mix of symbols and characters that don't form coherent words or sentences. It looks like it might be a random string of characters or possibly some kind of encoded message. If you have a specific formal text you'd like me to rewrite in an informal way, please provide it, and I'll be happy to help!",
        "formal_text": "Convert casual text to formal text: It seems like the text you provided is a mix of symbols and characters that don't form coherent words or sentences. It looks like it might be a random string of characters or possibly some kind"
    },
    {
        "casual_text": "In machine learning, explanations try to figure out why a model makes certain decisions by looking at the factors that influence those decisions. Most methods for explaining models either try to show all the factors at once, which can be hard to understand, or group them using rules of thumb, like gradient saliency (Simonyan et al., 2013; Li et al., 2016). But it's still not clear what makes a set of factors a good explanation.",
        "formal_text": "Convert casual text to formal text: In machine learning, explanations try to figure out why a model makes certain decisions by looking at the factors that influence those decisions. Most methods for explaining models either try to show all the factors at once"
    },
    {
        "casual_text": "You can look up articles from TV news shows and newspapers by checking the dates they aired or were published.",
        "formal_text": "Convert casual text to formal text: You can look up articles from TV news shows and newspapers by checking the dates they aired or were published."
    },
    {
        "casual_text": "From the results we got, it's clear that our segmentation algorithm works way better on Choi's text collection compared to what was reported before (like Choi, 2000; Choi et al., 2001; Utiyama and Isahara, 2001). The time it takes to run our algorithm is about the same as other methods (basically O(T2), where T is the number of sentences). Plus, our algorithm has a cool feature: it automatically figures out the best number of segments without needing us to decide it manually.",
        "formal_text": "Convert casual text to formal text: From the results we got, it's clear that our segmentation algorithm works way better on Choi's text collection compared to what was reported before (like Choi, 2000; Choi"
    },
    {
        "casual_text": "In this paper, we use some efficient algorithms for figuring out Probabilistic Context Free Grammar (PCFG) from a huge amount of speech transcripts—like millions of words. We show that you can actually learn grammar just from words and it works well, no matter how you start. To make sure our results are solid, we use two different algorithms for Variational Bayesian PCFG inference and tweak two algorithms that were originally made for Latent Dirichlet Allocation (LDA) topic models. The cool part is that all three algorithms handle big datasets and get better over time, ending up with similar accuracy in prediction and parsing.",
        "formal_text": "Convert casual text to formal text: In this paper, we use some efficient algorithms for figuring out Probabilistic Context Free Grammar (PCFG) from a huge amount of speech transcripts—like millions of words. We show that"
    },
    {
        "casual_text": "We start by grabbing documents from FinTabNet that have between 1 and 4 pages and 2 to 6 tables. Next, we toss out any documents that don’t have much text. Since we’re focusing on numerical reasoning, we also skip documents with tables that don’t have much numerical data. After that, we run a pre-processing script to pull out the structure of each table in HTML format. Any tables the script can’t handle get ignored. In the end, we’re left with 4,791 documents to work with for further annotation.",
        "formal_text": "Convert casual text to formal text: We start by grabbing documents from FinTabNet that have between 1 and 4 pages and 2 to 6 tables. Next, we toss out any documents that don’t have much text"
    },
    {
        "casual_text": "A good way to make use of the latest stuff in statistical machine translation (like what Koehn and his team did in 2003) is to mix the best parts of both worlds by blending SMT with Translation Memories (TMs). A big challenge here is figuring out how to measure how good the MT output is, kind of like how TMs have those fuzzy match scores to help post-editors.",
        "formal_text": "Convert casual text to formal text: A good way to make use of the latest stuff in statistical machine translation (like what Koehn and his team did in 2003) is to mix the best parts of both worlds by blending"
    },
    {
        "casual_text": "We also randomly picked another 100,000 queries that weren’t used in training and used different models to generate responses. We checked how often some common, boring responses popped up in the results, which you can see in Table 6. Our method really helps cut down on those generic answers. For example, we slashed the \"  (I don't know, either.)\" response by about 75% and \" (I want to know, too)\" by 77%.",
        "formal_text": "Convert casual text to formal text: We also randomly picked another 100,000 queries that weren’t used in training and used different models to generate responses. We checked how often some common, boring responses popped up in the results, which you can"
    },
    {
        "casual_text": "Two teams are looking into MT systems where the input is controlled through a back-and-forth between the user and the computer. In these systems, monolingual speakers type out messages in their own language, and the system automatically translates them into a language they don’t know. At the University of Manchester Institute of Science and Technology, they’re working on an experimental system that uses a straightforward example-based approach. Meanwhile, at the University of Grenoble, the LIDIA system lets users interactively clarify things and even translates back into their language to make sure everything makes sense.",
        "formal_text": "Convert casual text to formal text: Two teams are looking into MT systems where the input is controlled through a back-and-forth between the user and the computer. In these systems, monolingual speakers type out messages in their"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. We’re talking about something called \"Permutation Prediction\" (PP). Imagine you have a sentence with two important things in it, like two entities (let’s call them e1 and e2). We can split this sentence into five sections: the stuff before e1 (left context), e1 itself, the stuff between e1 and e2 (middle context), e2 itself, and the stuff after e2 (right context). Now, what if we mix up these five parts? Would a special tool (the entity pair encoder) still be able to figure out which parts belong to e1 and e2? That’s the question we’re curious about. To test this, we came up with a new idea called \"permutation prediction.\" Basically, we take all the possible ways to shuffle these five parts (there are 5! = 120 ways to do that). Each shuffle gets a number, from 1 to 120, depending on how it’s arranged. Then, we use the entity pair encoder to look at the shuffled sentence and guess which number (or \"class\") it belongs to. The goal is to make the encoder as good as possible at this guessing game by using something called cross-entropy loss, which helps compare the encoder’s guess to the correct answer. Of course, checking all 120 permutations would be a lot of work, so in practice, we just pick a few (n p) at random, making sure to include the correct one.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. We’re talking about something called \"Permutation Prediction\" (PP). Imagine you have a sentence with two important"
    },
    {
        "casual_text": "MT researchers have been using this method and testing it on some outputs from MT systems and reference translations from the WMT20 news translation tasks. We’ve put our findings in Table 3. The first row shows the original SacreBLEU scores, which are detokenized. The second and third rows show what happens when you lowercase everything or normalize punctuation—the scores go up. The last three rows show results for tokenized MT outputs. If you normalize punctuation and use aggressive tokenization with Moses scripts, the BLEU scores jump up by several points compared to the original SacreBLEU scores. Now, none of these scores in different rows can be directly compared, but MT papers still often report tokenized BLEU scores alongside tokenized or even detokenized scores from other papers, without really knowing how the tokenization was done. A lot of MT papers use the multi-bleu script from Moses to calculate these scores, even though the script itself warns: \"The scores depend on your tokenizer, which is unlikely to be reproducible from your paper or consistent across research groups.\" Even though Post (2018) tried to improve MT evaluation, we think it’s just a temporary fix for some shaky evaluation practices.",
        "formal_text": "Convert casual text to formal text: MT researchers have been using this method and testing it on some outputs from MT systems and reference translations from the WMT20 news translation tasks. We’ve put our findings in Table 3."
    },
    {
        "casual_text": "Tax diya gaya hai match ka.",
        "formal_text": "Tax diya gaya hai match ka. Convert casual text to formal text: Tax diya gaya hai match ka. Convert casual text to formal text: Tax diya gaya hai match ka."
    },
    {
        "casual_text": "When we're looking at how well a topic model is doing, we usually try to figure out how connected the words are within a topic. Once we have a way to measure how related two words are, we can define topic coherence as the average of these measurements for all the word pairs in the most likely words for that topic. Newman and his team (2010) used Pointwise Mutual Information (PMI) for this, based on how often words appear together in external text collections. Later, Mimno and his colleagues (2011) found that a tweaked version of PMI worked better when compared to what experts thought. AlSumait and his team (2009) figured out which topics were useless by seeing how different they were from the overall word distribution in the text. Fang and his team (2016a) decided to use the cosine similarity between word vectors to measure topic coherence. We liked this last method because it can be used in many different situations, so we used it to check how well tweet groups fit together. We looked at GloVe (Pennington and his team, 2014) and BERTweet (Nguyen and his team, 2020) word vectors, which were made using big Twitter text collections. To make things better and less jumpy, we followed Lau and Baldwin's (2016) idea of averaging the coherence scores for different numbers of top words (like 5, 10, 15, and 20).",
        "formal_text": "Convert casual text to formal text: When we're looking at how well a topic model is doing, we usually try to figure out how connected the words are within a topic. Once we have a way to measure how related"
    },
    {
        "casual_text": "In Figure 4, we use a triplet neural network (Hoffer and Ailon, 2015) to model f, with c being a random target pulled from the target knowledge base. While training, we generate k m triplets for each argument. From these, we apply a triplet loss function to reduce the cosine distance d between s i and c, while increasing d between s i and c.",
        "formal_text": "Convert casual text to formal text: In Figure 4, we use a triplet neural network (Hoffer and Ailon, 2015) to model f, with c being a random target pulled from the target knowledge base."
    },
    {
        "casual_text": "We're using these specific hyperparameters for all our self-training experiments. Any other possible hyperparameters are set to the default values. For more details, check out the code we've included.",
        "formal_text": "Convert casual text to formal text: We're using these specific hyperparameters for all our self-training experiments. Any other possible hyperparameters are set to the default values. For more details, check out the code we"
    },
    {
        "casual_text": "We're using Splitta v1.03, which you can find here: https://code.google.com/p/splitta/. For GLPK, we're on version 4.52, available at https://www.gnu.org/software/glpk/. We also use the stoplist from NLTK, which is here: http://www.nltk.org/. And we're rocking the Porter stemmer from NLTK as well.",
        "formal_text": "Convert casual text to formal text: We're using Splitta v1.03, which you can find here: https://code.google.com/p/splitta/. For GLPK,"
    },
    {
        "casual_text": "Theorem G. 3 (Soundness): If you get a goal configuration c using either LTF or LTL, the AM dependency tree that c describes will always be well-typed.",
        "formal_text": "Convert casual text to formal text: Theorem G. 3 (Soundness: If you get a goal configuration c using either LTF or LTL, the AM dependency tree that c describes will always well-"
    },
    {
        "casual_text": "When dealing with noun compounds, the parser starts by giving them a standard right-branching structure. Then, during Stage 1 processing, this structure gets flattened into a simple list of nouns. After that, Stage 2 can decide how to group these nouns together in different ways. The way this part of Lucy's design works is kind of inspired by Hobbs (1985). He suggested having a \"surface\" logical form that uses words similar to actual English ones and follows a structure that’s close to how the sentence is put together. Right now, Lucy can handle strings of adjectives and nouns as if they were idioms, as well as verb/particle and verb/preposition combinations. We’ve tested the idea of extending this to deal with full VP idioms like \"kick the bucket\" (meaning \"die\"), and it seems doable, but this feature isn’t in the system yet. Lucy’s logical form includes something called a discourse referent, which comes from ideas by Kamp (1984) and Helm (1982). This means that when a noun phrase like \"a bucket\" or \"the bucket\" shows up, we can usually refer back to it using \"it.\" But if we use the idiom \"kick the bucket\" to mean \"die,\" we can’t do that. So, idioms need to be spotted before discourse referents are created.",
        "formal_text": "Convert casual text to formal text: When dealing with noun compounds, the parser starts by giving them a standard right-branching structure. Then, during Stage 1 processing, this structure gets flattened into"
    },
    {
        "casual_text": "4) \"He handed the butcher ten bucks.\"",
        "formal_text": "\"He handed the butcher ten bucks.\" Convert casual text: 4) \"He handed the butcher ten bucks.\" Convert formal text: 4) \"He handed the butcher ten bucks.\" Convert casual text:"
    },
    {
        "casual_text": "We use the gated model and train it just like Schick and Schütze did in 2018. We also stick to the same character n-gram lengths as FastText.",
        "formal_text": "Convert casual text to formal text: We use the gated model and train it just like Schick and Schütze in 2018. We also stick to the same character n-gram lengths as FastText."
    },
    {
        "casual_text": "Constantly giving safe answers can make a chatbot really boring, so it's important to try and avoid that when creating the learning algorithms. To fix this, we need a more creative model that can handle both relevance and variety—basically, it should understand how to mix things up while still staying on topic when dealing with the way people actually talk.",
        "formal_text": "Convert casual text to formal text: Constantly giving safe answers can make a chatbot really boring, so it's important to try and avoid that when creating the learning algorithms. To fix this, we need a more creative"
    },
    {
        "casual_text": "Recently, some research has shown that the follow-up studies to (Snell et al., 2017) might not be as solid as they seem, especially when it comes to classifying short texts. There are two main reasons for this, according to (Dopierre et al., 2021). First, these comparative studies use simple datasets with few and not very distinct classes. For example, the SNIPS dataset (Coucke et al., 2018), which is pretty well-known, only has 7 classes, and the best model out there can get over 99% accuracy (Cao et al., 2020). Second, as we’ve gotten better at fine-tuning and refining BERT-based models (Niven and Kao, 2019; Liu et al., 2019b; Khetan and Karnin, 2020), it’s hard to say if these meta-learning frameworks are really cutting-edge because of their architecture or just because the text encoders they use have improved over time. (Dopierre et al., 2021) argues that Prototypical Networks (Snell et al., 2017), which originally used LSTM-based text encoders in NLP, are actually the best for intent detection when paired with a fine-tuned BERT model. So, improving Prototypical Networks turns out to be a pretty tough challenge in practice.",
        "formal_text": "Convert casual text to formal text: Recently, some research has shown that the follow-up studies to (Snell et al., 2017) might not be as solid as they seem, especially when it comes to classifying short"
    },
    {
        "casual_text": "For human evaluation, we ask people to decide if one caption is less, equally, or more informative than another when comparing it to a question-answer pair. We also collect ratings on two important things that should be good no matter who the audience is: (1) fluency (if the caption is grammatically correct and makes sense) and (2) fidelity (if the caption doesn’t say anything false about what’s in the image).",
        "formal_text": "Convert casual text to formal text: For human evaluation, we ask people to decide if one caption is less, equally, or more informative than another when comparing it to a question-answer pair. We also collect ratings on"
    },
    {
        "casual_text": "Also, we create a version of dialogue data where each utterance is split by a special marker called the separator. This could be something like 'EOU>' for models that use word embeddings or a '|' for models using subword embeddings. In all our tests, both news and dialogues are cut off at 400 tokens, and summaries are limited to 100 tokens. However, we didn’t set a cap on the length of the generated summaries.",
        "formal_text": "Convert casual text to formal text: Also, we create a version of dialogue data where each utterance is split by a special marker called the separator. This could be something like 'EOU>' for models"
    },
    {
        "casual_text": "We think it's both possible and a good idea to explain how preferences come together in a more meaningful way, without getting into the nitty-gritty of complex algorithms. Basically, we're more interested in understanding what these preferences actually are, rather than focusing on how to calculate them quickly. We're not saying that everything about super-complex models should be explained at this level. But, most language models don't really make use of the cool stuff that can only be explained when you look at things in a distributed way.",
        "formal_text": "Convert casual text to formal text: We think it's both possible and a good idea to explain how preferences come together in a more meaningful way, without getting into the nitty-gritty of complex algorithms."
    },
    {
        "casual_text": "Basically, the classifiers can either have separate weights and calculations or share them. But the most exciting and useful scenario is when they share both.",
        "formal_text": "Convert casual text to formal text: Basically, the classifiers can either have separate weights and calculations or share them. But the most exciting and useful scenario when they share both."
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. First, we start by setting up G  (y|x) using something called maximum likelihood estimation (MLE). This is a standard method where we look at a bunch of captions (x, ) from a different domain—basically, captions that aren’t specific to what we’re working on but are still useful. This helps us get a good starting point for our system by giving it some basic ideas about images, even if those ideas aren’t exactly what we’re aiming for in the end. Now, we have these generic captions, which we can think of as y = ( 1, . . . ,  n ). To make things work better, we try to minimize something called cross-entropy.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. First, we start by setting up G  (y|x) using something called maximum likelihood estimation (MLE). This"
    },
    {
        "casual_text": "Let's break down what MACHAMP can do. For regular tasks where you need to predict things at the token level, like part-of-speech tagging, MACHAMP uses a straightforward approach. It applies greedy decoding and pairs it with a softmax output layer, all based on the contextual embeddings it generates.",
        "formal_text": "Convert casual text to formal text: Let's break down what MACHAMP can do. For regular tasks where you need to predict things at the token level, like part-of-speech tagging, MACHAMP"
    },
    {
        "casual_text": "We use a partial block adjacency matrix to simplify calculations and cut down on noise when dealing with extreme multi-label classification. The tests we ran show that our approach can hold its own against the best methods out there.",
        "formal_text": "Convert casual text to formal text: We use a partial block adjacency matrix to simplify calculations and cut down on noise when dealing with extreme multi-label classification. The tests we ran show that our approach can hold its own"
    },
    {
        "casual_text": "The POS feature doesn't really care much about how much syntactic data you throw at it. This is probably because LaSO tweaks its importance in a smart way—if the syntactic info isn't good, it just kind of ignores it.",
        "formal_text": "Convert casual text to formal text: The POS feature doesn't really care much about how much syntactic data you throw at it. This is probably because LaSO tweaks its importance in a smart way—if the"
    },
    {
        "casual_text": "In a study by Liu et al. (2020), they looked at 13 different ways to model context for neural and neural-symbolic CDSP parsers using two popular datasets. Turns out, none of these methods consistently outperformed the others across all scenarios. The best performers were usually the ones that combined the last k utterances for decoding and copied parsing actions from previous MRs. Liu et al. (2020) also broke down context into 12 detailed types organized in a hierarchy based on linguistic phenomena. They checked how different linguistic factors affected the models. One cool thing they found was that all the methods struggled with coreference issues that needed complex reasoning. But keep in mind, they didn’t compare these methods with ones that specifically handle coreference resolution. Another cool finding was that the models did better on sentences that just added to the meaning of previous sentences, rather than ones that replaced part of the meaning of earlier sentences.",
        "formal_text": "Convert casual text to formal text: In a study by Liu et al. (2020), they looked at 13 different ways to model context for neural and neural-symbolic CDSP parsers using two popular"
    },
    {
        "casual_text": "To set up fine-grained Named Entity Recognition (NER) in TexSmart, we need to create an ontology of entity types. The TexSmart ontology was put together semi-automatically, using the term clusters shown in Figure 2. Each term cluster is tagged with one or more hypernyms, which act as the type names for that cluster. First, we did some basic stats on the term clusters to identify the most common type names (i.e., the ones with lots of clusters). Then, we manually created one or more formal types from these popular type names and added the type name to the list of names for those formal types. For instance, the formal type \"work. movie\" was created from the type name \"movie,\" and the word \"movie\" was added to the name list for \"work. movie.\" Similarly, formal types \"language. human_lang\" and \"language. programming\" were created from the type name \"language,\" and the word \"language\" was added to the name lists for both of these types. Each formal type also has a sample instance list, which we usually keep short to save time. These instances are picked manually from the clusters that match the type names. We also manually define the supertype/subtype relationships between the formal types. This process gives us a type hierarchy with around 1,000 formal types, each with a standard ID (like \"work. movie\") and a list of names.",
        "formal_text": "Convert casual text to formal text: To set up fine-grained Named Entity Recognition (NER) in TexSmart, we need to create an ontology of entity types. The TexSmart ontology was put"
    },
    {
        "casual_text": "In Section 2, we take a quick look at how the idea of auxiliary verbs has changed over time, focusing on the key points that matter for our discussion. Section 3 dives into the auxiliation process and gives examples in Brazilian Portuguese for four types of auxiliaries: tense, aspect, modality, and passive voice. Section 4 shows how auxiliary verbs can be annotated in UD, weighing the pros and cons, and suggests a way to simplify different annotation methods into a single, common interpretation of auxiliaries. Finally, Section 5 wraps up our study.",
        "formal_text": "Convert casual text to formal text: In Section 2, we take a quick look at how the idea of auxiliary verbs has changed over time, focusing on the key points that matter for our discussion. Section 3 dives into the"
    },
    {
        "casual_text": "Okay, let me break this down in simpler terms: In the first part, they're talking about using a specific method (A. Yoder, 2015) to figure out how to make something work better. The idea is to use this method to improve how things are done, especially when it comes to making decisions or solving problems. The method helps by focusing on the core issues and not getting distracted by other stuff. Then, they mention a concept called \"mental models,\" which are like mental shortcuts people use to understand things. The first mental model (let's call it Model 1) is more about general understanding, while the second one (Model 2) is more detailed and specific. For Model 2, they used 215 participants to test how well it worked in real-world situations. The results showed that Model 2 was better at handling complex problems compared to Model 1. Model 1 is described as being more intuitive and easy to use, but Model 2 is shown to be more effective in solving problems, especially when things get complicated. Finally, they compare the two models: 1. Model 1 is simpler and more general, while Model 2 is more complex and specific. 2. The participants using Model 2 were able to focus better on the main problem, while those using Model 1 sometimes got distracted. C' = workbook 1 2 3",
        "formal_text": "Convert casual text to formal text: Okay, let me break this down in simpler terms: In the first part, they're talking about using a specific method (A. Yoder, 2015) to figure out how to make something"
    },
    {
        "casual_text": "We’ve come up with a cool new interactive method called PIIA to make text-to-SQL work better, especially for tricky SQL queries across different areas. PIIA makes the whole process super easy for users by asking multiple-choice questions and keeping the number of questions as low as possible. It’s designed to work with any parser, so it can team up with whatever base parser you’re using and be ready for real-world systems. We ran a bunch of tests with five different base parsers on two big cross-domain datasets, and the results show that PIIA really works well, both in simulations and when people tried it out.",
        "formal_text": "Convert casual text to formal text: We’ve come up with a cool new interactive method called PIIA to make text-to-SQL work better, especially for tricky SQL queries across different areas. PII"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: MTP is similar to MTPC, MTS is similar to MTSC, and TT is similar to TTC. Making sure these were comparable was super important for this study—if they weren’t, any comparison of style or syntax would’ve been messed up. The corpora were put together carefully to make sure they were comparable. The design criteria included things like diatopic, diachronic, diasystematic, and domain stuff. All the translated texts are in British or American English as the source language and Spanish (from Spain) as the target language. Both the translated and non-translated corpora are about the same size. MTP is made up of biomedical translations done by professional translators—either in-house or freelancers working for certified companies in Europe. It’s a specialized reference corpus because it doesn’t include full documents, just bits and pieces from translation memory (TM) segments. The text types vary a lot, from research papers and clinical essays to textbooks, product descriptions, PILs, user guides, and instructions for surgical equipment. The comparable non-translated biomedical Spanish corpus has a similar mix of text types and topics. It’s a bit of a mixed bag, including fragments and whole documents: TM segments from different sources than the ones used for MTP, a small diabetes-related corpus, and a custom virtual corpus that was put together to match MTP in terms of sub-domains, topics, level of specialization, and text types.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: MTP is similar to MTPC, MTS is similar to MTSC, and TT is similar to TTC."
    },
    {
        "casual_text": "We take a close look at how the latest unsupervised NMT systems perform on a bunch of real and made-up translation tasks.",
        "formal_text": "Convert casual text to formal text: We take a bunch of real and made-up translation tasks. Convert casual text to formal text: We take a bunch of real and made-up translation tasks. Convert casual text to"
    },
    {
        "casual_text": "Sometimes, when translating, a word-for-word approach just doesn’t work well in the target language. In those cases, you need to think outside the box and use different strategies, like changing the part of speech or finding a more idiomatic expression, which might even be culturally specific. Take the phrase \"vivid memories,\" for example, as in \"I still have vivid memories of my childhood.\" The bilingual Cambridge dictionary suggests translating \"vivid\" as ; ; , which kind of means \"seeing something brought to life.\" But these words don’t really pair well with \"memory\" () in Chinese. So, the best translation isn’t always sticking to the same structure (ADJ+N), but instead, you might shift things around. For instance, you could say  (clearly remember),  (impressive, unforgettable), or use some four-character idioms like  and , which both convey the idea of remembering something very clearly.",
        "formal_text": "Convert casual text to formal text: Sometimes, when translating, a word-for-word approach just doesn’t work well in the target language. In those cases, you need to think outside the box and use different strategies, like"
    },
    {
        "casual_text": "On the surface, the answer is a big \"no.\" For example, you can use \"if\" to bring in a noun clause after a verb that shows what someone's trying to do with their words.",
        "formal_text": "Convert casual text to formal text: On the surface, the answer is a big \"no.\" For example, you can use \"if\" to bring in a noun clause after a verb that shows what someone's"
    },
    {
        "casual_text": "We tweaked GEC models to work with five CEFR levels: A2, B1, B2, C1, and C2. Table 1 shows that our models did better than the \"Random\" baseline for all levels. The biggest jump, 5.2 F0.5 points, was for A2, the easiest level. We think this happened because A2 has more errors, less variety in words, and isn't as common in the random sample the baseline was trained on. On the other hand, for B1 and B2, which are more common in the random sample, the improvements were smaller: 0.7 and 0.2 F0.5 points, respectively. Our adapted models outperformed the JD single model across all levels, especially for A2 and C1, where the difference was huge. For Turkish, the improvement was 3.6 F0.5 points. For languages that aren't as common in the random sample (like Greek, Turkish, Arabic, Polish, and Russian), we saw consistent improvements of over 2 F0.5 points.",
        "formal_text": "Convert casual text to formal text: We tweaked GEC models to work with five CEFR levels: A2, B1, B2, C1, and C2. Table 1 shows that our models did better than the \"Random\""
    },
    {
        "casual_text": "Let’s talk about how relation heads affect things. When we’re figuring out the relationships between different representations, we split them into m relation heads. We tested how changing the number of these relation heads affects performance on SST-2 and MNLI. As you can see in Table 2, using more relation heads helps because it makes it easier for the model to understand those high-dimensional relationships by breaking them down into smaller, more manageable pieces. However, we noticed that once m gets too big, adding even more doesn’t really help much—it just makes things slower and uses up more memory. So, in our setup, we went with m = 64.",
        "formal_text": "Convert casual text to formal text: Let’s talk about how relation heads affect things. When we’re figuring out the relationships between different representations, we split them into m relation heads. We tested how changing the number of"
    },
    {
        "casual_text": "Using themes with frames gives you that sense of continuity without needing to bring everything up again from the start.",
        "formal_text": "Convert casual text to formal text: Using themes with frames gives you that sense of continuity without needing to bring everything up again from the start. Convert casual text to formal text: Using themes with frames gives you that sense of"
    },
    {
        "casual_text": "Hey, just wondering, how much would it cost for a double room with full board?",
        "formal_text": "Convert casual text to formal text: Hey, just wondering, how much would ... Convert casual text to formal text: Hey, just wondering, how much would ... Convert casual text to formal text: Hey, just wondering,"
    },
    {
        "casual_text": "Text-image matching is definitely the hardest part of our system. Even though we’re using a really advanced method to pair text with images, the results just aren’t great. To get a better idea of how well we could do if everything was perfect, we picked five topics for each language and manually matched the text and images ourselves. The results for these topics are in Tables 7 and 8. The tests show that if we had the perfect text-image matches, the quality of the summaries would improve a lot. This proves that visual info is super important for our system. In Figures 4 and 5, you can see an image along with the text descriptions we got using different methods. From this, we can say that the image captions really matter.",
        "formal_text": "Convert casual text to formal text: Text-image matching is definitely the hardest part of our system. Even though we’re using a really advanced method to pair text with images, the results just aren’t great. To get"
    },
    {
        "casual_text": "Got it! Let's break this down in simpler terms: We need to tweak the main part of the L T definition. So, let’s say M is a tree with some extra features, like (O, D, d), and u is just any random node in that tree. Now, for any well-formed formulas (wffs)...",
        "formal_text": "Convert casual text to formal text: Got it! Let's break this down in simpler terms: We need to tweak the main part of the L T definition. So, let’s say M is a tree with some extra features"
    },
    {
        "casual_text": "We also have something in common with the legal question answering task (Kim et al., 2014a), which focuses on answering yes/no questions from Japanese legal bar exams. Both of us think that relevant law articles are super important for making decisions in civil law systems. But there’s a difference: this task asks participants to first find the relevant articles from the Japanese Civil Code and then use those articles to answer the yes/no questions. The first part is usually seen as an information retrieval task, and the second part is more like a textual entailment task (Kim et al., 2014b; Carvalho et al., 2016).",
        "formal_text": "Convert casual text to formal text: We also have something in common with the legal question answering task (Kim et al., 2014a), which focuses on answering yes/no questions from Japanese legal bar exams. Both"
    },
    {
        "casual_text": "Even though evaluating these systems was kind of tricky (check out section 4.1), we still managed to build and test a bunch of TTS systems for the Indigenous languages mentioned in section 3. We already had a basic concatenative model for Kanien'kéha that we made earlier using Festival and Multisyn (Taylor et al., 1998; Clark et al., 2007). On top of that, we trained some fresh FastSpeech2 models for each language and also tweaked some models for 25k steps using a multilingual, multispeaker FastSpeech2 model that was pre-trained with a mix of VCTK (Yamagishi et al., 2019), Kanien'kéha, and Gitksan recordings. To make things work, we created a rule-based system to map how words are written to how they sound for each language, using the 'g2p' Python library. This helped us align and synthesize at the phone level instead of just the character level (Pine et al., Under Review).",
        "formal_text": "Convert casual text to formal text: Even though evaluating these systems was kind of tricky (check out section 4.1), we still managed to build and test a bunch of TTS systems for the Indigenous languages mentioned in section 3. We already"
    },
    {
        "casual_text": "Okay, let's break this down into something more readable and less confusing. Here's the informal version: \"So, there's this thing where we're looking at some numbers and stuff. Like, 0.001, 0.445, and a bunch of other small numbers popping up here and there. There's also a 0.036 and a 0.223 mixed in. Then there's this word 'droit' and some other stuff like 'information' and 'réclament' showing up. There's a 0.499 and a 0.152, and then we have 'nos concitoyens' which sounds like it's about citizens or something. There's also a 0.171 and a 0.323 in there. At the end, there's a 0.954 and a bunch of 0.001s scattered around. It's kind of a mix of numbers and words, but it seems like it's trying to say something about rights, information, and citizens, with a lot of tiny numbers thrown in for good measure.\"",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down into something more readable and less confusing. Here's the informal version: \"So, there's this thing where we're looking at some numbers and"
    },
    {
        "casual_text": "People are starting to worry more about the environmental impact of machine learning research, especially since certain tasks, like neural architecture search, are pumping out a crazy amount of carbon emissions—like, \"boiling the ocean\" levels (thanks, Strubell et al., 2019). All that extra carbon is a big contributor to global warming. Plus, things like searching for the right parameters in research and development make the problem even worse. And when you're using cloud-based machines, the environmental damage is closely tied to how much money you're spending.",
        "formal_text": "Convert casual text to formal text: People are starting to worry more about the environmental impact of machine learning research, especially since certain tasks, like neural architecture search, are pumping out a crazy amount of carbon emissions—like, \"b"
    },
    {
        "casual_text": "For our starting points, we’ve got two basic models we’re calling \"return all\" and \"vector similarity.\" The return all model is pretty simple—it just says that for any given relationship, all the definitions should be included. Obviously, this isn’t meant to create a useful network because it’d link a ton of definitions and lemmas that don’t really belong together. It’s basically a worst-case scenario that gives us the best possible recall but terrible precision. This helps us see the minimum precision we need to improve upon.",
        "formal_text": "Convert casual text to formal text: For our starting points, we’ve got two basic models we’re calling \"return all\" and \"vector similarity.\" The return all model is pretty simple—it just says that for"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We're trying to figure out how much a case helps us understand the meaning of a verb, which we call CCD. Basically, if the different examples of a case (like subjects or objects) cover a wide range of meanings, it’s more helpful for figuring out what the verb is actually doing. For example, if a verb has multiple meanings (let’s say it has n different senses, like s1, s2, ..., sn), and each sense has its own set of examples for a particular case (let’s call them $s1, c, $s2, c, etc.), then the case is more useful if these sets don’t overlap much. In other words, if the sets of examples for each verb sense in a case have fewer common elements, the case is doing a better job of helping us tell the verb’s different meanings apart. This is what equation 6 is all about—it helps us calculate how much a case contributes to making the verb’s meaning clear.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. We're trying to figure out how much a case helps us understand the meaning of a verb, which we call C"
    },
    {
        "casual_text": "Since the dictionary is pretty small and the system is straightforward, it’s not too hard to set up or keep running, and it’s super fast. That’s why JICST decided to use it for their huge database of over 400,000 citations each year, saving them money too. Thanks to some improvements, the system can now handle both Japanese and Western languages. We even got an award for this work—the Japan Association of Information and Documentation’s Prize of Learning in 1980—and we’ve got a patent for it (Japan Patent Kokai Sho 55 (1980) -102074).",
        "formal_text": "Convert casual text to formal text: Since the dictionary is pretty small and the system is straightforward, it’s not too hard to set up or keep running, and it’s super fast. That’s why JICST decided to"
    },
    {
        "casual_text": "Step 3: Cross-project split. Check out the middle part of Figure 4. Projects are randomly assigned to training, validation, and test sets. This is separate from the time segments and in-project splits we did in steps 1 and 2.",
        "formal_text": "Convert casual text to formal text: Step 3: Cross-project split. Check out the middle part of Figure 4. Projects are randomly assigned to training, validation, and test sets. This is separate from the time segments and in-project"
    },
    {
        "casual_text": "The proof follows a similar approach. First, we show a related lemma: if i is the active node, then O c (i) is less than or equal to W c. After that, we create a function called C LT L (check out Algorithm 4) that generates a valid sequence of transitions. We keep applying this sequence until we reach the desired goal configuration.",
        "formal_text": "Convert casual text to formal text: The proof follows a similar approach. First, we show a related lemma: if i is the active node, then O c (i) is less than or equal"
    },
    {
        "casual_text": "This means we're looking at the t-th n-gram feature for a specific scale, S m. After that, we use max pooling (MP) along the T-axis for each n-gram feature matrix to pull out the most important features for each scale. Finally, we combine all these features together to create a multi-scale feature representation called F, which is a matrix with dimensions M  d.",
        "formal_text": "Convert casual text to formal text: This means we're looking at the t-th n-gram feature for a specific scale, S m. After that, we use max pooling (MP) along the T"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We're talking about systems that let you chat with machines, like search engines that talk to you or robots you can interact with. The tricky part is that the environment keeps changing, and the way we interact isn't always predictable. Then there's the whole thing about using structured knowledge—basically, organized information—to help with tasks in natural language processing (NLP), like generating text. This helps with stuff like expanding on ideas, filtering out unnecessary details, making sure the meaning is clear, and adapting to what the user needs. The main goal here is to tackle these challenges by answering some key questions.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. We're talking about systems that let you chat with machines, like search engines that talk to you or robots you can interact"
    },
    {
        "casual_text": "Another way to tackle the identifiability issue is by bumping up d_v to d_e and throwing in the heads' outputs. This setup is basically what we call the \"Add\" mode in the regular settings. When d_k is 8 or more, we noticed that the performance of Add doesn't take as big of a hit as Con when d_k increases. This might be because the bigger value vector gives Add more parameters, which helps make up for the drop in accuracy. On bigger datasets, Add actually does a bit better than Con. If you look at fig. 1, you'll see that we can make the value vector bigger to increase the space where each token is projected. A higher-dimensional space can hold more semantic info, which helps with the specific task at hand.",
        "formal_text": "Convert casual text to formal text: Another way to tackle the identifiability issue is by bumping up d_v to d_e and throwing in the heads' outputs. This setup is basically what we call"
    },
    {
        "casual_text": "In Figure 1, the function transprob gives the probability for a specific trlgraln. The functions initial-step and final-step handle [be tl'aliSlt[ons I%L sltlillllce ], Olll|dlll'ieg.",
        "formal_text": "Convert casual text to formal text: In Figure 1, the function transprob gives the probability for a specific trlgraln. The functions initial-step and"
    },
    {
        "casual_text": "• Seq2Seq: the basic sequence-to-sequence model, introduced by Sutskever et al. in 2014.",
        "formal_text": "• Seq2Seq: the basic sequence-to-sequence model, introduced by Sutskever et al. in 2014. Convert casual text to formal text: • Seq2Seq:"
    },
    {
        "casual_text": "A bunch of awesome people helped out with designing the task, getting the data ready, and setting up the software. We want to give a shoutout to Dotan Dvir from Hebrew University of Jerusalem for taking charge of the UCCA annotation work. Dan Flickinger at Stanford University did some fresh gold-standard annotations for around 1,000 WSJ strings, which are part of the EDS evaluation graphs from 2020. Sebastian Schuster from Stanford gave us tips on how to turn the old-school PTB and OntoNotes treebank annotations into Universal Dependencies, version 2.x, using more up-to-date tokenization methods. Anna Nedoluzhko and Ji Mrovsk from Charles University in Prague improved the PTG annotation for LPPS data by adding stuff that was missing, like coreference. Milan Straka from the same university shared an updated version of his UDPipe parser and helped train morpho-syntactic parsing models for Czech, English, and German (for the MRP companion trees). Jayeol Chun from Brandeis University was super helpful with converting Chinese AMR annotations, preparing the Chinese morpho-syntactic companion trees, and getting companion alignments ready for the English AMR graphs.",
        "formal_text": "Convert casual text to formal text: A bunch of awesome people helped out with designing the task, getting the data ready, and setting up the software. We want to give a shoutout to Dotan Dvir from Hebrew University of"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: We compared our approach to a few different methods, which you can check out in Table 1. At the top of the table, you'll find the Lead baseline and Oracle. For CNN/DM, the Lead baseline is Lead-3, and for XSum, it's Lead-1. In the middle part of Table 1, there's a basic transformer-based summarizer that takes \"sentence representation + position encoding\" as input. There are also two variations of this: one without positional encoding and another that only uses positional encoding. At the bottom, you'll see Shuffling (Grenander et al., 2019), which is a recent method aimed at reducing the lead bias in summarization. There's also Learned-Mixin (Clark et al., 2019), a general debiasing method designed for NLP tasks when you know the type of bias in your training data and have a bias-only model. In our case, the bias is lead bias, and the bias-only model is a transformer trained with just positional encoding.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: We compared our approach to a few different methods, which you can check out in Table 1. At the top of the table"
    },
    {
        "casual_text": "One thing to keep in mind is that all four of the code-mixed datasets we picked are written in the Latin script. So, we're not dealing with any mixing of different scripts here.",
        "formal_text": "Convert casual text to formal text: One thing to keep in mind is that all four of the code-mixed datasets we picked are written in the Latin script. So, we're not dealing with any mixing of different script"
    },
    {
        "casual_text": "Some characters are consistently marked as \"S\" or \"K\" once the context is clear. We can take advantage of this to narrow down our search when decoding. Let’s say c i is the i th character compared to the current character c 0, and t i is the tag for c i. The context templates we’re using are shown in TABLE 7. When looking at a training corpus, if a context C around c 0 always gives c 0 a specific tag t, we can use this rule to help us filter options. We determine how \"always\" this happens by checking the data.",
        "formal_text": "Convert casual text to formal text: Some characters are consistently marked as \"S\" or \"K\" once the context is clear. We can take advantage of this to narrow down our search when decoding. Let’s say c"
    },
    {
        "casual_text": "Visual question answering is all about figuring out the answer by thinking about both the question and the image together. Most methods these days try to combine the question and image into one shared space that both can understand. In these methods, the question is usually looked at as a whole, which makes it tough to explain how the answer is actually found (Tan and Bansal, 2019; Lu et al., 2019; Selvaraju et al., 2020).",
        "formal_text": "Convert casual text to formal text: Visual question answering is all about figuring out the answer by thinking about both the question and the image together. Most methods these days try to combine the question and image into one shared space that both can understand"
    },
    {
        "casual_text": "In this analysis, if the resolver said \"None\" for any items, those were just treated as missing values. (a) Ambiguous Condition (b) Unambiguous Condition",
        "formal_text": "Convert casual text to formal text: In this analysis, if the resolver said \"None\" for any items were just treated as missing values. (a) Ambiguous Condition (b) Unambiguous Condition (c)"
    },
    {
        "casual_text": "We need to add these compound terms into the transducer. To do this, we'll create a local regular grammar that explains how these compounds change morphologically, based on the inflectional model from (Kartunnen et al., 1992). The idea is that only the two main parts of the compounds can change. So, either N1 or A1, and N2 or A2 in patterns like VI prep N2, N1 N2, 41 N2, and Vl A2. In our data, we found two types of morphological changes:",
        "formal_text": "Convert casual text to formal text: We need to add these compound terms into the transducer. To do this, we'll create a local regular grammar that explains how these compounds change morphologically, based on"
    },
    {
        "casual_text": "Lexicalized methods could totally use pretrained word embeddings too, since they help show how words are related in terms of both grammar and meaning. There have been some newer ideas about contextual word embeddings popping up lately.",
        "formal_text": "Convert casual text to formal text: Lexicalized methods could totally use pretrained word embeddings too, since they help show how words are related in terms of both grammar and meaning. There have been some newer ideas about contextual"
    },
    {
        "casual_text": "Step 4: Get rid of sentences that are less than 10 words long, any duplicates, and ones that start and end with quotation marks.",
        "formal_text": "Convert casual text to formal text: Step 4: Get rid of sentences that are less than 10 words long, any duplicates, and ones start and end with quotation marks. Step 5: Get rid of sentences that are less than 10 words"
    },
    {
        "casual_text": "Plus, our setup has some cool new features: The phrases people use to point things out come from conversations between two people, and they might mention something more than once.",
        "formal_text": "Convert casual text to formal text: Plus, our setup has some cool new features: The phrases people use to point things out from conversations between two people, and they might mention something more than once. Plus, our setup has some cool new"
    },
    {
        "casual_text": "When we looked at how well different classifiers did, linear SVM came out on top with an accuracy of 0.788, which is better than what human annotators achieved. A lot of the citations in the paper don’t fit neatly into just one category, so some mistakes in classification are bound to happen. This is kind of obvious when you see disagreements between the annotators, as mentioned by Cohan et al. in 2014. This issue affects how diverse the results are and ultimately how good the summaries turn out to be.",
        "formal_text": "Convert casual text to formal text: When we looked at how well different classifiers did, linear SVM came out on top with an accuracy of 0.788, which is better than what human annotators achieved. A lot of"
    },
    {
        "casual_text": "We've got two kinds of features: (i) input embeddings, which are for q, qi, and cim, and (ii) pairwise features, which are for (q, qi), (q, cim), and (qi, cim). Check out Figure 2a for more details.",
        "formal_text": "Convert casual text to formal text: We've got two kinds of features: (i) input embeddings, which are for q, qi, and cim, and (ii) pairwise features"
    },
    {
        "casual_text": "The Szeged Treebank, created by Csendes and others in 2005, is a manually annotated treebank for Hungarian that includes 82,000 sentences. It doesn't just mark up the phrase structure; it also notes the grammatical roles of the verbs' arguments and adds morphological details. The treebank covers texts from six different areas: short business news, newspapers, law, literature, compositions, and informatics. But in this paper, we're only looking at the short business news part.",
        "formal_text": "Convert casual text to formal text: The Szeged Treebank, created by Csendes and others in 2005, is a manually annotated treebank for Hungarian that includes 82,000 sentences. It doesn't"
    },
    {
        "casual_text": "We discovered that a translation recommendation model, when trained on scores from automatic evaluation metrics, can achieve a precision of over 0.9 and a recall of more than 0.75, as long as the thresholds are set right for each post-editor. When we tested it against the consensus of post-editors, the model still managed to hit a precision of over 0.8. This backs up the method from (He et al., 2010) that uses automatic evaluation metrics to estimate the effort needed for post-editing.",
        "formal_text": "Convert casual text to formal text: We discovered that a translation recommendation model, when trained on scores from automatic evaluation metrics, can achieve a precision of over 0.9 and a recall of more than 0.75, as long as the"
    },
    {
        "casual_text": "Hey, just a heads-up: when we take out the type constraints in LTL (let's call it \"LTL, no types\"), the accuracy takes a nosedive. Turns out, up to half of the predicted AM dependency trees end up being not well-typed, which means they can't be turned into a proper graph for evaluation. The neural model doesn't seem to figure out how to build well-typed trees on its own—those type constraints are super important.",
        "formal_text": "Convert casual text to formal text: Hey, just a heads-up: when we take out the type constraints in LTL (let's call it \"LTL, no types\"), the accuracy takes a nosedive"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: We're looking at three parts here: 1. The difference between F1 and F2 at point x1, which is |F1(x1) - F2(x1)|. 2. Then, we're taking the difference between F1 and F2 from x1 to x2, so it's |F1(x2) - F1(x1) - [F2(x2) - F2(x1)]|. 3. Finally, we're looking at the difference between F1 and F2 at point x2, which is |F1(x2) - F2(x2)|. All of this is multiplied by 2, as shown in equation (11).",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: We're looking at three parts here: 1. The difference between F1 and F2 at point x1, which is |F"
    },
    {
        "casual_text": "We can look at this connection in a few different ways. For every statement, can we figure out the right events it's talking about? That's what we call utterance-level alignment.",
        "formal_text": "Convert casual text to formal text: We can look at this connection in a few different ways. For every statement, can we figure out the right events it's talking about? That's we call utterance-level alignment"
    },
    {
        "casual_text": "One way to mix things up is by shuffling around phrases in a translated sentence. To get a rough idea of how many different ways you can rearrange these phrases, you can look at places where two-word sequences don’t match up. If a phrase is stuck between two of these mismatches, you can move it around however you want because doing so won’t mess up the matching ngrams, and that means it won’t lower the Bleu score.",
        "formal_text": "Convert casual text to formal text: One way to mix things up is by shuffling around phrases in a translated sentence. To get a rough idea of how many different ways you can rearrange these phrases, you can look at"
    },
    {
        "casual_text": "Here are some of the latest projects that have been approved. We'll have to wait and see which ones actually turn out to be useful. MABLe is supposed to be a multilingual writing system that helps people create documents in a language they're not very familiar with, guiding them step by step. MAY is going to give users access to yellow pages information in multiple languages. MULTIMETEO will take basic weather data from meteorological computers and turn it into forecasts in different languages. RECALL is a module for language learners that gives them feedback and translations. SPARKLE is working on tools for analyzing sentence structure that can be easily adapted to various languages, as well as semi-automatic ways to gather vocabulary. SPEEDATA will let people input spoken words directly into a system used for land registries in Italy and Germany. LINGUANET is developing a multilingual communication system for police, building on the experience of a similar system used for the Channel Tunnel between English and French. TREE will allow people to search for job opportunities in a networked database in multiple languages. TRADE is a project aimed at translating social security reports into Italian, Spanish, and English.",
        "formal_text": "Convert casual text to formal text: Here are some of the latest projects that have been approved. We'll have to wait and see which ones actually turn out to be useful. MABLe is supposed to be a multilingual writing"
    },
    {
        "casual_text": "The key findings are all laid out in Table 6, covering both the cross-framework (middle section) and cross-lingual (bottom section) tracks. The results are broken down into smaller parts. Edge attributes are only found in PTG and UCCA. Even though they’re still not predicted very well, there’s a noticeable improvement compared to MRP 2019 (where the best score for UCCA edge attributes was 0.12 F1, but now it’s up to 0.36). Anchors, on the other hand, are predicted with much lower scores than in MRP 2019, likely because we didn’t include the bi-lexical Flavor (0) frameworks in MRP 2020. Edges and tops are a bit more accurate now, while labels and properties are slightly less accurate. However, these results aren’t directly comparable since the frameworks and data are different. For a deeper look at how things stack up between MRP 2019 and MRP 2020, check out 8.",
        "formal_text": "Convert casual text to formal text: The key findings are all laid out in Table 6, covering both the cross-framework (middle section) and cross-lingual (bottom section) tracks. The results are broken down into smaller parts"
    },
    {
        "casual_text": "Check out the evaluation results for POS Tagging and coarse-grained NER in Table 3. The speeds are shown in sentences per second and were tested on a machine with a Platinum 8255C CPU running at 2.50GHz. Just a heads-up: the speed numbers for Log-linear and CRF were measured using just one thread, but for DNN, they used 6 threads.",
        "formal_text": "Convert casual text to formal text: Check out the evaluation results for POS Tagging and coarse-grained NER in Table 3. The speeds are shown in sentences per second and were tested on a machine with a Platinum 8255"
    },
    {
        "casual_text": "The difference between inflection and derivation isn't super clear-cut; it's more of a gradual thing, not black and white (Haspelmath and Sims, 2010). Take the suffix \"ly\", for example—some people see it as inflectional, and others think it's derivational (Bauer, 2019). We do our best to avoid dealing with inflectional stuff as much as we can (like by using lemmatization), but let's be real—there's no perfect way to separate the two in actual language.",
        "formal_text": "Convert casual text to formal text: The difference between inflection and derivation isn't super clear-cut; it's more of a gradual thing, not black and white (Haspelmath and"
    },
    {
        "casual_text": "We tested our approach on a big dataset (from Cheng et al., 2020). The results show that our method did way better than the best method out there, improving the F1 score by 7.11%.",
        "formal_text": "Convert casual text to formal text: We tested our approach on a big dataset (from Cheng et al., 2020). The results show that our method did way better than the best method out there, improving the F1"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Azzam the American, also known as Adam Yahiye Gadahn, was connected to companies like Gadahn STX Finland Kvaerner Masa Yards, Aker Finnyards, and STX Finland Ltd.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Azzam the American, also known as Adam Yahiye Gadahn was connected to companies like Gadahn ST"
    },
    {
        "casual_text": "When training neural networks, words are usually turned into \"word vectors,\" which are kind of like spread-out representations of words. These word vectors can either be made beforehand or created specifically for a certain task. Even if you use a pre-trained word vector model, which is also made with a neural network, the training process can be super quick. Some recent cool work shows that you can process over 100 billion tokens in just one day on a single computer (Mikolov et al., 2013c). Another perk of using a pre-trained word vector model is that it’s super flexible. You can use it later for all sorts of different tasks. Plus, there’s no real difference between a pre-trained word vector model and one made specifically for a task. So, it makes sense to use them together. In this paper, we suggest doing domain adaptation using big pre-trained word vector models instead of just raw text. Basically, we’re adapting from a big pre-trained word vector model into one that’s specific to the task at hand. This way, we can use massive general domain (GD) corpora without slowing things down too much. We can also tweak richer GD word representations to fit into in-domain (ID) training.",
        "formal_text": "Convert casual text to formal text: When training neural networks, words are usually turned into \"word vectors,\" which are kind of like spread-out representations of words. These word vectors can either be made beforehand or created specifically for"
    },
    {
        "casual_text": "Sure! Here's the informal version: The annotation files shared in the paper by Lee et al. (2021) were originally in Brat Annotation standoff format, which are the .ann files, along with their matching news articles in .txt format. These were converted into json format. Then, each sentence in the dataset was processed using the Stanford CoreNLP toolkit. This included breaking the text into sentences, splitting words, tagging parts of speech, identifying named entities, and doing dependency parsing to create dependency parse trees. For the model input, we used a \"multichannel\" approach (as seen in \"1\" in Figure 5), which combines three parts. Let’s say W = w1, w2, ..., wn represents a sentence with n tokens, where wi is the i-th token.",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: The annotation files shared in the paper by Lee et al. (2021) were originally in Brat Annotation standoff format, which are the"
    },
    {
        "casual_text": "We're using a Logistic Regression (LR) model that takes into account both argument-relevant (ArgF) and sarcasm-relevant (SarcF) features. Unless otherwise stated, all the features are pulled from the current turn, ct.",
        "formal_text": "Convert casual text to formal text: We're using a Logistic Regression (LR) model that takes into account both argument-relevant (ArgF) and sarcasm-relevant (SarcF) features"
    },
    {
        "casual_text": "When aligning sentences and cleaning up parallel corpora, all the extra words from complex languages can make things tricky. This \"sparsity problem\" makes it harder to match up sentence pairs, which lowers the confidence in our matches and messes with how well we can classify them. As a result, we end up with smaller or less accurate parallel corpora. When they made ParIce, an English-Icelandic parallel corpus, the filtering process cut the size by about 20%. But even after that, around 5% of what was left had issues (check out Section 3 for more details). We’re planning to work with the same data and aim to bring those numbers down. This brings us to the third and final research question we’re focusing on in this proposal:",
        "formal_text": "Convert casual text to formal text: When aligning sentences and cleaning up parallel corpora, all the extra words from complex languages can make things tricky. This \"sparsity problem\" makes it harder to match up sentence pairs,"
    },
    {
        "casual_text": "Statistical language modeling has done really well in speech and language processing, as shown by studies like those from Clarkson and Rosenfeld (1997) and Stolcke (2002). But, this success mostly relies on having a ton of good text data in a language. Without enough text data to train on, it's super hard to create a practical and useful statistical language model. That's why most of the progress has been made with languages like English, Mandarin, and Japanese, which have had a lot of resources put into them. Right now, there are over 6000 living languages spoken around the world (Gordon et al., 2005), and most of them don't have much written text, so they're considered resource-poor (Nakov and Ng, 2009). Many of these languages are spoken by a huge number of people (like some Chinese and Indian languages), so there's still a big need to develop speech and language processing systems for them.",
        "formal_text": "Convert casual text to formal text: Statistical language modeling has done really well in speech and language processing, as shown by studies like those from Clarkson and Rosenfeld (1997) and Stolcke (2002). But, this success mostly relie"
    },
    {
        "casual_text": "Sure! Here's a more informal version: When we talk about \"spaces,\" we're really talking about different ways to group or organize things based on their meanings. We're trying to find a space that can show the special features of a \"frame.\" One key feature is that words in the same frame often go together or can replace each other (like \"assassin\" and \"kill\" or \"assassinate\" and \"kill\"). This means they're either related in a way that they can be used together (syntagmatic) or they can be swapped out for each other (paradigmatic). In a perfect world, if we compare \"assassinate\" with \"kill\" and \"assassin\" with \"kill,\" we'd get a high similarity score for both pairs. We're checking out three different spaces that seem to do a good job with this: 1. **Word-based space**: Here, the context is made up of words that show up near the main word in a sentence. This kind of space is good at showing general semantic connections. If two words are close in this space, they're probably related in some way, either because they can be used together (syntagmatic) or because they can replace each other (paradigmatic), like being synonyms, hypernyms, or antonyms. 2. **Syntax-based space**: In this space, the context is based on the grammar of the sentence (like \"X-VSubj-man\" where X is the main word). This space is great at showing semantic similarity. If two words are close in this space, they're likely to be in a paradigmatic relationship, meaning they fit into a kind of \"is-a\" hierarchy. Since the context is based on grammar, words with the same part of speech are much closer to each other than words with different parts of speech.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more informal version: When we talk about \"spaces,\" we're really talking about different ways to group or organize things based on their meanings. We'"
    },
    {
        "casual_text": "The problem of overfitting here comes from a bigger issue: when dealing with really unbalanced data in stochastic optimization, the model tends to focus too much on the larger class. This happens especially when the loss function is broken down into parts for each class (like with cross-entropy loss). Because of this, our solution also helps with regular imbalanced classification problems, especially when dealing with limited batch sizes.",
        "formal_text": "Convert casual text to formal text: The problem of overfitting here comes from a bigger issue: when dealing with really unbalanced data in stochastic optimization, the model tends to focus too much on the larger class"
    },
    {
        "casual_text": "Some researchers have already figured out how to automatically figure out things like rhetorical decisions and intentions based on stuff like tense, aspect, pronoun patterns, it-clefts, and discourse markers. But even after making these judgments, we still need to figure out all the possible ways to interpret the discourse that fit with these judgments and are actually valid. This paper offers ways to find and list all the valid structures of a discourse, which helps us study how text structures and intentions are connected in a more measurable way.",
        "formal_text": "Convert casual text to formal text: Some researchers have already figured out how to automatically figure out things like rhetorical decisions and intentions based on stuff like tense, aspect, pronoun patterns, it-clefts,"
    },
    {
        "casual_text": "The backward search works by coming up with the N best guesses one after the other, and you don’t have to decide on N beforehand. It’s way less complicated than the forward search.",
        "formal_text": "Convert casual text to formal text: The backward search works by coming up with the N best guesses one after the other, and you don’t have to decide on N beforehand. It’s way less complicated than the forward"
    },
    {
        "casual_text": "Given how tricky Thai spaces can be for the SBD task, we’re suggesting a word-based labeling method. Instead of trying to classify spaces, we treat Thai SBD as a word labeling problem. Basically, we see spaces as regular words and label each word as either SB (Sentence Boundary) or nSB (non-Sentence Boundary). Figure 2 shows a comparison between the space-based classification method and our word-based labeling approach. For reference, Figure 1 gives an example of Thai text where the first sentence has two spaces, but there’s no space at the end of the sentence, marked by eol>. \"eol\" just means end-of-line.",
        "formal_text": "Convert casual text to formal text: Given how tricky Thai spaces can be for the SBD task, we’re suggesting a word-based labeling method. Instead of trying to classify spaces, we treat Thai SBD as"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: Take a training pair (x_t, y_t) from dataset D. Then, update the parameter  by subtracting the learning rate _t multiplied by the gradient of the loss function L with respect to , using the training pair (x_t, y_t). This is basically the gradient step.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: Take a training pair (x_t, y_t) from dataset D. Then, update the parameter"
    },
    {
        "casual_text": "Here's Figure 5: It shows how the encoder-decoder with exact hard monotonic attention works in three different scenarios. On the left, it's in the regular, common-practice setup. In the middle, it's in a \"wug test\"-like situation. And on the right, it's the same \"wug test\"-like setup, but with our best data hallucination method added in.",
        "formal_text": "Convert casual text to formal text: Here's Figure 5: It shows how the encoder-decoder with exact hard monotonic attention works in three different scenarios. On the left, it's in the regular, common-"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. We need to figure out the sentiment polarity (let's call it pol i) for a specific item,  i. To do this, we'll look at two things: SC (r1) and cf e( i• ). Basically, we're using these two pieces of information to predict how positive or negative the sentiment is for  i.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. We need to figure out the sentiment polarity (let's call it pol i) for a specific item,"
    },
    {
        "casual_text": "b. Not everyone showed up. No women came. c. More than half the issues were tackled by everyone. d. It's not true that nobody came. e. At least one problem was handled by everyone. The main rule for figuring out UDRSs is called the Replacement Rule (RR). Basically, if you have a UDRS K:  in a database A, and A supports K: ->>/C, then you can add K: g to A. RR works based on a substitution rule. Here's how the >>-rules work: SUBST: Let’s say you have a DRS component hK in a UDRS U, and A is your UDRS database. If K: ' is what you get by replacing K with K' in K, then A supports KK: >>/C' if either (i) or (ii) is true.",
        "formal_text": "Convert casual text to formal text: b. Not everyone showed up. No women came. c. More than half the issues were tackled by everyone. d. It's not true that nobody came. e."
    },
    {
        "casual_text": "Most computers store text as a bunch of characters, which include letters, spaces, numbers, and punctuation marks.",
        "formal_text": "Convert casual text to formal text: Most computers store text as a bunch of characters, which include letters, spaces, numbers, and punctuation marks. Convert casual text to formal text. Convert casual text to formal text."
    },
    {
        "casual_text": "What happens if the amount of available data for the source and target languages is super different? This is pretty common with low-resource language pairs that include English, since there's usually tons of data for English but not so much for the other language.",
        "formal_text": "Convert casual text to formal text: What happens if the amount of available data for the source and target languages is super different? This is pretty common with low-resource language pairs that include English, since there's usually tons of"
    },
    {
        "casual_text": "We adjust the final score so it always stays between 0 and 1. To handle nodes that don't match in both the alignment area and the alignment range, we tweak Equation (2) by dividing it by:",
        "formal_text": "Convert casual text to formal text: We adjust the final score so it always stays between 0 and 1. To handle nodes that don't match in both the alignment area and the alignment range, we tweak Equation (2) by dividing"
    },
    {
        "casual_text": "To handle labels that aren't perfect because of distant supervision, we’re stepping away from the usual sequence labeling approach and coming up with a fresh prediction model. Instead of trying to label each individual token, we’re focusing on figuring out if two tokens next to each other belong to the same entity (or if they’re “broken”). The main idea here is that, even if distant supervision messes up the boundaries of an entity, the connections inside the entity usually stay intact and are less affected by noise. So, we’ve created a new tagging system called “Tie or Break” to make better use of this noisy data. Our new model, called AutoNER, works by first finding all possible entity spans by looking for these ties, and then figuring out the type of each entity. We’ve built a cool neural network architecture to do this. In our tests, AutoNER performed better than the Fuzzy CRF model, so it seems to be working well.",
        "formal_text": "Convert casual text to formal text: To handle labels that aren't perfect because of distant supervision, we’re stepping away from the usual sequence labeling approach and coming up with a fresh prediction model. Instead of trying to"
    },
    {
        "casual_text": "So, E R is basically a matrix that holds all the dense embeddings for different parts like syntactic categories, predicate contexts, and characters, and they're all trained together. Lastly, since the model only calculates probabilities for x t, r t pairs that definitely lead to w t, the probability of w t given these variables, P(w t | q t1 t x t r t ), is always the same, no surprises there.",
        "formal_text": "Convert casual text to formal text: So, E R is basically a matrix that holds all the dense embeddings for different parts like syntactic categories, predicate contexts, and characters, and they're"
    },
    {
        "casual_text": "Lex: This rule is pretty broad. At its most basic, it lets you switch a sentence from having an indefinite quantifier with a specific meaning to one with a non-specific meaning. If the difference between specific and non-specific comes from a universally quantified noun phrase (NP), the rule works because (a, l, s)(every, l, s) is true. But if there are other elements that affect scope between the indefinite and the universal in some cases, the rule only applies if those elements act just like the universal, meaning they let the indefinite stay non-specific. So, if that element is another universally quantified NP, we can use the rule, but if it's a negation, we can't.",
        "formal_text": "Convert casual text to formal text: Lex: This rule is pretty broad. At its most basic, it lets you switch a sentence from having an indefinite quantifier with a specific meaning to one with a non-specific meaning"
    },
    {
        "casual_text": "First off, we generate a list of SLU hypotheses using a Stochastic Conceptual Language Model. This is pretty much the same method as described in (Raymond and Riccardi, 2007), but we use the SRILM toolkit (Stolcke, 2002) to train the language model and then turn it into a Stochastic Finite State Transducer (SFST). This approach lets us work with a variety of language models, whether they're backed-off or interpolated, and we can use all sorts of smoothing techniques (Chen and Goodman, 1998) to make it work better.",
        "formal_text": "Convert casual text to formal text: First off, we generate a list of SLU hypotheses using a Stochastic Conceptual Language Model. This is pretty much the same method as described in (Raymond and Ric"
    },
    {
        "casual_text": "To really understand this stuff, you need to dive into some serious fieldwork. Look at things like locus, topic and comment, speech acts, how speakers feel, and reference-related stuff (like anaphora, deixis, ellipsis, definite descriptions, etc.). In computational linguistics, these areas are usually looked at from the perspective of understanding language. But when it comes to generating language, the methods can be pretty different from analyzing it (check out Nirenburg and Raskin, 1987 for an example). To highlight what’s needed for natural language generation, we’ll focus on a generation-oriented way of dealing with speech acts and speaker attitudes.",
        "formal_text": "Convert casual text to formal text: To really understand this stuff, you need to dive into some serious fieldwork. Look at things like locus, topic and comment, speech acts, how speakers feel, and reference-related stuff (like"
    },
    {
        "casual_text": "In these situations, you don't have to translate word by word because the number of larger chunks of language, or even whole sentences, you're dealing with might still be pretty limited. Translating whole sentences or using sentence patterns could work well here and is already being used to some extent. The tricky parts of this kind of machine translation aren't as big of a deal and are similar to the challenges in regular MT, so you don't need to worry about any extra complications.",
        "formal_text": "Convert casual text to formal text: In these situations, you don't have to translate word by word because the number of larger chunks of language, or even whole sentences, you're dealing with might still be pretty limited. Translat"
    },
    {
        "casual_text": "Alright, so there are O(n2) entries in the chart, and that means the whole algorithm can take up to O(n5 f(n)2) time in the worst-case scenario. Sections 6.1 and 6.2 talk more about the function f(n).",
        "formal_text": "Convert casual text to formal text: Alright, so there are O(n2) entries in the chart, and that means the whole algorithm can take up to O(n5 f(n)2) time in the worst-case"
    },
    {
        "casual_text": "It's calculated by adding up the info you get from each item. If you have two things, X and Y, you can use the probability of their score differences to figure it out.",
        "formal_text": "Convert casual text to formal text: It's calculated by adding up the info you get from each item. If you have two things, X and Y, you can use the probability of their score differences to figure it out."
    },
    {
        "casual_text": "We took three of our test sets—Text+Berg, Twitter, and Combilex—and split them up based on the length of the input strings. Then, we tested two types of models, PCRF-Seq2Seq and encoder-decoder neural models, on these smaller groups of data. As you can see in Figures 2 and 3, there’s a clear pattern: PCRF-Seq2Seq holds up pretty well no matter how long the input strings are. On the other hand, the encoder-decoder models start to struggle a lot more as the sequences get longer, especially the ones without an attention mechanism. For shorter sequences, though, things look a bit different. Standard encoder-decoder models actually do a little better than both their attention-based versions and PCRF-Seq2Seq in tasks like Twitter spelling correction (check out Figure 3) and G2P conversion. But this isn’t the case when we look at the full datasets, where they don’t perform as well. On the Text+Berg data, all the models are pretty much on par when it comes to short sequences, as shown in Figure 2.",
        "formal_text": "Convert casual text to formal text: We took three of our test sets—Text+Berg, Twitter, and Combilex—and split them up based on the length of the input strings. Then, we tested"
    },
    {
        "casual_text": "We took six meetings, each with three summaries from different sources, made copies of them, and then randomly picked two different meetings to assign to each judge for annotation. As an example, an annotation sample would include an ASR transcript and three human-written summaries. Since the AMI dataset is pretty small, we set the batch size and initial learning rate for BERTSumExt to 8 and 5e-4, respectively. For BERTSumExtAbs, the batch size is 16, and the initial learning rates for BERT and the transformer decoder are 0.001 and 0.01, respectively. The ROUGE scores (ROUGE-1 and ROUGE-2) for the model are shown in Table 6, which compares the performance for summaries of 300 words on the AMI Corpus. The two-step model (Shang et al., 2018) is also included for comparison. BART(base) refers to BART with 6 encoder and decoder layers and 140 million parameters. Li et al.’s model uses audio, video, and text, while the transformer models (the last two rows) have a lot more pre-training data. Oh, and in one of the meetings, the group talked about the shape of the device and decided to make it simpler.",
        "formal_text": "Convert casual text to formal text: We took six meetings, each with three summaries from different sources, made copies of them, and then randomly picked two different meetings to assign to each judge for annotation. As an example, an annotation"
    },
    {
        "casual_text": "One big challenge is figuring out how to deal with unclear parts of the sentence as soon as possible, so we can decide on the final translation quickly. A lot of sentences are still confusing until we’ve looked at the whole thing, so figuring out what’s what puts some limits on how we handle the translation process. But, like a human translator, we don’t start translating until we’re pretty sure we understand the sentence. This lets our model take a \"wait and see\" approach when there are multiple possible meanings while processing the input. However, sometimes there are still some unclear parts left, and the model has to pick one option, even if it might be wrong. This gets even trickier when the source and target languages have really different structures. For example, in English, \"not\" comes before the verb, but in Japanese, \"not\" comes after the verb, and the verb is at the end of the sentence. In cases like this, we can’t start translating until we’ve processed the verb at the end and checked if there’s a \"not\" after it. So, we have to wait until we see the next part of the sentence to clear up the confusion. Luckily, most Japanese sentences are made up of multiple parts, which makes it possible to translate them as we go.",
        "formal_text": "Convert casual text to formal text: One big challenge is figuring out how to deal with unclear parts of the sentence as soon as possible, so we can decide on the final translation quickly. A lot of sentences are still confusing until we’"
    },
    {
        "casual_text": "So, the N-best list keeps track of the top options (where N is the spot where we last stopped). One cool thing about this algorithm is that it quickly gets rid of mismatched combinations, helping us throw out bad options all at once.",
        "formal_text": "Convert casual text to formal text: So, the N-best list keeps track of the top options (where N is the spot where we last stopped). One cool thing about this algorithm that it quickly gets rid of mismatched combinations, helping"
    },
    {
        "casual_text": "Semi-supervised learning has become pretty popular lately because it lets us train big models without needing a ton of training data. Researchers have been working on pre-trained masked language models, like BERT (Devlin et al., 2018), where just an encoder is used to rebuild the text. Liu and Lapata (2019) took BERT and turned it into a seq2seq encoder, showing better results on a bunch of abstractive summarization tasks. Similarly, other researchers have come up with pre-trained seq2seq models using a different semi-supervised approach, where the model learns to reconstruct the original text, like BART (Lewis et al., 2019) and MASS (Song et al., 2019). In our work, we focus on transfer learning and show that by pre-training with the right mix of text data, a seq2seq model can easily adapt to a new domain with just a few examples.",
        "formal_text": "Convert casual text to formal text: Semi-supervised learning has become pretty popular lately because it lets us train big models without needing a ton of training data. Researchers have been working on pre-trained masked language models"
    },
    {
        "casual_text": "We did an error analysis and found that a lot of the mistakes the resolver made were with the pronoun 'it'—about half of the errors, to be exact. We noticed that if we just ignored 'it' and didn’t try to resolve it, the resolver’s accuracy went up by around 16 points. We also figured out that some of the errors come from how we automatically tag gender: it looks like a lot of coreference chains have mentions of different genders mixed together. But, we think the system does a pretty good job with masculine and feminine pronouns, which is all we need for our experiments since they only deal with those two.",
        "formal_text": "Convert casual text to formal text: We did an error analysis and found that a lot of the mistakes the resolver made were with the pronoun 'it'—about half of the errors, to be exact. We noticed"
    },
    {
        "casual_text": "• Our smaller model actually works better than the original black-box and uses way fewer settings to tweak. Plus, it can run as a real-time service on devices with limited resources, like your phone or something.",
        "formal_text": "Convert casual text to formal text: • Our smaller model actually works better than the original black-box and uses way fewer settings to tweak. Plus, it can run as a real-time service on devices with limited resources, like"
    },
    {
        "casual_text": "The main difference between our method and the old ones is that we don’t rely on guessing or testing the results of the rule compilation by hand. Instead, we use a more solid approach: we create finite-state transducers to model phonological rules that mimic how loanwords are adapted, and then we automatically calculate how well these rules work and tweak them to improve their coverage. The cool thing about using a computer is that we can instantly see how accurate the system is during each step of developing and tweaking the rules, which would be almost impossible to do with old-school, manual methods.",
        "formal_text": "Convert casual text to formal text: The main difference between our method and the old ones is that we don’t rely on guessing or testing the results of the rule compilation by hand. Instead, we use a more solid approach"
    },
    {
        "casual_text": "Based on the research by Yin and colleagues (Yin et al., 2018b), we're using a BiLSTM network along with a self-attention mechanism to process the text before and after the zero pronoun. The self-attention mechanism helps us figure out how much importance each word in the surrounding text should get. This way, we can create a more detailed and effective representation of the zero pronoun.",
        "formal_text": "Convert casual text to formal text: Based on the research by Yin and colleagues (Yin et al., 2018b), we're using a BiLSTM network along with a self-attention mechanism to"
    },
    {
        "casual_text": "The go-to model for topic modeling is called latent Dirichlet allocation (LDA), which was introduced by Blei et al. in 2003. It's a type of model that works with documents. In this setup, each document is labeled with an \"i,\" and the words in the documents are the data we can see. The hidden stuff, or latent variables, includes i (which tells us the topic mix for a document) and z (which shows how the words are assigned to topics). There's also a model parameter called k, which deals with how words are distributed across topics. These notations get a bit more detailed in Table 1 later on. The process the model uses to create documents is like this:",
        "formal_text": "Convert casual text to formal text: The go-to model for topic modeling is called latent Dirichlet allocation (LDA), which was introduced by Blei et al. in 2003. It's a type of"
    },
    {
        "casual_text": "CodeXGLUE has another dataset on security vulnerabilities. It’s not as real-world as REVEAL because it’s balanced, but it’s often used by Transformer-based models to test their tools for vulnerability detection (VD). To compare with existing methods, we used the CodeXGLUE train/valid/test splits for training and testing. We followed the benchmark’s design and used accuracy as the evaluation metric. The results are in Table 1. We compared our approach with four deep-learning-based VD tools. VulDeePecker and SySeVR use program slices and RNN/CNN to identify vulnerable patterns. Devign uses graph-based neural networks (GNN) to analyze program data dependencies. REVEAL combines GNN with SMOTE and triplet loss to handle imbalanced data. We also tested pre-trained models like RoBERTa, CodeBERT, and GraphCodeBERT, along with a 12-Layer Transformer model trained from scratch. In our case, the best version of DISCO, which uses contrastive learning and the NT-MLM objective, outperformed all the baselines, including the graph-based approaches and larger pre-trained models. This shows that DISCO can effectively understand code semantics and data dependencies even with limited data, which helps in spotting vulnerable patterns. We found that including hard negative samples (like buggy code examples) improved DISCO’s performance.",
        "formal_text": "Convert casual text to formal text: CodeXGLUE has another dataset on security vulnerabilities. It’s not as real-world as REVEAL because it’s balanced, but it’s often used by Transformer-based models to"
    },
    {
        "casual_text": "The recombination algorithm works by looking for the highest value in the matrix, which is between sequence i and sequence j. Once it finds that, it combines sequence i and sequence j by sticking them together. After that, it wipes out all the numbers in the matrix that were linked to sequence j (both in its row and column). The new combined sequence (i and j together) takes over the values that were originally for sequence j in the matrix.",
        "formal_text": "Convert casual text to formal text: The recombination algorithm works by looking for the highest value in the matrix, which is between sequence i and sequence j. Once it finds that, it combines sequence i and"
    },
    {
        "casual_text": "We ran the baseline on all datasets except for German nouns. We think the reason German nouns stand out is because of the high level of syncretism in German noun tables. To understand why syncretism is an issue, take the German noun \"Gräben\" as an example. It has eight forms in its paradigm, but four of them are exactly the same: \"Gräben\". This same form is the most common one among the top 10,000 forms in German Wikipedia. According to Section 2, this results in 12 training examples where both the input and output are \"Gräben\". This heavily pushes the system to just copy the input to the output. But this approach won't ever give the right answer because, by design, the missing forms can't be \"Gräben\". This seems more like a problem with our datasets than the model itself. So, a key future step in tackling the PCFP from an acquisition standpoint is to create more realistic and accurate datasets that better reflect how learners are exposed to words, both in terms of types and frequencies. This will help us truly assess how hard the PCFP really is. There's a big jump when you move from seeing just one form in each inflection table to seeing two forms. With just two forms, we're already close to the accuracies reported in earlier studies (like Malouf, 2016 and 2017) that used almost complete tables for training—only 10% of the forms were missing. Also, our encoder-decoder model performs way better than the generator model designed for the same task, even when both have the same amount of training data. This holds true for almost all of our datasets.",
        "formal_text": "Convert casual text to formal text: We ran the baseline on all datasets except for German nouns. We think the reason German nouns stand out is because of the high level of syncretism in German noun tables"
    },
    {
        "casual_text": "The Text Generator creates Chinese abstracts at three different levels: 1. **Discourse Model Level**: This part decides the overall structure and content of the abstract, following the discourse model we talked about earlier. It keeps an eye on how the sentences flow together to make sure the final text is well-organized and makes sense. 2. **Rhetorical Structure Level**: Here, it builds a paragraph with multiple sentences that are logically connected. It uses appropriate Chinese conjunctions to ensure the text is not only coherent but also sounds good rhetorically. This is based on the rhetorical structure theory by Mann and Thompson, which has been adapted for Chinese. 3. **Single Clause Level**: This level focuses on creating a grammatically correct sentence for a given predicate, using the predicate templates we discussed before. Basically, it makes sure each sentence is correct and fits in with the rest of the text.",
        "formal_text": "Convert casual text to formal text: The Text Generator creates Chinese abstracts at three different levels: 1. **Discourse Model Level**: This part decides the overall structure and content of the abstract, following the discourse model we talked"
    },
    {
        "casual_text": "We’ve come up with four categories to describe different feelings—positive, negative, conflict (which means both positive and negative at the same time), and neutral (when there’s no clear positive or negative vibe). To make sure everyone’s on the same page and we get consistent results, we’ve created some guidelines after testing them out on a small chunk of data a few times. After that, we had more people go through the whole dataset and label everything using those guidelines.",
        "formal_text": "Convert casual text to formal text: We’ve come up with four categories to describe different feelings—positive, negative, conflict (which means both positive and negative at the same time), and neutral (when there’s no clear positive or"
    },
    {
        "casual_text": "This feature hits its max value of 1 when all the content words are translated with a perfect probability of 1. The \"functional words translation strength\" feature is what we're talking about here. The idea is that if the functional words around content words are aligned correctly, they'll likely stay aligned in parallel sentences too. This is because, from a dependency-syntactic perspective, functional words (like prepositions, determiners, articles, particles, etc.) usually connect with or influence nearby content words. To get the dictionaries mentioned in this section, we ran GIZA++ on the JRC Acquis corpus. For pairs of source and target words, if they're not in the dictionary, we use a 0 to 1 normalized Levenshtein distance to assign a \"translation probability\" based on how similar the strings are. If the source and target words are similar enough (we set the threshold at 0.7 through experiments), we consider them translations. Mathematically, if   is the highest-scoring pair of aligned functional words near (within 3 words) the aligned pair of content words  , | | is the count of the best alignment found by ( ), and ( ) is the probability of the functional word pair from the dictionary, then...",
        "formal_text": "Convert casual text to formal text: This feature hits its max value of 1 when all the content words are translated with a perfect probability of 1. The \"functional words translation strength\" feature is what we're talking about here. The idea"
    },
    {
        "casual_text": "Okay, so for APPLY(, j), let's say i is the active node. Since we added an edge to j during the transition, W c + 1 is just the same as W c. Now, we'll break down O c again.",
        "formal_text": "Convert casual text to formal text: Okay, so for APPLY(, j), let's say i is the active node. Since we added an edge to j during the transition, W c +"
    },
    {
        "casual_text": "To see if our idea about how premise and conclusion targets are connected holds up, we tweaked Seq2Seq by adding a pointer generator (from See et al., 2017) and a little extra feature that tells the model if a word is part of a target or not. This helps the model figure out that connection. We’ll call this version Seq2Seq (with premise targets).",
        "formal_text": "Convert casual text to formal text: To see if our idea about how premise and conclusion targets are connected holds up, we tweaked Seq2Seq by adding a pointer generator (from See et"
    },
    {
        "casual_text": "In simpler terms, when working with a binary classifier in this BNNJM model, we can easily figure out the gradient for one example using something called Maximum Likelihood Estimation (MLE). This is cool because we don’t have to go through the whole vocabulary and calculate a softmax, which would be a pain. But, on the flip side, we do need to come up with \"positive\" and \"negative\" examples for the classifier to work with. The positive ones are pretty straightforward—we can just grab them directly from a parallel corpus where words are aligned between two languages.",
        "formal_text": "Convert casual text to formal text: In simpler terms, when working with a binary classifier in this BNNJM model, we can easily figure out the gradient for one example using something called Maximum Likelihood Estimation"
    },
    {
        "casual_text": "We also used supervised models that were built on top of multilingual BERT (Devlin et al., 2019). The training data came straight from WMT, which is the official source. We trained each model for 5 epochs, with a batch size of 12 and a learning rate set to 10-5. To get the best word-level performance, we tweaked the threshold on the development set.",
        "formal_text": "Convert casual text to formal text: We also used supervised models that were built on top of multilingual BERT (Devlin et al., 2019). The training data came straight from WMT, which is the"
    },
    {
        "casual_text": "Sure! Here's a more casual version: A. 1 Span, Column space, and Row space Let’s say you have a set of vectors V = v1, v2, ..., vn. The span of V, written as span(V), is basically all the possible combinations you can make by adding and scaling the vectors in V together.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: A. 1 Span, Column space, and Row space Let’s say you have a set of vectors V = v1,"
    },
    {
        "casual_text": "Using pattern-based or lexicon-based methods to figure out how intense adjectives are. Pattern-based methods look through big collections of text for phrases that show which word is stronger. For instance, something like \"not just X, but Y\" suggests that Y is more intense than X.",
        "formal_text": "Convert casual text to formal text: Using pattern-based or lexicon-based methods to figure out how intense adjectives are. Pattern-based methods look through big collections of text for phrases that show which word is stronger. For"
    },
    {
        "casual_text": "For a better comparison, we're also looking at the top models from Kim et al. (2020). It turns out that XLNet =0 performs the worst among all models when it comes to S-F1 and VP segment recall, but it has the least VP-A errors. This makes us think that the errors in segment recall might be grouped under a different type, like PP attachment. On the other hand, the right-skewed model XLNet =1.5 shows a big improvement over XLNet =0 in SBAR recall and is pretty much on par with S-DIORA in that area.",
        "formal_text": "Convert casual text to formal text: For a better comparison, we're also looking at the top models from Kim et al. (2020). It turns out that XLNet =0 performs the worst among all"
    },
    {
        "casual_text": "To get a better understanding of this, we got two bilingual folks to re-label 100 examples each in both English and French, following our usual validation process. We picked these examples from two separate random groups of development data, so the annotators didn’t see the original English text for any of the French translations they worked on. Without any special training or warm-up, these annotators matched the original English labels 85% of the time on the English data and 83% of the time on the French translations. This shows that the general meaning stayed pretty consistent between the two languages. Since most sentences are pretty straightforward to translate, especially the ones made by the workers, it doesn’t seem like the translation process added much confusion.",
        "formal_text": "Convert casual text to formal text: To get a better understanding of this, we got two bilingual folks to re-label 100 examples each in both English and French, following our usual validation process. We picked these examples from two"
    },
    {
        "casual_text": "In our initial experiments, we wanted to see if the model could put together separate concepts it learned during training. We used a selection of adjective-noun and verb-noun pairs from Nikolaus et al. (2019) and tweaked their train, validation, and test sets. The pairs we looked at are listed in Table 7.",
        "formal_text": "Convert casual text to formal text: In our initial experiments, we wanted to see if the model could put together separate concepts it learned during training. We used a selection of adjective-noun and verb-noun pairs from Nikola"
    },
    {
        "casual_text": "Sure! Here's a more casual version: The program sub-module is set up kind of like the program generator in FinQANet (Chen et al., 2021). Its main job is to create a runnable program that can answer the question. To do this, it uses an LSTM for decoding. At each step T, the LSTM picks one token from three options: 1) numbers pulled from the data, 2) predefined operators, and 3) tokens it already made in earlier steps. Once the program is done being built, the sub-module runs it and gets the answer it predicts.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: The program sub-module is set up kind of like the program generator in FinQANet (Chen et al., 202"
    },
    {
        "casual_text": "These models create summaries from both documents and videos, but they handle images differently. They use guidance strategies to figure out which parts of the text are most important.",
        "formal_text": "Convert casual text to formal text: These models create summaries from both documents and videos, but they handle images differently. They use guidance strategies to figure out which parts of the text are most important."
    },
    {
        "casual_text": "Okay, so let’s break this down in simpler terms. For each nonterminal (let’s call it A) that’s left-recursive, we look at all its expansions that don’t start with another left-recursive nonterminal. If there’s more than one of these expansions, we replace the original set of rules. Now, since all the new nonterminals we create this way aren’t left-recursive, Paull’s algorithm (with our best setup) won’t keep substituting these new nonterminals into other rules. This usually means the algorithm does fewer substitutions overall. We didn’t actually test how much bigger the grammar gets just from doing this NLRG transformation, but it’s pretty clear it adds two symbols for each left-recursive nonterminal we fix. So, the grammar size will go up by at most twice the number of left-recursive nonterminals. But, if some of those nonterminals only have one non-left-recursive expansion, the increase might be smaller. In Table 3, the fifth line (LF+NLRG+PA) shows what happens when we apply LF first, then NLRG, and finally PA. This combo makes the non-left-recursive version of the CT grammar a bit smaller and cuts the size of the ATIS grammar’s non-left-recursive form by a whopping 27.8 times compared to just doing LF+PA. However, the PT grammar’s non-left-recursive form is still way over the 5,000,000 symbol limit.",
        "formal_text": "Convert casual text to formal text: Okay, so let’s break this down in simpler terms. For each nonterminal (let’s call it A) that’s left-recursive, we look at all its expansion"
    },
    {
        "casual_text": "Factored models are a thing now, where we add word-level details like Part-of-Speech tags or lemmas to help with translation. People have been using source-side factors in statistical MT (Haddow and Koehn, 2012) and in NMT too (Sennrich et al., 2016b; Hoang et al., 2016). On the target side, Garca-Martnez et al. (2017) use factors to expand the usual NMT setup, allowing it to produce multiple sequences. While the main goal was to handle bigger vocabularies, Wilken and Matusov (2019) came up with some cool new uses for target-side factors, like predicting word case or handling subword segmentation. Our approach takes ideas from all these studies but stands out by using both source and target factors to bring in more semantic understanding in NMT.",
        "formal_text": "Convert casual text to formal text: Factored models are a thing now, where we add word-level details like Part-of-Speech tags or lemmas to help with translation. People have been using source-"
    },
    {
        "casual_text": "This paper presents PanPhon 1, a tool that includes a database connecting over 5,000 IPA segments (both simple and complex) to their definitions based on 21 articulatory features (check out Table 1 for details). It also comes with a Python package to work with this database and handle the representations it provides. While earlier papers (covered in Section 4) have talked about experiments using it, this is the first comprehensive description of PanPhon. It works alongside another package called Epitran 2. Table 1 gives an example of IPA segments and feature vectors from PanPhon. The whole process, known as the Epitran-Panphon pipeline, is shown in Figure 1. Here’s how it works: Epitran takes word tokens in their written form, like the Spanish word \"Madrid.\" It then converts this into a phonemic (not phonetic) representation using IPA, which in this case is /madid/. After that, Epitran uses a function from PanPhon to turn this IPA string into a sequence of feature vectors. Finally, it outputs this sequence, matching it with the original written form, capitalization, and Unicode character categories. The paper also demonstrates that the subsegmental features encoded in PanPhon are pretty handy for NLP tasks. Specifically, it’s been shown to boost performance in named entity recognition and converting lossy orthographies into IPA.",
        "formal_text": "Convert casual text to formal text: This paper presents PanPhon 1, a tool that includes a database connecting over 5,000 IPA segments (both simple and complex) to their definitions based on 21 articulatory features"
    },
    {
        "casual_text": "Ravfogel et al. (2020) found that having a binary gender concept helps the BIOS model make better predictions. We used amnesic probing (check out section 3.2) to mess with the gender concept, which was based on the binary gender labels in BIOS, and then applied our contrastive projection (section 3.3). Table 5 shows the top 5 pairs of professions and their gender percentages in BIOS, ranked by our contrastive measure  contr p, q. The top pairs are similar in meaning but different in gender representation (like paralegal and attorney). This proves that the model is using the gender concept to tell apart professions that are otherwise pretty similar.",
        "formal_text": "Convert casual text to formal text: Ravfogel et al. (2020) found that having a binary gender concept helps the BIOS model make better predictions. We used amnesic probing (check out section"
    },
    {
        "casual_text": "Sure! Here's the informal version: 1. https://github.com/yikangshen/PRPN 2. https://github.com/harvardnlp/urnng 3. https://github.com/davidswelt/dmvccm 4. https://github.com/DrDub/cclparser 5. https://github.com/iesl/diora",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: 1. https://github.com/yikangshen/PRPN 2. https://github.com/harvardnlp/ur"
    },
    {
        "casual_text": "So, if we take a sentence like \"John looked it up,\" it leads to this specific way of understanding it, where we focus on the particle meaning alone.",
        "formal_text": "Convert casual text to formal text: So, if we take sentence like \"John looked it up,\" it leads to this specific way to understanding it, where we focus on the particle meaning alone. Convert casual text to formal text:"
    },
    {
        "casual_text": "The NoAdapt model gets an average F1 score of 92.74%, which is better than what CRFs without fancy hand-crafted features can do. By using adaptation techniques, we consistently see a big improvement. First, the 1D&E model bumps the F1 score up to 94.16%. Then, using a generic LSTM (1D&L) instead of just generic embeddings pushes the score to 94.9%. The KD&E model, which is similar to what Ando and Zhang (2005) did, improves on 1D&E but doesn't do better than 1D&L. Finally, KD&L, which is based on Daumé III (2009), hits the best score of 95.5%. This shows that having multiple domain-specific LSTMs along with a generic one really helps across most domains.",
        "formal_text": "Convert casual text to formal text: The NoAdapt model gets an average F1 score of 92.74%, which is better than what CRFs without fancy hand-crafted features can do. By using adaptation techniques, we consistently see"
    },
    {
        "casual_text": "Alright, so we've got this d-dimensional embedding matrix called X. We do something called Singular Value Decomposition (SVD) on it, which gives us a diagonal matrix . This  has d singular values on its main diagonal, labeled 1, 2, ..., d, all sorted from largest to smallest. Our goal here is to figure out how different two embedding spaces are by looking at the stats of their singular values. In the next part, we'll explain what kind of stats we're talking about, and in section 2, we'll use those stats to check if these spaces are isomorphic, which basically means they're kind of the same structure-wise.",
        "formal_text": "Convert casual text to formal text: Alright, so we've got this d-dimensional embedding matrix called X. We do something called Singular Value Decomposition (SVD) on it, which gives"
    },
    {
        "casual_text": "Right now, the collection has 10 sections, with 100 documents in each. We’re planning to add more sections later to include different areas, so we can try out different algorithms for domain adaptation in Named Entity Linking.",
        "formal_text": "Convert casual text to formal text: Right now, the collection has 10 sections, with 100 documents in each. We’re planning to add more sections later to include different areas, so we can try out different algorithms for domain adaptation in Name"
    },
    {
        "casual_text": "Actually, there were just a few instances where words were in the dictionary but not in the Google News Corpus. Since there are so few of these cases, leaving them out wouldn’t really make a big difference in the clustering results.",
        "formal_text": "Convert casual text to formal text: Actually, there were just a few instances where words were in the dictionary but not in the Google News Corpus. Since there are so few of these cases, leaving them out wouldn’t really make"
    },
    {
        "casual_text": "Relevance for identification or retrieval might not always match up with how often something appears or how easily we can recall it. We'll explore this idea more in the future.",
        "formal_text": "Convert casual text to formal text: Relevance for identification or retrieval might not always match up with how often something appears or how easily we can recall it. We'll explore this idea more in the future."
    },
    {
        "casual_text": "We won't get into the details of why the disjunctive interaction model works here—it's just a standard approach we use. Both how things sound (syntax) and what they mean (semantics) are considered. The semantic stuff is baked into the conditional probabilities, like P[+cg] + c7] and P[+cgl + cs]. The loops in the original graph show how both the syntax and semantics support a certain interpretation. For a more complicated example, check out Figure 6. It shows how a \"rest in the afternoon\" schema creates a strong semantic preference that beats out a weaker syntactic preference caused by the vague term \"rest area.\"",
        "formal_text": "Convert casual text to formal text: We won't get into the details of why the disjunctive interaction model works here—it's just a standard approach we use. Both how things sound (syntax)"
    },
    {
        "casual_text": "Following the work of Dubey and colleagues in 2013, we're trying to create a model that can handle pronouns in everyday text and also explain how long it takes to read certain parts. Another goal is for our model to make sense of the human preferences found in studies on psycholinguistics. To do this, we're building on pronoun resolution systems from NLP, like the one by Soon and others in 2001. We're using weighted features to represent the different factors that affect pronoun resolution. This helps us figure out how important each factor is and how they might work together.",
        "formal_text": "Convert casual text to formal text: Following the work of Dubey and colleagues in 2013, we're trying to create a model that can handle pronouns in everyday text and also explain how long it takes to read certain parts"
    },
    {
        "casual_text": "Cells with the same attribute names usually show similar ideas. We use these metrics to check how alike the cells are.",
        "formal_text": "Convert casual text to formal text: Cells with the same attribute names usually show similar ideas. We use these metrics to check how alike the cells. Cells with the same attribute names usually show similar ideas. We use these metrics to check"
    },
    {
        "casual_text": "(3) We’ve built a detector that’s super flexible and works across different datasets, attacks, and models without needing to be retrained. We tested it with transformers and a mix of contextual and genetic attacks.",
        "formal_text": "Convert casual text to formal text: (3) We’ve built a detector that’s super flexible and works across different datasets, attacks, and models without needing to be retrained. We tested it with transformers and"
    },
    {
        "casual_text": "Hey, check this out: the big space between each group of dots lines up perfectly with the 1-1-1's.",
        "formal_text": "Convert casual text to formal text: Hey, check this: the big space between each group of dots lines perfectly with the 1-1-1's. Convert casual text to formal text: Hey, check this: the big space between each"
    },
    {
        "casual_text": "After filtering, we got 47, 227, 789 phrase pairs. Out of those, 670, 154 features showed up more than 10 times.",
        "formal_text": "Convert casual text to formal text: After filtering, we got 47, 227, 789 phrase pairs. Out of those, 670, 154 features showed up more than 10 times."
    },
    {
        "casual_text": "You can find the list here: https://github.com/dwyl/english-words. We only trained the model for values between 0 and 3.2 because we didn’t have enough data for bigger threshold values.",
        "formal_text": "Convert casual text to formal text: You can find the list here: https://github.com/dwyl/english-words. We only trained the model for values between 0 and 3.2 because we"
    },
    {
        "casual_text": "Human-friendly and machine-friendly CNLs are designed with different purposes, so it's no surprise that they cover different things. O'Brien (2003) found that the rule sets for these two types of CNLs don't overlap much, and even within the same category, the rules can vary a lot. But because the structure of CNLs is usually simpler and more predictable than full natural language, they're generally easier for people to understand and for computers to process. A perfect CNL for representing knowledge should also be super easy to write and flexible enough to describe whatever problem you're dealing with.",
        "formal_text": "Convert casual text to formal text: Human-friendly and machine-friendly CNLs are designed with different purposes, so it's no surprise that they cover different things. O'Brien (2003) found that the rule sets for these"
    },
    {
        "casual_text": "But the normalization constant is a tricky thing because it requires summing up all possibilities, which is hard to do. Broderick and his team came up with a workaround: they proposed using some algorithm A to estimate the posterior. This algorithm takes a small sample (called a minibatch) and the previous estimate of the posterior, and then spits out a new, approximate version of the posterior.",
        "formal_text": "Convert casual text to formal text: But the normalization constant is a tricky thing because it requires summing up all possibilities, which is hard to do. Broderick and his team came up with a workaround: they"
    },
    {
        "casual_text": "Basically, when we're figuring out how to index stuff, we start with the big picture—like general categories that don’t get too detailed (think small feature-trees). Then, we move on to the more specific stuff that adds extra details to those general ideas (like expanding those smaller trees through unification). Most of the time, this means we work from the main parts of the sentence to the details, like starting with the verb phrase (VP) and then looking at what comes after it. This is similar to what Proudian and Pollard talked about in HPSG back in 1985.",
        "formal_text": "Convert casual text to formal text: Basically, when we're figuring out how to index stuff, we start with the big picture—like general categories that don’t get too detailed (think small feature-trees)."
    },
    {
        "casual_text": "The dataset we end up with includes three pieces of information for each term: the original term we observed, one of the \"intended\" terms that the algorithm figured out, and how many times that intended term was meant. For a single term, it's normal to have several possible combinations because the algorithm decides based on the context.",
        "formal_text": "Convert casual text to formal text: The dataset we end up with includes three pieces of information for each term: the original term we observed, one of the \"intended\" terms that the algorithm figured out, and how many times that"
    },
    {
        "casual_text": "The other group of models uses the Transformer architecture (Vaswani et al., 2017). This includes models like ReCoSa (Zhang et al., 2019a) and CHMAM (Tao et al., 2018), which rely on something called the Multi-Head Attention Mechanism (MHAM) and an attention weight regularizer. Both ReCoSa and CHMAM are designed to pull out more useful and varied information from the conversation history.",
        "formal_text": "Convert casual text to formal text: The other group of models uses the Transformer architecture (Vaswani et al., 2017). This includes models like ReCoSa (Zhang et al., 2019"
    },
    {
        "casual_text": "Another approach is to mix things up by adding more types of relationships, which can actually make the negative signals stronger. Even though there's still some noise around, RLL is better at spotting the useful and helpful negative stuff. This gives the neural encoder more chances to pick up on meaningful signals. So, in this situation, it might be a good idea to go with a bigger C.",
        "formal_text": "Convert casual text to formal text: Another approach is to mix things up by adding more types of relationships, which can actually make the negative signals stronger. Even though there's still some noise around, RLL is better at spotting"
    },
    {
        "casual_text": "When making predictions, we'll only say that events m_i and m_j are the same if both the coreference probabilities y_ij and y_ji are higher than the threshold .",
        "formal_text": "Convert casual text to formal text: When making predictions, we'll only say that events m_i and m_j are the same if both the coreference probabilities y_ij and y_j"
    },
    {
        "casual_text": "You can check out info on multilingual BERT here: https://github.com/google-research/bert/blob/master/multilingual.md And if you're looking for a list of Wikipedias, you can find it here: https://meta.wikimedia.org/wiki/List_of_Wikipedias",
        "formal_text": "Convert casual text to formal text: You can check out info on multilingual BERT here: https://github.com/google-research/bert/blob/master/multilingual.md And if"
    },
    {
        "casual_text": "It seems like the text you provided is a mix of symbols and characters that don't form coherent words or sentences. It looks like it might be a scrambled or encoded message, or possibly just random characters. Could you clarify or provide more context so I can help you rewrite it in a more informal way?",
        "formal_text": "Convert casual text to formal text: It seems like the text you provided is a mix of symbols and characters that don't form coherent words or sentences. It looks like it might be a scrambled or encoded message"
    },
    {
        "casual_text": "Basically, there are two main types of info used for figuring out who someone is when their name is unclear (Malin, 2005): personal info and relational info (which can be explicit or implicit). Personal info tells you about the person’s background, like their life details. Relational info shows how the person is connected to other things, like if \"John Smith\" is part of the \"Labor Party.\" Explicit relational info is usually easy to find in the text, but implicit relational info might be hidden and not directly linked to the person’s name. Sometimes, figuring out who someone is can be tricky and might need that implicit relational info to give context. For instance, if two news articles talk about something happening in the \"Labor Party,\" that might help us figure out that \"John Smith\" is connected to the party, even if we don’t have any personal or explicit info about him.",
        "formal_text": "Convert casual text to formal text: Basically, there are two main types of info used for figuring out who someone is when their name is unclear (Malin, 2005): personal info and relational info (which can be"
    },
    {
        "casual_text": "We matched English word vectors with six other languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We used 300-dimensional fastText vectors that were trained on Wikipedia and Common Crawl. We made all the words lowercase, kept only the 200,000 most common ones, and did five rounds of Iterative Normalization to clean things up.",
        "formal_text": "Convert casual text to formal text: We matched English word vectors with six other languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We used 300"
    },
    {
        "casual_text": "Formatting relies on how often words appear together in the text and uses medical terms from a computer dictionary.",
        "formal_text": "Convert casual text to formal text: Formatting relies on how often words appear together in the text and uses medical terms from a computer dictionary."
    },
    {
        "casual_text": "2. X-Bar Theory is a piece of GB-theory that deals with how different categories relate to each other and sets rules for the basic structures underneath.",
        "formal_text": "X-Bar Theory is a piece of GB-theory that deals with how different categories relate to each other and sets rules for the basic structures underneath. It is a piece of GB-theory that deals"
    },
    {
        "casual_text": "In this paper, we introduce Hierarchical Meta-Embeddings (HME), a method that combines various pre-trained monolingual embeddings at the word, subword, and character levels into one language-agnostic representation without needing specific language tags. To handle different segmentations, we incorporate a Transformer (from Vaswani et al., 2017) that learns which subwords are important in a given sentence. We test our model on Named Entity Recognition for English-Spanish code-switching data, using Transformer-CRF, a transformer-based encoder for sequence labeling, as implemented by Winata et al. (2019). Our experiments show that HME beats the current state-of-the-art system in terms of absolute F1 score. The analysis reveals that for English-Spanish mixed texts, not only do similar languages like Portuguese or Catalan help, but even languages with Celtic origins significantly boost performance.",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce Hierarchical Meta-Embeddings (HME), a method that combines various pre-trained monolingual embeddings at the word, sub"
    },
    {
        "casual_text": "Equation (5) boosts the chances for nodes with higher centrality, and those already carrying heavy weights tend to \"suck up\" the weights from their neighbors, either directly or through their neighbors' neighbors. The whole thing works by tweaking matrix M based on , and then updating  based on the new M. As this process repeats, you start seeing a \"rich-get-richer\" effect (check out Figure 3 (c) and (d)). By adding DivRank into the mix, we get rank r  i and the overall biased ranking score G i for sentence s i from date t, which helps summarize C t.",
        "formal_text": "Convert casual text to formal text: Equation (5) boosts the chances for nodes with higher centrality, and those already carrying heavy weights tend to \"suck up\" the weights from their neighbors, either directly or through"
    },
    {
        "casual_text": "Opinion relation extraction is a big deal in fine-grained sentiment analysis. If we have human-made labels (like in the MPQA corpus by Deng and Wiebe, 2015), we can turn this into a supervised relation extraction problem, similar to what Kobayashi et al. (2007) and Johansson and Moschitti (2013) did. There are two main approaches people use: pipeline models, which first find possible opinion expressions and targets, then figure out the correct connections between them (like Wu et al., 2009, and Yang and Cardie, 2012), and joint models, which handle opinion expressions, targets, and relations all at once with a single model (Yang and Cardie, 2013, 2014). One thing to keep in mind with supervised methods is that they rely on specific domains and human-made annotations.",
        "formal_text": "Convert casual text to formal text: Opinion relation extraction is a big deal in fine-grained sentiment analysis. If we have human-made labels (like in the MPQA corpus by Deng and Wiebe, 2015),"
    },
    {
        "casual_text": "We shared a cool, free resource called the Gender Identified Enron Corpus and used it to look at how power, gender, and language are connected. We also talked about something called \"gender environment\" and found that how power shows up really changes depending on the gender environment. Plus, we saw that gender-related stuff can help make power prediction better. For next steps, we’re thinking about trying out machine learning algorithms that might handle feature interactions better than the SVM with a quadratic kernel we used here.",
        "formal_text": "Convert casual text to formal text: We shared a cool, free resource called the Gender Identified Enron Corpus and used it to look at how power, gender, and language are connected. We also talked about something"
    },
    {
        "casual_text": "We noticed that for most languages, once the dictionary hits 1,000 words, adding more doesn't really help with how well the system works. So, we decided to stick with a dictionary size of 1,000.",
        "formal_text": "Convert casual text to formal text: We noticed that for most languages, once the dictionary hits 1,000 words, adding more doesn't really help with with well the system works. So, we decided to stick with a dictionary size of 1,000"
    },
    {
        "casual_text": "2 Weight Poisoning Attack Framework 2.1 The \"Pre-train and Fine-tune\" Approach",
        "formal_text": "2 Weight Poisoning Attack Framework 2.1 The \"Pre-train and Fine-tune\" Approach. Convert casual text: 2 Weight Poisoning Attack Framework 2.1 The \"Pre-train and Fine-tune\""
    },
    {
        "casual_text": "Himanshu Aggarwal and his team came up with a system in 2006 that uses Conditional Random Fields for Hindi. They used a morph analyzer to figure out root words and possible parts of speech (POS tags) for training. The system was tested on a corpus of 21,000 words with 27 different POS tags, and it got an accuracy of 82.67%.",
        "formal_text": "Convert casual text to formal text: Himanshu Aggarwal and his team came up with a system in 2006 that uses Conditional Random Fields for Hindi. They used a morph analyzer to figure out root words and"
    },
    {
        "casual_text": "A lot of research has been done on semisupervision for classifying images, videos, and text (Li et al., 2019; Mallinar et al., 2019). For instance, Wang et al. (2009) used semisupervised learning algorithms for video annotation. They came up with a method that uses both labeled and unlabeled data to estimate class probabilities, based on the classic kernel density estimation approach. Habernal and Gurevych (2015) also developed a clustering-based semisupervised method to label unlabeled text, aiming to improve scene text recognition using semi-supervised learning on unannotated data. Rajendran et al. (2016) proposed a semisupervised algorithm for argument detection. In this paper, we’re mainly looking at methods that were originally developed for other tasks, like the ones by Rajendran et al. (2016, 2018). Specifically, we’re focusing on self-supervision, which is a model-agnostic way of automatically labeling unlabeled data.",
        "formal_text": "Convert casual text to formal text: A lot of research has been done on semisupervision for classifying images, videos, and text (Li et al., 2019; Mallinar et al., 2019"
    },
    {
        "casual_text": "First off, we’re working on moving EURODICAUTOM to a relational database system. To get started, we’ve come up with a new database structure. Among other things, we’ve added some rules to make sure values are consistent, and we’ve moved synonyms into their own separate spots. This sometimes causes a bit of overgeneration, which you can spot in the notes for the German entries in the example below. By the way, we’re already using this updated data model (with some tidying up and normalization) for output in MULTITERM.",
        "formal_text": "Convert casual text to formal text: First off, we’re working on moving EURODICAUTOM to a relational database system. To get started, we’ve come up with a new database structure. Among other"
    },
    {
        "casual_text": "To figure out what temporal expressions mean, we created some semantic templates for things like absolute time phrases and izen. We’ve got a collection of 150 lexical entries in these templates. Out of those, 92 are linked to CCG categories, and 58 are directly tied to specific words. You can check out examples of these templates in Table 2.",
        "formal_text": "Convert casual text to formal text: To figure out what temporal expressions mean, we created some semantic templates for things like absolute time phrases and izen. We’ve got a collection of 150 lexical entries in these"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: 1. We're looking at a bunch of stuff involving \"k\" and \"t\". 2. There's a function \"f\" that depends on \"O\" and \"L\". 3. We're summing up some values for each \"k\" and \"t\". 4. The whole thing is wrapped up in some brackets and symbols, but the main idea is to calculate something based on these \"k\" and \"t\" values. Basically, it's about crunching numbers using \"k\" and \"t\" to get a final result.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: 1. We're looking at a bunch of stuff involving \"k\" and \"t\". 2. There's a"
    },
    {
        "casual_text": "The second part goes into more detail about DCGs and introduces a basic grammar. Then, in the third part, we'll look at an ATN grammar that's kind of like the DCG one and talk about why it feels a bit clunky.",
        "formal_text": "Convert casual text to formal text: The second part goes into more detail about DCGs and introduces a basic grammar. Then, in the third part, we'll look at an ATN grammar that's kind of"
    },
    {
        "casual_text": "CTC Training. The Connectionist Temporal Classification (CTC) algorithm, introduced by Graves et al. in 2006, lets you use a special \"blank\" token in your vocabulary. It uses dynamic programming to handle these blank tokens, which are kind of like hidden alignments. Plus, non-autoregressive generation has this issue where words can get repeated in quick succession (Gu et al., 2018). To fix that, CTC combines repeated words unless they're split by something. For instance, if you have the token sequence \"a aabb,\" CTC turns it into \"aab.\" So, (a aabb) = aab.",
        "formal_text": "Convert casual text to formal text: CTC Training. The Connectionist Temporal Classification (CTC) algorithm, introduced by Graves et al. in 2006, lets you use a special \"blank\""
    },
    {
        "casual_text": "The method for finding the right parts of the text involves flipping the usual way we create sentences and checking from the end to the beginning.",
        "formal_text": "Convert casual text to formal text: The method for finding the right parts of the text involves flipping the usual way we create sentences and checking from the end to the beginning."
    },
    {
        "casual_text": "When it comes to data structure, the connection between OC and OQ is kind of like the difference between paired data and two-sample data in statistical tests. With OC, we look at how the system sorts items into classes and create a confusion matrix using the actual and predicted classes. On the other hand, OQ is more about comparing how the system's distribution of items matches up with the actual distribution, without worrying about which specific item in one matches up with an item in the other.",
        "formal_text": "Convert casual text to formal text: When it comes to data structure, the connection between OC and OQ is kind of like the difference between paired data and two-sample data in statistical tests. With OC, we"
    },
    {
        "casual_text": "Here’s the informal version: So, the sentence pairs break down like this: European Parliament (from Koehn, 2005) has 183,793 pairs; Pharmaceuticals has 190,443; Software has 196,168; and Hardware has 196,501. After running five iterations of the EM algorithm, we didn’t see any major improvement in how well the data fits. Oh, and we used the same setup as the baselines to train the latent domain-focused language models (LMs) for our model. Specifically, we used interpolated 4-gram Kneser-Ney LMs with BerkeleyLM. This setup was consistent for all the experiments in this study.",
        "formal_text": "Convert casual text to formal text: Here’s the informal version: So, the sentence pairs break down like this: European Parliament (from Koehn, 2005) has 183,793 pairs; Pharmaceuticals has 190"
    },
    {
        "casual_text": "Alright, so here, X_j is the j-th feature vector we've observed, and y is either 0 or 1, which is our label. W is just a vector holding all the coefficients. To figure out the model parameters in equation 3, we just need to adjust the weights W. Once we do that, we can calculate the log likelihood and its gradient function by taking the partial derivative of W with respect to the first-order terms.",
        "formal_text": "Convert casual text to formal text: Alright, so here, X_j is the j-th feature vector we've observed, and y is either 0 or 1, which is our label. W is just"
    },
    {
        "casual_text": "In Appendix B, we take a closer look at how these models' results differ, but we're just talking about it in a general, non-technical way.",
        "formal_text": "Convert casual text to formal text: In Appendix B, we take a closer, non-technical way. In general, non-technical way. Convert casual text to formal text: In Appendix"
    },
    {
        "casual_text": "Let’s say we have a token, let’s call it j. There’s no state c that comes from a part of the transition sequence s where j is on the stack. Basically, j doesn’t have any incoming edges, so it’s doing its job just fine.",
        "formal_text": "Convert casual text to formal text: Let’s say we have a token, let’s call it j. There’s no state c that comes from a part of the transition sequence s where j is"
    },
    {
        "casual_text": "So, there are two main ways people have been using neural networks to work with medical text. One approach is all about capturing the order of words, and it's often linked to recurrent neural networks (RNNs). Stuff like Atten-tiveLSTM (Shi et al., 2017), Bi-GRU (Mullenbach et al., 2018), and HA-GRU (Baumel et al., 2018) falls into this category. The other approach uses convolutional neural networks (CNNs), like CAML (Mullenbach et al., 2018) and MultiResCNN (Li and Yu, 2020). These methods focus more on local patterns in the text rather than the order of words. Even though they don't capture the sequence, they've been really good at predicting medical codes.",
        "formal_text": "Convert casual text to formal text: So, there are two main ways people have been using neural networks to work with medical text. One approach is all about capturing the order of words, and it's often linked to recurrent"
    },
    {
        "casual_text": "The latest models handle entities independently when figuring out entity alignment. Basically, if a target entity is matched with a source entity with high confidence, it's less likely to be matched with another source entity. So, we turned entity alignment into a maximum bipartite matching problem, which can be solved using the Hungarian algorithm. This way, we can consider how different alignment decisions affect each other and make better overall decisions. Our experiments, including some tests where we removed parts of the model to see what difference it made, show that our approach (LatsEA) and the global alignment strategy work well. To sum up, here's what we've contributed:",
        "formal_text": "Convert casual text to formal text: The latest models handle entities independently when figuring out entity alignment. Basically, if a target entity is matched with a source entity with high confidence, it's less likely to be"
    },
    {
        "casual_text": "We used the Kyoto Text Corpus as our annotated dataset. It had a total of 1,675,188 characters.",
        "formal_text": "Convert casual text to formal text: We used the Kyoto Text Corpus our annotated dataset. It had a total of 1,675,188 characters. We used the Kyoto Text Corpus. It had a total of 1,67"
    },
    {
        "casual_text": "Okay, so we're working with the ACE 2004-NWIRE dataset, which is basically the newswire part of the ACE 2004 corpus. This dataset has been used by researchers like Poon and Domingos (2008) and Haghighi and Klein (2009). For our development phase, we focused on the first corpus, called ACE2004-ROTH-DEV. The rest of the corpora are kept aside for testing later. We used the Stanford parser (developed by Klein and Manning in 2003) to analyze all the documents. This parser helps us figure out the syntactic structure, which we then use to pinpoint the main words in mentions and to decide the order of these mentions within a sentence. We'll go into more detail about this in the next section. To make sure our results are comparable to earlier work, we didn't use the perfect, hand-labeled named entity tags or mention types. Instead, we relied on the labels given by the Stanford NER tool (created by Finkel et al. in 2005).",
        "formal_text": "Convert casual text to formal text: Okay, so we're working with the ACE 2004-NWIRE dataset, which is basically the newswire part of the ACE 2004 corpus. This dataset has been used by researchers like"
    },
    {
        "casual_text": "T here represents the length of the output sequence. In NLP, a lot of encoder-decoder models use LSTM (long short-term memory) layers as their hidden units. LSTMs are an extension of regular RNN hidden layers and come with a memory cell that can \"remember\" and \"forget\" features. This helps solve the issue of 'vanishing gradients' and lets the model capture long-range dependencies.",
        "formal_text": "Convert casual text to formal text: T here represents the length of the output sequence. In NLP, a lot of encoder-decoder models use LSTM (long short-term memory) layers as their hidden units"
    },
    {
        "casual_text": "To help the model make use of the old comment, this system uses similarity to C old (check out section 6) as a way to tweak the top choices from the previous model. The new ranking score is a mix of the original beam score and the METEOR score comparing the candidate prediction to C old. Both scores are weighted equally at 0.5, and this balance was fine-tuned using validation data.",
        "formal_text": "Convert casual text to formal text: To help the model make use of the old comment, this system uses similarity to C old (check out section 6) as a way to tweak the top choices from the previous model. The new"
    },
    {
        "casual_text": "After the sequence modeling layer, it's pretty common to slap on a sequential Conditional Random Field (CRF) layer (shoutout to Lafferty et al., 2001). This layer helps figure out the best labels for the entire character sequence all at once.",
        "formal_text": "Convert casual text to formal text: After the sequence modeling layer, it's pretty common to slap on a sequential Conditional Random Field (CRF) layer (shoutout to Lafferty et"
    },
    {
        "casual_text": "You can find a Java version of HAPS and the Moonstone corpus online for free.",
        "formal_text": "Convert casual text to formal text: You can find a Java version of HAPS and the Moonstone corpus online free."
    },
    {
        "casual_text": "This project looks at how models that learn from natural language explanations can handle new tasks without needing labeled examples. Previous research in this area (like Srivastava et al. in 2017 and 2018, Hancock et al. in 2018, Murty et al. in 2020, Andreas et al. in 2018, Wang* et al. in 2020, and Ye et al. in 2020) has used explanations as a way to teach models, but they mostly tested on just a few tasks—like 2-3 relation extraction tasks in some papers and 7 email categorization tasks in another. Because there aren’t many big benchmarks for testing how well models learn from explanations across different tasks, we created CLUES, a benchmark with a bunch of classification tasks paired with natural language explanations. Over the years, a lot of work has gone into building structured and semi-structured knowledge bases, like the tables on Wikipedia or data from e-commerce sites. Making models that can work with this kind of structured data is really important because it makes machine learning more accessible, even for people who aren’t experts. So, in this project, we focus on creating classification tasks that use structured data.",
        "formal_text": "Convert casual text to formal text: This project looks at how models that learn from natural language explanations can handle new tasks without needing labeled examples. Previous research in this area (like Srivastava et al."
    },
    {
        "casual_text": "Sure! Here's a more casual version: Basically, we figure out how good an alignment (a) and segmentation (s) are by using a model. This model works kind of like the one in that paper by Bansal et al. from 2011, where it assumes stuff is semi-Markov and independent.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Basically, we figure out how good an alignment (a) and segmentation (s) are by using a model. This model works kind"
    },
    {
        "casual_text": "Alright, let's talk about child labor in Africa, specifically in South Africa. It's a big issue where kids are forced to work instead of going to school. This affects a lot of children and is a major problem in the labor force there.",
        "formal_text": "Convert casual text to formal text: Alright, let's talk about child labor in Africa, specifically in South Africa. It's a big issue where kids are forced to work instead of going to school. This affects"
    },
    {
        "casual_text": "It seems like there's a lot of hype around neural architectures these days, especially when it comes to encoder-decoder models for Seq2Seq tasks. For instance, Rao et al. (2015) got some great results with their encoder-decoder G2P model, hitting a really low error rate on the CMUdict dataset (Kominek and Black, 2004). But, when you look closer, the neural architecture itself isn't all that impressive on its own. It only really shines when paired with a weighted finite state transducer, where it edges out traditional models. Similarly, Faruqui et al. (2016) claimed their inflection generation neural architecture performed \"par or better\" compared to traditional methods. But digging into their results, it turns out their system isn't consistently better—sometimes it's worse, and other times it's better.",
        "formal_text": "Convert casual text to formal text: It seems like there's a lot of hype around neural architectures these days, especially when it comes to encoder-decoder models for Seq2Seq tasks. For instance"
    },
    {
        "casual_text": "Figuring out how words connect in a document is tricky. People have tried to get around the whole \"bag-of-words\" thing (Salton and McGill, 1983) by using clusters of related words (Slonim and Tishby, 2000) and adding synonyms to make document vectors better. Since content words can be grouped into categories, there's been a lot of interest in simpler, lower-dimensional ways to represent them. Latent Semantic Analysis (LSA) (Deerwester et al., 1990) is one of the most famous methods for this. In LSA, documents are broken down into hidden concepts, and all words are turned into simpler, smaller vectors. But the way \"relatedness\" is defined can vary depending on the words you're looking at. Plus, stuff like numbers, abbreviations, and the style of writing can give you clues about what a document is about. The problem is, after you simplify things with LSA, that extra info gets lost.",
        "formal_text": "Convert casual text to formal text: Figuring out how words connect in a document is tricky. People have tried to get around the whole \"bag-of-words\" thing (Salton and McGill, 1983) by"
    },
    {
        "casual_text": "Markov models have been known to have their limitations for quite some time (check out Chomsky's work from 1956). It's pretty straightforward to come up with examples where a trigram model falls short, but a more advanced model would nail it. Take this sentence, for example: \"The dog on the hill barked.\" A trigram model would probably give a low probability to the word \"barked.\" But a more sophisticated linguistic model could figure out that \"dog\" is the main noun right before \"barked\" and would give \"barked\" a high probability since the chance of a dog barking (P(barked|dog)) is pretty high.",
        "formal_text": "Convert casual text to formal text: Markov models have been known to have their limitations for quite some time (check out Chomsky's work from 1956). It's pretty straightforward to come up with examples where a trigram"
    },
    {
        "casual_text": "So, we double-checked this by talking to some researchers and looking at how they manually organize research paper catalogs. Turns out, almost everything in Japanese literature research fits into categories of writers and works. But for the stuff that doesn’t fit there, we’ve got around 100 other concepts like dialectology ready to create a starting point.",
        "formal_text": "Convert casual text to formal text: So, we double-checked this by talking some researchers and looking at how they manually organize research paper catalogs. Turns out, almost everything in Japanese literature research fits into categories of writers and"
    },
    {
        "casual_text": "The KL divergence, written as D(p||q), between two probability distributions p(x) and q(x) over the same set of possible outcomes , is shown in equation 1.",
        "formal_text": "Convert casual text to formal text: The KL divergence, written as D(p||q), between two probability distributions p(x) and q(x), over the same set of possible outcomes"
    },
    {
        "casual_text": "Basically, an RE model checks if two things in a sentence are connected and, if they are, it figures out what kind of relationship they have from a set list of options. We’re mainly looking at neural network RE models because they’re the best at this task right now. The cool thing about these models is that they use word embeddings as input, which makes it easier to transfer the model to other languages using cross-lingual word embeddings. In this paper, we’re using English as our main language to work with.",
        "formal_text": "Convert casual text to formal text: Basically, an RE model checks if two things in a sentence are connected and, if they are, it figures out what kind of relationship they have from a set list of options."
    },
    {
        "casual_text": "The basic alphabet for the language LT(Prop) includes a few key things: two constant symbols, s and t, a set of Boolean operators that are good enough to handle truth functions, two unary modalities called  and T, a binary modality that looks like : , a modality that can have any number of inputs (we call it •), and the usual brackets, ( and ). On top of that, we’ve got a collection of propositional symbols called Prop. These symbols in Prop are kind of like the building blocks provided by the linguistic theory we’re working with—different situations might use different sets of these symbols. For example, Prop could be something like S, NP, VP, N, V, DET, CONJ.",
        "formal_text": "Convert casual text to formal text: The basic alphabet for the language LT(Prop) includes a few key things: two constant symbols, s and t, a set of Boolean operators that are good"
    },
    {
        "casual_text": "Apart from TAG-TAG, TAG-WORD, and TAG-REASON, we also added the method from (Heymann and Garcia-Molina, 2006) as a baseline, which we call HEYMANN. The HEYMANN method is meant to identify \"similar-to\" relationships rather than \"subsumption\" relationships. \"Similar-to\" relationships are symmetrical, while \"subsumption\" relationships are stricter and not symmetrical. In our tests, we used the same evaluation process for TAG-TAG, TAG-WORD, TAG-REASON, and HEYMANN, and only subsumption relationships were counted as correct.",
        "formal_text": "Convert casual text to formal text: Apart from TAG-TAG, TAG-WORD, and TAG-REASON, we also added the method from (Heymann and Garcia-Molina, 2006) as"
    },
    {
        "casual_text": "We haven't really looked into GGP's parsing complexity broadly yet, but some key factors that limit chart parsing are pretty well understood, thanks to studies on context-free parsing by Sheil back in 1976. Basically, the number of steps it takes is around O(nD), and the space needed is O(n2), no matter how you approach the parsing. The length of the input sentence (n) plays a big role here. Interestingly, the size of the grammar doesn't affect the complexity, but the branching factor—which kinda tells you how uncertain or unpredictable the grammar is—acts like a multiplier.",
        "formal_text": "Convert casual text to formal text: We haven't really looked into GGP's parsing complexity broadly yet, but some key factors that limit chart parsing are pretty well understood, thanks to studies on context-free"
    },
    {
        "casual_text": "Alright, let's walk through the algorithm using an example with the word \"sound.\" We'll focus on the sentence: \"I liked the sound of the harpsichord.\"",
        "formal_text": "Convert casual text to formal text: Alright, let's walk through the algorithm using an example with the word \"sound.\" We'll focus on the sentence: \"I liked the sound of the harpsichord"
    },
    {
        "casual_text": "These days, automated tools for tagging stuff in media don't always give great results because there used to be a lot of mixed-up guidelines for this kind of work. Take something like the TV show \"Star Trek: Deep Space Nine\"—some tools might not tag it at all, or they might tag it twice, once for the \"Star Trek\" franchise and again for the space station \"Deep Space Nine,\" but they probably won't tag it correctly as \"Star Trek: Deep Space Nine.\" In our opinion, it would be better if the second tag pointed to the actual TV show instead of the space station. And it's not just shows—characters can be a mess too. For example, \"James Bond\" can be tagged as both a literary character and a person, depending on the source. To deal with these kinds of problems, we've made a special collection of data focused on tagging media-related things properly. We're sharing this collection, along with the rules we used to tag it and some extra tags for tricky, nested entities.",
        "formal_text": "Convert casual text to formal text: These days, automated tools for tagging stuff in media don't always give great results because there used to be a lot of mixed-up guidelines for this kind of work. Take something like"
    },
    {
        "casual_text": "Alright, let’s break this down in simpler terms. The person doing the post-editing will get clear instructions in their own language about how to deal with weird-looking word combinations. Plus, some words with lots of possible meanings will keep showing up in the same way, so they won’t have to waste time making the same decisions over and over. So, the post-editor’s job shouldn’t be too tough. They should be able to turn the machine’s rough output into a readable translation way faster than a human translator could do it from scratch. If the machine can keep up with a human translator’s speed, then combining the machine’s work with a post-editor could be just as fast and accurate as having a full human translator. Even if the mixed machine-human approach costs more, there are situations where cost isn’t the main concern. Also, human labor costs are probably going to go up over time, while the cost of running the machine will likely go down once these machines are mass-produced and new tech, like transistors, is used to make them better and cheaper.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in simpler terms. The person doing the post-editing will get clear instructions in their own language about how to deal with weird-looking word combinations. Plus"
    },
    {
        "casual_text": "(2) Don’t include extra stuff that’s not needed; (3) make sure it’s easy to read; (4) stick to the word/character limits. For the English summary, keep it under 300 words, and for the Chinese one, make it 500 characters or less.",
        "formal_text": "Convert casual text to formal text: (2) Don’t include extra stuff that’s not needed; (3) make sure it’s easy to read; (4) stick to the word/character limits. For the English summary, keep it under"
    },
    {
        "casual_text": "We picked  = 0.1 and = 1e-6 for the structure-aware uncertainty stuff. For our bachelor recognizer, we used just one GCN layer with 500 inputs and 400 outputs. We also set K = 5 for the model ensemble and  = 1.5,  = 0.1, and N_neg = 10 for training. The batch size for sampling is 100 when dealing with 15K data points and 1000 for 100K data points.",
        "formal_text": "Convert casual text to formal text: We picked  = 0.1 and = 1e-6 for the structure-aware uncertainty stuff. For our bachelor recognizer, we used just one GCN layer with 500 inputs and 400 output"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. So, we have a parametric type hierarchy called (P, __p, a). From this, we get another hierarchy, (I(P), I-z), which is kind of like a derived version. Now, the key point here is that we're dealing with feature values, and this setup assumes that these feature values cover all possible types in the system. This is the most flexible and powerful scenario for parametric types, but it also means things can get pretty complex and tough to handle computationally.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. So, we have a parametric type hierarchy called (P, __p, a). From this, we get"
    },
    {
        "casual_text": "Lexical weighting helps deal with the issue of unreliable phrase probabilities, especially for longer phrases, but it doesn't fix the problem of breaking down context. A lot of the work in picking the right translation still falls on the language model (LM), which is really good at what it does but can only use context from the target language. Plus, decisions made early on—like only keeping the top n translation options for each part of the source text—are based only on the translation models and the target context within that phrase.",
        "formal_text": "Convert casual text to formal text: Lexical weighting helps deal with the issue of unreliable phrase probabilities, especially for longer phrases, but it doesn't fix the problem of breaking down context. A lot of the work in"
    },
    {
        "casual_text": "So, when we look at how well the Punjabi text summarization system works for Punjabi news articles, it does pretty well at a 10% compression rate. This is because, at that level, the system usually picks the headline and the next line, which is often enough to give a good idea of the whole article. But for Punjabi stories, the system doesn't do as well at 10% compression. That's because stories don't have headlines, and the few lines that get picked for the summary aren't enough to capture the full meaning of the story. We also tested the system by doing question answering and keyword association tasks at different compression levels—10%, 30%, and 50%—for both Punjabi news articles and stories. For news articles, the question answering accuracy was 78.95% at 10%, 81.38% at 30%, and 88.75% at 50%. The system struggled a bit at 10% because news articles are usually short, and at that compression level, it mainly picks the headline and the next line, which isn't enough to answer all the questions. For Punjabi stories, the question answering accuracy was 80.65% at 10%, 84.26% at 30%, and 90.72% at 50%. And for news articles, the keyword association accuracy was 80.13% at 10%, and then it goes up to 92% at 30%.",
        "formal_text": "Convert casual text to formal text: So, when we look at how well the Punjabi text summarization system works for Punjabi news articles, it does pretty well at a 10% compression rate. This is because, at that level"
    },
    {
        "casual_text": "We do part-of-speech tagging, or shallow parsing, for each set of texts and look at the sequences of tags to see how the sentences are structured. To figure out how similar two sets of texts are in terms of their grammar, we compare vectors of n-grams using cosine and recurrence metrics. These comparisons are set up as permutation tests, following the method by Nerbonne and Wiersma (2006).",
        "formal_text": "Convert casual text to formal text: We do part-of-speech tagging, or shallow parsing, for each set of texts and look at the sequences of tags to see how the sentences are structured. To"
    },
    {
        "casual_text": "Our system is doing pretty well, but it’s not quite up to par with stuff like Google Translate. That’s mainly because we’re working with less training data and have less powerful computers. However, our system has a cool advantage: we can feed it a ton of articles and sensitive info every day without involving any third parties in the translation process. How well it performs and how long it takes to translate depends on the number of sentences, how complicated they are, and the languages involved. The news domain keeps changing based on what’s happening in the world, while the parallel data we usually have is more static and often tied to government stuff. We’re thinking about ways to improve our system by updating the language model with the latest English articles.",
        "formal_text": "Convert casual text to formal text: Our system is doing pretty well, but it’s not quite up to par with stuff like Google Translate. That’s mainly because we’re working with less training data and have less powerful"
    },
    {
        "casual_text": "The CCL parser doesn't include punctuation in its output, even when it's trained with punctuation. This means it can't be properly tested on datasets that have punctuation. Also, the right branching baseline works really well when there's no punctuation, but its performance drops a lot when punctuation is added, especially with how it handles trailing punctuation. To fix this, we borrowed a post-processing trick from (Drozdov et al., 2019) and tweaked it to either add punctuation back or adjust how it's connected in the parse tree: for trailing punctuation, we manually attach it to the root of the parse tree, and for punctuation within the sentence, we attach it to the lowest common ancestor of the two words around it in the tree. Just a heads-up, this method will result in non-binary parse trees.",
        "formal_text": "Convert casual text to formal text: The CCL parser doesn't include punctuation in its output, even when it's trained with punctuation. This means it can't be properly tested on datasets that"
    },
    {
        "casual_text": "For every word w_t, where W is the weight matrix, b is the bias vector, and tanh(•) is the element-wise hyperbolic tangent function.",
        "formal_text": "Convert casual text to formal text: For every word w_t, where W is the weight matrix, b is the bias vector, and tanh(•) is the element-wise hyperbolic tangent"
    },
    {
        "casual_text": "During the training phase, we fed the models with a mix of news articles and tweets. The total number of texts in the training dataset was 13,809 (5,822 news articles and 7,987 tweets). We used 90% of this data for training and reserved the remaining 10% for testing. To make sure everything was balanced, we ran each model through four different training/testing splits, keeping roughly the same proportion of news articles and tweets in each split. After training, we tested the models on 1,007 unseen feedback items that were manually created by monitoring experts. We tried out a bunch of classic machine learning models like Logistic Regression, Decision Tree, k-Nearest Neighbors, Linear SVM, and Random Forest, all using TF-IDF representation. But the results weren’t great. We also experimented with convolutional neural networks (CNN) built on top of a distilled version of RoBERTa (as described by Sanh et al., 2019 and Liu et al., 2019). This was in addition to the standard fine-tuning of the transformer model. The CNN followed the architecture outlined in Kim (2014). The results are shown in Table 2.",
        "formal_text": "Convert casual text to formal text: During the training phase, we fed the models with a mix of news articles and tweets. The total number of texts in the training dataset was 13,809 (5,822 news articles and"
    },
    {
        "casual_text": "Sure! Here's a more casual version of that text: Basically, this rule says that all the important words in a mention should also show up in the earlier mention we're comparing it to. This idea is based on the idea that people usually don't throw in new info when they talk about the same thing again (thanks, Fox, 1993). Normally, when you mention the same thing over and over, the descriptions get shorter and less detailed. For instance, if you say something like \"the [Florida Supreme Court]'s big move\" and then later just say \"the [Florida court] did something dramatic,\" those two mentions are talking about the same thing. But in the example below, those two mentions are actually about different things and belong to separate groups.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of that text: Basically, this rule says that all the important words in a mention should also show up in the earlier mention we're"
    },
    {
        "casual_text": "Okay, so basically, this formula is like a way to figure out how related a document (d) is to a query (q). It takes into account all the words (w) in the vocabulary (V). Here's what each part means: - C * (w | d) is how common the word is in the document. - C(w | q) is how common the word is in the query. - idf(w) is a measure of how rare the word is across all documents. - p is a number that helps balance things out. - avdl is the average length of documents. - |d| is the length of the specific document we're looking at. The formula mixes these things together, giving more weight (0.8) to the average document length and a bit less (0.2) to the actual length of the document we're checking.",
        "formal_text": "Convert casual text to formal text: Okay, so basically, this formula is like a way to figure out how related a document (d) is to a query (q). It takes into account all the words (w)"
    },
    {
        "casual_text": "Once all the skeleton nodes are ready, we build a graph using those nodes, but we leave out the right parenthesis, as shown in Figure 3. To get the global structure info, we slap a GAT network on top of the hidden states.",
        "formal_text": "Convert casual text to formal text: Once all the skeleton nodes ready, we build a graph using those nodes, but we leave out the right parenthesis, as shown in Figure 3. To get the global structure info"
    },
    {
        "casual_text": "For this part, we're just focusing on LP-consistency and not worrying about whether the categories are legal or not. Sometimes, there are categories in local trees where some feature values haven't been filled in yet when we need to check LP-consistency. This might mean that one or more (transitive) LP statements can't be used on the sequence of daughters we're looking at.",
        "formal_text": "Convert casual text to formal text: For this part, we're just focusing on LP-consistency and not worrying about whether the categories are legal or not. Sometimes, there are categories in local trees where some feature values"
    },
    {
        "casual_text": "First off, this research is all about spotting offensive language in multiple languages, so they used a pre-trained multilingual BERT model (Devlin et al., 2019) that’s been set up with HuggingFace. You can find these pre-trained models in HuggingFace’s model repository. BERT supports 104 languages, including Tamil, Kannada, and Malayalam. It’s got around 110 million parameters, with 12 layers, 768 hidden states, and 12 heads (Gopalan and Hopkins, 2020). Usually, BERT takes input data in a specific format, using special tokens like [CLS] to mark the start and [SEP] to mark the end or separation of sentences. Plus, you need to break the text into tokens that match BERT’s vocabulary. For each sentence, BERT needs input IDs, which are basically a sequence of numbers that link each token to its place in BERT’s tokenizer vocabulary. For any given token, its input representation is created by combining the token, segment, and position embeddings (Devlin et al., 2019). You can also find a visual representation of the BERT model to help you understand it better.",
        "formal_text": "Convert casual text to formal text: First off, this research is all about spotting offensive language in multiple languages, so they used a pre-trained multilingual BERT model (Devlin et al."
    },
    {
        "casual_text": "Okay, so here's the deal: We've got some numbers and stuff for this thing called \"Epistemic candidates\"—like, 9966 of them, to be exact. Then there's \"All words,\" which is a whopping 175,964 words. And \"Table 1\" is just a summary of all this data we collected. Oh, and \"Annotation Results\"? That's just the fancy name for what we found after labeling or tagging everything.",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: We've got some numbers and stuff for this thing called \"Epistemic candidates\"—like, 9966 of them, to be exact."
    },
    {
        "casual_text": "We're suggesting we create a model for a defendant that includes both semantic details and statistical traits, then figure out how important he is using a learning-to-rank approach.",
        "formal_text": "Convert casual text to formal text: We're suggesting we create a model for a defendant that includes both semantic details and statistical traits, then figure out how important he using learning-to-rank approach. Convert casual text"
    },
    {
        "casual_text": "So, it's still unclear who \"him\" refers to, and it could be either John or Bill. Right now, with the way people are voting, John is more popular than Bill.",
        "formal_text": "Convert casual text to formal text: So, it's still unclear who \"him\" refers to, and it could be either John or Bill. Right now, with the way people are voting, John is more popular than Bill."
    },
    {
        "casual_text": "We looked at two different ways of training: supervised and semi-supervised. The supervised system, which we call \"Sup,\" is trained using labeled bilingual data. On the other hand, the semi-supervised system, or \"+Unsup,\" also uses some unlabeled English text for training. In this case, we guess one Chinese translation for each English sentence and use that for the training process.",
        "formal_text": "Convert casual text to formal text: We looked at two different ways of training: supervised and semi-supervised. The supervised system, which we call \"Sup,\" is trained using labeled bilingual data. On the other hand"
    },
    {
        "casual_text": "TTK has this cool visualization thing called TBox, which Verhagen came up with back in 2007. It uses arrows pointing from left to right, boxes inside other boxes, and stacked boxes to show stuff like what happens before, what's part of something bigger, and what happens at the same time. Check out figure 3 for an example!",
        "formal_text": "Convert casual text to formal text: TTK has this cool visualization thing called TBox, which Verhagen came up with back in 2007. It uses arrows pointing from left to right, boxes inside other boxes, and stacked"
    },
    {
        "casual_text": "We fine-tune a bi-directional language model to recognize slang words and understand how they’re used by focusing on the masked language modeling task. To do this, we use a dataset from the Urban Dictionary that was scraped and collected by Wilson et al. (2020). After cleaning and trimming the data (check out Appendix A.1 for the specifics), we end up with a training set of 200,000 text sequences packed with slang. For our bi-directional model, we go with RoBERTa (Liu et al., 2019). We pick it over the original BERT (Devlin et al., 2019) because it’s better and allows for more subword units. We think this could be helpful for slang since some of the smaller parts of these words might not have been recognized by BERT. Plus, we go with the smaller 125M parameter version to save on computational resources. We train the model using the Adam optimizer (Kingma and Ba, 2015) with different learning rates. After testing, we found that the lowest loss on the test set came with a learning rate of  = 106, so that’s what we use for scoring semantic change. If you want more details on the training setup, check out Appendix A.2.",
        "formal_text": "Convert casual text to formal text: We fine-tune a bi-directional language model to recognize slang words and understand how they’re used by focusing on the masked language modeling task. To do this,"
    },
    {
        "casual_text": "Check out Table 5 for how different systems handle event coreference when using gold event triggers. The co-reference cut-off threshold is set by tweaking it to get the best CoNLL average score across ten runs. Oh, and Table 4 shows the results for event detection, which is about identifying triggers. \"Span\"/\"Type\" just means matching the span or type, respectively.",
        "formal_text": "Convert casual text to formal text: Convert casual text to formal text: Check out Table 5 for how different systems handle event coreference when using gold event triggers. The co-reference cut-off threshold is set by tweaking"
    },
    {
        "casual_text": "A way to automatically find hyponym-hypernym pairs in a specific area you're interested in.",
        "formal_text": "Convert casual text to formal text: A way automatically find hyponym-hypernym pairs in a specific area you're interested in. Convert casual text to formal text. Convert casual text to formal text."
    },
    {
        "casual_text": "The network can be visualized as a 3D grid with dimensions 5  5  K. In this grid, the spot at position (i, j, k) has a value if the i-th sentence is connected to the j-th sentence through the k-th type of relationship.",
        "formal_text": "Convert casual text to formal text: The network can be visualized as a 3D grid with dimensions 5  5  K. In this grid, the spot at position (i, j, k) has"
    },
    {
        "casual_text": "The tricky part is that a semantic parser designed for a new language needs to understand questions based on the local ontology. For instance, a restaurant guide in New York might handle queries about places near Times Square, but one in Italy should be able to answer questions about restaurants near landmarks like the \"Colosseo\" or \"Fontana di Trevi\" in Rome, all in Italian. Plus, the parser has to be able to handle situations where the sentences mention things in the new language that weren’t part of its training data.",
        "formal_text": "Convert casual text to formal text: The tricky part is that a semantic parser designed for a new language needs to understand questions based on the local ontology. For instance, a restaurant guide in New York might"
    },
    {
        "casual_text": "The first part is the prior, which comes from Equation 3, and the second part is how likely it is to get the sentence s_i, t_i from the formula that's labeled by y_i.",
        "formal_text": "Convert casual text to formal text: The first part is the prior, which comes from Equation 3, and the second part is how likely it it to get the sentence s_i, t_i from the formula that's"
    },
    {
        "casual_text": "At the very least, short social media posts often don’t have titles. Plus, due to the character limit (like those 140-character tweets), most keywords might not even show up in the post (meaning low recall). In cases like this, you can create structured posts to add in any missing keywords. For instance, TAKG (Wang et al., 2019) uses topic modeling, where topics found in other documents help the encoder pick up on contexts from related posts. But, having just a small, fixed number of topics (like 15 or 30) can make it hard to tell apart different documents that happen to share similar topics.",
        "formal_text": "Convert casual text to formal text: At the very least, short social media posts often don’t have titles. Plus, due to the character limit (like those 140-character tweets), most keywords might not even show up in the"
    },
    {
        "casual_text": "Later on, we'll talk about how some methods add an extra step to figure out the best groups of mentions for a document. But even with that, they're still relying on confidence scores that come from looking at things locally.",
        "formal_text": "Convert casual text to formal text: Later on, we'll talk about how some methods add an extra step to figure out the best groups of mentions for a document. But even with that, they're still relying"
    },
    {
        "casual_text": "If the community wants to go over those maximum amounts we just talked about, they can, but only if they figure out what those extra amounts should be... and then decide on them.",
        "formal_text": "Convert casual text to formal text: If the community wants to go over those maximum amounts we just talked about... they can, but only if they figure what those extra amounts should be... and then decide on them. Convert casual text"
    },
    {
        "casual_text": "The Attention Transformer figures out the attention weights using dot-product attention. The encoder-decoder cross-attention, which we call  ij, is calculated by looking at the target hidden state s i and the source hidden state h j.",
        "formal_text": "Convert casual text to formal text: The Attention Transformer figures out the attention weights using dot-product attention. The encoder-decoder cross-attention, which we call  ij, is calculated by looking at"
    },
    {
        "casual_text": "Pretty much every conversation or piece of writing has people’s opinions in it. Like, it’s super hard to make something totally objective. When it comes to writing, one of the main things people do is sneak in their own thoughts and views, whether they say it outright or not. This does two things:",
        "formal_text": "Convert casual text to formal text: Pretty much every conversation or piece of writing has people’s opinions in it. Like, it’s super hard to make something totally objective. When it comes to writing, one of the main things people"
    },
    {
        "casual_text": "Alright, so we've got an algorithm to tackle our problem, and it's laid out in Algorithm 1. Here, T represents the number of iterations we'll run, and (t) is a parameter that helps us figure out the step size for updating each Lagrangian multiplier. If you want more details on that, check out (Korte and Vygen, 2008). With this algorithm, we update each multiplier and pick a set of words from the source sentence to compress it. The goal is to make our compression as close as possible to what bunsetsubased methods would produce. But, as we mentioned earlier, if bunsetsu includes words that aren't important, our method will focus on breaking their unit constraints and dropping those unimportant words.",
        "formal_text": "Convert casual text to formal text: Alright, so we've got an algorithm to tackle our problem, and it's laid out in Algorithm 1. Here, T represents the number of iterations we'll run"
    },
    {
        "casual_text": "When someone asks a question about a knowledge graph (KG) to get the right answer (like Singh and team mentioned in 2018), things have changed a lot. Nowadays, with smart helpers like Alexa and Siri becoming super popular, the focus has moved to having conversations where you ask multiple questions, and the system has to understand things like pronouns and unfinished sentences (Christmann and others in 2019, Shen and others in 2019). It's kind of like in Figure 1, where you can see how this works.",
        "formal_text": "Convert casual text to formal text: When someone asks a question about a knowledge graph (KG) to get the right answer (like Singh and team mentioned in 2018), things have changed a lot. Nowadays, with smart help"
    },
    {
        "casual_text": "For sentiment classification, the authors used eye-tracking data from people who were asked to do sentiment analysis.",
        "formal_text": "Convert casual text to formal text: For sentiment classification, the authors used eye-tracking data from people asked to do sentiment analysis."
    },
    {
        "casual_text": "WebWOZ is a web-based tool for creating WOZ experiments, which lets you easily combine different language technologies (LTs) like speech recognition or text-to-speech. It’s built with modern web tech like Java, HTML, and CSS, so it works on any up-to-date web browser. Normally, it connects to pre-made LT components through web services, like automatic speech recognition (ASR), machine translation (MT), or text-to-speech (TTS). But for this specific setup, we’ve hooked it up with our own ASR system (a mix of Google and JULIUS) and MARY TTS. When the ASR processes speech, the result shows up in the top part of the interface. From there, the person running the experiment (the \"wizard\") can pick a response from a list of pre-written options or type something new on the spot. Either way, the chosen text gets sent to the MARY TTS server, which then speaks it out loud.",
        "formal_text": "Convert casual text to formal text: WebWOZ is a web-based tool for creating WOZ experiments, which lets you easily combine different language technologies (LTs) like speech recognition or text-to-speech."
    },
    {
        "casual_text": "• Delete(k): Pick a spot, k, and then randomly choose a number, m, from 1 to 6. Starting from that spot, delete m words.",
        "formal_text": "Convert casual text to formal text: • Delete(k): Pick a spot, k, and then randomly choose a number, m, from 1 to 6. Starting from that spot, delete m words."
    },
    {
        "casual_text": "There are two main types of global algorithms: generative probabilistic models and similarity-based models. Eisenstein and Barzilay (2008) think of a document as a series of parts created by hidden topic variables. Misra et al. (2011) and Du et al. (2013) have similar ideas. On the other hand, Malioutov and Barzilay (2006) and Kazantseva and Szpakowicz (2011) use similarity-based methods. Both of these approaches start with a matrix showing how similar the sentences in a document are to each other. The first one uses graph cuts to identify groups of sentences that fit together well, while the second one tweaks a clustering algorithm to do the same thing.",
        "formal_text": "Convert casual text to formal text: There are two main types of global algorithms: generative probabilistic models and similarity-based models. Eisenstein and Barzilay (2008) think of a document as a series of parts created by"
    },
    {
        "casual_text": "Token-based MLM takes a token and rebuilds it using the tokens around it, effectively packing contextual info into each token's representation. Inspired by this, we came up with a tree-based context-aware pre-training task to capture structural context, like parent, sibling, and child nodes. As shown in Figure 2, we turn ASTs into sequences, hoping the flattened trees can keep the local structure info (like sub-trees with terminal nodes), which some previous studies (Chakraborty et al., 2020) have already shown is possible. To make this happen, we created the AST node-type masked language model (NT-MLM). For a given AST-type sequence T from source code C, we mask some AST types t p |p  loc m  with the special token [MASK] and replace others t q |q  loc r  with random tokens. This ensures that if a source code token is masked or replaced, its corresponding AST type gets the same treatment. NT-MLM then learns to recover the masked AST type t i |i  M  using the Transformer encoder's output h i. We also defined the loss for NT-MLM to guide this process.",
        "formal_text": "Convert casual text to formal text: Token-based MLM takes a token and rebuilds it using the tokens around it, effectively packing contextual info into each token's representation. Inspired by this, we came up with"
    },
    {
        "casual_text": "To show how well the MSEP system can handle new situations, we tested it on different domains and compared it to a supervised system. The TAC-KBP dataset has two types of text: newswire (NW) and discussion forum (DF), with about the same number of documents in each. When the supervised system is trained on NW and tested on DF, it struggles because it's dealing with unfamiliar data. But the MSEP system adapts really well. Table 7 shows that MSEP does better than the supervised methods when they're out of their usual domain, for both tasks. The improvements are actually pretty significant, with p  0.05.",
        "formal_text": "Convert casual text to formal text: To show how well the MSEP system can handle new situations, we tested it on different domains and compared it to a supervised system. The TAC-KBP dataset has two types"
    },
    {
        "casual_text": "So, basically, this is what gets passed back as the result for the first \"Pl\" in the original \"vp(X, Pt)\" goal. Since our \"np(X, P[, F)\" goal matched \"P\" with \"Pl,\" the \"s(P)\" goal works out with the result being (Forall Y (=> (And (woman Y) (breathes Y)) (loves John Y))).",
        "formal_text": "Convert casual text to formal text: So, basically, this is what gets passed back as the result for the first \"Pl\" in the original \"vp(X, Pt)\" goal. Since our \"np("
    },
    {
        "casual_text": "To prove that UD performs better than BD, we compare both algorithms in the same search space. This means we make sure that any candidate in the UD out-forest would get the same score if it were evaluated by BD. We don't need to stress about differences caused by the missing context estimation factor, est(•), since this factor only comes into play when sorting the queue, Q, based on heuristicCost(•). Also, we don't have to worry about candidates that are scored without any missing child or parent link, because in that case, the scoring function (3) for BD is the same as scoring function (5) for UD. However, for candidates that are scored with a parent link, we tweak the cost(•) function by removing the parent link factor when adding the candidate to the out-forest. And for candidates that are scored with a missing child, we adjust the score once the link to the missing child is established in the out-forest. By doing this, both UD and BD end up scoring the same derivation with the same score, making them just two different methods to explore the same search space.",
        "formal_text": "Convert casual text to formal text: To prove that UD performs better than BD, we compare both algorithms in the same search space. This means we make sure that any candidate in the UD out-forest would get the"
    },
    {
        "casual_text": "After looking at the research on punctuation disambiguation and the comma insertion work by Beeferman and team (mentioned in Section 2), we decided to use the Wall Street Journal text for our experiment. This text is already tagged with parts of speech, so it’s reliable, and figuring out sentence boundaries is pretty straightforward. We started by tweaking the text—we got rid of all the punctuation and made everything uppercase. Then, we split the data: 90% of it, which had 965 sentence breaks, became our training set, while the remaining 10%, with 107 sentence breaks, was set aside as unseen test data. First, we dug into the training set to gather some stats. For each word, we calculated the chances of it being the first or last word in a sentence. We also did the same for each part of speech tag, figuring out how likely it was to appear at the start or end of a sentence. Every word boundary in the corpus was turned into a feature vector with 13 elements, as shown in Table 2. The test corpus vectors follow the same format, except they don’t include the classification (feature 13).",
        "formal_text": "Convert casual text to formal text: After looking at the research on punctuation disambiguation and the comma insertion work by Beeferman and team (mentioned in Section 2), we decided to use the"
    },
    {
        "casual_text": "These thresholds aren’t super precise because our pruning method works with groups of possible next steps (called successors) from multiple hypotheses during the process, instead of just focusing on the next steps for a single hypothesis.",
        "formal_text": "Convert casual text to formal text: These thresholds aren’t super precise because our pruning method works with groups of possible next steps (called successors) from multiple hypotheses during the process, instead just focusing on the next"
    },
    {
        "casual_text": "The training texts had 6580 unique words and 6945 unique tag trigram types. In the open test sentences, there were 247 words and 213 tag trigrams that weren’t in the training data. So, we needed to smooth both the part-of-speech trigram probabilities and the word output probabilities to handle texts with unknown stuff. We tested our system, which uses smoothed part-of-speech trigrams along with a word model, on these open test sentences. Table 4 shows how many words were correctly segmented and tagged. In Table 4, \"label consistency 2\" shows the accuracy of segmentation and tagging, but it doesn’t count differences in conjugation forms.",
        "formal_text": "Convert casual text to formal text: The training texts had 6580 unique words and 6945 unique tag trigram types. In the open test sentences, there were 247 words and 213 tag trigrams that weren’t in"
    },
    {
        "casual_text": "Wikipedia version: enwiki-20190301. The spaCy model is called \"en_core_web_md\" and it's version 2.1.8. It was trained using the OntoNotes dataset.",
        "formal_text": "Convert casual text to formal text: Wikipedia version: enwiki-20190301. The spaCy model is called \"en_core_web_md\" and it's version 2.1.8. It was trained using the On"
    },
    {
        "casual_text": "Okay, so basically, we're looking for the best match, right? We're trying to find the value of t that gives us the highest score when we multiply WD by IF WD of (ws, wit). This is all part of Algorithm 1, which is our transliteration framework.",
        "formal_text": "Convert casual text to formal text: Okay, so basically, we're looking for the best match, right? We're trying to find the value of t that gives us the highest score when we multiply WD"
    },
    {
        "casual_text": "Seq2Seq models with neural networks have become super popular and effective for all kinds of text generation tasks. People like Sutskever, Vinyals, Nallapati, and Lebret have been working on this for years. More recently, big pre-trained Seq2Seq models (like the ones by Lewis) have made it easier to train models on specific tasks without needing tons of labeled data. They still manage to produce text that sounds smooth and grammatically correct. But here's the thing: these generative models often end up creating pretty boring and generic text, which kind of limits how useful they can be. Holtzman and his team pointed this out in 2020. This issue is especially noticeable in tasks where there are many possible inputs leading to the same output. For example, a model might spit out something like: \"Amazing food variety for a coeliac friendly staff and great service. Apartment ideal for business trip maybe needs a bit updating for a family stay. Will definitely be back for leisure stay. Ideally situated.\"",
        "formal_text": "Convert casual text to formal text: Seq2Seq models with neural networks have become super popular and effective for all kinds of text generation tasks. People like Sutskever, Vinyals, Nallapati, and"
    },
    {
        "casual_text": "In the experiments with smaller batch sizes, this method boosts recall, which means the model is less biased towards the \"larger\" class, which in our case is the negative one. The same results hold for both PN and PU setups, suggesting that proportional batching can be helpful for any imbalanced classification problem when you're dealing with limited batch sizes.",
        "formal_text": "Convert casual text to formal text: In the experiments with smaller batch sizes, this method boosts recall, which means the model is less biased towards the \"larger\" class, which in our case is the negative one. The same results"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Here's what the numbers mean for different features: - **Average Sentence**: -0.5, 0.6, 0.1, 0.0 - **Type/Token**: 0.0, -0.9, 0.6, 0.3 - **Length**: -1.0, 0.5, 0.0, 0.3 - **Ratio**: 0.0, -0.9, 0.9, 0.1 - **Sentence Length**: -0.3, 0.5, -0.1, 0.0 - **Numeral/Token**: -0.3, 0.6, -0.1, -0.1 - **Standard Deviation**: -0.5, 0.4, 0.0, 0.1 - **Ratio**: -0.7, 0.7, 0.4, -0.1 - **Average Paragraph**: -0.4, 0.3, -0.1, 0.1 - **Single Lines/Length**: 0.3, 0.1, -0.1, -0.2 - **Sentence Ratio**: 0.0, -0.3, 1.1, -0.4 - **Paragraph Length**: -0.4, 0.4, -0.2, 0.1 - **Single Line Standard Deviation**: -0.3, 0.2, 0.0, 0.1 - **Distribution**: -0.1, 0.4, -0.6, 0.1 - **Relative tf-idf values**: 0.1, -0.1, 0.1, 0.0 - **Topic Average**: -0.4, 0.8, -0.3, 0.0 - **Top 10 weighted words**: 0.4, -0.2, -0.5, 0.1 - **Precision**: -0.4, 0.8, -0.2, -0.1 Basically, these numbers show how different features are performing or scoring in different categories.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. Here's what the numbers mean for different features: - **Average Sentence**: -0.5,"
    },
    {
        "casual_text": "MV-RNN,,,(CNN)(Zeng et al., 2014; Kim, 2014; Collobert et al., 2011),(GCN),,, ,(AGGCNs)(Guo et al., 2019; Zeng et al., 2015), ,PCNN+MIL(Zeng et al., 2015)PCNN+ATT(Lin et al., 2016)MIMLCNN(Jiang et al., 2016),,MLSSA(Du et al., 2018)LFDS, ,RESIDE(Vashishth et al., 2018), ,,ACE04,Li et al., 2014Miwa et al., 2016,Katiyar et al., 2017Sun et al., 2018 ,,",
        "formal_text": "Convert casual text to formal text: MV-RNN,,,,(CNN)(Zeng et al., 2014; Kim, 2014; Collobert et"
    },
    {
        "casual_text": "Graph-structured groups. Basically, the groups in G can overlap without being inside each other. When this happens, the Hasse diagram of G turns into a directed acyclic graph (DAG). Just like with tree-structured groups, a feature group only gets picked if all the groups it’s connected to (its ancestors) are also chosen. Jenatton and his team (2009) came up with a method to figure out the groups by looking at the pattern of sparsity we want. We’re going to explain a step-by-step approach that uses this idea to select feature templates from broad to specific.",
        "formal_text": "Convert casual text to formal text: Graph-structured groups. Basically, the groups in G can overlap without being inside each other. When this happens, the Hasse diagram of G turns into a directed acycl"
    },
    {
        "casual_text": "Alright, so W(s) and b(s) are the parameters of this feed-forward neural network. To make sure we really understand the syntax, position, and other connections between a zero pronoun and its possible candidates, we add some extra features (v(f e)) that we handpick and feed into our neural network model. We take these features from previous research on zero anaphora resolution (like Chen and Ng, 2013; Chen and Ng, 2016) and turn them into vectors to calculate a score that helps us figure out the best match between the zero pronoun and its candidate.",
        "formal_text": "Convert casual text to formal text: Alright, so W(s) and b(s) are the parameters of this feed-forward neural network. To make sure we really understand the syntax, position, and other connections between"
    },
    {
        "casual_text": "We also checked out DialoGPT, which is a model based on GPT-2 and was trained on a huge collection of dialog data. We went with the small version 4 for our evaluation.",
        "formal_text": "Convert casual text to formal text: We also checked out DialoGPT, which is a model based on GPT-2 and was trained on a huge collection of dialog data. We went with the small version 4 for"
    },
    {
        "casual_text": "The linear programming setup guarantees that we can find a unique and efficient solution for the matching. After we get the solution, let's say the highest value of the objective function is S. To figure out the precision, we divide S by the total weight of the n-grams from the translation candidates. For recall, we do the same but with the n-grams from the reference. Finally, we mix precision and recall to get the F-0.8 measure.",
        "formal_text": "Convert casual text to formal text: The linear programming setup guarantees that we can find a unique and efficient solution for the matching. After we get the solution, let's say the highest value of the objective function is S. To figure"
    },
    {
        "casual_text": "Structure-aware uncertainty sampling usually does better than the other methods, but ActiveEA is even better in most cases. ActiveEA also shows that it works well across different datasets, EA models, and bachelor proportions.",
        "formal_text": "Convert casual text to formal text: Structure-aware uncertainty sampling usually does better than the other methods, but ActiveEA is even better even in most cases. ActiveEA also shows that it works well across different datasets, E"
    },
    {
        "casual_text": "The subpopulation assumption is about how distributions overlap, not individual cases. We don't need the training and test data to overlap for our assumptions to hold.",
        "formal_text": "Convert casual text to formal text: The subpopulation assumption is about how distributions overlap, not individual cases. We don't need the training and test data to overlap for our assumptions to hold."
    },
    {
        "casual_text": "When looking at how the Aspell systems stack up against our web-based suggestion systems for news data with artificial misspellings, our web-based system comes out on top across the board. We can see improvements in every error metric we checked. Our best system (System 7) slashes the error rate by 45.7% compared to the best Aspell system (System 3), bringing it down from 4.83% to 2.62%. Plus, our \"no good suggestion\" rate is just 10%, while the Aspell system has an 18% rate. Even without using any language model scores, our web-based system still beats the Aspell system.",
        "formal_text": "Convert casual text to formal text: When looking at how the Aspell systems stack up against our web-based suggestion systems for news data with artificial misspellings, our web-based system comes out on top across the board. We"
    },
    {
        "casual_text": "All the books and short stories in our dataset are by Russian authors, mostly classics like novels by A. S. Pushkin, F. M. Dostoevsky, and A. P. Chekhov. We’ve also got some more modern stuff from writers like B. Akunin and V. Pelevin. In total, there are 6 collections of novels and 16 individual books included in this dataset.",
        "formal_text": "Convert casual text to formal text: All the books and short stories in our dataset are by Russian authors, mostly classics like novels by A. S. Pushkin, F. M. Dostoevsky, and A."
    },
    {
        "casual_text": "The rest of the paper goes like this: In Section 2, we talk about related work. Section 3 covers the multilingual subjectivity analysis problem and the two multiview AdaBoost methods we came up with. Section 4 is all about the experimental results and what we learned from them. Finally, we wrap things up.",
        "formal_text": "Convert casual text to formal text: The rest of the paper goes like this: In Section 2, we talk about related work. Section 3 covers the multilingual subjectivity analysis problem and the two multiview AdaBoost methods we came up with"
    },
    {
        "casual_text": "If an entity in the dev/test data has already appeared in the training data, we call it a \"seen entity.\" If it hasn’t, it’s an \"unseen entity.\" To figure out how much of the dev/test data’s entities have been seen in the training data, we use something called the \"entity coverage ratio.\" This ratio for a dataset D te is written as r(D te ), and it’s calculated using the equation below.",
        "formal_text": "Convert casual text to formal text: If an entity in the dev/test data has already appeared in the training data, we call it a \"seen entity.\" If it hasn’t, it’s an \""
    },
    {
        "casual_text": "In Section 5, we’ll find out that our exact algorithm is usually way too heavy on the computer when dealing with a lot of sentences or a lot of features. So, now we’re going to introduce two",
        "formal_text": "Convert casual text to formal text: In Section 5, we’ll find out that our exact algorithm is usually way too heavy on the computer when dealing with a lot of sentences or a lot of features. So, now we’re"
    },
    {
        "casual_text": "In Section 3.1, we talked about how our dataset includes a lot of people who aren’t part of the 145 core employees we already figured out the gender for. To try and figure out the gender of these other folks, we first look at their first names and then check how tricky it is to guess their gender by using the Social Security Administration’s (SSA) baby names list. First, we’ll explain how we measure how confusing a name is based on the SSA data, and then we’ll show how we use that info to guess the gender of the people in our dataset.",
        "formal_text": "Convert casual text to formal text: In Section 3.1, we talked about how our dataset includes a lot of people who aren’t part of the 145 core employees we already figured out the gender for. To try and"
    },
    {
        "casual_text": "In Section 3.3, they talk about how they calculate the weights for the edges in this cross-modal graph. They do this by looking at how similar words are and how they connect to the image parts (like attributes and objects) and also by using the dependency tree from the text. This method could work for other situations where you need to deal with emotions in multi-modal learning. But, if you’re dealing with different types of tasks or data where there’s not much emotional info or it’s hard to figure out the dependency tree, this approach might not work as well. So, for future work, it might be a good idea to find ways to automatically figure out those edge weights without needing extra knowledge from outside sources.",
        "formal_text": "Convert casual text to formal text: In Section 3.3, they talk about how they calculate the weights for the edges in this cross-modal graph. They do this by looking at how similar words are and how they connect to the image"
    },
    {
        "casual_text": "So, the second thing they need is for the editor to handle different and changing types of entries.",
        "formal_text": "Convert casual text to formal text: So, the second thing they need is for the editor to handle different and changing types of entries. Convert casual text to formal text: So, the second thing they need is for the editor to handle"
    },
    {
        "casual_text": "The group of extra contexts, called A wt, includes all the important contexts for a specific word, like w t. For example, if we're talking about the word \"dog,\" then A dog in the thesaurus would be all the synonyms for \"dog\" listed in that thesaurus.",
        "formal_text": "Convert casual text to formal text: The group of extra contexts, called A wt, includes all the important contexts for a specific word, like w t. For example, if we're talking about"
    },
    {
        "casual_text": "Even when you use automatic tools to change a sentence from a constituency tree to a dependency tree, there are some conjunctions or adverbs that still need to be checked manually.",
        "formal_text": "Convert casual text to formal text: Even when you use automatic tools to change a sentence from a constituency tree to a dependency tree, there are some conjunctions or adverbs that still need to be checked manually"
    },
    {
        "casual_text": "Here, kb represents the average predicted probability for label k within bin b, q kb is the actual observed probability for label k in bin b, B kb is the number of samples in bin b for label k, and n is the total number of samples. Basically, the lower the ECE, the better the model is at being calibrated.",
        "formal_text": "Convert casual text to formal text: Here, kb represents the average predicted probability for label k within bin b, q kb is the actual observed probability for label k in bin b, B k"
    },
    {
        "casual_text": "We've expanded this architecture to work in a multi-view way, where we handle both the context and the response from two different angles. Here, we'll start by giving a quick overview of the word sequence model. After that, we'll dive into the details of the utterance sequence model and the multi-view response selection model.",
        "formal_text": "Convert casual text to formal text: We've expanded this architecture to work in a multi-view way, where we handle both the context and the response from two different angles. Here, we'll start by giving a quick"
    },
    {
        "casual_text": "We can use the idea that when you multiply two matrices, P and Q, the rank of the resulting matrix is no bigger than the smallest rank of P or Q. So, rank(P Q)  min rank(P), rank(Q). This means we can figure out the upper limit for rank(T) in equation (4) based on that.",
        "formal_text": "Convert casual text to formal text: Convert casual text to formal text: We can use the idea that when you multiply two matrices, P and Q, the rank of the resulting matrix is no bigger than the smallest"
    },
    {
        "casual_text": "So, we end up with L + 1 layers of sample representations, which we can write as h l  L l=0, where each h l is a vector in R d.",
        "formal_text": "Convert casual text to formal text: So, we end up with L + 1 layers of sample representations, which we can write as h l  L l=0, where each h l is"
    },
    {
        "casual_text": "Sure! So, avoiding those extra questions that users might ask to clarify things is definitely a good thing. But we can also use what we've learned from our research to make other parts of how people interact with systems better. For example, figuring out topics and activities can get tricky if the way a user breaks things down isn't helpful or even clashes with how the system understands things. If that happens, the conversation might fall apart or the system might have to double-check basic stuff, like asking, \"Do you know what cups are?\" So, what might seem like simplifying things for the user could actually be leaving out important details that the system needs to understand everything properly.",
        "formal_text": "Convert casual text to formal text: Sure! So, avoiding those extra questions that users might ask to clarify things is definitely a good thing. But we can also use what we've learned from our research to make other parts of"
    },
    {
        "casual_text": "When it comes to saving compressed text, the two-byte-word encoding method works no matter what word list you're using. You can check out some examples in the earlier section.",
        "formal_text": "Convert casual text to formal text: When it comes to saving compressed text, the two-byte-word encoding method works no matter what word list you're using. You can check out some examples in the earlier section."
    },
    {
        "casual_text": "The neural methods (Roth and Lapata, 2016; Foland and Martin, 2015) are pretty language-independent but aren't super easy to interpret. These methods use neural network encoders to indirectly encode SSDP information. Roth and Lapata (2016) and Foland and Martin (2015) turn SS-DPs into continuous embeddings using either an LSTM model or a Convolutional Neural Network. Shi et al. (2020) take it a step further by combining SSDP info with semantic dependency stuff using a Transformer (Vaswani et al., 2017). Their work shows better performance in one or more languages, but figuring out how the model actually works is tricky. Neural encoders, like the LSTM model in Roth and Lapata (2016), map SSDPs into a high-dimensional space. This space is super complex, making clustering based on Euclidean distances not very effective. Roth and Lapata (2016) try to explain their model's behavior using clustering analysis, suggesting it picks up on a lot of linguistic stuff. But the linguistic phenomena they capture are kind of scattered and only apply to a few syntactic patterns.",
        "formal_text": "Convert casual text to formal text: The neural methods (Roth and Lapata, 2016; Foland and Martin, 2015) are pretty language-independent but aren't super easy to interpret. These methods use neural"
    },
    {
        "casual_text": "Lately, people have been using statistical methods to figure out the possible outcomes of a conversation, which helps them deal with the uncertainty from the NLU module. In the most basic setup, where there's no ASR or NLU, like in a text-based chatbot (Henderson et al., 2013), they just look at the words used in the conversation, kind of like a \"bag of words.\" If there's an NLU module, they might use standardized ways of understanding what the conversation is about (Bunt et al., 2010). And if the ASR part of the system picks up on tone or rhythm (Milone and Rubio, 2003), that can also be part of what they consider. A statistical dialog state tracker keeps track, at every moment, of the chances that the conversation is in a certain state, which is like the system's guess about what's going on. The whole process of filling in the blanks, so to speak, involves gathering information and putting it all together—that's dialog state tracking. The goal is to figure out the right answers for each part of the conversation as early as possible. From now on, we'll think of the state as a bunch of variables, each with a set of possible answers we already know about.",
        "formal_text": "Convert casual text to formal text: Lately, people have been using statistical methods to figure out the possible outcomes of a conversation, which helps them deal with the uncertainty from the NLU module. In the most basic setup, where there"
    },
    {
        "casual_text": "Rule 1: If any part of what's in Cf(Ui-1) shows up as a pronoun in Ui, then Cb(Ui) will also be a pronoun.",
        "formal_text": "Convert casual text to formal text: Rule 1: If any part of what's in Cf(Ui-1) shows as a pronoun in Ui, then Cb(Ui) will also be a"
    },
    {
        "casual_text": "The main goal of D-MILN's final objective function is a mix of document-level loss and some extra regularizations to keep things diverse. To keep it simple, let's just talk about the objective function for one document:",
        "formal_text": "Convert casual text to formal text: The main goal of D-MILN's final objective function is a mix of document-level loss and some extra regularizations to keep things diverse. To keep it simple, let'"
    },
    {
        "casual_text": "We can use the hidden representations H, which are based on individual tokens, to make predictions for the entire document.",
        "formal_text": "Convert casual text to formal text: We can use the hidden representations H, which are based on individual tokens to make predictions for the entire document. Convert casual text to formal text: We can use the hidden representations H"
    },
    {
        "casual_text": "Everything looks spot on with the domain choices, and the conversation flows really smoothly.",
        "formal_text": "Convert casual text to formal text: Everything looks spot on with the domain choices, and the conversation flows really smoothly. Convert casual text to formal text: Everything looks spot on with the domain choices, and the conversation flows really smoothly. Con"
    },
    {
        "casual_text": "So far, people have come up with different ways to summarize multiple documents. One method is called MEAD, which uses things like cluster centroids, sentence position, and TF.IDF to score sentences. Another one, NeATS (created by Lin and Hovy in 2002), adds extra features like topic signature and term clustering to pick out important stuff. It also uses MMR (made by Goldstein and others in 1999) to avoid repeating the same information.",
        "formal_text": "Convert casual text to formal text: So far, people have come up with different ways to summarize multiple documents. One method is called MEAD, which uses things like cluster centroids, sentence position, and TF.IDF to score"
    },
    {
        "casual_text": "2. Make sure there's at least one word with a regular label. Even if the whole sentence is labeled differently, there should still be one word that’s labeled normally.",
        "formal_text": "Convert casual text to formal text: 2. Make sure there's at least one word with a regular label. Even if the whole sentence is labeled differently, there should still be one word that’s labeled normally"
    },
    {
        "casual_text": "Alright, so slot filling is usually seen as a sequence labeling thing, so we use the conlleval 4 to measure the F1 score at the token level. For intent detection, we just go with classification accuracy. Oh, and here's a little twist: some of the ATIS utterances have more than one label. Following what others have done (like Tur et al. in 2010 and Zhang and Wang in 2016), we say an utterance is correctly classified if it matches any of the ground truth labels.",
        "formal_text": "Convert casual text to formal text: Alright, so slot filling is usually seen as a sequence labeling thing, so we use the conlleval 4 to measure the F1 score at the token level. For intent detection"
    },
    {
        "casual_text": "Here’s the same info, but in a more relaxed and conversational tone: Let’s talk about the performance of different models: - **Retrieval-based**: - Recall: 0.23 (0.25) - Precision: 0.61 (0.67) - F-measure: 0.34 (0.37) - Another set: Recall 0.28 (0.30), Precision 0.72 (0.78), F-measure 0.41 (0.44) - **Seq2seq**: - Recall: 0.06 (0.07) - Precision: 0.07 (0.08) - F-measure: 0.07 (0.08) - Another set: Recall 0.10 (0.13), Precision 0.11 (0.13), F-measure 0.10 (0.13) - **Case frame-based**: - Recall: 0.10 (0.10) - Precision: 0.62 (0.62) - F-measure: 0.16 (0.16) - Another set: Recall 0.05 (0.05), Precision 0.75 (0.75), F-measure 0.09 (0.09) The numbers in brackets are the \"relaxed\" measures.",
        "formal_text": "Convert casual text to formal text: Here’s the same info, but in a more relaxed and conversational tone: Let’s talk about the performance of different models: - **Retrieval-based**:"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way. We’re doing things a bit differently here compared to what’s mentioned in [4]. We’re approaching it from the bottom up instead of the top down to better explain our point. But don’t worry, this isn’t a huge difference—it’s more of a tweak. So, following what we talked about earlier, we’re assuming that each semantic in a semantic structure is connected to something. For the words in (3), we’ve got this idea. The diagram should help show how we plan to adjust the parsing system from Section 1 to produce something called IRS’s as output instead of f-structures. Instead of passing around partially filled f-structures based on the words, we’re passing around partially filled IRS’s, which get completed through unification. The LFG control system will automatically handle putting the discourse referents (basically, the things being talked about) in the right spot for the verb. So, we don’t have to do any extra work to figure out the grammatical relationships in a sentence. But what about the logical relationships? Remember, every clause has a main head, and the features of each phrase are tied to the features of its head. For example, in (3), the head of S -> NP VP is the VP, and the head of VP -> V NP is the V (the verb). This shows how the verb plays a key role in figuring out and limiting the grammatical relationships in the sentence.",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in a simpler way. We’re doing things a bit differently here compared to what’s mentioned in [4]. We’re approaching it from"
    },
    {
        "casual_text": "Summarizing opinions and arguments has been a hot topic lately. People are really into breaking down what’s being said in reviews (like Amplayo and Lapata, 2021, or Elsahar et al., 2021) or in argumentative texts (Wang and Ling, 2016; Syed et al., 2020). KPA is part of this trend, but it brings in a more numerical approach to show how opinions are spread out in the data being analyzed. If you want to dive deeper into how KPA connects to argument clustering and summarization, check out Bar-Haim et al. (2020a, b).",
        "formal_text": "Convert casual text to formal text: Summarizing opinions and arguments has been a hot topic lately. People are really into breaking down what’s being said in reviews (like Amplayo and Lapata, 2021, or"
    },
    {
        "casual_text": "GPU was first used in topic modeling back in 2011 by Mimno et al. They used it to focus on words that appeared together in the same documents a lot, based on how often they showed up together in the whole dataset. Later, in 2013, Chen et al. used GPU to handle a problem in topic modeling where prior knowledge about a domain could mess things up. They did this by boosting the importance of rare words in the knowledge sets. But even with these improvements, they were still just working with individual words. Most topic models, like LDA, treat topics as a mix of single words and assume that the order of words doesn’t really matter. However, some researchers have tried to include word order by using n-gram language models. For example, Wallach in 2006 came up with the Bigram Topic Model (BTM), which combines bigram statistics with topic modeling to better represent documents. Then, Wang et al. in 2007 took it a step further with the Topical N-gram Model (TNG), which is like a more advanced version of BTM. It generates words in order by first picking a topic, then deciding if it’s a unigram or bigram, and finally selecting the word from a distribution that’s specific to the topic and whether it’s a unigram or bigram. Even though the \"bag-of-words\" assumption, which means ignoring word order, isn’t always perfect in real-world applications, it’s still much faster and easier to work with than more complex models that try to account for word order when finding important n-grams. Our approach is a bit different from these earlier works in two main ways. First, we stick with the \"bag-of-words\" or more accurately, the \"bag-of-terms\" assumption.",
        "formal_text": "Convert casual text to formal text: GPU was first used in topic modeling back in 2011 by Mimno et al. They used it to focus on words that appeared together in the same documents a lot, based on"
    },
    {
        "casual_text": "Sure! Here's a more casual version: The patent kind code tells you where a document is in the filing process. For example, \"A\" means it's an application, and \"B\" means it's a granted patent. There are also different publication levels, numbered 1 through 9. You can find more details here: http://www.wipo.int/standards/en/part_03.html. Also, check out this link: http://gargantua.sourceforge.net.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: The patent kind code tells you where a document is in the filing process. For example, \"A\" means it's an application,"
    },
    {
        "casual_text": "Looking at the syntactic differences between the corpora, it’s pretty clear from our findings that translated texts show more variation in syntax across all the pairs we compared and based on all the measures we used (1-C, R, and Rsq). It’s also obvious that the syntax differences are bigger when comparing texts from different domains. Based on these results, we can say there’s no sign that syntax tends to become more similar over time. In fact, Table 4 even suggests that translated texts differ more in syntax compared to non-translated texts in our data.",
        "formal_text": "Convert casual text to formal text: Looking at the syntactic differences between the corpora, it’s pretty clear from our findings that translated texts show more variation in syntax across all the pairs we compared and based on"
    },
    {
        "casual_text": "Sure! Here's a more casual version: \"Fine-grained class examples: man, woman, child, boy, girl, bicycle (or bike), airplane (or plane, jet, jetliner), cow (or cattle), TV (or television), refrigerator (or freezer), laptop (or computer). Table 10 lists some object categories and their fine-grained versions that we use in our substitutivity split. When we're doing the inference part, we swap the generated fine-grained word with one of its synonyms.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: \"Fine-grained class examples: man, woman, child, boy, girl, bicycle (or bike), airplane (or plane, jet"
    },
    {
        "casual_text": "(2) A new approach that trains T-cells to target cancer cells... Any system that analyzes opinions using a subjectivity lexicon can get confused by subjectivity clues that are actually used in an objective way (leading to incorrect results). In a paper we wrote back in 2009 (Akkaya et al.), we introduced the idea of Subjectivity Word Sense Disambiguation, which is all about figuring out whether words in a text are being used in a subjective or objective sense. Think of it as a simplified, task-focused version of word sense disambiguation. We found that by using this subjectivity information, we could improve both subjectivity and sentiment analysis a lot, because it helps avoid those incorrect results.",
        "formal_text": "Convert casual text to formal text: (2) A new approach that trains T-cells to target cancer cells.. Any system that analyzes opinions using a subjectivity lexicon can get confused by subjectivity clues that are actually"
    },
    {
        "casual_text": "So, basically, this thing happens where a translation might get the same Bleu score as the one in Table 1, even though real people would totally give it a lower score. The issue gets even trickier because Bleu treats all the words in the reference sentences the same, no matter if they’re important or just filler words (like Babych and Hartley pointed out in 2004). So, leaving out key words doesn’t hurt the score any more than skipping over little words that don’t really matter.",
        "formal_text": "Convert casual text to formal text: So, basically, this thing happens where a translation might get the same Bleu score as the one in Table 1, even though real people would totally give it a lower score. The issue gets even"
    },
    {
        "casual_text": "Okay, let's break this down in simpler terms. We have a theory called T, which is defined using two sets: NODE (a bunch of nodes) and ATOM (a bunch of atoms). The set of global contexts for T is made up of all pairs that look like (N, (t)), where N is a node from NODE and (t) is a sequence of atoms from ATOM. We'll call these contexts C. Now, the evaluation relation  is basically a way to map things from the set of contexts (C) and sequences of descriptions (DESC*) to ATOM*. We write cF4) to say that  evaluates to fl in the global context C.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in simpler terms. We have a theory called T, which is defined using two sets: NODE (a bunch of nodes) and ATOM (a"
    },
    {
        "casual_text": "When we look at the bilingual transfer learning models (check Table3 (2)) compared to the Arabic-only models (Table3 (1)), we see that using MBERT gives us an average F1 improvement of 3, and GIGABERT gives us 2.5. This proves that bilingual transfer learning works pretty well. Both the PIPELINE and JOINT approaches show similar improvements, which means it's the extra training data, not the model itself, that makes the difference. What's cool is that MBERT and GIGA-BERT give us pretty much the same gains, showing that both models transfer knowledge from English to Arabic equally well. Even though MBERT starts a bit behind in Arabic performance (by 15.29 F1), it catches up. It's kind of unexpected since GIGABERT is a bilingual model made just for English-Arabic. We're definitely going to look into this next.",
        "formal_text": "Convert casual text to formal text: When we look at the bilingual transfer learning models (check Table3 (2)) compared to the Arabic-only models (Table3 (1)), we see that using MBERT gives us an average F1"
    },
    {
        "casual_text": "To figure out how premises connect to the conclusion, we first need to pinpoint the targets in the premises. This whole idea of finding target phrases in argumentative text was brought up by Bar-Haim et al. (2017). Here, we're dealing with it using BIO sequence labeling, which means we classify each word as the start, part of, or not part of a target. Since identifying premise targets isn't our main goal, we just grab a top-notch neural sequence tagger (from Akbik et al., 2018) and train it on a claim stance dataset. Then, we use it to automatically tag targets in all the input premises.",
        "formal_text": "Convert casual text to formal text: To figure out how premises connect to the conclusion, we first need to pinpoint the targets in the premises. This whole idea of finding target phrases in argumentative text was brought up by Bar-Haim"
    },
    {
        "casual_text": "For each entity e 1 j, we think its neighbors pointing to it can help it figure out everything. So, we say e 1 i e 1 j, and e 1 i is part of N in j with w ij = 1, where N in j is the group of neighbors pointing to e 1 j. In this case, we assume all those neighbors have the same effect on e 1 j.",
        "formal_text": "Convert casual text to formal text: For each entity e 1 j, we think its neighbors pointing to it can help it figure out everything. So, we say e 1 i e 1 j, and"
    },
    {
        "casual_text": "For each sentence in the test set, we calculate a unique set of weights. We think this approach might work better because dividing the training data into smaller chunks lets the EM process assign language model weights more freely. But, the weights we get this way might not be as reliable since we're only using one sentence at a time for the estimation, which isn't much data.",
        "formal_text": "Convert casual text to formal text: For each sentence in the test set, we calculate a unique set of weights. We think this approach might work better because dividing the training data into smaller chunks lets the EM process assign"
    },
    {
        "casual_text": "For this project, we stuck with a single model that was trained on English tweets for all four codemixed datasets, since they all mixed in English. In the future, it would be cool to try out different models that might work better. We could also look into improving the optimization process, like using the Token Ratio when picking pseudo-labels and maybe even trying out active learning.",
        "formal_text": "Convert casual text to formal text: For this project, we stuck with a single model that was trained on English tweets for all four codemixed datasets, since they all mixed in English. In the future, it would"
    },
    {
        "casual_text": "The pronominal system plays a big role in this productivity stuff, especially in transitive paradigms where agent and patient pairs get combined, like you can see in Figure 1.",
        "formal_text": "Convert casual text to formal text: The pronominal system plays a big a big in this productivity stuff, especially in transitive paradigms where agent and patient pairs get combined, like you see in Figure 1. Figure 1. Convert"
    },
    {
        "casual_text": "Yeah, the context of words in the title and body really matters when trying to figure out what they mean. Like, in the first webpage title in Fig. 1, the words \"Weight\" and \"Exercise\" around \"Loss\" help us understand it’s about fitness, not losing money. Transformers (Vaswani et al., 2017) are pretty good at handling this kind of context stuff, so we use two separate Transformers to learn how words in the title and body fit together. We call the sequences of hidden representations for the title and body E t = [e t 1, e t 2, . . . , e t N ] and E b = [e b 1, e b 2, . . . , e b P ], respectively. Now, not all words are equally important. For example, in Fig. 1, \"MUST\" is way more important than \"About\" when trying to figure out if a title is clickbait. So, we use attention mechanisms (Yang et al., 2016) to pick out the key words in the title and body and combine them into unified representations, which we call e t and e b. Here’s how we do it:",
        "formal_text": "Convert casual text to formal text: Yeah, the context of words in the title and body really matters when trying to figure out what they mean. Like, in the first webpage title in Fig. 1, the words \"Weight\" and"
    },
    {
        "casual_text": "The baselines are the current ways of measuring stuff with permutations, like KENDALL's tau, HAMMING, and ULAM used in that Birch and Osborne paper from 2011, and also in Isozaki et al., 2010. There's also SPEARMAN rho from the same Isozaki et al. paper. Plus, there's the FUZZY Reordering Score, which is a way to measure reordering, but I don't have the source for that one right now.",
        "formal_text": "Convert casual text to formal text: The baselines are the current ways of measuring stuff with permutations, like KENDALL's tau, HAMMING, and ULAM used in that Birch and Osborne"
    },
    {
        "casual_text": "For the pseudo-disambiguation task, the test set is made up of tuples like (R, t, r, r'). To create this test set, we followed the same approach that Rooth et al. (1999) and Erk et al. (2010) used.",
        "formal_text": "Convert casual text to formal text: For the pseudo-disambiguation task, the test set is made up of tuples like (R, t, r, r'). To create this test set,"
    },
    {
        "casual_text": "In this part, we’ll dive deeper into the topic with both numbers and descriptions, plus take a closer look at the mistakes and the stuff that’s still tricky.",
        "formal_text": "Convert casual text to formal text: In this part, we’ll dive deeper into the topic with both numbers and descriptions, plus take a closer to the mistakes and the stuff still tricky."
    },
    {
        "casual_text": "To boost performance even more, we’re using these popular techniques that are commonly used for GEC tasks:",
        "formal_text": ":: Convert casual text to formal text: To boost performance even further, we’re using these popular techniques that are commonly for GEC tasks: Convert casual text to formal text: To boost performance even more, we’"
    },
    {
        "casual_text": "Besides being a standalone resource, plWordNet is also part of bigger projects that work with multiple languages or different types of data. For example, it’s included in WordTies (Pedersen et al., 2012), Open Multilingual WordNet (Bond and Foster, 2013), and even in projects that deal with things like gestures, like the one where gestures were categorized based on verb groups in plWordNet (Lis and Navarretta, 2014). It’s also been mentioned in a resource for textual entailment (Przepiórkowski, 2015) and has been used for connecting ontologies with lexicons (Jastrzb et al., 2016).",
        "formal_text": "Convert casual text to formal text: Besides being a standalone resource, plWordNet is also part of bigger projects that work with multiple languages or different types of data. For example, it’s included in WordTies"
    },
    {
        "casual_text": "Our concept is based on the Mixture of Experts models, which were introduced by Jacobs et al. back in 1991. Basically, we think that for each example x, there's a mix of M different training topics it could belong to. We use t to represent the topic. Here's how we model p(y|x):",
        "formal_text": "Convert casual text to formal text: Our concept is based on the Mixture of Experts models, which were introduced by Jacobs et al. back in 1991. Basically, we think that for each example x,"
    },
    {
        "casual_text": "This paper has two main contributions. First, we introduce a new problem called QA-style sentiment analysis and create a big dataset with labels specifically for this task. We’re releasing this dataset to encourage more research in this area. Second, we come up with a fancy model called a hierarchical matching network to tackle the challenges of QA-style sentiment classification. Here’s how it works: we break down both the question and the answer into individual sentences and pair them up as [Q-sentence, A-sentence] units. Then, we use something called a QA bidirectional matching layer to encode these pairs and dig into the sentiment info. Finally, the model’s self-matching attention layer helps figure out which of these encoded pairs are most important, making it easier to figure out the overall sentiment of the QA pair. The experiments show that our approach works way better than some other strong methods out there for this type of sentiment classification.",
        "formal_text": "Convert casual text to formal text: This paper has two main contributions. First, we introduce a new problem called QA-style sentiment analysis and create a big dataset with labels specifically for this task. We’re releasing this"
    },
    {
        "casual_text": "Back when these experiments were done, SWESIL only had one SL dependency pair to work with. Since there wasn’t any extra context to help, it wasn’t always clear which pair was better than the others. Basically, this just means that without more info, SWESIL thinks all those pairs are equally likely.",
        "formal_text": "Convert casual text to formal text: Back when these experiments were done, SWESIL only had one SL dependency pair to work with. Since there wasn’t any extra context to help, it wasn’t always clear which pair was"
    },
    {
        "casual_text": "Godot is a cool mobile robot called the RWI Magellan Pro, with a built-in PC running Linux (check out Fig. 1). It’s shaped like a cylinder, about 50 cm tall and 41 cm across. Godot’s got a bunch of sensors: 16 sonars, infrared sensors, bumpers, an odometry thing, and a color camera with a pan-tilt feature. The onboard computer hooks up to the local network using a wireless LAN connection. For navigation, Godot mainly uses the sonars, infrared sensors, and odometry—it doesn’t really rely on the bumpers or the camera for that. But the camera does come in handy for giving the user live feedback (see Fig. 2). This lets the user kind of \"see through Godot's eyes\" during a conversation.",
        "formal_text": "Convert casual text to formal text: Godot is a cool mobile robot called the RWI Magellan Pro, with a built-in PC running Linux (check out Fig. 1). It’s shaped like"
    },
    {
        "casual_text": "In this part, we're testing a language model (LM) trained with a large-margin criterion alongside a strong Chinese-to-English neural machine translation (NMT) system. The NMT model was trained on 2 million parallel sentence pairs. Following the approach of Shen et al. (2016), we used the NIST 06 newswire section (616 sentences) for development and the NIST 08 newswire section (691 sentences) for testing. We trained the model using the OpenNMT-py 2 package with its default settings: batch size is 64, word embedding size is 500, dropout rate is 0.3, target vocabulary size is 50K, and we trained for 20 epochs. After that, we got a minimum development perplexity of 7.72.",
        "formal_text": "Convert casual text to formal text: In this part, we're testing a language model (LM) trained with a large-margin criterion alongside a strong Chinese-to-English neural machine translation ("
    },
    {
        "casual_text": "Hey, just a heads-up: in most SENSEVAL datasets, they usually give you separate training and test data. But for some target words in SENSEVAL1, they didn't have both. So, we decided to mix the training and test data together into one big dataset and run 10-fold cross validation instead.",
        "formal_text": "Convert casual text to formal text: Hey, just a heads-up: in most SENSEVAL datasets, they usually give you separate training and test data. But for some target words in SENSEVAL1, they didn"
    },
    {
        "casual_text": "Our guess that tweaking the original rules for English wouldn’t make much of a difference because they’re already pretty well-tuned turned out to be kind of right. In the second and third experiments, the model that used a simpler set of rules for English got better on some test data after tweaking, but when it came to the final test, it didn’t show any improvement. On the other hand, the model trained on Chinese did show a little bit of improvement in both cases. We’re not totally sure why the gains we saw in the second experiment didn’t carry over to the final evaluation, but looking at Figure 5 and the results with Chinese, we think that using EM for reestimation could help adapt parsing models to new languages or different types of text.",
        "formal_text": "Convert casual text to formal text: Our guess that tweaking the original rules for English wouldn’t make much of a difference because they’re already pretty well-tuned turned out to be kind of right. In the second"
    },
    {
        "casual_text": "Mapping rules are like special instructions that help connect the meaning of something in one language to how it’s said in another. In this case, we’re talking about how to turn ideas into Arabic sentences. There are two kinds of mapping rules: lexical mapping rules and structural mapping rules. Lexical mapping rules use a special list called the mapping lexicon to change words in the original idea (called the IF representation) into their Arabic equivalents. Structural mapping rules, on the other hand, figure out how to arrange those Arabic words into a proper sentence. They take the main parts of the sentence—like the subject, verb, and object—from the IF representation and put them together in a way that makes sense in Arabic. For example, Figure 4 shows a structural mapping rule that pulls out four key parts of an Arabic sentence: an optional part for adding extra sentences, the subject, the verb, and the thing the verb is acting on (the complement). These parts are then used to build the actual Arabic sentence.",
        "formal_text": "Convert casual text to formal text: Mapping rules are like special instructions that help connect the meaning of something in one language to how it’s said in another. In this case, we’re talking about how to turn ideas into Arabic"
    },
    {
        "casual_text": "Our neural POS tagging and language modeling work really well on un-segmented data and in real-time situations. They’re on par with the best methods out there and are helpful for improving things like utterance segmentation and disfluency detection, which is what we’re aiming for. Looking at Table 1, the top-performing systems for individual tasks don’t come close to the best joint systems in terms of any important metric. Our model with all four tasks scored the highest in F rpS, F e, F uttSeg, ACC P OS, and P erplexity. Throwing in language modeling, utterance segmentation, and POS tagging also boosts disfluency detection in models with two or three tasks. The improvements are small, but they’re steady across all joint models. However, when it comes to F e, adding more than two tasks doesn’t make a difference if the input is just lexical. Our four-task model beats Hough and Schlangen (2017)’s joint model in both disfluency detection and utterance segmentation.",
        "formal_text": "Convert casual text to formal text: Our neural POS tagging and language modeling work really well on un-segmented data and in real-time situations. They’re on par with the best methods out there and are"
    },
    {
        "casual_text": "To check how accurate the facts are in our summaries, we use something called the Factual F1 score. The factual accuracy score we use in our reward system looks at how accurate a single summary is, but if we try to compare it across a whole bunch of summaries, it can be kind of misleading, just like how accuracy can be a tricky measure in things like search results (Manning et al., 2008). To explain why, imagine there's a rare medical condition in our data. A model that always says the condition isn't there (like, v = 0; no disease present) might look really accurate, but it wouldn't actually be helpful in real life. Instead, for each condition, we look at what the model predicts for all the test cases and calculate its F1 score. Then, we average out the F1 scores for all the conditions to get the overall Factual F1 score for the model.",
        "formal_text": "Convert casual text to formal text: To check how accurate the facts are in our summaries, we use something called the Factual F1 score. The factual accuracy score we use in our reward system looks at how accurate a"
    },
    {
        "casual_text": "In Table 4, we compare different ways of representing layers using both APD-based and distribution-based metrics. We noticed that the distribution-based metrics didn’t really show much, even when we tried using dimensionality reduction techniques. Sure, a couple of them had a tiny positive correlation, like in Figure 9, where you can see the explained variance by the number of components in PCA for words like \"bromance\" and \"whadja,\" but we decided to just skip that approach altogether. On the flip side, the APD results look way better—they show a strong correlation for a lot of the setups, which tells us that APD is pretty solid at picking up on semantic changes. We’ve got some of these results in Table 6, showing the polysemy score distribution for slang words, but we left out the hybrid words for this part. Now, when it comes to the nonslang meanings of those hybrid words, we found an average polysemy score of (6. 880  6. 080), which is way different (p  0. 001) from the nonslang words, which had a score of (3. 079  2. 780). This is kinda cool because it suggests that if we had included nonslang words with hybrid meanings in our nonslang sample, the difference in polysemy between slang and nonslang words would’ve been even bigger. Some examples of words in this category with high MW polysemy scores are \"split,\" \"down,\" and \"walk.\"",
        "formal_text": "Convert casual text to formal text: In Table 4, we compare different ways of representing layers using both APD-based and distribution-based metrics. We noticed that the distribution-based metrics didn’t really show much, even when we tried"
    },
    {
        "casual_text": "There are a few common mistakes that both parsing methods seem to struggle with. For example, dealing with coordination and multiple modifiers tends to cause the most errors in both cases. But when we look at the actual numbers, using the gold standard dataset for training helps reduce these errors. On the other hand, figuring out the parent node for conjunctions or adverbs gets a lot better when the parser is trained on gold standard data. This is likely because these elements aren’t marked in the constituency treebank, so the training data for these things is pretty messy in the silver standard treebank. Overall, it seems like certain grammatical stuff—like how things are attached, for example—works better when trained on gold standard data. Here’s a quick breakdown of the terms: - **goldTrain**: Training the Bohnet parser using the gold standard data. - **silverTrain**: Training the Bohnet parser using the silver standard data. - **BerkeleyConv**: Training the Berkeley parser on gold standard constituency data and then converting the output to dependency format. - **convDep**: Training the Bohnet parser without dependency labels on the silver standard data.",
        "formal_text": "Convert casual text to formal text: There are a few common mistakes that both parsing methods seem to struggle with. For example, dealing with coordination and multiple modifiers tends to cause the most errors in both cases. But when"
    },
    {
        "casual_text": "We worked with the data from the shared task on identifying offensive language in Dravidian languages, organized by Chakravarthi and others in 2021 (and also mentioned in their earlier works in 2020). The data was nicely annotated at the comment or post level. The training data included 35,139 samples for Tamil, 16,010 for Malayalam, and 6,217 for Kannada. You can check out the details in Table 1, Table 2, and Table 3.",
        "formal_text": "Convert casual text to formal text: We worked with the data from the shared task on identifying offensive language in Dravidian languages, organized by Chakravarthi and others in 2021 (and also mentioned in their earlier works in 2020"
    },
    {
        "casual_text": "For each word, we use word embedding to capture its grammar and meaning, and position embedding to show where it is in the sentence. Then, we combine both the word embedding and position embedding to create the input vector for that word, which goes into the CNN. (Check out Figure 2 for more details.)",
        "formal_text": "Convert casual text to formal text: For each word, we use word embedding to capture its grammar and meaning, and position embedding to show where it is in the sentence. Then, we combine both the word embedd"
    },
    {
        "casual_text": "Check out the results for all the models in Table 1. Here's what we can see:",
        "formal_text": "Convert casual text to formal text: Check out the results for all the models in Table 1. Here's what we see: Check out the results for all the models in Table 1. Here's what we see: Check out the results for"
    },
    {
        "casual_text": "The superscript t stands for parameters in the teacher model, [N] is just the set of numbers from 1 to N, L t CE is the cross-entropy loss used when training the teacher, and  BERT 12 represents the parameters of BERT 12.",
        "formal_text": "Convert casual text: The superscript t stands for parameters in the teacher model, [N] is just the set of numbers from 1 to N, L t CE is the cross-entropy loss used when training the teacher"
    },
    {
        "casual_text": "So, figuring out how to use resources from similar areas to boost accuracy in our main area is super important.",
        "formal_text": "Convert casual text to formal text: So, figuring how to use resources from similar areas to boost accuracy in our main area. Convert casual text to formal text. Convert casual text to formal text. Convert casual text to formal"
    },
    {
        "casual_text": "Once we have the segmentation (r, s, t) for each word w, it's pretty straightforward to figure out the paradigms. We just group together words that have the same immediate root r, no matter what transformation rules are applied. For instance, words like \"reporting,\" \"reported,\" and \"reports\" are segmented as (report, -ed, NULL), (report, -ing, NULL), and (report, s, NULL). Similarly, \"baked,\" \"baking,\" and \"bakes\" are segmented as (bake, -ed, DEL-e), (bake, -ing, DEL-e), and (bake, -s, NULL). From these, we can create a paradigm using these two sets of words: -ed, -ing, -s  report, bake.",
        "formal_text": "Convert casual text to formal text: Once we have the segmentation (r, s, t) for each word w, it's pretty straightforward to figure out the paradigms. We just group together words that have the"
    },
    {
        "casual_text": "In Tables 6 and 7, we show how our method works on the JOBS and GEO datasets. We used the same settings and rules as we did for ATIS, and we kept the same limit on how much we can expand things. VAR-STREAM is way faster than Fixed and VAR-BATCH in this setup. Check out Table 7 for the GEO dataset, which deals with geographical queries. VAR-STREAM is way more efficient than both FIXED and VAR-BATCH when it comes to expansions per step, and this really shows in the actual time it takes.",
        "formal_text": "Convert casual text to formal text: In Tables 6 and 7, we show how our method works on the JOBS and GEO datasets. We used the same settings and rules as we did for ATIS, and we kept"
    },
    {
        "casual_text": "Here’s the info in a more casual way: Token recall: - 86.0% for the reference/MT-search-space. - 50.0% for the reference/MT-output. - 12.3% for the stem-only reference/MT-output, and out of that, 11.2% is reachable. Table 1 breaks down the lexical coverage analysis for the baseline SMT system (English-Russian wmt12).",
        "formal_text": "Convert casual text to formal text: Here’s the info in a more casual way: Token recall: - 86.0% for the reference/MT-search-space. - 50.0% for the reference/"
    },
    {
        "casual_text": "Hey, just a heads-up: SKT and SKT  are basically the same network, but they're tested with different types of knowledge—prior knowledge for SKT and posterior knowledge for SKT . That's why the KL divergence stays the same.",
        "formal_text": "Convert casual text to formal text: Hey, just a heads-up: SKT and SKT  are basically the same network, but they're tested with different types of knowledge—prior knowledge for SKT and posterior"
    },
    {
        "casual_text": "To make sure that the sounds at the edges of words can actually connect to each other, we use a special table called the phoneme connectivity table. This table lists all the pairs of sounds that can connect in Korean, based on how the letters at the ends of words change. The table helps us figure out which sound combinations are allowed in the language. When we convert a word into its possible sound sequences, we often end up with a bunch of options. We then organize these sequences into something called a phoneme graph. This graph helps us find the right sequence of sounds for a given sentence. The connectivity check goes through this graph and weeds out any sequences that don’t follow the rules of Korean grammar.",
        "formal_text": "Convert casual text to formal text: To make sure that the sounds at the edges of words can actually connect to each other, we use a special table called the phoneme connectivity table. This table lists all the pairs of sounds that can"
    },
    {
        "casual_text": "For a fair comparison, we ran all our experiments in this part with the same setup as our baseline system. When we compared everything to the baseline, all the improvements we saw were statistically significant (p  0.005). Table 8 shows how well the coreference resolution worked when we included automatically detected zero pronouns. Here's what we found:",
        "formal_text": "Convert casual text to formal text: For a fair comparison, we ran all our experiments in this part with the same setup as our baseline system. When we compared everything to the baseline, all the improvements we saw were statistically significant"
    },
    {
        "casual_text": "People have tried to figure out how translation differences affect machine translation (MT) by adding noise to sentence alignments. A study by Goutte et al. in 2012 showed that statistical MT is pretty good at handling noise and only starts to struggle when the noise levels get really high. On the other hand, neural MT systems seem to be more sensitive to noise, according to Chen et al. in 2016. This might be because they often give high probabilities to rare events, as Hassan et al. pointed out in 2018. There have also been efforts to measure how similar the meaning of two pieces of text is, whether they're in the same language or different ones. Agirre et al. explored this in 2016. In another study, Mueller and Thyagarajan in 2016 came up with a network for comparing sentences in the same language. They used a basic LSTM layer to create sentence representations and found that a simple SVM classifier using these representations did really well in a task called textual entailment, setting a new state-of-the-art. Similarly, He and Lin in 2016 worked on the same goal but used multiple convolutional layers and looked at how words interact with each other in pairs.",
        "formal_text": "Convert casual text to formal text: People have tried to figure out how translation differences affect machine translation (MT) by adding noise to sentence alignments. A study by Goutte et al. in 2012 showed that statistical"
    },
    {
        "casual_text": "In simpler terms, \"described by\" here means we can figure out the AM dependency tree just by looking at the set of edges E c and the graph constants G c. We don't need to make any extra guesses or assumptions to prove this theorem.",
        "formal_text": "Convert casual text to formal text: In simpler terms, \"described by\" here means we can figure out the AM dependency tree just by looking at the set of edges E c and the graph constants G c. We don"
    },
    {
        "casual_text": "Our aim is to come up with a straightforward and efficient solution that anyone can easily follow. To start, we’ll break down the main text into individual sentences and tweak them by only keeping the important words (after stemming them). We’ll also figure out the average number of words per sentence () and how much that number usually varies (). Based on that, we’ll decide if a sentence is short or long.",
        "formal_text": "Convert casual text to formal text: Our aim is to come up with a straightforward and efficient solution that anyone can easily follow. To start, we’ll break down the main text into individual sentences and tweak them by only keeping the important"
    },
    {
        "casual_text": "When it comes to measuring how related things are, other methods like using the web (Cimiano and Wenderoth, 2007) or looking at how words are connected in sentences (Pustejovsky et al., 1993) might help make the rankings of the found relationships better.",
        "formal_text": "Convert casual text to formal text: When it comes to measuring how related things are, other methods like using the web (Cimiano and Wenderoth, 2007) or looking at how words are connected in sentences (Puste"
    },
    {
        "casual_text": "Like the S2R layer, the R2S layer helps get a sentence-focused representation of relation instances. But there are two main differences: 1) The query vector is vi in T, which represents each relation instance, and 2) the key vector is lj, which represents each sentence. At the end, the R2S layer gives us a matrix T that shows the sentence-focused representation for all possible relation instances. In this matrix, i is the row for the i-th relation instance. We also get a weight matrix W R2S that’s in the shape of R k•(k1)m.",
        "formal_text": "Convert casual text to formal text: Like the S2R layer, the R2S layer helps get a sentence-focused representation of relation instances. But there are two main differences: 1) The query vector is vi in T, which"
    },
    {
        "casual_text": "Besides creating phrase alignments, the annotator also needs to give labels to these alignments. We’ve got four labels, which are based on two things: whether there’s a word order difference or not, and whether there are unaligned function words or not. Here’s what each label means and an example for each, shown in Figure 2: - **REO**: This stands for reordering without any unaligned function words (see Figure 2a). - **UFW**: This one is for alignments with unaligned function words (see Figure 2b). - **REU**: This label is for reordering that also includes unaligned function words (see Figure 2c). - **STD**: This is for structural divergence caused by differences between languages (see Figure 2d). Let’s break it down with the examples: - **Figure 2a**: This shows a reordering of the words under the aligned VP nodes. It’s pretty common to see this kind of word order difference between Chinese and English. In Chinese, the PP modifier comes before the verb, but in English, it comes after. - **Figure 2b**: Here, there’s an unaligned function word—the English infinitive marker \"to,\" which doesn’t have a match in Chinese. - **Figure 2c**: This one has both reordering (like the difference in the order of \"powerhouse\" and \"economy\") and unaligned function words (like \"\" in Chinese and \"of\" in English). - **Figure 2d**: This example shows structural divergence caused by differences between Chinese and English. We’ll talk more about this in Section 4.",
        "formal_text": "Convert casual text to formal text: Besides creating phrase alignments, the annotator also needs to give labels to these alignments. We’ve got four labels, which are based on two things: whether there’s"
    },
    {
        "casual_text": "Okay, so I equals 1 if h(e_i) matches y_i, and I equals 0 if it doesn't. Basically, example-based precision is just 1.",
        "formal_text": "Convert casual text to formal text: Okay, so I equals 1 if h(e_i) matches y_i, and I equals 0 if it doesn't. Basically, example-based"
    },
    {
        "casual_text": "Alright, so we're talking about this thing called InfoNCE (yeah, that's what Oord et al. came up with in 2018). It's basically a way to estimate a lower limit for Mutual Information (MI) and can help us get a rough idea of I(z1; z2). Once we have that, we can tweak it to make it better.",
        "formal_text": "Convert casual text to formal text: Alright, so we're talking about this thing called InfoNCE (yeah, that's what Oord et al. came up with in 2018). It'"
    },
    {
        "casual_text": "Okay, so basically, we're taking version 3 from AllenNLP, tweaking it a bit, and training it again on RAMS using the same setup: adding some special input markers and fine-tuning BERT. If an argument has more than one role label, we just stick them together to create a new class.",
        "formal_text": "Convert casual text to formal text: Okay, so basically, we're taking version 3 from AllenNLP, tweaking it a bit, and training it again on RAMS using the same setup: adding some special input markers and"
    },
    {
        "casual_text": "[Po186] No doubt, figuring out how different things people say are connected and understanding the full structure of conversations that focus on getting stuff done. Grosz and Sidner [GS86] say that a solid way to understand what’s being said is by using something called constraint satisfaction to make sense of the words; basically, when there’s evidence",
        "formal_text": "Convert casual text to formal text: [Po186] No doubt, figuring out how different things people say are connected and understanding the full structure of conversations that focus on getting stuff done. Grosz and Sidner [GS"
    },
    {
        "casual_text": "The annotation guidelines are divided into two main groups. One group focuses on figuring out whether a question-answer pair is neutral or not. Basically, if the question and answer don't match, it's labeled as neutral. In these cases, the answer doesn't properly respond to the question. For instance, E1 is an example where the question is about the screen, but the answer is about the battery.",
        "formal_text": "Convert casual text to formal text: The annotation guidelines are divided into two main groups. One group focuses on figuring out whether a question-answer pair is neutral or not. Basically, if the question and answer don"
    },
    {
        "casual_text": "From Table 2, here's what we can see: 1) On FB15k-237, OTE beats RotatE, and GC-OTE tops all other models across the board. Specifically, MRR jumps from 0.338 in RotatE to 0.361, which is about a 7% boost in performance. OTE, by increasing the sub-embedding dimension from 2 to 20, and the graph context each contribute roughly half of this improvement. 2) On WN18RR, OTE outperforms RotatE, and GC-OTE sets a new record (at least based on what we've seen in published papers). These results highlight how effective OTE and graph context are for predicting missing links in knowledge graphs.",
        "formal_text": "Convert casual text to formal text: From Table 2, here's what we can see: 1) On FB15k-237, OTE beats RotatE, and GC-OTE tops all other models across the"
    },
    {
        "casual_text": "Sure! Here's a more casual version: You know how some lines in a show or movie can make people laugh? Well, instead of focusing on the actor's lines, we're looking at the text itself to figure out what makes the audience crack up. Basically, we're turning this into a \"yes or no\" question about whether a piece of text is funny. And then, for the second part, we're trying to pinpoint exactly which words or phrases in the actor's lines are the ones that trigger the laughter.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: You know how some lines in a show or movie can make people laugh? Well, instead of focusing on the actor's lines, we"
    },
    {
        "casual_text": "Tweets are turned into TF-IDF representations and placed in a vector space that matches their cluster.",
        "formal_text": "Convert casual text to formal text: Tweets are turned into TF-IDF representations and placed in a vector space that matches their cluster. Convert casual text to formal text: Tweets are turned into TF-ID"
    },
    {
        "casual_text": "A big earthquake hit Aceh province in Indonesia on Tuesday. Lots of houses got messed up, and a bunch of villagers got hurt. The word \"damaged\" might sound vague on its own, but when you add the info about when and where it happened, it makes more sense. We tested our system by hiding these kinds of details from the local conversation, and the results are in Table 5.",
        "formal_text": "Convert casual text to formal text: A big earthquake hit Aceh province in Indonesia on Tuesday. Lots of houses got messed up, and a bunch of villagers got hurt. The word \"damaged\" might sound vague on"
    },
    {
        "casual_text": "We tweak our model's hyper-parameters, like the learning rate, embedding dimension (d), and sub-embedding dimension (d_s), using grid search during training. The embedding dimension is basically the number of parameters in each entity's embedding. Each entity's embedding is made up of K sub-embeddings, each with dimension d_s, so d = K  d_s. Our model training has two main steps: first, we train it using either the OTE or RotatE models. Then, we fine-tune it with graph context-based models on top of these pre-trained ones. We pick the parameter settings that give us the best MRR (Mean Reciprocal Rank) and use early stopping on the validation set to decide when to stop. For training, we use the Adam algorithm (Kingma and Ba, 2014).",
        "formal_text": "Convert casual text to formal text: We tweak our model's hyper-parameters, like the learning rate, embedding dimension (d), and sub-embedding dimension (d_s), using grid search during"
    },
    {
        "casual_text": "It's pretty obvious that the system gets way more powerful when you let these structures work together. But this also means there are some tricky things to deal with when putting it into action.",
        "formal_text": "Convert casual text to formal text: It's pretty obvious that the system gets way more powerful when you let these structures work together. But this also means there are some tricky things to deal with when putting it into action. Convert"
    },
    {
        "casual_text": "In 2016, they used an expressive grammar, but it wasn't broad coverage, and the step counts were based on X-bar trees instead of the derivation trees that a provably correct parsing algorithm would need to deal with (Stanojevi and Stabler, 2018). They used a full-throated parser but worked with the Penn Treebank phrase structure without really paying attention to long-distance dependencies. Figure 2 gives an example of one of those dependencies.",
        "formal_text": "Convert casual text to formal text: In 2016, they used an expressive grammar, but it wasn't broad coverage, and the step counts were based on X-bar trees instead of the derivation trees that a prov"
    },
    {
        "casual_text": "(1) H S represents the output vectors from the shared encoder SE, which takes X as its input. The whole point of this shared encoder is to figure out the common information between the gold labels (the real ones) and the auto-generated labels (the ones made by the system). So, during this training stage, the main aim is to reduce the loss, which is basically the difference or error between what the model predicts and what it should predict.",
        "formal_text": "Convert casual text to formal text: (1) H S represents the output vectors from the shared encoder SE, which takes X as its input. The whole point of this shared encoder is to figure out the common information between the gold"
    },
    {
        "casual_text": "We put together a Chinese legal event extraction dataset by manually labeling stuff from a collection of Chinese legal documents using an open-source annotation tool. This dataset has 2380 examples, with 11 different event types, 26 roles for event arguments, and 17 roles between arguments. Seven Master of Laws students helped with the labeling, and it took them a month to finish. For comparison, we picked some top-notch methods: 1. DMCNN (from Yubo et al., 2015) uses a dynamic multi-pooling CNN to grab sentence-level features. 2. DBRNN (by Sha et al., 2018) uses a dependency-bridge RNN to find event triggers and arguments. 3. PLMEE (Yang et al., 2019) uses pre-trained language models for event extraction. To test how well our hierarchical event feature and pedal attention mechanism work, we set up a few models: 1. BERT-base just uses BERT's word representations for trigger extraction and role prediction. 2. JHEE adds our hierarchical event feature on top of BERT-base. 3. PAJEE adds the pedal attention mechanism to BERT-base. 4. PAJHEE combines both the hierarchical event feature and pedal attention mechanism. To keep things fair, all the candidate arguments are generated by our candidate argument extraction module, and we only look at the final results for evaluation.",
        "formal_text": "Convert casual text to formal text: We put together a Chinese legal event extraction dataset by manually labeling stuff from a collection of Chinese legal documents using an open-source annotation tool. This dataset has 2380 examples, with 11 different"
    },
    {
        "casual_text": "• Anti-vaccine: Said they were super unlikely to get the vaccine (14% of people asked). • Pro-vaccine: Had already gotten the shot, or",
        "formal_text": "Convert casual text: • Anti-vaccine: Said they were super unlikely to get the vaccine (14% of people asked). • Pro-vaccine: Had already gotten the shot, or had a friend who had. Con"
    },
    {
        "casual_text": "Basically, our method splits pairs of parallel sentences into two parts, connects the two target parts with their correct translation from the two source parts (whether it's direct or flipped), and then keeps doing this over and over with the new pairs we get.",
        "formal_text": "Convert casual text to formal text: Basically, our method splits pairs of parallel sentences into two parts, connects the two target parts with their correct translation from the two source parts (whether it's direct or flipped), and"
    },
    {
        "casual_text": "We're aiming to find a group of instructions that work together across different recipes. To do this, we turn the graph (which has loops) into a collection of trees by using a maximum spanning tree algorithm. Check out Figure 4 for an example of one of these trees, made for a specific dish. A path in this tree, with no more than one node from each recipe, gives us a set of instructions that align well together. For instance, in the zoomed-in part of the tree in Figure 4, all the differently colored nodes along the path from the yellow node to the green node make up a set of instructions that align nicely across recipes.",
        "formal_text": "Convert casual text to formal text: We're aiming to find a group of instructions that work together across different recipes. To do this, we turn the graph (which has loops) into a collection of trees by using"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way: 1. F.lchild is set to F1, and F.rchild is set to F2. 2. Now, for every possible span (u, v) where u and v are between 0 and the width of F, we do the following: - If the span (u, v) intersects with the width of F1, then: - We look for a split point U somewhere between u and v. - We then use the CHART function to get two things: (FL, uL, vL) for the left part from u to U, and (FR, uR, vR) for the right part from U to v.",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in a simpler way: 1. F.lchild is set to F1, and F.rchild is set to F2. 2. Now, for every possible span"
    },
    {
        "casual_text": "a. There's a huge plant right in front of my house. (5)  There's a big building right in front of my house.",
        "formal_text": "Convert casual text to formal text: a. There's a huge plant right in front of my house. (5)  There's a big building right in front of my house. a. There's"
    },
    {
        "casual_text": "We're testing five tasks: part-of-speech tagging (POS), named entity recognition (NER), dependency parsing (DEP), constituency parsing (CON), and semantic role labeling (SRL). For each of these tasks, we're using a top-of-the-line decoder (except for POS) to set a modern standard for multitask learning on these tasks. We've simplified things to make the model more efficient. For POS, we're using a simple linear layer as the decoder. This layer takes the final embedding of each token from BERT and creates an output vector. Each spot in this vector represents the score for a specific POS tag.",
        "formal_text": "Convert casual text to formal text: We're testing five tasks: part-of-speech tagging (POS), named entity recognition (NER), dependency parsing (DEP), constituency parsing (CON"
    },
    {
        "casual_text": "Some folks have tried tweaking the groundtruth labels by either manually rewriting examples (like Gardner et al., 2020; Qin et al., 2019) or using functions to create small changes (Ribeiro et al., 2020). But doing it manually is a pain—it takes 4-5 minutes per counterfactual (Kaushik et al., 2020), and there’s a risk of missing stuff (like in Figure 1B, where \"kids\" and \"no one\" got skipped). On the other hand, automated tools for analyzing models usually focus on other things, like generating examples where the model’s prediction changes (Ross et al., 2020; Zhang et al., 2019a). This means they miss out on counterfactuals that keep the prediction the same but are still super important for understanding or shaping how the model works, like \"kids\" vs. \"no one\" or \"great\" vs. \"scary\" in Figure 1D. But here’s the thing: counterfactual generation doesn’t have to be tied to a specific task. The same set of counterfactuals in Figure 1 could be useful for all kinds of applications. Plus, for stuff like explaining or analyzing models, having a general pool of counterfactuals might be better because the relationships you’re looking at can be more flexible and user-driven. In this project, we’re setting up the task of counterfactual generation in a more structured way, separating the generation process from how the counterfactuals are used. So, given an input x (like in Figure 1A), our generator creates a bunch of counterfactuals, X = x1, x2, ..., where the relationship between x and xi is application-neutral (like in Figure 1B).",
        "formal_text": "Convert casual text to formal text: Some folks have tried tweaking the groundtruth labels by either manually rewriting examples (like Gardner et al., 2020; Qin et al., 2019) or using"
    },
    {
        "casual_text": "Plus, phonological changes can happen depending on the structure of words or the sounds allowed in a language.",
        "formal_text": "Convert casual text to formal text: Plus, phonological changes can happen depending on the structure words or the sounds allowed in a language. Convert casual text to formal text: Plus, phonological changes can happen depending on the"
    },
    {
        "casual_text": "We’ll explain how we evaluate the model’s results. We had two smart folks rate how formal, fluent, and accurate the outputs were, using a scale from 0 to 2. To keep things fair, we mixed up the order of the sentences randomly while they were working on it.",
        "formal_text": "Convert casual text to formal text: We’ll explain how we evaluate the model’s results. We had two smart folks rate how formal, fluent, and accurate the outputs were, using a scale from 0 to 2."
    },
    {
        "casual_text": "We split the data into a training set, a development set, and a test set. We checked how well the model did on all three by looking at how accurately it could identify the antecedents of third person singular pronouns in the data. You can find the accuracy and size of each part in Table 2. Now, the big question is: are these results good enough? It's tricky to compare our results to the best in the field because we’re only focusing on third person singular pronouns. This means our system doesn’t handle coreference chains, so we can’t really measure its performance the usual way (as Luo, 2005 pointed out). Another thing that’s different from the standard approach is that we don’t have a mention detection module. Instead, we used the gold mention annotations and the singleton mentions we pulled out (check out section 3.2 for more on that). That being said, we still want to get an idea of how well our classifier is doing. The closest study we found is Yang et al. 2004, even though they did use a mention detection module. They trained different systems to handle third person pronoun resolution and reported their accuracy using a success metric. When they tested on the MUC-6 corpus, their metric ranged from 70.0 to 74.7 for the different systems they built. On the MUC-7 corpus, the metric was between 53.8 and 62.5.",
        "formal_text": "Convert casual text to formal text: We split the data into a training set, a development set, and a test set. We checked how well the model did on all three by looking at how accurately it could identify the"
    },
    {
        "casual_text": "The variables s and r, which are part of the Beta distribution we use as a prior for our decision variables, are random in our model. Unfortunately, there isn’t a straightforward conjugate distribution for these parameters, so we can’t solve for them analytically. Instead, we’ll estimate the integral by repeatedly sampling these variables. Since both s and r are positive real numbers, we’ll use a Gamma prior for them, as explained in Section 3.",
        "formal_text": "Convert casual text to formal text: The variables s and r, which are part of the Beta distribution we use as a prior for our decision variables, are random in our model. Unfortunately, there isn’t a"
    },
    {
        "casual_text": "The findings back up our original idea: women bosses use fewer ODPs than men bosses. But here's the thing: among women, there's no real difference between bosses and regular employees. The big gap between bosses and regular workers overall? That's all because of the guys. So, turns out there's a more specific—and way more interesting—hypothesis that holds true: only male bosses use more ODPs than their subordinates.",
        "formal_text": "Convert casual text to formal text: The findings back up our original idea: women bosses use fewer ODPs than men bosses. But here's the thing: among women, there's no real difference between"
    },
    {
        "casual_text": "The rest of this paper goes like this: In Section 2, we talk about some earlier work on guessing the part of speech (POS) for unknown Chinese words. Sections 3, 4, and 5 explain our new approach, which has two models and an extra step connecting them. Specifically, Section 3 treats POS guessing for unknown Chinese words as a sequence-labeling problem and suggests a model that uses internal component features to handle this. Section 4 calculates a credibility score for each guess based on the sequence type of the word's internal structure. This ties together the model from Section 3 and the one in Section 5. Section 5 introduces a model that uses overall context information to adjust the guesses from the first model that have lower credibility scores. Section 6 shows the experiments we did, the results, and how our methods compare to earlier work. Finally, Section 7 wraps things up with our conclusions.",
        "formal_text": "Convert casual text to formal text: The rest of this paper goes like this: In Section 2, we talk about some earlier work on guessing the part of speech (POS) for unknown Chinese words. Sections 3, 4, and 5 explain our"
    },
    {
        "casual_text": "Once the models are set up, they've already picked up some info from the training data we've tweaked. But, since this augmented data has some noise in it—like D trans and D gen—we move on to the relabeling stage. This part uses a mix of strategies: relabeling specific instances, co-training, and re-weighting instances to minimize the noise's effect.",
        "formal_text": "Convert casual text to formal text: Once the models are set up, they've already picked up some info from the training data we've tweaked. But, since this augmented data has some noise in it—like D"
    },
    {
        "casual_text": "But in this situation, no matter if we're talking about location, room, service, or value, a basic model would focus more on words like \"great\", \"ordinary\", \"small\", \"minimum\", and \"expensive\". It would then mistakenly spread the negative feelings from the overall review to all these aspects. So, the sentiment about the location gets wrongly labeled as negative when it should actually be positive.",
        "formal_text": "Convert casual text to formal text: But in this situation, no matter if we're talking about location, room, service, or value, a basic model would focus more on words like \"great\", \"ordinary\", \"small"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. The movement of V-Markers is key to how our algorithm works. First off, a V-Marker starts on the first element of the CSC. When a G-Marker reaches the element with the V-Marker, the V-Marker shifts to the next element in the CSC (check out figure la). After that, we do something called unification to make sure the sentence makes sense grammatically. In figure lb, dl is a closed class lexical item, which is just a fancy way of saying it’s a word or phrase that doesn’t change much. When a G-Marker hits the first element, the V-Marker on that first element moves to the third element, passing through the second element, which is also a closed class item. The cool thing is, the second element doesn’t necessarily need a G-Marker. The algorithm grabs the actual word or phrase (lexical realization) for that element when the V-Marker passes by. Now, if a G-Marker hits an element without a V-Marker, the G-Marker just hangs out there. But if another G-Marker comes along and hits an element with a V-Marker, the V-Marker moves to the next element. If that next element already has a G-Marker, the V-Marker keeps moving to the next one in line (see figure lc). Most of the time, G-Markers work their way up from the bottom to handle the generation process. But sometimes, that’s not enough to figure out the right sentence structure or words to express a meaning. In those cases, the algorithm switches to a top-down approach. This means it starts at each element of the activated CSC and searches downward to find the best sentence structure and words to match the meaning.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. The movement of V-Markers is key to how our algorithm works. First off, a V-Marker starts on"
    },
    {
        "casual_text": "• During testing, we have to compare every n-gram (up to the size of the longest word in our lists) in a sentence with the words in our lists, which can take a lot of time. Chiu and Nichols (2016) used 4 lists with over 2.3 million entries in their work.",
        "formal_text": "Convert casual text to formal text: • During testing, we have to compare every n-gram (up to the size of the longest word in our lists) in a sentence with the words in our lists, which can take"
    },
    {
        "casual_text": "Alright, so each word has these specific features, and we use a bunch of yes-or-no variables to represent them.",
        "formal_text": "Convert casual text to formal text: Alright, so each word has these specific features, and we use a bunch of yes-or-no variables representing. Convert casual text to formal text. Convert casual text to formal"
    },
    {
        "casual_text": "In this paper, we dive into two main topics about classifying troll farms. First, we check out how three different types of features affect how well our classifier works. We're talking about content, behavioral, and stylistic features. It makes sense that troll farms pushing a certain political agenda would have common words or phrases (like #fakenews). But we also think that how they write could help improve how well we can predict things. If we assume that some people at these troll farms manage multiple accounts, even if the topics they talk about are different, their writing style should still be pretty similar. So, we're guessing that features that work for figuring out who wrote something (like Sari et al., 2018 did) could also be useful for identifying troll farms.",
        "formal_text": "Convert casual text to formal text: In this paper, we dive into two main topics about classifying troll farms. First, we check out how three different types of features affect how well our classifier works. We're talking"
    },
    {
        "casual_text": "The basic word translation model we're using isn't really tailored to our specific test set, so we're aiming to create a more specialized model just for this test set. We'll mix this new model with the general one to improve how words are translated. We're pulling bilingual phrase pairs from XML files that our systems made. Using IBM Model-1, we figure out which words in these phrases match up with each other. Since the phrases are usually short, these word alignments tend to be pretty accurate. We gather all these word alignment counts from the entire test set and use them to calculate the chances of a word in one language being translated to another. This gives us a test set-specific translation model, which we then blend with the general model to get better results.",
        "formal_text": "Convert casual text to formal text: The basic word translation model we're using isn't really tailored to our specific test set, so we're aiming to create a more specialized model just for this test set."
    },
    {
        "casual_text": "Our method works by projecting the target language's word embeddings into the source language's embedding space while keeping the word order intact. This means it tends to work better when the target language has a similar word order to the source language. We tested this out in our experiments, and it turned out to be true. The source language we used is English, which is part of the SVO family—that’s where the subject comes first, then the verb, and finally the object. Languages like Spanish, Italian, Portuguese, German, and Chinese also follow this SVO structure. For these languages, our method got over 70% accuracy, which is pretty good. However, things didn’t go as well for languages with different word orders. Japanese is an SOV language, meaning the subject comes first, then the object, and finally the verb. Arabic is a VSO language, where the verb comes first, followed by the subject and then the object. For these languages, our method didn’t perform as well, with lower accuracy rates.",
        "formal_text": "Convert casual text to formal text: Our method works by projecting the target language's word embeddings into the source language's embedding space while keeping the word order intact. This means it tends to work better"
    },
    {
        "casual_text": "Cook and Stevenson (2009) came up with a way to manually spot different types of word formations in a noisy channel setup. They gave each type a few simple numbers to describe it, making sure all possible transformations of that type were equally likely. They then used expectation maximization to figure out the best values for these numbers. This method is different from most other unsupervised models, which tend to work in a step-by-step process. Contractor et al. (2010) took a different approach. They used string edit distance to find similar-looking word options and then used a language model to decode the message. Gouws et al. (2011) improved on this by creating a special dictionary of closely related word pairs, like \"you\" and \"u\". We use string edit distance like Contractor et al. (2010) and also look for strongly related word pairs like Gouws et al. (2011). But instead of just using these as filters in a step-by-step process, we add them as features to a single, combined log-linear model. More recent methods have tried to boost accuracy by using more external resources and more complex systems. Han and Baldwin (2011) started with string similarity measurements and then used dependency parsing to find words that fit the context. Liu et al. (2011) gathered noisy training pairs from Google search snippets using carefully designed queries. They then trained a conditional random field (Lafferty et al., 2001) to create a character-based translation model.",
        "formal_text": "Convert casual text to formal text: Cook and Stevenson (2009) came up with a way to manually spot different types of word formations in a noisy channel setup. They gave each type a few simple numbers to describe it,"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms: **Emotion Distribution:** An emotion distribution, called e_t, is like a list of numbers (a vector) with N values that add up to 1. This list represents how someone (like a user or a group) feels at a specific time, t. For example, it could show how much someone is feeling happy, sad, angry, etc. at that moment. At a certain time, t, we figure out a user's emotion distribution (e_t(u)) by looking at all the posts they made during that time and combining (or aggregating) the emotions from those posts. Similarly, for a group or community, we do the same thing but look at the emotions of all the users in that group at that time. **Community Emotion Burst:** An emotion burst happens when something important or exciting happens to a community. This is a time period, [t_s, t_e], where the emotions in the community are either: 1) Way different from how they usually feel on average, or 2) The emotions are all over the place, not evenly spread out. In short, it's like when a group of people suddenly starts feeling really intense emotions because something big happened.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms: **Emotion Distribution:** An emotion distribution, called e_t, is like a list of numbers (a vector) with"
    },
    {
        "casual_text": "Any setup created using LTF or LTL can be finished up to reach a goal configuration.",
        "formal_text": "Convert casual text to formal text: Any setup created using LTF or LTL can be finished up to reach a goal configuration. Convert casual text to formal text: Any setup created using LTF or LTL can be finished up"
    },
    {
        "casual_text": "The weighted hidden state for aspect k is called  kl h l. Then, we use r s k to predict the sentiment polarity, which we call .",
        "formal_text": "Convert casual text to formal text: The weighted hidden state for aspect k is called  kl h l. Then, we use r s k to predict the sentiment polarity,"
    },
    {
        "casual_text": "Since we worked with datasets from various fields, we fine-tuned multilingual BERT (from Devlin et al., 2019) on the target side of the parallel corpora to create the MLMs we used.",
        "formal_text": "Convert casual text to formal text: Since we worked with datasets from various fields, we fine-tuned multilingual BERT (from Devlin et al., 2019) on the target side of the parallel"
    },
    {
        "casual_text": "A compositional distributional model is all about creating a vector that captures the meaning of a whole phrase or sentence by mixing together the vectors of the individual words. The easiest way to do this is by using basic math operations on the word vectors (Mitchell and Lapata, 2010). Basically, the vector for a bunch of words w1, ..., wn is made like this:",
        "formal_text": "Convert casual text to formal text: A compositional distributional model is all about creating a vector that captures the meaning of a whole phrase or sentence by mixing together the vectors of the individual words. The easiest way to do"
    },
    {
        "casual_text": "In this project, we show that using the usual method of training with maximum likelihood on a big, mixed-up dataset doesn't really work for this purpose. Imagine a big pile of training data (black area) mostly made up of news articles (red) and just a few restaurant reviews (blue). The standard model (gray) just follows the overall data and doesn't pay much attention to the reviews, so it doesn't do well with them. A better model should try to give equal importance to all kinds of topics, so it can handle whatever comes up during testing.",
        "formal_text": "Convert casual text to formal text: In this project, we show that using the usual method of training with maximum likelihood on a big, mixed-up dataset doesn't really work for this purpose. Imagine a big pile of training"
    },
    {
        "casual_text": "There are plenty of nodes where comparing AIs shows that moving a part of the tree is possible, but the way the child nodes are split up isn't the same. Figure 3 shows this. The AI strings for the main nodes in the trees have the same stuff, but how they're divided or the order they appear in isn't matching. These parts of the tree can't be used directly for pattern finding, so we need to dig deeper into the analysis.",
        "formal_text": "Convert casual text to formal text: There are plenty of nodes where comparing AIs shows that moving a part of the tree is possible, but the way the child nodes are split up isn't the same. Figure"
    },
    {
        "casual_text": "We tested our system using the MT evaluation corpora from the 2011 ACL WMT 3 conference. We looked at eight corpora, focusing on English-to-other (like Spanish, German, French, and Czech) and other-to-English news text. Following the usual approach (for example, TER was introduced by comparing it with BLEU and Meteor, AMBER was compared with BLEU and Meteor-1.0, and MP4IBM1 was compared with BLEU), we compared the scoring results of LEPOR against three widely recognized metrics: BLEU (Papineni et al., 2002), TER (Snover et al., 2006), and Meteor (version 1.3) (Denkowski and Lavie, 2011). Additionally, we included the latest AMBER (a modified version of BLEU) (Chen and Kuhn, 2011) and MP4IBM1 (without reference translation) (Popovic et al., 2011) to see how LEPOR stacks up in this study. The correlation results are summarized in Table 1. The metrics are ranked based on their average performance across the eight corpora, from best to worst. Table 1 shows that LEPOR-A and LEPOR-B scored the highest among all the metrics, with LEPOR-B coming out on top in terms of mean scores. BLEU, AMBER (the modified version of BLEU), and Meteor-1.3 showed inconsistent performance—better on some translation languages and worse on others—leading to a medium overall level. TER and MP4IBM1 ended up with the lowest mean correlation scores.",
        "formal_text": "Convert casual text to formal text: We tested our system using the MT evaluation corpora from the 2011 ACL WMT 3 conference. We looked at eight corpora, focusing on English-to-other (like Spanish"
    },
    {
        "casual_text": "Alright, so in step-2, everything involves dense matrix-matrix products, which are perfect for libraries that are optimized for GPUs and TPUs. The total number of super- and sub-diagonal blocks is limited—it won’t exceed twice the number of blocks at the starting level, which is 2N. Now, for step-1 (coarsening) and step-3 (interpolation), they both use sparse matrices with consistent sparsity patterns. So, matrices P(l) and R(l) aren’t actually built explicitly—you can apply them using standard library functions without any hassle. For instance, if you’re using the Jax Numpy library, coarsening can be handled by summing along the row axis, and interpolation can be done by repeating along the row axis. That’s why step-1 and step-3 also only involve dense matrix operations.",
        "formal_text": "Convert casual text to formal text: Alright, so in step-2, everything involves dense matrix-matrix products, which are perfect for libraries that are optimized for GPUs and TPUs. The total number of super- and"
    },
    {
        "casual_text": "One big challenge with expanding document sets, especially when you think about it as an imbalanced classification problem, is that the positive examples are super rare and there's hardly any labeled data for them. Sure, optimizing with BER helps with the imbalance, but we're still stuck with barely any labeled data. So, we decided to look into pretraining as a possible solution. We used SciBERT (from Beltagy et al., 2019) to get pretrained contextual embeddings, specifically for the PubMed domain. If a PubMed abstract is longer than SciBERT's 512 word-piece limit, we use a sliding-window method to average the embeddings for any word-piece that shows up in multiple windows.",
        "formal_text": "Convert casual text to formal text: One big challenge with expanding document sets, especially when you think about it as an imbalanced classification problem, is that the positive examples are super rare and there's hardly any labeled data for"
    },
    {
        "casual_text": "Step 4: Grouping. Alright, so now that we've split the dataset into smaller chunks across three different areas—time, within a project, and across projects—we need to group the right pieces together to create the training (Train), validation (Val), and standard test (TestS) sets for each method. You can see this process illustrated in the bottom left part of Figure 4.",
        "formal_text": "Convert casual text to formal text: Step 4: Grouping. Alright, so now that we've split the dataset into smaller chunks across three different areas—time, within a project, and across projects—we need to"
    },
    {
        "casual_text": "In this paper, we take a look at and expand some transfer learning methods for identifying new entities. We use both the usual datasets and some new ones that haven’t been used for this before. As far as we know, this is the first time these transfer learning methods have been directly compared. We also run experiments on seven new pairs of source and target domain corpora, which almost doubles the total number of pairs studied in all previous work. Plus, we’re sharing our code so that others can check our results and use our benchmarks to compare their own transfer learning techniques in the future.",
        "formal_text": "Convert casual text to formal text: In this paper, we take a look at and expand some transfer learning methods for identifying new entities. We use both the usual datasets and some new ones that haven’t been used for"
    },
    {
        "casual_text": "Our approach to improving the reading step by adding more detailed descriptions of relationships is kind of like what Weissenborn et al. (2017) and Mihaylov and Frank (2018) did. They both added background commonsense knowledge to reading comprehension systems. They used structured knowledge bases to pull out info about the semantic relationships between entities. In our case, instead of that, we grab text snippets that mention pairs of entities and turn them into vector representations to show the relationships between those entities.",
        "formal_text": "Convert casual text to formal text: Our approach to improving the reading step by adding more detailed descriptions of relationships is kind of like what Weissenborn et al. (2017) and Mihaylov and Frank (2018) did. They both added"
    },
    {
        "casual_text": "FIXED-OURS is a bit slower compared to Fairseq's version. But here's the thing: even though both systems get pretty close BLEU scores on the development set, FIXED-OURS actually does better on the test set. It scores 49.75 vs 49.57 for De-En and 39.19 vs 38.98 for Ru-En. You can check out Table 3 for more details on the De-En experiment.",
        "formal_text": "Convert casual text to formal text: FIXED-OURS is a bit slower compared to Fairseq's version. But here's the thing: even though both systems get pretty close BLEU scores"
    },
    {
        "casual_text": "The approach we're talking about here builds on statistical log-linear models, which were introduced by Berger et al. in 1996 and Papineni et al. in 1998. In a typical log-linear model used for machine translation, you have a bunch of feature functions that each evaluate how good a translation is based on different aspects. Almost every Statistical Machine Translation (SMT) system includes two main feature functions: a translation model (TM) and a target language model (LM). The TM checks how accurate the translation is, while the LM looks at how smooth and natural the translated text sounds. Both of these are based on probability distributions, which are calculated using maximum-likelihood estimation from training data. To find the best translation, you maximize a weighted sum of these feature functions.",
        "formal_text": "Convert casual text to formal text: The approach we're talking about here builds on statistical log-linear models, which were introduced by Berger et al. in 1996 and Papineni et al. in"
    },
    {
        "casual_text": "• Over Copying: Basically, two dags are made to create a new one. This usually happens when copies of two input dags are created before combining them into a single new dag through a destructive unification operation.",
        "formal_text": "• Over Copying: Basically, two dags are made to create a new one. This usually happens when copies of two input dags are created before combining them into a single new dag through a destructive"
    },
    {
        "casual_text": "The hyponymy tree structure lets you explore the whole thing by expanding or hiding parts of it. While this classification method keeps each section neat and easy to check out, we need something better to handle over 88,000 pieces of data in E-HowNet. So, we’ve added a search feature that lets you look up lexical senses in two different ways:",
        "formal_text": "Convert casual text to formal text: The hyponymy tree structure lets you explore the whole thing by expanding or hiding parts of it. While this classification method keeps each section neat and easy to check out, we need something better to handle"
    },
    {
        "casual_text": "Sure! Here's the informal version: \"Check out this link for the translation task details: http://www.statmt.org/wmt17/translation-task.html. And here's the GitHub page for Electra by Google Research: https://github.com/google-research/electra.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: \"Check out this link for the translation task details: http://www.statmt.org/wmt17/translation-task.html"
    },
    {
        "casual_text": "The faithfulness assumption basically says that if the global Markov assumption holds, then all the independence relationships in the distribution are shown by d-separations in the graph. In simpler terms, it means that if two things are independent given some other stuff, you'll see that reflected in how the graph is connected.",
        "formal_text": "Convert casual text to formal text: The faithfulness assumption basically says that if the global Markov assumption holds, then all the independence relationships in the distribution are shown by d-separations in the graph. In simpler terms"
    },
    {
        "casual_text": "Maximum entropy (ME) models, sometimes called log-linear or exponential learning models, are a versatile machine learning method for tasks like classification and prediction. They've been pretty successful in natural language processing, such as part of speech tagging and named entity recognition. These models can handle features from all sorts of different sources to help with classification. Each feature acts like a rule for the model. For example, in question classification, a feature might be a specific word that’s linked to a certain type of question. The maximum entropy model is the one that has the highest entropy while still following all the rules. In our work, we’re using the Stanford Maximum Entropy implementation, as described by Manning and Klein in 2003.",
        "formal_text": "Convert casual text to formal text: Maximum entropy (ME) models, sometimes called log-linear or exponential learning models, are a versatile machine learning method for tasks like classification and prediction. They've been pretty successful"
    },
    {
        "casual_text": "In 1986, she started up Hamilton Technologies, Inc. in Cambridge, Massachusetts, and became the CEO.",
        "formal_text": "Convert casual text to formal text: In 1986, she started up Hamilton Technologies, Inc. in Cambridge, Massachusetts, and became the CEO. Convert casual text to formal text: In 1986, she started up Hamilton Technologies, Inc. in"
    },
    {
        "casual_text": "Alright, so he simplified the analysis of a sentence to include stuff that's useful when parsing a whole text, like things related to the topic or knowledge from a database tied to the lexicon. Since I/G is a type of unification grammar, it lets you build patterns that can tweak or adjust these discourse features in later sentences. For example, info from one sentence can guide the search in the lexicon by making sure certain features match up. Basically, we're hoping that having all these description tools work in a similar way makes it easier for other parts of the system to expand or tweak them. But yeah, that's kind of looking ahead.",
        "formal_text": "Convert casual text to formal text: Alright, so he simplified the analysis of a sentence to include stuff that's useful when parsing a whole text, like things related to the topic or knowledge from a database"
    },
    {
        "casual_text": "(iii) We created six extra sentences for each of S, S+, and S by throwing in phrases like \"less than,\" \"no more than,\" \"exactly,\" \"at least,\" \"no less than,\" and \"more than.\" This gave us a total of 21 sentences with the right answers marked. Table 3 shows some of these sentences.",
        "formal_text": "Convert casual text to formal text: (iii) We created six extra sentences for each of S, S+, and S by throwing in phrases like \"less than,\" \"no more than,\" \"exactly,\""
    },
    {
        "casual_text": "Alright, so to make a prediction with some context, we take the context-encoded embedding vectors of the input pair, x i, and feed them into a fully-connected layer that uses a softmax activation function.",
        "formal_text": "Convert casual text to formal text: Alright, so to make a prediction with some context, we take the context-encoded embedding vectors of the input pair, x i, and feed into a"
    },
    {
        "casual_text": "So, like Mohammad and Hirst said back in 2006, the Distributional Profile (DP) of a word, let's say \"u,\" kind of bundles up all the different meanings it can have. Take the word \"bank,\" for instance. It can mean either \"river bank\" or \"financial institution.\" Now, imagine \"bank\" in the \"financial institution\" sense shows up 100 times with the word \"money\" in a big collection of texts. On the other hand, when \"bank\" means \"river bank,\" it pops up 80 times with the word \"boat.\" So, the DP for \"bank\" will keep track of both its connections with \"money\" and \"boat\":",
        "formal_text": "Convert casual text to formal text: So, like Mohammad and Hirst said back in 2006, the Distributional Profile (DP) of a word, let's say \"u,\" kind of bundles up all"
    },
    {
        "casual_text": "We've come up with a bunch of predictors to help figure out family expansion, and they're all based on measuring certain properties of w *. These predictors are inspired by research in psycholinguistics and NLP. They can be grouped into three main categories: (i) a type frequency-based predictor (|F |), (ii) token frequency-based predictors (f r, z (p) ), and",
        "formal_text": "Convert casual text to formal text: We've come up with a bunch of predictors to help figure out family expansion, and they're all based on measuring certain properties of w *. These predictors are"
    },
    {
        "casual_text": "We're using the Bing API (http://www.bing.com/developers/) to grab sentences. In about 7 out of 9 cases, a single pair of entities in YAGO has more than one relationship. For instance, some people have both ACTEDIN and PRODUCED a movie.",
        "formal_text": "Convert casual text to formal text: We're using the Bing API (http://www.bing.com/developers/) to grab sentences. In about 7 out of 9 cases, a single pair of entities in"
    },
    {
        "casual_text": "Alright, let's keep the generator G fixed and update the discriminators DDP and DKB by tweaking them to minimize the stuff in Equation (9) using gradient descent.",
        "formal_text": "Convert casual text to formal text: Alright, let's keep the generator G fixed and update the discriminators DDP and DKB by tweaking them to minimize the stuff in Equation (9) using gradient descent. Convert casual"
    },
    {
        "casual_text": "You can grab the word embedding initialization from this link: https://github.com/npow/ubottu.",
        "formal_text": "Convert casual text to formal text: You can grab the word embedding initialization from this link: https://github.com/npow/ubottu."
    },
    {
        "casual_text": "3. Word order isn't just about making sentences flow smoothly; it also carries important compositional information. When we test our probes on sentences where the words are shuffled locally—so they still make sense close-up but lose their overall structure—we see that the syntactic information gets messed up (check out Figure 3).",
        "formal_text": "Word order isn't just about making sentences flow smoothly; it also carries important compositional information. When we test our probes on sentences where the words are shuffled locally—so they still make sense close-up"
    },
    {
        "casual_text": "We’ve come up with a new way to do cross-lingual topic modeling that mixes the good parts of explicit and latent topic models. We’ve shown how it works for matching documents across languages. In particular, we’ve found that this method works better than some popular topic models like Explicit Semantic Analysis (ESA), Latent Semantic Indexing (LSI), and Latent Dirichlet Allocation (LDA). Plus, it beats a basic approach where you just translate words from one language to another. Another cool thing is that setting up this model takes less time than training a machine translation system using a parallel corpus. We’ve also introduced a handy shortcut called L-Solve, which makes the whole process way faster by cutting down on the heavy calculations needed for these topic models.",
        "formal_text": "Convert casual text to formal text: We’ve come up with a new way to do cross-lingual topic modeling that mixes the good parts of explicit and latent topic models. We’ve shown how it works for matching documents across"
    },
    {
        "casual_text": "Early results showed that using cosine similarity with a linear kernel barely did better than just returning everything as a baseline. So, we decided to try out some kernel tricks (shoutout to Cristianini and Shawe-Taylor, 2000) and it actually worked out well. The Gaussian kernel is usually the first one people recommend to test as a starting point (thanks to Schölkopf et al. in 1995 and Joachims in 1998 for that tip). It’s based on something called a radial basis function (RBF) and only cares about how far apart things are. The Laplacian kernel is kind of like the Gaussian one but uses a different way of measuring distance—L1 instead of L2. Both of these kernels are part of the RBF family and have this one parameter, , that we had to tweak. We compared them to cosine similarity to see how they stacked up.",
        "formal_text": "Convert casual text to formal text: Early results showed that using cosine similarity with a linear kernel barely did better than just returning everything as a baseline. So, we decided to try out some kernel tricks (shout"
    },
    {
        "casual_text": "Evaluation Metric. To figure out how well the classification works, we use different methods for each part of the task. For Task A, we go with the macro-averaged F1 score. For Task B, we also use the Exact Match Ratio (EMR) along with the macro-averaged F1 score (Basile et al., 2019).",
        "formal_text": "Convert casual text to formal text: Evaluation Metric. To figure out how well the classification works, we use different methods for each part of the task. For Task A, we go with the macro-averaged F1 score. For"
    },
    {
        "casual_text": "Instead of simplifying things by cutting out ideas, you could try using an approximation if the results are good enough. Like in the studies by Takamura and Okumura (2009) and Lin and Bilmes (2010), we’re using the greedy method from Khuller et al. (1999) to tackle the budgeted maximum coverage problem. This method gives us a performance guarantee of about 1/2 * (1 - 1/e). Table 4 shows how the model, which balances effectiveness and speed best (when DF is 3 or more), compares to the greedy approximation without any pruning.",
        "formal_text": "Convert casual text to formal text: Instead of simplifying things by cutting out ideas, you could try using an approximation if the results are good enough. Like in the studies by Takamura and Okumura (2009)"
    },
    {
        "casual_text": "The summarization model needs to find the exact parts of the document that relate to the aspect we're looking for. This can be tricky, especially when training with limited or messy guidance. Our method makes things easier by giving the model a head start with pre-calculated connections between the document and the aspect.",
        "formal_text": "Convert casual text to formal text: The summarization model needs to find the exact parts of the document that relate to the aspect we're looking for. This can be tricky, especially when training with limited or messy guidance. Our method"
    },
    {
        "casual_text": "2) In \"He paid his bill,\" the person (the subject) is giving up something valuable, like money. The \"bill\" (the object) means there's a specific amount owed, and that amount is supposed to go to some faraway person or thing (that we could figure out if we wanted to) as the one who should get it.",
        "formal_text": "Convert casual text to formal text: 2) In \"He paid his bill,\" the person (the subject) is giving up something valuable, like money. The \"bill\" (the object) means there's a specific amount"
    },
    {
        "casual_text": "The stuff I just mentioned also applies to another type of reduction called strong reduction or 11-reduction, but I don't have room to explain it right now. You can check out Morrill et al. (1990) for more details.",
        "formal_text": "Convert casual text to formal text: The stuff I just mentioned also applies to another type of reduction called strong reduction or 11-reduction, but I don't have room to explain it right now. You can check out Morrill"
    },
    {
        "casual_text": "Training an RNNLM by focusing on minimizing perplexity (PPL) doesn’t always mean it will give a better score to the correct \"gold\" reference compared to a worse hypothesis. This is kind of annoying. Check out Tab. 1 for an example. The reference comes from the text labels of the dev93' set from the Wall Street Journal (WSJ) dataset. The hypothesis was generated by a CTC-based (Graves et al., 2006) ASR system trained on the WSJ training set. The words in red are the mistakes made by the hypothesis. Then, we trained an RNNLM on the Common Crawl corpus by minimizing PPL. The training setup was pretty standard (Jozefowicz et al., 2016) with a vocabulary of the 400K most common words. Any word not in the vocabulary gets replaced with an UNK token. After training, we used this RNNLM to score the sentences. The weird thing is, the LM-score for the incorrect hypothesis was higher than the score for the correct reference. This actually makes sense because \"a decade as concerns\" seems to be a more common phrase. In the training corpus, we found that \"a decade as concerns\" shows up once, but \"its defeat is confirmed\" doesn’t appear at all. Plus, \"a decade as\" appears 2,280 times, while \"its defeat is\" only shows up 24 times. But here’s the problem: if there’s another hypothesis that matches the reference exactly, it won’t be ranked as the best candidate. That’s not ideal.",
        "formal_text": "Convert casual text to formal text: Training an RNNLM by focusing on minimizing perplexity (PPL) doesn’t always mean it will give a better score to the correct \"gold\" reference compared to"
    },
    {
        "casual_text": "This dip in performance could be due to translation mistakes, shifts in writing style, or issues caused by the machine translation systems, which sometimes mess things up between the training and test data.",
        "formal_text": "Convert casual text to formal text: This dip in performance could be due to translation mistakes, shifts in writing style, or issues caused by the machine translation systems, which sometimes mess things things things between the training and test data. Convert"
    },
    {
        "casual_text": "The entity argument includes these six types of entities: party, item, time, location, organization, term, criminal charges, and behavior.",
        "formal_text": "Convert casual text to formal text: The entity argument includes these six types of entities: party, item, time, location, organization, term, criminal charges, and behavior. Convert casual text to formal text: The entity argument includes these"
    },
    {
        "casual_text": "Alright, let’s talk about models that work with text by looking at the characters that make it up. This includes our CHARAGRAM model and a few others we use as comparisons. We’ll represent a text sequence made of characters as x = x1, x2, ..., xm. This includes spaces between words and special markers for the start and end of the sequence. For any part of the sequence, we’ll use xji to refer to the characters from position i to position j, so xji = xi, xi+1, ..., xj. If i and j are the same, then xji is just the single character xi. Our CHARAGRAM model takes a sequence of characters, x, and turns it into a vector by combining the vectors of its character n-grams (like small chunks of characters) and then applying a nonlinearity to each element.",
        "formal_text": "Convert casual text to formal text: Alright, let’s talk about models that work with text by looking at the characters that make it up. This includes our CHARAGRAM model and a few others we use as comparisons"
    },
    {
        "casual_text": "So, the output event structure usually isn't very detailed. Adding an inference part along with some background knowledge should help narrow down the options for arranging events in a straight line. Depending on the Aktionsart, the events in the DRS can be broken down into smaller parts that mark the start and end of the main event, like Moens and Steedman suggested back in 1986 (also see Eberle, 1988b). These smaller parts don't really have a time span of their own. On this smaller scale, \"before\" and \"equivalent\" pretty much cover the basic ways events can be related in time. With this more detailed breakdown, the event substructure in a DRS matches up with a unique interval structure, as long as we use Allen's interval structures to define \"before\" and \"overlap\" (check out Allen, 1983). This means we can use systems that work with intervals, like Allen's, but also ones that deal with point-like events, like the event calculus from Kowalski and Sergot in 1985. Plus, we can handle temporal relationships with different levels of precision, which is pretty handy.",
        "formal_text": "Convert casual text to formal text: So, the output event structure usually isn't very detailed. Adding an inference part along with some background knowledge should help narrow down the options for arranging events in a straight line."
    },
    {
        "casual_text": "Alright, here's the deal: we're sharing all the settings and training info so you can get the same results we did. The setup uses an encoder-decoder model with LSTM units. We start by using Glove embeddings for the word lookup matrix, and then we train it along with everything else to make it fit the specific problem better. Turns out, this part is super important for better performance. If we just used random starting points or kept the Glove embeddings fixed, the results weren't as good.",
        "formal_text": "Convert casual text to formal text: Alright, here's the deal: we're sharing all the settings and training info so you can get the same results we did. The setup uses an encoder-decoder model with"
    },
    {
        "casual_text": "We also tweaked the sequence generation setup to work with the training pairs we’re proposing. In each layer, the attention module figures out how much weight to give to the token representations from the encoder, based on what’s already been generated in the decoder, and then spits out the context c_h. To factor in the future conversation X_f, we added another encoder to create contextualized token representations of X_f, which the attention module then pulls out as the context c_f. This new encoder uses the same parameters as the original one. The attention module’s output is basically the combination of the past context c_h and the future context c_f. Lastly, the training criterion we’re using is the negative log-likelihood, like this:",
        "formal_text": "Convert casual text to formal text: We also tweaked the sequence generation setup to work with the training pairs we’re proposing. In each layer, the attention module figures out how much weight to give to the token representations from"
    },
    {
        "casual_text": "Compared to the usual decoding approach (like the one by Zens and Ney in 2008), the method we're suggesting can skip a bunch of complicated calculations, such as figuring out rest costs and dealing with reorderings during decoding. This is because each path in the reordering lattice already has all the info about word order. So, our method ends up making the decoding process simpler than the old way.",
        "formal_text": "Convert casual text to formal text: Compared to the usual decoding approach (like the one by Zens and Ney in 2008), the method we're suggesting can skip a bunch of complicated calculations, such as figuring out"
    },
    {
        "casual_text": "The Shapiro-Wilk and Anderson-Darling tests both showed that the PHEME data is normally distributed. Most of the Intruded and Chained clusters are kind of in the middle when it comes to coherence, so their score ranges are pretty similar. That's why we decided to group them together.",
        "formal_text": "Convert casual text to formal text: The Shapiro-Wilk and Anderson-Darling tests both showed that the PHEME data is normally distributed. Most of the Intruded and Chained clusters are"
    },
    {
        "casual_text": "We took a look at how Quantity Cell Graph and Quantity Comparison Graph work in our model. The results from our ablation study are in Table 3. It turns out that using both graphs in Graph2Tree gives the best performance. Even using just one of the graphs still works better than not using either one (which is like a fully-connected graph). Cool thing is, adding either graph to the quantity representation beats the baseline GTS model for this task, showing how important it is to represent quantities well in MWP tasks. From this, we think that improving how we represent quantities, understanding the relationships between them, and keeping their numerical values intact all help improve results for MWP tasks. Oh, and if we combine both types of graphs into one big graph, the performance goes down. We guess that might be because combining them adds some extra noise.",
        "formal_text": "Convert casual text to formal text: We took a look at how Quantity Cell Graph and Quantity Comparison Graph work in our model. The results from our ablation study are in Table 3. It turns out that using both graph"
    },
    {
        "casual_text": "From what [Keller87] says, there are two ways to deal with the issues mentioned in sections 2.2 and 2.3. One option is to make sure all the local trees are LP-consistent and all the categories are legal after you've built the whole tree for the input string. The other option is to tweak the grammar format and/or the FIPs to limit the problems.",
        "formal_text": "Convert casual text to formal text: From what [Keller87] says, there are two ways to deal with the issues mentioned in sections 2.2 and 2.3. One option is to make sure all the local trees are LP-"
    },
    {
        "casual_text": "To really understand how our method works, we looked at how our model makes predictions. We picked 100 random samples from the test set of RR-submission and checked the results ourselves. Turns out, there are two main reasons why it makes mistakes.",
        "formal_text": "Convert casual text to formal text: To really understand how our method works, we looked at how our model makes predictions. We picked 100 random samples from the test set of RR-submission and checked the results ourselves. Turns out"
    },
    {
        "casual_text": "In Section 2, they talked about how earlier attempts to check if machines could match human performance didn’t go so well because the tests weren’t set up properly. So, in our new evaluation, we’re doing things differently. First, we’re running a statistical power analysis. This means we’re making sure that if there’s a tie between a system and a human (or even between two systems), the tests we use to decide who’s better will actually be strong enough to avoid mistakes. A Type II error, for example, would be saying a system is as good as a human when it’s not. This is super important, especially when you’re looking at whole documents instead of just sentences. Why? Because rating entire documents takes way more time, which usually means fewer documents get rated in the end. For example, Läubli et al. (2018) only used 55 documents when they re-evaluated Hassan et al. (2018). That’s a big deal when you compare it to the usual practice of evaluating 1,500 segments in standard machine translation tests. In Läubli’s case, that’s a crazy 96% drop in sample size. Since the whole point of this research is to see if machines can tie with humans, having such a tiny sample size is a huge risk. It could seriously mess up the results because the tests wouldn’t be strong enough to tell the difference. So, we’re being extra careful to avoid that.",
        "formal_text": "Convert casual text to formal text: In Section 2, they talked about how earlier attempts to check if machines could match human performance didn’t go so well because the tests weren’t set up properly. So, in our new evaluation,"
    },
    {
        "casual_text": "Sure! Here's a more casual version: 2. All the languages in the LM family share a few basic things: the unary operator \"-\", the binary operator \".\", and some logical words like \"all\" and \"some\". Plus, they have the usual logical connectives.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: 2. All the languages in the LM family share a few basic things: the unary operator \"-\", the binary operator \".\", and"
    },
    {
        "casual_text": "We use these representations to figure out how similar each part of the question is to each part of the document. This is done by creating a matrix S  R mn that shows the similarities between the tokens (words or pieces) in the question and the document. We do this using a tri-linear function, as described by Seo and others in 2017.",
        "formal_text": "Convert casual text to formal text: We use these representations to figure out how similar each part of the question is to each part of the document. This is done by creating a matrix S  R mn that shows"
    },
    {
        "casual_text": "The dataset has a couple of cases where the answer is marked as uncertain. However, de Marneffe et al. (2010) didn't include those in their results, and we're following their lead by leaving them out too.",
        "formal_text": "Convert casual text to formal text: The dataset has a couple of cases where the answer is marked as uncertain. However, de Marneffe et al. (2010) didn't include those in their results, and we'"
    },
    {
        "casual_text": "GPT-2 is a language model that's built using something called Transformer blocks (Vaswani et al., 2017), and it has L layers in total. We chose GPT-2 for our study because exposure bias is a common issue in neural text generation models, and we wanted to use it as an example. When we input a sequence of tokens y t  t=1, 2, •••, T 1, which we call the conditioned passage, the model spits out states from each layer.",
        "formal_text": "Convert casual text to formal text: GPT-2 is a language model that's built using something called Transformer blocks (Vaswani et al., 2017), and it has L layers in total. We chose"
    },
    {
        "casual_text": "To fill in D 1, we figure out the entropy for a term i by looking at how many documents it shows up in and how many it doesn’t, treating these as probabilities. For the joint entropy H(I, J), we calculate four probabilities based on all the possible scenarios: documents where both terms appear, ones where I is there but not J, ones where J is there but not I, and so on. Using 1 in our tests has actually worked better for the applications we’ve tried.",
        "formal_text": "Convert casual text to formal text: To fill in D 1, we figure out the entropy for a term i by looking at how many documents it shows up in and how many it doesn’t, treating these as probabil"
    },
    {
        "casual_text": "To check out how well this method works for finding a good model setup, we tried it on the task of noun phrase chunking. We used a Linear Chain CRF as our starting point, which was built with Mallet (McCallum, 2002). The features we used are all shown in Figure 1. We trained and tested the model using the noun phrase chunking corpus mentioned in Ramshaw & Marcus (1995). To help the hill-climbing system improve, we set aside 10% of the original training data at random as a separate set for feedback.",
        "formal_text": "Convert casual text to formal text: To check out how well this method works for finding a good model setup, we tried it on the task of noun phrase chunking. We used a Linear Chain CRF as our starting"
    },
    {
        "casual_text": "The main idea behind topic segmentation is figuring out where one story ends and another begins in a bunch of text or audio. A \"story\" here is just a chunk of text where all the sentences are talking about the same thing. For example, this could mean breaking down a long stream of broadcast news into separate news stories. The Topic Detection and Tracking (TDT) project (Wayne, 2000; Allan, 2002) has worked on this kind of thing before, where sentences about the same news story are grouped together. Even though we're looking at TDT data in this paper, we want to think about topic segmentation in a more general way, not just tied to news or a specific field.",
        "formal_text": "Convert casual text to formal text: The main idea behind topic segmentation is figuring out where one story ends and another begins in a bunch of text or audio. A \"story\" here is just a chunk of text where all"
    },
    {
        "casual_text": "We calculate P(s) with n-gram language models trained on web data, using Stupid Backoff (Brants et al., 2007). We look at both forward and backward contexts when we can. Unlike Brill and Moore (2000), we noticed that people usually have both left and right context when they're editing a document.",
        "formal_text": "Convert casual text to formal text: We calculate P(s) with n-gram language models trained on web data, using Stupid Backoff (Brants et al., 2007). We look at both forward"
    },
    {
        "casual_text": "Our training setup and the way we tweak things (like hyper parameters) are pretty much the same as what (Vaswani et al., 2017) did. To break it down, for Chinese-English translation, we stuck with the 30K most common words for both the source and target languages. For English-German and English-French translations, we went with 50K subword tokens as our vocabulary, using Byte Pair Encoding (BPE) as suggested by (Sennrich et al., 2015).",
        "formal_text": "Convert casual text to formal text: Our training setup and the way we tweak things (like hyper parameters) are pretty much the same as what (Vaswani et al., 2017) did. To break it down"
    },
    {
        "casual_text": "When you're going through a tagged sentence T = (w 1 /p 1, . . . , w n /p n ) from the end to the beginning, you categorize each word w i into one of three groups: Left, Right, or Shift.",
        "formal_text": "Convert casual text to formal text: When you're going through a tagged sentence T = (w 1 /p 1, . . . , w n /p n ) from the"
    },
    {
        "casual_text": "How batch size and dropout affect things. The batch size impacts how quickly the model learns and its final accuracy, while the dropout rate also plays a big role in performance. We looked at how these two settings influence things by using a 1:1 ratio for our data (all CTB data and an equal amount of randomly selected PD data). We tracked the accuracy of our neural multi-view learning model over different training epochs, trying out different combinations of dropout rate (d) and batch size (b). The results are available here: [link 1] and [link 2]. For the stacking model, we set the batch size to 100 for the PD sub-model. The results are in Figure 5. The top two dashed lines at epoch 30 show a 20% dropout rate, the middle solid lines show no dropout, and the bottom dotted lines show a 50% dropout rate. Without dropout, the model starts off doing better but then gets worse after 10 epochs, which suggests it’s overfitting the training data. On the other hand, with a 50% dropout rate, the model starts off way worse, meaning the dropout might be too high and causing underfitting.",
        "formal_text": "Convert casual text to formal text: How batch size and dropout affect things. The batch size impacts how quickly the model learns and its final accuracy, while the dropout rate also plays a big role in performance. We looked at"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. First, h(x) is just a way to describe a feature of a hypothesis, which we'll call x. Next, P B(x) and W B(x) are sets that tell us where the phrases and words start and end in this hypothesis x. The label \"phrase_bound\" is there to show that a particular word boundary is actually the end or start of a phrase. v(b) is a function that gives us a feature vector for a word boundary, and I P B (b) is like a check to see if a word boundary b is part of the set P B(x).",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. First, h(x) is just a way to describe a feature of a hypothesis, which we'll call"
    },
    {
        "casual_text": "You can check out (Garca-Varea et al., 1998; Garca-Varea & Casacuberta, 2001) if you want more info on this decoding algorithm.",
        "formal_text": "Convert casual text to formal text: You can check out (Garca-Varea et al., 1998; Garca-Varea & Casacuberta, 2001) if you want more"
    },
    {
        "casual_text": "All the methods mentioned earlier are focused on types, meaning they mostly try to expand the vocabulary rather than the annotations that go along with it. In our previous work (Fürstenau and Lapata, 2009), we came up with a way to get new training examples by transferring annotations from existing FrameNet sentences to new, unseen ones. The method we’re suggesting now is different—it’s based on tokens. But there’s a catch: it only creates annotations for verbs that FrameNet already recognizes as being linked to a specific frame.",
        "formal_text": "Convert casual text to formal text: All the methods mentioned earlier are focused on types, meaning they mostly try to expand the vocabulary rather than the annotations that go along with it. In our previous work (Fürstenau and Lapat"
    },
    {
        "casual_text": "In this piece, we’re saying that even though the usual training method gets a lot of flak, it’s basically what happens when you mix autoregressive modeling with maximum-likelihood training. So, if you want to boost performance at test time, the key thing to focus on is just better regularization to improve generalization. Turns out, a lot of the fixes people suggest for exposure bias are actually doing just that, but they’re usually aiming for some implicit metric that’s not the maximum-likelihood one.",
        "formal_text": "Convert casual text to formal text: In this piece, we’re saying that even though the usual training method gets a lot of flak, it’s basically what happens when you mix autoregressive modeling with maximum-like"
    },
    {
        "casual_text": "So, like, back in 2007, Szpektor and his crew talked about this thing, or you could also look at model-building systems from Bos and Markert in 2006.",
        "formal_text": "Convert casual text to formal text: So, like back in 2007, Szpektor and his crew talked about this thing, or you could also look at model-building systems from Bos and Markert in 2006. So, like back"
    },
    {
        "casual_text": "Generating comments for program source code is another aspect of comment generation. The process takes in a piece of program source code, like a Java method, and spits out its comment or a summary. Iyer and his crew (2016) along with Hu and his team (2018) tackled this by treating it as a machine translation problem—turning source code into comments—using sequence-to-sequence models. This means that generating feedback comments could probably be handled the same way, translating learner sentences into feedback comments.",
        "formal_text": "Convert casual text to formal text: Generating comments for program source code is another aspect of comment generation. The process takes in a piece of program source code, like a Java method, and spits out its comment"
    },
    {
        "casual_text": "Basically, if you have a document d with a bunch of tokens like t1, t2, ..., t|d|, we can break it down into smaller chunks called segments. Each segment si will have c tokens, and there will be |s| of these segments in total.",
        "formal_text": "Convert casual text to formal text: Basically, if you have a document d with a bunch of tokens like t1, t2, ..., t|d|, we can break it"
    },
    {
        "casual_text": "1a) The storm wiped out 120,000 people in Jamaica and five in the Dominican Republic, then headed west toward Mexico.",
        "formal_text": "1a) The storm wiped out 120,000 people in Jamaica and five in the Dominican Republic, then headed toward Mexico. Convert casual text to formal text: 1a) The storm wiped out 120,000 people in Jamaica and five"
    },
    {
        "casual_text": "We tried out different ways of representing things using two factors: IJB type and whether it's local or distributed. This was done for each of the four types of errors.",
        "formal_text": "Convert casual text to formal text: We tried out different ways representing things using two factors: IJB type and whether it's local or distributed. This was done for each of the four types of errors."
    },
    {
        "casual_text": "Non-Wikipedia, Non-Entity Stuff: (like \"strange things,\" \"Early studies,\" or \"A link\") – Our classifier from Section 4 can handle filtering those out.",
        "formal_text": "Convert casual text to formal text: Non-Wikipedia, Non-Entity Stuff: (like \"strange things,\" \"Early studies,\" or \"A link\") – Our classifier from Section 4 can"
    },
    {
        "casual_text": "The McRae norms, from a study by McRae and colleagues in 2005, are a collection of feature norms gathered from 725 participants for 541 different concepts. These concepts include both living and non-living things, like alligators, chairs, and accordions. The participants were asked to come up with features for each concept, focusing on physical traits, functions, and other properties. This resulted in a bunch of concept-feature pairs, like \"airplane used-for-passengers\" or \"bear is-brown.\"",
        "formal_text": "Convert casual text to formal text: The McRae norms, from a study by McRae and colleagues in 2005, are a collection of feature norms gathered from 725 participants for 541 different concepts"
    },
    {
        "casual_text": "A DM's ability to adapt can be linked to the information it gets from the NLU. When we talk about DM flexibility, we're referring to its functions like handling pronouns, connecting ideas, keeping track of changes in conversation, and being able to go back to earlier topics (McTear et al., 2016).",
        "formal_text": "Convert casual text to formal text: A DM's ability to adapt can be linked to the information it gets from the NLU. When we talk about DM flexibility, we're referring to its functions like handling pronou"
    },
    {
        "casual_text": "To show how well MORE-RLL works for learning semantic representations, we used t-SNE (a method by Maaten and Hinton from 2008) to visualize the semantic space of relational representations. We picked 4 random relation types from the FewRel and NYT+FB-sup test sets, with 100 instances for each type. We then created representations for these instances using both MORE-RLL (with GloVe and CNN) and RSNs, and colored them based on their actual types. Figure 3 shows the visualization for FewRel. The semantic space from MORE-RLL looks really clear—almost all four types maintain their similarity within a sort of bubble, with a clear gap between different categories. On the other hand, RSNs try to squeeze the representations of the same type into one spot. This makes the points in each cluster for RSNs much denser than those for MORE-RLL. But focusing too much on the similarity between pairs of points might mess up the similarity structure within classes, so RSNs can end up splitting samples from the same category into multiple subgroups.",
        "formal_text": "Convert casual text to formal text: To show how well MORE-RLL works for learning semantic representations, we used t-SNE (a method by Maaten and Hinton from 2008) to visualize the semantic space of"
    },
    {
        "casual_text": "Like Madaan et al. (2021), we’re checking out how Polyjuice stacks up against other generators, specifically RoBERTa and T5, which focus on replacing words and phrases, and the original GPT-2, which just generates stuff without being tied to a specific input. For a given input x and its counterfactuals X, we measure diversity using self-BLEU (Zhu et al., 2018) within X. As for closeness, we look at the average distance between x and each x in X, using two methods: the normalized word-level Levenshtein edit distance (Levenshtein, 1966), which MiCE (Ross et al., 2020) uses, and the syntactic tree edit distance (Zhang and Shasha, 1989), as used in GYC (Madaan et al., 2021).",
        "formal_text": "Convert casual text to formal text: Like Madaan et al. (2021), we’re checking out how Polyjuice stacks up against other generators, specifically RoBERTa and T5, which focus on replacing"
    },
    {
        "casual_text": "Before, we used these measures to figure out how two argumentative parts were connected. But since our speeches have multiple arguments and we don’t know where they are in the text, we tweaked the method to work at the speech level. This means we looked at every sentence in the supporting speech and every possible sentence in the counter speech. For each measure, we compared one supporting speech sentence to all the potential counter speech sentences and combined the similarities using a function f. This gave us a \"sentence-to-speech\" similarity. Then, we took all those similarities and combined them again using another function g, which gave us a \"speech-to-speech\" similarity. We call these speech-to-speech measures w f g for word-based similarities and e f g for embedding-based similarities. When combining things, we considered using the max (), min (), average (+), or product (). For example, w + means we took the highest word-based similarity for each supporting sentence compared to all counter sentences, and then averaged those numbers.",
        "formal_text": "Convert casual text to formal text: Before, we used these measures to figure out how two argumentative parts were connected. But since our speeches have multiple arguments and we don’t know where they are in the text, we tweaked"
    },
    {
        "casual_text": "You can check out our PCRF-Seq2Seq implementation here: https://github.com/UKPLab/coling2016-pcrf-seq2seq",
        "formal_text": ""
    },
    {
        "casual_text": "In this paper, our team JPL suggests four ways to improve RAG's ability to handle different types of knowledge: multi-task learning, data augmentation, pretraining, and contrastive learning. Multi-task learning, along with extra pretraining on datasets for conversational question answering, and data augmentation, help the model get better at handling specific tasks. Contrastive learning...",
        "formal_text": "Convert casual text to formal text: In this paper, our team JPL suggests four ways to improve RAG's ability to handle different types of knowledge: multi-task learning, data augmentation, pretraining, and contrastive learning"
    },
    {
        "casual_text": "To boost how well PLMs understand entities, a simple approach is to use external entity embeddings from a knowledge graph (KG), entity descriptions, or text corpora. Researchers like Zhang et al. (2019) and others have tried this. These models often align these external embeddings with their original word embeddings to incorporate external knowledge. However, previous methods haven't explored using entity embeddings directly from the PLM itself, which makes it hard to adapt these embeddings to different domains. Some recent studies have tried to integrate knowledge into PLMs by doing extra pre-training. For example, they might create an additional entity vocabulary from text corpora or use entity-related tasks to improve entity representation. But this extra pre-training can be really expensive, especially when you need to update or extend the vocabulary for different tasks. In this paper, we propose a straightforward and effective solution called the Pluggable Entity Lookup Table (PELT) to integrate knowledge into PLMs. Specifically, we take a closer look at the relationship between the input features and output representations in masked language modeling.",
        "formal_text": "Convert casual text to formal text: To boost how well PLMs understand entities, a simple approach is to use external entity embeddings from a knowledge graph (KG), entity descriptions, or text corpora. Researchers"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. We have a subset C_t, and we're dealing with rankings. Let's call R the set of rankings r_i for i from 1 to the size of C_t. Each r_i is the final ranking for s_i that we need to figure out. Now, we want to optimize this thing called the objective cost function, O(R). Here's what it looks like: O(R) =  * sum from i=1 to |C_t| of [G_i * (r_i * _i - r_i * G_i)2] +  * sum from i=1 to |C_t| of [L_i * (r_i * _i - r_i * L_i)2] Basically, we're trying to minimize this O(R) by tweaking the rankings r_i. The  and  are just weights to balance the two parts of the equation.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way. We have a subset C_t, and we're dealing with rankings. Let's call R the set of rankings"
    },
    {
        "casual_text": "Based on these results, we move on to train the VAE with the topic and discourse goals (check out Section 3.1), using C = 6.0 and the \"prepend\" method. Now, we want to see how well the encoder can pick up on the topics or tell the difference between the real stories and the ones with mistakes (negative samples).",
        "formal_text": "Convert casual text to formal text: Based on these results, we move on to train the VAE with the topic and discourse goals (check out Section 3.1), using C = 6.0 and the \"prepend\" method. Now"
    },
    {
        "casual_text": "Okay, so first off, we're looking at the BLEU scores for the full translation results (no splitting involved) in Table 1. You can see that the model's accuracy is pretty much on par with the best models out there (shoutout to Vaswani et al., 2017). Then, we check out the BLEU scores for the left and right halves in Table 2. From this, we've got a few things to note.",
        "formal_text": "Convert casual text to formal text: Okay, so first off, we're looking at the BLEU scores for the full translation results (no splitting involved) in Table 1. You can see that the model's accuracy is pretty"
    },
    {
        "casual_text": "So, we've got w and b, which are just parameters. At the end of the day, the model tries to minimize the cross-entropy loss between the actual labels and what the model predicts. The formula for this is L = CrossEntropy(y_true, y_pred). To do this, we use stochastic gradient descent.",
        "formal_text": "Convert casual text to formal text: Convert casual text to formal text: So, we've got w and b, which are just parameters. At the end of the day, the model tries to minimize the cross-"
    },
    {
        "casual_text": "It's just a mix between the one-hot target distribution ( p_v ) and the smoothing prior ( 1_V ), where ( m ) (which is between 0 and 1) decides how much of each to use. You can figure this out using either the divergence inequality or the Lagrange multiplier method. Check out Appendix A for more details.",
        "formal_text": "Convert casual text to formal text: It's just a mix between the one-hot target distribution ( p_v ) and the smoothing prior ( 1_V ), where"
    },
    {
        "casual_text": "A bold result means it's way better (like, p  0.01 in a t-test) compared to the other stuff in the same box.",
        "formal_text": "Convert casual text to formal text: A bold result means it's way better (like, p  0.01 in a t-test compared to the other stuff in the same box.). A bold result means"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. So, \"x e t\" is how we represent an entity \"e\" in a sentence \"S t.\" Similarly, \"h e t\" is the representation of a specific mention of that entity in the sentence, and \"h v t\" is the representation of an action related to that entity in the same sentence. Now, if the entity \"e\" isn't mentioned in sentence \"S t,\" we just use a zero vector to represent it for that sentence. Also, if there are multiple mentions of the entity \"e\" in sentence \"S t,\" we average all those mentions to get \"h e t.\" We do the same thing if there are multiple actions related to the entity.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. So, \"x e t\" is how we represent an entity \"e\" in a sentence \"S t.\""
    },
    {
        "casual_text": "Yeah, as tohnakers, it's kinda our job to point out chances and give heads-up warnings when needed, plus be ready to provide stuff whenever someone asks for it.",
        "formal_text": "Convert casual text to formal text: Yeah, as tohnakers, it's kinda our job to point out chances and give heads-up warnings when needed, plus be ready to provide stuff whenever someone asks for"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: So, we have \"\" (Taro) as a noun phrase (NP) with some specific features. The whole thing is saying that Taro is a noun and has some properties (F) related to him. Then, there's this part that says \"\" (ga), which is a case marker, and it's used to show the subject of the sentence. Next, there's \"\" (come-NP), which is like saying \"coming\" or \"to come.\" It's connected to the subject (Taro) and the whole thing forms a sentence (S). So, in simpler terms, it's like saying: \"Taro, who has certain properties, is coming (or will come).\"",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: So, we have \"\" (Taro) as a noun phrase (NP) with some specific features. The"
    },
    {
        "casual_text": "For our next project, we're thinking about looking at how eye movements are linked to how well people understand what they read. In this study, everyone answered some questions about the passages they read. By checking out how different eye movement patterns connect to how accurate their answers were, we might get a better idea of the reading strategies that both regular readers and those with cognitive issues use.",
        "formal_text": "Convert casual text to formal text: For our next project, we're thinking about looking at how eye movements are linked to how well people understand what they read. In this study, everyone answered some questions about the passages they read."
    },
    {
        "casual_text": "During prediction, the QA model M(q, y) calculates the probability of having no answer, p M (NONE|q, y), and the probability of the most probable answer span, p M (y i. . . j |q, y). We tweak how accurate this reward is by using the log odds ratio c, which compares the \"no answer\" option to the span option, as a setting when deciding on the prediction.",
        "formal_text": "Convert casual text to formal text: During prediction, the QA model M(q, y) calculates the probability of having no answer, p M (NONE|q, y), and the probability of the"
    },
    {
        "casual_text": "The TreeLSTM module has 512 neurons. We're using a mini-batch size of 64 for all models, just like Yang et al. (2016) recommended. Also, dropout is set to 50% for all the models.",
        "formal_text": "Convert casual text to formal text: The TreeLSTM module has 512 neurons. We're using a mini-batch size of 64 for all models, just like Yang et al. (2016) recommended. Also, drop"
    },
    {
        "casual_text": "So, we've got 450 training examples, which is way less than 1% of the whole TyDi QA training set. If we double that to 100 examples per language, we see a boost in performance, with an average F1 score of 71.7. Adding even more data—like 500 examples per language, which totals 4,500 examples—makes things even better, bringing the average F1 score up to 76.7. Basically, with less than 10% of the training data, we've closed the performance gap by over 82%. And for some languages, like Finnish, the improvement is even bigger—like over 92%.",
        "formal_text": "Convert casual text to formal text: So, we've got 450 training examples, which is way less than 1% of the whole TyDi QA training set. If we double that to 100 examples per language, we see"
    },
    {
        "casual_text": "Alright, let’s break this down in simpler terms. So, in the last section, we talked about how the real world—which is super complex and never-ending—is described using languages that have a limited number of words. Because of this, words in any language don’t really have super clear or exact meanings. They kind of cover a range of things or events. This fuzziness in word meanings makes translating tricky. In one language, a single word might cover a bunch of similar things or events, but in another language, you might need different words to describe each of those things. It’s like one word doing the job of several in another language (check out Figure 6 for an example).",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in simpler terms. So, in the last section, we talked about how the real world—which is super complex and never-ending—is described using languages"
    },
    {
        "casual_text": "We dig into the difference between how words and sentences are represented when we learn them.",
        "formal_text": "Convert casual text to formal text: We dig into the difference how words and sentences represented when we learn them."
    },
    {
        "casual_text": "A bunch of things can affect how students learn, like how hard the material is (Person et al., 1995), how much work they have (Craig et al., 2004), or how often they show up to class (Ames, 1992), and so on. These things can change over time and might even be connected to each other. The people involved—like teachers and students—don’t always agree on what makes good feedback. So, when we’re giving feedback, we need to think about everyone’s preferences to make sure the feedback works for both students and teachers.",
        "formal_text": "Convert casual text to formal text: A bunch of things can affect how students learn, like how hard the material is (Person et al., 1995), how much work they have (Craig et al."
    },
    {
        "casual_text": "In this experiment, we’re comparing our method to a few other baseline methods, which we talked about in Section 5.4. We used Equation (3) to combine the community emotions for all the methods we tested. The parameters 0 and 1 were set based on some trial and error, with 0 being 5 times the minimum emotion value and 1 being half of the maximum emotion value. The results are in Table 3. Table 3 shows that EmoPeakFind and TopKE-moVari didn’t perform as well as the other methods. EmoPeakFind used a pretty basic rule that ended up creating a lot of noisy peaks, which hurt its precision. TopKE-moVari only looks at how emotions change between two time intervals, and picking the right value for k is tricky because you don’t know how many events are happening until after you’ve found them. It’s worth noting that both EmoPeakFind and TopKE-moVari are rule-based methods, while KLB, MBurst, and our method are based on state machines. This suggests that state machine models work better for detecting emotion bursts in communities. What’s kind of surprising is that MBurst didn’t do as well as KLB, especially since MBurst was designed to handle multiple streams. The issue is that MBurst assumes that different streams will have similar states at the same time, meaning there’s a positive correlation between them. But that assumption doesn’t really hold when you’re dealing with different emotion streams.",
        "formal_text": "Convert casual text to formal text: In this experiment, we’re comparing our method to a few other baseline methods, which we talked about in Section 5.4. We used Equation (3) to combine the community emotions for all the"
    },
    {
        "casual_text": "We tried out three different methods for training our models. First, we trained some models using word-level normalization, where the source and target are made up of individual words broken down into characters, separated by spaces. To help the models understand the context better, we also trained another set using chunked data. This means we fed in groups of three words at a time, split into characters, and marked the word boundaries with an underscore ( ). Finally, we trained a set of models at the sentence level. Here, the models learned to normalize entire sentences, with words broken into characters and separated by underscores.",
        "formal_text": "Convert casual text to formal text: We tried out three different methods for training our models. First, we trained some models using word-level normalization, where the source and target are made up of individual words broken down into characters, separated"
    },
    {
        "casual_text": "In this paper, we introduce a new neural relation extraction model that focuses on encoding information from relation paths. Unlike other models out there, ours can work with sentences that have either both target entities or just one of them, making it better at handling messy or noisy data. When we tested it on real-world datasets, our model showed some pretty solid improvements in relation extraction compared to the standard methods.",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce a new neural relation extraction model that focuses on encoding information from relation paths. Unlike other models out there, ours can work with sentences that have either"
    },
    {
        "casual_text": "The space reconstructor helps rebuild the semantic space to make sure the user is happy. It follows a specific method for this process, which we'll explain below.",
        "formal_text": "Convert casual text to formal text: The space reconstructor helps rebuild the semantic space to make sure the user is happy. It follows a specific method for this process, which we're explains below."
    },
    {
        "casual_text": "The whole point of using DCGs is that they make things clearer and more organized.",
        "formal_text": "Convert casual text to formal text: The whole point of using DCGs that they make things clear and more organized. Convert casual text to formal text: The whole point of using DCGs that they make things clear and more organized"
    },
    {
        "casual_text": "Discourse relations usually have a direction. Our QA format shows this by putting discourse units either in the question or the answer. In some question types, the order is fixed by the question itself. For example, in Table 4, ex. 1, since the question asks about the condition, the condition will always be in the answer. There’s also a pattern for symmetric relations, where the meaning stays the same no matter how you arrange the question and answer, like in ex. 2 in Table 4. Lastly, some relation types, like cause-effect (reason vs. result) or certain time-related ones (before vs. after), are considered reversed. In these cases, two QA pairs with different question types can actually mean the same thing if you swap the discourse units, as seen in ex. 3 in Table 4. These directionality patterns affect how we annotate and evaluate things, which we’ll talk about later.",
        "formal_text": "Convert casual text to formal text: Discourse relations usually have a direction. Our QA format shows this by putting discourse units either in the question or the answer. In some question types, the order is fixed by the question itself"
    },
    {
        "casual_text": "We use a first-order linear conditional random field (CRF) to label a sequence of characters. Following Xue's approach from 2003, we use the \"BIES\" tagging system. For this, we use the CRF++ tool, version 3. The features we work with are character n-grams within a 5-character window and tag bigrams. For a character c0 in the sequence c2 c1 c0 c1 c2, we pull out the following features: character unigrams.",
        "formal_text": "Convert casual text to formal text: We use a first-order linear conditional random field (CRF) to label a sequence of characters. Following Xue's approach from 2003, we use the \"BIES"
    },
    {
        "casual_text": "Computational branding analytics (CBA) is all about digging into stuff like blogs, tweets, reviews, or forum posts to figure out what people are saying about a brand—like trends, demographics, and other insights. In this case, we’re focusing on a chunk of Yelp reviews about coffee shops. Why? Because Yelp gives us some handy info: each review comes with a 5-star rating, tells us which coffee shop it’s about, and includes the reviewer’s name, which helps us guess their gender. For this paper, we’re breaking down CBA into three smaller parts to make it easier to tackle.",
        "formal_text": "Convert casual text to formal text: Computational branding analytics (CBA) is all about digging into stuff like blogs, tweets, reviews, or forum posts to figure out what people are saying about a brand—like trends,"
    },
    {
        "casual_text": "The Europe Media Monitor (EMM) 2 collects around 100,000 news articles every day, covering around 50 languages. It pulls these articles from about 3,400 carefully chosen news websites, plus a couple hundred specialist and government sites, and around 20 commercial news providers. The system checks these websites every five minutes to grab the latest articles. If a site has an RSS feed, it uses that, but if not, it digs through the sometimes messy HTML pages to get the news text. All the articles are converted into Unicode and go through a series of steps where each part adds more info. No matter how the files are saved, the system uses UTF-8-encoded RSS format.",
        "formal_text": "Convert casual text to formal text: The Europe Media Monitor (EMM) 2 collects around 100,000 news articles every day, covering around 50 languages. It pulls these articles from about 3,400 carefully chosen news websites, plus a couple"
    },
    {
        "casual_text": "We mostly compare how well different models perform by looking at their accuracy. Here are the baselines we’re checking against: 1. **EMB (Qadir et al., 2016)**: This method takes the word embeddings for the vehicle and event, adds them together (element-wise sum), and then picks the answer that’s closest to this combined vector based on cosine distance. 2. **Meta4meaning (Xiao et al., 2016)**: This approach leans toward properties that are strongly linked to both the topic and the vehicle. It also gives more weight to properties that are more related to the vehicle than the topic. The strength of these associations is measured using statistical significance.",
        "formal_text": "Convert casual text to formal text: We mostly compare how well different models perform by looking at their accuracy. Here are the baselines we’re checking against: 1. **EMB (Qadir et al., 2016)"
    },
    {
        "casual_text": "To understand the visual stuff and how things relate in each image, we grab attribute tokens T A (i) (like \"leafy\" from \"leafy(tree)\") and relationship tokens T R (i) (like \"man\" and \"playing\" from \"playing(man, frisbee)\") from the Visual Genome dataset, which has scene graphs. We only keep the tokens that show up in both sets, so T V G (i) = T A (i)  T R (i), to focus on the most important ones. The rest we leave out.",
        "formal_text": "Convert casual text to formal text: To understand the visual stuff and how things relate in each image, we grab attribute tokens T A (i) (like \"leafy\" from \"leafy(tree)"
    },
    {
        "casual_text": "For the Tr-De dataset, the scores are usually lower, showing that this dataset (and maybe the language pair) is tougher. But now, we can see that the code-switched versions give way better scores. We were kinda surprised by this, and even Multilingual and Language-aware 7, which can be adjusted differently depending on the context, confirmed this with the authors. Besides the numbers in the table, we also checked precision and recall. Precision is way higher (1.1 to 3 times, check Appendix B) than recall, especially for Tr-De, which matches what others have found before (van der Goot, 2019). This means the model is pretty cautious and only changes things when it's pretty sure, which is actually a good thing.",
        "formal_text": "Convert casual text to formal text: For the Tr-De dataset, the scores are usually lower, showing that this dataset (and maybe the language pair) is tougher. But now, we can see that the code-switched"
    },
    {
        "casual_text": "The decor could use a little improvement, and having a small bar would make the place feel more welcoming. Figure 6 shows some examples of how BART (a machine learning model) handles different sentences. Let me break it down for you: - **(a)**: In this case, the category \"miscellaneous\" isn't mentioned in the sentence, so BART can't figure it out. But our method can still combine different sentiments from different aspects to get the right answer. - **(b)**: The sentence says, \"the value on the kids menu is good.\" BART might get confused because \"good\" is talking about the value, not the menu itself. But our method gets it right and doesn't get distracted by other parts of the sentence. - **(c)**: This one's a bit tricky because it involves \"if\" statements, which are hard for BART to handle. But our method can still figure out the negative sentiment in \"if there was... would be a bit more inviting.\" So, our method does a better job than BART in these tricky situations.",
        "formal_text": "Convert casual text to formal text: The decor could use a little improvement, and having a small bar would make the place feel more welcoming. Figure 6 shows some examples of how BART (a machine learning model) handles different sentences"
    },
    {
        "casual_text": "After that, we sifted out 95.5 million paper clusters using the criteria we mentioned earlier and took them out of our dataset. You can check out the breakdown of these filtered papers in Table 2. Turns out, a whole bunch of these clusters got the boot—80 million of them didn’t have any publisher-provided abstract or PDF, and 13 more came from the Unpaywall 2019-04-19 data dump. We used the cld2 tool to check the language, setting a threshold of 0.9 for the English language score. These papers without much text aren’t super helpful for our dataset right now. While they might still be useful as citation points in S2ORC, they’re generally not as high-quality, so we filtered them out to make the dataset better overall.",
        "formal_text": "Convert casual text to formal text: After that, we sifted out 95.5 million paper clusters using the criteria we mentioned earlier and took them out of our dataset. You can check out the breakdown of these filtered papers in"
    },
    {
        "casual_text": "We suggested using an attentive bi-GRU network to learn how titles are represented and set up two Siamese networks to check how relevant the title is to the body and how relevant the image is to the body. Then, we mixed the title representation with these relevance scores to make our final prediction. Dong et al. (2019) came up with a model for spotting clickbait by combining a bi-GRU network with an attention network to understand both titles and bodies. They made a similarity score based on how similar the titles and bodies were overall and in detail. They used this similarity score along with the title and body representations to predict if something was clickbait. But, these methods miss out on looking at the style of titles when learning how to represent them, which is actually a big clue for spotting clickbait. Plus, they don’t think about how the title and body connect, which is usually key for figuring out how relevant they are to each other. Unlike those methods, our approach uses a character-level Transformer to pick up on the style of titles, which helps us spot clickbait better. Also, it can model how the title and body interact using co-attention to make their representations even stronger.",
        "formal_text": "Convert casual text to formal text: We suggested using an attentive bi-GRU network to learn how titles are represented and set up two Siamese networks to check how relevant the title is to the body and how relevant the image is"
    },
    {
        "casual_text": "Alright, let’s break this down. We’re going to list some attributes and their corresponding functions for certain grammar rules. Then, we’ll add these to the rules so that when we run the functions, we get the translation of the sentence. But we’re only focusing on the rules needed for the part of figure 5. The other attributes are just extra stuff that helps out but isn’t the main focus here.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down. We’re going to list some attributes and their corresponding functions for certain grammar rules. Then, we’ll add these to the rules so that"
    },
    {
        "casual_text": "For ANNs to work with words, we gotta turn symbols into numbers and make sure everything has a fixed length. It's best to do this while keeping in mind those three constraints we talked about earlier.",
        "formal_text": "Convert casual text to formal text: For ANNs to work with words, we gotta turn symbols into numbers and make sure everything has a fixed length. It's best to do this while keeping in mind those three constraints we"
    },
    {
        "casual_text": "Out of 1570 test cases, we got 35 correct, which we think is because that dataset has a lot more different ideas to deal with. But when we tried training and testing on both datasets, our performance shot up (MT QM R+AD: 0.569 over 1595 instances).",
        "formal_text": "Convert casual text to formal text: Out of 1570 test cases, we got 35 correct, which we think is because that dataset has a lot more different ideas to deal with. But when we tried training and testing on both datasets"
    },
    {
        "casual_text": "Just so you know, we’re keeping this rule simple right now to make things easier to understand. Later on, we’ll give you a more precise version of the same idea.",
        "formal_text": "Convert casual text to formal text: Just so you know, we’re keeping this rule simple right now to make things easier to understand. Later on, we’ll give you a more precise version of the same idea."
    },
    {
        "casual_text": "x and 1 r x represent the first and last n-1 words of a continuous string, respectively.",
        "formal_text": "Convert casual text to formal text: x and 1 r x represent the first and last n-1 words of a continuous string, respectively. Convert casual text to formal text: x and 1 r x"
    },
    {
        "casual_text": "In these patterns, a sentence shows how something is part of something else. They use certain verbs. The \"part\" and \"whole\" words are usually in noun phrases or prepositional phrases with specific prepositions. We use these patterns in our app.",
        "formal_text": "Convert casual text to formal text: In these patterns, a sentence shows how something is part something else. They use certain verbs. The \"part\" and \"whole\" words are usually in noun phrases or prepositional"
    },
    {
        "casual_text": "1. Input: T is the set of MeSH terms (the topic we're searching for); n + is the number of positive examples we have; IR and  T are a black-box machine learning-based IR system and its query settings.",
        "formal_text": "Convert casual text to formal text: 1. Input: T is the set of MeSH terms (the topic we're searching for); n + is the number of positive examples we have; IR and  T are"
    },
    {
        "casual_text": "Okay, so structural rules don't really fit well in categorial grammars when we're talking about describing language. But, hey, in real natural language, we do see stuff that’s commutable, iterable, or optional. That means we need a way to say, \"Hey, these specific types can have structural operations, but we don't want them everywhere.\" To make this happen, we’re gonna borrow an idea from Girard’s linear sequent logic (from 1987). His system doesn’t use contraction or weakening rules, which is kinda cool. We’re thinking of something similar, but we’re calling it \"structural modalities.\" Basically, we’ll have a system of universal modalities that let us handle the logic of things like commutable, iterable, or optional elements.",
        "formal_text": "Convert casual text to formal text: Okay, so structural rules don't really fit well in categorial grammars when we're talking about describing language. But, hey, in real natural language, we do see stuff that"
    },
    {
        "casual_text": "This part gives a quick overview of the natural logic stuff (from MacCartney and Manning, 2009) that our work is built on. If you want more info, check out these papers: (MacCartney and Manning, 2008; MacCartney and Manning, 2009; MacCartney, 2009; Angeli et al., 2016).",
        "formal_text": "Convert casual text to formal text: This part gives a quick overview of the natural logic stuff (from MacCartney and Manning, 2009) that our work is built on. If you want more info, check out these papers"
    },
    {
        "casual_text": "Out of the two hybrid methods, the sense-filtered-counts method worked better with the smaller bootstrapped concept-word co-occurrence matrix, while the sense-proportional method did better with the larger one. We think this happened because the bootstrapping method from Mohammad and Hirst (2006) basically sets small co-occurrence counts to 0. These tiny counts can mess things up for the sense-filtered-counts method more than the other one (since even a small non-zero value can lead to including the full co-occurrence count of the related word). So, the bootstrapped matrix fits this method better.",
        "formal_text": "Convert casual text to formal text: Out of the two hybrid methods, the sense-filtered-counts method worked better with the smaller bootstrapped concept-word co-occurrence matrix, while the sense-proportional method did better"
    },
    {
        "casual_text": "ij extends the Euclidean norm. Also, for orthogonal matrices, they work similarly to how unit vectors do:",
        "formal_text": "Convert casual text to formal text: ij extends the Euclidean norm. Also, for orthogonal matrices, they work similarly to how unit vectors do: ij extends the Eucli"
    },
    {
        "casual_text": "We check how well our system can handle numerical comparisons. We've set up a way to test its ability to understand numbers in both tables and text in English.",
        "formal_text": "Convert casual text to formal text: We check how well our system can handle numerical comparisons. We've set up a way to test its ability to understand numbers in both tables and text in English."
    },
    {
        "casual_text": "The second batch of results focuses on how well the top discourse parsers perform within the same dataset, which has been a common evaluation in the past. Most of these parsers, except for CODRA (Joty et al., 2015), have only been tested on RST-DT. We also trained and tested the Two-Stage parser on the Instr-DT corpus. When looking at how these parsers handle discourse parsing within the same dataset, the Two-Stage parser consistently performs the best on RST-DT structure prediction. On the other hand, the parser by Yu et al. (2018) gets the top results for RST-DT nuclearity prediction when using RST-Parseval. CODRA, meanwhile, performs best on the Instr-DT corpus. Table 4 breaks down how nuclearity and stochasticity affect overall performance, measured using micro-average precision with original Parseval (Par.) and RST Parseval (R-Par.). The results are averaged over 10 runs, and if there are stochastic components involved, they use 3 different generation processes. The best performance is highlighted in bold. Each sample is the average performance from 10 random subsets taken from 3 independently created treebanks.",
        "formal_text": "Convert casual text to formal text: The second batch of results focuses on how well the top discourse parsers perform within the same dataset, which has been a common evaluation in the past. Most of these parsers, except"
    },
    {
        "casual_text": "DISAANA keeps an eye on tweets as they happen, spotting info related to disasters and showing it in a neat, organized way. It works in two ways: QA and problem-listing. In the QA mode, you can just ask something like, \"What’s running low in Kumamoto?\" and it’ll give you a list of stuff that’s in short supply (check out Figure 2a). The answers are sorted into categories like medical supplies to make them easier to understand. You can also see the shortages on a map (Figure 2c). In the problem-listing mode, you don’t even need to ask—it just gives you a list of issues happening in a certain area, like \"people trapped under rubble,\" using a method from Varga et al. (2013) (Figure 2b).",
        "formal_text": "Convert casual text to formal text: DISAANA keeps an eye on tweets as they happen, spotting info related to disasters and showing it in a neat, organized way. It works in two ways: QA and"
    },
    {
        "casual_text": "Psychological theories about irony, like echoic reminder theory (Kreuz and Glucksberg, 1989) and implicit display theory (Utsumi, 2000b), haven’t really been fully applied to analyzing text yet. Neuropsychology researchers who’ve looked into how the brain reacts to sarcasm say that understanding it depends a lot on not just the context of what’s being said, but also the speaker’s mood, personality, and things like facial expressions and tone of voice (Shamay-Tsoory et al., 2005). Without those non-verbal cues, figuring out sarcasm from just text has to rely mostly on the words and the situation, though you can still guess a bit about the speaker’s personality and mood from the text. There are some fancy models that use things like the contrast between positive words and negative situations (Riloff et al., 2013), specific words, punctuation, and even emojis (González-Ibánez et al., 2011) to detect sarcasm. They work pretty well, but they usually miss out on understanding the speaker’s mindset and the broader context of what’s being said. Kreuz and Link (2002) point out that sarcasm is more likely when the speaker and listener share a lot of knowledge—like understanding the situation and knowing each other well.",
        "formal_text": "Convert casual text to formal text: Psychological theories about irony, like echoic reminder theory (Kreuz and Glucksberg, 1989) and implicit display theory (Utsumi, 2000b), haven’"
    },
    {
        "casual_text": "We built something new, but it’s not super important for understanding the main points, so we stuck it in the appendix.",
        "formal_text": "Convert casual text to formal text: We built something new, but it’s not super important for understanding the main points, so we stuck it in the appendix. Convert casual text to formal text: We built something new,"
    },
    {
        "casual_text": "Comparing it to (4), we can see that the right side of the expression gives us something similar to the vector components we'd expect when we're looking at how often w1 and w2 appear together. So, for the multiplicative model, the combined vector hi can be seen as a rough estimate of a vector that captures the distributional characteristics of the phrase w1 w2.",
        "formal_text": "Convert casual text to formal text: Comparing it to (4), we can see that the right side of the expression gives us something similar to the vector components we'd expect when we're looking at how often w1 and"
    },
    {
        "casual_text": "So, let's break down what these results mean. The starting point is Moses, a phrase-based translation system, with no fancy setup—just a basic distance-based reordering model. Now, there are two ways we can make it better: (a) We can tell Moses to use a more advanced reordering model called the lexicalized bidirectional msd model. This is already part of Moses and works seamlessly with the rest of the translation process. (b) Alternatively, we can stick with the simple distance-based model in Moses, but tweak the training and test data beforehand using our own linear reordering model. This preprocessing step will definitely change the way Moses learns phrases, and it could either improve things or make them worse. So, it's up to us to decide which route to take!",
        "formal_text": "Convert casual text to formal text: So, let's break down what these results mean. The starting point is Moses, a phrase-based translation system, with no fancy setup—just a basic distance-based reorder"
    },
    {
        "casual_text": "In datasets that are about 5 MB, around half of the 30k BPE vocabulary doesn’t show up during training. But with datasets around 10 MB, pretty much every vocab token appears. Check out Appendix Figure 6 for more details.",
        "formal_text": "Convert casual text to formal text: In datasets that are about 5 MB, around half of the 30k BPE vocabulary doesn’t show up during training. But with datasets around 10 MB, pretty much every vo"
    },
    {
        "casual_text": "Basically, our second method works by taking the top-ranked premise targets and creating possible conclusion targets from them. We then compare these possible conclusions to a knowledge base and pick the one that’s most similar. The knowledge base can come from any collection of argumentative texts, as long as it’s based on our target identifier. For our tests, we just used all the conclusion targets from the training data. To predict a conclusion target, we first grab the top k premise targets (where k is more than 1) using our ranking system. Then, we make average embeddings (s 1, s 2, etc.) for all possible groups of these targets, where each group has m targets and m is more than 1. Next, we train a function f on our training arguments to map each s i to a new space where it looks more like the correct conclusion target c and less like other targets. Figure 3 shows how this works. We figure out the best k and m by testing different options on the validation set.",
        "formal_text": "Convert casual text to formal text: Basically, our second method works by taking the top-ranked premise targets and creating possible conclusion targets from them. We then compare these possible conclusions to a knowledge base and pick the one that’"
    },
    {
        "casual_text": "Alright, here's what we found in our study. As far as we know, there aren't any open datasets or benchmarks out there for figuring out product issues in e-commerce, especially when it comes to multiple levels. So, we had to work with our own private data from customer feedback. Unfortunately, because of privacy stuff, we can't share the exact numbers for how well our models did. Instead, we'll just compare them to some basic models we set up as a starting point.",
        "formal_text": "Convert casual text to formal text: Alright, here's what we found in our study. As far as we know, there aren't any open datasets or benchmarks out there for figuring out product issues in"
    },
    {
        "casual_text": "During semantic analysis, we take each word or \"leaf\" from the CCG tree we got from the syntactic analysis and give it a meaning based on some semantic templates. Then, we put these words together following the CCG tree structure to create a logical formula that shows the overall meaning of the whole sentence. This process is done using ccg2lambda for Japanese sentences.",
        "formal_text": "Convert casual text to formal text: During semantic analysis, we take each word or \"leaf\" from the CCG tree we got from the syntactic analysis and give it a meaning based on some semantic templates"
    },
    {
        "casual_text": "The idea that S and I are independent when we know U lets us pick any speech dataset to train P(S|U). This means we can control speaker traits and other sound features separately from the I2U system (Henter et al., 2018; Akuzawa et al., 2018). Table 1 lists the five datasets we used to train the S2U, I2U, and U2S models. We intentionally used different datasets for each part to see how well the units hold up when moving between different areas, like changes in speakers, speaking styles (scripted or spontaneous), and types of content (books, newspapers, or image descriptions). Out of the three datasets with both images and speech pairs—Places, Flickr8k, and MSCOCO—we picked the last two for training I2U models. They have five captions per image, which works better for caption evaluation metrics like SPICE (Anderson et al., 2016). Plus, they're popular image captioning datasets with lots of text-based comparisons in the research. Places only has one spoken caption per image and hasn’t been used for captioning before.",
        "formal_text": "Convert casual text to formal text: The idea that S and I are independent when we know U lets us pick any speech dataset to train P(S|U). This means we can control speaker traits and other sound features separately from the I"
    },
    {
        "casual_text": "Baseline method 2 gave a score of zero for three programs because their summaries didn't have any words in common with the target program. On the other hand, the proposed method 2 did a better job and actually scored those programs. This made its results more in line with the gold-standard data, showing a higher correlation.",
        "formal_text": "Convert casual text to formal text: Baseline method 2 gave a score of zero for three programs because their summaries didn't have any words in common with the target program. On the other hand, the proposed method 2 did"
    },
    {
        "casual_text": "The main goal of this project is to make a big difference in solving computational problems related to language generation (LG) when dealing with multiple tasks, languages, and modes. Specifically, Multi3Generation is all about combining these three areas and seeing how they can improve LG solutions. The project has a few key goals for building skills and knowledge:",
        "formal_text": "Convert casual text to formal text: The main goal of this project is to make a big difference in solving computational problems related to language generation (LG) when dealing with multiple tasks, languages, and modes. Specifically, Multi3Gen"
    },
    {
        "casual_text": "Looking at things at the node level instead of the whole subgraph makes it easier to tell vertices apart, which helps with ranking and picking the right ones. Plus, keeping scores consistent within each node's neighborhood makes the system more resistant to noise. This is super helpful when dealing with messy stuff like speech transcriptions.",
        "formal_text": "Convert casual text to formal text: Looking at things at the node level instead of the whole subgraph makes it easier tell vertices apart, which helps with ranking and picking the right ones. Plus, keeping scores consistent within each"
    },
    {
        "casual_text": "Neural Sequence Models: Even though having a big dataset gives us tons of word-context pairs to work with, the huge number of possible contexts makes it impossible to estimate everything directly. That's why traditional n-gram models use shorter contexts and smoothing methods to handle unseen situations. Neural language models, on the other hand, don't limit the context and instead use neural representations for context. This could be a hidden state from a recurrent neural network (RNN), like an LSTM state (Hochreiter and Schmidhuber, 1997), or a set of attention weights, which you'd find in a transformer model (Vaswani et al., 2017). While the ideas here apply to all autoregressive models, we're mainly focusing on RNNs, which use a fixed-size continuous representation h(w 1: t1 ) for the context. Unlike transformers, RNNs can be easily adapted to variational autoencoders with just one bottleneck (Bowman et al., 2016), which is a pretty cool type of generative model.",
        "formal_text": "Convert casual text to formal text: Neural Sequence Models: Even though having a big dataset gives us tons of word-context pairs to work with, the huge number of possible contexts makes it impossible to estimate everything directly"
    },
    {
        "casual_text": "Creating story-like visual explanations by watching movies and reading books. Presented at the IEEE International Conference on Computer Vision, pages 19-27.",
        "formal_text": "Convert casual text to formal text: Creating story-like visual explanations by watching movies and reading books. Presented at the IEEE International Conference on Computer Vision, pages 19-27."
    },
    {
        "casual_text": "We use KEWE for checking how easy documents are to read, with the help of a supervised approach. We tested it on four datasets, some in English and some in Chinese. The results show that our method works better than other popular readability assessment tools and even beats classic text-based word embedding models on all the datasets. Plus, when we combine our knowledge-enriched word embedding with some manually created features, the performance gets even better.",
        "formal_text": "Convert casual text to formal text: We use KEWE for checking how easy documents are to read, with the help of a supervised approach. We tested it on four datasets, some in English and some in Chinese. The"
    },
    {
        "casual_text": "After the session, the system grouped together papers with titles that had words like \"nature,\" \"development,\" \"transfer,\" \"fablizalion,\" or \"generation.\" The user's query, which was in Japanese, was saved in this specific group. This setup means that next time, the system can quickly answer the same query and also add any new papers about the same topic to this group.",
        "formal_text": "Convert casual text to formal text: After the session, the system grouped together papers with titles that had words like \"nature,\" \"development,\" \"transfer,\" \"fablizalion,\" or \"generation.\" The user's query"
    },
    {
        "casual_text": "While testing, we notice that people tend to focus more on the words at the beginning of the source text. To see how this \"position bias\" affects things, we picked a group of samples with the same source length (77 sentences) from the WMT15 DeEn test set. We then figured out the average attention each source position got within this group. Since different source positions might be paid attention to different numbers of times in SiMT, we made sure to average the attention weights based on how many times each position was looked at. This way, every source position gets a fair evaluation. To break it down: if  ij is the attention weight between a target word y i and a source word x j, the average attention weight A j for source position j is calculated like this:",
        "formal_text": "Convert casual text to formal text: While testing, we notice that people tend to focus more on the words at the beginning of the source text. To see how this \"position bias\" affects things, we picked a group of samples"
    },
    {
        "casual_text": "We add alignment info to the existing parallel text for specific words, which helps us create bilingual word lists that fit the context of the translation. This gives us a cleaner and more customized bilingual list compared to what you'd usually find in real-life situations. But, we could also use this as a base to make a messier list that looks more like what you'd actually find in the real world (for example, by throwing in unrelated words, adding different forms of the same word, or simplifying the words by removing extra stuff, or just picking some words randomly).",
        "formal_text": "Convert casual text to formal text: We add alignment info to the existing parallel text for specific words, which helps us create bilingual word lists that fit the context of the translation. This gives us a cleaner and more customized bilingual list compared"
    },
    {
        "casual_text": "So, the amount of possible words the model looks at can change based on the situation, and this makes the text sound pretty natural without repeating stuff too much. Lately, Massarelli and his team (2020) found out that using top-k or top-p sampling can lead to more made-up sentences, which matches what Wikipedia says.",
        "formal_text": "Convert casual text to formal text: So, the amount of possible words the model looks at can change based on the situation, and this makes the text sound pretty natural without repeating stuff too much. Lately, Massarelli and"
    },
    {
        "casual_text": "We're using the definitions of subjective and objective from a few sources: (Wiebe et al., 2005; Wiebe and Mihalcea, 2006; Wilson, 2008). Subjective expressions are basically words or phrases that people use to talk about their thoughts, feelings, and emotions—stuff like guesses, opinions, emotions, and beliefs. A broad term for these kinds of mental states is \"private state\" (Quirk et al., 1985), which means something going on inside someone's head that other people can't see or confirm directly. (Wiebe and Mihalcea, 2006) provide some examples to help explain this.",
        "formal_text": "Convert casual text to formal text: We're using the definitions of subjective and objective from a few sources: (Wiebe et al., 2005; Wiebe and Mihalcea, 2006; Wilson,"
    },
    {
        "casual_text": "The two key things about the generation phase that creates surface structure trees are the control structure used and how the system's language knowledge is spread across its different parts.",
        "formal_text": "Convert casual text to formal text: The two key things that creates surface structure trees are the control structure used and how the system's language knowledge spread across its different parts. Convert casual text to formal text: The two key things"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: We're looking to minimize u, which is defined as (w, ), and maximize v, which is equal to . The relationship between u and v is given by (u, v) = 1, which leads to: w + C1 + (1  M  Hw) with the following constraints: - Q is the set of all u = (w, ) where w and  are both greater than or equal to 0. - S is the set of all v =  where  is greater than or equal to 0. In simpler terms, we're trying to find the smallest u and the biggest v while keeping everything non-negative and following the rules above.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: We're looking to minimize u, which is defined as (w, ), and maximize v, which is equal"
    },
    {
        "casual_text": "\"The Sarah Jane Adventures\" starred Elisabeth Sladen, who played investigative journalist Sarah Jane Smith. The show was made by CBBC. They kicked things off with a special on New Year's Day in 2007, and the full series started on September 24, 2007. In 2008, the second series came out, and it was cool because it brought back Brigadier Lethbridge-Stewart. The third series in 2009 had a crossover with the main show, featuring David Tennant as the Tenth Doctor. In 2010, Matt Smith showed up as the Eleventh Doctor, and Katy Manning returned as Jo Grant. Sadly, the fifth series, which was supposed to have three stories, only got partially aired in autumn 2011 because Elisabeth Sladen passed away earlier that year.",
        "formal_text": "Convert casual text to formal text: \"The Sarah Jane Adventures\" starred Elisabeth Sladen, who played investigative journalist Sarah Jane Smith. The show was made by CBBC. They kicked things off with"
    },
    {
        "casual_text": "2. The candidate people look at a lot, but isn't the right one, usually comes right after the correct candidate in the list.",
        "formal_text": "The candidate people look at a lot, but isn't the right one, usually comes right after the correct candidate in the list. 2. The candidate people look at a lot, but isn't the right one, usually"
    },
    {
        "casual_text": "Customized engines are usually worth the cost when there's a strong case for a good return on investment and you're only dealing with a few language pairs.",
        "formal_text": "Convert casual text to formal text: Customized engines are usually worth the cost when a strong case for a good return on investment and you're only dealing with a few language pairs. Convert casual text to formal text:"
    },
    {
        "casual_text": "One of the big issues with phrase-based statistical machine translation (PBMT), as mentioned by Koehn et al. (2003) and Och and Ney (2004), is dealing with the different word orders between the source and target languages. To tackle this, decoding-time reordering models (like the ones by Koehn et al. (2005), Zens and Ney (2006), and Galley and Manning (2008)) try to measure the positional relationship between phrases during decoding. But here's the catch: these reordering models often struggle to consider the overall structure of the source sentence, which can lead to the decoder producing word orders that don't make sense grammatically. Plus, using these models makes the decoding process super complicated because the decoder has to handle phrases in all kinds of random orders.",
        "formal_text": "Convert casual text to formal text: One of the big issues with phrase-based statistical machine translation (PBMT), as mentioned by Koehn et al. (2003) and Och and Ney (2004), is dealing with"
    },
    {
        "casual_text": "After aligning everything, we added up the edge scores (i, j =1 n s ij (t i, h j )) for each possible pair of (c t, c h ). This helped us find the c t that was the best match for each c h from the hypothesis, following the reciprocal best hit method from Mushegian and Koonin (1996). We picked pairs (c t, c h ) where c t was the highest-scoring match for c h and vice versa. If we couldn’t find any reciprocal best hits for the commitments from the hypothesis, the system would automatically give a TE judgment of NO.",
        "formal_text": "Convert casual text to formal text: After aligning everything, we added up the edge scores (i, j =1 n s ij (t i, h j )) for each possible pair"
    },
    {
        "casual_text": "The action predictor is basically a fully connected layer. It grabs the combined output from the cross-modal encoder up to the current moment, t, and uses that to figure out the action, a t, for the view, v t.",
        "formal_text": "Convert casual text to formal text: The action predictor is basically a fully connected layer. It grabs the combined output from the cross-modal encoder up to the current moment, t, and uses that to figure out the"
    },
    {
        "casual_text": "Pre-trained language models like ELMo, GPT, BERT, XLNet, and RoBERTa have made a big difference in improving NLP tasks. But even though they’ve boosted accuracy, these models need a lot of computing power and take more time to process, which can be a problem for apps that don’t have much resources or need quick responses. So, it would be great to find ways to make these models less heavy on the computer while still keeping the accuracy decent.",
        "formal_text": "Convert casual text to formal text: Pre-trained language models like ELMo, GPT, BERT, XLNet, and RoBERTa have made a big difference in improving NLP tasks. But even"
    },
    {
        "casual_text": "We're calling this model the context-ignorant or Context model, which is different from the earlier context-aware or +Context model. The results from these experiments on the development set are in Table 4. The PARSEVAL results for the development and test sets are in Tables 5 and 6. It looks like the reranked models did better than the basic generative model when it comes to F1. The reranked model that used extra-sentential context did better than the one that didn't on the development set, but not on the test set. Using Bikel's randomized parsing evaluation tool, we found that both reranking models did better than the baseline generative model in terms of recall and precision, with statistical significance. The context-ignorant reranker did better on recall (p  0.01), but not on precision (p = 0.42). However, the context-aware model had the best exact match scores in both the development and test sets.",
        "formal_text": "Convert casual text to formal text: We're calling this model the context-ignorant or Context model, which is different from the earlier context-aware or +Context model. The results from these experiments on"
    },
    {
        "casual_text": "We've already talked about why there are differences in how nouns and adjectives are mapped. Adverbs are just starting to get mapped, and verbs haven’t been touched yet.",
        "formal_text": "Convert casual text to formal text: We've already talked about why there are differences in how nouns and adjectives are mapped. Adverbs are just starting to get mapped, and verbs haven’t touched"
    },
    {
        "casual_text": "The JULIUS open-source speech recognition engine, which is designed for Large Vocabulary Continuous Speech Recognition (LVCSR), uses n-grams and context-dependent Hidden Markov Models (HMM) to turn spoken words into written text (Lee et al., 2008). How well it works depends on having the right language-specific stuff, like acoustic models, language models, and dictionaries. We’ve got some basic resources set up for English, German, Italian, and French, but they’re still pretty limited. To make things work better, we’ve also added online speech recognition for these languages using Google’s Speech API. This lets us run experiments with users and at the same time gather the data we need to improve and expand JULIUS’s language resources.",
        "formal_text": "Convert casual text to formal text: The JULIUS open-source speech recognition engine, which is designed for Large Vocabulary Continuous Speech Recognition (LVCSR), uses n-grams and context-dependent Hidden Mar"
    },
    {
        "casual_text": "Basically, we’re saying that people from ASEAN countries can go study in other places, learn about how things work, and see what other countries have done to succeed. Then, they can bring that knowledge back home to help their own country grow. In the end, this helps some of the poorer people in ASEAN countries because they get better access to development. So, we shouldn’t get rid of ASEAN. Our last point is about keeping the peace in the region. Southeast Asia has a lot of potential for conflict—some countries are buddy-buddy with China and might like the idea of China being super powerful, while others are closer to the U.S. Some countries are communist, others are capitalist. There’s been trouble in the past, like the conflict over East Timor, and there are still tensions simmering under the surface in many places. But one of the best ways to avoid fights between countries is to make sure everyone’s interests are tied together through trade. If everyone benefits from peace and loses out during conflict, it’s way less likely that a war will happen. That’s why we think ASEAN is a great tool for keeping the peace in Southeast Asia. So, because ASEAN fights against colonialism, helps with development, and promotes peace in the region, we shouldn’t disband it. Thanks!",
        "formal_text": "Convert casual text to formal text: Basically, we’re saying that people from ASEAN countries can go study in other places, learn about how things work, and see what other countries have done to succeed. Then, they can"
    },
    {
        "casual_text": "In this paper, we talk about the issues that come up from this kind of interaction and offer solutions using our logic-based transfer framework [8]. Our transfer model sees the transfer process as deduction; it creates the target linguistic description without messing with the source description. This is a big difference from the usual tree-transducer model, which slowly changes the source description and is really hard to manage.",
        "formal_text": "Convert casual text to formal text: In this paper, we talk about the issues that come up from this kind of interaction and offer solutions using our logic-based transfer framework [8]. Our transfer model sees the transfer process as deduction;"
    },
    {
        "casual_text": "A big goal in news captioning is to create captions that get the names and other specific details right. If you look at Table 8, you'll see that compared to Tell, Jo-GANIC+MSTR+NEE (auto) bumps up the named entity precision and recall scores by 5.77% and 4.65% on GoodNews, and by 8.63% and 6.39% on NYTimes800k. And if we use the best-case versions of our models, the results are even better.",
        "formal_text": "Convert casual text to formal text: A big goal in news captioning is to create captions that get the names and other specific details right. If you look at Table 8, you'll see that compared to Tell, Jo-GA"
    },
    {
        "casual_text": "We haven't decided on the exact form of p  yet. Based on a lot of research in MT, we're starting with a simple linear model.",
        "formal_text": "Convert casual text to formal text: We haven't decided on the exact form of p  yet. Based on a lot of research in MT, we're starting with a simple linear model."
    },
    {
        "casual_text": "So, S t  R m is the current hidden state, and m is the size of this hidden state. S i  R m is the hidden state from the previous time step i, while S t  R m is the result of the attention mechanism at time step t. Now, let's talk about CAN-biGRU: When we're trying to figure out the emotion in a conversation, we don't just look at what was said before the current sentence. We also consider what comes after it. That's because emotions tend to stay consistent over short periods during a chat. If we only focus on past stuff, it might be tricky to get the emotion right. But looking at future stuff can help us out. So, by using both past and future info, we can get a better overall picture. That's where Bidirectional GRU (biGRU) comes in. It helps us model the conversation by looking at the sequence of sentences both forward and backward. This way, we can pull out more useful context for emotion classification.",
        "formal_text": "Convert casual text to formal text: So, S t  R m is the current hidden state, and m is the size of this hidden state. S i  R m is the hidden state from the"
    },
    {
        "casual_text": "• To figure out which of the pre-set error types could happen in a specific sentence (basically, narrowing down the list of possible errors to check).",
        "formal_text": "Convert casual text to formal text: • To figure out which of the pre-set error types could happen in a specific sentence (basically, narrowing down the list possible errors to check). • To figure out which of the pre"
    },
    {
        "casual_text": "We're still growing the list of products that use MT for localization and adding more language pairs that Adobe licenses. At the same time, we're looking into other ways to use MT technology. Here's a quick rundown of some key use cases we're focusing on, highlighting the different needs each one has for an MT solution.",
        "formal_text": "Convert casual text to formal text: We're still growing the list of products that use MT for localization and adding more language pairs that Adobe licenses. At the same time, we're looking into other ways to use"
    },
    {
        "casual_text": "We mix our structure-aware uncertainty sampling with the bachelor recognizer to create the final acquisition function.",
        "formal_text": "Convert casual text to formal text: We mix our structure-aware uncertainty sampling to create the final acquisition function. Convert casual text to formal text: We mix our structure-aware uncertainty sampling to create the final acquisition function. Con"
    },
    {
        "casual_text": "In the parametric methods, the first approach gives all the hidden states in a layer the same window size to look at the sequence's context. The second approach, though, figures out a different window size for each hidden state.",
        "formal_text": "Convert casual text to formal text: In the parametric methods, the first approach gives all the hidden states in a layer the same window size to look at the sequence's context. The second approach, though, figures out a"
    },
    {
        "casual_text": "A 15% drop in errors (from 4.72% to 4.1%) might not sound like a huge deal at first, but trust me, cutting out that last 5% of mistakes is super tough. There are all sorts of issues like messy data, disagreements between people labeling things, and tricky long-distance connections between words that make it really hard. Check out some of the tricky examples: 39) maet 48-48 ovr k kr diy gy hai match 48-48 overs of do has been be-pres 'Match has been made of 48-48 overs'.",
        "formal_text": "Convert casual text to formal text: A 15% drop in errors (from 4.72% to 4.1%) might not sound like a huge deal at first, but trust me, cutting out that last 5% of mistakes is super tough."
    },
    {
        "casual_text": "We took a pre-trained GPT-2 model and tweaked it using around 1.2 million recipes (as mentioned in section 2.1) with a basic language modeling goal. This version doesn’t use the original recipe at all. Instead, it just uses the title and the ingredient list of the recipe we want to create as a starting point. From there, it generates the full recipe step by step.",
        "formal_text": "Convert casual text to formal text: We took a pre-trained GPT-2 model and tweaked it using around 1.2 million recipes (as mentioned in section 2.1) with a basic language modeling goal. This"
    },
    {
        "casual_text": "Our neural network setup has four layers. First up, there's the embedding layer that turns words in a sentence into word embeddings. Next, the context layer takes those word embeddings and uses either a recurrent or convolutional neural network to turn them into context-aware vector representations. After that, the summarization layer groups and pools these vectors to summarize the whole sentence. Finally, the output layer gives us the classification label for the relation type.",
        "formal_text": "Convert casual text to formal text: Our neural network setup has four layers. First up, there's the embedding layer that turns words in a sentence into word embeddings. Next, the context layer takes those word"
    },
    {
        "casual_text": "During fine-tuning, we only focus on the steps in a recipe that have a high alignment score and are paired up (check section 2.3 for more details). But when it's time to test the model, we use it to rewrite all the steps in the original recipe. Also, while fine-tuning, we use something called \"teacher forcing.\" This means that when we're rewriting step n, the context from the target recipe includes the actual true steps from 1 to (n-1). However, during the test phase, the context is based on the steps the model has already generated, from 1 to (n-1).",
        "formal_text": "Convert casual text to formal text: During fine-tuning, we only focus on the steps in a recipe that have a high alignment score and are paired up (check section 2.3 for more details). But when"
    },
    {
        "casual_text": "(3) Tämä nuori viehättävä tyttö, hänen vanha käyräjalkainen isänsä.",
        "formal_text": "(3) Tämä nuori viehättävä tyttö, hänen vanha käyräjalkainen isänsä."
    },
    {
        "casual_text": "For the i-th word in the current sentence, we use multi-head attention (Vaswani et al., 2017) to pull in context from the k-th context sentence.",
        "formal_text": "Convert casual text to formal text: For the i-th word in the current sentence, we use multi-head attention (Vaswani et al., 2017) to pull in context from the k-"
    },
    {
        "casual_text": "For a deeper dive, we zeroed in on three specific features: (11) case marker, (15) intra/inter, and (18) dependency. We then calculated precision (P), recall (R), and F-measure (F) for each combination of these features in our test cases. Table 4 compares the BiReg+F syn model and the FixRank+F syn model, which had the highest overall accuracy according to Table 3. One key difference between these two models is that the FixRank model performs better in terms of F-measure for directly dependent arguments, especially for ga arguments. When it comes to directly dependent ga arguments, the FixRank model has a better recall value while maintaining a similar precision value. On the flip side, for directly dependent arguments with other case markers, the BiReg model has better precision. We also looked at how many directly dependent arguments each model picked as the answer. The FixRank model selected 19,612 directly dependent arguments, with 13,751 of those being ga arguments. Meanwhile, the BiReg model picked 12,520 directly dependent arguments, including 7,208 ga arguments. These numbers indicate that the FixRank model is more likely to choose directly dependent arguments without much regard for their case markers.",
        "formal_text": "Convert casual text to formal text: For a deeper dive, we zeroed in on three specific features: (11) case marker, (15) intra/inter, and (18) dependency. We then calculated precision (P), recall ("
    },
    {
        "casual_text": "Okay, so softmax(•) is just a function that normalizes stuff along the last dimension, and c(0) is the relation-aware representation we get from the sentence s. After that, we combine this relation-aware representation c(0) with the original sentence representation s using a gate mechanism (like the one from He et al., 2016) and add a residual connection. We also throw in layer normalization (thanks to Ba et al., 2016) for good measure.",
        "formal_text": "Convert casual text to formal text: Okay, so softmax(•) is just a function that normalizes stuff along the last dimension, and c(0) is the relation-aware representation we get from the sentence s"
    },
    {
        "casual_text": "So, k is the size of the window, and we make sure any words outside the window are treated as zero vectors. Then, we multiply q_i by a convolution matrix W that’s in R(dc  (k  d)), where dc is the dimension of the sentence embeddings. Basically, this gives us the output of the convolution layer.",
        "formal_text": "Convert casual text to formal text: So, k is the size of the window, and we make sure any words outside the window are treated as zero vectors. Then, we multiply q_i by a convolution"
    },
    {
        "casual_text": "As mentioned earlier, regular semantic role labelers look at the whole sentence to make their decisions. But we’re trying something different—we want to assign semantic roles step by step as we go through the sentence. This means we’ll create a bunch of (maybe not fully complete) semantic dependency triples for each part of the sentence we’ve looked at so far. Keep in mind, not every word is linked to a predicate, so the set of triples won’t always change with each new word. Also, the triples might not be fully complete because we might not have seen the predicate or the argument yet (we call these predicate-incomplete or argument-incomplete triples).",
        "formal_text": "Convert casual text to formal text: As mentioned earlier, regular semantic role labelers look at the whole sentence to make their decisions. But we’re trying something different—we want to assign semantic roles step by step as we go through the"
    },
    {
        "casual_text": "So, for the same grammar, the edit machine we get has around 300K parts, with 4 states and 16,796 arcs. The average time it takes to process something is about 0.5 seconds. The setup of this 4-state edit machine is shown in Figure 6. Figure 6 shows the 4-edit machine.",
        "formal_text": "Convert casual text to formal text: So, for the same grammar, the edit machine we get has around 300K parts, with 4 states and 16,796 arcs. The average time it takes to process something is about 0.5"
    },
    {
        "casual_text": "Automatic metrics for judging translation quality have a lot of perks compared to human evaluation. They’re super quick and basically cost nothing to run, as long as you already have a reference translation. Plus, their results are consistent and reliable. Because of this, automatic metrics have been the go-to for MT evaluation over the past 20 years. Newer metrics that align better with human judgments keep popping up too. In this section, we’re diving into how automatic metrics are used in MT research, based on our annotations for A1 and A2. One thing that probably won’t surprise anyone is that BLEU is the most popular metric by far. A whopping 98.8% of the papers we looked at mention BLEU scores. As you can see in Figure 1, the number of papers using BLEU has stayed pretty steady over the years. However, back in the day, BLEU scores were often backed up by results from other metrics like TER and METEOR. Nowadays, not so much. Most papers—about 74.3%—only rely on BLEU to evaluate their MT systems, without any other metrics or human evaluation. That number jumps to 82.1% if we focus on just the years 2019 and 2020.",
        "formal_text": "Convert casual text to formal text: Automatic metrics for judging translation quality have a lot of perks compared to human evaluation. They’re super quick and basically cost nothing to run, as long as you already have a reference"
    },
    {
        "casual_text": "Theorem 5.1 (Soundness): Basically, if you get a goal configuration using LTF or LTL, it will always match up with a properly typed AM dependency tree.",
        "formal_text": "Convert casual text to formal text: Theorem 5.1 (Soundness): Basically, if you get a goal configuration using LTF or LTL it will always match up with a properly typed AM"
    },
    {
        "casual_text": "Deep learning has been really successful in a bunch of areas like computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al., 2013), and natural language processing tasks (Bahdanau et al., 2015; Kalchbrenner et al., 2014; Yih et al., 2014; Bitvai and Cohn, 2015). But, these deep models can get way too confident when dealing with messy or noisy data, which makes them easy targets for sneaky attacks (Nguyen et al., 2015; Tabacof and Valle, 2016). The main reason neural networks struggle with these sneaky changes is because they're built to work in a mostly linear way, which makes them easier to optimize but also more vulnerable. Fawzi et al. (2015) came up with a cool way to analyze how well classifiers can handle these sneaky changes and found that linear models usually don't do a great job with noisy stuff.",
        "formal_text": "Convert casual text to formal text: Deep learning has been really successful in a bunch of areas like computer vision (Krizhevsky et al., 2012), speech recognition (Graves et al.,"
    },
    {
        "casual_text": "We discovered that CHARAGRAM performs better on word similarity tasks when we use more character n-grams. This is covered in Section 4. Our top result came from the biggest model, SL999 Hill et al. 201452 Schwartz et al. (2015) 56 Faruqui and Dyer (2015) 58 Wieting et al. (2015) 66. 7 CHARAGRAM (large) 70. 6 (Luong et al., 2013), with SL999 for model selection. We got a Spearman's  of 47.1, which beats the 41.8 from Soricut and Och (2015) and is close to the 47.8 reported by Pennington et al. (2014), who trained on a 42 billion token corpus.",
        "formal_text": "Convert casual text to formal text: We discovered that CHARAGRAM performs better on word similarity tasks when we use more character n-grams. This is covered in Section 4. Our top result came from the biggest model,"
    },
    {
        "casual_text": "In Hebrew, when a verb has a sentential object, it's like \"n99.\" In German, it's \"kennen,\" and in French, it's \"conaitre.\"",
        "formal_text": "Convert casual text to formal text: In Hebrew, when a verb has a sentential object, it's like \"n99.\" In German, it's \"kennen,\" and in French, it's"
    },
    {
        "casual_text": "We're using the CL method to deal with the issue of VQA models not performing well when there's not much data available. Curriculum learning, which was introduced by Bengio and his team back in 2009, is all about controlling the sequence in which the model gets exposed to different data examples. The idea is to make the most out of the training samples by carefully managing the order in which the data is fed into the model.",
        "formal_text": "Convert casual text to formal text: We're using the CL method to deal with the issue of VQA models not performing well when there's not much data available. Curriculum learning, which was introduced by Bengio and his team back"
    },
    {
        "casual_text": "We're working on cutting down gender bias in CNN models trained on two datasets: Biosbias (from De-Arteaga et al., 2019) and Waseem (Waseem and Hovy, 2016). For Biosbias, the goal is to predict someone's occupation based on a bio paragraph, like figuring out if the person is a 'surgeon' (class 0) or a 'nurse' (class 1). The problem is, there's a big gender imbalance in these jobs, so the model often uses gender clues to make its guesses. This leads to mistakes, like classifying female surgeons or male nurses incorrectly. For Waseem, the task is detecting abusive language—deciding if a text is abusive (class 1) or not (class 0). Previous research found that this dataset has a strong bias against women (Park et al., 2018). Basically, texts about women are often flagged as abusive, even when they're not. We also tested the models trained on Waseem using another dataset called Wikitoxic (Thain et al., 2017) to see how well they work on different data. To measure gender bias, we used two metrics: false positive equality difference (FPED) and false negative equality difference (FNED) (Dixon et al., 2018). The lower these numbers are, the less biased the model is. We also tried the same experiments with bidirectional LSTM networks (BiLSTMs), but we had to adjust how we made the word clouds (check out Appendix C for details). Unfortunately, the results with BiLSTMs weren't as good as with CNNs.",
        "formal_text": "Convert casual text to formal text: We're working on cutting down gender bias in CNN models trained on two datasets: Biosbias (from De-Arteaga et al., 2019) and Wasee"
    },
    {
        "casual_text": "The rest of this paper is laid out like this: Section 2 talks about older methods that use either individual or shared user info. Section 3 compares how useful these two types of info are, using numbers. In Section 4, we bring in the Kappa statistic to check how consistent people are when they click on the same search query. We also try to show how web search is changing based on this. Section 5 looks at using individual user info as a topic after we’ve really dug into shared user info. Section 6 explores the idea of a personalization curve, trying to figure out which kinds of queries get the most out of individual user info. Finally, Section 7 wraps things up with conclusions and ideas for what’s next.",
        "formal_text": "Convert casual text to formal text: The rest of this paper is laid out like this: Section 2 talks about older methods that use either individual or shared user info. Section 3 compares how useful these two types of info are, using numbers"
    },
    {
        "casual_text": "Here’s the list in a more casual way: 0.1, -0.5, -3.2, 0.0, -0.1, -4.6, -1.4, -3.9, -4.0, -0.1, -0.8, 0.0, 0.0",
        "formal_text": "-0.1, -4.6, -1.4, -3.9, -4.0, -0.1, -0.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0,"
    },
    {
        "casual_text": "Basically, we check how similar a made-up state is to real ones. We do this by looking at how many real states are close to the made-up one. If there aren’t many real states nearby, it means the made-up one is pretty unique, which makes any mistakes in it more important.",
        "formal_text": "Convert casual text to formal text: Basically, we check how similar a made-up state is to real ones. We do this by looking at how many real states are close to the made-up one. If there aren"
    },
    {
        "casual_text": "Alright, so our main evaluation is based on a dataset that has questions and answers about images. Basically, if the generated caption backs up the correct answers most of the time, we think it's doing a good job and providing enough information.",
        "formal_text": "Convert casual text to formal text: Alright, so our main evaluation is based on a dataset that has questions and answers about images. Basically, if the generated caption backs up the correct answers most of the time,"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. We're looking at how the Transformer model does in two different situations: the \"common-practice\" setting and the \"wug test\"-like setting. In the \"common practice\" setting, we're basically using data from past shared tasks and related research (like the work by Cotterell et al. from 2016, 2017, McCarthy et al. in 2019, and Vylomova et al. in 2020). This data usually includes a good number of lemmas, and there's some overlap between the lemmas used in training and those in the test sets. We're using this shared task data to represent the common-practice setting. Now, in the \"wug test\" setting, things are a bit different. Here, we control the number of lemmas for training, but not the inflected forms (as explained in Section 2). The lemmas we ask the model to inflect are always new, never seen before. Surprisingly, the Transformer model performs really badly in the \"wug test\"-like setting, even though there's a lot of training data for languages like those from 2018 or the Niger-Congo languages, which have very regular and simple inflections. The performance is way worse than in the common-practice setting, even when there are seven times more training triples for languages like Finnish, Spanish, and Turkish (check out Figure 1 for details).",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way. We're looking at how the Transformer model does in two different situations: the \"common-practice\" setting and the \""
    },
    {
        "casual_text": "To get a better understanding of how things are going at the segment level, Figure 4 shows a comparison between our method and Thot for the first 300 segments of the Autodesk test set used in the multi-domain experiment. The plot highlights the differences in TER (Translation Edit Rate) between the MT output and our approach, as well as between MT and Thot for automatically post-edited segments. What we see is that our approach changes fewer segments compared to Thot. This is because our method only builds a model if it finds useful data in the knowledge base; otherwise, it leaves the MT segment as is. When Thot modifies these untouched MT segments, it often makes things worse instead of better (as you can see from the many negative peaks for Thot in Figure 4). This suggests that, compared to other online approaches, the output from our solution is more likely to be helpful for human translators. This benefit isn’t just about making post-editing easier or more enjoyable—it also saves time by providing better suggestions overall.",
        "formal_text": "Convert casual text to formal text: To get a better understanding of how things are going at the segment level, Figure 4 shows a comparison between our method and Thot for the first 300 segments of the Autodesk test set used"
    },
    {
        "casual_text": "Inspired by these findings, we decided to bring the idea of concreteness into the unsupervised neural L-PCFG model (Zhu et al., 2020) in two ways: at the word level and the phrase level. First off, at the word level, we added a prior that boosts dependency structures where the heads have higher word concreteness scores. These scores are human-rated numbers for common English words (Brysbaert et al., 2013). When we tested this on the English MSCOCO dataset, we found that using these word concreteness priors significantly improved the dependency induction performance compared to the original neural L-PCFG, increasing the directed attachment accuracy (DAS) by 50% (check out Section 4 for more details). We also looked into how this affects the predicted roots and noticed that the concreteness priors act like a \"shortcut\" that nudges more perceptible entities to become heads in the dependency parse. For instance, in the example shown in Figure 1, the CONCRETE L-PCFG picks the word \"fans\" as the root of the sentence because it's more tangible than the action \"observing.\" By including word concreteness priors for the root, the model ends up learning rules that prefer the lexical head with higher word concreteness.",
        "formal_text": "Convert casual text to formal text: Inspired by these findings, we decided to bring the idea of concreteness into the unsupervised neural L-PCFG model (Zhu et al., 2020) in two ways: at"
    },
    {
        "casual_text": "Word embeddings are created by training word2vec on a bunch of unlabeled data. For English, Catalan, German, and Spanish, we use the most recent Wikipedia data. For Chinese, we grab the raw text from the Xinhua news section (covering 2000-2010) in the fifth edition of Chinese Gigaword (LDC2011T13). To break down the Chinese text into individual words, we use the LTP toolkit (developed by Che et al. in 2010).",
        "formal_text": "Convert casual text to formal text: Word embeddings are created by training word2vec on a bunch of unlabeled data. For English, Catalan, German, and Spanish, we use the most recent"
    },
    {
        "casual_text": "We built the translation model just using the groups we got from sorting bilingual data into clusters.",
        "formal_text": "Convert casual text to formal text: We built the translation model just using the groups we got from sorting bilingual data into clusters."
    },
    {
        "casual_text": "Overall, these results show that PLM-ICD works well across different datasets and hits the best scores on multiple metrics for both MIMIC-3 and MIMIC-2.",
        "formal_text": "Convert casual text to formal text: Overall, these results show that PLM-ICD works well across different datasets and hits the best scores on multiple metrics for both MIMIC-3 and MIMIC-2. Overall, these results show that"
    },
    {
        "casual_text": "Okay, so let’s break this down in simpler terms. We’re dealing with something called a syntactic category, and if we work through it, we can create a graph that’s almost like a proof structure—it just needs these special \"axiom links\" to be complete. In this setup, the parsing process works like this: you take the syntactic categories of the words and their order, then you add these non-crossing axiom links to make it into a proper proof structure or proof net. This means you’re basically proving that a sentence like \"John lives in Paris\" makes sense, given the types of words in a certain order. To check if \"John lives in Paris\" is a valid sentence according to the rules in Table 3 (focusing on the first two columns), you need to find the right axiom links between the parts of the sentence shown in Figure 1(a). If you do it right, the proof structure will be correct. Figure 1(b) shows that it’s possible to do this successfully, but there’s a catch: in the proof net, the order of the syntactic categories is flipped compared to the actual order of the words in the sentence. And if you look at Figure 1(c), it shows that a sentence like \"John lives Paris in\" can’t be parsed correctly because it doesn’t work with the rules.",
        "formal_text": "Convert casual text to formal text: Okay, so let’s break this down in simpler terms. We’re dealing with something called a syntactic category, and if we work through it, we can create a"
    },
    {
        "casual_text": "Adversarial examples have been a hot topic in NLP lately, but most of the research has been about messing with models that deal with meaning—like changing sentiment analysis or question-answering results without actually altering what the text says. Surprisingly, no one seems to have looked into how these tricks might work on syntactic tasks, like dependency parsing. That’s where we come in. We’re focusing on neural network-based dependency parsers and asking: Can we create examples that confuse these parsers without messing up the original sentence structure? And more importantly, can we make these parsers tougher and less vulnerable to such attacks?",
        "formal_text": "Convert casual text to formal text: Adversarial examples have been a hot topic in NLP lately, but most of the research has been about messing with models that deal with meaning—like changing sentiment analysis or question-ans"
    },
    {
        "casual_text": "CSCs cover general cases and syntactic rules. What sets our model apart is that it uses cases for generation, which isn't something most other generators do—they mostly just rely on syntactic rules.",
        "formal_text": "Convert casual text to formal text: CSCs cover general cases and syntactic rules. What sets our model apart is that it uses cases for generation, which isn't something most other generators do—they mostly just"
    },
    {
        "casual_text": "It's clear that our vanilla depLCNN+NS, which doesn't use any extra lexical features, still beats the best systems out there—MVRNN+ and CNN+—by a big margin. Those systems actually use extra lexical features, but our approach with dependency paths works even better. When we add similar lexical features to our model, it gets even stronger, improving by 1.6%. This makes it way better than any other system we've seen.",
        "formal_text": "Convert casual text to formal text: It's clear that our vanilla depLCNN+NS, which doesn't use any extra lexical features, still beats the best systems out there—MVRNN+ and CNN"
    },
    {
        "casual_text": "Thanks to the big jump in computing power over the past few years, neural networks are starting to be used in clustering-based OpenRE tasks to tackle some of these problems. For example, you can check out studies by Simon et al. (2019), Hu et al. (2020), and Gao et al. (2020). But even with these advancements, most of these methods stick to unsupervised or self-supervised approaches and don’t really take full advantage of the high-quality human-labeled datasets we have now. There have been some unconventional methods that have shown great results, like the one by Zhang et al. (2021), but they rely on extra knowledge, which makes it tricky to compare them directly. On the other hand, there’s another supervised method that learns similarity metrics from labeled data and then transfers that relational knowledge to open-domain scenarios. This method is called Relational Siamese Networks (RSNs), as introduced by Wu et al. (2019). The thing is, RSNs focus on learning a similarity classifier instead of directly building relational representations. This might affect how well or efficiently clustering works in downstream tasks.",
        "formal_text": "Convert casual text to formal text: Thanks to the big jump in computing power over the past few years, neural networks are starting to be used in clustering-based OpenRE tasks to tackle some of these problems. For example, you can"
    },
    {
        "casual_text": "This paper introduces a new approach to NER where we get some KB tags—like official names and types—during both training and when we're using the model. We looked into how to make the most of this extra info and found that CRF models can really make use of this non-traditional supervision. Plus, combining these models with the KB helps even more, especially when we beef up document gazetteers to fully take advantage of those KB tags.",
        "formal_text": "Convert casual text to formal text: This paper introduces a new approach to NER where we get some KB tags—like official names and types—during both training and when we're using the model. We looked into"
    },
    {
        "casual_text": "In the \"vp\" clause, we changed our original \"Pl\" to \"P\" and added a new \"PI\" variable. This \"PI\" will become \"(loves John Y)\" if the \"=canny\" goal works out. The \"Y\" is still up in the air, but we can tell it'll be filled in by the next \"np(Y, (loves John Y), P)\" goal. It's pretty clever of \"transv\" to send back a variable like this so it matches up with one that's already been named.",
        "formal_text": "Convert casual text to formal text: In the \"vp\" clause, we changed our original \"Pl\" to \"P\" and added a new \"PI\" variable. This \"PI\" will become \"(loves"
    },
    {
        "casual_text": "We’re mainly looking at a few key traits of questions when it comes to graph queries: how complex their structure is, what kind of function they serve, how common they are, how they’re paraphrased, and how many answers they can have. To measure structure complexity, we count the number of edges, but we cap it at 3. For commonness, we only consider questions where log 10 (p(q)) is at least 40 (check out Eq. 1 for more details). As mentioned in Section 7.4.2, these kinds of questions are already super tough for current QA systems. But hey, our framework can still create questions with different distributions of these traits. You can find some stats in Table 1, and even more detailed ones in Appendix D. There are also a few example questions in Table 2. When it comes to paraphrasing, we’re dealing with stuff like commands (the first example) and \"Wh\" questions, light verbs (\"Who did nine eleven?\"), and changes in sentence structure (\"The September 11 attacks were carried out with the involvement of what terrorist organizations?\"). At the entity level, we test how well QA systems handle abbreviations (\"NYC\" for New York City), world knowledge (\"Her Majesty the Queen\" for Elizabeth II), or even common typos (\"Shakspeare\" for William Shakespeare). Numbers and dates are also pretty common, like \"Which computer operating system was released on Sept. the 20th, 2008?\"",
        "formal_text": "Convert casual text to formal text: We’re mainly looking at a few key traits of questions when it comes to graph queries: how complex their structure is, what kind of function they serve, how common they are, how they"
    },
    {
        "casual_text": "We divided our features into four categories: meta data, text, markup, and language features. You can check out Table 1 for a quick summary and explanation of all these features. The way we organize and classify these features is pretty similar to what Adler et al. did in 2011. We borrowed some features from Adler et al. (2011), Javanmardi et al. (2011), and Bronner and Monz (2012), and you can see which ones in Table 1. We calculate these features based on the parts of the text that get edited. For example, if e i is the edited part in the previous version (r v1), we call that t v1. The same part in the new version (r v) is called t v. In cases where something is inserted, t v1 is empty, and if something is deleted, t v is empty. For relocations, t v1 and t v are the same. Table 1 shows the value of each feature for an example edit from Figure 1, where a link [[Dactyl|Dactylic]] is modified by adding more details to the target. For spell-checking, we use Jazzy dictionaries for both British and US-American English. And for detecting markup elements, we rely on the Sweble Wikitext parser, as mentioned by Dohrn and Riehle in 2011.",
        "formal_text": "Convert casual text to formal text: We divided our features into four categories: meta data, text, markup, and language features. You can check out Table 1 for a quick summary and explanation of all these features. The way we"
    },
    {
        "casual_text": "To run experiments in zero-shot mode, we basically follow the setup from (Liu et al., 2020b). First, we mix samples from the other 6 domains for training. Then, we divide the samples in the target domain into two groups: 500 samples for validation and the rest for testing. For few-shot experiments (with 50 samples), we add 50 more samples from the target domain to the training set. We tweak all the hyperparameters using the validation set and then report the F1-score on the test set. Our best results come when s is set to 0.15, set to 0.1, and M is set to 2.",
        "formal_text": "Convert casual text to formal text: To run experiments in zero-shot mode, we basically follow the setup from (Liu et al., 2020b). First, we mix samples from the other 6 domains for training"
    },
    {
        "casual_text": "You can grab predicates one at a time using this interface, or you can just download them all at once from the tagging lexicon tool.",
        "formal_text": "Convert casual text to formal text: You can grab predicates one at a time using this interface, or just download them all at once from the tagging lexicon tool. Convert casual text to formal text: You"
    },
    {
        "casual_text": "This setup needs an interface that can handle a few specific tasks. First, any queries the grammar writer explicitly mentions need to be sent to DATR. Then, each of these queries, along with their \"resulthag\" values, has to be converted into a PATR path equation (which is kind of like a description of a feature structure) and sent to the PATR system. The tricky part about this whole process is that for every unique PATR path, you need to know the exact DATR query that goes with it. It might sound easy to just figure out which paths are defined for a particular node, but that doesn't work because of inheritance—basically, the whole network could be involved. So, in practice, all the PATR structures (except for the basic values) have to be defined twice: once in the DATR statements and again in the queries. This duplication is a pain, and it can't be avoided unless you set up types for the feature structures that can be used when writing the queries.",
        "formal_text": "Convert casual text to formal text: This setup needs an interface that can handle a few specific tasks. First, any queries the grammar writer explicitly mentions need to be sent to DATR. Then, each of these queries,"
    },
    {
        "casual_text": "The rest of the paper goes like this: In Section 2, we talk about related work, which includes collecting parallel corpora and some English-Indonesian machine translation methods. Section 3 covers the datasets we used for training and testing. Section 4 explains the advanced and basic machine translation techniques we used for our comparison. Section 5 goes over our experiment setup, the results we got, and what we learned from them. Lastly, Section 6 wraps up the paper and mentions some ideas for future work.",
        "formal_text": "Convert casual text to formal text: The rest of the paper goes like this: In Section 2, we talk about related work, which includes collecting parallel corpora and some English-Indonesian machine translation methods. Section 3 covers the"
    },
    {
        "casual_text": "We've got the solution to the GNMTF optimization problem from equation 7, and we're laying it out as a theorem here. The nitty-gritty details about the optimization theory will be covered in the next section.",
        "formal_text": "Convert casual text to formal text: We've got the solution to the GNMTF optimization problem from equation 7, and we're laying it out as a theorem here. The nitty-grr"
    },
    {
        "casual_text": "So, Prob(POSilx) is basically the chance that the word x fits into the category POSi. We figure this out using the training data we already have.",
        "formal_text": "Convert casual text to formal text: So, Prob(POSilx) is basically the chance that the word x fits into the category POSi. We figure this out using training data we already have."
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. First, \"n_i\" and \"n_j\" represent the number of documents where the English and Spanish words show up, respectively. \"n\" is just the total number of documents we're looking at. Now, when we say a word occurs in a document, we’re being a bit picky. We only count it as occurring if it appears more than its usual frequency across all the documents. As for the Log-likelihood ratio and MI (Mutual Information), they’re pretty similar—they just use different constants. Since they’re so alike, we decided to stick with MI for our experiments.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. First, \"n_i\" and \"n_j\" represent the number of documents where the English and Spanish words show up, respectively"
    },
    {
        "casual_text": "In this paper, we came up with a fresh approach to phrase-based decoding that uses several preordering options. Our method works better than older phrase-based machine translation systems and doesn't need any fancy reordering during decoding.",
        "formal_text": "Convert casual text to formal text: In this paper, we came up with a fresh approach to phrase-based decoding that uses several preordering options. Our method works better than older phrase-based machine translation systems and doesn'"
    },
    {
        "casual_text": "So, like we said earlier, we end up with these final versions of the sentences, S 1, S 2, . . . , S t, . . . , S N . After that, we use a fully-connected layer and a softmax layer to figure out the emotion for each sentence in the conversation.",
        "formal_text": "Convert casual text to formal text: So, like we said earlier, we end up with these final versions of the sentences, S 1, S 2, . . . , S t, . ."
    },
    {
        "casual_text": "Here, len refers to the output sequence length we're aiming for. LRPE and LDPE should be able to create sentences of any length, even if the exact length wasn't part of the training data. Takase and Okazaki (2019) took a different approach, focusing on character-based length constraints for summarization, meaning they limited the number of characters in the summary.",
        "formal_text": "Convert casual text to formal text: Here, len refers to the output sequence length we're aiming for. LRPE and LDPE should be able to create sentences of any length, even if the exact"
    },
    {
        "casual_text": "Even though models like BERT have done really well in a lot of tasks, they have a couple of issues when it comes to joint entity relation extraction. First, the training methods used for these models aren’t really designed for this specific task. For example, BERT and similar models don’t focus on the kind of entity-related knowledge that’s super important for finding entities and their relationships. Second, these models only give you ready-made representations for words and sentences, but not for entities or pairs of entities. To get those, you have to add extra parameters during the fine-tuning process, which can actually make things worse and affect how well the model performs on the joint extraction task.",
        "formal_text": "Convert casual text to formal text: Even though models like BERT have done really well in a lot of tasks, they have a couple of issues when it comes to joint entity relation extraction. First, the training methods used for these"
    },
    {
        "casual_text": "We're looking at four recent models: PRPN (by Shen et al., 2018a), ON-LSTM (also by Shen et al., 2019), DIORA (from Drozdov et al., 2019), and Compound PCFG (by Kim et al., 2019a).",
        "formal_text": "Convert casual text to formal text: We're looking at four recent models: PRPN (by Shen et al., 2018a), ON-LSTM (also by Shen et al., 2019),"
    },
    {
        "casual_text": "Q: Hey, I was wondering, who started Microsoft? P: Oh, I remember from a book that Bill Gates is the one who founded it.",
        "formal_text": "Convert casual text to formal text: Q: Hey, I was wondering, who started Microsoft? P: Oh, I remember from a book that Bill Gates is the one founded it. Q: Hey, I was wondering, who"
    },
    {
        "casual_text": "We use a BiLSTM layer on the sequence of sentence embeddings. Then, we slap a conditional random field (CRF) on top of the BiLSTM to make the final prediction. The loss function for the state tracking module is set up like this:",
        "formal_text": "Convert casual text to formal text: We use a BiLSTM layer on the sequence of sentence embeddings. Then, we slap a conditional random field (CRF) on top of the BiLS"
    },
    {
        "casual_text": "While tweaking the model might make it better at predicting stuff based on the pre-trained version, it would also blend norm-based info with individual corpus data. Instead, this study was all about comparing norm-based stuff to strictly individual corpora. So, we’re saving that extra tweak for later.",
        "formal_text": "Convert casual text to formal text: While tweaking the model might make it better at predicting stuff based on the pre-trained version, it would also blend norm-based info with individual corpus data. Instead, this study"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: Basically, we're looking at how something changes over time, specifically with weights (w) at different points (t and t-1). The main idea is that the change in a particular weight (w_r at time t) is influenced by its own value and the average of all the weights from the previous time step (t-1). The  (lambda) is just a constant that scales things up or down. So, in plain English: The change in weight w_r at time t is equal to  times the sign of w_r at time t, minus the average of all the weights from time t-1.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: Basically, we're looking at how something changes over time, specifically with weights (w) at different points (t and"
    },
    {
        "casual_text": "The loss function in MGF has two parts: one for AM and another for APE.",
        "formal_text": "Convert casual text to formal text: The loss function in MGF has two parts: one for AM and another for APE. Convert casual text to formal text: The loss function in MGF has two parts: one for AM and"
    },
    {
        "casual_text": "Pereira (from [Percira, 1985]) came up with a way to do structure-sharing by letting the result graph share info with the original graph by storing stuff in something called the 'environment'. But this method has a log(d) overhead, where d is the number of nodes in the graph. This overhead happens when you try to rebuild the whole graph from the 'skeleton' and the updates in the 'environment'. In the new system, though, since the arcs point directly to the original graph structures, there's no extra overhead when accessing nodes. Plus, during unification, charges are called way more times than needed—like 3299 times for sentence 9 when unify-dg was only called 480 times.",
        "formal_text": "Convert casual text to formal text: Pereira (from [Percira, 1985]) came up with a way to do structure-sharing by letting the result graph share info with the original graph by storing stuff in something"
    },
    {
        "casual_text": "A recent study looked at historical English letters and compared different LSTM setups. It found that bi-directional recurrent neural networks (BRNN) work better than one-directional RNNs. But using different attention models or making the architecture deeper didn’t help improve the results. Adding extra data, like social metadata or century info, actually made the accuracy worse. The study suggests that post-processing is the best way to improve a character-level NMT normalization model. This same approach has also been used successfully for OCR post-correction (Hämäläinen and Hengchen, 2019).",
        "formal_text": "Convert casual text to formal text: A recent study looked at historical English letters and compared different LSTM setups. It found that bi-directional recurrent neural networks (BRNN) work better than one-directional RNN"
    },
    {
        "casual_text": "ij represents how connected token i is in the final graph. Check out Figure 2, which explains how a graph convolutional network works for relation extraction. On the left, you'll see the whole setup, and on the right, there's a closer look at how the graph convolution is calculated for the word \"relative\" to make things clearer. There's also a complete unlabeled dependency parse of the sentence included for comparison.",
        "formal_text": "Convert casual text to formal text: ij represents how connected token i is in the final graph. Check out Figure 2, which explains how a graph convolutional network works for relation extraction. On the left, you'"
    },
    {
        "casual_text": "[Judgment] Yeah, it's true, but there are exceptions—like some people with disabilities might have fewer or more fingers.",
        "formal_text": "Convert casual text to formal text: [Judgment] Yeah, it's true, but there are exceptions—like some people might have fewer or more fingers. Convert casual text to formal text: [J"
    },
    {
        "casual_text": "• Union: Train a CRF with lots of features using both the source and target training data combined.",
        "formal_text": "• Union: Train a CRF with lots of features using both the source training data combined."
    },
    {
        "casual_text": "We've got a predicted syntactic dependency tree called y syn (check out the bottom part of Figure 1(a)). The main thing we're trying to do is figure out the semantic dependencies for each predicate p i (see the top part of Figure 1(a)). These dependencies help us figure out which arguments go with each predicate and what roles they play.",
        "formal_text": "Convert casual text to formal text: We've got a predicted syntactic dependency tree called y syn (check out the bottom part of Figure 1(a)). The main thing we're trying to do is figure"
    },
    {
        "casual_text": "The PosEdiOn analyzer can handle a bunch of files instead of needing a full PosEdiOn project. You can do this by heading over to the Files tab. There, you can pick the raw MT files (use the --raw option), the post-edited ones (--ped), and if you want, you can also add reference files (--refs) to calculate HTER, HBLEU, and HEd values. If you include the reference files, it’ll also give you TER, BLEU, and Ed scores. This setup lets you use the PosEdiOn analyzer without having to rely on the whole PosEdiOn tool.",
        "formal_text": "Convert casual text to formal text: The PosEdiOn analyzer can handle a bunch of files instead of needing a full PosEdiOn project. You can do this by heading over to the Files tab"
    },
    {
        "casual_text": "Different traditions and communities have their own ways of understanding and talking about semantic graphs, even when it comes to how they draw these graphs. This variety can be pretty confusing, but the MRP task series aims to bring some order to it all by creating more consistent terms and guidelines. This should help everyone navigate this complex and diverse field more easily. Below, you'll find some semi-formal definitions of key concepts related to graphs. These definitions are designed to work across all the different frameworks involved in the shared task.",
        "formal_text": "Convert casual text to formal text: Different traditions and communities have their own ways of understanding and talking about semantic graphs, even when it comes to how they draw these graphs. This variety can be pretty confusing, but the MRP task"
    },
    {
        "casual_text": "In both situations, the noun phrase (NP) either decides or has to match the overall structure. Similarly, when we're trying to find a verb phrase, we need to figure out which token (like a variable name or constant) stands for the subject (if the verb phrase is part of a sentence, or S) or the main noun (if the verb phrase is acting as a relative clause). This is done by using the SENDR function to set the subJvar register in the sub-computation to the right value.",
        "formal_text": "Convert casual text to formal text: In both situations, the noun phrase (NP) either decides or has to match the overall structure. Similarly, when we're trying to find a verb phrase, we need to figure"
    },
    {
        "casual_text": "(6) Another classic by Grisberg, I’m a huge fan of Stevie—she’s one of the best R&B singers I know. Darwin Halstead introduced me to her, so if you’re a fan too, you should definitely grab this DVD. It’s awesome and totally amazing. This woman really brings a heartfelt vibe to her R&B music. Contradiction: There’s one review (7) that seems a bit mixed up. The person says their husband isn’t a big fan of the product, but then also says he loves it. Kind of confusing, right?",
        "formal_text": "Convert casual text to formal text: (6) Another classic by Grisberg, I’m a huge fan of Stevie—she’s one of the best R&B singers I know. Darwin Halstead introduced"
    },
    {
        "casual_text": "A metric C(•) is considered tight if it's tight for every . If it's only tight for some  but not all, then it's semi-tight.",
        "formal_text": "Convert casual text to formal text: A metric C(•) is considered tight if it's tight for every . If it's only tight for some  but not all, then it's semi-"
    },
    {
        "casual_text": "The localist connectionist models [2; 20; 5; 1; 19; 7] offer better ways to handle reasoning and make the parsing process more understandable when it comes to how humans actually do it. But, these networks have to be carefully designed for each specific case.",
        "formal_text": "Convert casual text to formal text: The localist connectionist models [2; 20; 5; 1; 19; 7] offer better ways to handle reasoning and make the parsing process more understandable when it comes to how humans"
    },
    {
        "casual_text": "The results show that our model works for sentence-level DRTS parsing too.",
        "formal_text": "Convert casual text to formal text: The results show that our model works for sentence-level DRTS parsing too. Convert casual text to formal text: The results show that our model works for sentence-level DRTS"
    },
    {
        "casual_text": "Basically, log loss can be really sensitive to bad or messy data. That's because it expects the model to give high probabilities to all possible options. The problem is, log loss doesn't have a cap. If the model accidentally gives zero probability to even one option, the whole thing blows up and the loss becomes infinite.",
        "formal_text": "Convert casual text to formal text: Basically, log loss can be really sensitive to bad or messy data. That's because it expects the model to give high probabilities to all possible options. The problem is, log loss doesn"
    },
    {
        "casual_text": "Aspect: Palestinian Summary: The Palestinian foreign minister said, \"The world is one step closer to putting an end to a long period of people getting away with bad stuff and unfairness.\" The Palestinian Authority just joined the International Criminal Court as its 123rd member. This move means the court can now look into any alleged crimes that happened in Palestine since June.",
        "formal_text": "Palestinian foreign minister said, \"The world is one step closer to putting an end a long period of people getting away with bad stuff and unfairness.\" The Palestinian Authority just joined the International Criminal Court as its 123rd member."
    },
    {
        "casual_text": "Thanks to everyone—authors, speakers, members of the programme committee, reviewers, secondary reviewers, and attendees—for helping make EAMT-2011 a success. We hope you enjoy the programme we’ve put together. We know it’s packed, but we think it’ll be worth your time!",
        "formal_text": "Convert casual text to formal text: Thanks to everyone—authors, speakers, members of the programme committee, reviewers, secondary review, and attendees—for helping make EAMT-2011 a success! We hope you enjoy the"
    },
    {
        "casual_text": "The numbers are: 5.96  0.21 (0.00  0.00), 52.68  0.43 (0.00  0.00), 68.95  0.47 (0.00  0.00) for LAV N, and 30.11  1.27 (2.00  1.37), 85.41  1.07 (48.50  4.00), 94.83  0.53 (83.40  4.37). The blue bars on the left show how well our system did, and the green bars on the right show the baseline system's performance. The graphs break down the accuracy for tables with 1, 2, 3, 4, and more than 4 forms.",
        "formal_text": "Convert casual text to formal text: The numbers are: 5.96  0.21 (0.00  0.00), 52.68  0.43 (0.00  0.00), 68.95  0.47 (0.00"
    },
    {
        "casual_text": "So, like, the stuff we talked about earlier? It can be explained using the example in Figure 1. Let's say we have a sentence in Chinese that says \"",
        "formal_text": "Convert casual text to formal text: So, like, the stuff we talked earlier? It can be explained using the example in Figure 1. Let's say we have a sentence in Chinese that says \" \" and that says"
    },
    {
        "casual_text": "The table pointed out that the semantic feature set F sem doesn't perform well, except when dealing with intersentence cases using the BiReg model. We just automatically tagged the arguments with their semantic categories. Like Taira et al. (2008) did, it might be worth checking if manually tagging the semantic categories could make F sem work better.",
        "formal_text": "Convert casual text to formal text: The table pointed out that the semantic feature set F sem doesn't perform well, except when dealing with intersentence cases using the BiReg model. We just automatically tagged the arguments with their"
    },
    {
        "casual_text": "In Fig. 2, you can see that our method, which uses neural networks, is split into two parts. The first part, at the bottom, is the joint encoder. It grabs feature representations for both EClass and ECause instances. Then, the second part, at the top, is the linear decoder. It takes those representations and assigns labels to the instances based on them.",
        "formal_text": "Convert casual text to formal text: In Fig. 2, you can see that our method, which uses neural networks, is split into two parts. The first part, at the bottom, is the joint encoder. It grabs feature"
    },
    {
        "casual_text": "The rest of this paper goes like this: Section 2 gives some background info and talks about related research. Section 3 explains the approach we’re proposing. Section 4 covers the experiments we did. Section 5 wraps things up with conclusions and ideas for what’s next.",
        "formal_text": "Convert casual text to formal text: The rest of this paper goes like this: Section 2 gives some background info and talks about related research. Section 3 explains the approach we’re proposing. Section 4 covers the experiments we did."
    },
    {
        "casual_text": "When it comes to comparing translated and non-translated texts, we noticed that the p-values—which are basically numbers that tell us how likely it is that two sets of texts are different—are usually smaller for translated pairs than for non-translated ones. We checked this using both the t-test and the chi-square test. Smaller p-values mean there's a higher chance that the two texts being compared are actually different. This pattern holds true for most things we looked at: how fancy the words are (lexical richness), how much of the text is made up of actual words (lexical density), how long the sentences are, and how many sentences are simple. For the first two—lexical richness and density—translated texts are more different from each other in two cases, but in one case (MTS - TT), it's actually the opposite. However, when we looked at discourse markers—those little words or phrases that help guide the conversation—translated texts were more similar to each other, except for the MTP - MTS pair.",
        "formal_text": "Convert casual text to formal text: When it comes to comparing translated and non-translated texts, we noticed that the p-values—which are basically numbers that tell us how likely it is that two sets of texts are"
    },
    {
        "casual_text": "This paper introduces a system for Named Entity (NE) recognition that relies solely on the information found within lists of names. Section 3 shows how these lists can be created automatically from text that’s been labeled. Sections 4 and 5 talk about experiments where these automatically generated lists are used and compared to lists made by hand. The next section dives deeper into the NE task itself.",
        "formal_text": "Convert casual text to formal text: This paper introduces a system for Named Entity (NE) recognition that relies solely on the information found within lists of names. Section 3 shows how these lists can be created automatically from"
    },
    {
        "casual_text": "First, we're going to compare our English neural network models for relation extraction (RE) with the best models out there using the ACE05 English dataset. This dataset covers six different areas: broadcast conversation (bc), broadcast news (bn), telephone conversations (cts), newswire (nw), usenet (un), and webblogs (wl). We're following the same setup as in some earlier studies (Plank and Moschitti, 2013; Gormley et al., 2015; Nguyen and Grishman, 2016). They used news articles (which is a mix of bn and nw) for training, half of the bc data for development, and the rest for testing.",
        "formal_text": "Convert casual text to formal text: First, we're going to compare our English neural network models for relation extraction (RE) with the best models out there using the ACE05 English dataset. This dataset covers six different areas: broadcast"
    },
    {
        "casual_text": "Sorry, we can’t go into more examples because we’re running out of room.",
        "formal_text": "Convert casual text to formal text: Sorry, we’re running out of room."
    },
    {
        "casual_text": "Alright, so we've got a target and a relative, and we need to figure out how their parts-of-speech (POS) N-grams and positions are distributed. Using a labeled corpus would be way too limited for this, so instead, we turn to the much bigger, unlabeled Google Ngrams corpus (shoutout to Franz and Brants, 2006).",
        "formal_text": "Convert casual text to formal text: Alright, so we've got a target and a relative, and we need to figure out how their parts-of-speech (POS) N-grams and positions are"
    },
    {
        "casual_text": "It looks like using TPD instead of UPD as the noise distribution for training the NNJM with NCE can make the training process faster and slightly improve performance. However, things are different for the BNNJM. Using different noise distributions really impacts translation performance here. The BNNJM with UPD doesn’t do better than the baseline system, probably because it uses fewer noise samples during training. On the other hand, the BNNJM with TPD performs really well, even better than the NNJM with TPD on Chinese-to-English and French-to-English translation tasks.",
        "formal_text": "Convert casual text to formal text: It looks like using TPD instead of UPD as the noise distribution for training the NNJM with NCE can make the training process faster and slightly improve performance. However, things are different for"
    },
    {
        "casual_text": "We've come up with a new type of loss function that combines both generative and discriminative approaches. This helps NMT models learn better by using \"unlearned\" training data. By carefully picking and choosing which data to focus on and adding some extra, targeted losses, we can make the MT model even more effective.",
        "formal_text": "Convert casual text to formal text: We've come up with a new type of loss function that combines both generative and discriminative approaches. This helps NMT models learn better by using \"unlearned\" training data."
    },
    {
        "casual_text": "Okay, let's break this down into something simpler: We're talking about a model here, right? It's like a system that uses some steps to process stuff. First, it takes in some data (let's call it \"d\") and then it does a bunch of calculations with things like W, V, Q, and K. These are just symbols representing different parts of the process. Then, it moves on to another part of the model, still working with that data \"d.\" This time, it uses T, V, Q, and K again, but in a slightly different way. After that, it does something called \"softmax()\" which is just a fancy way of turning numbers into probabilities. Finally, it spits out some numbers like 2, 3, 3, 5, and 3. These are probably results or outputs from the model. So, in short, this is just a description of how a model processes data and gives us some numbers at the end.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down into something simpler: We're talking about a model here, right? It's like a system that uses some steps to process stuff. First,"
    },
    {
        "casual_text": "Here's a simpler version: A cool example (that should make anyone who's worried about figuring out the rules feel better).",
        "formal_text": "Convert casual text to formal text: Here's a simpler version: A cool example (that should make anyone who's worried about figuring out the rules feel better). Convert casual text to formal text: Here's"
    },
    {
        "casual_text": "Since you can only visit each node once, there are a few rules you gotta follow.",
        "formal_text": "Convert casual text to formal text: Since you only visit each node once, there are a few rules gotta follow. Convert casual text to formal text. Convert casual text to formal text. Convert casual text to formal"
    },
    {
        "casual_text": "We also check out how our model stacks up against a few other neural deep learning methods for AES:",
        "formal_text": "Convert casual text to formal text: We also check out how our model stacks out against a few other neural deep learning methods for AES: We also check out how our model stacks up against a few other neural deep learning"
    },
    {
        "casual_text": "Swimming is super beneficial, right? It’s got loads of benefits. You know, infinitival verbs and verbal nouns look the same, so sometimes it’s tricky to tell them apart. But when a verbal noun is in the oblique form and followed by a postposition, like in example 6, it’s easier to figure out. The real challenge is when it’s in the direct form. In example 7, the verb \"jn\" is part of a verb group (VG), so it’s tagged as an infinitival verb. But in example 8, it’s used as a verbal noun and even has a possessive pronoun, \"mer,\" modifying it. This makes it confusing—is it a noun or a verb? Well, if an infinitival verb is right before a possessive pronoun or a genitive postposition, like in 8, it should be tagged as a verbal noun. And this info can help the POS tagger do its job better.",
        "formal_text": "Convert casual text to formal text: Swimming is super beneficial, right? It’s got loads of benefits. You know, infinitival verbs and verbal nouns look the same, so sometimes it’s tricky to"
    },
    {
        "casual_text": "We’ve got two main systems in our experiments: the regular HPB SMT baseline and the CCG-augmented HPB SMT baseline. The CCG-augmented one uses single-category CCG labels and adds some strict syntactic rules (shoutout to Almaghout et al., 2010). For the regular HPB SMT baseline, we used the Moses Chart Decoder. We also used the GIZA++ toolkit to handle word and phrase alignment, and we went with the \"grow-diag-final\" method for refinement (thanks, Koehn et al., 2003). We set both the maximum phrase length and rule span to 12 words. For decoding, the chart maxes out at 20 words, and anything beyond that only uses glue grammar rules. The hierarchical rules we extracted can have up to 2 nonterminals. To fine-tune everything, we did minimum error rate training (props to Och, 2003). For the language model, we used a 5-gram trained on the target side of the parallel corpus with the SRILM toolkit and modified Kneser-Ney smoothing. The CCG-augmented HPB system also runs on the Moses Chart Decoder, which has a feature to pull syntax-augmented rules from annotated data. We kept the same rule extraction and decoding settings as the regular HPB baseline. For the CCG-augmented experiments, we used the CCG parser from the C&C tools to handle the training data and combine CCG categories during glue grammar rule application.",
        "formal_text": "Convert casual text to formal text: We’ve got two main systems in our experiments: the regular HPB SMT baseline and the CCG-augmented HPB SMT baseline. The CCG-augmented one uses single-catego"
    },
    {
        "casual_text": "So, this project is kind of inspired by stuff that deals with breaking down topics and handling data where objects interact with each other based on different relationships. We’re trying to apply that idea to figure out how sentences are related across multiple topics by treating them like different types of relationships. Basically, we’re looking at each topic as its own type of relationship and building a network of sentences within each topic. There’s a lot of work in areas like Information Retrieval and Data Mining that already tackles multi-relational data. For example, scholars cite each other at conferences, and depending on the field, publications cite other papers based on things like who wrote them, their titles, abstracts, and keywords. Websites also link to each other using different anchor texts. This kind of complex linking structure helps us bring in multiple relationships between objects when figuring out how important or popular something is. In Figure 1(a), we’ve got an example of how we represent sentences with multiple relationships using our SentTopic-MultiRank model. There are five sentences and K relationships between them (K is the number of topics from LDA, and we’re just showing three topics, z1, z2, and zK, as an example). We can also show these multi-relational objects in a tensor format, which is like a multi-dimensional array. In Figure 1(b), we’ve got a three-way array where each 2D plane is an adjacency matrix for one type of relationship.",
        "formal_text": "Convert casual text to formal text: So, this project is kind of inspired by stuff that deals with breaking down topics and handling data where objects interact with each other based on different relationships. We’re trying to apply that idea to figure"
    },
    {
        "casual_text": "We look for plagiarism because it's pretty normal for these fancy RNNs that search really well to copy big chunks of the training data. We think that if we set some strict rules about rhyme, meter, repetition, and tricky words with unclear stresses, it'll help prevent plagiarism.",
        "formal_text": "Convert casual text to formal text: We look for plagiarism because it's pretty normal for these fancy RNNs that search really well to copy big chunks of the training data. We think that if we set some strict rules about"
    },
    {
        "casual_text": "The whole thing isn’t too complicated. You start by looking at the text, beginning with the first word, and then follow the pointers to the next word in the list. This way, you go through the words in the same order as they appear in the original text. As you do this, you can spot all the n-grams in each paragraph and use them in a formula to figure out the correlation weight.",
        "formal_text": "Convert casual text to formal text: The whole thing isn’t too complicated. You start by looking at the text, beginning with the first word, and then follow the pointers to the next word in the list. This way,"
    },
    {
        "casual_text": "We're trying out a new method to get image captioning models to come up with more detailed descriptions by using natural language inference. We create these inference graphs and use the PageRank algorithm to give scores to the nodes. Based on that, we use reference sampling and special rewards to help the model generate better, more descriptive captions. We tested the model and it did well on different evaluation metrics, and we also did some in-depth analysis.",
        "formal_text": "Convert casual text to formal text: We're trying out a new method to get image captioning models to come up with more detailed descriptions by using natural language inference. We create these inference graphs and use the PageRank"
    },
    {
        "casual_text": "So, o_j basically stores all the past transitions and their context, and then we plug that into Equation 7. Besides the left-to-right aggregation we talked about earlier, where we combine stuff from a premise and hypothesis pair step by step, we also do aggregation on these binarized constituency parses, which are like tree structures. For each node j in the tree, we have a random variable z_j that represents the reasoning states after looking at node j and its sub-tree. We use s_j to show the distribution of z_j. If node j is a leaf node, we start by using the projected relation distribution p_j to set up s_j. For non-leaf nodes, we figure out s_j by combining the distributions from its left child (lc) and right child (rc) step by step.",
        "formal_text": "Convert casual text to formal text: So, o_j basically stores all the past transitions and their context, and then we plug that into Equation 7. Besides the left-to-right aggregation we talked about"
    },
    {
        "casual_text": "MacCartney and Manning (2009) looked at all 16 possible ways sets can relate to each other and got rid of 9 that didn't really mean anything. This left them with 7 main relations for natural logic, which they called B =  , , , , |, , # , and you can see them in Table 1.",
        "formal_text": "Convert casual text to formal text: MacCartney and Manning (2009) looked at all 16 possible ways sets can relate to each other and got rid of 9 that didn't really mean anything. This left them with 7 main relations"
    },
    {
        "casual_text": "The supervised model we got wasn't as great as the fully-lexicalized DMV, but it was still over five points more accurate than using gold part-of-speech tags (check out Table 1: flat). Unsupervised accuracy was lower compared to gold tags, and the main perk of manually created categories might be that they can tag a word differently depending on the context.",
        "formal_text": "Convert casual text to formal text: The supervised model we got wasn't as great as the fully-lexicalized DMV, but it was still over five points more accurate than using gold part-of-speech tags"
    },
    {
        "casual_text": "The Computational Linguistics model seems to be doing pretty well. For instance, it can correctly generate \"a big rabbit\" when it focuses on the object \"rabbit\". Similarly, it can accurately recognize actions like \"crouching\" and the surroundings like \"grass and flowers\" when it pays attention to the different parts of the sketchy scene. However, it doesn’t mention anything about the weather, like \"on a cloudy day\", which might be because the \"cloud\" is too small and not very noticeable. It’s also interesting that the model doesn’t describe the \"trees\" even though they take up a big part of the scene. As mentioned earlier, the model tends to focus on the most noticeable parts of a sketchy scene. In this case, the \"rabbit\" was considered the main object, and since \"rabbit\" often appears with \"grass\" and \"flowers\" in the dataset, the \"trees\" didn’t get much attention. There are a few more examples in Figure 5 that show how the model works. Overall, it does a good job identifying the main objects in the scenes, except for the \"woman\" and \"house\" in the first example. The reason for this might be that the \"woman\" is hidden behind a \"tree\" in the sketch, making it hard for the model to recognize her properly.",
        "formal_text": "Convert casual text to formal text: The Computational Linguistics model seems to be doing pretty well. For instance, it can correctly generate \"a big rabbit\" when it focuses on the object \"rabbit\". Similarly"
    },
    {
        "casual_text": "Here are the scores from Table 3: the final score after tweaking the model weights for the development dataset (dev), and the BLEU and ME-TEOR scores for the test dataset (test). We're showing results for two different datasets: BTEC and NIST05K, which are from different domains and have different sentence lengths.",
        "formal_text": "Convert casual text to formal text: Here are the scores from Table 3: the final score after tweaking the model weights for the development dataset (dev), and the BLEU and ME-TEOR scores for the test dataset ("
    },
    {
        "casual_text": "The BLEU scores are still pretty low, even with the language model, because there's only one Chinese reference to use for scoring.",
        "formal_text": "Convert casual text to formal text: The BLEU scores are still pretty low, even with the language model. Convert casual text to formal text: The BLEU scores are still pretty low, even with the language model. Con"
    },
    {
        "casual_text": "When you give a model a question and a passage with the right answer in it, the model has to find that answer. We measure how well it does using exact match or average F1 scores (Rajpurkar et al., 2016). We mainly look at the average F1 score because it’s less strict. From what we’ve seen, the exact match score is usually 3-9 points lower than the F1 score. This is all based on Wikipedia articles. SQuAD 1.1 makes sure the passages have answers to the questions (Rajpurkar et al., 2016). SQuAD 2.0 makes things harder by adding 50,000 questions that don’t have answers in the passages, and the model has to figure out when there’s no answer (Rajpurkar et al., 2018). Since the test set isn’t public, we create tricky examples and test the models on the standard dev set. We also use the answerable questions from SQuAD 2.0 instead of SQuAD 1.1 to test models trained on SQuAD 1.1. This helps us compare how well models do on SQuAD 1.1 versus SQuAD 2.0 for questions with answers. We found that the performance on answerable questions from SQuAD 2.0 is pretty similar to SQuAD 1.1.",
        "formal_text": "Convert casual text to formal text: When you give a model a question and a passage with the right answer in it, the model has to find that answer. We measure how well it does using exact match or average F1"
    },
    {
        "casual_text": "Alright, here's what we've got for the results: 1. Is it food? Yep, it is. - Is it food? Still yes. 2. Is it on the left? Yep. - Is it a cake? Yep, it's a cake. 3. Is it in the front? Yep. - Is it the dark brown one? Yep, that's it. 4. Is it on the top? Nope. - Is it the whole cake? Yep, it's the whole thing. 5. Is it in the middle?",
        "formal_text": "Convert casual text to formal text: Alright, here's what we've got for the results: 1. Is it food? Yep, it is. - Is it food? Still yes. 2. Is it"
    },
    {
        "casual_text": "The overall loss function for the span prediction model we're proposing, called L total, is basically a mix of two things: the loss from predicting dialogue acts, L dialogueact, and the loss from predicting the actual spans, L span. It's like a weighted combo of both.",
        "formal_text": "Convert casual text to formal text: The overall loss function for the span prediction model we're proposing, called L total, is basically a mix of two things: the loss from predicting dialogue acts, L dialogueact, and"
    },
    {
        "casual_text": "However, some recursive predicates aren't potentially modular. This includes things like \"reverse,\" which checks if two arguments are the reverse of each other. But this isn't an issue for natural language processing because only features like \"SLAST,\" which don't need those non-modular predicates, can create potentially infinite patterns.",
        "formal_text": "Convert casual text to formal text: However, some recursive predicates aren't potentially modular. This includes things like \"reverse,\" which checks if two arguments are the reverse of each other. But this is"
    },
    {
        "casual_text": "In Section 3, we told the annotators to pick the word that most reminded them of an experiment and label it as EX-PERIMENT. They also had to connect any related details to that word. Plus, the EXPERIMENT labels could be linked together if they were about the same experiment or a variation of it. Table 10 gives some numbers on how many EXPERIMENT labels were in each sentence and how often the main annotator actually connected them. In the training data, there were 703 sentences about experiments, and 135 of those had more than one word that felt like an experiment. Specifically, 114 sentences had two experiment-related words, 18 had three, and 3 had four (check Table 10 for the details). Out of those 114 sentences with two experiment labels, only 2 didn’t link the labels together. When we showed these cases to our main annotator, they thought one of them should have been linked after all.",
        "formal_text": "Convert casual text to formal text: In Section 3, we told the annotators to pick the word that most reminded them of an experiment and label it as EX-PERIMENT. They also had to connect any related details to"
    },
    {
        "casual_text": "Hey, so for all the questions I just answered, I think I’m pretty good at getting, understanding, and using basic health info to make smart decisions. I mean, I trust doctors a lot (TD), but I also like to stay informed and make choices that make sense for me. So yeah, I feel confident in my ability to handle that stuff.",
        "formal_text": "Convert casual text to formal text: Hey, so for all the questions I just answered, I think I’m pretty good at getting, understanding, and using basic health info to make smart decisions. I mean, I trust doctors a"
    },
    {
        "casual_text": "Here’s the informal version of the algorithm: **Algorithm 1: Attended-Value Probing Function PseudoCluster(H, S):** - Start with C = 0 (a matrix of zeros with dimensions m x d). - Also, start with n = 0 (a vector of zeros with dimensions m x 1). - For each tuple (b, e, ) in the set S: - Update C by adding the mean of the sub-sequence H[b] to H[e]. - Increment n by 1. - Finally, return C divided by n. In simpler terms: - You’re taking a bunch of data (H) and a set of ranges (S). - For each range, you calculate the average of the data in that range and add it to C. - You also keep track of how many ranges you’ve processed with n. - At the end, you divide C by n to get the final result.",
        "formal_text": "Convert casual text to formal text: Here’s the informal version of the algorithm: **Algorithm 1: Attended-Value Probing Function PseudoCluster(H, S)"
    },
    {
        "casual_text": "In our first tests, WordPerfect was the only off-the-shelf post-editor that scored way better than the basic machine translation output. So, we decided to dig deeper into how WordPerfect worked in both modes: 1) Grammar Only, where the spell check was turned off, and 2) All Features, where everything, including spell check, was on by default. We ran 40 shortened documents in each mode and looked closely at the results.",
        "formal_text": "Convert casual text to formal text: In our first tests, WordPerfect was the only off-the-shelf post-editor that scored way better than the basic machine translation output. So, we decided to dig deeper into"
    },
    {
        "casual_text": "CCG, which is a kind of mildly context-sensitive grammar, helps us understand how our brains process language word by word, even better than those old-school Penn Treebank-style context-free grammars. Those can be parsed in different ways—leftcorner, top-down, or bottom-up—but CCG still does a better job. This improvement is probably because CCG handles \"movement\" constructions (like the ones in Figure 2) more realistically, which basic context-free grammars just can't do. CCG's flexible way of dealing with sentence structure might help us figure out both the quick and slower parts of language comprehension using just one grammar system. The Revealing operation, created to make CCG parsing more like how humans do it, actually improves how well it matches up with what we know about brain activity in certain areas linked to language comprehension.",
        "formal_text": "Convert casual text to formal text: CCG, which is a kind of mildly context-sensitive grammar, helps us understand how our brains process language word by word, even better than those old-school Penn Treebank-style"
    },
    {
        "casual_text": "Unlike WSD, which uses pre-defined sense lists, our approach is based on data, so the senses we get depend on the text we use. This method can work with any domain as long as there's a BERT-like model available. We tested it on PubMed abstracts from scientific papers using SCIBERT (Beltagy et al., 2019). The senses we got include scientific terms that aren't usually in standard sense lists (see section 6). Figure 1 gives examples of the senses we found for some words from the English Wikipedia corpus. For each sense, we show 5 community-based examples (section 3) and 5 close neighbors in the sense-aware embedding space (section 8). More examples are in Appendix A. The code and resources are up on GitHub at github.com/allenai/WSIatScale.",
        "formal_text": "Convert casual text to formal text: Unlike WSD, which uses pre-defined sense lists, our approach is based on data, so the senses we get depend on the text we use. This method can work with any domain"
    },
    {
        "casual_text": "We tested ExEnt and the other methods on how well they can handle new tasks without any prior training, just like in Section 5.1. We trained different models for CLUES-Real and CLUES-Synthetic. Figure 6 shows how well all the models performed on these new tasks. On CLUES, ExEnt did better than the other methods, which makes us think that using entailment as a middle step helps pull together info from multiple explanations more effectively. On CLUES-Real, ExEnt improved by 18% compared to the others, and on CLUES-Synthetic, it was a 11% improvement.",
        "formal_text": "Convert casual text to formal text: We tested ExEnt and the other methods on how well they can handle new tasks without any prior training, just like in Section 5.1. We trained different models for CLUES-Real and C"
    },
    {
        "casual_text": "While training, we pick from the distribution. When we're doing predictions, we just go with the argmax. If s hits a certain limit, smax, we stop everything for all tokens, which ends the episode. We set smax to 1.5 times the smallest number of steps needed to correctly handle any training example (which is 12).",
        "formal_text": "Convert casual text to formal text: While training, we pick from the distribution. When we're doing predictions, we just go with the argmax. If s hits a certain limit, smax, we stop everything"
    },
    {
        "casual_text": "Lately, a lot of normalization techniques have been using neural machine translation (NMT) on a character level, kind of like how older SMT-based methods did it. Bollmann and Sgaard (2016) used a bidirectional long short-term memory (bi-LSTM) deep neural network to normalize historical German text at the character level. They also checked how well the model worked when extra data was added during training (like multi-task learning). Their results showed that neural network-based normalizations were better than those done by conditional random fields (CRF) and Norma, especially when the models were trained with extra data. Tursun and Cakici (2017) tried out LSTM and the noisy channel model (NCM), which is often used for spell-checking, to normalize Uyghur text. They used a base dataset of around 200 sentences from social media, both automatically and manually normalized. They also created synthetic data by scraping news websites and adding noise to it by randomly swapping characters with corrupted ones. Both methods did a great job normalizing the text, showing their effectiveness. Similarly, Mandal and Nanmaran (2018) used an LSTM network to successfully normalize code-mixed data with an accuracy of 90.27%.",
        "formal_text": "Convert casual text to formal text: Lately, a lot of normalization techniques have been using neural machine translation (NMT) on a character level, kind of like how older SMT-based methods did it. Boll"
    },
    {
        "casual_text": "Alright, big thanks to our sponsors, the Japanese Society for the Promotion of Sciences and the Laboratoire Parole et Langage. Their awesome support helped us give fee waivers to PhD students who were first authors of accepted papers and cover the costs for our invited speakers. Here’s a quick rundown of some of the teams and their work: - **LangResearchLab_NC at CMCL2021 Shared Task**: Predicting Gaze Behaviour Using Linguistic Features and Tree Regressors by Raksha Agarwal and Niladri Chatterjee. - **TorontoCL at CMCL 2021 Shared Task**: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction by Bai Li and Frank Rudzicz. - **LAST at CMCL 2021 Shared Task**: Predicting Gaze Data During Reading with a Gradient Boosting Decision Tree Approach by Yves Bestgen. And that’s a wrap for the teams!",
        "formal_text": "Convert casual text to formal text: Alright, big thanks to our sponsors, the Japanese Society for the Promotion of Sciences and the Laboratoire Parole et Langage. Their awesome support helped us give fee waivers to PhD"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Basically, what we're saying is that if you sum up the KL divergence for each of the M distributions, it's the same as averaging the log ratio of the probabilities for each (f, c) pair across those distributions.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Basically, what we're saying is that if you sum up the KL divergence for each of the M distributions, it"
    },
    {
        "casual_text": "Okay, so K_c(i) is equal to  G_c(i) if i is part of the domain of G_c. If it’s not, then K_c(i) is just . Basically, if G_c(i) exists, we use it, and if it doesn’t, we go with  instead.",
        "formal_text": "Convert casual text to formal text: Okay, so K_c(i) is equal to  G_c(i) if i is part of the domain of G_c. If it’s not,"
    },
    {
        "casual_text": "We use three types of stem changes, which we call transformation rules: deletion, substitution, and duplication. This setup is similar to what Narasimhan et al. (2015) did. These three rules were mostly created to handle stem changes that happen when suffixes are added. Each rule is defined by the specific characters that change. Here's a quick rundown of the three rules:",
        "formal_text": "Convert casual text to formal text: We use three types of stem changes, which we call transformation rules: deletion, substitution, and duplication. This setup is similar to what Narasimhan et al. (2015) did. These"
    },
    {
        "casual_text": "The way things are organized in these two wordnets is different because they were built with different ideas in mind. One has more detailed groups of words (lower synset granularity), while the other has broader ones (higher synset granularity). They also handle hyponymy differently—one uses \"and\" to group things, and the other uses \"or.\" Plus, they code similar ideas in different ways, with one relying on hyponymy and the other on meronymy. These differences lead to more focus on I-hyponymy than I-synonymy, and a lot of WordNet hasn’t been mapped yet because the mapping only goes one way, from plWordNet to WordNet.",
        "formal_text": "Convert casual text to formal text: The way things are organized in these two wordnets is different because they were built with different ideas in mind. One has more detailed groups of words (lower synset granularity), while"
    },
    {
        "casual_text": "Based on the time they were developed and the methods they used, we can roughly group the earlier works on event coreference resolution into three main categories.",
        "formal_text": "Convert casual text to formal text: Based on the time they were developed and the methods they used, we can roughly group the earlier works on event coreference resolution into three main categories. Based on the time they developed and methods they used,"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way: We collected data by grabbing posts and comments from a subreddit called r/depression help. This subreddit is all about giving advice and support to people dealing with depression. The posts there have tags like \"SEEKING HELP,\" \"SEEKING ADVICE,\" or \"REQUESTING SUPPORT.\" We focused on posts with these tags because they’re asking for advice or help. After filtering, we ended up with around 21,000 posts. Each post has a title, a description, and some comments. On average, each post has about 5 comments. Then, we chopped up the main text of each post into smaller bits (called chunks) that are less than 512 tokens long. This helps with processing. Now, here’s the cool part: we used this data to build two models (Model 2 and Model 3) using a deep learning model called T5. We started with the posts and comments from r/depression help, but we removed any comments that weren’t asking questions or seeking information. This left us with a dataset of posts and questions, which we used to fine-tune T5. Before fine-tuning, we also filtered the data using something called PHQ-9 to make sure it was relevant. For comparison, we also used a basic version of T5 (Model 1) that wasn’t fine-tuned. This one often came up with questions that were irrelevant, unsafe, confusing, or just repetitive. In short, we took posts from a depression support subreddit, filtered them, and used the data to train better models that can generate helpful questions.",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in a simpler way: We collected data by grabbing posts and comments from a subreddit called r/depression help. This subred"
    },
    {
        "casual_text": "We came up with two types of features for splitting sentences: the phrase-embedding feature and the segmentation-specific feature. The final feature for each segmentation is just a mix of these two. Oh, and the phrase-embedding feature is also used for sentiment classification, not just sentence segmentation.",
        "formal_text": "Convert casual text to formal text: We came up with two types of features for splitting sentences: the phrase-embedding feature and the segmentation-specific feature. The final feature for each segmentation is just a mix of"
    },
    {
        "casual_text": "In this paper, we treat both the translation and generation methods as ways to boost the amount of data. Our main goal is to deal with the issue of noise in these augmented datasets. We come up with a structured way to learn from multiple noisy augmented datasets for cross-lingual SLU, especially when there's no perfectly labeled data in the target language. Our key technical contribution is a set of methods to clean up the noise, like relabeling instances, co-training, and re-weighting instances.",
        "formal_text": "Convert casual text to formal text: In this paper, we treat both the translation and generation methods as ways to boost the amount of data. Our main goal is to deal with the issue of noise in these augmented datasets. We come"
    },
    {
        "casual_text": "The tagger we used for the experiments is a standard ITMM tagger. It uses the Viterbi algorithm to figure out the most likely sequence of parts of speech for each group of words. This is based on a probabilistic model like this one: (1) l'(, , , 1, . . . , , , , , , t, , . . . , , , , ).",
        "formal_text": "Convert casual text to formal text: The tagger we used for the experiments is a standard ITMM tagger. It uses the Viterbi algorithm to figure out the most likely sequence of parts of speech for each group of words"
    },
    {
        "casual_text": "When it comes to creating something, the main thing is figuring out, based on the situation and what the person wants to achieve, if they should use persuasive techniques to reach a goal. If they decide to go that route, they also need to think about whether they'll do it directly or indirectly in the final text. If they pick a direct approach, they have to decide if they'll use specific words, like \"promise\" or \"order,\" to make it clear what kind of action they're performing.",
        "formal_text": "Convert casual text to formal text: When it comes to creating something, the main thing is figuring out, based on the situation and what the person wants to achieve, if they should use persuasive techniques to reach a goal."
    },
    {
        "casual_text": "Sure! Here's the informal version: - **Binary Approach**: For each slot type, train a separate binary classifier. The idea is that each slot type is predicted independently of the others. After getting the predictions, you combine them with the slots that match the schema you're working with. For each binary slot tagger focusing on a specific slot type, the labeled data is created programmatically. - **Post Approach**: Train one big model using all the domain data. Then, take the best parse from the tagger and remove any slots that don’t fit the schema you’re working with.",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: - **Binary Approach**: For each slot type, train a separate binary classifier. The idea is that each slot type is predicted independently"
    },
    {
        "casual_text": "x, y. In the last layer, s in(L in ) is just the identity function, and b in(L in ) is set to 0 because h x, y already includes a bias. So, the final result of the input projection is:",
        "formal_text": "Convert casual text to formal text: x, y. In the last layer, s in(L in ) is just the identity function, and b in(L in ) is set to 0 because"
    },
    {
        "casual_text": "In this part, we add a new child node to the one we picked earlier. This new node is chosen randomly from all the possible moves. At first, this new node has no wins and no simulations, so W i = 0 and S i = 0.",
        "formal_text": "Convert casual text to formal text: In this part, we add a new child node to the one we picked earlier. This new node is chosen randomly from all the possible moves. At first, this new node has no"
    },
    {
        "casual_text": "In Table 9, you'll find a confusion matrix showing the errors for different types of dialogue acts: CONVENTIONAL CLOSING (Cl), CONVENTIONAL OPENING (Op), DOWNPLAYER (Dp), EXPRESSIVE (Ex), NO ANSWER (No), OPEN QUESTION (Qu), REQUEST (Rq), RESPONSE ACK (Ack), STATEMENT (St), THANKS (Ta), YES ANSWER (Yes), and YESNO QUESTION (YN). The rows show the correct dialogue acts, and the columns show the ones that were misclassified.",
        "formal_text": "Convert casual text to formal text: In Table 9, you'll find a confusion matrix showing the errors for different types of dialogue acts: CONVENTIONAL CLOSING (Cl), CONVENTional OPENING (Op),"
    },
    {
        "casual_text": "We figure that for a perfect system (or what we call the human gold standard), the correlation index should be 1, meaning it gets 100% accuracy on both the monothematic dataset and the original RTE dataset. So, we think of CI = 1 as the ideal situation. To see how close a system S is to being perfect, we just subtract its actual correlation from this ideal CI.",
        "formal_text": "Convert casual text to formal text: We figure that for a perfect system (or what we call the human gold standard), the correlation index should be 1, meaning it gets 100% accuracy on both the monothematic dataset and the original RTE"
    },
    {
        "casual_text": "Sure, when we use a model trained on this kind of transformed data for new inputs, it’ll produce transformed outputs. So, the transformation needs to be reversible, meaning we can convert the model’s output back to the original, un-transformed value.",
        "formal_text": "Convert casual text to formal text: Sure, when we use a model trained on this kind of transformed data for new inputs, it’ll produce transformed outputs. So, the transformation needs to be reversible, meaning"
    },
    {
        "casual_text": "In this paper, we’re focusing on how segregatory coordination constructions are generated. Segregatory coordination is when smaller units are coordinated in a way that’s logically the same as coordinating whole clauses. For example, \"John likes Mary and Nancy\" is basically the same as saying \"John likes Mary\" and \"John likes Nancy\" separately. Now, there are other types of coordination, like combinatory and rhetorical coordination, but they work differently in text generation systems. Since these can’t really be broken down into separate clauses, we’ll define them here, but we won’t go into more detail about them in this paper. For combinatory coordination, the sentence \"Mary and Nancy are sisters\" isn’t the same as saying \"Mary is a sister\" and \"Nancy is a sister\" separately. And sometimes, \"and\" can act as a rhetorical marker, like in \"The train sounded the whistle and [then] departed the station.\"",
        "formal_text": "Convert casual text to formal text: In this paper, we’re focusing on how segregatory coordination constructions are generated. Segregatory coordination is when smaller units are coordinated in a way that’s logically the"
    },
    {
        "casual_text": "Silent reading and reading aloud might look similar but they could also be different in some ways. The main difference between the two is how the brain handles phonological and semantic information. In silent reading, people often wonder if the brain goes straight from letters to meaning or if it first turns letters into sounds and then connects those sounds to meaning. Harm and Seidenberg (2004) looked into this using a computer-based method and found that both ways happen at the same time, depending on things like how common a word is or how consistent its spelling is with its sound. When it comes to reading aloud, it used to be thought that the brain didn’t need to understand the meaning of the words, just match letters to sounds and say them. But models and brain scans (like fMRI) have shown that understanding the meaning does happen, though maybe not as much as in silent reading.",
        "formal_text": "Convert casual text to formal text: Silent reading and reading aloud might look similar but they could also be different in some ways. The main difference between the two is how the brain handles phonological and semantic information. In silent"
    },
    {
        "casual_text": "We tested these drops with a few other dependency parsers and got similar results. For instance, the phrase \"(R = 28% (10/26); K=10% (3/29); chi2 test: p=0.014)\" shows this.",
        "formal_text": "Convert casual text to formal text: We tested these drops with a few other dependency parsers and got similar results. For instance, the phrase \"(R = 28% (10/26); K=10% (3/29);"
    },
    {
        "casual_text": "So, like, if the sequence is \"an apple. It is\", we switch it to \"It is an apple.\" And for the code, we use the one from this link: https://github.com/google-research/google-research/tree/master/rouge.",
        "formal_text": "Convert casual text to formal text: So, like, if the sequence is \"an apple. It is\", we switch it to \"It is an apple.\" And for the code, we use the one from this link: https"
    },
    {
        "casual_text": "So, they tested a method on a dataset called the Science News corpus from It-Bank 1 to figure out how well it could spot nonanaphoric pronouns like \"it.\" Their fancy distributional method got them results of 81.4% precision, 71.0% recall, and an F1-measure of 75.8. This was way better than the old rule-based approach from Lappin and Leass (1994), which only scored 93.4% precision, 21.0% recall, and an F1-measure of 34.3. It also outperformed the rule-based method from Cherry and Bergsma (2005), which had 66.4% precision, 49.7% recall, and an F1-measure of 56.9.",
        "formal_text": "Convert casual text to formal text: So, they tested a method on a dataset called the Science News corpus from It-Bank 1 to figure out how well it could spot nonanaphoric pronouns like \""
    },
    {
        "casual_text": "In our marketing, we talk about \"made-to-measure\" solutions, which basically means stuff that’s customized to fit perfectly. But the term also reflects the methods we’ve fine-tuned over the years to get the exact results our clients need.",
        "formal_text": "Convert casual text to formal text: In our marketing, we talk about \"made-to-measure\" solutions, which basically means stuff that’s customized to fit perfectly. But the term also reflects the methods we’ve fine-"
    },
    {
        "casual_text": "Li et al. (2020) had a similar idea to ours, which is to make word representations better by using context. But their work isn't open source, and they didn't use the same dataset, so we can't really compare it to MINER. Instead, we compared our method to some baselines like this:",
        "formal_text": "Convert casual text to formal text: Li et al. (2020) had a similar idea to ours, which is to make word representations better by using context. But their work isn't open source, and"
    },
    {
        "casual_text": "The interface stacks the source and target language segments, with the source on top and the target below for editing. In Figure 2, you can see the PosEdiOn interface: the upper box shows the source segment, and the lower one lets the translator make changes. There are toolbar buttons at the bottom that let you scroll through the whole document to see more context.",
        "formal_text": "Convert casual text to formal text: The interface stacks the source and target language segments, with the source on top and the target below for editing. In Figure 2, you can see the PosEdiOn interface: the upper box shows"
    },
    {
        "casual_text": "Alright, so , , , , and  are just parameters that help balance the losses from different parts of the system. During training and when making predictions, any entities that aren't part of the user's memory graph are ignored. What's cool about E2E-UMGR is that it’s different from old-school recommender systems. It can figure out updates to the graph on the fly and even do zero-shot reasoning, which means it doesn’t need to know about users or items beforehand. The policy space is flexible because the entities in the policy are based on how important they are in the user’s memory graph, not some fixed list set up by the model.",
        "formal_text": "Convert casual text to formal text: Alright, so , , , , and  are just parameters that help balance the losses from different parts of the system. During training and when making predictions"
    },
    {
        "casual_text": "We've expanded CAN to work in multi-task setups by adding ACD as a secondary task and using CAN for both ALSC and ACD.",
        "formal_text": "Convert casual text to formal text: We've expanded CAN to work in multi-task setups by adding ACD as a secondary task and using CAN for both ALSC and ACD. Convert casual text to formal"
    },
    {
        "casual_text": "JLSTMSurp (from Jozefowicz et al., 2016) is a two-layer LSTM model that uses CNN character inputs. It was trained on around 800 million tokens from the 1B Word Benchmark dataset (Chelba et al., 2014).",
        "formal_text": "Convert casual text to formal text: JLSTMSurp (from Jozefowicz et al., 2016) is a two-layer LSTM model that uses CNN character inputs. It was"
    },
    {
        "casual_text": "Keyphrases are super useful because they give readers a quick idea of what a document is about. Plus, they make it easier to search for stuff, like when you hashtag posts on social media with keyphrases that were predicted. In our case, we’re looking at keyphrase tasks that let us use words not actually in the document as keyphrases. We call these \"absent words.\" This is important because the document might not have enough context, so it’s missing a lot of potentially relevant keyphrases. The reason? The keyphrases might be using different words than what’s in the document—kind of like a vocabulary mismatch.",
        "formal_text": "Convert casual text to formal text: Keyphrases are super useful because they give readers a quick idea of what a document is about. Plus, they make it easier to search for stuff, like when you hashtag posts on social media"
    },
    {
        "casual_text": "Och's one-dimensional algorithm from 2003 has been around for almost ten years and has been analyzed and tweaked a lot during that time. Now, we're looking at multidimensional approaches for the first time. We think these multidimensional methods have a lot of potential, but there's definitely room to make them work better and faster, especially when dealing with larger tasks. One cool idea for future work could be adapting LP-MERT to use more compact ways of representing the search space, like hypergraphs, to make things even more efficient.",
        "formal_text": "Convert casual text to formal text: Och's one-dimensional algorithm from 2003 has been around for almost ten years and has been analyzed and tweaked a lot during that time. Now, we're looking at"
    },
    {
        "casual_text": "In this paper, we're introducing a new framework that builds on the CKY-like bottom-up method. We’ve also created and tested a specific version of this framework, and our experiments show that using more context leads to better accuracy for TSP tasks that need to consider things that aren’t right next to each other, like HMT.",
        "formal_text": "Convert casual text to formal text: In this paper, we're introducing a new framework that builds on the CKY-like bottom-up method. We’ve also created and tested a specific version of this framework"
    },
    {
        "casual_text": "Okay, so here's the deal: the monitor keeps an eye on the whole reconstruction process. If it thinks things are taking too long, it steps in by raising the threshold to stop the process when it detects inconsistency. Figure 3 gives an example of how this space reconstruction works. In Fig. 3(a), the distance between entities A and B is 10. Now, let's say a new estimate puts that distance at 5. The reconstructor checks out the neighbors of both A and B. It decides to move entity B because B's neighbors are less crowded than A's. The reconstructor picks one spot out of eight possible positions around A where B can go with the least amount of new inconsistency. In Fig. 3(b), B is placed to the left of A. Any new inconsistencies, like those involving B and G, are checked and noted in *inconsistent. After a few rounds of trial and error to reduce inconsistency, the space settles into the final configuration shown in Fig. 3(c), which is all good and consistent.",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: the monitor keeps an eye on the whole reconstruction process. If it thinks things are taking too long, it steps in by raising the threshold to stop the process"
    },
    {
        "casual_text": "Ukrainian President says it's not true when @cnnbrk shared that Russia claims their records show a Ukrainian warplane was within 5 km of #MH17 on the day it crashed.",
        "formal_text": "Convert casual text to formal text: Ukrainian President says it's not true when @cnnbrk shared that Russia claims their records show a Ukrainian warplane was within 5 km of #MH17 on the day it"
    },
    {
        "casual_text": "We checked how well the systems did on pairs that included metaphors to see if there was a big difference compared to their overall performance in the official RTE challenges. Since the RTE-1 results aren’t out there for everyone to see, we focused on the 7 runs that are available, including 4 out of the top 5 systems. Even though there aren’t many metaphor pairs to work with, Table 1 shows that the accuracy tends to be lower when metaphors are involved, which matches what Bos and Markert (2006) found.",
        "formal_text": "Convert casual text to formal text: We checked how well the systems did on pairs that included metaphors to see if there was a big difference compared to their overall performance in the official RTE challenges. Since the RTE-1"
    },
    {
        "casual_text": "I gotta point out that Bcguraev's use of the teon case is way broader than what you usually see in linguistics. It’s not just about attaching prepositions to nouns or verbs; it also covers other ways stuff gets attached to or modified by nouns. Like, it includes stuff like determiners (you know, words like \"a\") and even handles plural or singular forms. Take the phrase \"the high frequency oscillator\" as an example. In Figure 2, which shows how this is structured, the connection between \"oscillator\" and the determiner \"the\" is marked by something called the case-label det. And the modifier \"high frequency,\" which is shown in the complex structure on the lower right of the figure, is linked to \"oscillator\" using nmod.",
        "formal_text": "Convert casual text to formal text: I gotta point out that Bcguraev's use of the teon case is way broader than what you usually see in linguistics. It’s not just"
    },
    {
        "casual_text": "Two people who know a lot about reviews and topic models did the manual labeling. The process had two parts: first, they labeled the topics, and second, they labeled the important words for each topic. After the first part, they checked how much they agreed and talked about the topics they didn’t agree on until they reached a common understanding. Then, they moved on to the next part, where they labeled the most important words for each topic, based on how likely they were to appear in that topic. To measure how much they agreed, they used something called Kappa scores. The score for topic labeling was 0.838, and for the words, it was 0.846. Both scores show they agreed a lot. For checking the quality, they used a method called Precision@n, which looks at how accurate the top n words are for a topic. They used Precision@15 and Precision@30 because the top 15 words usually give a good idea of the topic. But since they also used phrases in their work, some important words might have been ranked lower, so they went up to 30. This method, Precision@n, has been used before by others, like Zhao et al. in 2010 and Chen et al. in 2013.",
        "formal_text": "Convert casual text to formal text: Two people who know a lot about reviews and topic models did the manual labeling. The process had two parts: first, they labeled the topics, and second, they labeled the"
    },
    {
        "casual_text": "All the comparison methods stick to the standard dataset splits. So, we’re just quoting the results from (Chen and Cardie, 2018; Yang and Shang, 2019) to keep things fair.",
        "formal_text": "Convert casual text to formal text: All the comparison methods stick to the standard dataset splits. So, we’re just quoting the results from (Chen and Cardie, 2018; Yang and Shang, 2019 to keep"
    },
    {
        "casual_text": "Here are five of the best improvements in sentence alignment before and after figuring out word matches that weren't in the Japanese-English dictionary. The results got better once we handled the word correspondences that weren't in the bilingual dictionary.",
        "formal_text": "Convert casual text to formal text: Here are five of the best improvements in sentence alignment before and after figuring out word matches that weren't in the Japanese-English dictionary. The results got better once we handled the word correspondences that"
    },
    {
        "casual_text": "We're looking at how connectives (like \"and\" or \"but\") are translated from English to French or Arabic. Here's how we're doing the evaluation, using these symbols:",
        "formal_text": "Convert casual text to formal text: We're looking at how connectives (like \"and\" or \"but\") are translated from French or Arabic. Here's how we're doing the evaluation, using these symbols:"
    },
    {
        "casual_text": "When you plug this IF into the mapper, it spits out the FS structure you see in Figure 6.",
        "formal_text": "Convert casual text to formal text: When you plug this IF into the mapper out the FS structure you see in Figure 6. Convert casual text to formal text: When you plug this IF into the mapper, it"
    },
    {
        "casual_text": "So, the tasks in CLUES-Real have explanations from different teachers and students for each teacher. This gives us a lot of info about how teachers and students perform differently, showing how well different tasks can be learned through language. In section 4, we share some details about how teachers and students did in our setup.",
        "formal_text": "Convert casual text to formal text: So, the tasks in CLUES-Real have explanations from different teachers and students for each teacher. This gives us a lot of info about how teachers and students perform differently, showing how"
    },
    {
        "casual_text": "Overall performance: Our model works better than the usual computer methods for pulling out names of people killed by police. We checked this by looking at F1 scores from standard extractors (see Table 5 vs. Table 6; the differences are legit, statistically speaking).",
        "formal_text": "Convert casual text to formal text: Overall performance: Our model works better than the usual computer methods for pulling out names of people killed by police. We checked this by looking at F1 scores from standard extractors (see Table 5"
    },
    {
        "casual_text": "Alright, so for the model p(w| (d) ), we can define the translated representation like this:",
        "formal_text": "Convert casual text to formal text: Alright, so for the model p(w| (d) ), we can define the translated representation like this: Alright, so for the model p(w|"
    },
    {
        "casual_text": "These CS normalizers are either based on rules and work for specific languages (like Barik et al. did in 2019) or they mix things up by using Hindi back-transliteration and normalization (as Sharma et al. did in 2016 and Bhat et al. in 2017 and 2018). So, they don’t really work for other lexical normalization datasets out of the box. In this project, though,",
        "formal_text": "Convert casual text to formal text: These CS normalizers are either based on rules and work for specific languages (like Barik et al. did in 2019) or they mix things up by using Hindi back-trans"
    },
    {
        "casual_text": "Okay, let's break this down in simpler terms. So, we're talking about defining something called \"delay.\" Imagine we have a sentence, and we're looking at each word in that sentence one by one. Let's call the ith word in the sentence \"e i.\" Now, for each word \"e i,\" we look at the words in the original sentence (the source) that it connects to. We find the highest number (or index) among those connected words and call that \"a i.\" So, the delay for \"e i\" is basically how far back or ahead it is in terms of these connections. We'll call that delay \"d.\"",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in simpler terms. So, we're talking about defining something called \"delay.\" Imagine we have a sentence, and we're looking at each"
    },
    {
        "casual_text": "The Inference Engine helps out with the Knowledge Base to figure out possible conclusions in an article. It also works with the Discourse Manager to guide the informant through the reasoning process that leads to each conclusion.",
        "formal_text": "Convert casual text to formal text: The Inference Engine helps out with the Knowledge Base to figure out possible conclusions in an article. It also works with the Discourse Manager to guide the informant through the reasoning process that leads to each conclusion"
    },
    {
        "casual_text": "Sure, you could interpret (3a) as having more than one boy, like one boy per pizza, but with (3b), you can't have more than one girl involved.",
        "formal_text": "Convert casual text to formal text: Sure, you could interpret (3a) as having more than one boy, like one boy per pizza, but with (3b), you can't have more than one girl involved. Convert casual text"
    },
    {
        "casual_text": "We think the main reason DIORA struggles with supervised parsing is because it can't really fix mistakes once they happen. DIORA is great at finding the best tree based on the numbers it gets for each part of the sentence, but the problem is in how it assigns those numbers. It only uses local information, so it might give a low score to a part of the tree that should actually be higher if it had more context. This can be a big deal when a sentence is ambiguous and needs more context to make sense. For example, take the sentence \"We saw the dog with my phone.\" The way it's parsed depends on the context. In the next part, we'll show how we fixed this issue in DIORA.",
        "formal_text": "Convert casual text to formal text: We think the main reason DIORA struggles with supervised parsing is because it can't really fix mistakes once they happen. DIORA is great at finding the best tree based on"
    },
    {
        "casual_text": "Since there aren't many examples of split-antecedent anaphora in ARRAU, we decided to use four extra corpora to help the system perform better. These extra corpora came from either the Phrase Detectives (PD) corpus, which was annotated by lots of people, or the gold-standard annotated version of ARRAU.",
        "formal_text": "Convert casual text to formal text: Since there aren't many examples of split-antecedent anaphora in ARRAU, we decided to use four extra corpora to help the system perform better. These extra corp"
    },
    {
        "casual_text": "In this case, we can think of the predicted full-sentence length as a hidden variable while translating. The goal is to help the model understand the tricky relationship between incomplete source words. Using hidden variables has been shown to be really useful for handling these kinds of dependencies (Lee et al., 2018; Su et al., 2018; Shu et al., 2020; Song et al., 2021). Since we're treating the full-sentence length as a hidden variable, the model becomes better at figuring out these dependencies, which helps cut down on position bias.",
        "formal_text": "Convert casual text to formal text: In this case, we can think of the predicted full-sentence length as a hidden variable while translating. The goal is to help the model understand the tricky relationship between incomplete source words. Using"
    },
    {
        "casual_text": "Alright, let’s break this down. The test system is just a regular PC with a 3.16 GHz Core 2 Duo processor and 8 GB of RAM. The retrieval engine we’re using is SMART 5, but it’s been tweaked to handle positional indexing and language modeling. Basically, for every term in the postings list, we keep track of the document ID where it shows up and the exact position of that term in the document. As for the EBMT system, it’s based on the translation-by-analogy idea from Nagao’s 1984 work.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down. The test system is just a regular PC with a 3.16 GHz Core 2 Duo processor and 8 GB of RAM. The retrieval"
    },
    {
        "casual_text": "If the proposal distribution looks like Q(t k 1: n |s 1: n ), then the importance weights are calculated based on that.",
        "formal_text": "Convert casual text to formal text: If the proposal distribution looks like Q(t k 1: n |s 1: n ), then the importance weights are calculated based on that. Convert casual text to formal"
    },
    {
        "casual_text": "We went with single-head attention. When we tested it against multi-head attention, the results were pretty much the same. 5 Because the number of documents linked to each keyphrase can be different, we adjusted the tf-idf weights so they all have the same scale.",
        "formal_text": "Convert casual text to formal text: We went with single-head attention. When we tested it against multi-head attention, the results were pretty much the same. 5 Because the number of documents linked to each keyphrase can be different,"
    },
    {
        "casual_text": "Text and audio. This thing creates summaries by using the words in documents and the stuff people say, but it doesn't use any special tricks or guidance to do it.",
        "formal_text": "Convert casual text to formal text: Text and audio. This thing creates summaries by using the words in documents and the stuff people say, but it doesn't use any special tricks or guidance to do it."
    },
    {
        "casual_text": "Since matching arguments to KPs can be pretty subjective, we decided to use a ternary label system. This means that if the annotators couldn't agree, the argument-KP pair got an \"undecided\" label. Later on, these undecided pairs are sorted into either positive or negative matches, which gives us two different gold standards—one that might be too strict and another that could be too lenient. For future tasks, it might be better to switch to a binary labeling system, which would give us just one gold standard. This could work by only marking an undecided pair as positive if a certain number of annotators agreed it was a match and no other KP was labeled as matching the same argument.",
        "formal_text": "Convert casual text to formal text: Since matching arguments to KPs can be pretty subjective, we decided to use a ternary label system. This means that if the annotators couldn't agree, the argument-"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: - **w_i**: Just think of this as a single word in a sentence. - **w_i w_i+1**: This is called a bigram, which means two words in a row, like \"word one\" and \"word two.\" - **S**: This is the whole sentence. - **tf_d(w_i)**: This is how many times the word **w_i** shows up in a specific document or text (let's call it **d**). - **df_D(w_i)**: This is how many documents in a collection (let's call it **D**) contain the word **w_i**. So, in short: - **w_i** = a word - **w_i w_i+1** = two words together - **S** = the full sentence - **tf_d(w_i)** = how often a word appears in a specific text - **df_D(w_i)** = how many texts have that word in them",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: - **w_i**: Just think of this as a single word in a sentence. - **w_"
    },
    {
        "casual_text": "We're using this version: praw.readthedocs.io/en/latest. You can check out all our code here: http://www.cs.cmu.edu/pmichel1/mtnt/.",
        "formal_text": "Convert casual text to formal text: We're using this version: praw.readthedocs.io/en/latest. You can check out all our code here: http://www.cs.c"
    },
    {
        "casual_text": "We started by picking 120 random pairs from the list of concept pairs to have all five annotators work on. When we looked at their scores (more on that in Section 4), we noticed that most of the pairs got marked as not related. Check out Table 2 for the Krippendorff's , average , Cohen's , and Spearman's  for each annotator.",
        "formal_text": "Convert casual text to formal text: We started by picking 120 random pairs from the list of concept pairs to have all five annotators work on. When we looked at their scores (more on that in Section 4), we noticed that most"
    },
    {
        "casual_text": "Unlike Bronner and Monz (2012) and earlier studies on vandalism classification, we created a model that handles multilabeling and uses a more detailed edit category system. Our feature set is based on previous work but includes a bunch of new features we added.",
        "formal_text": "Convert casual text to formal text: Unlike Bronner and Monz (2012) and earlier studies on vandalism classification, we created a model that handles multilabeling and uses a more detailed edit category system. Our feature"
    },
    {
        "casual_text": "A likely reason is that the model picks up on helpful insights about how things are organized in semantic spaces (like tree structures) and starts to understand certain intents and slots during its training on the source data. For example, the better performance on reminders might be partly because reminders are kinda similar to other source domains like alarms and timers. Plus, the target domains have some slots in common with the source ones, like SL: DATE_TIME and SL: LOCATION. When the model sees a ton of examples of these slots in the source domains, it gets better at figuring out what they mean, which boosts its overall performance.",
        "formal_text": "Convert casual text to formal text: A likely reason is that the model picks up on helpful insights about how things are organized in semantic spaces (like tree structures) and starts to understand certain intents and slots during its training on the source"
    },
    {
        "casual_text": "Other researchers have checked out how language works together with other stuff to influence how Reddit communities react. Lakkaraju and his buddies (2013) came up with a way to guess how popular resubmitted posts would be, showing that the title really matters. Jaech and his team (2015) looked at timing and different language stuff to rank comments and saw that things vary a lot between different communities. In our project, we’re focusing on the language used in communities, but we’re trying out different models to understand it better.",
        "formal_text": "Convert casual text to formal text: Other researchers have checked out how language works together with other stuff to influence how Reddit communities react. Lakkaraju and his buddies (2013) came up with a way to guess how popular re"
    },
    {
        "casual_text": "We used the AdamW optimizer (Loshchilov and Hutter, 2019), which is pretty standard for fine-tuning pretrained Masked Language Models (MLM). To fine-tune these models on our benchmark tasks, we tried out learning rates of 1e-5, 2e-5 and went with 1e-5 because it worked better on the validation set for the seen tasks. We kept the batch size at 2, but used a gradient accumulation factor of 8 to make up for it. The random seed for all our experiments was set to 42. We trained all the models for 20 epochs, with each epoch having 100 batches. In each batch, the model focused on one of the tasks from the seen split.",
        "formal_text": "Convert casual text to formal text: We used the AdamW optimizer (Loshchilov and Hutter, 2019), which is pretty standard for fine-tuning pretrained Masked Language Models (MLM)."
    },
    {
        "casual_text": "VIM definitions basically mean that repeatability R 0 is when you measure the same thing multiple times under exactly the same conditions (C) each time.",
        "formal_text": "Convert casual text to formal text: VIM definitions basically mean that repeatability R 0 is when you measure the same thing multiple times under exactly the same conditions (C each time.). Repeatability R 0 is when you measure"
    },
    {
        "casual_text": "Okay, let's break it down in simpler terms. So, let's say  0 turns into  j after a certain number of steps, and that number of steps is at least 1. In each step...",
        "formal_text": "Convert casual text to formal text: Okay, let's break it down in simpler terms. So, let's say  0 turns into  j after a certain number of steps, and that number of steps"
    },
    {
        "casual_text": "A6. Did they copy the previous work instead of reproducing it? And did they make sure that all the MT systems used the exact same pre-processed training, validating, and testing data? Just answer yes or no.",
        "formal_text": "Convert casual text to formal text: A6. Did they copy the previous work instead of reproducing it? And did they make sure that all the MT systems used the exact same pre-processed training, validating, and"
    },
    {
        "casual_text": "Alright, let's dive into discriminative training, specifically in the supervised setup, like how it’s done in MERT (Och, 2003) and similar methods. The goal here is to tweak the parameters  of a fancy translation system   (x). This system takes Chinese input x and spits out an English translation y =   (x). But here’s the thing:   doesn’t have to be a probabilistic model. For instance,  could be the settings for a scoring function that  uses, along with some tricks for pruning and decoding, to pull out a high-scoring translation of x.",
        "formal_text": "Convert casual text to formal text: Alright, let's dive into discriminative training, specifically in the supervised setup, like how it’s done in MERT (Och, 2003) and similar methods. The goal here"
    },
    {
        "casual_text": "In context-based methods, all the A La Carte models do really well, just like the selective Word2Vec algorithm. When it comes to the Filtered CRW task, using the ideas from Word2Vec helps improve performance for both additive and A La Carte models. This seems to work especially well when dealing with unfiltered, real-world examples of new words. But on the original CRW task, these techniques actually make things worse, probably because the original words are already in the model. The Neural ALC model is the top performer in context-based tasks: the flexibility from the neural network lets it handle different situations better. One thing that’s interesting is that Nonce2Vec’s performance drops when more than 16 context sentences are used. This might be because of an imbalance in how important the training data is (check out Figure 3 for more details).",
        "formal_text": "Convert casual text to formal text: In context-based methods, all the A La Carte models do really well, just like the selective Word2Vec algorithm. When it comes to the Filtered CRW task, using"
    },
    {
        "casual_text": "Check out Table 1 for the partial results. One thing that stands out is how the accuracy goes from 67.69% down to 36.7% when we mask just the top-20 most frequent event-related words in the in-domain set. This shows the model is super sensitive to these event-related patterns. On the flip side, the small drop in accuracy when we mask the top-20 out-of-domain words might mean we're masking some words the model hasn't seen during training compared to not masking anything at all. These experiments back up our idea that the baseline classifier is mainly picking up on topical connections, which makes it clear we need a better approach to avoid bias in classification. We'll explain that next.",
        "formal_text": "Convert casual text to formal text: Check out Table 1 for the partial results. One thing that stands out is how the accuracy goes from 67.69% down to 36.7% when we mask just the top-20 most frequent event-"
    },
    {
        "casual_text": "We tackle figuring out the main subjects of scientific papers in two steps: first, identifying the pathogen, and second, describing it. Let me walk you through how we handle each of these steps.",
        "formal_text": "Convert casual text to formal text: We tackle figuring out the main subjects of scientific papers in two steps: first, identifying the pathogen, and second, describing it. Let me walk you through how we handle each of these"
    },
    {
        "casual_text": "Basically, F kb is like a big set of sentences, F kb = f 1, f 2, . . . , f n , that hold all the general knowledge needed to answer and explain science questions. One cool thing about F kb is that it’s reusable, meaning each fact f i can be used again and again to help explain different questions.",
        "formal_text": "Convert casual text to formal text: Basically, F kb is like a big set of sentences, F kb = f 1, f 2, . . . , f n"
    },
    {
        "casual_text": "Okay, so here's the deal: Cseg&Tagl. 0 is just the first step of our investigation. Our ultimate goal is to create a system that can accurately segment and tag Chinese text with about 99% and 95% precision, respectively, no matter the context. There's still a lot of work to be done to get there. Right now, we think it's totally doable in the near future, and we’ve got a better idea of how to make it happen. We're currently working on the second round, focusing on two main things: (1) tweaking the algorithm, especially the parts related to agents and cache, to make it better; and (2) improving the knowledge base by adding more resources (like text corpora and unknown word banks) and refining the lexicon, tagged corpus, and rule base.",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: Cseg&Tagl. 0 is just the first step of our investigation. Our ultimate goal is to create a system that can accurately segment"
    },
    {
        "casual_text": "The Online Primal Subgradient method, introduced by Ratliff et al. in 2007, calculates the subgradient for the margin objective for each instance by doing a loss-augmented decode. It then uses these subgradients for each instance to optimize the overall objective using Ada-Grad, which was developed by Duchi et al. in 2011. Ada-Grad can be used with either L1 or L2 regularization. The basic version of Ada-Grad updates every weight when processing a batch. To speed things up, we split the updates into two types. When the subgradient isn't zero, we do the regular update. But when the subgradient is zero, we delay the update until the weight is needed again. This approach saves time because we only update the weights that have a nonzero subgradient in the current batch, which is usually a smaller number. Algorithm 1 provides a pseudocode for our implementation, which was inspired by Dyer's work in 2013.",
        "formal_text": "Convert casual text to formal text: The Online Primal Subgradient method, introduced by Ratliff et al. in 2007, calculates the subgradient for the margin objective for each instance by doing a loss-augmented"
    },
    {
        "casual_text": "Okay, so we've got this thing where S_k is calculated by adding up U(k-j+1) l_j  for j from 1 to k-1. After that, we tweak M_k by combining S_k with S(1) l_k  and using the task loss L_T to fine-tune it.",
        "formal_text": "Convert casual text to formal text: Okay, so we've got this thing where S_k is calculated by adding up U(k-j+1) l_j  for j from 1 to k-1"
    },
    {
        "casual_text": "We can kinda guess the log loss part E[log p  (x)] using samples, but the entropy part...",
        "formal_text": "Convert casual text to formal text: We can kinda guess the log loss part E[log p  (x)] using samples, but entropy part... Convert casual text to formal text: We can kind"
    },
    {
        "casual_text": "In NEL and NLP, the goal is to create different ways of looking at the same data. Over the past few years, \"lenses\" have become more popular for switching between these different views (Rajkumar et al., 2013). In NLP, lenses are usually set up as annotation systems that show how a human annotator sees the data. This is a big deal in tools like GATE (Maynard, 2009) and for working with big parallel datasets, especially in multilingual projects (e.g., Iranzo-Sánchez et al., 2019). In the Semantic Web (SW), ontologies have been used as lenses to organize data, which eventually led to a whole field called Ontology-Based Data Access (Calvanese et al., 2015). More recently, lenses have been used to handle chemistry data (Batchelor et al., 2014), manage huge datasets (Lenzerini, 2018), or make sense of big data and AI tasks (Gao et al., 2018).",
        "formal_text": "Convert casual text to formal text: In NEL and NLP, the goal is to create different ways of looking at the same data. Over the past few years, \"lenses\" have become more popular for switching between these different"
    },
    {
        "casual_text": "Textual Features. The transcriptions for each video are the basis of the textual data we're working with. To pull out textual features, we used 300-dimensional GloVe embeddings (from Pennington et al., 2014) to turn words into word vectors. These word vectors are then combined to create a final representation of the entire text. Sure, we could have gone with fancier methods like convolutional or recurrent neural networks, but we stuck with the same pre-trained approach we used for the other data types.",
        "formal_text": "Convert casual text to formal text: Textual Features. The transcriptions for each video are the basis of the textual data we're working with. To pull out textual features, we used 300-dimensional GloVe embedd"
    },
    {
        "casual_text": "The idea of \"gender environment\" is all about who's in the group when people are talking or communicating. In the studies we looked at (like Holmes and Stubbe, 2003, and Herring, 2008), it’s usually about a steady group of coworkers who chat regularly. Since we're focusing on email conversations (or threads), we tweak the idea to apply to one email thread at a time. We think that when someone is chatting, they make choices based on their own gender and the genders of the people they're talking to in that specific conversation (like an email thread). So, we see the \"gender environment\" as something unique to each person in the conversation and how they see the others. In other words, we use \"gender environment\" to think about who a person might be talking to in a conversation. For instance, if there's a chat with five women and one man, from the man's perspective, it feels like he's talking to an all-female group. But from the women's side, it's more like they're chatting with a mostly-female group. To define the gender environment for a person p in a thread t, here's what we do: we assume the gender environment is something each person p in thread t has. We take all the people in the thread t, call them P t (check out Section 4.1 for more), and then we remove p from that group: P t  p. After that, we figure out what percentage of the remaining group are women.",
        "formal_text": "Convert casual text to formal text: The idea of \"gender environment\" is all about who's in the group when people are talking or communicating. In the studies we looked at (like Holmes and Stubbe, 2003, and"
    },
    {
        "casual_text": "Al-Sabbagh and the gang (2013) looked at how people use words that show uncertainty in tweets, both in Modern Standard Arabic (MSA) and Egyptian Arabic (EA). They got two people to tag these words and see how often they agreed. For the meaning part, they agreed 90% of the time, and for how big a part of the sentence the word affected, they agreed 93% of the time. But they only did this for 548 words that showed uncertainty.",
        "formal_text": "Convert casual text to formal text: Al-Sabbagh and the gang (2013) looked at how people use words that show uncertainty in tweets, both in Modern Standard Arabic (MSA) and Egyptian Arabic (EA). They got"
    },
    {
        "casual_text": "Multi-Task Learning (MTL) is all about boosting the performance of a neural network by letting it share some of its parts with other networks that are trained to do different tasks. This idea goes back to Caruana in 1997. By sharing weights, the shared parts get better training and the model ends up with more detailed and useful representations. If you're interested in diving deeper into the topic, check out the reviews by Ruder (2017) and Zhang and Yang (2018) for a broader look at how MTL is used in NLP.",
        "formal_text": "Convert casual text to formal text: Multi-Task Learning (MTL) is all about boosting the performance of a neural network by letting it share some of its parts with other networks that are trained to do different"
    },
    {
        "casual_text": "When translating from right to left, the left side of the sentence (which is the later part in this case) is actually more accurate than the right side. This is kind of weird because earlier studies said that mistakes build up over time, making the end of the translation worse. But here, the end is better, which doesn't really match what we thought before.",
        "formal_text": "Convert casual text to formal text: When translating from right to left, the left side of the sentence (which is the later part in this case) is actually more accurate than the right side. This is kind of weird because earlier studies said"
    },
    {
        "casual_text": "A response with a bigger s mtv (y = 1|c, r) is more likely to get picked. We'll look into other combination models later on.",
        "formal_text": "Convert casual text to formal text: A response with a bigger s mtv (y = 1|c, r) is more likely to get picked. We'll look into other combination models later."
    },
    {
        "casual_text": "Check out Table 3 for the results of our method and some other recent work we thought was relevant. We made sure to include all the systems that reported results under the same conditions as us—basically, using gold mentions and the same datasets. We left out two big studies that only reported results for a version of the task that involves finding mentions (Haghighi and Klein, 2010; Stoyanov, 2010). The Haghighi and Klein (2009) results come in two flavors: with semantics (+S) and without (S). To see how much our multi-pass system adds, we also ran a single-pass version of our system that uses all the same features as the multi-pass one (marked as \"single pass\" in the table).",
        "formal_text": "Convert casual text to formal text: Check out Table 3 for the results of our method and some other recent work we thought was relevant. We made sure to include all the systems that reported results under the same conditions as us—basically,"
    },
    {
        "casual_text": "Metaphors play a big role in how we think, and if we want to create systems that understand language in a way that makes sense to us, we need to focus on incorporating them better in the future. So, any effort in this area is pretty significant.",
        "formal_text": "Convert casual text to formal text: Metaphors play a big role in how we think, and if we want to create systems that understand language in a way that makes sense to us, we need to focus on incorporating"
    },
    {
        "casual_text": "Alright, here's how we set up our experiment. We do a 5-fold cross-validation. First, we shuffle our data randomly, then we divide it into folds that stay the same for all our tests. In our experiments, we change the B(oth) instances to Y instances, which means we're focusing on affixoid uses. The seven instances that the annotators weren't sure about? We leave those out. We use the features we talked about in section 4.1 that can be automatically extracted. For our classifier, we use SVM (Vapnik, 1995), specifically the SVM light version (Joachims, 1998). We also played around with the cost-factor, which makes mistakes on positive examples (Y class) more important than mistakes on negative examples (N class). But changing this only helped in the worst-performing setups where we'd otherwise just match the majority baseline. Table 5 shows the results for different features using SVM light's default settings. The majority baseline we mention is a classifier that always predicts non-affixoid because that's the most common class for each suffixoid and in the whole dataset. Here's a quick look at some numbers: - Compositionality: 0.66, 0.00, 0.00, 0.00, 0.66, 1.00, 0.79, 0.33, 0.50, 0.40 - Polarity: 0.66, 0.00, 0.00, 0.00, 0.66, 1.00, 0.81, 0.33, 0.50, 0.40",
        "formal_text": "Convert casual text to formal text: Alright, here's how we set up our experiment. We do a 5-fold cross-validation. First, we shuffle our data randomly, then we divide it into folds"
    },
    {
        "casual_text": "Sure! Here's the informal version: If you want to compare different machine translation systems based on their scores, you can only say one method or algorithm is better if they’ve all been trained, tested, and validated using the exact same pre-processed data. The only exception is if the method or algorithm you’re using actually relies on a specific dataset or pre-processing step.",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: If you want to compare different machine translation systems based on their scores, you can only say one method or algorithm is better if they’ve all been"
    },
    {
        "casual_text": "In this part, we’ll talk about earlier attempts to use neural networks to handle word coherence across sentences and also look at work aimed at boosting machine translation by considering the bigger picture (discourse level). Back in 2012, Mikolov and Zweig came up with a RNN-LDA model to create a context-aware language model. They added contextual info to the usual RNNLM by using a real-valued input vector, which was basically the probability distribution from LDA topics based on a chunk of text before the current sentence. They trained an LDA model using documents from the Penn Treebank (PTB) dataset, each around 10 sentences long. Their method did better than the RNNLM in terms of perplexity on PTB data when using topic-based context instead of the full history of previous sentences. Then, in 2014, Le and Mikolov took the Continuous Bag-of-Words Model (CBOW) and Continuous Skip-gram Model (Skip-gram) (from Mikolov et al. in 2013) and added a paragraph vector. In their approach, the paragraph vector is learned similarly to how word vectors are, so if you have N paragraphs and each is mapped to P dimensions, you end up with N  P parameters. Our model, however, is a bit different. We learn sentence vectors using a RNN framework with almost unlimited sentence history. Instead of relying on the sentence ID, our sentence vectors are based solely on the words in the sentence, treating them as a bag of words as input.",
        "formal_text": "Convert casual text to formal text: In this part, we’ll talk about earlier attempts to use neural networks to handle word coherence across sentences and also look at work aimed at boosting machine translation by considering the bigger picture ("
    },
    {
        "casual_text": "With more software in our stuff, we're getting things out faster. But don't worry, we're still on top of getting translations done in time.",
        "formal_text": "Convert casual text to formal text: With more software in our stuff, we're getting things out faster. But don't worry, we're still on top of getting translations done in time."
    },
    {
        "casual_text": "In Section 1, we start by using Bleualign for unsupervised alignment. Instead of using length-based methods like Sennrich and Volk (2011) to create an initial training set, we train Monoses and use it to give Bleualign machine translations of the sentences we're trying to align. Monoses is trained by creating crosslingual word embeddings from monolingual corpora using word2vec and Vecmap (Artetxe et al., 2018a), which helps build a phrase table. We then train a Statistical Machine Translation (SMT) system with this data and use it to translate a monolingual corpus from one of the two languages. The translated data is then used to train an SMT system in the other direction. We build a new phrase table and repeat the process three times to get the final model.",
        "formal_text": "Convert casual text to formal text: In Section 1, we start by using Bleualign for unsupervised alignment. Instead of using length-based methods like Sennrich and Volk (2011) to create an initial training set, we train Monoses and"
    },
    {
        "casual_text": "The attention weights determine how much each hidden state contributes. In this case, the connection between the words \"sat\" and \"mat\" can be clearly picked up.",
        "formal_text": "Convert casual text to formal text: The attention weights determine how much each hidden state contributes. In this case, the connection between the words \"sat\" and \"mat\" can clearly picked up."
    },
    {
        "casual_text": "Here's how this paper is set up: In Section 2, we’ll give a quick overview of the hybridization approach we used for machine translation, especially for regular text. Then, in Section 3, we’ll talk about how we handle dialects and normalize them, especially for messy or dialectal text. Section 4 will cover some experiments we did and what we learned from them. Finally, we’ll wrap things up in Section 5.",
        "formal_text": "Convert casual text to formal text: Here's how this paper is set up: In Section 2, we’ll give a quick overview of the hybridization approach we used for machine translation, especially for regular text. Then, in"
    },
    {
        "casual_text": "The PERIN system uses XLM-R (Conneau et al., 2019) on the encoder side to create contextualized token embeddings. On the decoder side, it employs different attention heads to handle multiple tasks: predicting node labels, identifying anchors for nodes, and figuring out the edges between nodes along with their labels. Since the node label set is usually huge, instead of trying to predict the labels directly, PERIN narrows down the options by predicting \"relative rules.\" These rules help map surface token strings to node labels in meaning representation graphs, which is kind of like how Factored Concept Labels work in Wang and Xue (2017). Another cool thing about PERIN is that it’s trained with a permutation-invariant loss function. This means the loss value stays the same no matter how the nodes in the graph are ordered. This is useful because nodes in most MRP 2020 meaning representation graphs are unordered. By using this loss function, the system avoids penalizing the model for producing the right nodes in a different order than what’s in the training data.",
        "formal_text": "Convert casual text to formal text: The PERIN system uses XLM-R (Conneau et al., 2019) on the encoder side to create contextualized token embeddings. On the de"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We're trying to figure out P (r, s, t), where r and s are independent of each other, but t depends on both r and s. The rules we're dealing with aren't tied to specific word forms; instead, they're based on the sounds and endings of the words. We think that the rule t relies on some features pulled from both r and s, which we'll call f (r, s). So, P (r, s, t) can be split up like this:",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. We're trying to figure out P (r, s, t), where r and s are independent of"
    },
    {
        "casual_text": "Active learning goes beyond just IRT. Analyzing how training progresses and using active learning techniques (Settles, 2009) can help pick out specific items or spot low-quality ones (Brodley and Friedl, 1999). For instance, Swayamdipta et al. (2020) and Pleiss et al. (2020) came up with ways to use training dynamics to find tricky items and annotation mistakes. Rahman et al. (2020) even used active learning to create a test collection. By measuring how well examples can differentiate between the best and the rest, test set creators can \"zoom in on the key areas\" (Boyd-Graber and Börschinger, 2020), focusing on examples that highlight the most interesting differences between different systems.",
        "formal_text": "Convert casual text to formal text: Active learning goes beyond just IRT. Analyzing how training progresses and using active learning techniques (Settles, 2009) can help pick out specific items or spot low-quality ones ("
    },
    {
        "casual_text": "Our findings show that if we add term-to-term alignment info and then do EVD, we can make cross-language IR work way better.",
        "formal_text": "Convert casual text to formal text: Our findings show that if we add term-term alignment info and then do EVD, we can make cross-language IR work way better."
    },
    {
        "casual_text": "In this paper, we use the word2vec tool to pre-train word embeddings on the NYT corpus. We only keep words that show up more than 100 times in the corpus for our vocabulary. We tweak our model using the validation set, and we use grid search to find the best parameters, which are highlighted in bold. For the learning rate of SGD, we try out  values like 0.1, 0.01, and 0.001.",
        "formal_text": "Convert casual text to formal text: In this paper, we use the word2vec tool to pre-train word embeddings on the NYT corpus. We only keep words that show up more than 100 times in the"
    },
    {
        "casual_text": "Check out Figure 3 for how the joint system is set up. The parser and semantic role labeler are linked in the embedding layer, using the same vector lookup tables for characters, words, and POS tags. Basically, the Bi-LSTM model from Section 2.1 is used for the SRL task, while the Stack-LSTM model from Section 2.2 handles the parsing task. Both the Bi-LSTM labeler and Stack-LSTM parser share the embedding layer. During training, we focus on minimizing the loss from both the semantic role labeler and the parser, which both affect the embedding layer. This helps create better vector representations for each token, making both tasks more accurate. However, since the neural structures are different, they don’t share any other parameters. This joint model is a simpler version of shared training (like what Collobert et al. did in 2011), but it doesn’t use shared decoding (like Sutton and McCallum in 2005 or Zhang and Clark in 2008b). Instead, syntax and semantic roles are assigned separately, which avoids messing things up due to errors.",
        "formal_text": "Convert casual text to formal text: Check out Figure 3 for how the joint system is set up. The parser and semantic role labeler are linked in the embedding layer, using the same vector lookup tables for characters,"
    },
    {
        "casual_text": "The integration of existing morphological processing tools has resulted in a really cool CAI tool. This tool offers a dictionary lookup, shows examples from real texts, and displays morphological info—all online. Adding support for other languages would be pretty straightforward since the basic framework is already in place. Even though the prototype is still a work in progress, the early results are looking great. By February, it was advanced enough for some communication students to do a user study. We'll share the full details later, but the study showed that people are interested in using it. In the near future, we're planning to organize the text data by lexemes and also want to add features like a teaching or diagnosing module to make the tool even better and more like real CAL software.",
        "formal_text": "Convert casual text to formal text: The integration of existing morphological processing tools has resulted in a really cool CAI tool. This tool offers a dictionary lookup, shows examples from real texts, and displays morph"
    },
    {
        "casual_text": "RelationFactory is super handy for testing out random queries and tweaking its settings on the fly, so you can instantly see what’s happening. You can easily switch different expansion methods, validation tools, and patterns on or off. This helps you figure out where the slowdowns or mistakes are coming from in relation extraction. Plus, the demo lets you check out how it handles different types of text—like stuff from Wikipedia or various TAC KBP corpora, which include news articles and web content. It also comes with a bunch of diagnostic tools. If you have a gold key for your queries, you can break down the errors and even see examples of specific types of mistakes. For instance, the tool for missed recall does a few checks to help you out.",
        "formal_text": "Convert casual text to formal text: RelationFactory is super handy for testing out random queries and tweaking its settings on the fly, so you can instantly see what’s happening. You can easily switch different expansion methods, validation"
    },
    {
        "casual_text": "Okay, so basically, in classical stats, if you use a model trained with Maximum Likelihood Estimation (MLE) and you have enough data, it should work well on new data. But, when it comes to p test...",
        "formal_text": "Convert casual text to formal text: Okay, so basically, in classical stats, if you use a model trained with Maximum Likelihood Estimation (MLE) and you have enough data, it should work well"
    },
    {
        "casual_text": "Basically, our user study shows how important visual interfaces are for helping people label data in semi-supervised learning. We found that, compared to the old-school list format, using a scatterplot lets people make way more annotations without sacrificing accuracy, which leads to better machine learning models. We think this could really shake things up in active learning, especially when it comes to how we choose what data to label and how visual tools work with active learning.",
        "formal_text": "Convert casual text to formal text: Basically, our user study shows how important visual interfaces are for helping people label data in semi-supervised learning. We found that, compared to the old-school list format, using a"
    },
    {
        "casual_text": "This refers to the group of entities that have type information assigned to them. On the other hand, E u represents the group of entities that don't have any type information.",
        "formal_text": "Convert casual text to formal text: This refers to the group of entities that have type information assigned to them. On the other hand, E u represents the group of entities that don't have any type information."
    },
    {
        "casual_text": "Here's the English part of the preprocessed parallel training data for the German-English WMT 2017 News translation task, as it was shared on the website. It includes around 5.85 million sentences.",
        "formal_text": "Convert casual text to formal text: Here's the English part of the preprocessed parallel training data for the German-English WMT 2017 News translation task, as was shared on the website. It includes around 5.85 million sentences"
    },
    {
        "casual_text": "A key idea behind TexSmart is to really focus on creating and using unsupervised or lightly-supervised algorithms for a task. This is done with big amounts of structured, semi-structured, or unstructured data. The main goal here is to make it simpler to update our models by including new data without needing a ton of manual labeling by people.",
        "formal_text": "Convert casual text to formal text: A key idea behind TexSmart is to really focus on creating and using unsupervised or lightly-supervised algorithms for a task. This is done with big amounts of structured, semi-structured,"
    },
    {
        "casual_text": "Figuring out what things in a text are referring to and connecting them to the right entries helps us make sense of documents and search queries. A lot of research uses knowledge bases like Freebase (Chiu et al., 2014), YAGO (Yosef et al., 2011), and Dbpedia (Olieman et al., 2014). Wikify (Mihalcea and Csomai, 2007) was one of the first projects to link text to Wikipedia pages. It looks at all possible n-grams that match Wikipedia concepts, like links and titles, and considers them as potential candidates. They use a voting system that combines knowledge-based and data-driven methods to figure out the best match. Cucerzan (2007) uses four types of resources to find candidates: entity pages, redirect pages, disambiguation pages, and list pages. Then, they compare the context of the text with the information on Wikipedia pages, including category tags, to pick the right candidate. They also look at all the n-grams in the document and keep the ones that have a high enough probability. To narrow down the candidates, they use the structure of Wikipedia links, focusing on how common and related the terms are.",
        "formal_text": "Convert casual text to formal text: Figuring out what things in a text are referring to and connecting them to the right entries helps us make sense of documents and search queries. A lot of research uses knowledge bases like Freebase ("
    },
    {
        "casual_text": "Pretty much anything that can be thought of or talked about individually can be a discourse object. So, stuff like objects and facts stored in a database, what the user types in, the commands they give, and the system's responses—all of these are potential discourse objects. Terms like \"discourse elements\" (Sidner, 1984) and \"discourse entities\" (Webber, 1984) are used to describe the things that are \"specified\" or brought up in a conversation. These things and how they relate to each other make up the discourse model of the person speaking. Hayes (1984) calls the objects, events, commands, states (and so on) that a system needs to understand \"entities.\" Following that idea, I think of a discourse object as something very broad—basically, the whole universe of discourse is just a bunch of these objects. Even a relationship between discourse objects counts as a discourse object. Sometimes, that relationship might also be part of the description of one or more of the objects involved.",
        "formal_text": "Convert casual text to formal text: Pretty much anything that can be thought of or talked about individually can be a discourse object. So, stuff like objects and facts stored in a database, what the user types in, the commands they"
    },
    {
        "casual_text": "Normally, either DBA or SBA spits out some relation. If both DBA and SBA give results, we combine them. Basically, if DBA's correspondence score is above 0.8, we go with DBA's result. If not, we pick SBA's result. There are some rare situations where neither of them finds any relations, which just means the analysis didn’t work.",
        "formal_text": "Convert casual text to formal text: Normally, either DBA or SBA spits out some relation. If both DBA and SBA give results, we combine them. Basically, if DBA's correspondence"
    },
    {
        "casual_text": "Researchers have been looking into how to retrieve questions from CQA data, and one of the main challenges is dealing with word ambiguity and the lexical gap issue. Back in 2005, Jeon and his team suggested using a word-based translation model to tackle the lexical gap problem automatically. Then, in 2008, Xue and colleagues came up with a word-based translation language model specifically for question retrieval. Their findings showed that this model not only improved retrieval results but also reached the best performance at the time. After that, people kept working on these word-based translation models, focusing on finding good parallel data to learn better translation probabilities. Lee and his team in 2008 tried to refine these probabilities by using question-answer pairs and picking the most important terms to create more efficient translation models. Later, in 2009, Bernhard and Gurevych proposed using definitions and glosses from different sources as parallel training data. Finally, in 2010, Cao and his team added category information to the word-based translation model for question retrieval, exploring a new way to improve it.",
        "formal_text": "Convert casual text to formal text: Researchers have been looking into how to retrieve questions from CQA data, and one of the main challenges is dealing with word ambiguity and the lexical gap issue. Back in 2005, Jeon"
    },
    {
        "casual_text": "Okay, so here's what you're asking for in a simpler way: You want to access the 2013 TAC evaluation queries, and you're pointing to the response for 2013. The run directory is set to /TAC_RUNS/run2013/, the index is located at /TAC_CORPORA/2013/index, and the list of relations you're using is in /CFG/rellist2013. Lastly, the configuration file for the relations is at /CFG/relations2013.config.",
        "formal_text": "Convert casual text to formal text: Okay, so here's what you're asking for in a simpler way: You want to access the 2013 TAC evaluation queries, and you're pointing to the response for 2013. The"
    },
    {
        "casual_text": "ERD14 3 is a dataset from the ERD Challenge (Carmel et al., 2014) and has two parts: one for long texts and one for short texts. In this paper, we're only looking at the short-text part. It has 500 queries for development and another 500 for testing. Since there's no training set, we use the development set for training and tweaking our model. This dataset can be evaluated using either Freebase or Wikipedia, as the challenge organizers provided a mapping between the two, showing how entities in one match up with the other. We decided to use Wikipedia for our evaluation.",
        "formal_text": "Convert casual text to formal text: ERD14 3 is a dataset from the ERD Challenge (Carmel et al., 2014) and has two parts: one for long texts and one for short texts. In this"
    },
    {
        "casual_text": "For ACD, the input is basically a sentence X = x1, ..., xn = x1:n, with each xi representing the i-th word. For ACSA, you also get a list of pre-identified aspect categories. In Section 3.1, we talk about the pre-trained language models we use. Then, in Section 3.2, we cover the classification methods. Section 3.3 is all about MLM methods, and in Section 3.4, we explain our generation method.",
        "formal_text": "Convert casual text to formal text: For ACD, the input is basically a sentence X = x1, ..., xn = x1:n, with each xi representing the i"
    },
    {
        "casual_text": "When breaking down words, MORPA keeps in mind that Dutch word stems can change their spelling when they're inflected or used as the base for a new word. We don't need to dive deep into the spelling rules here, but in (3), you can see how things like \"vowel gemination\" and \"devoicing of stem-final consonants\" come into play. For more details, check out [Heemskerk and van Heuven, 1993].",
        "formal_text": "Convert casual text to formal text: When breaking down words, MORPA keeps in mind that Dutch word stems can change their spelling when they're inflected or used as the base for a new word. We don"
    },
    {
        "casual_text": "Basically, when k = 100 and p = 0.9, we used a loss truncation method with c = 0.6 for summarization. This gave us entropies of 18.08, 20.01, and 17.93. On the other hand, with k = 2 and p = 0.4, we used rejection sampling for summarization at c = 0.6 and  = 0.1, which resulted in entropies of 3.71, 4.02, and 3.84.",
        "formal_text": "Convert casual text to formal text: Basically, when k = 100 and p = 0.9, we used a loss truncation method with c = 0.6 for summarization. This gave us"
    },
    {
        "casual_text": "Okay, so let’s break this down in simpler terms. First, we’re using something called a Siamese dual-encoder (Lowe et al., 2017) to encode both the context and the responses. Now, let’s talk about the decoder. The decoder starts with the context encoding and generates a response one token at a time. Each new token depends on the ones that came before it. The decoder is trained to make sure it’s generating the right words by minimizing something called log-perplexity for each word in the correct response. But here’s the issue: when we train the decoder on real conversation logs, it often ends up giving generic or totally irrelevant responses. To fix this, we add a classifier to the decoder. At each step of decoding, this classifier checks if the response so far is relevant to the conversation context. The classifier is trained to say \"yes\" (relevance of 1) if the response matches the correct answer and \"no\" (relevance of 0) if it’s just some random response. At the same time, the decoder is still working on minimizing the word loss, which is the log-perplexity for each word in the correct response. So, for any response r, the relevance loss can be written like this:",
        "formal_text": "Convert casual text to formal text: Okay, so let’s break this down in simpler terms. First, we’re using something called a Siamese dual-encoder (Lowe et al."
    },
    {
        "casual_text": "From what we know, this is the first time someone has tried to classify an account of any kind of sexism without assuming that the categories can't overlap.",
        "formal_text": "Convert casual text to formal text: From we know, this is the first time someone tried to classify an account of any kind of sexism without assuming that the categories can't overlap. Convert casual text to"
    },
    {
        "casual_text": "First, we set up three solid baseline models: one using plain ELMo (Experiment 1.1), another combining ELMo with BLSTM and CRF, as suggested by Peters et al. (2018) (Experiment 1.2), and a multilingual BERT model from Devlin et al. (2019) (Experiment 1.3). We tried running ELMo and BERT with their weights frozen, but we got the best results when we fine-tuned the whole models. You can check out the results in Table 2.",
        "formal_text": "Convert casual text to formal text: First, we set up three solid baseline models: one using plain ELMo (Experiment 1.1), another combining ELMo with BLSTM and CRF, as suggested by Peter"
    },
    {
        "casual_text": "Okay, so we're dealing with minimizing z, which depends on A and B. The function F(z) is defined as: F(z) = -l * sum from i=1 to l of [t_i * log(p_i) + (1 - t_i) * log(1 - p_i)]. Basically, it's a formula that sums up some terms involving logs and probabilities, with t_i and p_i in there.",
        "formal_text": "Convert casual text to formal text: Okay, so we're dealing with minimizing z, which depends on A and B. The function F(z) is defined as: F(z) = -l * sum from"
    },
    {
        "casual_text": "Dealing with exact inference in complex Bayesian models can be really tough, so we usually have to rely on approximate methods. Blei and his colleagues came up with a variational EM algorithm for LDA back in 2003. Then, Griffiths and Steyvers showed in 2004 how Gibbs sampling could be used for approximate inference in LDA. Gibbs sampling is a type of Markov chain Monte Carlo algorithm, and that's the method we're using in this paper. It's pretty straightforward to figure out, works at a similar speed to other methods, and gets close to the global maximum. On the other hand, EM algorithms might only reach a local maximum.",
        "formal_text": "Convert casual text to formal text: Dealing with exact inference in complex Bayesian models can be really tough, so we usually have to rely on approximate methods. Blei and his colleagues came up with a variational"
    },
    {
        "casual_text": "On the encoder side, we add dependency syntax structures to the inputs, which have been shown to work well for similar tasks like RST discourse parsing. As mentioned in Section 4.1, we use a GAT module to represent the encoder output. We turn the document into a dependency graph using an undirected adjacent matrix, created with a standard dependency parser (Chen and Manning, 2014). Then, we update the hidden states of each node using a multi-layer GAT network on this matrix, called A.",
        "formal_text": "Convert casual text to formal text: On the encoder side, we add dependency syntax structures to the inputs, which have been shown to work well for similar tasks like RST discourse parsing. As mentioned in Section 4.1,"
    },
    {
        "casual_text": "First off, it's tricky to figure out exactly what something is when it could be one of a few similar things. Take the sentence \"Nitrogen exists naturally in the atmosphere. Bacteria in soil fix the nitrogen. Nitrogen is now usable by living things.\" Here, \"fixed nitrogen\" and \"gas-based nitrogen\" are two different things, but they're both about nitrogen. A model would struggle to tell which one \"nitrogen\" is referring to in each part of the sentence.",
        "formal_text": "Convert casual text to formal text: First off, it's tricky to figure out exactly what something is when it could be one of a few similar things. Take the sentence \"Nitrogen exists naturally in the atmosphere. Bac"
    },
    {
        "casual_text": "To make it easier to explain and understand how complicated the algorithm is, we need to introduce a few more terms related to the hierarchy. Besides the sequence length L, the number of hierarchy levels M, and the embedding or feature size d from Eq. (1), we’re adding these new terms: 1) N r: the numerical rank of the off-diagonal blocks (for example, it’s 2 in Eq. (20)). This also happens to be the size of the diagonal blocks at the lowest level (level-0); 2)",
        "formal_text": "Convert casual text to formal text: To make it easier explain and understand how complicated the algorithm is, we need to introduce a few more terms related to the hierarchy. Besides the sequence length L, the number of hierarchy levels M,"
    },
    {
        "casual_text": "In Section 3, we talked about how CDRNN uses distributional regression, which means it has an IRF that shows how predictors affect the spread of the prediction over time. You can visualize the IRF for the variance just like you would for the mean.",
        "formal_text": "Convert casual text to formal text: In Section 3, we talked about how CDRNN uses distributional regression, which means it has an IRF that shows how predictors affect the spread of the prediction over time. You can visualize the"
    },
    {
        "casual_text": "Sure! Here's a more casual version: \"Softmax (GPT-2): We're using just one softmax, the input hidden state, and the partition shown in Figure 2 (a) and Equation 1. The basic setup is the same as the original GPT-2, but we've added an extra linear layer that changes the hidden state h M ct into the facet embedding f ct, 1, just like in other methods.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: \"Softmax (GPT-2): We're using just one softmax, the input hidden state, and the partition shown in Figure 2"
    },
    {
        "casual_text": "We’ve got some evaluation results here. We tested on the CoNLL-2014 benchmark using the official M2 scorer (Dahlmeier and Ng, 2012), and also on the BEA-2019 and FCE benchmarks with ERRANT for scoring.",
        "formal_text": "Convert casual text to formal text: We’ve got some evaluation results here. We tested on the CoNLL-2014 benchmark using the official M2 scorer (Dahlmeier and Ng, 2012), and also"
    },
    {
        "casual_text": "The left side of the equation represents the reconstruction loss (L R ), while the right side is the KL loss (L KL ). The KL loss helps push the latent space closer to a predefined prior, making the latent space more structured and regular.",
        "formal_text": "Convert casual text to formal text: The left side of the equation represents the reconstruction loss (L R ), while the right side is the KL loss (L KL ). The KL loss helps push the latent space closer"
    },
    {
        "casual_text": "4) The term [voiced] mentioned in the description of FD likely stands for a specific setup of the larynx or vocal cords. In (5), it's described as a particular arrangement of glottal states.",
        "formal_text": "Convert casual text to formal text: 4) The term [voiced] mentioned in the description of FD likely stands for a specific setup of the larynx or vocal cords. In (5), it's described"
    },
    {
        "casual_text": "It's common to think about how we understand metaphors as involving the idea of transferring properties and relationships from one thing (the source) to another (the target). In this way, a metaphorical statement gives us information about the target by showing how things in the source and target connect. Lakoff and his team suggest that these connections between the source and target are part of broader patterns called \"conceptual metaphors\" (Lakoff, 2004), which we can also call \"metaphorical views.\" For example, with the GPS T-H pair we talked about earlier, a metaphorical view like MIND AS PHYSICAL SPACE would highlight the link between the mind (source) and something like a special container or incubator (target). Most of the computer-based approaches to dealing with metaphors have been about creating systems that can take a metaphorical phrase, do some thinking, and come up with the right answer—which a researcher already knows. The big challenges here have been making these systems work on a larger scale and actually testing how well they perform. Plus, figuring out the meaning of a text like \"the GPS being incubated in the mind of Ivan Getting\" is super tricky because you have to sift through all the possible ways \"incubating\" could be understood and decide which one is the right one.",
        "formal_text": "Convert casual text to formal text: It's common to think about how we understand metaphors as involving the idea of transferring properties and relationships from one thing (the source) to another (the target). In this way,"
    },
    {
        "casual_text": "We're comparing our method to these baselines: (1) ITDD: it's a Transformer-based model that handles multi-turn dialogues and knowledge by building them up step by step and then decoding the response in two stages (Li et al., 2019); (2) BART cat:",
        "formal_text": "Convert casual text to formal text: We're comparing our method to these baselines: (1) ITDD: it's a Transformer-based model that handles multi-turn dialogues and knowledge by building them step by step and"
    },
    {
        "casual_text": "In this paper, we introduced a new word-based labelling method for Thai sentence boundary detection (SBD) for the first time. We used a Linear Chain Conditional Random Field (LCRF) for sequence labelling, which worked really well compared to previous results on the ORCHID dataset. We also looked at how POS tagging could help with SBD using the TaLAPi corpus. We compared cascade models and joint models, and came up with a \"2-step\" joint model that combines POS tagging and SBD. This new model was more than twice as fast as the \"1-step\" joint model, but still got almost the same accuracy for SBD when using the same features. With the extra speed, we could add more features to improve SBD while keeping POS tagging performance on par.",
        "formal_text": "Convert casual text to formal text: In this paper, we introduced a new word-based labelling method for Thai sentence boundary detection (SBD) for the first time. We used a Linear Chain Conditional Random Field ("
    },
    {
        "casual_text": "So, on the brain cancer test set, the F1 score was 566, which is way lower than what the MTL Model got. Here's a breakdown of the flops/inst and total instances for different models: - **MTL Model**: 218,767,889 flops/inst, 20k instances, Ratio OP 1 - **OP+MTL**: 218,783,260 flops/inst, 20k instances, Ratio OP 1 - **Multi-pass**: 218,724,880 flops/inst, 427k instances, Ratio OP 23 - **Multi-pass+Silver**: 218,724,880 flops/inst, 497k instances, Ratio OP 25 Table 4 shows the computational complexity in terms of flops per instance and the total number of instances.",
        "formal_text": "Convert casual text to formal text: So, on the brain cancer test set, the F1 score was 566, which is way lower than what the MTL Model got. Here's a breakdown of the flops/"
    },
    {
        "casual_text": "The system works with a dialogue manager that handles stuff like speech recognition, speech synthesis, and inference by using OAA-solvables. It can also ask for details like the robot's location and what's going on around it.",
        "formal_text": "Convert casual text to formal text: The system works with a dialogue manager that handles stuff like speech recognition, speech synthesis, and inference by using OAA-solvables. It can also ask for details like the robot'"
    },
    {
        "casual_text": "Instead, we use a Recurrent Neural Network (RNN) language model (LM). We gather 94,882 English songs (which have about 32 million word tokens) to train our model. We train a two-layer recurrent network with Long Short-Term Memory (LSTM) units, which were introduced by Hochreiter and Schmidhuber back in 1997. When we decode using this LM, we use a beam search that’s guided by a Finite State Automaton (FSA). Each beam state C t, i is made up of a tuple containing (h, s, word, score). Here, h represents the hidden states of the LSTM at step t in the ith state, and s is the FSA state at the same step t in the ith state. The model generates one word at a time. At the start, h 0, 0 is the initial hidden state of the LSTM, s 0, 0 is the starting state of the FSA, word 0, 0 is set to START>, and score 0, 0 is set to 0. To expand a beam state C t, i, we first feed h t, i and the current word into the LM, which gives us an updated hidden state h next. The LM also provides a probability distribution P (V ) over the entire vocabulary V for the next word. Then, for each succeeding state s suc of s t, i in the FSA and for each word w next along the edges from s t, i to s suc, we create a new state (h next, s suc, w next, score t, i + log(P (w next ))) and add it to the next beam.",
        "formal_text": "Convert casual text to formal text: Instead, we use a Recurrent Neural Network (RNN) language model (LM). We gather 94,882 English songs (which have about 32 million word tokens) to train"
    },
    {
        "casual_text": "Apart from just ranking sentences, there are methods that focus on ranking concepts to pick sentences. One approach uses centroid words with tf*idf scores above a certain threshold. Filatova and Hatzivassiloglou (2004) took a different route and used atomic events as their concept. Additionally, metrics like Basic Element (Hovy et al., 2006), ROUGE (Lin and Hovy, 2003), and Pyramid (Passonneau et al., 2005) all measure how well the concepts in generated summaries match those in human-written ones.",
        "formal_text": "Convert casual text to formal text: Apart from just ranking sentences, there are methods that focus on ranking concepts to pick sentences. One approach uses centroid words with tf*idf scores above a certain threshold. Filat"
    },
    {
        "casual_text": "Hey, so here's something important to keep in mind: the bottom part of the probability in Equation 2 is based on scores from every possible sequence of tags. Let's say there are Y unique tags in total. The calculation for the sequence probability would normally take O(Y T ) time (we're skipping the BLSTM feature stuff for now). But, since the CRF layer only deals with first-order dependencies, we can use dynamic programming during training. This trick helps cut down the time complexity to O(T Y 2 ).",
        "formal_text": "Convert casual text to formal text: Hey, so here's something important to keep in mind: the bottom part of the probability in Equation 2 is based on scores from every possible sequence of tags. Let's say there are"
    },
    {
        "casual_text": "This paper talks about a system that can quickly and accurately fix spelling mistakes, even with proper names. The algorithm they came up with, called PM, solves the problem of slow speed and high memory use that other methods, like edit distance, have.",
        "formal_text": "Convert casual text to formal text: This paper talks about a system that can quickly and accurately fix spelling mistakes, even with proper names. The algorithm they came up with, called PM, solves the problem of slow speed and high memory"
    },
    {
        "casual_text": "Zhai and his team came up with a tool called ccMix back in 2004 to tackle something called comparative text mining (CTM). Basically, if you have news articles about the same event but from different sources, ccMix helps you figure out the common stuff shared by all the sources and also the unique bits that only one source talks about.",
        "formal_text": "Convert casual text to formal text: Zhai and his team came up with a tool called ccMix back in 2004 to tackle something called comparative text mining (CTM). Basically, if you have news"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: We have a set of sentence pairs: C = (S1, T1), ..., (S_N, T_N). We also have a maximum number of iterations we want to do, which is M, and a depth value  for something called \"seg rev.\" The goal is to get a word alignment A = A1, ..., A_N. Here's how we do it step by step: 1. Start with an empty alignment, so A is empty. 2. Loop through the iterations from 1 to M: - Set I and C to empty. - If A is still empty: - Go through each sentence pair from 1 to N. - Use \"seg rev\" with depth  on the pair (S_n, T_n, A_n) to get RS and RI. - Add (RS, T_n) to the set C.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: We have a set of sentence pairs: C = (S1, T1), ..., (S_N,"
    },
    {
        "casual_text": "Alright, so P(f) doesn't depend on the domain D. Also, we're just assuming that P(D) stays the same, like a constant. The formula we're looking at here is all about finding the best domain. It's like trying to figure out the argmax.",
        "formal_text": "Convert casual text to formal text: Alright, so P(f) doesn't depend on the domain D. Also, we're just assuming that P(D) stays the same, like a constant. The formula"
    },
    {
        "casual_text": "There are two main methods for SWTs, called distributional and distributed semantics (Hermann and Blunsom, 2014). Both methods use word vectors. In the first approach, the vectors are sparse, high-dimensional, and explicit (Levy and Goldberg, 2014). In the second one, they're dense, low-dimensional, and more generalized. Both approaches are based on the distributional hypothesis (Harris, 1968), which says that words and their translations usually show up in similar contexts.",
        "formal_text": "Convert casual text to formal text: There are two main methods for SWTs, called distributional and distributed semantics (Hermann and Blunsom, 2014). Both methods use word vectors. In the first approach, the vector"
    },
    {
        "casual_text": "The Lucy experiment has really pushed the boundaries of using canonical representations, making it possible to keep things simple with a conduit control model where sentences are the main unit of data. But this has had some big effects on the different parts of the system, especially the knowledge sources in each module. Let's take a look at how this has impacted the grammar, the relationship between syntax and semantics, and the parts of semantics that deal with making sure terms in the knowledge base are consistent.",
        "formal_text": "Convert casual text to formal text: The Lucy experiment has really pushed the boundaries of using canonical representations, making it possible to keep things simple with a conduit control model where sentences are the main unit of data. But this"
    },
    {
        "casual_text": "So, MACHAMP is pretty flexible and works with the latest general-purpose NLP stuff, which we’ll talk about in Section 2.2. The core of MACHAMP is AllenNLP, a Python library built on PyTorch (Paszke et al., 2019). It’s got a bunch of modules for different deep learning techniques and NLP tasks. The idea is to make it modular, easy to use, and adaptable. It’s worth mentioning that around the same time MACHAMP came out, another toolkit called jiant (Pruksachatkun et al., 2020) was also developed. Plus, AllenNLP started supporting multi-task learning with its release 2.0. MACHAMP stands out because it’s super easy to configure and works with a wide range of multi-task setups.",
        "formal_text": "Convert casual text to formal text: So, MACHAMP is pretty flexible and works with the latest general-purpose NLP stuff, which we’ll talk about in Section 2.2. The core of MACHAMP is AllenNLP"
    },
    {
        "casual_text": "In this paper, we came up with a MANN model that uses external knowledge. Turns out, this model did 7% better than the best models out there. We took a closer look at some of the data from SCT to see how our model performed. The success of our model isn’t just because of its well-thought-out design, but also because it combines semantic relationships it learned from outside sources.",
        "formal_text": "Convert casual text to formal text: In this paper, we came up with a MANN model that uses external knowledge. Turns out, this model did 7% better than the best models out there. We took a closer look"
    },
    {
        "casual_text": "When it comes to figuring out why people revise stuff, Zhang and Litman (2016) looked at both the argumentative parts of writing and surface-level changes based on Faigley and Witte (1981). They came up with eight categories of revision reasons, like changing claims or ideas, improving reasoning, dealing with rebuttals, organizing better, making things clearer, and so on. Tan and Lee (2014) focused on how revisions affect the strength of statements in academic writing. There's also a lot of research on spotting specific types of revisions in Wikipedia, like figuring out if a change is vandalism (Harpalani et al., 2011; Adler et al., 2011) or if it introduces language bias or tries to maintain a neutral point of view (Recasens et al., 2013). Instead of just identifying one specific type of revision intention at a time, our goal is to create a big, organized system to categorize all the different reasons people make edits.",
        "formal_text": "Convert casual text to formal text: When it comes to figuring out why people revise stuff, Zhang and Litman (2016) looked at both the argumentative parts of writing and surface-level changes based on Faigley and"
    },
    {
        "casual_text": "In the usual method, we notice that some words around the main word aren't really connected to it. Generally, the farther away a context word is from the main word, the less they have to do with each other. This becomes even clearer when we remove common words like \"the\" or \"and.\" Sometimes, a word that was far from the main word ends up in the context window, making the context less useful for understanding the main word. To fix this, we suggest using a weighted co-occurrence that changes based on how far apart the two words are. We call this Distance-Sensitive Co-occurrence (DSC):",
        "formal_text": "Convert casual text to formal text: In the usual method, we notice that some words around the main word aren't really connected to it. Generally, the farther away a context word is from the main word, the less"
    },
    {
        "casual_text": "The kappa scores for the petfoodrecall and vtech datasets were pretty consistent among the raters, showing good agreement. However, the spitzer dataset didn't do as well—there was only fair agreement. As for the iPhone dataset, the evaluators had a lot of disagreement on what they thought were subtopics.",
        "formal_text": "Convert casual text to formal text: The kappa scores for the petfoodrecall and vtech datasets were pretty consistent among the raters, showing good agreement. However, the spitzer dataset didn't"
    },
    {
        "casual_text": "We tackled this optimization problem using a branch-and-bound algorithm, which was first introduced by Land and Doig back in 1960. This graph alignment problem is generally considered NP-hard, as Klau pointed out in 2009, and is typically solved in an approximate way, kind of like how beam search works. But, the unique structure of constraints 1 to 3, which come from the requirement that the alignment function must be injective (one-to-one), lets us solve this optimization exactly. Now, our version of the branch-and-bound algorithm doesn’t usually run in polynomial time, but in practice, we found it works pretty well. We were able to compute optimal alignments efficiently in almost all cases—only less than 0.1% of the alignment pairs in our data took too long to solve. This good performance is mainly because we don’t have to deal with aligning entire graphs, and the number of nodes in the subgraphs we’re aligning is limited.",
        "formal_text": "Convert casual text to formal text: We tackled this optimization problem using a branch-and-bound algorithm, which was first introduced by Land and Doig back in 1960. This graph alignment problem is generally considered NP-hard"
    },
    {
        "casual_text": "Alright, so once the file is properly uploaded into the system, the app will unlock the function tabs, which let you use all the cool features of the web system. There are four main tabs in the application:",
        "formal_text": "Convert casual text to formal text: Alright, so once the file is properly uploaded into the system, the app will unlock the function tabs, which let you use all the cool features of the web system. There are four main tab"
    },
    {
        "casual_text": "Lately, the Mixture of Softmax (MoS) approach (Yang et al., 2018) has been getting a lot of attention as one of the few tweaks that actually works for transformer language models (Narang et al., 2021; Anonymous, 2021). At the same time, Parthiban et al. (2021) pointed out that the theory behind the softmax bottleneck (Yang et al., 2018) doesn't fully explain why MoS improves things. To address this, our findings give some geometric insights into why and when MoS and similar multiple embedding methods perform better. We also suggest that even with a huge hidden state size, the softmax bottleneck might not be fully resolved. For instance, no matter how big the hidden state size gets, if \"queen\" minus \"king\" equals \"woman\" minus \"man\" in the embedding space, language models can't predict words along the longer diagonal of a parallelogram as the top two outputs.",
        "formal_text": "Convert casual text to formal text: Lately, the Mixture of Softmax (MoS) approach (Yang et al., 2018) has been getting a lot of attention as one of the few tweaks"
    },
    {
        "casual_text": "In this classification, we look at each word w i in a sentence with tags, T = (w 1 /p 1, . . . , w i /p i, . . . , w N /p N ), and describe it using a bunch of features. Sometimes, the POS tags we get are way too detailed, so we simplify things by creating a rough part-of-speech tag, q i, like this.",
        "formal_text": "Convert casual text to formal text: In this classification, we look at each word w i in a sentence with tags, T = (w 1 /p 1, . . . , w i"
    },
    {
        "casual_text": "We did a detailed look at our system and found out how awesome our factored objective, double-sided compositional approach, and use of discourse structure really are. Check out Figure 5 to see the best route through the trellis for the example sentence we mentioned earlier. To make things clearer, we broke down each event into its three main parts. This helps us understand how the attribute: value classifiers work together to create an alignment.",
        "formal_text": "Convert casual text to formal text: We did a detailed look at our system and found out how awesome our factored objective, double-sided compositional approach, and use of discourse structure really are. Check out Figure 5 to see"
    },
    {
        "casual_text": "A bunch of researchers have come up with different tricks for figuring out what abbreviations mean. For example, they look at things like initials, capitalization, where syllables break, common words, how long the abbreviation is, and how often it shows up with certain words (Park and Byrd, 2001; Wren and Garner, 2002; Liu and Friedman, 2003; Okazaki and Ananiadou, 2006; Zhou et al., 2006; Jain et al., 2007). Schwartz and Hearst (2003) made a straightforward algorithm that picks the shortest phrase with all the letters of the abbreviation. Adar (2004) came up with four rules to decide which of several options is the most likely full form. Ao and Takagi (2005) created more specific criteria for deciding whether a candidate is a good match or not.",
        "formal_text": "Convert casual text to formal text: A bunch of researchers have come up with different tricks for figuring out what abbreviations mean. For example, they look at things like initials, capitalization, where syllable"
    },
    {
        "casual_text": "So, the non-overlapping group-Lasso can be thought of as this two-step hierarchical Bayes model: for each m from 1 to M, it works like this...",
        "formal_text": "So, the non-overlapping group-Lasso can be thought as this two-step hierarchical Bayes model: for each m from 1 to M, it works like this... Convert casual text to formal text"
    },
    {
        "casual_text": "Back in 1991, Elman pointed out that messing around with artificial systems can help us cut through the noise of real-world language data. But to make sure our model actually learns recursive patterns and not just simple, limited ones, we need to test it on structures that are more deeply nested than what it was trained on. That’s exactly what we did. Generalised Dyck languages are perfect for this kind of testing (Bernardy, 2018). Now, LSTMs (and GRUs) can handle some deeper nesting, but their performance drops as the nesting gets more complex, just like how they struggle with natural language agreement stuff. Other studies have shown similar results (Hewitt et al., 2020; Sennhauser and Berwick, 2018). The same thing happens with generative self-attention models (Yu et al., 2019), while BERT-like models, which aren’t generative, just can’t handle this task at all (Bernardy et al., 2021).",
        "formal_text": "Convert casual text to formal text: Back in 1991, Elman pointed out that messing around with artificial systems can help us cut through the noise of real-world language data. But to make sure our model actually learns recurs"
    },
    {
        "casual_text": "Sure! Here's a more casual version: This is an interactive argument pair. For instance, in Figure 1, the review argument spans are set X, which is â v 1, â v 2 = (3, 5), (6, 9), and the rebuttal argument spans are set .",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: This is an interactive argument pair. For instance, in Figure 1, the review argument spans are set X, which is â v"
    },
    {
        "casual_text": "So far, none of the element-wise penalty models we’ve looked at earlier consider how the local features depend on each other. Taking a cue from Gao et al. (2012), we decided to group together local features that have similar patterns or distributions. The idea is that if two features show very similar behavior in the training data, their weights shouldn’t end up wildly different after the learning process for the same task. In our new objective function, we want to add something that penalizes situations where features that are super similar end up with completely different weights. This could happen because of noise or gaps in the training data. To make this happen, we start by defining a matrix called A, which measures how similar two features are to each other. In simpler terms, think of A as a kind of map showing how close different features are. In the world of spectral graph theory, this matrix can be seen as a graph with weighted connections. Each feature is a node, and the strength of the connection between two nodes shows how similar those features are. We also introduce another thing called a diagonal degree matrix D. This D matrix helps keep track of how connected each feature is, kind of like counting how many friends each node has in the graph, but with weights.",
        "formal_text": "Convert casual text to formal text: So far, none of the element-wise penalty models we’ve looked at earlier consider how the local features depend on each other. Taking a cue from Gao et al."
    },
    {
        "casual_text": "Alright, so next, we take the sequence of body word representations, which we call E_b, and use that as our query. Meanwhile, we use the sequence of title word representations, E_t, as both the key and the value to figure out a hidden representation sequence.",
        "formal_text": "Convert casual text to formal text: Alright, so next, we take the sequence of body word representations, which we call E_b, and use that as our query. Meanwhile, we use the sequence of title word representations"
    },
    {
        "casual_text": "Alright, so Model B didn't perform as well because it was built to focus on the connections between multiple events in a sentence using an attention mechanism. But in the commodity news dataset, the events aren't as strongly connected as they are in the ACE2005 dataset. On the other hand, Model D uses a simpler LCA subtree with just the basic info, while Model E includes extra important context that's been really helpful for argument role classification. Table 4 breaks down how well the argument classification works for different types of arguments, showing how the proposed solution works well with the dataset, especially since the arguments are pretty similar. It’s clear that arguments of the same type, like Final_value, Initial_value, and Difference, can be better sorted out and classified when you use a contextual subtree that includes the shortest path between the event trigger and its argument, along with some key extra info. The symbols (, , ) in Table 4 help group these arguments by their entity type.",
        "formal_text": "Convert casual text to formal text: Alright, so Model B didn't perform as well because it was built to focus on the connections between multiple events in a sentence using an attention mechanism. But in the commodity news dataset, the"
    },
    {
        "casual_text": "Our knowledge is pretty similar to knowledge that comes from retrieval systems. On NumerSense, the knowledge we get from retrieval only boosts performance by 0.18% on the test-core and 1.02% on the full test. Meanwhile, our method does way better—it beats the retrieval-based approach by 8.83% and 7.37%, respectively. This shows that knowledge pulled from a kind of random knowledge base isn’t nearly as helpful as the knowledge we generate. On CSQA2, we couldn’t quite match the performance of web-retrieved knowledge, but our method still closed the gap without needing to use Google search. For QASC, the \"retrieved\" knowledge is actually the gold standard from a knowledge base that was used to create the dataset. So, in that case, our generated knowledge doesn’t measure up to the retrieved knowledge. Overall, our generated knowledge is about on par with retrieved knowledge when it comes to improving performance. It’s most useful when there isn’t a good in-domain knowledge base to pull from.",
        "formal_text": "Convert casual text to formal text: Our knowledge is pretty similar to knowledge that comes from retrieval systems. On NumerSense, the knowledge we get from retrieval only boosts performance by 0.18% on the test-core and"
    },
    {
        "casual_text": "So, if we input this IF into the mapper, it'll show the FS that's in Figure 8. Basically, it's about a train trip where the starting point is Cairo and the destination is Aswan.",
        "formal_text": "Convert casual text to formal text: So, if we input this IF into the mapper, it'll show the FS that's in Figure 8. Basically it's about a train trip where the starting point"
    },
    {
        "casual_text": "So, as mentioned by Schuler in a couple of his papers (2018, 2021), CDR works pretty well with time series data where the predictors and responses don’t line up perfectly in time—like when you’re dealing with things like word onsets that don’t match up with fMRI scan times. The version of CDR that Shain and Schuler put out in 2021 can handle predictors that are measured at different times too, as long as each predictor has its own set of timestamps. This means, for example, that Shain et al. (2020) could use the same model to analyze linguistic features (which are tied to specific words) and sound power (which they measured in regular 100ms intervals). Now, dealing with predictors that don’t line up in time is trickier with CDRNN, especially when you’re using the RNN part. The approach CDR uses doesn’t work here because, in CDRNN, input dimensions that don’t match up are (1) just grouped together randomly and (2) incorrectly treated as steps in the RNN sequence. A better way to handle this is to combine the predictors in the order they happen and fill in the gaps with zeros. For instance, if you have predictor A and predictor B measured at different times, you’d sort their values together into one timeline, setting the B values for A events to zero and vice versa.",
        "formal_text": "Convert casual text to formal text: So, as mentioned by Schuler in a couple of his papers (2018, 2021), CDR works pretty well with time series data where the predictors and responses don’t line up"
    },
    {
        "casual_text": "Creating hand-crafted features is important, but it’s also super time-consuming. It’d be great if we could just automatically learn useful features from text. For figuring out how readable a whole document is, a good approach is to combine the representations of the words in it (Kim, 2014). When it comes to representing words, a cool trick is to turn them into dense, low-dimensional vectors, which is called word embedding. There are already some word embedding models out there (like the ones by Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) that could help with readability assessment, but they have a downside: they mainly focus on how words relate to each other in terms of syntax or meaning, and they don’t really consider how easy or hard those words are to read. So, words like \"man\" and \"gentleman\" might end up close in the vector space because they’re similar in function or topic, even though they’re not equally easy to understand. This means we need to find a way to include reading difficulty when training these word embeddings.",
        "formal_text": "Convert casual text to formal text: Creating hand-crafted features is important, but it’s also super time-consuming. It’d be great if we could just automatically learn useful features from text. For figuring out how"
    },
    {
        "casual_text": "Orthogonal transforms have some pretty cool features. For instance, you can get the inverse matrix by just flipping it around (transposing it). Plus, it keeps the L2 norm of a vector intact after the transformation. In our case, we're mainly interested in that handy trick of finding the inverse by transposing. This helps cut down on the number of model parameters we need to deal with (check out Table 3 for details).",
        "formal_text": "Convert casual text to formal text: Orthogonal transforms have some pretty cool features. For instance, you can get the inverse matrix by just flipping it around (transposing it). Plus, it keeps the L2 norm"
    },
    {
        "casual_text": "Tables 2 and 3 show the results for the two tasks with two different training setups. Basically, throwing in some unsupervised data gives better results than just using supervised data. For IWSLT, it improves by at least 1.3 BLEU points, and for NIST, it’s around 0.5 BLEU points.",
        "formal_text": "Convert casual text to formal text: Tables 2 and 3 show the results for the two tasks with two different training setups. Basically, throwing in some unsupervised data gives better results than just using supervised data. For IWS"
    },
    {
        "casual_text": "Plus, the semi-supervised More is way faster—it works 77 times quicker than the combination and 23 times faster than what Shen et al. came up with in 2007, all on a single CPU and the development data set.",
        "formal_text": "Convert casual text to formal text: Plus, the semi-supervised More is way faster—it works 77 times quicker than the combination and 23 times faster than what Shen et al came up with in 2007, all on"
    },
    {
        "casual_text": "Context-free embeddings, whether at the word, character, or component level (like those from Lu et al., 2016; Yu et al., 2017; Cao et al., 2018; Devlin et al., 2019), have been used a lot in various Chinese NLP tasks. However, no one has really looked at how well they work for creating character sets in CALL systems, at least not in a measurable way. One option is to just use the character embeddings directly. Another idea is to compare the similarity between character and component embeddings, which was kind of hinted at in a study about the 'illness' component (Yu et al., 2017). Or, you could use embeddings of words that contain the character, though the character's meaning might play different roles in different words (Xu et al., 2016). In our study, we’re testing out these different methods to see how well they create character sets.",
        "formal_text": "Convert casual text to formal text: Context-free embeddings, whether at the word, character, or component level (like those from Lu et al., 2016; Yu et al., 2017;"
    },
    {
        "casual_text": "1: Start by learning h2 directly from dataset D. 2: For every unlabeled example (x, y1) in D_unlab: 3: Calculate y2 using h2(x). 4: If (y1, y2) is true, toss (x, y2) back into D. 5: Re-train h2 with the updated (or bigger) D. 6: Repeat steps 2-5 if you're still going with the example.",
        "formal_text": "Convert casual text to formal text: 1: Start by learning h2 directly from dataset D. 2: For every unlabeled example (x, y1) in D_unlab: 3: Calculate y2"
    },
    {
        "casual_text": "To prevent any issues caused by similar feature vectors, we check how far apart the vertical context vector a C i is from its original context vector a C i using the Mahalanobis distance (check out Eq. 7 for the formula). If the distance is small, it means the irrelevant context vector a C i is pretty close to the original context vector a C i. This implies that the original context has a lot of extra information. So, the bigger this distance value is, the less extra information the caption has.",
        "formal_text": "Convert casual text to formal text: To prevent any issues caused by similar feature vectors, we check how far apart the vertical context vector a C i is from its original context vector a C i using the Mahalan"
    },
    {
        "casual_text": "CELEX is basically a big database with all the official documents from the Office for Official Publications of the European Communities. It has a bunch of legal stuff in multiple languages. You can easily search for what you need online, and it lets you use lots of different ways to find the info you're looking for.",
        "formal_text": "Convert casual text to formal text: CELEX is basically a big database with all the official documents from the Office for Official Publications of the European Communities. It has a bunch of legal stuff in multiple languages. You can easily"
    },
    {
        "casual_text": "For the scoring experiment, we found out that scoring is super tough, and we still need to do more work to get better results. But hey, we did get some promising stuff—almost a 0.7 correlation for the advanced class in a task where people often disagree a lot.",
        "formal_text": "Convert casual text to formal text: For the scoring experiment, we found out that scoring is super tough, and we still need more work to get better results. But hey, we did get some promising stuff—almost a"
    },
    {
        "casual_text": "Using hints while learning, especially when you've got 1000 or fewer data points, makes a big difference and can even work better than just trying to teach yourself.",
        "formal_text": "Convert casual text to formal text: Using hints while learning, especially when you've got 1000 or fewer data points makes a big difference and can even work better and can even work better just trying to teach yourself. Con"
    },
    {
        "casual_text": "Each number under a node tells you which group the template belongs to.",
        "formal_text": "Convert casual text to formal text: Each number under a node tells you which group the template belongs to. Convert casual text to formal text: Each number under a node tells you which group the template belongs to"
    },
    {
        "casual_text": "Turns out, just throwing a bunch of explanations together doesn’t really help when it comes to tackling new tasks without any prior experience (check out Figure 6 for the proof). So, we came up with ExEnt, which actually figures out how each explanation contributes to predicting the label for a given example.",
        "formal_text": "Convert casual text to formal text: Turns out, just throwing a bunch of explanations together doesn’t really help when it comes to tackling new tasks without any prior experience (check out Figure 6 for the proof). So,"
    },
    {
        "casual_text": "One more thing to keep in mind: like other systems with lots of rules and constraints, CLG doesn’t promise that every single constraint will be fully solved, even after the final step. So, (a) the system won’t crash just because some constraints are still hanging around, and (b) the data you see along the way—and even the final result—might still have some unresolved bits. This means what you’re looking at isn’t just one specific thing, but a whole group of possible outcomes.",
        "formal_text": "Convert casual text to formal text: One more thing to keep in mind: like other systems with lots of rules and constraints, CLG doesn’t promise that every single constraint will be fully solved, even after the final step. So"
    },
    {
        "casual_text": "So, government folks basically have the same goals as big money owners and are connected to them through all kinds of social, economic, and political stuff.",
        "formal_text": "Convert casual text to formal text: So, government folks basically have the same goals as big money owners and are connected through all the social, economic, and political stuff. So, government folks basically have the same goals as big money owners and"
    },
    {
        "casual_text": "Okay, here's a simpler way to say that: Lemma G. 2 says that if we have a configuration (let's call it c) that comes from LTL, then c is considered a goal configuration if two things are true: first, the set S c is empty, and second, G c is defined for some i.",
        "formal_text": "Convert casual text to formal text: Okay, here's a simpler way to say that: Lemma G. 2 says that if we have a configuration (let's call it c) that comes from L"
    },
    {
        "casual_text": "The heart of our om  translation system is made up of two main parts: the understanding/analysis part called TINA, and the generation part called GENESIS. These parts work with a bunch of files that tell them how the source and target languages are structured. You can see how everything flows together in Figure 2.",
        "formal_text": "Convert casual text to formal text: The heart of our om  translation system is made up of two main parts: the understanding/analysis part called TINA, and the generation part called GENESIS. These parts work with"
    },
    {
        "casual_text": "So, our model needs to consider how both predicates and variables affect the sentences in the documents. A good approach is to let the predicates guide the words we use and let the variables decide the sentence structure. For instance, the formula card(x)  freecell(y)  empty(y) has three predicates and two variables. The predicates directly influence the words: sentences based on this formula are more likely to include words related to each predicate. So, a sentence like \"Put the cards on the empty freecells\" makes more sense than something like \"Columns are constructed by playing cards in alternating colors.\"",
        "formal_text": "Convert casual text to formal text: So, our model needs to consider how both predicates and variables affect the sentences in the documents. A good approach is to let the predicates guide the words we use and let the variables decide"
    },
    {
        "casual_text": "Results. Figure 7 shows the same trend as the BLEU score, but with a smaller improvement of 23% when using the adversarial term.",
        "formal_text": "Figure 7 shows the same trend as the BLEU score, but with a smaller improvement of 23% when using the adversarial term."
    },
    {
        "casual_text": "These models are often called nclass models, with the most popular ones being biclass (n = 2) and triclass (n = 3) models. To figure out the lexical and contextual probabilities for an nclass tagger, people usually use one of two methods:  1The terms 'RF training' and 'ML training' come from Merialdo's 1994 work. It's worth noting that using relative frequencies to estimate probabilities is actually a form of maximum likelihood estimation (MLE).",
        "formal_text": "Convert casual text to formal text: These models are often called nclass models, with the most popular ones being biclass (n = 2) and triclass (n = 3) models. To figure out the lexical and contextual"
    },
    {
        "casual_text": "We found a total of 304 different English and Arabic hashtags in the sample we looked at.",
        "formal_text": "Convert casual text to formal text: We found a total of 304 different English and Arabic hashtags in the sample we looked at. Convert casual text to formal text: We found a total of 304 different English and Arabic"
    },
    {
        "casual_text": "MultiWOZ 2.0, which was introduced by Budzianowski and team in 2018, is a big dataset for multi-domain conversations with thousands of dialogues spread across seven different areas. To keep things consistent with earlier research, like Zhao et al. in 2019 and Budzianowski et al. in 2018, we're using the same validation and test sets, each with 1000 dialogues. For evaluating how well the conversations are handled, we look at two main things: the Inform Rate, which checks if the system gave the right entity, and Request Success, which sees if it answered all the requested details. Additionally, we use BLEU, a method from Papineni et al. in 2002, to gauge how smooth and natural the responses sound. To get a full picture of the system's performance, we calculate a combined score that blends these metrics together: (Inform Rate + Request Success)  0.5 + BLEU, just like in previous studies by Budzianowski et al. in 2018, Mehri et al. in 2019, and Pei et al. in 2019.",
        "formal_text": "Convert casual text to formal text: MultiWOZ 2.0, which was introduced by Budzianowski and team in 2018, is a big dataset for multi-domain conversations with thousands of dialogues spread across seven different areas. To keep things"
    },
    {
        "casual_text": "• In the past, public sector clients have been hesitant to use MT, CAT, and TMS because they were worried about data protection and didn’t know how to measure the return on investment. • We’re going to explain how our team managed to meet the client’s goals, handle their data protection concerns, and create a practical, domain-specific Post-Edited Machine Translation (PEMT) solution that public sector clients can actually use.",
        "formal_text": "Convert casual text to formal text: • In the past, public sector clients have been hesitant to use MT, CAT, and TMS because they were worried about data protection and didn’t know how to measure the return on"
    },
    {
        "casual_text": "We're using the Adam optimizer (from Kingma and Ba, 2015) to handle all the parameter updates. When we're pre-training on the StreetLearn dataset, the learning rate for the RCONCAT model, GA model, and the VLN Transformer is set to 2.5  10-4. For fine-tuning BERT, we use a smaller learning rate of 1  10-5. We pre-train RCONCAT and GA for 15 epochs, while the VLN Transformer gets pre-trained for 25 epochs.",
        "formal_text": "Convert casual text to formal text: We're using the Adam optimizer (from Kingma and Ba, 2015) to handle all the parameter updates. When we're pre-training on the StreetLearn dataset, the"
    },
    {
        "casual_text": "We set up spellchecking and autocorrection as a three-step process. This system uses confidence classifiers, which let us adjust the balance between precision and recall to suit our needs for both spellchecking and autocorrection. Here's how it works: 1. First, all possible suggestions for a word are ranked based on their P(s|w) scores. 2. Next, a spellchecking classifier decides if the word is misspelled. 3. Finally, if the word is flagged as misspelled and there are suggestions available, an autocorrection classifier checks if the top suggestion is the right one.",
        "formal_text": "Convert casual text to formal text: We set up spellchecking and autocorrection as a three-step process. This system uses confidence classifiers, which let us adjust the balance between precision and recall to suit our needs"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. We're talking about a pre-trained word vector model called *look-up pretrained*, which is fixed and doesn't change. Then, we have two word vectors for the input s: *x t* and *x * t*. These vectors represent the input s, and *h * t* is the current hidden state, while *h * t1* is the hidden state from the previous step. For instance, if we take an input s from the ID training data, we can get these two word vectors. After that, we can feed these vectors into two different hidden layers (RNN and RNN * 3). Now, here's something interesting: the language models (LMs) we're using are trained from scratch using the ID training data, but they also borrow some knowledge from a different word vector model called GD. So, the hidden state in Equation 6 isn't exactly the \"GD hidden state\"; it uses the word embeddings from the GD model, but the actual data being fed in is still from the ID dataset. In Figure 1, you can see the process we're proposing in this paper. The domain-adapted training is explained in Sections 3.1, 3.2, and 3.3.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in simpler terms. We're talking about a pre-trained word vector model called *look-up pretrained*, which is fixed"
    },
    {
        "casual_text": "In this part, we're sharing the results of our experiments for spotting sentences that describe experiments, pulling out mentions of entities, and identifying specific details about the experiments. For breaking down the text into smaller pieces (tokenization), we used ChemDataExtractor, which is really good at handling chemical formulas and units.",
        "formal_text": "Convert casual text to formal text: In this part, we're sharing the results of our experiments for spotting sentences that describe experiments, pulling out mentions of entities, and identifying specific details about the experiments. For breaking down"
    },
    {
        "casual_text": "In this paper, we use the Chow-Liu tree algorithm (Chow and Liu, 1968) as our BNSL method. This algorithm works by turning the relationships between variables into a tree structure, where each node has only one parent and no loops are allowed. First, it calculates the mutual information between every pair of variables, then it picks the maximum spanning tree from that data as the approximation. While this isn't a perfect representation of the data, it works well for a lot of applications (Suzuki, 2010; Tavassolipour et al., 2014; Hassan-Moghaddam and Jovanovic, 2018; Ding et al., 2019), especially when you want to figure out the main influencer for each variable. Plus, it's super fast when dealing with a large number of variables.",
        "formal_text": "Convert casual text to formal text: In this paper, we use the Chow-Liu tree algorithm (Chow and Liu, 1968) as our BNSL method. This algorithm works by turning the relationships between variables into"
    },
    {
        "casual_text": "The HAPS algorithm is pretty complicated, so most people probably won’t care about how it’s derived. We’re just giving the basic info here—like how it connects to factor graphs and how it’s built from the Affinity Propagation model. If you want all the nitty-gritty details, check out the paper by Kazantseva from 2014.",
        "formal_text": "Convert casual text to formal text: The HAPS algorithm is pretty complicated, so most people probably won’t care about how it’s derived. We’re just giving the basic info here—like how it connects to factor"
    },
    {
        "casual_text": "Okay, so let’s break this down in simpler terms. In step 4 of the algorithm, we’re finding w2 by minimizing L(w; X = P1X; Z). Since L is convex and w is linear, the gradient of L with respect to w is just some scalar times x, like w L(xi) = ixi. After doing t steps of stochastic gradient descent (SGD), w2 ends up being a mix of the input vectors x1, x2, ..., xt. Now, we’re only working with xi that are in the nullspace of w1, and since the nullspace stays the same when you add things together, at every step t, w2 will also be in the nullspace of w1. This is true for the final optimal w2* as well.",
        "formal_text": "Convert casual text to formal text: Okay, so let’s break this down in simpler terms. In step 4 of the algorithm, we’re finding w2 by minimizing L(w; X = P1X;"
    },
    {
        "casual_text": "To get the abstract, we use regular expressions on a collection of 3,938 PubMed Central (PMC) articles in XML format. These articles have a special tag for the abstract, which we consider the correct version or \"gold standard.\" We ran our algorithm on the raw text pulled from the XML files and then checked how well the abstracts we got matched up with the ones from the XML tags.",
        "formal_text": "Convert casual text to formal text: To get the abstract, we use regular expressions on a collection of 3,938 PubMed Central (PMC) articles in XML format. These articles have a special tag for the"
    },
    {
        "casual_text": "We grabbed four groups of around five hundred English-Japanese sentence pairs from the test set. The English sentences in these groups were translated by the E-J machine translation systems and then ranked by a Japanese native speaker. Similarly, the Japanese sentences were translated by the J-E MT systems and ranked by an English native speaker. The overall performance was calculated by averaging the results across the four groups. Specifically, the performance of the second method we proposed was calculated using four-fold cross validation, as mentioned in Mitchell's 1997 work.",
        "formal_text": "Convert casual text to formal text: We grabbed four groups of around five hundred English-Japanese sentence pairs from the test set. The English sentences in these groups were translated by the E-J machine translation systems and then"
    },
    {
        "casual_text": "The length-aware framework can work with most existing SiMT methods. We'll use wait-k and MMA as examples to show how it works a bit differently when paired with a fixed or adaptive policy. The LAF predicts the full sentence length based on the source words x g(i) it has received so far. The main thing is figuring out g(i), which can be different depending on whether you're using a fixed or adaptive policy.",
        "formal_text": "Convert casual text to formal text: The length-aware framework can work with most existing SiMT methods. We'll use wait-k and MMA as examples to show how it works a bit differently when paired with"
    },
    {
        "casual_text": "For the training data, we grabbed a parallel corpus from the web using our own crawler. This corpus has around 9.5 million sentences and 160 million words on average, with at least 8 million sentences and 140 million words for each language pair. For the development and test data, we picked and manually translated 3,000 and 5,000 sentences, respectively, from other web sources for each language pair. We tweaked all the hyperparameters for each method using the development data, and then did the final evaluation with the test data. For word alignment, we used IBM Model 1 (from Brown et al., 1993) and HMM alignment (Vogel et al., 1996) on the best-ranked source sentences and their corresponding target sentences. Based on these alignment results, we built the phrase table, which was shared across all decoding methods. For the English language model, we used a 4-gram model with stupid backoff smoothing (Brants et al., 2007), which was the same for all setups. We decided on each configuration for word alignment and the language model based on some preliminary experiments with the baseline system. For the baseline system, we used a standard PBMT system, similar to what Och and Ney (2004) did, but with a lexical reordering model (Zens and Ney, 2006) boosted by a cutting-edge preordering method using bracketing transduction grammar (Nakagawa, 2015). We also used a similar decoding strategy and other basic feature functions as in Moses (Koehn et al.).",
        "formal_text": "Convert casual text to formal text: For the training data, we grabbed a parallel corpus from the web using our own crawler. This corpus has around 9.5 million sentences and 160 million words on average, with at least"
    },
    {
        "casual_text": "We're focusing on finding how ideas connect in a text, not just how often words repeat. Instead of looking at how words link together, we're looking at how the overall meaning ties everything together. We’re suggesting a way to measure how similar different parts of a text are by considering how words relate to each other across those parts. This method kind of acts like those \"lexical chains\" people talk about, but we’re doing it by averaging how similar a part of the text is to a few parts that came before it. We also handle synonyms by using a mix of different ways to index the text. Our tests on breaking up text into segments show that this approach works way better than the basic method we started with.",
        "formal_text": "Convert casual text to formal text: We're focusing on finding how ideas connect in a text, not just how often words repeat. Instead of looking at how words link together, we're looking at how the overall meaning"
    },
    {
        "casual_text": "Figure 1 in our dataset gives a quick look at how we put everything together. First, we gather monolingual data (check out Section 4.1). Then, we create parallel data by translating stuff (Section 4.2). After that, we make a test set with new compound words (Section 4.3) and provide a way to automatically evaluate it (Section 4.4).",
        "formal_text": "Convert casual text to formal text: Figure 1 in our dataset gives a quick look at how we put everything together. First, we gather monolingual data (check out Section 4.1). Then, we create parallel data by translating stuff"
    },
    {
        "casual_text": "In MERT, the main idea is to reduce the number of errors, called E(r, e), by comparing translation options with a bunch of reference translations, which we can label as r S 1 = r 1. . . r S. Following what Och did in 2003, we assume that this error count can be broken down sentence by sentence, meaning it's additive.",
        "formal_text": "Convert casual text to formal text: In MERT, the main idea is to reduce the number of errors, called E(r, e), by comparing translation options with a bunch of reference translations, which we can label"
    },
    {
        "casual_text": "Finally, since our model creates embeddings for character n-grams, we took a look at the nearest neighbors for some n-grams. In row 2, we have examples related to food, and in row 3, it's about speed, but with different levels of detail. The n-grams in the last row show up in paraphrases of 2, while the second-to-last row has n-grams found in words connected to language.",
        "formal_text": "Convert casual text to formal text: Finally, since our model creates embeddings for character n-grams, we took a look at the nearest neighbors for some n-grams. In row 2, we have"
    },
    {
        "casual_text": "• Global Recurrence: Creating specific (slot and intent) global sequences by using the local context info we got from the last Local Calculation.",
        "formal_text": "• Global Recurrence: Creating specific (slot and intent global sequences by using the local context info we got from the last Local Calculation."
    },
    {
        "casual_text": "F(   svo CpSbj ) and F(   svo CpObj ) are defined as shown in Equation 11.",
        "formal_text": "Convert casual text to formal text: F(   svo CpSbj ) and F(   svo CpObj ) are defined as shown in Equation 11."
    },
    {
        "casual_text": "The original text has a different meaning, and it's a bit tricky to figure out. Because of that, the reader might end up believing the wrong idea.",
        "formal_text": "Convert casual text to formal text: The original text has a different meaning, and it's a bit tricky figuring out. Because of that, the reader might end up believing the wrong idea."
    },
    {
        "casual_text": "A simple way to do this would be to just use noun phrases as they are. For instance, ipadic 1 treats a bunch of long proper nouns as single words, like \" \" (which means Kansai International Airport Company Connecting Bridge). But this method has some issues. For example, if you're searching for \"Kansai International Airport,\" it won't match the single word for the bridge. That's why we need to break it down into smaller parts.",
        "formal_text": "Convert casual text to formal text: A simple way to do this would be to just use noun phrases as they are. For instance, ipadic 1 treats a bunch of long proper nouns as single words, like"
    },
    {
        "casual_text": "In this part, we're going to talk about our new method for classifying sentiments in questions and answers. Check out Figure 2 for a quick look at how this method works.",
        "formal_text": "Convert casual text to formal text: In this part, we're going to talk about our new method for classifying sentiments in questions and answers. Check out Figure 2 for a quick look at how this method works."
    },
    {
        "casual_text": "To add strategy info to the main model, we used a unique token for each strategy. For every supporter's statement y, we added the matching strategy token before it: y = [st]  y, with [st] being the special strategy token. Then, using the whole conversation history x as input, the model creates a response based on the first predicted (or chosen) strategy token: P(|x). We looked at three different setups for using strategy annotations in our tests: (1) Oracle: responses are made based on the correct strategy tokens. (2) Joint: responses are made based on predicted (chosen) strategy tokens. (3) Random: responses are made based on randomly picked strategies. More details on the implementation can be found in Appendix C.",
        "formal_text": "Convert casual text to formal text: To add strategy info to the main model, we used a unique token for each strategy. For every supporter's statement y, we added the matching strategy token before it: y ="
    },
    {
        "casual_text": "In this part, we’re diving into how SWSD can be super useful for analyzing subjectivity in text. First off, we show that even if a subjectivity lexicon covers a lot of the subjective stuff in a text, it can still cause some major confusion about what those subjective expressions actually mean. After that, we test out SWSD in a few different systems that analyze opinions in context, comparing how well the ones that pay attention to word senses do versus the ones that don’t. All these systems are tweaks of parts from the Opinion-Finder system, which is all about spotting opinions in text.",
        "formal_text": "Convert casual text to formal text: In this part, we’re diving into how SWSD can be super useful for analyzing subjectivity in text. First off, we show that even if a subjectivity lexicon covers"
    },
    {
        "casual_text": "In this paper, we took a fresh look at the over-smoothing issue in TTS and came up with a new way to think about it: the extent of over-smoothing depends on the difference between how complex the data is and how well the model can handle it. From this angle, we grouped the existing approaches to tackle over-smoothing into two types: those that simplify the data and those that make the modeling methods stronger. We did a deep dive into these methods and analyzed them thoroughly. For the first type, simplifying the data, we noticed that both AR factorization and giving the model more variance info as input (like in FastSpeech 2) can help with over-smoothing. FastSpeech 2 also has the perk of being faster to generate speech compared to AR factorization. For the second type, improving the modeling methods, we saw that using a Laplacian mixture loss can boost the quality of the generated speech and is pretty straightforward. On the other hand, GAN and Glow can produce even better quality but at the expense of more complex training or bigger models. Taking all this into account, we tried mixing both types of methods and found that not only did it reduce over-smoothing even more, but it also improved the quality of the generated speech. This showed that the two types of methods work well together and complement each other. When we ran our analysis on a multi-speaker dataset, we came to similar conclusions. We found that giving the model more variance info made it easier to handle the target data distribution and reduced the need for a super powerful model.",
        "formal_text": "Convert casual text to formal text: In this paper, we took a fresh look at the over-smoothing issue in TTS and came up with a new way to think about it: the extent of over-s"
    },
    {
        "casual_text": "After following the rules mentioned in Section 4, Table 2 breaks down how many sentences we could potentially rewrite and how many actually got accepted. The rules that worked best and could be used in more situations were turning sentences passive and moving around verbs that introduce quotes. We ended up rewriting 32.2% of the sentences, cutting down the delay from 9.9 words per segment to 6.3 words per segment for the rewritten sentences, and from 7.8 words per segment to 6.7 words per segment overall. Here's a quick summary of how the rules did: - **Verb voice**: 39.9% applicable, 22.5% accepted - **Noun conjunction**: 50.0% applicable, 24.0% accepted - **Noun conjunction**: 26.4% applicable, 51.2% accepted - **Verb conjunction**: 4.8% applicable, 38.4% accepted Table 2 explains that these percentages show how often each rule category could be used (Applicable) and how often those rules made the sentence smoother (Accepted).",
        "formal_text": "Convert casual text to formal text: After following the rules mentioned in Section 4, Table 2 breaks down how many sentences we could potentially rewrite and how many actually got accepted. The rules that worked best and could be used in more situations were"
    },
    {
        "casual_text": "So, position-aware embedding gives us a lot of info about where things are, which is super helpful for other parts of the system (Zeng et al., 2014). For the i-th word, we figure out how far it is from the head (e(h)) and tail (e(t)) entities. Then, these two distance numbers get turned into smaller, low-dimensional vectors, x(ph)i and x(pt)i, which are in Rdp. These vectors are made by multiplying with a weight matrix that learns as it goes. In the end, we have a whole bunch of these position-aware embeddings all lined up.",
        "formal_text": "Convert casual text to formal text: So, position-aware embedding gives us a lot of info about where things are, which is super helpful for other parts of the system (Zeng et al., 2014"
    },
    {
        "casual_text": "In the sections that follow, we'll talk about related work (Section 2), the datasets we used (Section 3), our method for reducing the complexity of review representations (Section 4), and our semi-supervised approach to ranking reviews (Section 5). After that, we'll look at how well our methods perform, both in terms of numbers (Sections 6 and 7) and through a more hands-on, qualitative analysis (Section 8).",
        "formal_text": "Convert casual text to formal text: In the sections that follow, we'll talk about related work (Section 2), the datasets we used (Section 3), our method for reducing the complexity of review representations"
    },
    {
        "casual_text": "The team has a few key roles: the operators (UGV-1, UGV-2, UAV) who actually control the robots, a team leader (TL), and sometimes a mission commander (MC) depending on the mission. The MC is responsible for the whole mission and assigns tasks to the teams. The TL takes those tasks and splits them up among the operators, making sure everyone is on the same page and keeping the MC in the loop if they're around. The operators use the robots to do the tasks they're given and let the TL know how it's going, including any issues that come up. Everyone on the team uses a shared system that shows a digital map with points of interest (POIs) and where the robots are located. There's also a collection of photos taken by the robot cameras, and since 2017, a task list that the TL can update manually.",
        "formal_text": "Convert casual text to formal text: The team has a few key roles: the operators (UGV-1, UGV-2, UAV) who actually control the robots, a team leader (TL), and sometimes a"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Diplomacy (DM) is a dataset based on conversation logs from the online text-based game Diplomacy. It was put together by Peskov et al. in 2020. The dataset includes stuff like whether someone is lying or not during the game. For example: - DM 13, 132: Lies, True - MS 9, 676: Liar, True - OD 7, 168: Lies, False - LIAR 4, 560: Lies, False - BOL 502: Lies, True - MU3D 320: Lies, True - RLT 121: Liar, True Basically, it’s all about figuring out who’s being honest or not in the game.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Diplomacy (DM) is a dataset based on conversation logs from the online text-based game Diplomacy. It was put"
    },
    {
        "casual_text": "Seznam (Eckhardt et al., 2014) grabs possible candidates from Wikipedia and DBpedia. They figure out the best match by using PageRank on the graph.",
        "formal_text": "Convert casual text to formal text: Seznam (Eckhardt et al., 2014) grabs possible candidates from Wikipedia and DBpedia. They figure out the best match by using PageRank on the graph."
    },
    {
        "casual_text": "We're training our model for both tasks together. To do this, we adjust the model's parameters by trying to minimize a combined loss function. In this function, R is the regularization term we talked about earlier—it could be R_s or R_o.  is a special number we use to control how much the regularization part affects the whole loss. To make sure L_b doesn't mess things up by being too big, we divide it by the number of aspect categories.",
        "formal_text": "Convert casual text to formal text: We're training our model for both tasks together. To do this, we adjust the model's parameters by trying to minimize a combined loss function. In this function, R is the regularization"
    },
    {
        "casual_text": "Since some of our normalization models rely on language labels, we need a system that can identify languages at the word level. There are tons of methods out there for this, but early systems mostly used CRFs (Sequiera et al., 2015). Nowadays, neural network-based approaches are killing it in terms of performance (Zhang et al., 2018). We’re trying out three different architectures to see how the quality of language identification affects normalization (check out Section 4.1 for more details).",
        "formal_text": "Convert casual text to formal text: Since some of our normalization models rely on language labels, we need a system that can identify languages at the word level. There are tons of methods out there for this, but early systems mostly"
    },
    {
        "casual_text": "de Marneffe et al. (2010) put together a dataset for this task by pulling 123 examples of these indirect question-answer pairs (IQAP) from dialogue collections. In each of these exchanges, the hidden answer (which was labeled by people as either yes or no) is based on how the strength of the modifiers in the question compares to those in the answer. In their paper, the authors used a lexicon they made automatically to guess the polarity for each IQAP.",
        "formal_text": "Convert casual text to formal text: de Marneffe et al. (2010) put together a dataset for this task by pulling 123 examples of these indirect question-answer pairs (IQAP) from dialogue collections. In"
    },
    {
        "casual_text": "Alright, let’s dive into K-SRL, the instance-based labeling method we came up with for SRL. We’re going to compare it to the best systems out there to see how it stacks up. We’ll also take a closer look at how tweaking different parts of K-SRL affects its performance. This includes playing around with the minimum support variable k, testing out different composite features, and seeing how we can use relative label frequencies in the nearest neighborhood to help us decide which labels to assign with more confidence.",
        "formal_text": "Convert casual text to formal text: Alright, let’s dive into K-SRL, the instance-based labeling method we came up with for SRL. We’re going to compare it to the best systems out there to"
    },
    {
        "casual_text": "So, which one matters more for performance—the latent domain language models or the latent domain translation models? We ran some extra tests to figure this out. We basically turned off each component one by one and kept the rest of the model intact to see what happens. Turns out, the latent domain translation models are super important for how well the system works. The latent domain LMs, on the other hand, do help, but not as much. If you want more details on this, check out our earlier work (Cuong and Sima'an, 2014), where we dive deeper into the data selection stuff.",
        "formal_text": "Convert casual text to formal text: So, which one matters more for performance—the latent domain language models or the latent domain translation models? We ran some extra tests to figure this out. We basically turned off each component one by"
    },
    {
        "casual_text": "Comparing the proposed method to two reranking systems (DL > 0 or DL = 0), the version without distortion (DL = 0) usually gets a better BLEU score than the one with DL > 0. We think this might be because it uses a smarter way of preordering multiple options instead of just doing reordering during decoding based on distortion. These results make it pretty clear that you don’t really need decoding-time reordering if you already have a good preordering setup in place.",
        "formal_text": "Convert casual text to formal text: Comparing the proposed method to two reranking systems (DL > 0 or DL = 0), the version without distortion (DL = 0) usually gets a better BL"
    },
    {
        "casual_text": "Alright, so we're looking at a situation where |S| is greater than 2, and there's no chance of misclassifying any training points. Using our definition, F(X) = f = w, (X ) |w  R d+ , with  being a projection from X to R d, the L1-norm for (f, g f ) is:",
        "formal_text": "Convert casual text to formal text: Alright, so we're looking at a situation where |S| is greater than 2, and there's no chance of misclassifying any training points. Using our definition,"
    },
    {
        "casual_text": "(i) Alright, let's talk about how words work in this system. We're gonna look at some examples of how determiners, like \"a\" or \"the,\" are handled in these rules. For nouns and verbs, there's another rule that connects them, called t -n. This rule considers the specific meanings that a particular theory assigns to certain words.",
        "formal_text": "Convert casual text to formal text: (i) Alright, let's talk about how words work in this system. We're gonna look at some examples of how determiners, like \"a\" or \"the,\""
    },
    {
        "casual_text": "In Portuguese, the passive voice auxiliary is fully integrated into the language and can be used in all verb tenses. Tense auxiliaries are also pretty well established, but they need to be marked as such, though only in certain verb tenses. Modals and aspectual verbs are less integrated, but that doesn’t make their annotation any less important for NLP tasks.",
        "formal_text": "Convert casual text to formal text: In Portuguese, the passive voice auxiliary is fully integrated into the language and can be used in all verb tenses. Tense auxiliaries are also pretty well established, but they need to be"
    },
    {
        "casual_text": "[brown]: Class-based n-grams (Brown et al., 1992). This is the oldest and one of the simplest systems we tried. It uses a bigram model where each word type is assigned to a hidden class (like a category), and it calculates the probability of the whole text w1...wn based on that.",
        "formal_text": "Convert casual text to formal text: [brown]: Class-based n-grams (Brown et al., 1992). This is the oldest and one of the simplest systems we tried. It uses"
    },
    {
        "casual_text": "Here, 3 is just a number between 0 and 1 that helps adjust how much importance we give to the NLL loss.",
        "formal_text": "Convert casual text to formal text: Here, 3 is just a number between 0 and 1 that helps adjust how much importance we give to the NLL loss. Convert casual text to formal text: Here,"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Basically, this is a standard 6-layer Transformer model that’s designed to translate a sentence, but it also uses the sentence right before it as extra context (Tiedemann and Scherrer, 2017). The model takes both the previous sentence and the current one, sticking a special character in between to separate them. Jwalapuram and his team (2020) found that this straightforward approach works just as well—or even better—than fancier models like the ones from Voita et al. (2018), Zhang et al. (2018), and Miculicich et al. (2018).",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Basically, this is a standard 6-layer Transformer model that’s designed to translate a sentence, but it also uses the sentence right before"
    },
    {
        "casual_text": "We use a teacher model to create pseudo labels for unlabeled speech recognition (ASR) outputs. The teacher model starts off pretty weak, so if we used all the unlabeled ASR data right away, it would just add a lot of noise. Instead, we slowly increase the amount of unlabeled ASR data by randomly sampling from the full set in each iteration. For a sentence S = w1, w2, ..., wn, the teacher model gives pseudo labels T = t1, t2, ..., tn, where each ti is either O or D. Since the teacher model isn't perfect, using all the pseudo-labeled sentences to train a student model would introduce too much noise. To fix this, we use a sentence grammaticality judgment model to help pick out sentences with better-quality pseudo labels. For a sentence S = w1, w2, ..., wn and its pseudo labels T = t1, t2, ..., tn, we create a sub-sentence Ssub = w1, w2, ..., wm by removing the words labeled as D. If the grammaticality model correctly labels Ssub, we assume the pseudo labels T are accurate and keep (S, D) for training the student model.",
        "formal_text": "Convert casual text to formal text: We use a teacher model to create pseudo labels for unlabeled speech recognition (ASR) outputs. The teacher model starts off pretty weak, so if we used all the unlab"
    },
    {
        "casual_text": "To fix this, we’re tweaking the translation process like this:",
        "formal_text": ""
    },
    {
        "casual_text": "Zhang et al. (2018b) came up with this cool graph RNN called S-LSTM, which looks at how words in a sentence connect to each other. We took that idea and added our own twist by incorporating task-specific stuff, like slots and intents, using something we call collaborative memories. Now, the S-LSTM already does a good job with local features, but it kind of misses the bigger picture when it comes to the whole sentence's sequence. So, we brought in external BiLSTMs to handle that global sequential information. Our experiments showed that this extra step was super important for both tasks we were working on.",
        "formal_text": "Convert casual text to formal text: Zhang et al. (2018b) came up with this cool graph RNN called S-LSTM, which looks at how words in a sentence connect to each other. We took that"
    },
    {
        "casual_text": "That said, MoNoise is basically a feature-based Random Forest that relies on some external tools and models. For example, it uses a skipgram model trained on 5 million tweets, the Aspell tool, and an n-gram model trained on over 700 million tweets.",
        "formal_text": "Convert casual text to formal text: That said, MoNoise is basically a feature-based Random Forest that relies on some external tools and models. For example, it uses a skipgram model trained on 5 million tweet"
    },
    {
        "casual_text": "PENG (from White and Schwitter, 2009) is a CNL kind of like ACE, but it takes a simpler approach by focusing on a smaller, easier-to-handle part of English. Both ACE and PENG use language processors based on grammars written in a definite clause grammar (DCG) style. These DCGs are jazzed up with feature structures and are made to turn declarative and interrogative sentences into first-order logic using discourse representation structures. Unlike the original ACE, which uses the DCG directly and only deals with anaphoric references after building the discourse representation structure, PENG changes the DCG into a format that works with a top-down chart parser. It also handles anaphoric references while building the discourse representation structure during the parsing process. PENG was designed for incremental parsing and was the first CNL to have a predictive editor (Schwitter et al., 2003) backing it up. The PENG system offers both text-based and menu-based support for writing, which makes it easier for users by taking some of the pressure off learning and remembering all the rules of the CNL. It also generates a paraphrase for each sentence you enter, helping to clarify how it’s interpreted. The text editor in PENG dynamically checks grammar rules as you type, using lookahead information to make sure everything stays on track.",
        "formal_text": "Convert casual text to formal text: PENG (from White and Schwitter, 2009) is a CNL kind of like ACE, but it takes a simpler approach by focusing on a smaller, easier-to"
    },
    {
        "casual_text": "(b) Here's how well we did at pulling out the top 10 elements (on average) from the stack for different Dyck-2 strings using the hidden state vector from the LSTM. These results are about nested dependencies. Previous research in psycholinguistics (like Gibson, 1991, and McElree, 2001) has suggested that since humans have limited working memory, natural language should have nested dependencies that don't go too deep. Some studies (Jin et al., 2018; Noji et al., 2016) have tried to create parsers for natural language using depth-limited PCFGs. Our findings on LSTMs and depth-limited CFGs might explain why LSTMs are good at handling hierarchical dependencies in natural language datasets. As for Transformers, even though they didn't generalize well to longer sequences, in real-world applications (like BERT and GPT), they work within a fixed-length context window. Our results show that Transformers don't struggle with generalization when the training and validation sets have inputs of the same length.",
        "formal_text": "Convert casual text to formal text: (b) Here's how well we did at pulling out the top 10 elements (on average) from the stack for different Dyck-2 strings using the hidden state vector from the LSTM."
    },
    {
        "casual_text": "We're pretty excited about how well the taxonomization algorithm can rebuild WordNet's Animal hierarchy, which is one of the most detailed and well-developed ones. Plus, we've also tested it out on the Plant and Vehicle WordNet hierarchies to see how it performs.",
        "formal_text": "Convert casual text to formal text: We're pretty excited about how well the taxonomization algorithm can rebuild WordNet's Animal hierarchy, which is one of the most detailed and well-developed ones. Plus, we've"
    },
    {
        "casual_text": "Does the actor or receiver have a name? Also, what's the TF/IDF for the action, and what are the min, max, and average TF/IDF values for the actor or receiver?",
        "formal_text": "Convert casual text to formal text: Does the actor or receiver have a name? Also, what's the TF/IDF for the action, and what are the min, max, and average TF/IDF values"
    },
    {
        "casual_text": "I changed all the words to lowercase and swapped numbers with the word \"num.\" I picked a list of 20,000 words to use, and anything not on that list got turned into \"unk.\"",
        "formal_text": "Convert casual text to formal text: I changed all the words to lowercase and swapped numbers with the word \"num.\" I picked a list of 20,000 words to use, and anything not that list got turned into \"unk.\""
    },
    {
        "casual_text": "Okay, let’s break this down in simpler terms. So, there’s a diagram (Figure 4) that shows how predictions work based on Wikipedia data, as mentioned in Table 7. Here’s what the colors mean: - **Green circles**: These represent the \"facet embeddings\" from something called MFS. - **Orange circle**: This is the average of all those green circles (called MFS Avg). - **Blue circles**: These are word embeddings that are close to both the green and orange circles. The word \"project\" is highlighted because it’s the next word in the actual data we’re looking at.",
        "formal_text": "Convert casual text to formal text: Okay, let’s break this down in simpler terms. So, there’s a diagram (Figure 4) that shows how predictions work based on Wikipedia data, as mentioned in Table 7. Here"
    },
    {
        "casual_text": "The distributional inclusion hypothesis (DIH), which has been around since the late 90s and has been studied by folks like Dagan, Geffet, and others, basically says that if one word (let's call it u) means something that includes another word (v), then you can swap u for v in a sentence and it still makes sense. For example, \"cat\" includes \"animal,\" so in the sentence \"a cat is asleep,\" you can replace \"cat\" with \"animal\" and get \"an animal is asleep.\" Makes sense, right? But if you try to replace \"cat\" with something it doesn't include, like \"butterfly,\" you end up with \"a butterfly is asleep,\" which doesn't really work. Now, this idea has its limits. It only really works in situations where the meaning of the sentence goes \"upward,\" like when you're talking about something getting bigger or more general. It doesn't work so well in sentences with negations or words like \"all\" and \"none.\" So, you can't just swap \"cat\" for \"animal\" in sentences like \"all cats are asleep\" or \"a cat is not asleep.\" Even with these limitations, the DIH has been a pretty big deal in the world of distributional semantics. A lot of research has been done on it, and it's been shown to hold up pretty well in many cases.",
        "formal_text": "Convert casual text to formal text: The distributional inclusion hypothesis (DIH), which has been around since the late 90s and has been studied by folks like Dagan, Geffet, and others, basically says that if one"
    },
    {
        "casual_text": "Alright, so we've got this equation where U equals something, and because (crU) is nilpotent, (1 + crU) is invertible. The inverse of (1 + crU) is actually (1 - crU). In the next part, we'll dive deeper into how this works with a specific choice of the base ( i ).",
        "formal_text": "Convert casual text to formal text: Alright, so we've got this equation where U equals something, and because (crU) is nilpotent, (1 + crU) is invertible. The"
    },
    {
        "casual_text": "Registered users get an email every time there's a new broadcast.",
        "formal_text": "Registered users get an email every time there's a new broadcast. Convert casual text to formal text: Registered users get an email every time there's a new broadcast. Convert casual text to formal text: Registered users get"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: \"2\" is represented as a series of numbers and letters that look like this: 00001000-00010000 10000000 01000000 00100000 00000000 0O0O0100 00000000 00000010 00000001. Basically, it's a way to write the number \"2\" using a bunch of zeros, ones, and some letters mixed in.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: \"2\" is represented as a series of numbers and letters that look like this: 00001000-00010000 10000000"
    },
    {
        "casual_text": "We calculate the Kullback-Leibler divergence (D_KL) between the word distributions of the topic  that represents the cluster C and each tweet in C (Wan and Wang, 2016). You can find more details on how we compute D_KL in Appendix A. We also looked at other text summarization methods (Basave et al., 2014; Wan and Wang, 2016) like MEAD (Radev et al., 2000) and Lexrank (Erkan and Radev, 2004) to find the best tweet that represents the cluster, but our early tests showed that D_KL consistently picks the most fitting tweet. The way we define cluster coherence here has a linear time complexity, O(|T|).",
        "formal_text": "Convert casual text to formal text: We calculate the Kullback-Leibler divergence (D_KL) between the word distributions of the topic  that represents the cluster C and each tweet in C (W"
    },
    {
        "casual_text": "So, L xent is the cross-entropy loss for the task, like classification, and L cal is the calibration loss we calculate on the cross-validation set.  is just a number we use to weigh the calibration loss, L cal. In real-world use, we can figure out the best  through cross-validation. For more info, check out section 4. After that, we can calculate each loss term like this:",
        "formal_text": "Convert casual text to formal text: So, L xent is the cross-entropy loss for the task, like classification, and L cal is the calibration loss we calculate on the cross-validation set."
    },
    {
        "casual_text": "This paper talks about APT (Adversarial Paraphrasing Task), which is a way to create paraphrases using the adversarial approach. Paraphrases are sentences that mean the same thing but use different words and structures. We used APT to make a dataset created by humans (AP H) and two others made by machines (AP M T 5 and AP T w T 5). The main idea was to help paraphrase detectors get better at their job by not relying too much on word-level similarities. We tested this by seeing how well RoBERTa base, trained on TwitterPPDB, did on our APT benchmarks. It didn’t do great at first, but after training on our human or machine-generated datasets, it improved a lot. The code and datasets from this paper are available online for anyone to use. Paraphrase detection and generation have a lot of potential uses, but they haven’t been fully explored yet. For example, they could help in healthcare by making medical info easier to understand, in writing by adjusting style to suit readers better, and in education by simplifying complex texts for students. So, improving these techniques could be really useful in the future.",
        "formal_text": "Convert casual text to formal text: This paper talks about APT (Adversarial Paraphrasing Task), which is a way to create paraphrases using the adversarial approach. Paraphrases are sentences that mean"
    },
    {
        "casual_text": "Here’s the informal version: So, in the paper, they tested some models on the Penn Treebank validation set using F1 scores for binarized trees. The results are like this: - **Kitaev and Klein (2018)**: Binary - 87.5, N-ary - 84.0, Binary N-ary - 85.9, N-ary Binary - 83.6. - **DIORA**: Binary - 86.0, N-ary - 73.9, Binary N-ary - 81.7, N-ary Binary - 69.1. - **S-DIORA**: Binary - 89.9, N-ary - 77.5, Binary N-ary - 84.8, N-ary Binary - 73.2. Basically, DIORA didn’t perform that great due to some weaknesses, but S-DIORA did better, especially in the best setup they found (check Table 2 for more details).",
        "formal_text": "Convert casual text to formal text: Here’s the informal version: So, in the paper, they tested some models on the Penn Treebank validation set using F1 scores for binarized trees. The results are like this:"
    },
    {
        "casual_text": "We also did some tests to see how different feature sets work together. Turns out, combining WLS with vectors from the MUSE model works really well. We saw improvements for eight out of thirteen language pairs, with the biggest jump being 5% (for Hi-Or) and the smallest being 1% (for Hi-Ko, Hi-Ml, Hi-Ta, Hi-Te). It's worth mentioning that this combo didn't hurt performance for any language pair, which is why we included it in Table 3. Other combinations, like MUSE + VecMap, MUSE + XLM-R, or MUSE + PVS, didn't do as well and hurt performance for at least one language pair. When we compared our best model (MUSE + WLS) to the strongest baseline (from Kanojia et al., 2019b), we saw an average improvement of 9%, with the biggest improvement being 18% (for Hi-Ta). Compared to the weakest baseline (just WLS), our best model showed an average improvement of 50%, with the highest being 61% (for Hi-Or).",
        "formal_text": "Convert casual text to formal text: We also did some tests to see how different feature sets work together. Turns out, combining WLS with vectors from the MUSE model works really well. We saw improvements for eight out of"
    },
    {
        "casual_text": "To represent each sentence s j in a group B = s 1, . . . , s m  in a way that captures its meaning, we use three types of features: word embedding (from Mikolov et al., 2013), position embedding (Zeng et al., 2015), and entity embedding (Li et al., 2020). Combining these features has been shown to be really important and helpful for figuring out relationships between things, according to earlier research (Li et al., 2020). For simplicity, we'll skip mentioning the index j of the sentence. Basically, a sentence s is broken down into a list of n words, like s = [w 1, . . . , w n ]. Then, we use a method called word2vec (Mikolov et al., 2013) to turn these words into low-dimensional, real-valued vectors that represent them.",
        "formal_text": "Convert casual text to formal text: To represent each sentence s j in a group B = s 1, . . . , s m  in a way that captures its meaning"
    },
    {
        "casual_text": "The CRF's hypothesis space was set up based on the symbol-level IPA map we created in the first part of our process. One key thing to remember is that we allowed for many-to-many mapping between the orthographic input and the IPA output. This means that a single input character could be linked to multiple IPA symbols, and a group of orthographic characters (called a multigraph) could be matched to just one IPA symbol.",
        "formal_text": "Convert casual text to formal text: The CRF's hypothesis space was set up based on the symbol-level IPA map we created in the first part of our process. One key thing to remember is that we allowed for many"
    },
    {
        "casual_text": "According to the rules in ERD14 and GERDAQ, we set recall to 1.0 when the actual answer for a query is nothing, and we set precision to 1.0 when the guess we made is also nothing.",
        "formal_text": "Convert casual text to formal text: According to the rules in ERD14 and GERDAQ, we set recall to 1.0 when the actual answer for a query is nothing, and we set precision to 1.0 when the guess"
    },
    {
        "casual_text": "The extra findings from the study on model size with the Python dataset are in Tables 8, 9, and 10. The performance trends look pretty similar to what we saw with the Java dataset. For example, Table 9 shows that CODE-SCRIBE's performance doesn't get better when the number goes up from 2 to 12. We also checked out how CODE-SCRIBE does when we change the embedding size, starting from 128 up to 1024, in steps of 128. As you can see in Table 11, CODE-SCRIBE performs the worst with an embedding size of 128, but it gets way better when we bump it up to 256. The performance keeps improving steadily as we increase the size up to 512. After that, even though CODE-SCRIBE still gets a little better as the size grows from 512 to 1024, the improvements aren't as noticeable. This tells us that increasing the embedding size a bit definitely helps CODE-SCRIBE, but going too big doesn't really make a huge difference. Tables 12 and 13 give some examples of how R-AST, R-Copy, V-GCN, V-Dec, and our CODE-SCRIBE perform on the Java and Python datasets. Overall, CODE-SCRIBE seems to create better summaries for the code snippets. For example, in the first case in Table 12, only R-Copy and CODE-SCRIBE got the intent of the code right. In the third case, only CODE-SCRIBE picked up on the important detail, which was \"status panel\".",
        "formal_text": "Convert casual text to formal text: The extra findings from the study on model size with the Python dataset are in Tables 8, 9, and 10. The performance trends look pretty similar to what we saw with the Java dataset. For example, Table 9"
    },
    {
        "casual_text": "We could definitely do more to make the way we encode the related text for zero pronouns even better. For example, we could look at word sequences that go beyond just the current sentence. In Figure 2, we show how our model is doing on the development dataset. You can see that after the first epoch, the F-score on the test dataset is around 46%, and it keeps improving, reaching 52% after about 30 iterations. After that, it kind of levels off. It's pretty clear that paying attention to the right parts of the related text is super important for encoding zero pronouns. By using a self-attentive mechanism, our model is able to focus on the key parts of the context, which helps it understand the sentence better and more efficiently than models without attention. On top of that, by giving different levels of attention to the words in a candidate mention based on how they relate to the zero pronoun, our model can encode these mentions in a more natural way. All of this helps in picking the right antecedents, which leads to better overall performance.",
        "formal_text": "Convert casual text to formal text: We could definitely do more to make the way we encode the related text for zero pronouns even better. For example, we could look at word sequences that go beyond just the current sentence."
    },
    {
        "casual_text": "In theorem proving, we use the top-notch first-order logic automated theorem prover called Vampire (developed by Kovács and Voronkov in 2013). Vampire works with TPTP formats to check if a hypothesis can be proven from some premises using the logical formula we talked about in Section 4.2. The system gives three possible answers: \"yes\" if the hypothesis is provable, \"no\" if the opposite of the hypothesis is provable, and \"unknown\" if neither can be proven. For our experiments, we run Vampire in its fastest mode, CASC mode, and set a timeout of 300 seconds to make sure it doesn't run forever.",
        "formal_text": "Convert casual text to formal text: In theorem proving, we use the top-notch first-order logic automated theorem prover called Vampire (developed by Kovács and Voronkov"
    },
    {
        "casual_text": "The dataset has around 50,000 documents in three languages: English, French, and Spanish. Out of these, about 35,000 are labeled with useful snippets, which can be used to train and test models for extractive summarization. We tested how well three different models work on this dataset: LEAD4, TextRank (an unsupervised graph-based model from Mihalcea and Tarau, 2004), and NeuSum (a supervised neural model created by Qingyu et al. in 2018).",
        "formal_text": "Convert casual text to formal text: The dataset has around 50,000 documents in three languages: English, French, and Spanish. Out of these, about 35,000 are labeled with useful snippets, which can be used to"
    },
    {
        "casual_text": "In this project, we tested our networks at four different depths: 9, 17, 29, and 49 layers. Here, \"depth\" refers to the total number of convolutional layers. To calculate the depth, we add up the number of blocks with 64, 128, 256, and 512 filters, where each block has two convolutional layers. In Figure 1, for example, there are 2 blocks of each type, so the depth is 2  (2 + 2 + 2 + 2) = 16. If you include the very first convolutional layer, that brings the total to 17 layers. You can adjust the depth by adding or removing blocks with specific filter sizes. Table 2 shows the best setups we found for depths 9, 17, 29, and 49, along with the number of parameters for all the convolutional layers.",
        "formal_text": "Convert casual text to formal text: In this project, we tested our networks at four different depths: 9, 17, 29, and 49 layers. Here, \"depth\" refers to the total number of convolutional layers. To calculate the"
    },
    {
        "casual_text": "Lastly, we look at the back translation method, which is often used for unsupervised machine translation (Artetxe et al., 2018; Lample et al., 2018a). The main idea here is to start by setting up the model correctly to give it a strong foundation. Then, we keep going back and forth with translation—both forward and backward—to figure out how context connects with unpaired, non-conversational phrases.",
        "formal_text": "Convert casual text to formal text: Lastly, we look at the back translation method, which is often used for unsupervised machine translation (Artetxe et al., 2018; Lample et al."
    },
    {
        "casual_text": "So, the model does pretty well overall, but it struggles with words that describe positions. Take \"schräg rechts\" (which means diagonally to the right) as an example. This phrase is supposed to describe a position relative to a landmark, but the model ended up interpreting it as a position on the canvas instead. We thought that giving the model both types of descriptions—canvas-relative and object-relative—would help it figure out the difference, but it looks like the word \"rechts\" being used more often kind of messed things up.",
        "formal_text": "Convert casual text to formal text: So, the model does pretty well overall, but it struggles with words that describe positions. Take \"schräg rechts\" (which means diagonally to the right) as an example. This phrase is"
    },
    {
        "casual_text": "So, W e is a matrix in R deV, where de is the size of the word embedding, and V is the total number of words in the vocabulary. At each time step t (from 1 to c + 1) in the decoder, it takes in two things: the embedding of the word that came before, x t1, and a special vector called z t. This z t is made using something called soft attention, which was introduced by Xu et al. in 2015.",
        "formal_text": "Convert casual text to formal text: So, W e is a matrix in R deV, where de is the size of the word embedding, and V is the total number of words in the vocabulary. At each"
    },
    {
        "casual_text": "Oh, and BTW, the [CLS] token's final embedding from BERT is used to represent the root node.",
        "formal_text": "Convert casual text to formal text: Oh, and BTW, the [CLS] token's final embedding from BERT is used to represent the root node. Convert casual text to formal text: Oh, and BTW"
    },
    {
        "casual_text": "So, s is the part of the sentence we want to translate, and e is the part of an entry in the translation memory (TM). For fuzzy match scores (let’s call them F), the fuzzy match cost (hfm) is kind of like 1 minus F. (He et al., 2010) talked about using a bunch of features that don’t depend on the translation system itself. This is super handy when you’re using a third-party translation service or just treating the MT system like a black box—basically, you don’t need to know how it works inside.",
        "formal_text": "Convert casual text to formal text: So, s is the part of the sentence we want to translate, and e is the part of an entry in the translation memory (TM). For fuzzy match scores (let’s call them"
    },
    {
        "casual_text": "For instance, in the production PNG + PRAEP NPR, you can skip the whole first part where you determine +vai(PNG), since the first condition doesn’t apply to this production.",
        "formal_text": "Convert casual text to formal text: For instance, in the production PNG + PRAEP NPR, you can skip the whole first part where you determine +vai(PNG), since the first condition ’t be in"
    },
    {
        "casual_text": "We tested a neural network setup using graph convolutional networks for relation extraction and it worked well. We also came up with a method called path-centric pruning to make dependency-based models more reliable by cutting out unnecessary stuff without losing important details. After looking into it closely, we found that our model works really well alongside sequence models, and the pruning trick can be used on other dependency-based models too.",
        "formal_text": "Convert casual text to formal text: We tested a neural network setup using graph convolutional networks for relation extraction and it worked well. We also came up with a method called path-centric pruning to make dependency-based models more"
    },
    {
        "casual_text": "Sure! Here's a more casual version: The connection between these split-antecedent anaphors and their individual parts is kind of like the opposite of \"element-of.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: The connection between these split-antecedent anaphors and their individual parts is kind of like the opposite of \"element-of.\" Con"
    },
    {
        "casual_text": "We suggest organizing knowledge as complex relationships, like pairs of interactions and triplet-wise geometric angles, using multi-granularity representations.",
        "formal_text": "Convert casual text: We suggest organizing knowledge as complex relationships, like pairs of interactions and triplet-wise geometric angles using multi-granularity representations. Convert casual text to formal text: We suggest organizing knowledge as complex relationships,"
    },
    {
        "casual_text": "We use the same parameters for both forward entailment and reverse entailment. Basically, to calculate p j, we flip the order of b j, b j to reuse M T in this scoring function. M T is just a matrix from M T that’s tied to the forward entailment relation.",
        "formal_text": "Convert casual text to formal text: We use the same parameters for both forward entailment and reverse entailment. Basically, to calculate p j, we flip the order of b j, b"
    },
    {
        "casual_text": "This method for pulling out important information from text uses something called \"selective concept extraction,\" which was first described by Riloff back in 1993. It’s kind of like skimming through text but only focusing on the bits that are relevant to your topic, while skipping over everything else that doesn’t seem important. Hearst, who was one of the early folks working on pattern-based approaches in 1992, came up with lexico-syntactic patterns to automatically figure out relationships between words. Patterns like \"X and other Y\" or \"X such as Y\" were used to identify hypernyms and hyponyms. Later on, other researchers, like Roark and Charniak in 1998, Caraballo in 1999, and Maynard and Peters in 2009, tweaked this idea to also work for finding synonyms. The downside is that manually creating these patterns takes a lot of time and you need to know a fair bit about linguistics. Typically, systems using these patterns are really good at being precise but not so great at finding everything (low recall) because these patterns don’t show up that often. However, Ohshima and Tanaka’s work in 2009, which looked at web data, found that they could get much better recall rates. To avoid all the manual work of defining patterns, a bunch of supervised approaches have been suggested, as summarized by Stevenson and Greenwood in 2006.",
        "formal_text": "Convert casual text to formal text: This method for pulling out important information from text uses something called \"selective concept extraction,\" which was first described by Riloff back in 1993. It’s kind of like skimming"
    },
    {
        "casual_text": "We did the dialect normalization in two ways: first, we only applied it to new test data, and second, we applied it to both the training and test data. In Section 4, we share the results of our experiments.",
        "formal_text": "Convert casual text to formal text: We did the dialect normalization in two ways: first, we only applied it to new test data, and second, applied it to both the training and test data. In Section 4, we share the results"
    },
    {
        "casual_text": "In this setup, the way input sentences are understood is based on how different 'understanding' systems process them internally. So, the outcome of this understanding is shown using symbolic expressions that internal programs can read and use for specific tasks.",
        "formal_text": "Convert casual text to formal text: In this setup, the way input sentences are understood is based on how different 'understanding' systems process them internally. So, the outcome of this understanding is shown using symbolic expressions that internal"
    },
    {
        "casual_text": "For the encoding part, we're using a one-layer bidirectional LSTM from PyTorch. To get a fixed-size bottleneck, we take the last hidden state from both directions and average them. The input size, which is also the token embedding size, is 300.",
        "formal_text": "Convert casual text to formal text: For the encoding part, we're using a one-layer bidirectional LSTM from PyTorch. To get a fixed-size bottleneck, we take the last"
    },
    {
        "casual_text": "The goal here is to boost the score for the correct tags in the sequence, which we write as s  (x T 1,  T 1 ). One thing to point out is that the first part and the second part of this new sequence score don’t really depend on each other. This means we can train the BLSTM-based tagging model and the LSTM-based language model separately using cross-entropy.",
        "formal_text": "Convert casual text to formal text: The goal here is to boost the score for the correct tags in the sequence, which we write as s  (x T 1,  T 1 ). One thing to point out is that"
    },
    {
        "casual_text": "Alright, so we've got an encoder E and a decoder D. Our job is to take a bunch of inputs x 1. . . x N  and turn them into outputs  1. . .  N , where N is the number of data points. For instance, in machine translation, each x i is a sentence in the source language, made up of words or tokens, and each y i is the translated version. The decoder D works by taking e i = E(x i ) and a partial translation y i as input, and it builds the full translation y i one token at a time.",
        "formal_text": "Convert casual text to formal text: Alright, so we've got an encoder E and a decoder D. Our job is to take a bunch of inputs x 1. . . x"
    },
    {
        "casual_text": "So, how much filtering is too much? In our experiments, we set the thresholds for each filtering method to keep around 40% of the original training data. Both automatic and human evaluations show that the LM PPL method works the best overall. To figure out the perfect amount of filtering, we trained and tested more models by tweaking the lower limit of the LM PPL filter to use different amounts of data. We also tried combining all the filtering methods to train a model on only the least generic responses, based on the thresholds we set in Section 4.2. The results of these tests are in Table 3.4. Looking at the LM PPL filter alone, we found that the biggest improvements happen when we train with between 40 and 80% of the total data. According to metrics like chrF-src, Self-BLEU, and Uniq., more aggressive filtering (like 40%) works really well with almost no loss in chrF-tgt. But, super aggressive filtering (like 20%) drops performance across the board. What’s cool though is that combining all the filtering methods for aggressive filtering actually has a better effect, which suggests that using multiple filtering methods can really boost the quality of the training data.",
        "formal_text": "Convert casual text to formal text: So, how much filtering is too much? In our experiments, we set the thresholds for each filtering method to keep around 40% of the original training data. Both automatic and human evaluations show"
    },
    {
        "casual_text": "In this part, we’re going to look at some experiments to back up the methods we’re suggesting. First off, our experimental setup is pretty different from what’s been used before for domain adaptation in a bunch of ways:",
        "formal_text": "Convert casual text to formal text: In this part, we’re going to look at some experiments to back up the methods we’re suggesting. First off, our experimental setup is pretty different from what’s used before for domain adaptation"
    },
    {
        "casual_text": "We use the max operation to get rid of the noisy paths and pick the most representative one.",
        "formal_text": "Convert casual text to formal text: We use the max operation to get rid of the noisy paths and pick the most representative one. We use the max operation to pick the most representative one."
    },
    {
        "casual_text": "Text-based games are like puzzles where you make choices one after the other, which makes them perfect for reinforcement learning (RL). These games are what we call partially observable Markov decision processes (POMDPs)—basically, the text you see doesn’t give you all the info about the game world. To break it down, the game is a POMDP with a bunch of components: states (S), actions (A), transition probabilities (T), rewards (R), observations (), conditional observation probabilities (O), and a discount factor (). Even though the full state of the game has all the details, what you see (the observation) doesn’t. In this paper, we make a couple of assumptions: first, the words in each command come from a fixed set of words (V), and second, each action is made up of two words—a verb and an object. The goal for the agent is to rack up as much expected discounted reward as possible, which means maximizing something like E[ t  t r t ].",
        "formal_text": "Convert casual text to formal text: Text-based games are like puzzles where you make choices one after the other, which makes them perfect for reinforcement learning (RL). These games are what we call partially observable Markov decision processes"
    },
    {
        "casual_text": "BankAmerica's stock went up 1 3/4 points to 30 after PaineWebber upgraded their view on it to their top rating.",
        "formal_text": "Convert casual text to formal text: BankAmerica's stock went up 1 3/4 points to 30 after PaineWebber upgraded their view it their top rating. Convert casual text to formal text: BankAmerica's stock went up 1"
    },
    {
        "casual_text": "Most recent research on (shallow) discourse parsing has been pretty focused on individual parts of the task, like figuring out where arguments are (Knaebel et al., 2019) or classifying the type of discourse (Dai and Huang, 2019; Shi and Demberg, 2019; Van Ngo et al., 2019). The full (shallow) discourse parsers usually work in stages, like having different systems for handling implicit and explicit relations (Lin et al., 2014) or creating separate models for relationships within a sentence versus between sentences (Biran and McKeown, 2015). We’re also going with the pipeline approach for our baseline model (Section 6), which deals with both figuring out the relations and identifying the arguments, because our QA pairs kind of combine both of these things.",
        "formal_text": "Convert casual text to formal text: Most recent research on (shallow) discourse parsing has been pretty focused on individual parts of the task, like figuring out where arguments are (Knaebel et al"
    },
    {
        "casual_text": "Alright, so we've got two vectors, and we calculate the cosine similarity between them. We do the same thing for the Question-language Features. Earlier, we talked about translating the sentence (s 1best) and finding the cosine similarity with the original question (q). We use Equation 2 to get v q and v s 1best. Even though we could translate the sentence into English using four different methods, we only went with the one-best translation because it’s less heavy on the computer. So, in the QL view, we just have one lexical similarity feature (let’s call it LexQL). The whole process for these five lexical similarity features is laid out in Table 1. Once we’ve got all the numbers, we use a maximum-entropy model to figure out the feature weights. Oh, and even though it’s not shown in the figure or table, we also include the same set of features from the sentence right before the answer in the forum post to give a better idea of the bigger picture.",
        "formal_text": "Convert casual text to formal text: Alright, so we've got two vectors, and we calculate the cosine similarity between them. We do the same thing for the Question-language Features. Earlier, we talked"
    },
    {
        "casual_text": "- **Usability**: Making it easy for developers and users from all kinds of backgrounds to use the stuff we’ve built in TSNLP for all sorts of diagnosis and evaluation tasks. - **Suitability**: Making sure the system can handle storing and managing natural language test data (like in string processing) and giving really flexible tools to work with.",
        "formal_text": "Convert casual text to formal text: - **Usability**: Making it easy for developers and users from all kinds of backgrounds to use the stuff we’ve built in TSNLP for all sorts of diagnosis and evaluation tasks."
    },
    {
        "casual_text": "Basically, W_s is the group of words that can add the suffix s to make a new word. And len(w) just tells us how long the word w is. At the end, we pick the top N options from the list.",
        "formal_text": "Convert casual text to formal text: Basically, W_s is the group of words that can add the suffix s to make a new word. And len(w) just tells us how long the word"
    },
    {
        "casual_text": "where sim() is just a way to measure how similar things are (like cosine similarity), N p (w i ) means the group of the k words that are most similar to w i (so the ones with the highest similarity), and N n (w i ) is the group of the k words that are least similar to w i (the ones with the lowest similarity).",
        "formal_text": "Convert casual text to formal text: where sim() is just a way to measure how similar things are (like cosine similarity), N p (w i ) means the group of the k words that"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. First, let's look at the numbers: - ONTONOTES (Chiu and Nichols, 2016): 94.03 ( 0.23) and 84.57 ( 0.27) - Our model: 94.80 ( 0.10) and 86.44 ( 0.14) These are the F1 scores from Table 3, comparing our best setup to what Chiu and Nichols got in 2016. Now, here's what we found: 1. Our model does way better than older models that relied on a ton of manually created features (like Ratinov and Roth, 2009; Lin and Wu, 2009) and even beats a system from Luo et al. (2015) that used Named Entities (NE) and Entity Linking to improve both tasks at once. 2. We also outperform other neural network models that just use basic word embeddings. This shows that our extra lexical features work well alongside the usual word embeddings. 3. Our system performs just as well as the current top models, even though they use fancier architectures or more complex features. For example, Tran et al. (2017) used a stacked residual RNN (Bi-LSTM) with three layers and bias decoding, but our model is simpler and faster. They got 90.43 with a setup similar to ours. 4. The two systems that scored slightly higher on the CONLL dataset used embeddings from forward and backward Language Models trained on a huge dataset (the One Billion Word Benchmark by Chelba et al., 2013). They saw improvements of 0.8 to 1.2 points by using these LM embeddings, which suggests that our LS vectors are pretty efficient too.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. First, let's look at the numbers: - ONTONOTES (Chiu and Nichols, 2016"
    },
    {
        "casual_text": "The filtering step helps narrow down the new search space by picking out the strongest candidates for the final part, where we use the translation similarity measure (check out Section 4). The filtering needs to be super quick and accurate so we don’t accidentally toss out any useful parallel data. To do this, we calculate a viability score for each pair of sentences and only keep the ones that score above average. If we’ve got a pair made up of a source sentence *s* and a target sentence *t*, here’s the formula we use:",
        "formal_text": "Convert casual text to formal text: The filtering step helps narrow down the new search space by picking out the strongest candidates for the final part, where we use the translation similarity measure (check out Section 4). The filtering needs to"
    },
    {
        "casual_text": "After the translation step, we put the simplified expressions back together using the original text and the word alignment between the input and the output, which the decoder tells us about.",
        "formal_text": "Convert casual text to formal text: After the translation step, we put the simplified expressions back together using the original text and the word alignment between the input and the output, which the decoder tells us about. Convert casual"
    },
    {
        "casual_text": "On the flip side, a basic approach that just grabs and groups together phrases like \"N m no N h\" (using some way to measure how similar the nouns are) won't be able to build full-on nominal case frames. This is because of words having multiple meanings (polysemy) and needing different required cases. But hey, dictionary definitions can actually help with sorting this out, even for those tricky situations.",
        "formal_text": "Convert casual text to formal text: On the flip side, a basic approach that just grabs and groups together phrases like \"N m no N h\" (using some way to measure how similar the nouns are)"
    },
    {
        "casual_text": "Clearly, these two tasks are connected because the info you need to answer a new question often comes from threads of similar questions. We’re looking at tackling both tasks together, with the help of another related task: figuring out if a comment in a question-comment thread actually answers the main question.",
        "formal_text": "Convert casual text to formal text: Clearly, these two tasks are connected because the info you need to answer a new question often comes from threads of similar questions. We’re looking at tackling both tasks together, with the"
    },
    {
        "casual_text": "Figure 2 shows how the different versions of the DeReKo corpus—DeReKo-2018-I to DeReKo-2019-I—are connected, along with three virtual corpora (let’s call them 1, 2, and 3) and some texts. Corpus 1 was first defined based on DeReKo-2018-I and included two texts: HMP17/FEB. 18387 and AZM18/MAI. 11491. When DeReKo-2018-II came out, another text, GAZ18/JAN. 12539, was added to Corpus 1 because it fit the criteria for that corpus. At the same time, based on DeReKo-2018-II, Corpus 2 was created, and it included GAZ18/JAN. 12539. Later, with DeReKo-2019-I, Corpus 3 was added, and it included AZM18/MAI. 1149. The point here is that texts in DeReKo can be part of multiple corpora, which can make things complicated if you start removing texts.",
        "formal_text": "Convert casual text to formal text: Figure 2 shows how the different versions of the DeReKo corpus—DeReKo-2018-I to DeReKo-2019-I—are connected, along with three virtual corp"
    },
    {
        "casual_text": "This method works well alongside the one we're suggesting. Let's say someone wants to search for the word  but doesn't know the first kanji. Typing  would still work because the system can figure out that  is similar to . It would show the word  in the results, letting the user see both how it's pronounced [hmoN] and what it means, which is \"visit\".",
        "formal_text": "Convert casual text to formal text: This method works well alongside the one we're suggesting. Let's say someone wants to search for the word  but doesn't know the first kanji. Typing  would"
    },
    {
        "casual_text": "In Step Figure 1c), we show the average of the measure for languages we haven't looked at yet (where step > k). The measure we're reporting is the F1 score, and higher is better.",
        "formal_text": "Convert casual text to formal text: In Step Figure 1c), we show the average of the measure for languages we haven't looked at yet (where step > k). The measure we're reporting is the F1 score"
    },
    {
        "casual_text": "We also tested how PM works within a system that handles coreference across different documents, specifically for proper nouns. Fixing a typo in a proper name is trickier than fixing one in a regular word. Some mistakes aren't just random; they happen over and over, like when foreign names get transliterated. To deal with this, we paired PM with a statistical learning algorithm that looks at the surrounding characters in the text to guess the likelihood of a certain type of misspelling. Unlike with common words, where a typo is usually obvious, with proper names, things get murkier. For example, is \"John\" and \"Jon\" two different names, or is one just a misspelling of the other? To figure this out, we combine the evidence from string similarity with the context clues provided by the coreference system to help clear up the confusion.",
        "formal_text": "Convert casual text to formal text: We also tested how PM works within a system that handles coreference across different documents, specifically for proper nouns. Fixing a typo in a proper name is trickier than fixing one"
    },
    {
        "casual_text": "But, dialogue acts and responses can have really different lengths and vocab sizes, which makes it tricky to set the right weight, . So, instead of trying to tweak it manually, we’re using an uncertainty loss (shoutout to Kendall et al., 2018) to adjust it automatically.",
        "formal_text": "Convert casual text to formal text: But, dialogue acts and responses can have really different lengths and vocab sizes, which makes it tricky to set the right weight, . So, instead of trying to tweak it manually,"
    },
    {
        "casual_text": "Okay, so we have a bunch of words, and we turn them into a matrix using their word embeddings. Then, we look at the smallest eigenvalue of that matrix to figure out something about the words. First, we want to check if the four words we used in Section 5 (which are kind of like analogies) have a smaller smallest eigenvalue compared to four random words. To do this, we come up with something called the \"min eigenvalue ratio,\" which we’ll call S/R. Here, R is the average of the smallest eigenvalues from 1,000 sets of random N-word groups, and S is the average of the smallest eigenvalues from sets of words that are more like analogies (like the ones from the Google analogy dataset). We’re looking at the ratio instead of just the absolute value because the size of the word embeddings can be different depending on the model we’re using, and that would mess with the numbers.",
        "formal_text": "Convert casual text to formal text: Okay, so we have a bunch of words, and we turn them into a matrix using their word embeddings. Then, we look at the smallest eigenvalue of"
    },
    {
        "casual_text": "The findings reveal that surprisal from the character-based structural model (CharWSurp) had the biggest impact on improving the model's performance compared to other models when looking at both full and No-OOV sets of self-paced reading times (check out Figure 2; the difference between the CharWSurp model and the others is super significant, with p  0.001, based on a paired permutation test using by-item errors). Removing OOV words didn’t really change the overall pattern of LL across different models. Even though the pretrained language models were trained on way bigger datasets and tend to have lower perplexities on test data, this result suggests that CharWSurp might offer a better explanation of how humans experience processing difficulty. So, just being good at predicting the next word doesn’t fully capture the human-like processing costs we see in self-paced reading times. Looking at the residuals grouped by the lowest base category of the previous time step (c b d t1) from manual annotations (Shain et al., 2018), it’s clear that CharWSurp’s improvement over GPT2Surp was pretty consistent across different categories (see Figure 3 for more details).",
        "formal_text": "Convert casual text to formal text: The findings reveal that surprisal from the character-based structural model (CharWSurp) had the biggest impact on improving the model's performance compared to other models when looking at both"
    },
    {
        "casual_text": "In Daxenberger and Gurevych's 2012 paper, they broke down a 21-category system into three main types of edits: text-based (which change meaning), surface (edits that don't change meaning), and Wikipedia policy edits (like VANDALISM and RE-VERT). Within the text-based edits, they grouped things like templates, references (both internal and external links), files, and information. Each of these was then split into three subcategories: insertions (I), deletions (D), and modifications (M). Surface edits include stuff like paraphrasing, fixing spelling and grammar, moving things around, and markup changes. The markup edits cover anything that tweaks markup elements that don’t fit into the other categories, like adding, deleting, or modifying things. An example would be messing with apostrophes in '''bold text'''. They also added an \"OTHER\" category for edits that couldn’t be labeled due to segmentation issues. Figure 1 gives an example of an edit from WPEC, labeled as REFERENCE-M. WPEC was made through a manual annotation process with three people working on it. The agreement between the annotators, measured using Krippendorf's , was .67. The experiments in the study used the gold standard annotations from WPEC, which were decided by a majority vote for each edit.",
        "formal_text": "Convert casual text to formal text: In Daxenberger and Gurevych's 2012 paper, they broke down a 21-category system into three main types of edits: text-based (which change meaning"
    },
    {
        "casual_text": "To go along with Table 1, we’ve put the correlations for the best baselines in Figure 2. We looked at what happens when we reduce the number of gold references from 11 down to 1. For all four dimensions and all the baselines, we noticed that fewer references lead to lower correlation and more variance. But QU E S TEV A L doesn’t need any references at all. So, as the number of references drops, QU E S TEV A L does better compared to the other metrics. Plus, QU E S TEV A L can still evaluate systems even when there are no gold references available.",
        "formal_text": "Convert casual text to formal text: To go along with Table 1, we’ve put the correlations for the best baselines in Figure 2. We looked at what happens when we reduce the number of gold references from 11 down to 1. For all"
    },
    {
        "casual_text": "When we're dealing with sentence annotations, here's how we figure out the unreliability score for that annotation:",
        "formal_text": "Convert casual text to formal text: When we're dealing with sentence annotations, here's how we figure out the unreliability score for that annotation: Here's how we figure out the unreliability score for that annotation"
    },
    {
        "casual_text": "(2) Sometimes there are multiple \"findings\" or \"impression\" sections, but they don’t line up properly. Or (3) the \"findings\" have less than 10 words, or the \"impression\" has fewer than 2 words. Lastly, we swapped out all the date and time mentions with special tokens, like DATE>. For both datasets, we split them up by time into training, dev, and test sets. We did this to see if our model can handle future data after being trained on older stuff. You can check out the time ranges for each split in Table 6.",
        "formal_text": "Convert casual text to formal text: (2) Sometimes there are multiple \"findings\" or \"impression\" sections, but they don’t line up properly. Or (3) the \"findings\" have less than 10 words, or the"
    },
    {
        "casual_text": "The In Media Res Corpus Most of the datasets people use for labeling stuff are all about news or tweets talking about what's happening right now. But when it comes to labeling things like creative works—movies, books, etc.—there isn't really a standard way to do it. Everyone kind of does their own thing.",
        "formal_text": "Convert casual text to formal text: The In Media Res Corpus Most of the datasets people use for labeling stuff are all about news or tweets talking about what's happening right now. But when it comes to labeling things"
    },
    {
        "casual_text": "A big issue with random walk inference is how the knowledge base graph is connected. If there’s no way to get from one node to another in the graph, then PRA can’t predict any relationship between them. So, earlier research started using a text corpus to make the graph more connected (Lao et al., 2012; Gardner et al., 2013). But this method has its own downsides. While knowledge base relations are clear and distinct, surface text isn’t like that. For instance, \"The Nile flows through Cairo\" and \"The Nile runs through Cairo\" basically mean the same thing. Adding a text corpus does make the graph more connected, but it also makes things way more scattered and harder to work with.",
        "formal_text": "Convert casual text to formal text: A big issue with random walk inference is how the knowledge base graph is connected. If there’s no way to get from one node to another in the graph, then PRA can’t"
    },
    {
        "casual_text": "Alright, so in some semantic graph setups, there's a total order on the nodes, usually based on the order of the words in the sentence. These graphs are often called bi-lexical dependencies, likely because of some ideas Eisner talked about in 1997. Basically, they're ordered graphs. A cool way to show these bi-lexical dependency graphs is by drawing the connections as semicircles above the sentence. If the semicircles only cross at their ends in this drawing, the graph is called noncrossing. This is kind of like a fancy way of describing projectivity, which you might know from dependency trees.",
        "formal_text": "Convert casual text to formal text: Alright, so in some semantic graph setups, there's a total order on the nodes, usually based on the order of the words in the sentence. These graphs are often"
    },
    {
        "casual_text": "In this study, the annotators were told to work through the sentences one at a time. Basically, they’d read a sentence, annotate it, then move on to the next one, and keep going like that. To make this process easier, we created an annotation tool that only shows one sentence at a time on the screen. The annotator could move to the next sentence by pressing a key. They were also allowed to go back and change their previous annotations if needed. The tool kept track of how long each keystroke took. We ran the annotation process on Amazon Mechanical Turk (AMT). For each assignment, workers were given an annotation guideline and a link to a news editorial. For every sentence, they had to pick an attribute value and then choose the reasons for that value in two steps. The workers used the annotation tool to do this. Even though we based the annotation on a corpus of argument strategies, we only gave the annotators plain sentences without any extra info that might hint at those strategies.",
        "formal_text": "Convert casual text to formal text: In this study, the annotators were told to work through the sentences one at a time. Basically, they’d read a sentence, annotate it, then move on to"
    },
    {
        "casual_text": "You can use TexSmart in two ways: either by calling the HTTP API directly or by downloading the offline SDK. Just a heads-up, the results from the HTTP API and the SDK might be a bit different for the same input text. This is because the HTTP API uses a bigger knowledge base and supports more text understanding tasks and algorithms. If you want a detailed comparison between the SDK and the HTTP API, you can check it out here: https://ai.tencent.com/ailab/nlp/texsmart/en/instructions.html. **Offline Toolkit (SDK)** Right now, the SDK works on Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support is coming in version 0.3.0. The SDK supports a bunch of programming languages, including C, C++, Python (both versions 2 and 3), and Java (version 1.6.0 or higher). If you want to see how to use the SDK with different languages, check out the example codes in the ./examples sub-folder. For instance, the Python example in ./examples/python/en_nlu_example1.py shows how to process an English sentence using the TexSmart SDK. And the C++ example in ./examples/c_cpp/src/nlu_cpp_example1.cc demonstrates how to analyze both an English and a Chinese sentence. **HTTP API** The HTTP API for TexSmart has two parts: the text understanding API and the text matching API. The text understanding API can be accessed using HTTP-POST, and the URL is available on the webpage. The text matching API is used to calculate how similar two sentences are.",
        "formal_text": "Convert casual text to formal text: You can use TexSmart in two ways: either by calling the HTTP API directly or by downloading the offline SDK. Just a heads-up, the results from the HTTP API and the SD"
    },
    {
        "casual_text": "A popular and straightforward way to measure distance is using the l1 norm. This method just adds up the differences in brightness between corresponding pixels in two images, assuming they're lined up properly. Luckily, all kanji are designed to fit into the same size square, so aligning them is pretty straightforward—it's just a grid that works for all kanji. If we say p_x(i, j) is the brightness of the pixel at position (i, j) in the kanji x, then we calculate the l1 norm like this:",
        "formal_text": "Convert casual text to formal text: A popular and straightforward way to measure distance is using the l1 norm. This method just adds up the differences in brightness between corresponding pixels in two images, assuming they're lined up"
    },
    {
        "casual_text": "The first part of the process focused on labeling tweets within groups, taking a cue from the word intrusion task (Chang et al., 2009). Basically, people grouped tweets that talked about the same thing, and any tweets that didn’t fit were put into their own little groups. Depending on how well the tweets matched up, there could be a few different groups in one big cluster. This grouping was important because it set the stage for what came next. This sub-clustering step was a good balance between keeping costs low and making sure the process was still useful, since manually sorting through thousands of tweets just isn’t realistic. We didn’t worry too much about getting everyone to agree on the exact groups at this point, because that wasn’t the main goal. But if there were big disagreements, they’d be picked up later when we looked at quality and issue identification (more on that below). The second part was all about judging the quality of the clusters. This was the main thing we were focusing on. Similar to what Newman et al. (2010) did with topic words, people rated how well the tweets in a cluster fit together on a scale of 1 to 3 (Good, Intermediate, Bad). A cluster got a \"Good\" rating if most of the tweets were about the same thing (like one of those sub-groups we mentioned earlier), while a \"Bad\" rating went to clusters with a bunch of different topics mixed in.",
        "formal_text": "Convert casual text to formal text: The first part of the process focused on labeling tweets within groups, taking a cue from the word intrusion task (Chang et al., 2009). Basically, people"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way: 1. **ENTITY-ARG MATCH**: This is a yes/no thing that happens when two aligned entities get the same argument role label. 2. **ENTITY-NEAR-ARG MATCH**: This one groups Arg1 and Arg2 (and their subtypes) into one category to count matches. 3. **PREDICATE-ARG MATCH**: Another yes/no thing that pops up when at least two aligned arguments share the same role. 4. **PREDICATE-NEAR-ARG MATCH**: Similar to the last one, but it groups Arg1 and Arg2 (and their subtypes) into one category to count matches. 5. **SEMANTIC/PRAGMATIC FEATURES**: These are pulled out during preprocessing. 6. **NAMED ENTITY CLASS**: This feature has a unique value for each of the 150 named entity classes. 7. **TEMPORAL NORMALIZATION**: A yes/no thing that happens when temporal expressions are normalized to the same ISO 8601 format. 8. **MODALITY MARKER**: This one flags when both texts use the same modal verbs. 9. **SPEECH-ACT**: A yes/no thing that happens when the lexicons show the same speech act in both texts. 10. **FACTIVITY MARKER**: This flags when factivity markers show either TRUE or FALSE in both texts at the same time. 11. **BELIEF MARKER**: Similar to the last one, but for belief markers showing either TRUE or FALSE in both texts at the same time.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way: 1. **ENTITY-ARG MATCH**: This is a yes/no thing that happens when two aligne"
    },
    {
        "casual_text": "Even though we're focusing on the transfer method for future MT systems, we’re not saying it’s better than the interlingual approach. Our main point here is that the term 'understanding texts' in MT is pretty unclear. So, before we dive into comparing the pros and cons of these two methods, we need to figure out what we actually mean by this whole 'understanding' thing.",
        "formal_text": "Convert casual text to formal text: Even though we're focusing on the transfer method for future MT systems, we’re not saying it’s better than the interlingual approach. Our main point here is that the term"
    },
    {
        "casual_text": "Theorem 4: If G is a finite subset of I(P), then the sub-algebra of I(P) created by G, called I(G), is also finite.",
        "formal_text": "Convert casual text to formal text: Theorem 4: If G is a finite subset of I(P), then the sub-algebra of I(P created by G, called I(G), is"
    },
    {
        "casual_text": "W_s is just a model parameter, no big deal. Now, fine-tuning. With feature-level stacking, tagger A can be adjusted during the training of tagger B. The loss function gets sent back to tagger A through the w_i_a layer (you can see it in the red dotted lines in Figure 3(b)). This is one more cool thing about neural stacking compared to the old-school discrete stacking.",
        "formal_text": "Convert casual text to formal text: W_s is just a model parameter, no big deal. Now, fine-tuning. With feature-level stacking, tagger A can be adjusted during the training of tagger"
    },
    {
        "casual_text": "And 14. That's way fewer than in computer vision, where, for example, ImageNet has 1,000 classes. This means each example gives less gradient info, which could make training big models tougher. Plus, some tasks are super vague, especially sentiment analysis, where it's hard to pin down specific labels. Both the training and test sets have the same number of examples in each class. For more on how the datasets were made, you can check that out. Table 4 has the best results we found from other papers. We didn't use \"Thesaurus data augmentation\" or any fancy preprocessing, just lower-cased the text. Still, we beat the best convolutional neural networks for all these datasets. Our main point is to show that deep convolutional networks work great as text encoders. Data augmentation might make our results even better, and we'll look into that in future research.",
        "formal_text": "Convert casual text to formal text: And 14. That's way fewer than in computer vision, where, for example, ImageNet has 1,000 classes. This means each example gives less gradient info, which could make training big models tougher"
    },
    {
        "casual_text": "The process involves picking from a list of thirteen options that look like this: H (modal) [P true/false], where H stands for the person involved and P is some statement or event. The job of the annotators is to choose the option that best shows the meaning of the modal in a given situation. The agreement rates between different annotators are pretty good—0.82 for triggers and 0.76 for targets.",
        "formal_text": "Convert casual text to formal text: The process involves picking from a list of thirteen options that look like this: H (modal) [P true/false], where H stands for the person involved and P is some statement or"
    },
    {
        "casual_text": "Sure, there are algorithms with a = 2.3727, but the ones we actually use have a = 2.807 or a = 3 (thanks to Coppersmith and Winograd, 1990).",
        "formal_text": "Convert casual text to formal text: Sure, there are algorithms with a = 2.3727, but the ones we actually use have a = 2.807 or a = 3 (thanks to Coppersmith and Winograd"
    },
    {
        "casual_text": "What’s in a text usually starts off as a kind of web, where different ideas are closely connected to each other. But when we write things down, we have to follow a straight line, so a lot of those connections get lost in the usual way we read or write texts. Figuring out those connections automatically is like the first step in \"understanding\" a text, and it would definitely help with stuff like machine translation, summarizing, or answering questions.",
        "formal_text": "Convert casual text to formal text: What’s in a text usually starts off as a kind of web, where different ideas are closely connected to each other. But when we write things down, we have to follow a straight"
    },
    {
        "casual_text": "At each step, we deliberately included some weird or unnatural options for a reason. We did this on purpose to catch any mistakes or random responses (like spam) from the workers. We figured that picking one of these nonsensical or unnatural reasons would be a sign that the annotation isn't reliable. Here's what we mean by nonsensical or unnatural reasons: they're the kind of reasons that a good annotator wouldn't choose, and they help us spot random or spammy responses. For instance, if someone picks \"I am not familiar with the topic\" for knowledge awareness but also says \"I am an expert on the subject,\" that just doesn't add up. Table 2 has some examples of these nonsensical reasons.",
        "formal_text": "Convert casual text to formal text: At each step, we deliberately included some weird or unnatural options for a reason. We did this on purpose to catch any mistakes or random responses (like spam) from the workers. We figured"
    },
    {
        "casual_text": "We ran all our experiments on a workstation with 187 GiB of RAM. The workstation has either two Intel Xeon 5218 CPUs or two Intel Xeon 4110 CPUs. Each experiment can also use 1 Nvidia GTX 2080Ti GPU.",
        "formal_text": "Convert casual text to formal text: We ran all our experiments on a workstation with 187 GiB of RAM. The workstation has either two Intel Xeon 5218 CPUs or two Intel Xeon 4"
    },
    {
        "casual_text": "A topic is basically a bunch of related events tied to a particular time, place, and person or people (Nallapati et al., 2004). Topics that have two sides, like a debate or competition, tend to grab people's attention and get a lot of coverage. For these kinds of topics, figuring out which side certain people or names are on in the articles can really help readers understand the whole thing better. For example, during the 2008 U.S. presidential election, there were tons of online articles about the Democrat and Republican parties. Knowing who the key players were in each party would give readers a more complete picture of the whole election.",
        "formal_text": "Convert casual text to formal text: A topic is basically a bunch of related events tied to a particular time, place, and person or people (Nallapati et al., 2004). Topics that have two"
    },
    {
        "casual_text": "In this part, we tried out different data augmentation methods to see how they worked. We'll tell you about the SLU system we used, how we set up the experiments, and what the results were.",
        "formal_text": "Convert casual text to formal text: In this part, we tried out different data augmentation methods to see how they worked. We'll tell you about the SLU system we used, how we set up the experiments, and what the"
    },
    {
        "casual_text": "Another example where expectations come into play when trying to figure out what someone means is with sentences like (4). The way it's written makes it seem like it's making some kind of statement, but just looking at the words, you can't tell what it's actually about. If this sentence shows up at the start of a conversation, though, it's probably meant to tell you who the person is. In this situation, the expectations aren't based on something they said before, but on the general idea of how conversations usually start.",
        "formal_text": "Convert casual text to formal text: Another example where expectations come into play when trying to figure out what someone means is with sentences like (4). The way it's written makes it seem like it's making some kind of statement,"
    },
    {
        "casual_text": "For our next steps, we're thinking of focusing on three main areas. First, we want to work on improving how we collect data. Right now, we're using screenshots and OCR, which can mess things up with lots of non-text stuff and broken-up text due to weird webpage designs. Second, we'd like to get more people involved in our studies. And third, we're curious to see how our word2vec results stack up against newer models like BERT (Devlin et al., 2019). BERT has been doing really well across a bunch of NLP tasks, better than models with static word embeddings. Using BERT for language modeling isn't straightforward because of its bi-directional setup and masking, but Salazar et al. (2020) recently figured out how to get prediction values from BERT and similar models trained with masking loss. BERT's subword representations might also help clean up OCR errors, especially when only a few letters are wrong. The downside is, with our current corpus sizes of 300/500K tokens, it's not clear if we have enough data to properly handle all of BERT's parameters. One idea is to use a BERT model that's already been trained on a huge corpus and then fine-tune it with our smaller datasets.",
        "formal_text": "Convert casual text to formal text: For our next steps, we're thinking of focusing on three main areas. First, we want to work on improving how we collect data. Right now, we're using screenshots and O"
    },
    {
        "casual_text": "We showed that our approach works well across different languages and test sets, and it even improved how pronouns were translated. While we mainly focused on pronouns, the fine-tuning method we used isn’t limited to just that—it can also fix other types of errors in machine translations, like named entities or rare words. For future projects, we plan to test out how this method can be applied to other kinds of issues too.",
        "formal_text": "Convert casual text to formal text: We showed that our approach works well across different languages and test sets, and it even improved how pronouns were translated. While we mainly focused on pronouns, the fine-t"
    },
    {
        "casual_text": "You can find the code over at https://github.com/lena-voita/the-story-of-heads.",
        "formal_text": "Convert casual text to formal text: You can find the code over at https://github.com/lena-voita/the-story-of-heads.html. Convert casual text to formal text:"
    },
    {
        "casual_text": "In the call center data, natural language processing can be tricky because there's a lot of informal writing in it. The main issues are:",
        "formal_text": "Convert casual text to formal text: In the call center data, natural language processing can tricky because there's a lot of informal writing in it. The main issues are: Convert casual text to formal text: In the call center"
    },
    {
        "casual_text": "We analyze how the structural-aware model helps. Since the decoding process is broken down into two parts, we look at the benefits separately for each component: skeleton prediction and DRU parsing.",
        "formal_text": "Convert casual text to formal text: We analyze how the structural-aware model helps. Since the decoding process is broken down into two parts, we look at the benefits separately for each component: skeleton prediction and DRU par"
    },
    {
        "casual_text": "For our NMT task, we used the OpenNMT-Py toolkit (Klein et al., 2017) to run our experiments. We went with a Bidirectional RNN Encoder-Decoder setup with attention (Bahdanau et al., 2014). Both the encoder and decoder have three stacked LSTM layers (Hochreiter and Schmidhuber, 1997), and the hidden size of the model is set to 500 units. We optimized using stochastic gradient descent with an initial learning rate of 1 and a batch size of 1024. The training lasted for 150,000 steps, with the first 8,000 steps being a warm-up for the learning rate. We started with Byte-pair encoding (BPE) (Sennrich et al., 2015) merge operations to find the best baseline model with the right number of merge operations. After testing, we found that 2500 merge operations gave us the best BLEU scores (Papineni et al., 2002) for most language pairs. We're sharing the best results here, and the full details of the merge operation results are in the supplementary material. We're calling this our NMT-BPE Baseline.",
        "formal_text": "Convert casual text to formal text: For our NMT task, we used the OpenNMT-Py toolkit (Klein et al., 2017) to run our experiments. We went with a Bidirectional"
    },
    {
        "casual_text": "Okay, so based on the semantic classification of adverbial verbs from the previous section, this part is going to dive deeper into some stats about that classification. Picture 1 shows the percentage of adverbial verbs in each major semantic category. From Picture 2, we can see that the category representing \"ways\" has the highest percentage, hitting almost 60%. Next up is the \"state\" category with about 30%. All the other categories are under 20% individually, but together they make up over 40% of the total, carrying a lot of the semantic weight. Since the \"way\" category is such a big chunk at nearly 60%, it's broken down further into three smaller groups based on the specific meaning of the verbs and how they function in adverbial constructions: 1. **Speaking ways**: How someone talks. 2. **Collaborative ways**: Ways of doing things that involve more than one person. 3. **General ways**: All other ways that don't fit into the first two groups. The examples, sentences, and their counts along with the percentages for these three subcategories are shown in Table 3.",
        "formal_text": "Convert casual text to formal text: Okay, so based on the semantic classification of adverbial verbs from the previous section, this part is going to dive deeper into some stats about that classification. Picture 1 shows"
    },
    {
        "casual_text": "Second, DIV-FACTORIZED is the best way to handle this issue. Using segment-level constraints (DIV-TAGGED) helps models bounce back from the problems caused by divergences (DIV-AGNOSTIC), but it’s not always reliable. On the other hand, breaking things down at the token level (DIV-FACTORIZED) helps NMT systems recover from divergences in different setups and get translation quality that’s almost as good as the EQUIV-ALENTS model. This approach effectively reduces the negative effects of noisy training data from divergent samples. Third, when translating data that’s outside the usual domain, DIV-FACTORIZED actually does better than the EQUIV-ALENTS model, as shown in Table 4. The DIV-AGNOSTIC models perform about the same as EQUIV-ALENTS, but when you factorize the divergences, you get a boost of around +1 BLEU score in both directions. So, dealing with divergences is super important for NMT to make the most of the extra out-of-domain data provided by divergent samples.",
        "formal_text": "Convert casual text to formal text: Second, DIV-FACTORIZED is the best way to handle this issue. Using segment-level constraints (DIV-TAGGED) helps models bounce back from the problems caused"
    },
    {
        "casual_text": "In this case, Lake and Baroni (2018) came up with a straightforward experiment to check how well machine translation handles compositionality. Chen et al. (2020) and Li et al. (2019b) also looked into this. They created a new word called \"dax\" and used a simple pattern of sentence pairs in their training data, like \"I am daxy\" and \"je suis daxiste\". But for the test set, they used different patterns. The problem is, their test set only had 8 sentences, which isn’t much. Raunak et al. (2019) noticed a dip in performance when dealing with concatenated source sentences. Fadaee and Monz (2020b) played around with the source sentences by removing adverbs, swapping numbers, adding words that keep the syntax intact (like \"very\"), and changing the gender, and found some weird translation issues. Unlike these studies, we’re using a compound translation error rate to measure how well NMT handles compositionality.",
        "formal_text": "Convert casual text to formal text: In this case, Lake and Baroni (2018) came up with a straightforward experiment to check how well machine translation handles compositionality. Chen et al. (2020) and Li et"
    },
    {
        "casual_text": "Here’s everything from EFCAMDAT for C1 and C2 combined, focusing on scores above 90%, after removing the ones used to calculate divergence scores. We split the whole dataset into eleven smaller groups: 0-9, 10-19, and so on, up to 90-99, with 100 as its own separate group. This was done for each of the three levels: beginner, intermediate, and advanced. EFCAMDAT has a lot of documents with high scores, but not as many with lower ones. For some of the lower score ranges, there weren’t 70 documents available, so we just used whatever was there. None of these documents were part of the production corpus used to create the language profile.",
        "formal_text": "Convert casual text to formal text: Here’s everything from EFCAMDAT for C1 and C2 combined, focusing on scores above 90%, after removing the ones used to calculate divergence scores. We split the"
    },
    {
        "casual_text": "If you're looking at a fully automatic system that spots triggers and then figures out their details, the standard rates to go by are the ones from the w/NONE w/DP setup.",
        "formal_text": "Convert casual text to formal text: If you're looking at a fully automatic system that spots triggers and then figures out their details, the standard rates to go by are the ones from the w/NONE w/"
    },
    {
        "casual_text": "We aim to make the connection between sentences and the query description more consistent by treating it like a prior distribution for those sentences. The usual way to measure how related sentences are is by using cosine similarity. But instead, we’re going with a method that’s kind of like what Algorithm 2 uses: The MultiRank Algorithm with Prior Input. This involves working with two tensors and two initial probability distributions, x0 and y0.",
        "formal_text": "Convert casual text to formal text: We aim to make the connection between sentences and the query description more consistent by treating it like a prior distribution for those sentences. The usual way to measure how related sentences are is by using cosine"
    },
    {
        "casual_text": "The goal of writing captions for news images is to help readers get a clear idea of the key details in the picture—like who's there, when it happened, where it took place, what’s going on, why it matters, and how it’s happening—based on the article it’s tied to. We’re thinking of using the concept of these components to create better captions, but first, we need to figure out how to automatically identify these components in the actual captions we already have for training.",
        "formal_text": "Convert casual text to formal text: The goal of writing captions for news images is to help readers get a clear idea of the key details in the picture—like who's there, when it happened, where it took place,"
    },
    {
        "casual_text": "Okay, so let's break this down in a simpler way. We have some data here: - For b = 5, the numbers are 39.1 and 60.6. - For p = .1, it's 39.2 and 61.5. - For p = .5, it's 39.0 and 61.9. - For p = .75, it's 37.5 and 62.1. - For p = .95, it's 33.4 and 63.8. Now, the thing is, when we generate questions that aren't quite right, it seems to help more than it hurts. In Table 1, the improvements in QA (Question Answering) from having more diverse QG (Question Generation) are usually bigger than any drop in performance, which might be because the questions aren't as accurate. To see if this still works in a tougher situation, we tried using four QG models that were trained on SQuAD (a dataset) and applied them to NewsQA, which is a dataset of CNN news articles (Trischler et al., 2017). NewsQA is harder because it's not the same kind of data. We didn't do any extra tuning for this test, just straight up used the models. In Table 2, we looked at the results for the part of NewsQA where answers are possible. There are 76,000 training samples (we used these to make prompts for QG) and 4,000 test samples (used to check how well QA works). The scores are lower than in SQuAD, but the way BEAM and NS perform compared to each other is pretty much the same. This is true both when we look at how well they predict QA performance (using ROUGE-4) and when we look at the actual QA performance (using F1 score).",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in a simpler way. We have some data here: - For b = 5, the numbers are 39.1 and 60.6."
    },
    {
        "casual_text": "In this paper, we’re tackling part of the issue with user-initiated contributions by looking at how people handle similar situations in everyday conversations. Most of the current approaches in computing focus on formal ideas of relevance: a system tries to make sense of a user’s input by linking it to shared goals between the system and the user. If a connection is found, even something that seems off-topic, like a clarification, can be worked into the conversation. Our approach shows how we can narrow down the search for relevant connections by using the idea of \"recipient design\" from conversation analysis (Sacks et al., 1974, p. 727). This means treating user statements as direct instructions on how to fit them into the ongoing conversation. This method fits well with current discourse semantics, where each contribution to the conversation helps to create tighter structural and semantic rules for interpreting what’s being said (Webber et al., 1999; Asher and Lascarides, 2003). We’re taking this a step further by adding rules and conditions for handling clarification subdialogues.",
        "formal_text": "Convert casual text to formal text: In this paper, we’re tackling part of the issue with user-initiated contributions by looking at how people handle similar situations in everyday conversations. Most of the current approaches in computing focus"
    },
    {
        "casual_text": "The right side of Table 3 has the link prediction results for NELL23K and WD-singer. Just like in the left part of the table, we see something similar happening. Our model does better than older multi-hop reasoning models, which shows that our model can work well with lots of other knowledge graphs too.",
        "formal_text": "Convert casual text to formal text: The right side of Table 3 has the link prediction results for NELL23K and WD-singer. Just like in the left part of the table, we see something similar happening. Our model"
    },
    {
        "casual_text": "So, the model has a pretty high Self-BLEU score, which means it’s more likely to give the same response to different input reviews compared to models trained on more data. On the other hand, it does better in other areas like chrF-tgt, chrF-src, DIST-1, and Uniq., and the responses are also longer.",
        "formal_text": "Convert casual text to formal text: So, the model has a pretty high Self-BLEU score, which means it’s more likely to give the same response to different input reviews compared to models trained on more data. On"
    },
    {
        "casual_text": "The NEL performance on the In Media Res corpus is way worse compared to traditional datasets. This wasn't a surprise since adding creative works to the mix introduces a lot of errors. As expected, the tools struggled to tell the difference between a character and the franchise with the same name, and they didn't perform well on the YouTube influencer part of the corpus either. The influencer section is particularly tricky because some of the stuff mentioned there—like YouTube channels that got shut down or early work from famous influencers—are NIL, meaning they’re not in Wikipedia or related knowledge graphs like DBpedia and Wikidata.",
        "formal_text": "Convert casual text to formal text: The NEL performance on the In Media Res corpus is way worse compared to traditional datasets. This wasn't a surprise since adding creative works to the mix introduces a lot of"
    },
    {
        "casual_text": "This can result in either losing key information (undersampling) or messing up the relationship between samples (oversampling), which ends up hurting how well the model works (Wu et al., 2017).",
        "formal_text": "Convert casual text to formal text: This can result in either losing key information (undersampling) or messing up the relationship between samples (oversampling), which ends up hurting how well the model works (Wu"
    },
    {
        "casual_text": "• LSTM: A basic LSTM model uses word embeddings and combines the question and answer text into a single sequence.",
        "formal_text": "Convert casual text to formal text: • LSTM: A basic LSTM model uses word embeddings and combines the question and answer text into a single sequence."
    },
    {
        "casual_text": "It seems like you've pasted a series of symbols and characters that don't form coherent text. Could you clarify or provide more context so I can assist you better? If you're trying to convert formal text into informal language, please provide the actual text you'd like me to rewrite in a casual tone.",
        "formal_text": "Convert casual text to formal text: It seems like you've pasted a series of symbols and characters that don't form coherent text. Could you clarify or provide more context so I can assist you better? If you're"
    },
    {
        "casual_text": "Some earlier studies suggested ways to decode using graph structures for preordering (like Niehues and Kolss, 2009, and Herrmann et al., 2013a, 2013b). But these approaches are really tied to a specific graph structure that's built into the method itself. Another study looked at multi-source translation using a confusion network made from multiple source sentences (Schroeder et al., 2009; Jiang et al., 2011). The problem is, this confusion network is too limited to show the different ways preordering can happen, and it doesn't let you take advantage of alternative reorderings in the source sentences.",
        "formal_text": "Convert casual text to formal text: Some earlier studies suggested ways to decode using graph structures for preordering (like Niehues and Kolss, 2009, and Herrmann et al., 2013a,"
    },
    {
        "casual_text": "We tried out two different ways to pick the best K (number of clusters) for both K-means and GMM, where K is between 1 and 10. The first method is a tweak of what Giulianelli et al. (2020) did. They used the silhouette score (Rousseeuw, 1987) to find the best K after trying different starting points. But they didn’t consider the case where there’s just one cluster, so we added that in. If the best silhouette score is below 0.1, we just set K to 1. For K-means, we also played around with an automatic elbow method to find the best K based on the sum of squared distances to the cluster centers. This sum always goes down as K increases, so we used the elbow method to find the right balance. We still picked the clustering with the highest silhouette score after trying a bunch of random starting points. For GMM, we also tried using the Bayesian Information Criterion (Schwarz, 1978) to pick the best model.",
        "formal_text": "Convert casual text to formal text: We tried out two different ways to pick the best K (number of clusters) for both K-means and GMM, where K is between 1 and 10. The first method is a"
    },
    {
        "casual_text": "We talked about Lucy (Rich et al. 1987), and our main points are in Section 4. There, we look at how these design choices affected three parts of the Lucy system. The conclusion tries to take what we learned and apply it more broadly. Basically, our goal here is to check out this general design approach by showing how it worked in a specific project.",
        "formal_text": "Convert casual text to formal text: We talked about Lucy (Rich et al. 1987), and our main points are in Section 4. There, we look at how these design choices affected three parts of the Lucy system. The conclusion"
    },
    {
        "casual_text": "• If two arguments don’t have any words in common or similar meanings, it’s tough to pair them up. In this case, our ISRG method, which relies on matching words, just doesn’t work. Plus, even a pre-trained model struggles to connect these kinds of argument pairs. • Sometimes, our model only picks out a couple of key sentences instead of the whole argument. Other times, it lumps together multiple arguments into one big chunk. This happens because we treat both AM and APE as sentence-level tasks. Since argument boundaries can be all over the place, the model often gets confused and mixes things up.",
        "formal_text": "Convert casual text to formal text: • If two arguments don’t have any words in common or similar meanings, it’s tough to pair them up. In this case, our ISRG method, which relies on matching words"
    },
    {
        "casual_text": "For beginners or regular users, there’s a straightforward interface where you can just type in a text query without needing to specify anything else. The results show a list of matches with small images (thumbnails) of the video clips related to your query, along with the text from those clips. Clicking on a match lets you watch the full video clip. If you want more details—like what an advanced user might see—you can click the \"advance information\" link. The way the results are sorted is kind of sneaky. It’s based on how much the words in the transcript or the visual labels connect to your query across different media. The more connected they are, the higher up they appear in the list. The goal of the COSMOROE approach is to make all this happen automatically—processing video and audio files to figure out these connections and make searching through digital video archives smarter.",
        "formal_text": "Convert casual text to formal text: For beginners or regular users, there’s a straightforward interface where you can just type in a text query without needing to specify anything else. The results show a list of matches with small"
    },
    {
        "casual_text": "When n = 1, there’s only one option: flipping the current state (so either (0, 1) or (1, 0)). This basically simplifies to straightforward collapsed sampling. For other cases, we propose a new state in two steps. First, with n positions and the current number of boundaries m, we pick the number of new boundaries m′ from a probability distribution called f n (m′; m). Then, we randomly place those m′ boundaries. The probability is evenly split among all the possible arrangements, which is n C m′. There’s one special case though: if m isn’t 0 or n, and m′ happens to be the same as m, we do a permutation to make sure the new state h′ isn’t the same as the old one, h. To wrap it up, the proposal distribution works like this:",
        "formal_text": "Convert casual text to formal text: When n = 1, there’s only one option: flipping the current state (so either (0, 1) or (1, 0)). This basically simplifies to straightforward collapsed sampling."
    },
    {
        "casual_text": "The new phrase ends up using element-wise multiplication ( ) with both t nt and m, which is kind of like how the LSTM \"forget\" gate works.",
        "formal_text": "Convert casual text to formal text: The new phrase ends up using element-wise multiplication ( ) with both t nt and m, which is kind of like the LSTM \"forget\" gate works."
    },
    {
        "casual_text": "Applying existing entity linking systems to this problem isn’t easy because they rely heavily on the detailed info that knowledge bases provide for entities. For instance, they often use things like detailed descriptions, various attributes, and other features (Francis-Landau et al., 2016; Gupta et al., 2017; Tan et al., 2017). Some systems even use extra signals, like the anchor texts in Wikipedia articles (Guo and Barbosa, 2014; Globerson et al., 2016; Ganea et al., 2016). But on social media, most of that stuff either doesn’t exist or isn’t very good.",
        "formal_text": "Convert casual text to formal text: Applying existing entity linking systems to this problem isn’t easy because they rely heavily on the detailed info that knowledge bases provide for entities. For instance, they often use things like detailed descriptions,"
    },
    {
        "casual_text": "[3] how each part gets added to the model—like, whether it's a default rule picking one option from a few choices, or a heuristic that nudges where you focus your attention—and the reasons behind using that method.",
        "formal_text": "Convert casual text to formal text: [3] how each part gets added to the model—like, whether it's a default rule picking one option from a few choices, or a heuristic that n"
    },
    {
        "casual_text": "CAN-biGRU: At the top of the biGRU, they added a self-attention mechanism. This helps figure out how important different pieces of context are by looking at past and future info when trying to understand the emotion in the current sentence. This model works the best, improving by 2.8% and 4.6% compared to the baseline on two different datasets. Check out Table 4 for the experimental results of BERT and CAN-biGRU(*).",
        "formal_text": "Convert casual text to formal text: CAN-biGRU: At the top of the biGRU, they added a self-attention mechanism. This helps figure out how important different pieces of context are by looking at past"
    },
    {
        "casual_text": "Second, the more clean examples we have, the better our model learns, even if it means it doesn’t handle super noisy data as well. We’ll work on improving this in the future. Compared to our-NoM, the new model does a bit better across all datasets when it comes to the micro-F1 score. Now, let’s talk about feature-level transfer learning. We noticed a 4.5% boost in the micro-F1 score for AFET on the BBN dataset after swapping out the old hand-crafted features for the ones our model generates. This shows that the features our model learns are pretty useful. But if we do the same thing with the OntoNotes dataset, the improvement is way smaller. That’s mostly because the OntoNotes data is different from the Wiki data. We’ll get into more details about this in the next section.",
        "formal_text": "Convert casual text to formal text: Second, the more clean examples we have, the better our model learns, even if it means it doesn’t handle super noisy data as well. We’ll work on improving this in the"
    },
    {
        "casual_text": "So, let's look at a couple of sentences, S and T, from a parallel text. S is the source sentence with I source words, and T is the target sentence with J target words.",
        "formal_text": "Convert casual text to formal text: So, let's look at a couple of sentences, S and T, from a parallel text. S is the source sentence with I source words, and T is the target sentence with J"
    },
    {
        "casual_text": "To get a grasp on how global inheritance works in DATR, we need to talk about the idea of global context. Let's say we're trying to figure out the value of \"Dog: (sing)\" in a DATR theory. At first, the global context is just the pair (Dog, sing). From the theory, we know that \"Dog: (sing)\" gets its value locally from \"Noun: (sing)\", which in turn gets its value globally from the path \"(root)\". To figure out what \"(root)\" means, we look at the global context to find the current global node, which is \"Dog\". Then, we evaluate \"Dog: (root)\" to get \"dog\", which is what we needed.",
        "formal_text": "Convert casual text to formal text: To get a grasp on how global inheritance works in DATR, we need to talk about the idea of global context. Let's say we're trying to figure out the value of \""
    },
    {
        "casual_text": "When we start working on CALl systems, we'll notice that there are a bunch of problems in natural language processing that either haven't been thought about enough or haven't been framed properly. But if we’re serious about really understanding how natural languages work—like how people use them and learn them—we need to consider the system they were made for: humans. This is kind of the trade-off we have to deal with if we want to create programs that are useful not just in research labs, but also in the real world.",
        "formal_text": "Convert casual text to formal text: When we start working on CALl systems, we'll notice that there are a bunch of problems in natural language processing that either haven't been thought about enough or haven't"
    },
    {
        "casual_text": "We train the CoRA using two main goals. First, we want it to predict the overall relation label for the whole bag, which is what relation extraction is all about. Second, as a supporting task, we guide the model to assign the right multi-granular relation embeddings to each sentence during the augmenting process. Check out Figure 2: This is our Collaborating Relation-augmented Attention (CoRA) Network. The right side shows the main structure, while the left side is a sentence embedding method for relation extraction. The relations and their hierarchies shown here are based on the NYT dataset, with M = 2 in Eq. 14.",
        "formal_text": "Convert casual text to formal text: We train the CoRA using two main goals. First, we want it to predict the overall relation label for the whole bag, which is what relation extraction is all about. Second, as a supporting"
    },
    {
        "casual_text": "We then tried out the method from (Koehn and Schroeder, 2007) by feeding the two phrase tables straight to the decoder and tuning a system that used both tables at the same time. Each table got its own set of weights during tuning, so this combined translation model had more parameters than a regular single-table system. Unlike (Nakov, 2008), we didn't bother trying to fix any overlaps between the phrase tables because it wasn't necessary with multiple decoding paths. Any phrase pairs that showed up in both models would be handled separately by the decoder anyway. But the overlap between the phrase tables was really small, so it didn't cause much of an issue. Table 4 shows the baseline results for the in-domain translation system and the general-domain system, tested on in-domain data. The table also shows that blending the translation models improved the overall BLEU score, which was expected. However, using multiple decoding paths without any explicit model merging at all actually gave even better results—2 BLEU points higher than the best individual model and 1.3 BLEU points better than the best interpolated model, which used  = 0.9. So, our conclusion is that it might be more effective to not try to adapt the translation models directly and instead let the decoder handle it.",
        "formal_text": "Convert casual text to formal text: We then tried out the method from (Koehn and Schroeder, 2007) by feeding the two phrase tables straight to the decoder and tuning a system that used both"
    },
    {
        "casual_text": "In this part, we're showing something cool about INLP classifiers when we're dealing with binary stuff. Basically, we're proving that if you take two classifiers, w_i and w_j, from different steps i and j, they're going to be orthogonal (check out Lemma A.1). This orthogonality thing has some neat consequences for the matrix P that INLP spits out. First, because of this orthogonality, the projection matrices we get from different INLP steps can be multiplied in any order, and you'll get the same result (Corollary A.1.1). Second, P is a legit projection matrix (Corollary A.1.2). And third, P projects stuff into a subspace that's the intersection of all the nullspaces from the INLP classifiers.",
        "formal_text": "Convert casual text to formal text: In this part, we're showing something cool about INLP classifiers when we're dealing with binary stuff. Basically, we're proving that if you take two classifier"
    },
    {
        "casual_text": "Sure, here's a more casual version: We figure there's a bunch of unlabeled data for the languages we've already seen. To make this situation feel real, we set up a way to split the data so that each labeled example is only seen once. Let's say we check out these languages one by one in a specific order.",
        "formal_text": "Convert casual text to formal text: Sure, here's a more casual version: We figure there's a bunch of unlabeled data for the languages we've already seen. To make this situation feel real,"
    },
    {
        "casual_text": "In SRL, there are datasets like G&C Chai (2010, 2012), SemEval-2010 (Ruppenhofer et al., 2009, 2010), and 80Days (Feizabadi and Padó, 2014). However, most of these use different ontologies like Nombank (G&C) and FrameNet (SemEval-2010 and 80Days), focus on different domains (like novels), and are limited in scope (e.g., G&C and 80Days only cover 10 types of predicates). This lack of annotations makes it tricky to train and transfer models for extracting implicit arguments in events. Recently, Ebner et al. (2020) introduced the Roles Across Multiple Sentences (RAMS) dataset, which deals with implicit arguments across multiple sentences and covers a wide variety of event and role types. They also developed a span-based argument linking model and got pretty good results. But their approach mostly works in a simplified scenario where they assume the gold argument spans are already available. We took their work a step further and tackled the more challenging full detection problem, where we predict argument spans from all possible candidates. This is harder, as shown in Figure 1. For example, both \"3000 dollars\" and \"1000 dollars\" could fit the money role in a purchase event, but the correct choice depends on the context.",
        "formal_text": "Convert casual text to formal text: In SRL, there are datasets like G&C Chai (2010, 2012), SemEval-2010 (Ruppenhofer et al., 2009, 2010), and 80"
    },
    {
        "casual_text": "Check out Table 9 for the results. Basically, the multi-embedding methods keep beating the single-embedding ones when it comes to training, validation, and testing perplexity. The difference isn't as big as with similar words, though. Also, the gap gets wider when the nouns are really different. We think that when the word embeddings for nouns are farther apart, the next word distribution tends to be more complex, and multiple embeddings do a better job of capturing that.",
        "formal_text": "Convert casual text to formal text: Check out Table 9 for the results. Basically, the multi-embedding methods keep beating the single-embedding ones when it comes to training, validation, and testing perplex"
    },
    {
        "casual_text": "On the essay datasets, Target Embedding (learning) works really well in the optimistic scenario, especially for most scores. The hybrid approach doesn’t really show much improvement, though. This might be because the dataset is pretty small, so it’s hard to tell if there’s any real difference. In the pessimistic scenario, Premise Target (ranking) seems to be a better fit. The lower scores for Essay Conclusions could be due to having fewer premises (check out Figure 6), which makes it harder to pick a good conclusion target from the available premise targets. Figure 7 shows three examples of premise targets from the datasets, along with the actual conclusion target and the ones our approaches came up with.",
        "formal_text": "Convert casual text to formal text: On the essay datasets, Target Embedding (learning) works really well in the optimistic scenario, especially for most scores. The hybrid approach doesn’t really show much improvement, though. This"
    },
    {
        "casual_text": "In this project, we take a closer look at some tricky German words, especially those that show up only once in the language (hapaxes). These words often have a special part called an affixoid or semi-affix. Think of words like Gesetzeshengst, which means someone who's super strict about rules (literally 'law stallion'), or Mentalitätsmonster, someone with extreme focus (literally 'mentality monster'). Affixoids are kind of in between regular affixes and stems. There are a few ways to spot an affixoid, and most people agree on at least three main ones (based on Schmidt, 1987, and others): (i) they're used a lot; (ii) their meaning gets kind of vague or less specific; and (iii) they have a connection to a regular word. The first two points are usually checked by comparing the affixoid to the original word. For example, Weingott means a god of wine, where Gott is just the regular word for god. But in Gitarrengott, which means a super talented guitar player, Gott is an affixoid because the whole word isn't about a god anymore. These kinds of words often have a strong, evaluative meaning, like being really good at something (Meibauer, 2013). The last point helps us tell affixoids apart from regular affixes, which can only be used as part of another word. For example, German -heit, which is like English -hood in words like falsehood, is an affix because there's no standalone form of it.",
        "formal_text": "Convert casual text to formal text: In this project, we take a closer look at some tricky German words, especially those that show up only once in the language (hapaxes). These words often have a special part called an"
    },
    {
        "casual_text": "In the past few years, neural methods (like those by Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al., 2017) have really upped the game for machine translation (MT). But even with all that progress, neural machine translation (NMT) systems still mess up sometimes, especially when dealing with less common language pairs. So, checking how good a translation is becomes pretty important, especially in stuff like computer-assisted translation (CAT) (Barrachina et al., 2009). By evaluating translation quality, you can save a lot of time for people who need to fix the translations later (Specia, 2011).",
        "formal_text": "Convert casual text to formal text: In the past few years, neural methods (like those by Bahdanau et al., 2015; Luong et al., 2015; Vaswani et al"
    },
    {
        "casual_text": "Third, unlike methods that rely on bunsetsu, ours can actually shorten a bunsetsu in the original sentence. See, bunsetsu-based methods are stuck with these unit constraints, so they have to treat each bunsetsu as a single unit. That means even if there are some unimportant words in a bunsetsu, those methods can't just drop them. But our method? We use something called Lagrangian relaxation to loosen those unit constraints. So, we can actually get rid of unimportant words in a bunsetsu, even though our approach is kind of inspired by bunsetsu-based methods.",
        "formal_text": "Convert casual text to formal text: Third, unlike methods that rely on bunsetsu, ours can actually shorten a bunsetsu in the original sentence. See, bunsetsu-based methods are stuck"
    },
    {
        "casual_text": "Alright, here's the deal with the costs: Each sentence starts at 18. If you make a second QA pair, you get an extra 3. After that, every extra QA pair you add gets you 4 more. If you do adjudication, you earn 10 per sentence. For Dev and Test, it usually costs around 50.3 per sentence, and on average, there are about 2.11 QA pairs per sentence. As for Train, the average cost per sentence is about 37.1, with around 1.72 QA pairs per sentence.",
        "formal_text": "Convert casual text to formal text: Alright, here's the deal with the costs: Each sentence starts at 18. If you make a second QA pair, you get an extra 3. After that, every extra"
    },
    {
        "casual_text": "1. If even one vote says \"Excluded,\" then the final class for that test instance is \"Excluded.\" This is what we call a 1-vote scheme.",
        "formal_text": "If even one vote says \"Excluded,\" then the final class for that test instance is \"Excluded.\" This is we call a 1-vote scheme. 2. If even one vote says \"Excluded,\" then the"
    },
    {
        "casual_text": "3) Community emotion burst detection: Based on the community's emotions, we've come up with an algorithm to spot when those emotions spike or \"burst.\" 4) Event extraction: This part of the system is all about pulling out key event words for each emotional spike. We look at the frequency of words used in the microblog posts during that burst period and count how often each word shows up. Then, we pick the top 5 most frequent words to summarize what's happening during that emotional burst. There are other methods out there for doing this too (like the ones by Ritter et al. in 2012 and Li et al. in 2010).",
        "formal_text": "Convert casual text to formal text: 3) Community emotion burst detection: Based on the community's emotions, we've come up with an algorithm to spot when those emotions spike or \"burst.\" 4) Event extraction"
    },
    {
        "casual_text": "InLäubli et al. (2018) used the Sign test instead of the Wilcoxon rank sum, and it turned out to have similar statistical power for the effect size they were looking at. On the other hand, Toral et al. (2018) took a different approach—they only had document context for the source document, not for the MT output.",
        "formal_text": "Convert casual text to formal text: InLäubli et al. (2018) used the Sign test instead of the Wilcoxon rank sum, and it turned out to have similar statistical power for the effect size they were looking"
    },
    {
        "casual_text": "These issues come up because LDA’s math doesn’t really focus on capturing how words relate to each other in terms of meaning. A cool next step could be tweaking the parts of the model that use probabilities, training it on a good dataset, and creating a vector-based model that’s tailored specifically for combining words into meaningful phrases. We’re also thinking about exploring more advanced models that consider the structure of sentences. Our experiments so far, where we mixed these models with a parser, show that blending syntax and semantics gives us a lot of potential for improvement.",
        "formal_text": "Convert casual text to formal text: These issues come up because LDA’s math doesn’t really focus on capturing how words relate to each other in terms of meaning. A cool next step could be tweaking the parts of the"
    },
    {
        "casual_text": "Basically, d is the scaling factor, and d represents the number of dimensions in the layer's states.",
        "formal_text": "Convert casual text to formal text: Basically d is the scaling factor, and d represents the number dimensions in the layer's states. Basically d is the scaling factor, and d represents the number dimensions"
    },
    {
        "casual_text": "Another thing worth looking into is how we can use the weights we found for blending source language models (LMs) to blend target LMs. It might seem like a good idea at first, but there's a catch: the best weights for the source LMs might not be the best for the target LMs. The likelihood (how well the model fits the data) could be higher with different weights on the target side compared to the source side. We could figure out a way to map the source weights to the target ones using a parallel development or training set.",
        "formal_text": "Convert casual text to formal text: Another thing worth looking into is how we can use the weights we found for blending source language models (LMs) to blend target LMs. It might seem like a good idea at"
    },
    {
        "casual_text": "Okay, so let’s break this down in simpler terms. On a Reichenbachian view, tenses are all about connecting three different types of time: the time when the event being talked about happens, the time when the speaker is talking (the speech time), and the reference time. Originally, these were thought of as specific points in time. But over time, people started using interval-based methods to explain how tenses, adverbs, and aspects work together in sentences (check out v. Eynde in 1987 or Bras/Sorillo in 1988) or to describe how events unfold in texts (like Hinrichs in 1986 or Partee in 1984). If you’re interested, Büuerle has some good critiques of the Reichenbachian approach from 1979. Then, inspired by how things work in texts, the Kamp/Rohrer approach (from 1983 and 1985) tweaked the Reichenbachian idea by splitting the reference time into three more specific parts: temporal perspective points and location times, all based on context.",
        "formal_text": "Convert casual text to formal text: Okay, so let’s break this down in simpler terms. On a Reichenbachian view, tenses are all about connecting three different types of time: the time when the event"
    },
    {
        "casual_text": "A lot of the tools and ways to measure how good a text summary is were made for stuff like news articles, where there's just one person talking. So, they might not be the best fit for conversations with multiple people speaking.",
        "formal_text": "Convert casual text to formal text: A lot of the tools and ways to measure how good a text summary is were made for stuff like news articles, where there's just one person talking. So, they might not the best fit"
    },
    {
        "casual_text": "So, to sum it up, PCRF-Seq2Seq works the best out of all the systems we tested for the spelling correction tasks, especially when we have a lot of training data. It’s about 6-7 percentage points better than the best encoder-decoder model for both the Twitter 31K and Text+Berg 72K datasets. But when we have less training data, PCRF-Seq2Seq and the encoder-decoder models are pretty much on the same level. For the G2P task, a similar pattern shows up. All the classic systems seem to perform about the same, with DirecTL+ and PCRF-Seq2Seq just slightly ahead of the others. Now, for lemmatization, things look a bit different. For Finnish verbs, attention-based encoder-decoder systems are the clear winners, outperforming everything else. For German, neural models also come out on top, but only by a small margin compared to PCRF-Seq2Seq.",
        "formal_text": "Convert casual text to formal text: So, to sum it up, PCRF-Seq2Seq works the best out of all the systems we tested for the spelling correction tasks, especially when we have a lot of"
    },
    {
        "casual_text": "The domain ontology gives a clear explanation of how legal IF representations work. This explanation is based on the IF specification language, which everyone in the NESPOLE group agreed on. The domain ontology breaks down legal ideas, speech actions, legal arguments, legal values, and how they all connect, but in a more abstract way. In the IF, domain actions (like speech acts and concepts) are basically simple categories used to group each SDU according to what the speaker is trying to say. Figure 2 shows some examples of how this ontology stuff looks in Prolog.",
        "formal_text": "Convert casual text to formal text: The domain ontology gives a clear explanation of how legal IF representations work. This explanation is based on the IF specification language, which everyone in the NESPOLE group agreed"
    },
    {
        "casual_text": "In terms of how much work it takes to get this done, labeling counterfactuals is basically the same as labeling regular examples. So, no extra training for annotators or separate systems are needed. On the other hand, Kaushik et al. (2020) had to set up two different crowdsourcing tasks to create and label counterfactuals. Plus, it’s way less effort for annotators to just evaluate examples instead of making them. Kaushik et al. (2020) say it took about 2 minutes per counterfactual for NLI tasks before checking the quality, but for us, it was only 10 seconds per counterfactual on average. Even after we cleaned up the data (kicked out bad annotators and removed unclear counterfactuals), our time per NLI counterfactual was around 36 seconds (this is what we used in Table 5).",
        "formal_text": "Convert casual text to formal text: In terms of how much work it takes to get this done, labeling counterfactuals is basically the same as labeling regular examples. So, no extra training for annotators or separate systems"
    },
    {
        "casual_text": "We tried out a few different versions of our model as a starting point: BERT-SeqWD, which only looks at coreference scores based on entity or event representations without any cross-document linking, and BERT-SeqXdoc, which calculates coreference scores across documents but doesn’t use candidate composition. So, BERT-SeqXdoc just compares the query mention to all antecedent mentions from previous documents, instead of comparing it to clusters created with candidate composition. We also tested our model, SeqXdoc+IC, with and without adaptive pre-training for both event and entity coreference. For entity coreference, we compared our model against a few others:",
        "formal_text": "Convert casual text to formal text: We tried out a few different versions of our model as a starting point: BERT-SeqWD, which only looks at coreference scores based on entity or event representations without"
    },
    {
        "casual_text": "Okay, so basically, we're looking at two things here: first, whether the current word is connected to a verb in some way, even if it's not directly attached. Second, we're checking if this word is the beginning of a group of words that are linked to a verb.",
        "formal_text": "Convert casual text to formal text: Okay, so basically, we're looking at two things here: first, whether the current word is connected to a verb in some way, even if it's not directly attached. Second"
    },
    {
        "casual_text": "With the crazy amount of research papers being published in every field, it's getting super hard for researchers to stay on top of everything happening in their area. That's where scientific summarization comes in handy. It gives readers a quick, clear summary of what a paper is about and what it contributes. Scientific summarization is a bit different from summarizing regular articles for a few reasons (shoutout to Teufel and Moens, 2002). First off, scientific papers are usually way longer than your average news article. Second, the goal is to give a technical summary that highlights the key findings, contributions, or how the paper impacts the field. Lastly, scientific papers have a pretty standard structure. They usually start by introducing the problem, then talk about the hypotheses, methods, experiments, findings, and finally, the results and what it all means.",
        "formal_text": "Convert casual text to formal text: With the crazy amount of research papers being published in every field, it's getting super hard for researchers to stay on top of everything happening in their area. That's where scientific summarization comes"
    },
    {
        "casual_text": "The way the system handles syntax is kinda different. It uses two main things: phrasal and clausal templates, and production rules. The templates basically set the structure for how the sentences can look, like the branches of a tree. They don't care what words or phrases go into each spot in the structure. On the other hand, the production rules make sure the words fit into the right categories and are in the correct order.",
        "formal_text": "Convert casual text to formal text: The way the system handles syntax is kinda different. It uses two main things: phrasal and clausal templates, and production rules. The templates basically set the structure for how the"
    },
    {
        "casual_text": "Out-of-distribution generalization. No one has really looked into or tested how well metaphor detection works when the training and testing data come from different sources. This \"out-of-distribution\" thing means that the stuff we use to train and the stuff we use to test aren't from the same place (Duchi and Namkoong, 2018; Hendrycks et al., 2021; Hendrycks et al., 2020). In our case, it could be that the training data is in one language and the testing data is in another, or they could be from completely different areas or datasets. This makes things tricky when we're trying to see how well the system can handle new, unseen data, especially when it comes to figuring out metaphors.",
        "formal_text": "Convert casual text to formal text: Out-of-distribution generalization. No one has really looked into or tested how well metaphor detection works when the training and testing data come from different sources. This \"out-of-"
    },
    {
        "casual_text": "Code summarization is all about taking a piece of code and turning it into a simple sentence or phrase that people can understand. The two main things people focus on in this area are generating comments for the code and coming up with names for methods (check out section 5.1 for more on that). Table 1 already has a list of previous work done on these tasks, but let me give you a quick rundown of how they’ve evolved.",
        "formal_text": "Convert casual text to formal text: Code summarization is all about taking a piece of code and turning it into a simple sentence or phrase that people can understand. The two main things people focus on in this area are generating"
    },
    {
        "casual_text": "We looked at the words and phrases (lexical features) for each strategy by using a method called the log odds ratio with an informative Dirichlet prior. This method helps us compare unigrams and bigrams (single words and pairs of words) across different strategies to see which ones stand out. In Figure 3, we show the top 5 phrases for each strategy. Some strategies are strongly linked to specific phrases—like \"are you\" for questions or \"me\" for self-disclosure—based on their z-scores, which are way above 3. Next, we checked how these strategies are spread out during different parts of the conversation. For a conversation with L total turns, the k-th turn (where 1  k  L) is from the supporter and uses strategy st. We tracked where this turn happened in the conversation by looking at its position, k/L. To make it easier, we divided the conversation into six equal parts: [0, 1] split into [i/5, (i + 1)/5) for i from 0 to 4. Then, for all the conversations in ESConv, we calculated how often each strategy showed up in each of these six parts. We took these proportions and plotted them at six points: i/5 for i = 0 to 5. We connected the dots to see the trend, and that’s how we got Figure 4.",
        "formal_text": "Convert casual text to formal text: We looked at the words and phrases (lexical features) for each strategy by using a method called the log odds ratio with an informative Dirichlet prior. This method helps us compare unigram"
    },
    {
        "casual_text": "EmotionPush 1: This dataset includes conversations pulled from Facebook Messenger chats, with all the private stuff taken out. The emotion categories are the same as in the Friends dataset.",
        "formal_text": "Convert casual text to formal text: EmotionPush 1: This dataset includes conversations pulled from Facebook Messenger chats, with all the private stuff taken out. The emotion categories are the same as in the Friends dataset."
    },
    {
        "casual_text": "We used GraphChi, a cool library for processing graphs on a single machine (shoutout to Kyrola et al. in 2012), to handle both the feature selection and feature computation parts of PRA. For the logistic regression, we went with MAL-LET's setup, including both L1 and L2 regularization (props to McCallum in 2002). To figure out what counts as negative evidence, we made a closed world assumption, meaning any (source, target) pair we found during feature computation that wasn’t labeled as positive was treated as negative. We tweaked the parameters for our methods by doing a rough, manual grid search with cross-validation on the training data we’ll talk about later. The parameters we messed with included the L1 and L2 regularization settings, how many random walks we did for feature selection and computation in PRA, and some spikiness and restart parameters for vector space walks. Turns out, the results didn’t change much even if we fiddled with these parameters a bit.",
        "formal_text": "Convert casual text to formal text: We used GraphChi, a cool library for processing graphs on a single machine (shoutout to Kyrola et al. in 2012), to handle both the"
    },
    {
        "casual_text": "This might result in a mapping that’s kind of stuck in the middle of the targets but not close to any of them. To fix this, we suggest doing two things: (1) adding a hidden variable called \"focus\" that breaks down the process into two steps—selecting and generating (check out Section 3.1), and (2) training these two parts separately (see Section 3.2).",
        "formal_text": "Convert casual text to formal text: This might result in a mapping that’s kind of stuck in the middle of the targets but not close to any of them. To fix this, we suggest doing two things: (1) adding a"
    },
    {
        "casual_text": "So, T_w is the number of threads that have at least one occurrence of w, and T(f_w) is what we expect the number of threads to be if we use a Poisson model. And yeah, T(f_w) can be figured out the same way as before.",
        "formal_text": "Convert casual text to formal text: So, T_w is the number of threads that have at least one occurrence of w, and T(f_w) is what we expect the number of threads to be"
    },
    {
        "casual_text": "Max-margin training. Inspired by Collobert et al. (2011), we’re suggesting a pairwise ranking loss that makes sure positive and negative samples are as far apart as possible. Basically,",
        "formal_text": "Inspired by Collobert et al. (2011), we’re suggesting a pairwise ranking loss that makes sure positive and negative samples are as far apart as possible. Basically, we want to know how to convert casual text to"
    },
    {
        "casual_text": "We use dialog acts (DA) and overt displays of power (ODP) tags to figure out how interactions work in the content of messages. We get these DA and ODP tags using automatic tools that were trained on manual annotations. The DA tagger (from Omuya et al., 2013) is 92% accurate. The ODP tagger (from Prabhakaran et al., 2012) is 96% accurate and has an F-measure of 54%. The DA tagger labels each sentence as one of four types: Request Action, Request Information, Inform, or Conventional. The ODP tagger looks for sentences (usually requests) that add extra conditions for the person being addressed, beyond what the dialog act already says. For example, \"Please come to my office right now\" is tagged as an ODP, but \"It would be great if you could come to my office now\" isn’t, even though both are asking for the same thing. If you want more info on ODP, check out Prabhakaran et al. (2012). We use five features: ReqAction, ReqInform, Inform, Conventional, and ODPCount to count how many sentences in messages from a person have each of these labels. We also use a feature to track the percentage of requests from a person that didn’t get a reply, called the dangling request percentage (DanglingReq%), across all their messages.",
        "formal_text": "Convert casual text to formal text: We use dialog acts (DA) and overt displays of power (ODP) tags to figure out how interactions work in the content of messages. We get these DA and ODP tags using automatic"
    },
    {
        "casual_text": "Before we get into the grammar stuff, let's talk about why there's no polynomial that can work in this situation. The function p has to be a polynomial with these two variables, but no such polynomial exists. Here's why: if it did, it would need to match certain values depending on the range. For example, it would have to equal #(N) when p(N) is between 0 and 9, and then switch to /(D)  10 + /(N) when /(N) is between 10 and 99, and so on for higher numbers. But if the polynomial's degree is less than l, for some n, it would have to be exactly equal to /(D)  10n + /(N) because it would share all the same values in the range l@ to 10n - 1. That means it couldn't give the right values for the other intervals. So, no polynomial can do the job.",
        "formal_text": "Convert casual text to formal text: Before we get into the grammar stuff, let's talk about why there's no polynomial that can work in this situation. The function p has to be a polynomi"
    },
    {
        "casual_text": "When you're evaluating whether an abstract should be classified as \"Included\" (relevant) or \"Excluded\" (not relevant), there are two main goals to keep in mind: 1. **Objective 1** - Make sure you cover everything for the systematic review (include as many relevant documents as possible). 2. **Objective 2** - Keep things efficient for the reviewers (exclude as many irrelevant documents as you can to save time).",
        "formal_text": "Convert casual text to formal text: When you're evaluating whether an abstract should be classified as \"Included\" (relevant) or \"Excluded\" (not relevant), there are two main goals to keep in mind:"
    },
    {
        "casual_text": "Lastly, we take a look at how this affects search and scoring, and we think there's a lot more to explore about the difference between how things are trained and how they're tested.",
        "formal_text": "Convert casual text to formal text: Lastly, we take a look at how this affects search and scoring, and we think there's a lot more to explore about the difference between how things are trained and they're"
    },
    {
        "casual_text": "Okay, let me break this down in a simpler way: 2. s = c (G, i) G =  ([i, i + 1], i, G): s Init ([i, k], r,  ): s s = c (, k) + c 0 IGNORE    k ([i, k + 1], r,  ): s + s Skip-R ([i, k], r,  ): s s = c (, i  1) + c 0 IGNORE    i  1 Skip-L ([i  1, k], r,  ): s + s ([i, j], r1, 1): s1 ([j, k], r2, 2): s2  = (1, 2) defined s = c r1   r2 Arc-R [ ] ([i, k], r1,  ): s1 + s2 + s ([i, j], r1, 1): s1 ([j, k], r2, 2): s2  = (2, 1) defined s = c r2   r1 Arc-L [ ] ([i, k], r2,  ): s1 + s2 + s ([1, n + 1], r, [ ]): s = c 0 ROOT    r ([0, n + 1], r, [ ]): s + s Alright, let’s make this more conversational: 2. s is calculated based on G and i. G is defined as  over the range [i, i + 1], with i and G. Init sets s over [i, k], r, and . Then,",
        "formal_text": "Convert casual text to formal text: Okay, let me break this down in a simpler way: 2. s = c (G, i) G =  ([i, i + 1], i,"
    },
    {
        "casual_text": "We ran some experiments using the Gigaword headline generation dataset (Graff et al., 2003) and the DUC2004 dataset (Over and Yen, 2004). The results show that our NAUS model is top-notch for unsupervised summarization. It even beats its \"teacher\" (the search approach), which proves that NAUS can really clean up the noise in the search process. When it comes to speed, NAUS with truncating is a whopping 1000 times faster than the search approach. Even when we use dynamic programming to control the length, NAUS is still 100 times faster than search and way faster than autoregressive models. Plus, NAUS can generate summaries of different lengths compared to what it was trained on, which is pretty cool.",
        "formal_text": "Convert casual text to formal text: We ran some experiments using the Gigaword headline generation dataset (Graff et al., 2003) and the DUC2004 dataset (Over and Yen, 2004). The"
    },
    {
        "casual_text": "In this case, we’re trying to figure out who made the fake text that was created using a tweaked version of a pre-trained language model (LM). The person doing the figuring-out doesn’t know anything about the fine-tuning process. For this experiment, we’re only using GPT2 for generating the fine-tuned text, and the reasons for that are explained in Section 3.",
        "formal_text": "Convert casual text to formal text: In this case, we’re trying to figure out who made the fake text that was created using a tweaked version of a pre-trained language model (LM). The person doing"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Basically, you've got sentence-level stuff in both the review and the rebuttal. From there, you need to pull out a bunch of interactive argument pairs, like P = p1, p2, ... .",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Basically, you've got sentence-level stuff in both the review and the rebuttal. From there, you need to pull out"
    },
    {
        "casual_text": "This part gives a general overview of how the model is designed (for the more technical details, check out Appendix A). The CDRNN setup is laid out in Figure 1. The main thing we're trying to figure out is the deep neural IRF, g() (at the top), which helps us understand how an earlier event affects the prediction of a later response based on how far apart they are in time, . Basically, the IRF takes  and turns it into a matrix that calculates a weighted sum of the input vector x, along with a bias to account for the general timing of the stimulus (like its rate). This multiplication then tells us how much the stimulus event contributes to the prediction, like the mean and variance in a Gaussian prediction. By making the IRF a function of , the model works in continuous time. To handle the non-linear stuff from the stimulus features, the IRF is also influenced by a hidden state h. This h allows for non-linear effects from the stimulus sequence to shape the IRF. To get h, the model takes the predictors x and combines them with their timestamps t, then feeds them in as input.",
        "formal_text": "Convert casual text to formal text: This part gives a general overview of how the model is designed (for the more technical details, check out Appendix A). The CDRNN setup is laid out in Figure 1. The main thing"
    },
    {
        "casual_text": "BERT-COREF is a neural model that handles span extraction and coreference relation classification all in one go, end-to-end. It starts by listing out possible mention spans and then checks pairs of these spans to see how likely they are to refer to the same entity. The model learns to do both span extraction and coreference resolution at the same time by using backpropagation during training. This approach has shown some pretty cool results on the English coreference resolution task using the OntoNotes dataset (Pradhan et al., 2012).",
        "formal_text": "Convert casual text to formal text: BERT-COREF is a neural model that handles span extraction and coreference relation classification all in one go, end-to-end. It starts by listing out possible mention spans and"
    },
    {
        "casual_text": "ModrnTalk, which is short for Modern Talking - RoBERTa-Base (created by Reimer et al., 2021), is a version of the RoBERTa-Base model that's been tweaked using a mix of arguments and key points (kps). During training, the model starts with a warm-up phase using just 6% of the data.",
        "formal_text": "Convert casual text to formal text: ModrnTalk, which is short for Modern Talking - RoBERTa-Base (created by Reimer et al., 2021), is a version"
    },
    {
        "casual_text": "Alright, so we've got W ao, W ad, and W ae i, which are all learnable parameters. Here, W ao and W ad are in R mb, and W ae i is in R md. The variable m represents the hidden size of the attention mechanism, b is the hidden size of BERT, and d is the size of the word embedding. As you can see in Figure 1, the attention process works like this:",
        "formal_text": "Convert casual text to formal text: Alright, so we've got W ao, W ad, and W ae i, which are all learnable parameters. Here, W ao and W"
    },
    {
        "casual_text": "Alright, let's dive into what we found when we used one-sided discriminative labeling with some hints. We grabbed the real syntactic labels from the Penn Treebank—about 9000 sentences worth. Then, we plugged them into the LaSO sequence labeling software (shoutout to Daumé III and Marcu, 2005) with its default features. Our main aim here was to look at two key things:",
        "formal_text": "Convert casual text to formal text: Alright, let's dive into what we found when we used one-sided discriminative labeling with some hints. We grabbed the real syntactic labels from the Penn Treebank"
    },
    {
        "casual_text": "- **Word representation for candidate triggers and arguments:** We take the output from BERT (Devlin et al., 2018) as our word representation. If a word is made up of multiple tokens, we combine the vectors of those tokens by averaging them. We’ll call the word representation for a word w_i as E_i. - **Argument type embedding:** We turn the argument type T_a_i into a real-value vector by looking it up in a randomly initialized position embedding table. We’ll refer to this embedding as E_a_i. - **Event type embedding:** Just like with the argument type, we represent the event type T_e_i with a real-valued vector E_e_i. - **Dependency syntax edge embedding:** We convert an edge type T_d_i from the dependency parse into a real-valued vector E_d_i by looking it up in a trainable embedding table.",
        "formal_text": "Convert casual text to formal text: - **Word representation for candidate triggers and arguments:** We take the output from BERT (Devlin et al., 2018) as our word representation. If"
    },
    {
        "casual_text": "In Table 5, we check out what happens if we take out our similarity-based retrieval module. Turns out, the F-1 scores take a big hit. We also noticed a drop in scores when we randomly pick an event from the memory store. Kinda cool that the model does a bit better with random memory than with no retrieved or demonstration sequences at all. This lines up with what other NLP studies have found about how demonstrations can boost performance, especially with pre-trained language models in few-shot learning. In Figure 4, we look at how performance changes as documents get longer and have more events. We see that as the document length increases, it gets harder for both the baseline and the similarity-based retrieval setup.",
        "formal_text": "Convert casual text to formal text: In Table 5, we check out what happens if we take out our similarity-based retrieval module. Turns out, the F-1 scores take a big hit. We also noticed a"
    },
    {
        "casual_text": "This suggests that the student model finds a broader local minimum, which helps it generalize better.",
        "formal_text": "Convert casual text to formal text: This suggests that the student model finds a broader local minimum, which helps it generalize better. Convert casual text to formal text: This suggests that the student model finds a broader local"
    },
    {
        "casual_text": "The commit hash is bc5f8d15c6ce4bc678ba992860bfd4be6719cee8. Here's Figure 3, showing how the decoding time and average model scores change with different stack setups.",
        "formal_text": "Convert casual text to formal text: The commit hash is bc5f8d15c6ce4bc678ba992860bfd4be6719cee8. Here's Figure"
    },
    {
        "casual_text": "Apart from A co, we also figure out how related two entities are based on their categories. If entity e i and e j have shared categories, it’s likely they’re of the same type. For instance, in Figure 2, both Houston Rockets and Shanghai Sharks are listed under \"Basketball team clubs,\" so they’re both considered to be of the type BasketballTeam. We create A cat and use the Jaccard similarity coefficient to calculate each value in A cat [i, j].",
        "formal_text": "Convert casual text to formal text: Apart from A co, we also figure out how related two entities are based on their categories. If entity e i and e j have shared categories, it’s likely they’"
    },
    {
        "casual_text": "Alright, let’s break this down in simpler terms. We’re using a method called the memoizing recursive descent algorithm, which was introduced by Leermakers back in 1993. Since the grammar we’re dealing with isn’t left-binding, we can figure out all the terminal words (basically, the actual words in the sentence) as we go through the sentence and the grammar rules from left to right. Now, because the grammar isn’t combinatorial, the terminal words that get plugged into the spots for nonterminals (kind of like placeholders) are always just parts of the input sentence. So, we can represent these parts as a pair of numbers, like a start and end point in the sentence. The recursive descent algorithm works by computing something called set-valued recognition functions, which basically means it keeps breaking things down recursively until it figures everything out.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in simpler terms. We’re using a method called the memoizing recursive descent algorithm, which was introduced by Leermakers back in 1993"
    },
    {
        "casual_text": "The VQA model by Johnson and team (2017) has two main parts: a program generator (G) and an execution engine (E). The program generator takes a question (q) and predicts a program (p) to answer it. The execution engine then uses this program to combine different modules and runs the whole thing on an image to get the answer. Johnson and colleagues (2017) trained the model using a semi-supervised method. They showed that the program generator can work well even when trained on just a small number of possible programs (like 4% or less). To test how well the execution engine (E) performs with limited data, we did some experiments using regular supervised training but with smaller training sets. In all these experiments, we fed E the correct program and image pairs. Figure 1 shows the best accuracy we got for each experiment on CLEVR's validation set, where the execution engine was trained on a portion of CLEVR's training set, like 50% (check out Figure 2 for some examples from the CLEVR dataset). The results show that the execution engine doesn't do so well when it's trained on smaller subsets of data.",
        "formal_text": "Convert casual text to formal text: The VQA model by Johnson and team (2017) has two main parts: a program generator (G) and an execution engine (E). The program generator takes a question (q) and predict"
    },
    {
        "casual_text": "FB15k-237 # MR MRR Hits@10 @3 @1 MR MRR Hits@10 @3 @1 TuckER (Balazevic et al.). We also saw some cool improvements. Plus, our local-cognitive negative sampling method, which combines the best of both worlds, really shines in structured knowledge learning.",
        "formal_text": "Convert casual text to formal text: FB15k-237 # MR MRR Hits@10 @3 @1 MR MRR Hits@10 @3 @1 TuckER (Balazevic"
    },
    {
        "casual_text": "Okay, so L(X) is like a set where we have functions f that are real numbers and defined on X. These functions have something called a \"support,\" which is just a fancy word for the set of points where the function isn't zero. Now, the support of f has to be countable, meaning you can list all the points in it. Also, the sum of the squares of the function values at these points has to be finite, not infinite. So, L(X) is the collection of all such functions f.",
        "formal_text": "Convert casual text to formal text: Okay, so L(X) is like a set where we have functions f that are real numbers and defined on X. These functions have something called a \"support,\" which is just"
    },
    {
        "casual_text": "We're suggesting a neural network setup and an adversarial loss method that help with learning this transformation.",
        "formal_text": "Convert casual text to formal text: We're suggesting a neural network setup and an adversarial loss method that help with learning this transformation."
    },
    {
        "casual_text": "For the neural models we tested, we basically used the same method as Faruqui et al. in 2016. We decoded and evaluated the test data using an ensemble of 5 different models that were trained separately. This approach helps with the tricky optimization problem in neural networks and reduces the risk of getting stuck in a local optimum, as mentioned by Collobert et al. in 2011. The overall probability p ens for generating an output token y t is calculated by combining the probabilities from each individual model.",
        "formal_text": "Convert casual text to formal text: For the neural models we tested, we basically used the same method as Faruqui et al. in 2016. We decoded and evaluated the test data using an ensemble of 5 different models that"
    },
    {
        "casual_text": "Some European projects are working on automatically creating natural language texts from databases. The APOLLO project uses the CAT2 system we talked about earlier, with the goal of making training docs in French and English. DRAFTER is an experiment focused on creating multilingual instruction documents for technical writers.",
        "formal_text": "Convert casual text to formal text: Some European projects are working on automatically creating natural language texts from databases. The APOLLO project uses the CAT2 system we talked about earlier, with the goal of making training docs in"
    },
    {
        "casual_text": "Alignments #4-8 mess with the order of words in x to show how the abbreviation is made. For instance, alignment #6 shifts the word 'of' to sit between 'factor' and '1'. These shuffled alignments can represent abbreviations like receptor of estrogen (ER) or water activity (AW). We've got this parameter, d, which we call the distortion parameter. It sets the limit for how much reordering (or distortion) is allowed when figuring out the abbreviation letters.",
        "formal_text": "Convert casual text to formal text: Alignments #4-8 mess with the order of words in x to show how the abbreviation is made. For instance, alignment #6 shifts the word 'of' to sit"
    },
    {
        "casual_text": "To figure out g(i) while translating, SiMT needs a rule to decide whether to translate a target word right away or wait for the next source word. This decision can follow a fixed rule or an adaptive one.",
        "formal_text": "Convert casual text to formal text: To figure out g(i) while translating, SiMT needs a rule to decide whether to translate a target word right away or wait for the next source word. This decision can follow"
    },
    {
        "casual_text": "In this part, we’re introducing our clickbait detection method called SATC, which stands for style-aware title modeling and co-attention. The whole setup of SATC is shown in Fig. 2. It has four main parts: a content modeling module that learns how titles and bodies are represented based on their content, a style modeling module that picks up on the stylistic patterns in the title, an interaction modeling module that looks at how the title and body connect, and a clickbait prediction module that calculates the clickbait score. Let’s break down each of these modules in more detail. Oh, and Fig. 2 shows the full structure of our SATC approach for detecting clickbait.",
        "formal_text": "Convert casual text to formal text: In this part, we’re introducing our clickbait detection method called SATC, which stands for style-aware title modeling and co-attention. The whole setup of SATC is shown"
    },
    {
        "casual_text": "Alright, so the goal here is to show how to use \"locus\" to break down the interpellation of a phrase into two parts. One part is the interpellation of the addressed item, and the other is something related to it that this can combine with. Let's say, for example, we're looking at the VP \"a peach should be interpreted as.\"",
        "formal_text": "Convert casual text to formal text: Alright, so the goal here is to show how to use \"locus\" to break down the interpellation of a phrase into two parts. One part is the interpellation of the"
    },
    {
        "casual_text": "In the original dataset (Figure 1(a)), we noticed that a lot of mistakes happen when the model mixes up I and O labels—that’s 32% of the errors. When the model faces adversarial attacks, this kind of mix-up spreads to other IOB labels too. For keyboard (K) errors (Figure 1(b)), the most common mistake is confusing B with O, which happens in 16.6% of those cases. The same thing happens with swap (W) perturbations (Figure 1(c)), where this mix-up occurs 15% of the time. On the other hand, when synonyms (S) are used (Figure 1(d)), the error rates are much lower compared to K and W. We think this is because the entities are changed into similar ones. For example, \"stomach neoplasm\" becomes \"stomach tumor\". Lastly, no matter the type of attack, there are also issues with numbers and special characters. The model sometimes labels them as I (meaning they’re inside an entity), but they’re actually supposed to be O (outside an entity).",
        "formal_text": "Convert casual text to formal text: In the original dataset (Figure 1(a)), we noticed that a lot of mistakes happen when the model mixes up I and O labels—that’s 32% of the errors. When the"
    },
    {
        "casual_text": "- a syntax-based one (syntactic-driven processing), - and two that are related to word structure (lexical-driven processing).",
        "formal_text": "Convert casual text to formal text: - a syntax-based one (syntactic-driven processing), - and two that are related to word structure (lexical-driven processing)."
    },
    {
        "casual_text": "The best way to tell how good a dataset is comes down to how much people agree on its quality, whether it's different judges or the same judge grading it. To check this, we looked at how non-expert graders agreed on experiments with different ways of controlling data quality. We compared their results to what expert judges, or \"oracles,\" said. We used something called the Fleiss' kappa coefficient () to measure agreement rates. It’s calculated like this:  = (Pr(a) - Pr(e)) / (1 - Pr(e)), where Pr(a) is how often graders agree, and Pr(e) is the chance they’d agree by random. In our case, Pr(a) is how often two judges agree on whether system A is better, equal, or worse than system B for the same sentence. Here’s how the agreement scores break down: - \"None\":   0 - \"Slight\":   0.2 - \"Fair\":   0.4 - \"Moderate\":   0.6 - \"Substantial\":   0.8 - \"Almost perfect\":   1.0 This scale comes from Landis and Koch (1977).",
        "formal_text": "Convert casual text to formal text: The best way to tell how good a dataset is comes down to how much people agree on its quality, whether it's different judges or the same judge grading it. To check this,"
    },
    {
        "casual_text": "We looked into how our methods enhance the coreference reasoning of the basic RoBERTa model. To see how different approaches performed, we used RoBERTa-base as our starting point. In Table 1, RoBERTa-base + Hist. &Cont. + EG did the best job as the evidence generator (EG), and RoBERTa-base + Fu-sionMd. (+GCN) was the top performer when it came to the fusion model. We picked out a few examples from CoQA to explain things in more detail.",
        "formal_text": "Convert casual text to formal text: We looked into how our methods enhance the coreference reasoning of the basic RoBERTa model. To see how different approaches performed, we used RoBERTa-base as our starting point. In"
    },
    {
        "casual_text": "Umm... what's with all the \"q\"s? Is this some kind of code or something?",
        "formal_text": "Convert casual text to formal text: Um... what's with all the \"q\"s? Is this some code or something?"
    },
    {
        "casual_text": "The different levels of impact on the results make it a good idea to figure out the semantic label distributions for each hop pattern separately. Since some hop patterns don't have much impact, it makes sense to group together the ones that are similar in terms of their semantic label distributions.",
        "formal_text": "Convert casual text to formal text: The different levels of impact on the results make it a good idea to figure out the semantic label distributions for each hop pattern separately. Since some hop patterns don't have much impact, it makes"
    },
    {
        "casual_text": "It means that in any ordered tree, for any node u, if the feature structure linked to u begins with the point w and inv is true at w, then auz is also true at w. Plus, you can get to the propositional info fin from w by taking a (VFORM) transition to another node w'.",
        "formal_text": "Convert casual text to formal text: It means that in any ordered tree, for any node u, if the feature structure linked to u begins with the point w and inv is true at w, then au"
    },
    {
        "casual_text": "This paper introduces a phrase structure parsing algorithm that’s kind of a mashup of ideas from LR (Knuth, 1965), GLR (Tomita, 1988), and some newer stuff on dependency grammar by Huang and Sagae (2010). It also uses a discriminative weighting system inspired by Collins (2002). Now, discriminative phrase structure parsing can be a real pain when it comes to speed and efficiency, as Turian and Melamed (2006) and Finkel et al. (2008) pointed out. But we’ve figured out some shortcuts that not only make this approach manageable but also super efficient and accurate, especially for a language like French, which is packed with tons of vocabulary.",
        "formal_text": "Convert casual text to formal text: This paper introduces a phrase structure parsing algorithm that’s kind of a mashup of ideas from LR (Knuth, 1965), GLR (Tomita"
    },
    {
        "casual_text": "Another way to do zero-shot image classification is by using word embeddings straight up. Frome and his team (2013) created DeVISE, a model for zero-shot learning on ImageNet object recognition. The goal here is to have the image model predict a class's word embedding directly. DeVISE is trained by trying to minimize...",
        "formal_text": "Convert casual text to formal text: Another way to do zero-shot image classification is by using word embeddings straight up. Frome and his team (2013) created DeVISE, a model for zero-shot learning on Image"
    },
    {
        "casual_text": "This paper builds on some recent studies about pulling out logical structures from plain text (like Zettlemoyer and Collins in 2005, Ge and Mooney in 2005, Downey and others in 2005, and Liang and others in 2009). It also looks at how language is understood within a specific area (Chen and Mooney, 2008) and how using a practical area can help with interpreting text (Branavan and others, 2009). Our research stands out in a few ways:",
        "formal_text": "Convert casual text to formal text: This paper builds on some recent studies about pulling out logical structures from plain text (like Zettlemoyer and Collins in 2005, Ge and Mooney in 2005, Downey and others in"
    },
    {
        "casual_text": "The pilot project results were shared with a small group by the Innovation Fund liaison, and that part of the job was wrapped up. After that, things kind of slowed down because one of the main supporters got sick, and another one got let go for unrelated reasons. But then, after a chat with the Director of National Linguistics and Cultural Programs and a couple of other folks, STI got invited back to be a big part of the National Diversity Conference this year. The conference will focus on hands-on demos of upcoming tech related to language and cultural skills. STI is planning to team up with some app developers to show off mobile translation tools that work with speech on smartphones and newer tablets. The event is set for November 2012, which is about four and a half years since STI first got involved with the healthcare organization.",
        "formal_text": "Convert casual text to formal text: The pilot project results were shared with a small group by the Innovation Fund liaison, and that part of the job was wrapped up. After that, things kind of slowed down because one of the"
    },
    {
        "casual_text": "Like in earlier studies, we're using the official way to measure performance, which is the macro-averaged F1-score. This one considers directionality and skips the Other class.",
        "formal_text": "Convert casual text to formal text: Like in earlier studies, we're using the official way to measure performance, which is the macro-averaged F1-score. This one considers directionality and skips the Other class"
    },
    {
        "casual_text": "Previous research has looked at how edit types and intentions can help us understand how people work together on writing projects, like improving the quality of articles (Kittur and Kraut, 2008). For instance, Liu and Ram (2011) noticed that the quality of Wikipedia articles is linked to the types of contributors working on them. Similarly, Yang et al. (2016) found that articles at different quality levels need different kinds of editors. But, not many studies have dug into which specific types of edits actually predict article quality. On top of that, recent research shows that the number of active contributors on Wikipedia has been dropping since 2007. Halfaker et al. (2012) think that the semi-automated rejection of new editors' contributions is a big reason for this, but they didn't look into whether certain types of new editors' work get rejected more often and how that impacts their staying around. In this paper, we're using this new way of categorizing edits to see how edit intentions, new editors sticking around, and article quality are all connected.",
        "formal_text": "Convert casual text to formal text: Previous research has looked at how edit types and intentions can help us understand how people work together on writing projects, like improving the quality of articles (Kittur and Kraut, 2008). For instance"
    },
    {
        "casual_text": "Alright, since CHOOSE isn't around anymore, we need to make sure A(i) is empty when we do the INIT setup. There's an example of how this works in Appendix F.",
        "formal_text": "Convert casual text to formal text: Alright, since CHOOSE isn't around anymore, we need to make sure A(i) is empty when we do the INIT setup. There's an example this works in"
    },
    {
        "casual_text": "2 So, the overall change on the main test set (Total) is basically nothing, but when we look closer, we see that the new data helps reduce the error rate for low-frequency phrases by up to 3%. And, as we hoped, it doesn’t mess things up for the high-frequency stuff. Things get even better for the MARUPA examples we held back, where the baseline had a high error rate on purpose. We noticed that a lot of the collected phrases are so rare that they don’t even show up in the lowest-frequency category of the main test set, so it’s hard to fully understand how much MARUPA is helping. Across different languages, we found that for newer ones like Hindi, the examples we get aren’t as helpful and the quality is lower. This is because there are fewer user interactions and the underlying IC/SL system isn’t as developed yet. Overall, the results show that MARUPA can boost accuracy for those less common phrases.",
        "formal_text": "Convert casual text to formal text: 2 So, the overall change on the main test set (Total) is basically nothing, but when we look closer, we see that the new data helps reduce the error rate for low-frequency phrases"
    },
    {
        "casual_text": "So, we’re talking about a survey by Gao et al. (2018) that gives a big-picture look at neural methods in conversational AI. In this paper, we’re focusing on similar work, specifically semantic parsing approaches for conversations. Liang et al. (2017) came up with a Neural Symbolic Machine (NSM) that’s enhanced with a key-value memory network. In this setup, the keys and values come from a sequence model during different encoding or decoding stages. The NSM is trained using the REINFORCE algorithm with weak supervision and tested on the WebQuestionsSP dataset (Yih et al., 2016). Then, Saha et al. (2018) introduced a hybrid model that combines the HRED model (Serban et al., 2016) and the key-value memory network model (Miller et al., 2016). This model has three main parts: 1. The Hierarchical Encoder, which creates a representation for each utterance. 2. A higher-level encoder that makes a representation for the overall context. 3. The Key-Value Memory Network. It stores each candidate tuple as a key-value pair. The key is made up of the combined embeddings of the relation and the subject, while the value includes things like: - Finding objects in triples with a specific subject and predicate. - Finding subjects in triples with a specific object and predicate. - Filtering entities based on a given type or multiple types.",
        "formal_text": "Convert casual text to formal text: So, we’re talking about a survey by Gao et al. (2018) that gives a big-picture look at neural methods in conversational AI. In this paper, we"
    },
    {
        "casual_text": "To make sure our data is all about political talk and includes a good mix of epistemic modality, we picked tweets that have: (1) at least one popular political hashtag in English or Arabic, like #Egypt or # (Morsi), and (2) at least one word or phrase that shows epistemic modality, based on the Arabic Modality Lexicons by Al-Sabbagh et al. (2013, 2014). We ended up with 9,822 unique tweets, which have 9,966 possible triggers for epistemic modality that belong to 214 different types. You can check out the stats in Table 1.",
        "formal_text": "Convert casual text to formal text: To make sure our data is all about political talk and includes a good mix of epistemic modality, we picked tweets that have: (1) at least one popular political hashtag in English or Arabic"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: - ****: This represents some kind of distribution or pattern. - **k, w, i, j, l**: These are just labels or indices. For example: - **i**: Refers to a specific document. - **j**: Refers to a segment within that document. - **l**: Refers to a position or place within that segment. - **w**: Represents a word in that position. - **z**: Represents the topic associated with that word. - **w_L, z_I, K, , , , , **: These are just more variables or parameters used in the model. - **_1, _2, ..., _J**: These are different versions or instances of the parameter ****. - **w_L z_1, w_L z_2, ..., w_L z_J**: These are combinations of words and topics, repeated for different segments. - ****: This is just another variable or parameter. So, in short, this is a bunch of symbols and variables used to describe how words, topics, and other elements are connected in a model, with different versions or instances of some parameters.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: - ****: This represents some kind of distribution or pattern. - **k, w, i,"
    },
    {
        "casual_text": "On the Claim Stance test set, the identifier got an F1-score of 0.77. To see how well it works in other areas, we had human annotators check a random sample of 100 conclusions from the iDebate dataset. Each one was looked at by three annotators. According to the majority opinion, the tagger got 72% of the cases right.",
        "formal_text": "Convert casual text to formal text: On the Claim Stance test set, the identifier got an F1-score of 0.77. To see how well it works in other areas, we had human annotators check"
    },
    {
        "casual_text": "When we’re trying to pull out useful info from team conversations during missions, it’s super important to figure out what people are talking about—like objects, places, people, tasks, or events—and how they’re connected. That’s basically what reference resolution is all about. To get a clearer picture of how to do this, we looked at a bunch of real-world team chats from the TRADR project (check out their website for more, or read Kruijff-Korbayová et al.’s work from 2015). These chats came from teams of first responders using robots—both on the ground and in the air—to scout areas, look for victims or dangers, and collect data after something like a fire or explosion. In the first part of our study, which we’re sharing in this paper, we zeroed in on how people talked about mission-related objects, locations, and actors, and how they linked them together. We wanted to see what kinds of reference situations popped up, how often they happened, and what made them tricky to figure out. To keep track of all this, we annotated the data—basically, we tagged and labeled everything systematically so we could go back and analyze it more deeply later. Our goal wasn’t to create some perfect annotated dataset or invent a brand-new tagging system. Instead, we made a custom annotation scheme just for our analysis, and it kept changing and improving as we worked through the data.",
        "formal_text": "Convert casual text to formal text: When we’re trying to pull out useful info from team conversations during missions, it’s super important to figure out what people are talking about—like objects, places, people, tasks, or events"
    },
    {
        "casual_text": "We want to point out that our human evaluations were done by regular folks who read the news casually. The samples they saw had all the named entities taken out, so it was a fair comparison with the AI models. We also want to stress that removing named entities makes the task tougher, even for humans.",
        "formal_text": "Convert casual text to formal text: We want to point out that our human evaluations were done by regular folks who read the news casually. The samples they saw had all the named entities taken out, so it was a fair comparison"
    },
    {
        "casual_text": "As mentioned earlier, just focusing on the context without considering other useful info can be a problem. This becomes clear when you look at the word distributions we get. To fix this, we use a pattern called X and Y to create a substitutes vector that takes both the word and its context into account. Basically, we represent a target word w i using two parts: p  (w |w 1, . . . , w i, and) and p  (w |w n, . . . , w i, and). For our example, this looks like: LM  (s> I liked the sound and ) LM  ( and sound of the harpsichord. ) And here are the top words we get: feel: 0.15, felt: 0.11, thought: 0.07, smell: 0.06, sounds: 0.05 sight: 0.16, sounds: 0.11, rhythm: 0.04, tone: 0.03, noise: 0.03.",
        "formal_text": "Convert casual text to formal text: As mentioned earlier, just focusing on the context without considering other useful info can be a problem. This becomes clear when you look at the word distributions we get. To fix this, we use"
    },
    {
        "casual_text": "Most spelling systems these days need some special, language-specific stuff, like dictionaries, lists of common typos, or rule books. Systems that use statistical models need a lot of data with examples of spelling mistakes to learn from. But our system doesn’t need any of that fancy annotated data. Instead, we just use the internet as a big, messy source of information. Here’s how we do it: 1) We figure out stuff about typos by looking at how words are used on the web and use that to create a model of errors. 2) The words we see the most online become a messy list of possible corrections. 3) We use n-grams (little chunks of words) to build a language model (LM) that helps us make corrections that sound right in context. Since our error model works by looking at parts of words, we don’t have a fixed list of \"correct\" words to compare things to. So, whether a word is spelled wrong or right, it’s all good. Plus, when we combine this with our n-gram LM, the system can catch and fix real-word mistakes—like when you use the wrong word or mess up grammar.",
        "formal_text": "Convert casual text to formal text: Most spelling systems these days need some special, language-specific stuff, like dictionaries, lists of common typos, or rule books. Systems that use statistical models need a lot of data with"
    },
    {
        "casual_text": "Check out the annotation templates and interfaces we used for the explanation collection and verification stages in Figures 12, 13, 14, 15, and 16.",
        "formal_text": "Convert casual text to formal text: Check out the annotation templates and interfaces we used for the explanation collection and verification stages in Figures 12, 13, 14, 15, and 16. Check out the annotation templates and interfaces we used for the explanation collection and"
    },
    {
        "casual_text": "5. Break an egg into the same pan, scramble it up, and stir it into the veggies.",
        "formal_text": "Convert casual text to formal text: 5. Break an egg into the same pan, scramble it, and stir into the veggies. 6. Break an egg into the same pan, scramble it, and stir it into the veggies."
    },
    {
        "casual_text": "Negative sampling can really impact how well a graph embedding method works (Cai and Wang, 2018; Sun et al., 2019). Basically, comparing a tough negative example to the right positive one helps the model learn better. So, let’s say you have a correct triple x = (h, r, t) that’s part of the knowledge graph G(tr). Negative sampling involves messing with either the head or the tail of this triple to create a wrong one, like x = (h, r, t) or (h, r, t ), which isn’t in G(tr). G(tr) is just the knowledge graph we’re using to train the embedding model. For simplicity, we’ll focus on corrupting the tail in this explanation, but we also consider corrupting the head in our actual setup.",
        "formal_text": "Convert casual text to formal text: Negative sampling can really impact how well a graph embedding method works (Cai and Wang, 2018; Sun et al., 2019). Basically, comparing a"
    },
    {
        "casual_text": "Alright, so FINISH does something similar: it takes the child nodes l k of all MOD edges that go out from r  0 and puts them onto the stack. It also figures out their term type sets T k, just like in the MODIFY rule from section 5.2. We add the kids to the stack in the opposite order they were made, so when we take them off, they come out in the same order as the edges were drawn.",
        "formal_text": "Convert casual text to formal text: Alright, so FINISH does something similar: it takes the child nodes l k of all MOD edges that go out from r  0 and puts them onto the stack"
    },
    {
        "casual_text": "The algorithm works the same way no matter how big the parallel text you're aligning is, since it processes each sentence pair separately. This makes it super easy to run in parallel: you just split the work between the processors you have, and it speeds things up. Plus, another cool thing is that the alignments stay symmetric throughout the whole process. This is different from other models, like IBM models, which usually give better results if you run them in both directions and then combine the outputs using some tricks.",
        "formal_text": "Convert casual text to formal text: The algorithm works the same way no matter how big the parallel text you're aligning is, since it processes each sentence pair separately. This makes it super easy to run in parallel: you just split"
    },
    {
        "casual_text": "First, we need to figure out if \"n\" is an entity. If it is, we also look at the set of Freebase semantic types for \"n.\" For \"L,\" we use the 9.7 million assertions we managed to link in Section 2.1, and for \"U,\" we use the 5 million assertions we couldn’t link. We split the system into two parts. The first part (explained in Section 4) checks any noun phrase we couldn’t link and decides if it’s an entity. All the \"n\"s in \"U\" that are classified as entities go into a set called \"E.\" The second part (covered in Section 5) uses both \"L\" and \"U\" to predict the semantic types for each entity in \"E.\"",
        "formal_text": "Convert casual text to formal text: First, we need to figure out if \"n\" is an entity. If it is, we also look at the set of Freebase semantic types for \"n.\" For \"L,\" we use"
    },
    {
        "casual_text": "Let’s dive into the results for each dataset, focusing on how models handle different situations. You can find a quick summary in Section 7.",
        "formal_text": "Convert casual text to formal text: Let’s dive into the results for each dataset, focusing on how models handle different situations. You can find a quick summary in Section 7. Let’s dive into the results for each dataset,"
    },
    {
        "casual_text": "We used Pytorch 1.4.0 (Paszke et al., 2019) along with libraries like numpy (Harris et al., 2020) and scipy (Jones et al., 2001-) to code all the models. For running the experiments, we had two setups: (1) a GeForce RTX 2080 GPU with 12 GB of memory, 256 GB of RAM, and 40 CPU cores, or (2) a Tesla V100-SXM2 GPU with 16 GB of memory, 250 GB of RAM, and 40 CPU cores.",
        "formal_text": "Convert casual text to formal text: We used Pytorch 1.4.0 (Paszke et al., 2019) along with libraries like numpy (Harris et al., 2020)"
    },
    {
        "casual_text": "Okay, let’s say c1, c2, and so on represent the sequence of vector embeddings for the parts of the new phrase. The size of these vectors depends on the number of dimensions in the bidirectional LSTM that was used in the original composition function, as shown in Figure 2. We’ll use a semicolon (;) to show when we’re sticking two vectors together, like gluing them end-to-end.",
        "formal_text": "Convert casual text to formal text: Okay, let’s say c1, c2, and so on represent the sequence of vector embeddings for the parts of the new phrase. The size of these vectors depends on the"
    },
    {
        "casual_text": "This approach has its flaws because it relies on a symbolic representation that can easily get messed up when the training and test data don't match well. If the clustering method is too broad, the features won't be helpful, but if it's too detailed, there'll be more mismatches. Plus, verbs that fall between multiple clusters are tricky to handle this way. To fix these issues, we tweaked the PRA algorithm to use vector representations of edge types directly during the random walk process.",
        "formal_text": "Convert casual text to formal text: This approach has its flaws because it relies on a symbolic representation that can easily get messed up when the training and test data don't match well. If the clustering method"
    },
    {
        "casual_text": "Big thanks to Raphael Schumann for sharing some really helpful ideas on this project. We also want to give a shoutout to the Action Editor and reviewers for their feedback during the ACL Rolling Review. This research is partly supported by a bunch of cool organizations, including NSERC (grant No. RGPIN2020-04465), the Amii Fellow Program, the Canada CIFAR AI Chair Program, a UAHJIC project, a donation from DeepMind, and Compute Canada (www.computecanada.ca).",
        "formal_text": "Convert casual text to formal text: Big thanks to Raphael Schumann for sharing some really helpful ideas on this project. We also want to give a shoutout to the Action Editor and reviewers for their feedback during the A"
    },
    {
        "casual_text": "We turn entities and relationships in the KG into vectors in a special space. Then, at each step t, the action (r, e) can be shown as a t = [r; e], where r and e are just the vectors for r and e. As we talked about in Section 3.1, we use an LSTM to keep track of all the past actions. Basically, each action the agent picks gets fed into the LSTM, which then builds up a record of the path taken so far.",
        "formal_text": "Convert casual text to formal text: We turn entities and relationships in the KG into vectors in a special space. Then, at each step t, the action (r, e) can be shown as a"
    },
    {
        "casual_text": "A lot of the mistakes happen because of what users do, like sharing a ton of links. To fix this in the future, looking at how topics change over time might help. Basically, if someone never talks about politics and their patterns don’t match those of known troll accounts, we could cut down on these false alarms.",
        "formal_text": "Convert casual text to formal text: A lot of the mistakes happen because of what users do, like sharing a ton of links. To fix this in the future, looking at how topics change over time might help. Basically,"
    },
    {
        "casual_text": "Let's dive into how common graph queries really are. This study has two main perks. First off, it helps weed out super rare queries that don’t pop up much. Second, how common a query is can tell us a lot about the question itself. It’s pretty cool to see how this affects how hard the question is to answer. Take a look at Figure 4, for instance. It asks for \"the great-great-grandparents of Ernest Solvay.\" The question is simple and makes sense, but chances are, not many people would think to ask it. Ernest Solvay is known for the Solvay Conferences, but unless you’re into science, you probably don’t know much about him. While terms like \"Person\" and \"parents\" are pretty standard, asking about great-great-grandparents is way less common.",
        "formal_text": "Convert casual text to formal text: Let's dive into how common graph queries really are. This study has two main perks. First off, it helps weed out super rare queries that don’t pop up much. Second,"
    },
    {
        "casual_text": "To tackle the problem of having a small dataset, we decided to use some pre-trained English word vectors from the Google News Corpus, which has around 3 million words. Each word in this corpus is represented by a 300-dimensional vector. We used the English definitions (or \"glosses\") from Wolvengrey's dictionary (published in 2001) and matched each word with its corresponding vector from the Google News Corpus. Wolvengrey's dictionary has 21,717 entries and uses lemmas as its main words. We assumed that the meanings of English and Nêhiyawêwin words are pretty similar, especially when you look at the whole set of words in an English definition. We got rid of stop words and, if a content word from Wolvengrey's definitions wasn't in the Google News Corpus, we replaced it with a synonym. For example, the word \"mitêwin\" wasn't in the corpus, so we replaced it with something like \"medicine lodge\" or just left it out if a synonym was already in the definition. Since the Google News Corpus uses American spelling and Wolvengrey's dictionary uses Canadian spelling, we changed American spellings (like \"color\" and \"gray\") to Canadian ones (like \"colour\" and \"grey\"). If we didn't do this, those words wouldn't have matching vectors and couldn't be used for clustering.",
        "formal_text": "Convert casual text to formal text: To tackle the problem of having a small dataset, we decided to use some pre-trained English word vectors from the Google News Corpus, which has around 3 million words. Each word in"
    },
    {
        "casual_text": "The most common approach for semi-supervised sentiment classification is using some labeled data to help guide the process (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008; Wan, 2009; Li et al., 2011). But in a lot of cases, we don’t have any labeled data, which opens the door to unsupervised methods. The typical way to do unsupervised sentiment classification is by using a sentiment lexicon (Turney, 2002; Taboada et al., 2011) or by figuring out sentiment orientation through matrix factorization and clustering (Li et al., 2009; Hu et al., 2013). Instead of following those methods, we’ve come up with a different approach that combines word-level and document-level sentiment information (like how nearby words or documents often share the same sentiment) into one framework. Our framework uses geometric information to make up for the lack of labeled data, which is a big deal for sentiment classification. On top of that, some researchers have used matrix factorization for other NLP tasks, like relation extraction (Peng and Park, 2013) and question answering (Zhou et al., 2013). There are also a bunch of studies focusing on other parts of sentiment analysis, such as cross-domain sentiment classification (Blitzer et al., 2007; Pan et al., 2010; Hu et al., 2011; Bollegala et al.).",
        "formal_text": "Convert casual text to formal text: The most common approach for semi-supervised sentiment classification is using some labeled data to help guide the process (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008"
    },
    {
        "casual_text": "Plus, the LSTM-based language model (LM) might do a better job because it captures more context than the CRF layer. To really make the most of the LM, we also tried training the tagging model and the LM together at the sequence level. Sure, this slows things down a bit, but the combined system performs just as well as the top-notch NER models on four different datasets.",
        "formal_text": "Convert casual text to formal text: Plus, the LSTM-based language model (LM) might do a better job because it captures more context than the CRF layer. To really make the most of the LM,"
    },
    {
        "casual_text": "This paper introduces FastSeq, a framework that makes sequence generation way faster. With just a one-line code tweak, FastSeq can speed up sequence generation by 4 to 9 times for models in FairSeq (Ott et al., 2019) and Huggingface-Transformers (Wolf et al., 2020). The goal of FastSeq is to boost inference speed without sacrificing model accuracy or ease of use.",
        "formal_text": "Convert casual text to formal text: This paper introduces FastSeq, a framework that makes sequence generation way faster. With just a one-line code tweak, FastSeq can speed up sequence generation by 4 to"
    },
    {
        "casual_text": "• Evidence Generator (EG): This tool creates sentences that help figure out who or what a pronoun refers to, like \"She\" meaning \"Jessica.\" It adds these sentences as clues before the question. We're suggesting using a graph neural network to pull out features from a coreference graph. Then, we mix these graph features with sequence features from a pre-trained language model (PLM) to make the PLM better at figuring out coreference.",
        "formal_text": "Convert casual text to formal text: • Evidence Generator (EG): This tool creates sentences that help figure out who or what a pronoun refers to, like \"She\" meaning \"Jessica.\" It add"
    },
    {
        "casual_text": "We used the WER measure to evaluate our results because it let us compare them to other studies done on the same dataset. On the other hand, the BLEU score is kind of the go-to metric for checking how good a translation system is.",
        "formal_text": "Convert casual text to formal text: We used the WER measure to evaluate our results because it let us compare them to other studies done on the same dataset On the other hand, the BLEU score is kind of the go-to"
    },
    {
        "casual_text": "We’re looking at the methods shown in Figure 1. The KB method uses the original PRA algorithm, but only on the KB relations, as described by Lao and Cohen (2010). KB + SVO adds surface relations to the graph (check out Figure 1b). This is kind of similar to what Lao et al. (2012) did, but there are some key differences in how the graph is structured, which we’ll explain in Section 5. KB + Clustered SVO is based on Gardner et al. (2013), but we’re using a different graph setup from this paper (see Figure 1c). Their approach would’ve made the graphs way too big for the Freebase experiments. Finally, KB + Vector SVO is our own method (Figure 1d).",
        "formal_text": "Convert casual text to formal text: We’re looking at the methods shown in Figure 1. The KB method uses the original PRA algorithm, but only on the KB relations, as described by Lao and Cohen (2010). KB"
    },
    {
        "casual_text": "Alright, so for designing a probe to figure out the depth of Dyck-2 substrings hidden in the cell states of pretrained LSTMs, we used a simple Feed-Forward network with just one hidden layer. We set the hidden layer size to 32 and trained it using the Adam Optimizer (that's the one from Kingma and Ba, 2014) with a batch size of 200. To check how well it was doing, we looked at the accuracy on the validation set, but we only counted a sequence as correct if it got the depth right at every single step. Then, in the next set of experiments, we tried to predict the actual elements of the stack. We trained the LSTM on the NCP task and added an extra loss to help it predict the top 10 elements of the stack. To do this, we added 10 parallel linear layers on top of the LSTM's output. Each of these layers was responsible for predicting whether the i-th element in the stack was: 1) a round opening bracket, 2) a square opening bracket, or 3) if there was no element there at all. For each of these 10 layers, we calculated the Cross Entropy Loss, and then averaged them all together to get the auxillary loss. The final loss was calculated as:",
        "formal_text": "Convert casual text to formal text: Alright, so for designing a probe to figure out the depth of Dyck-2 substrings hidden in the cell states of pretrained LSTMs, we used a simple Feed"
    },
    {
        "casual_text": "• We’ve come up with OLM, a new way to explain why a model makes certain decisions, focusing on how it understands language. It works for any model doing NLP classification, and we check which explanation rules it follows. • We also propose a new rule called the zero-sum axiom for these explanation methods. • We tested our method by comparing its results to other black-box and gradient-based explanation techniques to see how it stacks up.",
        "formal_text": "Convert casual text to formal text: • We’ve come up with OLM, a new way to explain why a model makes certain decisions, focusing on how it understands language. It works for any model doing NLP"
    },
    {
        "casual_text": "Alright, so basically, D' is about recalculating the data from the original dataset (let's call it Dataset 1) by focusing on specific parts of it. These parts are related to certain features or variables that come from a different source, like the    Ü    Ü. The idea is to adjust or refine the original dataset based on this new information. It's kind of like tweaking the data to make it more accurate or relevant by incorporating these extra details.",
        "formal_text": "Convert casual text to formal text: Alright, so basically, D' is about recalculating the data from the original dataset (let's call it Dataset 1) by focusing on specific parts of it. These parts are related"
    },
    {
        "casual_text": "Experiment 4: This one compares two different update methods—early update and max violation update. The max violation update gets trained for 12 epochs, but the early update goes through 25 epochs.",
        "formal_text": "Convert casual text to formal text: Experiment 4: This one compares two different update methods—early update and max violation update. The max violation update gets trained for 12 epochs, but the early update goes"
    },
    {
        "casual_text": "CapVizWiz (Gurari et al., 2018): VizWiz is a system where visually impaired folks using a mobile app ask questions about stuff they need help with in their daily lives. Each question gets answered by someone who isn't there in person but helps out remotely.",
        "formal_text": "Convert casual text to formal text: CapVizWiz (Gurari et al., 2018): VizWiz is a system where visually impaired folks using a mobile app ask questions about"
    },
    {
        "casual_text": "WordNet, created by Fellbaum in 1998, is a big collection of English words where words that are related in meaning are grouped together using something called cognitive synonyms, or \"synsets.\" It's a handy tool for analyzing word meanings and has been used a lot in stuff like question classification (check out Krishnan et al. in 2005 and Schlaefer et al. in 2007). One common way to use WordNet is by looking at hypernyms: basically, Y is a hypernym of X if every X is a type of Y. For instance, if you're asked what breed of hunting dog the Beverly Hillbillies owned, you need to know that \"animal\" is a hypernym of \"dog.\" In this paper, we're suggesting two ways to add more meaning to WordNet. The first one directly adds hypernyms of the main words we talked about earlier, and the second one uses a WordNet similarity tool (thanks to Seco et al. in 2004) that kind of sneaks in the hypernym structure.",
        "formal_text": "Convert casual text to formal text: WordNet, created by Fellbaum in 1998, is a big collection of English words where words that are related in meaning are grouped together using something called cognitive synonyms, or \"sy"
    },
    {
        "casual_text": "We calculate a combined score for each answer option by using the question that works best with it, based on the inference model.",
        "formal_text": "Convert casual text to formal text: We calculate a combined score for each answer option by using the question that works best it, based on the inference model."
    },
    {
        "casual_text": "Sure! Here's the informal version: You can check out the Open Access full-text articles list tool here: [OpenFTList](https://www.ncbi.nlm.nih.gov/pmc/tools/openftlist/). And if you need the RefSeer dataset, you can find it here: [RefSeer on Box](https://psu.app.box.com/v/refseer).",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: You can check out the Open Access full-text articles list tool here: [OpenFTList](https://www.ncbi.n"
    },
    {
        "casual_text": "to adjust the L2 norm for each group of embeddings individually. We can rewrite Eq. 1 like this:",
        "formal_text": "Convert casual text to formal: to adjust the L2 norm for each group of embeddings individually. We can rewrite Eq. 1 like this: We can rewrite Eq. 1 like this: We can"
    },
    {
        "casual_text": "Okay, so here's the deal: when the syntax rules get too confusing and don't match up with what feels right, we'll dive deeper into that in Section 4. For now, it's important to mention that when it comes to labeling the time relationship between events and time expressions, or between a main event and a smaller event, there are two main steps. First, the person doing the labeling has to figure out which pairs of time-related things to label. Then, they need to decide what kind of time relationship to assign to each pair. To see which of these two steps is more challenging, we checked how well people agreed on their choices for each step. In Table 2, Column 3 shows how well they agreed on picking the right pairs, and Column 4 shows how well they agreed on assigning the correct time relationship, assuming both people picked the same pairs.",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: when the syntax rules get too confusing and don't match up with what feels right, we'll dive deeper into that in Section 4. For now, it"
    },
    {
        "casual_text": "We're testing our zero-shot activity recognition against a few other baseline models that also learn from attributes and embeddings. One of them is the \"Embarrassingly Simple Zero-shot Learning\" (ESZL) model by Romera-Paredes and Torr (2015). It's a straightforward linear model that predicts class labels using attributes and includes some regularization techniques. We're also comparing our results to a version of the DAP model by Lampert et al. (2014), which we talked about in Section 4.1.1. Plus, we're including DeVISE (Frome et al., 2013) in our comparisons, as mentioned in Section 4.2. For all these baselines, we're using visual features from a Resnet-152 CNN that's been fine-tuned on the imSitu V train classes. This is the same setup we discussed in Section 4.1.",
        "formal_text": "Convert casual text to formal text: We're testing our zero-shot activity recognition against a few other baseline models that also learn from attributes and embeddings. One of them is the \"Embarrassingly Simple"
    },
    {
        "casual_text": "A part-whole pattern shows that one thing is part of another thing. Take the example \"There's a valley on my mattress.\" Here, \"valley\" and \"mattress\" have a part-whole relationship, indicated by the word \"on.\" But in reality, \"valley\" isn't actually part of the mattress—it's more like an effect on the mattress. This is called a pseudo part-whole relation. For our purposes, though, we won't make a big deal out of the difference between this and a real part-whole relationship because they don't matter much for our feature mining task. In this case, \"noun 1 on noun 2\" is a helpful pattern that suggests noun 1 is part of noun 2. So if we know \"mattress\" is a class concept, we can guess that \"valley\" is a feature for \"mattress.\" There are lots of other phrases or sentences that show this kind of relationship, as mentioned in (Girju et al, 2006). Besides part-whole patterns, the \"no\" pattern is another important feature indicator, especially in opinion documents. We'll talk more about these patterns in Sections 3.2 and 3.3. Now, let's tackle the first issue: noise. With opinion words, part-whole patterns, and \"no\" patterns, we have three ways to identify features. But all of these are kind of vague, meaning they're not strict rules. This means we'll probably end up with some wrong features (also known as noise) when we use them. Getting rid of noise from our feature candidates is tricky. Instead of trying to prune out the noise directly, we're going to take a different approach: feature ranking.",
        "formal_text": "Convert casual text to formal text: A part-whole pattern shows that one thing is part of another thing. Take the example \"There's a valley on my mattress.\" Here, \"valley\" and \"mattress\""
    },
    {
        "casual_text": "We're looking at medium-sized sentences that are kind of in the middle—not too short, not too long. Based on what we talked about in the intro, we’re using the C# version of Lucene 4 to create an index of these sentences, treating each one as a Lucene document. For each of these documents, we add three extra fields that you can search through, two of which are related to the sentence length.",
        "formal_text": "Convert casual text to formal text: We're looking at medium-sized sentences that are kind of in the middle—not too short, not too long. Based on what we talked about in the intro, we’re using the C"
    },
    {
        "casual_text": "There’s no clear-cut separation between syntax and semantics. Syntax is basically a rough way of organizing semantics, but it has gaps that need to be filled in by transformations. So, every concept in a semantic network, along with its roles and restrictions, can be thought of as either a semantic definition or a pattern for forming semantic sentences.",
        "formal_text": "Convert casual text to formal text: There’s no clear-cut separation between syntax and semantics. Syntax is basically a rough way of organizing semantics, but it has gaps that need to be filled in by transformations."
    },
    {
        "casual_text": "In most basic noun phrases (check out rows 6-7 in Table 5 for examples), the model focuses mainly on the last noun in the phrase and pretty much ignores determiners and possessive words. At the same time, it gives some decent attention to adjectives. This makes sense because, traditionally, nouns are considered the main part of noun phrases, and adjectives tend to be more significant than determiners.",
        "formal_text": "Convert casual text to formal text: In most basic noun phrases (check out rows 6-7 in Table 5 for examples), the model focuses mainly on the last noun in the phrase and pretty much ignores determiners and"
    },
    {
        "casual_text": "Older models had different approaches to handling punctuation. While Jones (1994) and Spitkovsky et al. (2011) mentioned that paying attention to punctuation could be useful for unsupervised parsing, a lot of earlier models just got rid of punctuation. Some newer ones treat it like any other word. Only a handful, like CCL (Seginer, 2007), give punctuation special treatment. We tried out two setups for length 40—one with punctuation and one without.",
        "formal_text": "Convert casual text to formal text: Older models had different approaches to handling punctuation. While Jones (1994) and Spitkovsky et al. (2011) mentioned that paying attention to punctuation could be useful for un"
    },
    {
        "casual_text": "We tested our method using the GermEval 2017 dataset, which includes customer reviews about Deutsche Bahn AG shared on social media. Specifically, we focused on subtask C, which is all about finding two specific pieces of information from raw text.",
        "formal_text": "Convert casual text to formal text: We tested our method using the GermEval 2017 dataset, which includes customer reviews about Deutsche Bahn AG shared on social media. Specifically, we focused on subtask C, which is about finding"
    },
    {
        "casual_text": "LDA is a model that helps figure out the topics in a collection of documents using probability. It thinks of documents as a mix of hidden topics, and each topic has its own way of picking words. To create a word in a document, we first randomly pick a topic based on how common that topic is in the document. This \"commonness\" comes from a special distribution called a Dirichlet distribution, which has some tuning parameters. After choosing the topic, we pick a word from a list of words that are likely for that topic, and this list also comes from another Dirichlet distribution with its own parameters.",
        "formal_text": "Convert casual text to formal text: LDA is a model that helps figure out the topics in a collection of documents using probability. It thinks of documents as a mix of hidden topics, and each topic has its own way"
    },
    {
        "casual_text": "After combining the question with both image-level and object-level features (check out Section 4.3 for more details), we add these features to something called the \"counter feature,\" which was introduced in the paper. This counter feature helps the model keep track of how many objects there are. Then, we use a simple two-layer neural network to pick the answer from a list of options. The list is made up of all the correct answers from the training data that showed up more than 8 times. To figure out the best answer, we calculate something called \"logits\" using this equation:",
        "formal_text": "Convert casual text to formal text: After combining the question with both image-level and object-level features (check out Section 4.3 for more details), we add these features to something called the \"counter feature,\" which was introduced in"
    },
    {
        "casual_text": "We suggested using space to represent the meanings of words. We came up with a way to learn how to do this in tile space. We also showed that our methods work well in an information retrieval system.",
        "formal_text": "Convert casual text to formal text: We suggested using space to represent the meanings of words. We came up with a way to learn how to do this in tile space. We also showed that our methods work well in an information retriev"
    },
    {
        "casual_text": "• Using Transformers (DA) for back-translation, figuring out how formal or informal something is, and transferring skills across different tasks.",
        "formal_text": "• Using Transformers (DA) for back-translation, figuring how formal or informal something ... and transferring skills across different tasks. • Using Transformers (DA for back-translation, figuring how formal or"
    },
    {
        "casual_text": "We used the open-source MT toolkit called Moses (Koehn et al., 2007) for our baseline system, just with the default settings. As usual, we optimized the weights for the log-linear combination (eq. 3) using the Minimum Error Rate Training (MERT) method (Och, 2003). We only ran MERT for the baseline system and kept the same weights for all the other systems. Even though we could have re-run MERT when we changed the language model (LM), we didn’t do it on purpose to focus on how different LMs affect the SMT system.",
        "formal_text": "Convert casual text to formal text: We used the open-source MT toolkit called Moses (Koehn et al., 2007) for our baseline system, just with the default settings. As usual, we"
    },
    {
        "casual_text": "In Equation (15), it gives you the index of the last content word in b_j (for example, in Figure 3, C_Last(b_9) equals 16).",
        "formal_text": "Convert casual text to formal: In Equation (15), it gives you the index the last content word in b_j (for example, in Figure 3, C_Last(b_9) equals 16). Convert casual"
    },
    {
        "casual_text": "• We also tried out the multi-speaker TTS task and got pretty much the same results as before.",
        "formal_text": "Convert casual text to formal text: • We also tried out the multi-speaker TTS task and got pretty much the same results before. Convert casual text to formal text: • We also tried out the multi-speak"
    },
    {
        "casual_text": "So far, the studies we've talked about have been all about how often words show up. But there's a cool new method that looks at what words actually mean by using something called diachronic word embeddings (Hamilton et al., 2016). For example, del Tredici and his team (2019) checked out how meanings change quickly on Reddit and found some pretty big shifts, even over just eight years.",
        "formal_text": "Convert casual text to formal text: So far, the studies we've talked about have been all about how often words show up. But there's a cool new method that looks at what words actually mean by using something called di"
    },
    {
        "casual_text": "So, w is this big list of all the class parameters, like w = [w1...wK], and f(. , .) is a joint feature vector that’s broken down using this orthogonal feature thing (Rousu et al., 2006). Basically, this just means that different feature vectors don’t mess with each other. For instance,",
        "formal_text": "Convert casual text to formal text: So, w is this big list of all the class parameters, like w = [w1...wK], and f(. , .) is a joint feature vector"
    },
    {
        "casual_text": "We use model checking to compare the FOL structure and the FOL formula for inference, and we do this with NLTK while also optimizing things (check out Section 3.4 for more details). Basically, we're working with the FOL formula and the FOL structure and making some assumptions.",
        "formal_text": "Convert casual text to formal text: We use model checking to compare the FOL structure and the FOL formula for inference, and we do this with NLTK while also optimizing things (check out Section 3.4 for more details"
    },
    {
        "casual_text": "To make sure a fragment shows up in two parse trees, its labels have to be exactly the same, including all those categories, tags, and features we talked about earlier. The \"h 1 binarization\" thing means that fragments can include parts of bigger chunks; like, a string of kids from a big family of pieces. Figure 1 has an example of a parse tree, but it’s not binarized to keep things simple. The non-terminal labels have a syntactic category (in red) and a function tag (in green). The part-of-speech tags also have extra details (in black) in square brackets. Some labels have extra details that got passed down, and they’re marked with a colon at the beginning.",
        "formal_text": "Convert casual text to formal text: To make sure a fragment shows up in two parse trees, its labels have to be exactly the same, including all those categories, tags, and features we talked about earlier. The \"h"
    },
    {
        "casual_text": "These were coded in FJORTRAN IV for the IBM 7040 computer. They’re super modular and easy to understand, with pretty good documentation. They’re open to computational linguists who want to use them or tweak them for their own needs.",
        "formal_text": "Convert casual text to formal text: These were coded in FJORTRAN IV for the IBM 7040 computer. They’re super modular and easy to understand, with pretty good documentation. They’re open to computational lingu"
    },
    {
        "casual_text": "Here’s the casual version: Check out Table 6 from Finkel and Manning (2009) for the F1 scores across different genres in ONTONOTES (the numbers are from Chiu and Nichols, 2016). BC stands for broadcast conversation, BN is broadcast news, MZ is magazine, NW is newswire, TC is telephone conversation, and WB covers blogs and newsgroups.",
        "formal_text": "Convert casual text to formal text: Here’s the casual version: Check out Table 6 from Finkel and Manning (2009) for the F1 scores across different genres in ONTONOTES (the numbers are from Chiu and"
    },
    {
        "casual_text": "A good way to dive into the idea of auxiliation is to look at auxiliated verbs instead of just focusing on auxiliary verbs. An auxiliated verb can be either an auxiliary or a full verb. Basically, an auxiliated verb is one that takes the non-finite form that its auxiliary needs, comes after any preposition that the auxiliary requires, and has the same subject as its auxiliary. So, you can have a chain of auxiliation when all the verbs in the chain share the same subject. But, just because a chain of verbs has the same subject doesn’t mean they’re always an auxiliary followed by a full verb—it depends on how people define auxiliary verbs in different contexts and for different reasons. In these auxiliation chains, the first verb is only an auxiliary, the last verb is only auxiliated (a full verb), but the verbs in the middle are both auxiliary and auxiliated. This shows that these two categories—auxiliary and auxiliated—aren’t completely separate from each other.",
        "formal_text": "Convert casual text to formal text: A good way to dive into the idea of auxiliation is to look at auxiliated verbs instead of just focusing on auxiliary verbs. An auxiliated verb can be either"
    },
    {
        "casual_text": "In Section 2, we dive into the LU induction task and talk about some related work. Sections 3, 4, and 5 cover our distributional, WordNet-based, and combined models, respectively. After that, Section 6 shares our experimental results and compares them to other approaches. Lastly, Section 7 wraps things up with some final thoughts and ideas for future work.",
        "formal_text": "Convert casual text to formal text: In Section 2, we dive into the LU induction task and talk about some related work. Sections 3, 4, and 5 cover our distributional, WordNet-based, and combined models, respectively."
    },
    {
        "casual_text": "The final model boosts our system's pairwise recall by nearly 22 percentage points, but it comes with an 8-point drop in pairwise precision. Table 2 shows that the same pattern holds for all the other metrics. Once all the passes are done, we take the transitive closure of the clusters we've created and that's what we consider the system's final output.",
        "formal_text": "Convert casual text to formal text: The final model boosts our system's pairwise recall by nearly 22 percentage points, but it comes with an 8-point drop in pairwise precision. Table 2 shows that the same pattern holds for"
    },
    {
        "casual_text": "Our system needs a way to connect the content, function, and form of language in a text. We think the idea of \"text acts\" (Rothkegel, 1984) could be a good fit for this. Text acts are basically speech acts, but they focus on the creation of texts. When we translate, we're making a new text, right?",
        "formal_text": "Convert casual text to formal text: Our system needs a way to connect the content, function, and form of language in a text. We think the idea of \"text acts\" (Rothkegel, 1984) could"
    },
    {
        "casual_text": "For each phrase in the training file, grab its count (C(f)). Then, go through all the unique phrases.",
        "formal_text": "Convert casual text to formal text: For each phrase in the training file, grab its count (C(f). Then, go through all unique phrases. Convert casual text to formal text: For each phrase in the training file,"
    },
    {
        "casual_text": "Neural networks have been doing really well in various AI areas lately (LeCun et al., 2015), and because they’re so good at modeling stuff, a lot of researchers have been diving into creating new recommendation algorithms using Deep Learning. You can check out some examples like Barkan and Koenigstein (2016), He et al. (2017), Hidasi et al. (2015), and Covington et al. (2016).",
        "formal_text": "Convert casual text to formal text: Neural networks have been doing really well in various AI areas lately (LeCun et al., 2015), and because they’re so good at modeling stuff, a lot of researchers"
    },
    {
        "casual_text": "Let's break down the main ideas of this unsupervised NMT framework and talk about where it might run into trouble.",
        "formal_text": "Convert casual text to formal text: Let's break down the main ideas of this unsupervised NMT framework and talk about where might run into trouble. Convert casual text to formal text: Let's break down the main ideas of"
    },
    {
        "casual_text": "(2) Using knowledge distillation methods might take forever to rebuild a huge training set with AG, which goes against the whole point of NAG, which is to make things faster.",
        "formal_text": "(2) Using knowledge distillation methods might take forever to rebuild a huge training set with AG, which goes against the whole point of NAG, which is to make things faster. (3) Using knowledge distillation methods might take forever to rebuild"
    },
    {
        "casual_text": "Alright, so we're organizing the propositions based on the stuff in each slot. Here, we'll first look at ARG2, then MOD-PLACE, MOD-TIME, ARG1, and finally PRED. This way, we group similar propositions together, making it easier to see—just like in Figure 3.",
        "formal_text": "Convert casual text to formal text: Alright, so we're organizing the propositions based on the stuff in each slot. Here, we'll first look at ARG2, then MOD-PLACE, MOD-TIME"
    },
    {
        "casual_text": "We looked at these error types and thought about whether we could tweak the rules to get rid of them. This could help us move closer to the main goal: creating a solid phonological system that works perfectly and adapts foreign words correctly. But here's the thing: when we change one rule, it often messes up the whole system. Sure, it might fix a few mistakes, but it also screws up some of the stuff that was working fine before. Table 4 shows how changing the rules affects the number of errors. As you can see, none of the changes in Table 4 really work well because they all end up increasing the total number of mistakes the system makes. This tells us that the 34 rules we talked about in section 5.1 cover most of the important stuff in Japanese loanword phonology. The errors that are left are more like exceptions that don’t fit into the main rules. It also means there’s a clear limit to how predictable this phonological system can be when it comes to adapting loanwords. Basically, once you hit around 90% accuracy, there’s a bunch of inconsistencies that just can’t be fixed with any simple rule changes.",
        "formal_text": "Convert casual text to formal text: We looked at these error types and thought about whether we could tweak the rules to get rid of them. This could help us move closer to the main goal: creating a solid phonological system that"
    },
    {
        "casual_text": "Alright, we also tried mixing features from both structural and dependency information. Basically, we started with a basic feature set (unigrams with Boolean values), added relative position and author info, and then combined these with different dependency features like LabelList, LabelPrev, LabelAuthor, LabelPrev t, and LabelAuthor t. Table 8 has the results for each of these combinations. As you'd guess, CRFs worked really well with these combined features because they can handle both structural and dependency info. They hit the highest accuracy of 96.86%.",
        "formal_text": "Convert casual text to formal text: Alright, we also tried mixing features from both structural and dependency information. Basically, we started with a basic feature set (unigrams with Boolean values), added relative position"
    },
    {
        "casual_text": "Unlike the Yule+Match setups, there isn't a definite best option for Dictionary+Match combos. Initially, as we bump up the translation probability threshold, the performance improves, but then it drops again (as shown by the 'Average' line in Fig. 3(b)). On average, all the systems work better with a threshold of 0.01, so that's what we went with for our final tests. In this scenario, both Match(1) and Match(2)+Aug runs (orange and green bars) are pretty close in performance, so we decided to use both of these models in our final experiments.",
        "formal_text": "Convert casual text to formal text: Unlike the Yule+Match setups, there isn't a definite best option for Dictionary+Match combos. Initially, as we bump up the translation probability threshold"
    },
    {
        "casual_text": "At each time step t, we figure out the value using a max-pooling thingy across different window size filters. In simpler terms, q c t = max(q l 1, c t, q l 2, c t, . . . , q l L, c t ). The final output from the CNN layer can be written as q c t  T t=1. Taking inspiration from how well this has worked in other NLP tasks (like Luong et al., 2015; Yue-Hei Ng et al., 2015), we stack a bunch of Bi-LSTM (Hochreiter and Schmidhuber, 1997) layers to get a better understanding of the whole question's meaning. The first LSTM layer takes in the convoluted representation of the question, which is q c t  T t=1. Then, q r t = Bi-LSTM(q r t1, q c t ).",
        "formal_text": "Convert casual text to formal text: At each time step t, we figure out the value using a max-pooling thingy across different window size filters. In simpler terms, q c t = max(q"
    },
    {
        "casual_text": "Since the model's accuracy is pretty high, you don't see many mistakes in the output. It's also clear that the chunk model can still predict the right form even when it's shortened to just one character, like on line 5.",
        "formal_text": "Convert casual text to formal text: Since the model's accuracy is pretty high, you don't see many mistakes in the output. It's also clear that the chunk model can still predict the right form even when it's"
    },
    {
        "casual_text": "To make things easier to show, we use the predicted senses of the context words to improve the representation of each sense of a word. We should mention that the context can be bigger too. In equation 5, is just the count of words in the context. And in equation 6,  R hh is a weight matrix that the model can learn from.",
        "formal_text": "Convert casual text to formal text: To make things easier to show, we use the predicted senses of the context words to improve the representation of each sense of a word. We should mention that the context can be bigger too. In"
    },
    {
        "casual_text": "At SiMT, the way data comes in (streaming inputs) makes each spot in the sequence unfair, causing something called position bias. Here, we're diving into a theoretical breakdown of this position bias by looking at how the chances of decoding something differ depending on where it is in the sequence.",
        "formal_text": "Convert casual text to formal text: At SiMT, the way data comes in (streaming inputs) makes each spot in the sequence unfair, causing something called position bias. Here, we're diving into a theoretical breakdown"
    },
    {
        "casual_text": "CNNs have been shown to work well for a bunch of NLP tasks, like sentence classification (Kim, 2014) and matching (Hu et al., 2014). So, in this paper, we're using a CNN-based discriminator, which you can see in Figure 1. To make things easier to talk about later, we'll use \"r\" to refer to the fake response generated by the decoder. Basically, \"r\" represents a bunch of word distributions coming from the decoder RNN's hidden layers. In a regular Seq2Seq model, you'd use these distributions to sample the output response. Now, let's break down the discriminator's setup. First, the input includes: - The word embedding sequence Vq for a given query q. - The word embedding sequence Vr for the human-generated response r. - The word embedding sequence Vr, which is the approximate embedding for the fake response r. All these sequences are either padded with zeros or cut to the same fixed length. Then, two CNNs with shared parameters are used to process Vr and Vr, turning them into higher-level representations. Another separate CNN does the same thing for Vq. We call these abstraction layers (which are the max-pooling layers before the fully connected layers) Ar, Ar, and Aq, corresponding to r, r, and q, respectively.",
        "formal_text": "Convert casual text to formal text: CNNs have been shown to work well for a bunch of NLP tasks, like sentence classification (Kim, 2014) and matching (Hu et al., 2014). So, in"
    },
    {
        "casual_text": "Another way to do sense tagging is using Word Sense Disambiguation (WSD). There are two main types of WSD: Supervised WSD and Knowledge-based WSD. Supervised WSD has a big problem—getting enough labeled data. Even SemCor, the biggest manually tagged dataset, only has 226,036 tagged words. Among different supervised WSD methods, Zhong and Ng (2010) proposed a SVM-based approach, while Melamud et al. (2016) and Yuan et al. (2016) suggested using LSTMs with nearest neighbors classification. Knowledge-based WSD (like Moro et al., 2014; Pasini and Navigli, 2017) doesn’t rely on huge annotated datasets. Instead, it connects words to senses using a predefined sense inventory, like WordNet (Miller, 1992) or BabelNet (Navigli and Ponzetto, 2010). The effectiveness of knowledge-based WSD really depends on how good, complete, and available those resources are.",
        "formal_text": "Convert casual text to formal text: Another way to do sense tagging is using Word Sense Disambiguation (WSD). There are two main types of WSD: Supervised WSD and Knowledge-based WSD."
    },
    {
        "casual_text": "Basically, v(•, i, j) keeps track of how many times a word w i, j (which is part of the group C i) shows up in a certain set. |L| and |T| just tell us how many words are in L and T, respectively. The dominance score for C i is figured out using these numbers.",
        "formal_text": "Convert casual text to formal text: Basically, v(•, i, j) keeps track of how many times a word w i, j (which is part of the group C i)"
    },
    {
        "casual_text": "The two main things that impact how well this model works are picking the right definition for concepts and figuring out their importance. Typically, bigrams (pairs of words) are used to represent concepts (Berg-Kirkpatrick et al., 2011). To figure out how important each concept is, you can either count stuff manually, like how often it shows up in a document, or use supervised learning to get the weights (Li et al., 2013).",
        "formal_text": "Convert casual text to formal text: The two main things that impact how well this model works are picking the right definition for concepts and figuring out their importance. Typically, bigrams (pairs of words) are used to"
    },
    {
        "casual_text": "In this add-on, we’re going to break down how we collected our data (Section A.1), explain the nitty-gritty of our algorithm (Section A.2), and dive into the results of our alignment process (Section A.3).",
        "formal_text": "Convert casual text to formal text: In this add-on, we’re going to break down how we collected our data (Section A.1), explain the nitty-grritty of our algorithm (S"
    },
    {
        "casual_text": "First off, we checked out single task models to make sure our setups could hold their own against the best out there. We tested on dependency parsing (EWT), GLUE classification tasks, and machine translation (WMT14 DE-EN (Bojar et al., 2014), IWSLT15 EN-VI (Cettolo et al., 2014)) using mBERT for our embeddings. Check out Table 2 for our results on the test sets compared to other studies. For all the UD tasks, we did a bit better, but for the GLUE tasks, we consistently scored lower than the benchmarks. This is probably because of how we fine-tuned things, since the actual implementations are pretty similar. The biggest dips were in the machine translation tasks, which suggests that we might need to tweak our fine-tuning and pre-processing for these specific tasks.",
        "formal_text": "Convert casual text to formal text: First off, we checked out single task models to make sure our setups could hold their own against the best out there. We tested on dependency parsing (EWT), GLUE classification"
    },
    {
        "casual_text": "Here’s the list of suggestions, based on the best benchmarks and our model. There are up to three comments: (1) handling nulls in return types, (2) a reranked generation model, and (3) a reranked edit model. The order is random. If the same prediction shows up more than once, we group it into one suggestion and give credit to all the models involved if the user picks it. Also, we skip any prediction that’s exactly the same as the old comment (C old) because it’d just confuse the user. We also left out 6 examples from the test set where all three models predicted C old for the updated comment.",
        "formal_text": "Convert casual text to formal text: Here’s the list of suggestions, based on the best benchmarks and our model. There are up to three comments: (1) handling nulls in return types, (2) a reranked"
    },
    {
        "casual_text": "Okay, so N is just the number of tokens in C' (new). This model tends to favor comments that are more likely for M (new) and match the general style of comments. Now, about the similarity to C (old). So far, our model focuses on making accurate edits, but we also think edits should be as small as possible (kind of like using Levenshtein distance for spelling corrections). To make sure we pick predictions that update the comment accurately with the least changes, we use similarity to C (old) as a way to rerank options. We measure how similar the predicted comment is to C (old) using ME-TEOR (Banerjee and Lavie, 2005). The reranking score for each option is a mix of the original beam score, how likely the generation is, and the similarity to C (old), with weights of 0.5, 0.3, and 0.2, respectively. These weights were tweaked based on validation data.",
        "formal_text": "Convert casual text to formal text: Okay, so N is just the number of tokens in C' (new). This model tends to favor comments that are more likely for M (new) and match the general style of comments."
    },
    {
        "casual_text": "We used a training set with 91,544 examples from the MIMIC-CXR v2.0 dataset (Johnson et al., 2019). Each example is a free-text chest radiology report with sections like Background, Findings, and Impression. For validation, we had two sets, each with 2,000 reports. One came from MIMIC, and the other was from the Indiana University Chest X-Rays Report dataset (Indiana-University). The test set has 300 reports from Indiana University and 300 from Stanford University School of Medicine. All the report sections were tokenized using the Stanford CoreNLP tokenizer (Manning et al., 2014).",
        "formal_text": "Convert casual text to formal text: We used a training set with 91,544 examples from the MIMIC-CXR v2.0 dataset (Johnson et al., 2019). Each example is a"
    },
    {
        "casual_text": "Using hand-crafted rules to rearrange sentences before translation isn’t always dependable or helpful. That’s why forcing these rules on every training sentence before aligning words or extracting phrases can actually mess up the translation quality. Instead, we figured out a better approach: we only took the sentences that were actually reordered by the rules, along with their translated versions, and added those to the original training data. This way, the word alignment process, which uses an iterative algorithm, might work a bit better because it has to align both the reordered and the original sentences to the same translated sentence.",
        "formal_text": "Convert casual text to formal text: Using hand-crafted rules to rearrange sentences before translation isn’t always dependable or helpful. That’s why forcing these rules on every training sentence before aligning words or extracting phrases"
    },
    {
        "casual_text": "In this paper, we introduce a new challenge called Sketchy Scene Captioning. The idea is to create detailed descriptions for sketchy scenes using sequence learning. To make this happen, we built a fresh dataset with 1,000 sketchy scenes, each paired with sentence-level and paragraph-level captions. The results show that our captioning models can identify the main objects in these scenes and understand how they interact. This proves that generating multi-level captions for sketchy scenes is totally doable. Moving forward, we’re planning to expand the dataset and figure out how to better represent sketchy scenes for captioning. We hope this work sparks more research into understanding sketchy scenes better.",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce a new challenge called Sketchy Scene Captioning. The idea is to create detailed descriptions for sketchy scenes using sequence learning. To make this happen, we built"
    },
    {
        "casual_text": "The French dataset had the worst performance, but the fact that the Oracle (a perfect model) also did poorly suggests that the issue might be with the dataset itself or how the data was prepared, not the model. LEAD4 works well because it focuses on the position of important sentences. TextRank, on the other hand, relies on the content of those sentences. NeuSum's better performance could be due to how it combines both the position and content of sentences.",
        "formal_text": "Convert casual text to formal text: The French dataset had the worst performance, but the fact that the Oracle (a perfect model) also did poorly suggests that the issue might be with the dataset itself or how the data was prepared, not the"
    },
    {
        "casual_text": "Alright, so let's break this down in simpler terms. We've got: - l which is just the number of the layer we're looking at. - N_i represents the neighbors of a specific entity e_i. -  is the activation function we use. - norm(•) is a way to normalize things. - V(l) and b(l) are the parameters (kind of like settings) for the l-th layer. Now, after we process each entity e_i through all the layers of the GCN (Graph Convolutional Network), we take all the different representations we got and smash them together into one big representation for each entity.",
        "formal_text": "Convert casual text to formal text: Alright, so let's break this down in simpler terms. We've got: - l which is just the number of the layer we're looking at. -"
    },
    {
        "casual_text": "The sentence hypothesis selection module just looks at the final translation results from different systems, including the one from the glass-box combination. For every source sentence, it picks the \"best\" translation by using some feature functions to decide.",
        "formal_text": "Convert casual text to formal text: The sentence hypothesis selection module just looks at the final translation results from different systems, including the one from the glass-box combination. For every source sentence, it picks the \"best\" translation by using"
    },
    {
        "casual_text": "The languages you're working with can really make or break how well joint multilingual modeling and cross-lingual transfer work in cross-lingual NLP. Bender (2011) and Ponti et al. (2019) have pointed this out. Figuring out how these languages differ and how to measure that difference is usually the first step to creating more reliable NLP tech that works across multiple languages. O'Horan et al. (2016), Bjerva et al. (2019), and Ponti et al. (2019) have all talked about this. For example, picking the right source languages is super important if you want to successfully transfer things like dependency parsers or POS taggers. Naseem et al. (2012) and de Lhoneux et al. (2018) have shown this. Another example is machine translation—even if everything else like training data size and domain similarity stays the same, the quality of the translation can still depend a lot on how similar the languages are and their specific properties. Kudugunta et al. (2019) have highlighted this too.",
        "formal_text": "Convert casual text to formal text: The languages you're working with can really make or break how well joint multilingual modeling and cross-lingual transfer work in cross-lingual NLP. Bender (2011) and Ponti et al"
    },
    {
        "casual_text": "This paper was all about concept-predicate pairs without any context. Basically, we looked at sentences with quantifiers where the whole set of something was involved, like \"all birds\" or \"some cars.\" Now, the next step is to dive into sentences with context-specific subsets. For example, saying \"taxis are yellow\" could mean something different if you're talking about London versus New York. In our future work, we'll test our system on these kinds of examples, using fancy vector representations that take context into account, like the ones suggested by folks like Erk and Padó (2008) and Dinu and Lapata (2010). We also want to point out that the models we made here are a bit different from the usual formal semantics models. Instead of directly representing the real world, they kind of represent shared beliefs about the world, based on a dataset with feature norms. This means we need to tweak the standard denotation function and swap out the truth function for something we're calling a 'plausibility' function. This would show how likely it is that a typical person would agree with a given sentence. While this is a big shift from the usual model theory thinking, we think it could be a cool idea. It might let us keep the awesome benefits of set theory in a way that makes sense from a cognitive standpoint.",
        "formal_text": "Convert casual text to formal text: This paper was all about concept-predicate pairs without any context. Basically, we looked at sentences with quantifiers where the whole set of something was involved, like \"all birds\" or"
    },
    {
        "casual_text": "BLI test accuracy doesn’t always match up with how well the model performs on real tasks because it skips over words from the training dictionary. A simple solution would be to include those training words in the BLI test set, but figuring out the right mix between training and test words is tricky. BLI accuracy treats all test words as equally important, but that’s not true—some words matter more depending on the task. For example, \"the\" doesn’t really matter in document classification, but it’s super important in dependency parsing. So, instead of focusing on BLI, it’d make more sense to look at how the model performs on actual tasks.",
        "formal_text": "Convert casual text to formal text: BLI test accuracy doesn’t always match up with how well the model performs on real tasks because it skips over words from the training dictionary. A simple solution would be to include those training words"
    },
    {
        "casual_text": "We used GIZA++ (Och and Ney, 2003) to align the mix-domain corpus and then combined the results with grow(-diag)-final-and (Koehn et al., 2003) to make it symmetrical. We kept the phrase length to a max of seven words. For the language models (LMs), we used 4-grams with Kneser-Ney smoothing, trained on 2.2 million English sentences from Europarl and added 248.8K sentences from the News Commentary Corpus (WMT 2013). We fine-tuned the system using k-best batch MIRA (Cherry and Foster, 2012) and averaged the BLEU scores over multiple runs.",
        "formal_text": "Convert casual text to formal text: We used GIZA++ (Och and Ney, 2003) to align the mix-domain corpus and then combined the results with grow(-diag)-final-and (Ko"
    },
    {
        "casual_text": "Good news! The JUMAN morphological analyzer can work with phrases made up of multiple words. All we have to do is take care of the POS tagging.",
        "formal_text": "Convert casual text to formal text: Good news! The JUMAN morphological analyzer can work with phrases made up of multiple words. All we are taking care of the POS tagging."
    },
    {
        "casual_text": "We got a total of 33,882 text responses from 8,502 users through AMT and Qualtrics (with 126 responses missing, which is just 0.37%). The average length of the responses for the four different categories was between 179 and 226 characters. The people who answered through AMT were more similar to the general U.S. population in terms of race and gender. In a few sentences, just tell us what makes you most anxious or worried when you visit the doctor's office.",
        "formal_text": "Convert casual text to formal text: We got a total of 33,882 text responses from 8,502 users through AMT and Qualtrics (with 126 responses missing, which is just 0.37%). The average"
    },
    {
        "casual_text": "Using word-level details, like word n-grams and word length, just feels right in a word-based segmenter. On the other hand, character-level details, such as character n-grams, make more sense in a character-based segmenter. Sun (2010) did a deep dive into comparing these two methods, looking at both the theory and real-world results. Word-level info is better at showing how words connect in context, while character-level info shines when it comes to breaking down words and understanding their structure.",
        "formal_text": "Convert casual text to formal text: Using word-level details, like word n-grams and word length, just feels right in a word-based segmenter. On the other hand, character-level details, such"
    },
    {
        "casual_text": "To make things simpler and less resource-intensive, we suggest picking important representations one by one to create angles, which introduces the hyperparameters k1 and k2. We tried out different values for k1 and k2 by using token-level and sample-level triplet-wise relationships to guide the student model. To cut down on the options we had to consider, we just made k1 equal to k2. We plotted the accuracy for different values of k1 and k2, and you can see that in Figure 2. For token-level tasks, we noticed that bumping up k1 and k2 helps boost accuracy when they're small. But once k1 and k2 hit 20 or more, the accuracy starts to fluctuate. So, we decided to go with k1 = k2 = 20 for token-level angle calculations. On the other hand, for sample-level features using triplet-wise relationships, the accuracy just keeps going up as we increase k1 and k2. So, we just set k1 and k2 to match the batch size.",
        "formal_text": "Convert casual text to formal text: To make things simpler and less resource-intensive, we suggest picking important representations one by one to create angles, which introduces the hyperparameters k1 and k2. We tried out"
    },
    {
        "casual_text": "Alright, so rows 6 to 9 show the results for different versions of the A-C and B-C setups, where the B nodes are fully connected, and we’re tweaking how the A and C nodes are linked. The best results are right here, with improvements over the regular DNN B in terms of MAP (+0.61), AvgRec (+0.69), and MRR (+1.05). But the real standouts are the accuracy (+2.18) and F1 score (+11.25 points). This is kind of a big deal since we didn’t expect much improvement in subtask B.",
        "formal_text": "Convert casual text to formal text: Alright, so rows 6 to 9 show the results for different versions of the A-C and B-C setups, where the B nodes are fully connected, and we’re tweaking"
    },
    {
        "casual_text": "Hey, just so you know, our measure isn't totally perfect when it comes to comparing how similar two languages are in terms of their gender systems. For example, it doesn't always give a score of 1 when we compare the same language to itself (that's what the diagonal in Figure 1 shows). But, it can still be seen as a pretty good estimate. Basically, you could make it more accurate by dividing the score for transferring from language X to Y by the score for transferring from X to X.",
        "formal_text": "Convert casual text to formal text: Hey, just so you know, our measure isn't totally perfect when it comes to comparing how similar two languages are in terms of their gender systems. For example, it doesn't always"
    },
    {
        "casual_text": "Section 5.2 shows that better control units improve the quality of evaluations. But here's the catch: the time it takes to gather all the data shot up by 8 times. Table A.4 breaks down the evaluation period (how many days it takes to collect everything), grading time (how many hours are spent actually grading translations), and the average time per assignment. For the LOC+GOLD bw experiments, grading each task took anywhere from 2.5 to 6.5 hours. On the flip side, the time it takes to finish the whole evaluation really depends on the language. Some, like Hindi, Tagalog, and Spanish, take just 2 days, while others, like Russian, Chinese, and Korean, can take over 2 weeks. Looking at the average time to judge a single HIT, it seems the quicker the evaluation, the less reliable the judgments tend to be.",
        "formal_text": "Convert casual text to formal text: Section 5.2 shows that better control units improve the quality of evaluations. But here's the catch: the time it takes to gather all the data shot up by 8 times. Table A.4"
    },
    {
        "casual_text": "Lately, as neural network techniques have gotten better, deep-learning methods have been introduced and shown to work well for this task. Chen and Ng (2016) were the first to use a feed-forward neural network framework, where zero anaphoras are identified based on the word before them and their headword. But, their model doesn’t consider the context around the zero anaphora, which means it misses out on some important information. To fix this, some researchers have tried to improve things by looking at related text. Yin et al. (2017a) came up with a new memory-based neural network model that learns to encode zero anaphoras by looking at the surrounding text and mentions of antecedents. They use a multi-hop architecture to pull out abstract information from external memories, which helps explain zero anaphoras better. Yin et al. (2017b) focused on encoding global information for candidates, introducing a hierarchical candidate encoder that models the candidates more effectively. Liu et al. (2017) looked into creating fake training data for zero anaphora resolution. They used a two-step training strategy to deal with the differences between the fake data and real data. While these methods can figure out the meaning of zero anaphoras by looking at the context, they treat all words the same, ignoring the fact that different words carry different levels of importance.",
        "formal_text": "Convert casual text to formal text: Lately, as neural network techniques have gotten better, deep-learning methods have been introduced and shown to work well for this task. Chen and Ng (2016) were the first to use a feed"
    },
    {
        "casual_text": "Instead of using the usual evaluation method, we mess up the test data by adding noise to see how well our model holds up. When dealing with short texts, like tweets, it's pretty normal to come across words that aren't recognized because of typos, abbreviations, or different ways people talk (Han and Baldwin, 2011; Eisenstein, 2013). To mimic this, we randomly swap out words in each document with a special symbol to represent missing words. This happens to each word with a chance of , which can be 0, 0.1, 0.2, or 0.3.",
        "formal_text": "Convert casual text to formal text: Instead of using the usual evaluation method, we mess up the test data by adding noise to see how well our model holds up. When dealing with short texts, like tweets, it's pretty normal"
    },
    {
        "casual_text": "So,  top t1  Q || is just a fancy way of saying it’s a one-hot vector that represents the symbol sitting at the top of our stack. Meanwhile, (q t1, x t )  Q |Q||| is another one-hot vector, but this one is special for each combination of the current state (q) and the input (x). Basically, h is a way to describe all this in a compact form.",
        "formal_text": "Convert casual text to formal text: So,  top t1  Q || is just a fancy way of saying it’s a one-hot vector that represents the symbol sitting at the top of"
    },
    {
        "casual_text": "(2) Connecting the arguments in different speeches that disagree with each other.",
        "formal_text": "Convert casual text to formal text: (2) Connecting the arguments in different speeches that disagree eeeeeeeeeeeeeeeeeeeeeeeeeeeeee"
    },
    {
        "casual_text": "Slang is just a casual, offbeat part of language that often pops up in specific settings or follows trends in society (Dumas and Lighter, 1978). It helps people feel connected to a group (González, 1998; Bembe and Beukes, 2007; Carter, 2011) or even to a particular generation (Citera et al., 2020; Earl, 1972; Barbieri, 2008). Mattiello (2005) points out how slang adds new words to the language and follows its own unique way of forming words. Based on this, Kulkarni and Wang (2018) came up with a data-driven model to mimic how slang words are created, as Mattiello described. Some people have talked about how slang words tend to be short-lived (González, 1998; Carter, 2011), but this idea hasn’t really been tested using computational methods before.",
        "formal_text": "Convert casual text to formal text: Slang is just a casual, offbeat part of language that often pops up in specific settings or follows trends in society (Dumas and Lighter, 1978). It helps people feel connected to"
    },
    {
        "casual_text": "K R (e 1, e 2 ) = SK(s 1 1, s 1 2 ) + SK(s 2 1, s 2 2 ) (2) - SK(s 1 1, s 2 2 ) - SK(s 2 1, s 1 2 )",
        "formal_text": "K R (e 1, e 2 ) = SK(s 1 1, s 1 2 ) + SK(s 2 1, s 2 2 ) (2) - SK(s 1 1, s 2 2"
    },
    {
        "casual_text": "The Vanilla Information Bottleneck (VaniIB) uses the original information bottleneck idea on SpanNER, which was developed by Alemi et al. (2016). Unlike our approach, it just squashes all the input information together without any special treatment.",
        "formal_text": "Convert casual text to formal text: The Vanilla Information Bottleneck (VaniIB) uses the original information bottleneck idea on SpanNER, which was developed by Alemi et al. (2016). Unlike our approach"
    },
    {
        "casual_text": "Threading is all about making logical files that are customized for whatever app you're using. Key parts of threading include being able to pick and choose from a bunch of different pools, deciding which ones are more important based on what task you're doing, usually from the most specific to the most general. In interactive apps, you can also tweak these files by giving read/write access to the top pools in the list. When it comes to how apps use this, there are three main ways: 1. **Single access**: This just grabs data from the first pool in the list where the term shows up. 2. **Multiple access**: This lets you get data from any pool in the list where the term appears. 3. **Composite access**: This is a special version of multiple access that pulls together data from all the pools in the list where the term is found, creating a kind of data mix.",
        "formal_text": "Convert casual text to formal text: Threading is all about making logical files that are customized for whatever app you're using. Key parts of threading include being able to pick and choose from a bunch of different pools,"
    },
    {
        "casual_text": "Creating adversarial examples—inputs designed to trick a model—has become a big deal in figuring out where models are weak. Plus, throwing these adversarial examples into the training process, known as adversarial training, is now seen as one of the best ways to make models more robust. While there isn’t a ton of research out there on adversarial examples for NLP, some work has been done on tasks like reading comprehension (Jia and Liang, 2017), text classification (Samanta and Mehta, 2017; Wong, 2017; Liang et al., 2018; Alzantot et al., 2018), machine translation (Zhao et al., 2018; Ebrahimi et al., 2018; Cheng et al., 2018), and dialogue systems.",
        "formal_text": "Convert casual text to formal text: Creating adversarial examples—inputs designed to trick a model—has become a big deal in figuring out where models are weak. Plus, throwing these adversarial examples into"
    },
    {
        "casual_text": "Basically, the issues we talked about earlier, like sounding too formal or not being used enough, aren't as big of a deal when the languages are less similar. Also, nouns that are used a lot are less affected by the problem of having too many endings.",
        "formal_text": "Convert casual text to formal text: Basically, the issues we talked about earlier, like sounding too formal or not being used enough, aren't as big a deal when the languages are less similar. Also, noun"
    },
    {
        "casual_text": "Here's how this paper is laid out: Section 2 talks about our cross-lingual language modeling with syntactic reordering. Section 3 dives into speech recognition using cross-lingual language models. Sections 4 and 5 cover the experimental setup and the results we got. Finally, we wrap things up at the end of the paper.",
        "formal_text": "Convert casual text to formal text: Here's how this paper is laid out: Section 2 talks about our cross-lingual language modeling with syntactic reordering. Section 3 dives into speech recognition using cross-lingual"
    },
    {
        "casual_text": "Share the outcomes of the project at conferences, scientific meetups, and industry events. This will make a big difference not just in the countries involved but also in other places.",
        "formal_text": "Convert casual text to formal text: Share the outcomes of the project at conferences, scientific meetups, and industry events. This will make a big difference not just in the countries involved but also other places."
    },
    {
        "casual_text": "Some categories have way fewer questions than others—like astronomy, which only has 331 questions. So, we’re just focusing on literature and history, since they make up over 40% of the whole set. That gives us 21,041 history questions and 22,956 literature questions to work with.",
        "formal_text": "Convert casual text to formal text: Some categories have way fewer questions than others—like astronomy, which only has 331 questions. So, we’re just focusing on literature and history, since they make up over 40%"
    },
    {
        "casual_text": "We came up with a way to break down text into different topics using something called semantic cohesion scores. This method works for any type of content and doesn’t need any fancy training or special word databases. We calculate these scores using a mix of document indexing that involves spectral embedding in a space of hidden concepts for nouns. It also keeps proper nouns and other unique details of the text collection the same. We kind of borrowed the idea of lexical chains but made it simpler, which lets us use inner products to measure how similar things are. This similarity score looks at the meaning behind nouns, not just if the words match. Our semantic cohesion method worked pretty well for splitting text into topics.",
        "formal_text": "Convert casual text to formal text: We came up with a way to break down text into different topics using something called semantic cohesion scores. This method works for any type of content and doesn’t need any fancy training or special"
    },
    {
        "casual_text": "We think this new approach can make better use of the dictionary, and here's why. First off, even if distant supervision messes up the boundaries of an entity, most of the connections inside the entity stay intact. What's even cooler is that, when it comes to single-word entities, they're more likely to be wrong labels compared to multi-word ones. But here's the thing: with the Tie or Break method, those wrong single-word labels won't cause any issues. Whether the unigram is a real entity or a mistake, it always adds two Break labels around it.",
        "formal_text": "Convert casual text to formal text: We think this new approach can make better use of the dictionary, and here's why. First off, even if distant supervision messes up the boundaries of an entity, most of the connections"
    },
    {
        "casual_text": "This rule is basically a simpler version of the FFP because it doesn't bother with the difference between inherited and instantiated features. Check out section 5.3 if you want to dive deeper into that.",
        "formal_text": "Convert casual text to formal text: This rule is basically a simpler version of the FFP because it doesn't bother with the difference between inherited and instantiated features. Check out section 5.3 if you want to dive"
    },
    {
        "casual_text": "Sure! Here's the informal version: \"Check out this link to a writing prompt on Reddit: https://www.reddit.com/r/WritingPrompts/ And here's another one for a tool called Mallet: http://mallet.cs.umass.edu\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: \"Check out this link a writing prompt on Reddit: https://www.reddit.com/r/WritingPrompts"
    },
    {
        "casual_text": "A* matches all numbers, but it doesn't really match many regular words unless they're acronyms written in capital letters. So, we thought, \"Why not add A* to the list of word shape tokens we want to ignore?\" We're testing this out now to see how it works.",
        "formal_text": "Convert casual text to formal text: A* matches all numbers, but it doesn't really match many regular words unless they're acronyms written in capital letters. So, we thought, \"Why not add A* to the"
    },
    {
        "casual_text": "Let’s say c_ij represents the number of things (like tweets, for example) that are actually in category j but the system thinks they’re in",
        "formal_text": "c_ij represents the number of things (like tweets, for example) that are actually in category j but the system thinks they’re in category j. represents the number of things (like tweets"
    },
    {
        "casual_text": "Basically, we think our study is a cool move toward combining computational social science and derivational morphology. In the next phase, we plan to tweak our approach even more to handle MFEP better.",
        "formal_text": "Convert casual text to formal text: Basically, we think our study is a cool move toward combining computational social science and derivational morphology. In the next phase, we plan to tweak our approach even more to"
    },
    {
        "casual_text": "So, N + (0, 1) and N  (0, 1) are just the positive and negative parts of the normal distribution, but cut off at 0.",
        "formal_text": "Convert casual text to formal text: So, N + (0, 1) and N  (0, 1) are just the positive and negative parts of the normal distribution, but cut off at 0. So, N + (0, 1) and N"
    },
    {
        "casual_text": "To tackle the issue mentioned above, we're suggesting a fresh approach to data augmentation. We'll create synthetic samples by tweaking the latent representations of grammatically correct sentences. Here's how it works: we start with a specific grammatical error type and its classifier. This helps us find a perturbation vector in the latent space. We then add this vector to the latent representation of the input sentence and use a decoder to produce a sentence with the target grammatical error. This method allows us to generate various errors by simply changing the target error type. To make it even better, we've added some rules to help create specific local errors like spelling mistakes or incorrect punctuation.",
        "formal_text": "Convert casual text to formal text: To tackle the issue mentioned above, we're suggesting a fresh approach to data augmentation. We'll create synthetic samples by tweaking the latent representations of grammatically correct sentences"
    },
    {
        "casual_text": "Basically, our work focuses on three main areas: looking into Russian trolls, classifying text, and dealing with semi-supervised learning.",
        "formal_text": "Convert casual text to formal: Basically, our work focuses on three main areas: looking into Russian trolls, classifying text, and dealing with semi-supervised learning."
    },
    {
        "casual_text": "Clinic150 is a dataset (created by Larson et al. in 2019) that divides 150 different user intents into perfectly balanced groups. It was originally made for a chatbot to spot queries that don't fit the expected categories. However, in our work, we follow what Mehri et al. did in 2020 and ignore the out-of-scope class, focusing only on the 150 labeled categories.",
        "formal_text": "Convert casual text to formal text: Clinic150 is a dataset (created by Larson et al. in 2019) that divides 150 different user intents into perfectly balanced groups. It was originally made for a chat"
    },
    {
        "casual_text": "Like before, we added a new feature template to handle label bigrams. We picked feature templates (using the same setup as before) for budget sizes of 100, 200, and 300. We compared our results with MIRA (which uses all the features) and the sparseptron with a regular Lasso regularizer  L 1 , testing different values of C = 1/( N ). The results are in Table 2. We found that the template-based group-Lasso method works better—it’s more accurate and more compact. Also, being able to drop entire feature templates (instead of just individual features) makes testing faster than with the standard Lasso: fewer templates need to be processed, which speeds up the scoring.",
        "formal_text": "Convert casual text to formal text: Like before, we added a new feature template to handle label bigrams. We picked feature templates (using the same setup as before) for budget sizes of 100, 200, and 300. We"
    },
    {
        "casual_text": "The tense, aspect, and modality (TAM) of the verbs in both sentences can help clear up the confusion. But you'd also need to know who's doing what in the sentence and do a bit of grammar analysis to figure it all out.",
        "formal_text": "Convert casual text to formal text: The tense, aspect, and modality (TAM) of the verbs in both sentences can help clear up the confusion. But you'd also need to know who's doing what"
    },
    {
        "casual_text": "NMT systems can be pretty sensitive to noise in the training data, as shown by Khayrallah and Koehn in 2018. Here, noise refers to parts of the data that mess up the quality of the system's output. So, it's super important to align multilingual texts really well and get rid of any misalignments or bad translations that could hurt performance. In their study looking at how different types of noise affect MT quality, untranslated and misaligned segments caused the most trouble. Misaligned segments were the biggest issue in the ParaCrawl 1 parallel corpus they used, showing up twice as often as good segments. But misalignments can vary a lot—a segment might have an extra word, or it could have double the content of its counterpart, or anything in between. It's really helpful to figure out how different types and amounts of noise impact things, why it's bad to have noise in the first place, and whether some kinds of noise are less harmful than others. This brings us to our first research question:",
        "formal_text": "Convert casual text to formal text: NMT systems can be pretty sensitive to noise in the training data, as shown by Khayrallah and Koehn in 2018. Here, noise refers to parts of the data that mess up"
    },
    {
        "casual_text": "But, super high scores might not tell the whole story. There’s some debate about whether these probing tasks actually check if the model truly understands the structure of language or if the results are just being interpreted correctly (Hewitt and Liang, 2019; Zhang and Bowman, 2018; Voita and Titov, 2020; Pimentel et al., 2020b). To figure this out, the next part will look at different probing methods used with language models and how we judge how well a probe works.",
        "formal_text": "Convert casual text to formal text: But, super high scores might not tell the whole story. There’s some debate about whether these probing tasks actually check if the model truly understands the structure of language or if the results"
    },
    {
        "casual_text": "Event argument detection is super important for event extraction. It’s kind of like semantic role labeling (SRL), where the goal is to figure out which words or phrases fit into the roles of an event. But with event arguments, things can get a bit more complicated—they might not just be within the same sentence. Sometimes, they can be in different parts of the document, like when an argument is implied or hidden somewhere else. Take a look at Figure 1 for an example: in the purchase event triggered by the word \"bought,\" the money argument is actually found in the sentence before.",
        "formal_text": "Convert casual text to formal text: Event argument detection is super important for event extraction. It’s kind of like semantic role labeling (SRL), where the goal is to figure out which words or phrases fit into the roles of an"
    },
    {
        "casual_text": "Alright, let’s dive into a situation where the bad guy is using different versions of fine-tuned language models (LMs) that all come from the same original pre-trained LM. The person trying to figure out which fine-tuned model is being used knows about the fine-tuning and is trying to tell them apart. Table 7 shows how well this works when the text is made by fine-tuning GPT2 on four different subreddits: r/changemyview, r/technology, r/relationships, and r/conspiracy. XLNet-FT did the best again, but this time its accuracy was under 60%. Weirdly, CNN, which didn’t do well in earlier tests, performed almost as good as XLNet-FT this time. GPT2 barely did better than just guessing randomly (which would be 25%). Overall, the differences in the text made by these fine-tuned versions of the same pre-trained LM aren’t big enough for the attribution methods we’re using to work well. Our early look at the data suggests there’s a connection between the mistakes the attributor makes and how similar the vocabularies of the subreddits are. But we need more research to figure out why this isn’t working better and to come up with ways to improve how we identify the source of text from fine-tuned LMs.",
        "formal_text": "Convert casual text to formal text: Alright, let’s dive into a situation where the bad guy is using different versions of fine-tuned language models (LMs) that all come from the same original pre-t"
    },
    {
        "casual_text": "You can find the system on the LINDAT/CLARIN language resource repository, and it's all open source, just like the current version and the related lexicons and corpora. The search interface works with any browser, so you can use it freely.",
        "formal_text": "Convert casual text to formal text: You can find the system on the LINDAT/CLARIN language resource repository, and it's all open source, just like the current version and the related lexicons and corpor"
    },
    {
        "casual_text": "One popular approach people have been looking into is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe and Schwenk, 2019). This involves taking a pretrained model, fine-tuning it on a lot of data in the source language (like English), and then testing it directly on a different language without any additional training. Surprisingly, this often works pretty well (Wu and Dredze, 2019; Hu et al., 2020). But, there are some issues. The results you see in papers can vary a lot and aren’t always easy to replicate (Keung et al., 2020a; Rios et al., 2020). Plus, languages that are very different from English don’t perform as well as those that are more similar (Hu et al., 2020; Liang et al., 2020). Lauscher et al. (2020) argue that few-shot crosslingual transfer might be a better way to go. In this approach, you fine-tune the model on the source language first, and then give it a small number (like 10-100) of examples from the target language to fine-tune on again. This little bit of extra training can significantly boost performance in the target language without needing much extra effort or data (Garrette and Baldridge, 2013; Hedderich et al., 2020).",
        "formal_text": "Convert casual text to formal text: One popular approach people have been looking into is zero-shot crosslingual transfer (Pires et al., 2019; Conneau and Lample, 2019; Artetxe"
    },
    {
        "casual_text": "Okay, let’s break this down in a simpler way: In these explanations, x and y are just numbers that tell us which row we're looking at in the data for X and y. Now, z_y is a vector from Z that’s connected to the response at position y. Think of it as a special piece of information tied to that spot. Next, we have W_h, Z, W_IRF(1), Z, and W_s, Z, which are all matrices (basically tables of numbers) used to help with the z_y stuff. They’re there to process the random effects, and they come in different shapes and sizes depending on what they’re doing. Finally, we’re talking about some adjustments (offsets) for the hidden state at step y (h_Z_y), and also the weights and biases of the first layer of this thing called IRF. These are just parts of the model that help it learn and make predictions.",
        "formal_text": "Convert casual text to formal text: Okay, let’s break this down in a simpler way: In these explanations, x and y are just numbers that tell us which row we're looking at in the data for"
    },
    {
        "casual_text": "We wanted to see how well different training corpora represent someone's learning experience, so we gathered corpora for two people. We compared these individual corpora to a standard one (from Goldhahn et al., 2012). Using word2vec models (Mikolov et al., 2013), we calculated the semantic long-term memory structure for both the individuals and the standard corpus. Next, we looked at WPs for a sample of 45K sentences from Wikipedia and picked 134 sentences where the norm-based and individual WPs didn't correlate much. This was to avoid issues with multicollinearity in our eye movement data analysis. Our single-predictor and multiple regression analyses showed that the norm-based WP didn't have any significant effect on eye movement measures. One reason for this could be that we replaced words with a WP of 0 with ones that are common in the individual corpora but not in the norm-based one. This might have made it easier to predict using the individual corpora. The way we selected our stimuli was designed to make the eye-tracking experiment work well for our participants' individual knowledge, but this might not represent other types of knowledge as well.",
        "formal_text": "Convert casual text to formal text: We wanted to see how well different training corpora represent someone's learning experience, so we gathered corpora for two people. We compared these individual corpora to a standard"
    },
    {
        "casual_text": "For instance, a different way to describe the word \"Apple\" could be when you're talking about the fruit, not the tech company.",
        "formal_text": "Convert casual text to formal text: For instance, a different way to describe the word \"Apple\" could be when you're talking about the fruit, not the tech company. Convert casual text to formal text: For instance"
    },
    {
        "casual_text": "These findings suggest that mined coreference chains are a pretty handy resource and offer insights that work well alongside other techniques. While adding coreference-based embeddings does boost performance in antonym classification, the experiments also highlight that relying solely on coreference-based embeddings usually doesn’t work as well as sticking with text-based embeddings. This makes sense when you think about it—the amount of training data for the word embeddings is different in both cases. Coreference chains only cover a tiny fraction of the word-word relationships that the word2vec skip-gram model gets when it’s applied to raw text. If both methods had similar amounts of training data, we’d probably see similar results.",
        "formal_text": "Convert casual text to formal text: These findings suggest that mined coreference chains are a pretty handy resource and offer insights that work well alongside other techniques. While adding coreference-based embeddings does boost performance in ant"
    },
    {
        "casual_text": "We're planning to dive deeper into feature engineering for instance-based SRL. Specifically, we want to look into ways to automatically pick out the best features, especially when it comes to composite features. We're also thinking about testing how well things like word clusters or word embeddings work in instance-based SRL.",
        "formal_text": "Convert casual text to formal text: We're planning to dive deeper into feature engineering for instance-based SRL. Specifically, we want to look into ways to automatically pick out the best features, especially when it comes to composite features."
    },
    {
        "casual_text": "Okay, so in the intro rules, the result's meaning comes from a lambda-abstraction, which is based on the meaning of the assumption that gets discharged. This assumption can be represented by a variable that matches the right type. The connection between lambda-abstraction and function application is explained by the t-equality rule in (10), where c[/lV ] means \"replace # with //\". (If you want all the details, check out llindley and Seldin's 1986 work on the typed lambda-calculus.) Since only one assumption gets dropped, the resulting lambda-terms have this cool feature where each binder only connects to one variable occurrence. We call this the 'single-bind' property (thanks to van Benthem for pointing that out in 1983). The rules in (9) are kind of like the usual conditionalization rule in natural deduction, but the difference is that the latter lets you drop any number of assumptions, wherever they are.",
        "formal_text": "Convert casual text to formal text: Okay, so in the intro rules, the result's meaning comes from a lambda-abstraction, which is based on the meaning of the assumption that gets discharged. This assumption"
    },
    {
        "casual_text": "There are still a few directions we can explore for future work. First off, we could gather more attributes and see how they perform, maybe even bringing in some other linguistic theories about verbs and improving how we model context. Second, even though we used attributes to connect language and vision, we could dig deeper into how this works in future projects. Specifically, since our tests showed that unsupervised word embeddings really boost performance, it could be worth trying to learn data-driven attributes directly from a big dataset or dictionary definitions, all in one go. Lastly, for future research on action attributes, it would be great to include videos to better capture those attributes that need to show change over time.",
        "formal_text": "Convert casual text to formal text: There are still a few directions we can explore for future work. First off, we could gather more attributes and see how they perform, maybe even bringing in some other linguistic theories about verbs"
    },
    {
        "casual_text": "I think there's room to make this method even better by looking into other ways to reduce dimensions and trying out different alternatives to GMM. Plus, we could add some extra tricks to the feature vector, like considering sentence length and punctuation marks.",
        "formal_text": "Convert casual text to formal text: I think there's room to make this method even better by looking into other ways to reduce dimensions and trying out different alternatives to GMM. Plus, we could add some extra tricks to the feature vector"
    },
    {
        "casual_text": "Check this out: Here's a trace of the conditioned unification between patterns 7 and 8. We can tweak the program for \"btt. grcs, ta\" so it doesn't mess with any predicates that are defined by just one horn clause. Take the definition of cb, for example—it's only made up of (i) Instead of (j), right? So, we can prove cd(0, 1, P): c3(P). Plus, (1) can be swapped out for c4(0, 2, 0), all thanks to (k).",
        "formal_text": "Convert casual text to formal text: Check this out: Here's a trace of the conditioned unification between patterns 7 and 8. We can tweak the program for \"btt. grcs, ta\""
    },
    {
        "casual_text": "• Don’t dump all your preferences on the assistant in one go.",
        "formal_text": "Convert casual text to formal text: • Don’t dump all your preferences in one go."
    },
    {
        "casual_text": "The main takeaway is that literary judgments aren't random and can be explained pretty well by looking at the text itself. There's something inherently \"literary\" about these texts. Our model uses a mix of different features from the text, and they all add up to improve our predictions. We got a total score of 76.0% variation explained, which is pretty solid. This result holds up really well—it doesn't just predict broad categories, but also more specific ratings.",
        "formal_text": "Convert casual text to formal text: The main takeaway is that literary judgments aren't random and can be explained pretty well by looking at the text itself. There's something inherently \"literary\" about these texts."
    },
    {
        "casual_text": "HPSG's type system includes something called parametric types, like the one shown in Figure 1 from (PS94). Unlike regular types and features, we don't really understand how powerful parametric types are. In fact, they've never been properly formalized in a way that could be used for HPSG parsing, so we couldn't even compare them to other types. This paper talks about a formalization of parametric types, based on the typed attribute-value logic from (Car92). This logic is special because it has a strict idea of what's \"appropriate\"—it tells us which features an object of a certain type can have and what types those features can take. It also says that every feature structure must have the right values for all the features that match its type. Before, people thought that every parameter in a subtype had to be in all its supertypes and vice versa. This made it impossible to use something like Figure 1, because if _1_ was a list(X) and A_ was parametric, then everything else would have to be too. But this paper changes that (check out Section 2). It gets rid of that rule by saying there should be a simple most general type (which (Car92)'s logic already requires). This general type is then used during type-checking and figuring out new parameters.",
        "formal_text": "Convert casual text to formal text: HPSG's type system includes something called parametric types, like the one shown in Figure 1 from (PS94). Unlike regular types and features, we don't really understand how powerful parametri"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. In Table 3, we tested how well a Semantic-aware Model works when combined with BERT. The results show that this combo performs the best. This tells us that BERT and semantic dependency graphs can work together nicely for solving the Chinese zero pronoun problem. But when we compare the Semantic-aware Model without BERT to the Baseline Model with BERT, we see that BERT plays a bigger role in improving performance than the semantic dependency graph. Plus, BERT boosts the Baseline Model more than it does the Semantic-aware Model. This suggests that BERT might already be capturing some of the semantic information that the dependency graph would provide. Looking ahead, we think it would be cool to combine BERT with not just the semantic dependency graph, but also semantic role labeling. This could make BERT even better at handling Chinese zero pronouns in the future.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. In Table 3, we tested how well a Semantic-aware Model works when combined with BERT. The results show"
    },
    {
        "casual_text": "Event definition is all about figuring out what kinds of events exist and what roles the different pieces of information play in each type of event. Basically, it helps us understand what counts as an event and what details are important for that event. In the ACE event extraction program, they have a category called JUSTICE that lists legal event types, like arrest-jail, release-parole, trial-hearing, and so on. If we take arrest-jail as an example, the key bits of info include who the person is, who the agent is, what crime was committed, when it happened, and where it happened. This way of defining events is based on things happening or changes in situations, which is how events are generally defined. Following the ACE method, some studies have tweaked the event types in the legal field to fit specific legal needs. Maxwell and his team (2009) looked deeper into legal behaviors and states and came up with new legal event definitions. Lagos and colleagues (2010) saw legal events as things with a clear start and end, emphasizing the importance of timing in legal cases. Besides creating new event types for the legal world, some researchers have also changed the arguments used in legal events. Ingolfo and team (2012) focused on the core ideas tied to judgments and made those into argument roles. Bertoldi et al. (2014) argued that the arguments should be the info that legal experts actually care about when reading legal texts.",
        "formal_text": "Convert casual text to formal text: Event definition is all about figuring out what kinds of events exist and what roles the different pieces of information play in each type of event. Basically, it helps us understand what counts as an event and"
    },
    {
        "casual_text": "• We mix finite-state stuff with deep learning to make sure our poems are grammatically correct, while also getting the benefits of long-distance RNNs for better flow. • We use words that match the user's topic as rhyme words, which helps create poems that feel connected to the topic. This lets us make longer poems that stay on point. • We also adapt our method to work with different poetry styles and languages.",
        "formal_text": "Convert casual text to formal text: • We mix finite-state stuff with deep learning to make sure our poems are grammatically correct, while also getting the benefits of long-distance RNNs for better flow."
    },
    {
        "casual_text": "Sure! Here's the informal version: 1. The code for word2vec is over here: https://code.google.com/archive/p/word2vec/ 2. You can find the LDC2011T07 catalog at this link: https://catalog.ldc.upenn.edu/LDC2011T07 3. And here's where you can grab the enwik9.zip file: http://mattmahoney.net/dc/enwik9.zip",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: 1. The code for word2vec is over here: https://code.google.com/archive/p/word2vec/"
    },
    {
        "casual_text": "Okay, let’s break this down in a simpler way. A finite ordered tree is basically a structure O = (T, A), where T is a tree with a root and some ordering rules, and A is a function that assigns a list of nodes to each node in the tree. For each node u in the tree, the list A(u) has to follow two rules: 1. No repeats in the list. 2. The list A(u) = (u1, ..., uk) only if u is above u1, ..., uk in the tree. Also, there can’t be any node u' below u that isn’t in the list A(u). In short, the list A(u) for a node u includes all the nodes directly below it in the tree, and it’s empty if u doesn’t have any nodes below it. Now, let’s talk about models. A model M is just a pair (O, V), where O is this finite ordered tree we just described, and V is a function that assigns propositional symbols to sets of nodes. Basically, V links each symbol to a bunch of nodes in the tree. Finally, we define what it means for a model M to satisfy a formula  at a node u. We write this as M, u  , which means the model M makes  true at node u. And that’s it!",
        "formal_text": "Convert casual text to formal text: Okay, let’s break this down in a simpler way. A finite ordered tree is basically a structure O = (T, A), where T is a tree with a root"
    },
    {
        "casual_text": "Since both the original Japanese readers and the English readers are equally important, we came up with some guidelines to check how easy the Japanese sentences are to read and how well they translate into English, based on how good the English version feels.",
        "formal_text": "Convert casual text to formal text: Since both the original Japanese readers and the English readers are equally important, we came up with some guidelines to check how easy the Japanese sentences are read and how well they translate into English, based on how"
    },
    {
        "casual_text": "The TACRED dataset (Zhang et al., 2017) is a distant supervision dataset that includes relations not found in typical knowledge bases like Wikipedia or DBpedia. It was made using query entities and annotated system responses from the annual TAC KBP evaluations. Because of this, the data is really unique and tied to the TAC KBP corpus, which means it has some cultural differences. For instance, the sentence \"Billy Mays, ... pop culture icon, died at home in Tampa\" shows the relation city_of_death between \"Billy Mays\" and \"Tampa\". In Bengali, while \" \" means 'died' for regular people, for celebrities, it's more common to say \"    \" (last breath release did). These kinds of cultural nuances can't be fully captured just by translating very specific English sentences.",
        "formal_text": "Convert casual text to formal text: The TACRED dataset (Zhang et al., 2017) is a distant supervision dataset that includes relations not found in typical knowledge bases like Wikipedia or DBpedia. It was made"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. N is just the number of examples we use to train the model. yi and i are the actual and predicted labels for the i-th instance, respectively.  represents all the parameters that the model can adjust during training.  is the value that controls how much we care about L2-regularization, which helps prevent overfitting. Now, for the experimental setup part, we'll set up everything to test how well the model performs.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. N is just the number of examples we use to train the model. yi and i are the actual and predicted labels for"
    },
    {
        "casual_text": "Stepwise regression isn't a good fit for this kind of analysis because there just aren't enough language pairs to work with in each setup (for example, PanLex only has 14 pairs for each source-to-target language selection). This small number makes it hard to get reliable results, which is what stepwise regression needs. So, we decided to go with a standard multiple linear regression model instead. In this model, we use the isomorphism measure that has the strongest individual correlation along with some other linguistic measures. Just like with stepwise regression, we’ll share the overall correlation coefficient, r.",
        "formal_text": "Convert casual text to formal text: Stepwise regression isn't a good fit for this kind of analysis because there just aren't enough language pairs to work with in each setup (for example, PanLex only has"
    },
    {
        "casual_text": "We're using a top mapping approach for distillation based on hidden states. Basically, we align the student's hidden states with the top few layers of the teacher's transformer. Since the initial sequence representation (from chunk aggregation) is already somewhat contextualized, SkipBERT can learn from the deeper layers of the teacher without needing to go through the shallower ones.",
        "formal_text": "Convert casual text to formal text: We're using a top mapping approach for distillation based on hidden states. Basically, we align the student's hidden states with the top few layers of the teacher's transformer."
    },
    {
        "casual_text": "Another thing to consider is how unit B1 connects with the JUSTIFICATION relationship between units D1 and B1. This part is linked to the stuff you see in figures 1.b and 1.d.",
        "formal_text": "Convert casual text to formal text: Another thing to consider is how unit B1 connects with the JUSTIFICATION relationship between units D1 and B1. This part is linked to the stuff you see in figures 1.b and 1."
    },
    {
        "casual_text": "So, like we talked about before, filtering is a step in between that helps narrow down the stuff we're looking at before we do the final analysis. The filtering part, which adds two random sentences to each parallel one, is super fast and does a great job of cutting down the amount of stuff we have to look at for all the language pairs. But when it comes to keeping as much relevant stuff as possible (that's called recall), it works really well for English-Latvian and English-German, okay for English-Romanian and English-Greek, but it drops about 40% of the recall for the other three language pairs.",
        "formal_text": "Convert casual text to formal text: So, like we talked about before, filtering is a step in between that helps narrow down the stuff we're looking at before we do the final analysis. The filtering part, which add"
    },
    {
        "casual_text": "We're trying to see if we can use our calculated dissimilarity scores instead of human judgments. This would turn our supervised learning into unsupervised learning while still keeping the performance strong. The tricky part is figuring out how to calculate the dissimilarity score for a summary, especially since we don't know the correct order of the sentences. To handle this, we use a simple sentence alignment method between a computer-generated summary and a human-written summary from the same document cluster. Let's say we have a system-generated summary Ds = (Ss1, Ss2, ..., Ssn) and a human-written summary Dh = (Sh1, Sh2, ..., ShN). Note that n and N might not be the same. We consider the sentence order (1, 2, ..., N) in Dh as the original order, which we call . Then, we calculate  = (o1, o2, ..., on) based on Ds. For each oi in , we find the most similar sentence Shj, where j is between 1 and N, in Dh. We do this by calculating the cosine similarity between all tokens in Shj and Ssi. If none of the sentences in Dh have any similarity with Ssi, we just assign -1 to oi.",
        "formal_text": "Convert casual text to formal text: We're trying to see if we can use our calculated dissimilarity scores instead of human judgments. This would turn our supervised learning into unsupervised learning while still keeping the performance strong."
    },
    {
        "casual_text": "Unlike Sayeed and Demberg (2013), we decided to attach the role label annotations for prepositional phrases to the preposition itself, instead of the noun phrase that follows it. This choice was based on the approach used in the CoNLL 2005 shared task, as described by Carreras and Màrquez (2005).",
        "formal_text": "Convert casual text to formal text: Unlike Sayeed and Demberg (2013), we decided to attach the role label annotations for prepositional phrases to the preposition itself, instead of the noun phrase that follows it. This choice"
    },
    {
        "casual_text": "1. What language is the model supposed to work with? 2. Did the model learn the phrase \"Les avocats dorment\"?",
        "formal_text": "Convert casual text to formal text: 1. What language the model supposed to work with? 2. Did the model learn the phrase \"Les avocats dorment\"? 3. Convert casual text to formal text: 1. What language the"
    },
    {
        "casual_text": "So, when people try to assign keyphrases without repeating themselves, they usually look back at what they've already said. Inspired by that, we created a \"target side review context set\" that keeps track of the context of the phrases we've generated so far. By using this context along with an attention mechanism, we can better predict the next phrase. We call this the \"review mechanism.\"",
        "formal_text": "Convert casual text to formal text: So, when people try to assign keyphrases without repeating themselves, they usually look back at what they've already said. Inspired by that, we created a \"target side review context set"
    },
    {
        "casual_text": "Smart To-Do: Two-Step Process Alright, so here's how we handle creating To-Do items in two steps. In the first step, we...",
        "formal_text": "Smart To-Do: Two-Step Process Alright, so here's how we handle creating To-Do items in two steps. In the first step we... Convert casual text to formal: Smart To-Do: Two-"
    },
    {
        "casual_text": "Also, Uyghur metaphors can be split into three categories: analogy, metonymy, and simile. Here’s how the rules work for each type.",
        "formal_text": "Convert casual text to formal text: Also, Uyghur metaphors can be split into three categories: analogy, metonymy, and simile. Here’s how the rules work for each type."
    },
    {
        "casual_text": "NMT models usually get trained using something called CLM generative loss. This loss function works with an auto-regressive approach to estimate probabilities and generate target text. So, if you have a pair of sentences, one source (x) and one target (y), the CLM predicts the probability of the target sentence based on the source. This probability is written as P  (y 1: n |x), where n is the number of tokens (words or pieces of words) in the target sentence. The auto-regressive part means the model predicts each token one by one, using the previous tokens to help guess the next one.",
        "formal_text": "Convert casual text to formal text: NMT models usually get trained using something called CLM generative loss. This loss function works with an auto-regressive approach to estimate probabilities and generate target text. So, if you"
    },
    {
        "casual_text": "For the ROUGE-1 F1 score, NeuSum did better across all languages. For the English dataset, TextRank did better than the LEAD4 baseline. As for the ROUGE-2 F1 score, the results for each language were all pretty different.",
        "formal_text": "Convert casual text to formal text: Convert casual text to formal text: For the ROUGE-1 F1 score, NeuSum did better across all languages. For the English dataset, TextRank did better than the LEAD4"
    },
    {
        "casual_text": "Okay, let's break this down in simpler terms. So, based on Figure 2, words like \"hisho\" (which means \"secretary\") and \"shindai-sha\" (which means \"sleeping car\") in sentence (1) are kind of like \"joshu\" (meaning \"assistant\") and \"hikouki\" (meaning \"airplane\"), respectively. Both pairs are examples of things you can \"toru\" (which means \"to reserve\"). So, in sentence (1), \"toru\" can be understood as \"to reserve.\" But here's the thing: in Kurohashi's method, there are some important features missing that would help us figure out the exact meaning of the verb in question.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in simpler terms. So, based on Figure 2, words like \"hisho\" (which means \"secretary\") and \"shindai-"
    },
    {
        "casual_text": "The experiments in this paper used a joint-extraction method, where the event trigger and its arguments were trained together.",
        "formal_text": "Convert casual text to formal text: The experiments in this paper used a joint-extraction method, where the event trigger and its arguments trained together. Convert casual text to formal text: The experiments in this paper used a joint"
    },
    {
        "casual_text": "Sure! Here's a more casual version: s(x, y) = s_m(x) + s_m(y) + s_c(x, y) s_m(x) = FF_m(g_x) s_c(x, y) = FF_c(g_x, g_y, (x, y)) Basically, the total score s for x and y is made up of three parts: the score for x, the score for y, and a combined score for both x and y. The score for x (or y) is calculated using FF_m and g_x (or g_y). The combined score uses FF_c, g_x, g_y, and (x, y).",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: s(x, y) = s_m(x) + s_m(y) + s_c("
    },
    {
        "casual_text": "We've tested the translation checker on eight different texts: two novels, two software manuals translated by hand, three software manuals translated using a translation memory tool, and one set of dialogue fragments that was machine-translated. The novels aren't super interesting for checking consistency, mainly because they’re way less repetitive than the manuals, and also because consistency isn’t always a big deal in literary translations. For more details, check out the section \"Ange relationer mellan tabeller\" in Chapter 7, \"Grunder för tabeller.\" SOURCE 1:",
        "formal_text": "Convert casual text to formal text: We've tested the translation checker on eight different texts: two novels, two software manuals translated by hand, three software manuals translated using a translation memory tool, and one set of dialogue"
    },
    {
        "casual_text": "This part takes in a bunch of vector representations, each one matching up with a single token. These vectors are labeled as d t, and they're in a space with a certain dimension, l. Think of it like each token has its own little vector. For instance, this input could come from the co-attention layer we talked about in Section 3. The way this layer works is...",
        "formal_text": "Convert casual text to formal text: This part takes in a bunch of vector representations, each one matching up with a single token. These vectors are labeled as d t, and they're in"
    },
    {
        "casual_text": "So, as some research has pointed out (like Lapata and Lascarides in 2003), understanding implicit predicates can be super helpful for all kinds of NLP tasks—things like language generation, information extraction, question answering, and machine translation. A lot of these tasks involve paraphrasing or expanding queries, which is something Voorhees talked about back in 1994. Let’s say a search engine or a question-answering system gets a query like \"schnelle Bombe\" (quick bomb). Chances are, the person isn’t looking for info on bombs in general, but specifically about bombs that explode fast. Knowing about the predicates linked to the word \"Bombe\" could help predict what those implicit predicates might be. But sometimes, just guessing the most likely argument-predicate pairs isn’t enough to create a good paraphrase. You also need to know how the argument fits into the predicate’s structure—like whether it’s the subject or the object. For example, compare \"eine Bombe explodiert schnell\" (a bomb explodes quickly) with \"ein Buch schnell lesen/schreiben\" (to read/write a book quickly). In the first case, \"Bombe\" is the subject, but in the second, \"Buch\" is the object. Knowing this difference is key to making sure the paraphrase makes sense both semantically and grammatically.",
        "formal_text": "Convert casual text to formal text: So, as some research has pointed out (like Lapata and Lascarides in 2003), understanding implicit predicates can be super helpful for all kinds of NLP tasks—things like language"
    },
    {
        "casual_text": "The PLM accuracies vary between around 54%-64% for in-distribution cases and 50%-56% for out-of-distribution cases. We think this dip in performance for out-of-distribution data might be due to annotation biases. A randomly initialized classifier seems to take advantage of these biases better when the training and testing sets come from the same distribution. But when the distributions are different, the biases don’t carry over as well. Check out Table 6 for a comparison of cross-dataset and cross-lingual scenarios using the same model (XLM-R), training size, and testing set (LCC in English), but with different training sources.",
        "formal_text": "Convert casual text to formal text: The PLM accuracies vary between around 54%-64% for in-distribution cases and 50%-56% for out-of-distribution cases. We think this"
    },
    {
        "casual_text": "The current version of 3arif doesn’t handle modality entailment, which is what example 28 is about. Basically, the USER is calling out anyone who believes that Egypt could use the Iranian threat to blackmail the UAE. By criticizing that idea, it means the USER thinks it’s not true. Also, this version doesn’t deal with future tense, questions, commands, or hypothetical situations. That’s because they work differently when it comes to intensification and polarity, which we’re not covering here. But don’t worry, we’ll tackle that in future updates.",
        "formal_text": "Convert casual text to formal text: The current version of 3arif doesn’t handle modality entailment, which is what example 28 is about. Basically, the USER is calling out anyone who believes that Egypt could"
    },
    {
        "casual_text": "When figuring out a partial translation for a node v, the decoder starts by finding its ancestor nodes, which are the nodes that v can be reached from. These are shown as the gray nodes in Figure 1. The function IsCandidatePath(v′, v) then checks if there’s at least one path connecting v and v′ in the original preordering. This helps avoid listing too many random phrase pairs that aren’t actually useful.",
        "formal_text": "Convert casual text to formal text: When figuring out a partial translation for a node v, the decoder starts by finding its ancestor nodes, which are the nodes that v can be reached"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We have a bunch of arguments, some supporting a topic and others against it. The main goal of the KPA-2021 task is to create a summary using Key Points (KPs) that show the overall picture of these arguments. In the Generation Track, models have to do everything—come up with a list of KPs for each topic and stance, and then figure out how confident they are that each argument fits with each KP. This is pretty tough, so we also created the Matching Track. Here, experts already provide a set of KPs for each topic and stance, and models just need to predict how well each argument matches those KPs. In both tracks, the confidence score tells us how sure the model is that the KP really captures the main idea of the argument. This helps in creating a KP-based summary. Since only about 5% of the arguments in the real data are linked to more than one KP, we simplify things during evaluation. We only look at the KP with the highest confidence score for each argument and consider that a true positive.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. We have a bunch of arguments, some supporting a topic and others against it. The main goal of the KPA-20"
    },
    {
        "casual_text": "Alright, so we have two tensor links: one for the formula (B/A)—that’s the one in Table 1—and another for (BA), which is basically just flipping the polarities of the premises. Then, we also have two par links: one for the formula (AB) + and another for (A/B) +, which are the same idea as before.",
        "formal_text": "Convert casual text to formal text: Alright, so we have two tensor links: one for the formula (B/A)—that’s the one in Table 1—and another for (BA), which"
    },
    {
        "casual_text": "Parameter settings. One cool thing about HAPS is that you don’t need to know how many segments you want beforehand. Instead, you just set how much you care about each level. HAPS is pretty flexible with these preferences, so this general setting is a handy way to adjust the level of detail in your segmentation without having to pick the exact number of segments for each part of the tree. In our case, we kept things simple by setting the preferences evenly across the board, but you could definitely add more specific details if you wanted to.",
        "formal_text": "Convert casual text to formal text: Parameter settings. One cool thing about HAPS is that you don’t need to know how many segments you want beforehand. Instead, you just set how much you care about each level. HAPS"
    },
    {
        "casual_text": "Vocabulary growth models help us see how words change over time in a specific area by looking at the relationship between different types of words and how often they're used. Macau, being a mix of different languages and cultures, can show us what's important in society based on how people use words. But no one has really looked at how Macau's vocabulary has changed over time—until now. This paper is the first to create a big collection of texts from Macau Chinese over time. We used three different models to see how the vocabulary changed in these texts and picked the best one, the Heaps model, to dig deeper. It turns out that the way words change in Macau is closely tied to what's trending in the news, government policies, and everyday life. To make sure our method works, we also tested it on texts without any time information, and it still gave us good results. This is the first time anyone has studied how Macau's vocabulary has evolved using such a big collection of texts over time. This research helps us understand Macau's language life better and how it's developed over the years.",
        "formal_text": "Convert casual text to formal text: Vocabulary growth models help us see how words change over time in a specific area by looking at the relationship between different types of words and how often they're used. Macau, being"
    },
    {
        "casual_text": "To create a solid dataset that could be used as a reference for figuring out why people make edits on Wikipedia, we got four undergrads with some experience editing Wikipedia to tag these edits using a special system we made. This system was based on detailed guidelines that had been checked by actual Wikipedia editors and included examples to help. To help these students get more familiar with how Wikipedia works, we also gave them three one-hour training sessions. During these sessions, they labeled a small number of edits (about 50 each time) and talked through any differences in their tags until they all agreed.",
        "formal_text": "Convert casual text to formal text: To create a solid dataset that could be used as a reference for figuring out why people make edits on Wikipedia, we got four undergrads with some experience editing Wikipedia to tag these edit"
    },
    {
        "casual_text": "We went with the Multilingual BERT setup from Wolf et al. (2019). Since BERT gives us embeddings for WordPiece units (thanks, Wu et al., 2016) instead of whole words, we just averaged them out per word to get word-level embeddings. We know this is a pretty basic approach, and if we made it better, it could probably improve our results.",
        "formal_text": "Convert casual text to formal text: We went with the Multilingual BERT setup from Wolf et al. (2019). Since BERT gives us embeddings for WordPiece units (thanks, Wu"
    },
    {
        "casual_text": "In this part, we're showing how the Ranked List Loss (RLL) works and how it stacks up against other popular metric losses.",
        "formal_text": "Convert casual text to formal text: In this part, we're showing how the Ranked List Loss (RLL) works and stacks against other popular metric losses. Convert casual text to formal text: In this part"
    },
    {
        "casual_text": "Emotional diversity is basically how many different emotions are tagged to a single term. In the set of Figures 10, you can see the regression lines (with a 95% confidence interval) showing how emotional diversity changes for each filtering method depending on the injection ratio. The x-axis represents the number of different emotions for a term according to the NRC, and the y-axis shows the number of emotions after filtering. F1 filtering consistently gave a pretty high emotional diversity, more than 2 emotions per term, as shown in Figures 10(a), 10(b), and 10(c). As the injection ratio decreased, F1 filtering's emotional diversity even went up to 3 emotions per term. Threshold filtering, on the other hand, was tied to the best performance threshold. When the 30% threshold was used (Figures 10(a), 10(b), and 10(d)), it actually resulted in more emotions per term than F1 filtering. But when the best threshold was 20% (Figure 10(c)), the number of emotions dropped below 2 per term. In contrast, when the majority filtering was stricter at 70%, the emotional diversity was really low (Figures 10(a), 10(b), and 10(c)). However, when the majority was set at a more relaxed 60%, the emotional diversity increased. Both threshold and majority filtering methods kind of cap the emotional diversity at their upper limits and directly impact how emotions are distributed. Majority filtering tends to limit diversity because it needs a single annotation agreement, while threshold filtering pushes for more diversity by limiting the peak class annotation.",
        "formal_text": "Convert casual text to formal text: Emotional diversity is basically how many different emotions are tagged to a single term. In the set of Figures 10, you can see the regression lines (with a 95% confidence interval)"
    },
    {
        "casual_text": "A way to get creative data from lots of people is to hire workers, like we did for funny headlines in Hossain et al. (2019). Another option is to create a game to collect the data. Games work really well for gathering creative stuff because people enjoy both coming up with and judging creative things, especially humor. This idea of using games to label data has been proven to work in earlier studies (Von Dabbish, 2004, 2008).",
        "formal_text": "Convert casual text to formal text: A way to get creative data from lots of people is to hire workers, like we did for funny headlines in Hossain et al. (2019). Another option is to create"
    },
    {
        "casual_text": "The concept of using a matrix to organize the information comes from the \"similarity matrix\" mentioned in Kit et al. (2002). But, how we set up our recombination matrix and get the final result is different from their method.",
        "formal_text": "Convert casual text to formal text: The concept of using a matrix to organize the information comes from the \"similarity matrix\" mentioned in Kit et al. (2002). But, how we set up our recombin"
    },
    {
        "casual_text": "In Row 5, we focus only on subtask B, but we work with a set of ten related questions, using their connections to help us out. This gives us a small boost in all the measures we're tracking. Even more importantly, this approach is key to getting better results with the joint models.",
        "formal_text": "Convert casual text to formal text: In Row 5, we focus only on subtask B, but we work with a set of ten related questions, using their connections to help us out. This gives us a small boost in all"
    },
    {
        "casual_text": "Let's take a look at the noun phrases with conjunctions in the last three rows of Table 5. The way conjunctions work has been a big topic of debate in syntax for a long time (Johannessen, 1998, among others). Our model shows that when dealing with single nouns connected by conjunctions, it might pick either the first noun (8) or the last one (9). But when it comes to conjunctions involving multiple noun phrases (not just single words), the model always chooses the conjunction itself as the main element. These strategies have all been discussed separately in linguistics, and since our model uses all of them, it seems to share the same confusion that linguists have been dealing with.",
        "formal_text": "Convert casual text to formal text: Let's take a look at the noun phrases with conjunctions in the last three rows of Table 5. The way conjunctions work has been a big topic of debate in syntax for a"
    },
    {
        "casual_text": "The source segment needs to be decoded by both translation models, but only the phrase pairs with matching overlapping factors are considered. On top of that, the alignment info from the training data is kept for each phrase pair in the translation model, and both models have to agree on the alignments. This is all laid out in Equations 7 to 9.",
        "formal_text": "Convert casual text to formal text: The source segment needs to be decoded by both translation models, but only the phrase pairs with matching overlapping factors are considered. On top of that, the alignment info from the training data is kept"
    },
    {
        "casual_text": "Alright, so when we talk about \"small\" in algebra, J(t; 14a, r, ry) can be either positive or negative.",
        "formal_text": "Convert casual text to formal text: Alright, so when we talk about \"small\" in algebra, J(t; 14a, r, ry can be either positive or negative."
    },
    {
        "casual_text": "Basically, this is talking about combining two types of information: the visual stuff and the orientation stuff. The visual part comes from the second-to-last layer of a RESNET18 model (that’s a fancy neural network, thanks to He et al. in 2016) based on what you’re currently looking at. The orientation part is all about your current heading, which is represented by repeating a simple vector [sin, cos] 32 times. This idea was borrowed from Fried et al. in 2018. As mentioned in section 3.4, the full picture (or feature matrix) of a panorama is made by stitching together eight different views of the scene.",
        "formal_text": "Convert casual text to formal text: Basically, this is talking about combining two types of information: the visual stuff and the orientation stuff. The visual part comes from the second-to-last layer of a RESNET18"
    },
    {
        "casual_text": "Leaderboards need to do two main things: (1) consistently and effectively rank better models above worse ones (as mentioned by Tague-Sutcliffe in 1992 and Voorhees in 2003) and (2) help us focus on checking specific items and subjects (see section 5). The first point helps deal with the randomness that comes with limited evaluations, while the second lets us analyze errors and dig deeper into models (Belinkov and Glass, 2019; Zhang et al., 2019). First, we check if IRT models can accurately predict how subjects will respond (section 4.2). Then, a stability analysis shows that IRT rankings are slightly more reliable than the traditional ones (section 4.2.3). Finally, using IRT to pick items for annotation gives us rankings that are more closely aligned with the full test data (section 4.4).",
        "formal_text": "Convert casual text to formal text: Leaderboards need to do two main things: (1) consistently and effectively rank better models above worse ones (as mentioned by Tague-Sutcliffe in 1992 and Voorhees"
    },
    {
        "casual_text": "Earlier, we mentioned that the stuff we need to tag includes signs for pauses and parts we can't hear clearly. We figured this info would be useful for the tagging process. The symbol for inaudible speech (like when someone mumbles, so we can't make it out) is \"...\". We just threw that into our list of words and gave it the same \"part-of-speech\" tag as a period or something similar in written text. This means the tagger won't think the last word before the inaudible part is connected to the first word after it.",
        "formal_text": "Convert casual text to formal text: Earlier, we mentioned that the stuff we need to tag includes signs for pauses and parts we can't hear clearly. We figured this info would be useful for the tagging process"
    },
    {
        "casual_text": "Using the paraphrase pair we found, we can actually rewrite the original sentence by rearranging its words to match the order in the paraphrase. These paraphrase pairs give us examples to learn how to change word order when paraphrasing.",
        "formal_text": "Convert casual text to formal text: Using the paraphrase pair we found, we can actually rewrite the original sentence by rearranging its words to match the order in the paraphrase. These paraphrase pairs give us examples to"
    },
    {
        "casual_text": "A lot of the current ways we represent word meanings rely on the distributional hypothesis. This idea, which has been around since the 1950s (thanks, Harris!), suggests that words that show up in similar contexts tend to have similar meanings. Researchers like Turney and Pantel, and more recently Clark, have built on this concept. These representations, often called embeddings, can capture how people naturally think about word similarities and connections. They’ve been super useful in all kinds of NLP tasks, like figuring out word meanings across languages (Mikolov et al., 2013b), analyzing emotions in text (Socher et al., 2013), and even identifying important names in text (Turian et al., 2010; Guo et al., 2014).",
        "formal_text": "Convert casual text to formal text: A lot of the current ways we represent word meanings rely on the distributional hypothesis. This idea, which has been around since the 1950s (thanks, Harris!), suggests that words that"
    },
    {
        "casual_text": "Social media data is usually pretty messy since it’s user-generated. Different researchers have used all kinds of methods to clean this data before feeding it into a machine learning algorithm. We found that the steps they choose can change the size of the data, making it harder to compare results fairly between studies. Table 2 shows some examples of papers using three popular hate speech datasets and the different preprocessing techniques they used, which messes up the ability to compare them properly. Some studies use different train-test splits, like 70:30 or 80:20, while others do a train-test-validation split, such as 70:15:15, 60:20:20, or 80:10:10. Some even go for 10-fold or 5-fold cross-validation. All these different setups make it impossible to compare studies fairly unless every researcher reruns all the studies they want to compare, which is not only impractical but also super expensive. So, we’re pointing out what makes a dataset a good benchmark—something everyone can use to compare their results fairly.",
        "formal_text": "Convert casual text to formal text: Social media data is usually pretty messy since it’s user-generated. Different researchers have used all kinds of methods to clean this data before feeding it into a machine learning algorithm. We found that the"
    },
    {
        "casual_text": "We used the PD-CROWD, ELEMENT-OF, and SINGLE-COREF datasets, along with either the PRE-TRAIN or ANNEALING methods. For example, we started with PD-CROWD for PRE-TRAIN, then fine-tuned the model using the SINGLE-COREF dataset with the ANNEALING strategy. In total, we tested three different combinations (check out Table 3 for details). The best results came when we combined all three datasets, which gave us improvements of 0.4 percentage points and 0.9 percentage points over just using SINGLE-COREF alone, depending on whether we were doing a lenient or strict evaluation.",
        "formal_text": "Convert casual text to formal text: We used the PD-CROWD, ELEMENT-OF, and SINGLE-COREF datasets, along with either the PRE-TRAIN or ANNEAL"
    },
    {
        "casual_text": "To check how well our system works across different languages, we looked at how it compares to some other top-performing models like SyntagNet, EWISER, ARES, and MuLaN (Barba et al., 2020). All these systems are pretty new and have shown really good results.",
        "formal_text": "Convert casual text to formal text: To check how well our system works across different languages, we looked at how it compares to some other top-performing models like SyntagNet, EWISER, ARES, and MuLaN"
    },
    {
        "casual_text": "Dealing with all the hidden states from every token can be super resource-intensive and might even add unnecessary noise. In the original BERT setup (Devlin et al., 2018), predictions are made using just the output from the [CLS] token in the last layer. Some BERT variations, like SDNet (Zhu et al., 2018), take a different approach by averaging the [CLS] embeddings from all layers with some weights. Basically, the final logit can be calculated using h_final = sum(w_j * h_j) for j ranging from 1 to k, where w_j could be learned parameters or predefined values, h_j is the [CLS] embedding from layer j, and k is the total number of hidden layers. From this, if a smaller model can learn from the [CLS] representations in the teacher model's intermediate layers for any input, it could end up with similar generalization skills as the teacher model.",
        "formal_text": "Convert casual text to formal text: Dealing with all the hidden states from every token can be super resource-intensive and might even add unnecessary noise. In the original BERT setup (Devlin et al., 2018"
    },
    {
        "casual_text": "• P lm (e|D), P lm (f|D): These are the latent monolingual models that focus on a specific domain, D. They try to figure out how relevant e and f are for that domain, but for this project, we're treating them as language models (LMs). As I mentioned earlier, our approach to out-domain LMs is different from what others have done before, like in (Axelrod et al., 2011), where they used mix-domain LMs. Here, we’re highlighting the challenge of finding data to train these out-domain LMs and suggesting a workaround by creating pseudo out-domain data.",
        "formal_text": "Convert casual text to formal text: • P lm (e|D), P lm (f|D): These are the latent monolingual models that focus on a specific domain, D. They try to"
    },
    {
        "casual_text": "Alright, let's talk about Topic 2, which is supposed to be about pets. But when you look at the stuff about India, it’s all about energy and the environment, which doesn’t really fit. It seems like the environment stuff was just too big to leave out, even though it doesn’t match the other topics. So, it kind of snuck into this one. Now, the environment stuff showing up here isn’t totally random. You see, the pets topic also has words like \"water\" and \"plant,\" which are often linked to gardening. And guess what? Those words also pop up when people talk about the environment. So, it’s not a complete coincidence that the environment stuff ended up in this topic.",
        "formal_text": "Convert casual text to formal text: Alright, let's talk about Topic 2, which is supposed to be about pets. But when you look at the stuff about India, it’s all about energy and the environment, which doesn’"
    },
    {
        "casual_text": "Answering multiple-choice science questions is a common way to test how well a system can understand language and think through problems in Question Answering (QA). Like in other areas of NLP research, explainability has become a big deal in recent years. It's not enough just to predict the right answer; a QA system should also be able to explain how it got there. This is important to improve how these systems work and make them more reliable.",
        "formal_text": "Convert casual text to formal text: Answering multiple-choice science questions is a common way to test how well a system can understand language and think through problems in Question Answering (QA). Like in other areas of NLP"
    },
    {
        "casual_text": "For the Farsi-to-Arabic translation project, we started by testing the impact of adding reordered Farsi sentences along with their Arabic translations to the original training data, as explained in Section 3.6. Table 4 shows how this new system compares to the baseline, which was trained on the original, unreordered data. About 52% of the sentences were reordered using parse-based rules—we skipped POS-based rules for this. This change bumped up the training data size from 289K to 439K. For the translation part, we used a run-based penalty model for this experiment. The results were pretty good: we saw a solid improvement over the baseline, with a 1.2% boost in BLEU scores and a 0.6% improvement in WER. Funny enough, when we tried the same thing for Farsi-to-English translation, we didn’t notice much of an improvement. We think the main reason for the better results in the Farsi-to-Arabic case is because of better word alignment. In Farsi, the verb usually comes at the end of the sentence, but in Arabic, it’s often at the beginning. Due to how the alignment model works, this can cause issues. By seeing the same verb in both positions, the alignment algorithm might be able to figure it out better.",
        "formal_text": "Convert casual text to formal text: For the Farsi-to-Arabic translation project, we started by testing the impact of adding reordered Farsi sentences along with their Arabic translations to the original training"
    },
    {
        "casual_text": "The other group has rules to help tell the difference between good and bad stuff. For example, if the answer has words like \"disappointed\" or \"terrible,\" we mark it as negative. E5 is a good example of this kind.",
        "formal_text": "Convert casual text to formal text: The other group has rules to help tell the difference between good and bad stuff. For example, if the answer has words like \"disappointed\" or \"terrible,\" we mark it as"
    },
    {
        "casual_text": "Looking at Table 2 (which used gold labels), we didn’t really think subtask B would see much improvement. And yeah, rows 2-4 confirm that—when we used the pipeline approach, the IR measures stayed pretty much the same. But, interestingly, the classification accuracy went up by almost one point, recall improved too (though it came at the cost of lower precision), and the F1 score got a decent boost.",
        "formal_text": "Convert casual text to formal text: Looking at Table 2 (which used gold labels), we didn’t really think subtask B would see much improvement. And yeah, rows 2-4 confirm that—when we used the pipeline approach, the"
    },
    {
        "casual_text": "Okay, so this equation is basically saying that the probability of A_j being equal to i, given k and all the other A's (let's call them \"not j\"), and C, is proportional to two things multiplied together. First, it's proportional to the probability of A_j being equal to i, given all the other A's and C. Then, it's also proportional to the probability of k, given that A_j is equal to i and all the other A's. Finally, it simplifies to the probability of A_j being equal to i, given all the other A's and C, multiplied by k raised to the power of i. That's what equation (7) is saying in a nutshell.",
        "formal_text": "Convert casual text to formal text: Okay, so this equation is basically saying that the probability of A_j being equal to i, given k and all the other A's (let's call them \"not j"
    },
    {
        "casual_text": "Let's dive into how different settings for hyperparameters, like the learning rate and the number of training steps, affect how well PET performs compared to regular supervised learning. We'll specifically check out how these settings impact the accuracy on the test set.",
        "formal_text": "Convert casual text to formal text: Let's dive into how different settings for hyperparameters, like the learning rate and the number of training steps affect how well PET performs compared to regular supervised learning. We'll"
    },
    {
        "casual_text": "The top-performing setup for the basic RE system is shown in Figure 2. Here, we’ll quickly go over the other versions we tested.",
        "formal_text": "Convert casual text to formal text: The top-performing setup for the basic RE system is shown in Figure 2. Here, we’ll quickly p. Convert casual text to formal text: The top-performing setup for the basic RE"
    },
    {
        "casual_text": "The Mayan family results show that the CharCNN works well even with small datasets for closely related languages. This is a big deal because there are a lot of language families out there with not many examples to work with.",
        "formal_text": "Convert casual text to formal text: The Mayan family results show that the CharCNN works well even with small datasets for closely related languages. This is a big deal because there are a lot of language families out there with"
    },
    {
        "casual_text": "We noticed a couple of issues with this automatic generalization thing. First, the rules for picking patterns from long sentences are way too picky, especially when it comes to words. Turns out, stuff like clauses and noun phrases that act as modifiers don’t really matter for paraphrasing, so they could be handled more flexibly. Second, some important written expressions are getting turned into wild cards, which can mess things up. For example, \"H/\" (which means \"how many\") might make the sentence look like a question. So, we need a way to stop certain written expressions from being swapped out automatically.",
        "formal_text": "Convert casual text to formal text: We noticed a couple of issues with this automatic generalization thing. First, the rules for picking patterns from long sentences are way too picky, especially when it comes to words. Turns out,"
    },
    {
        "casual_text": "Lately, a bunch of NLP studies have been diving into lie detection by gathering datasets and using computer models to spot lies (Hirschberg et al., 2005; Pérez-Rosas et al., 2014; Peskov et al., 2020). However, most of these studies don’t really consider the traditional methods and findings in lie detection, and there’s hardly any follow-up research. This makes it tricky to figure out which datasets are actually good for training models. To bridge the gap between machine learning and lie detection research in psychology and linguistics, this study is all about analyzing verbal leakage cues. For the sake of simplicity, we’ll just call them leakage cues from now on. We’re looking at how these cues work in terms of how the data is collected and how well the models perform. We’ve got seven lie detection datasets to play with, and we’re analyzing them using word categories from LIWC2015 (Pennebaker et al., 2015). Through this research, we’re trying to answer three main questions: 1. How does the way we collect data affect strong leakage cues? 2. What’s the deal with the number and type of strong leakage cues in lie detection tasks? 3. Do strong leakage cues actually help make the models more reliable? We’re hoping that by answering these questions, we can figure out how to build and pick the right datasets for lie detection tasks.",
        "formal_text": "Convert casual text to formal text: Lately, a bunch of NLP studies have been diving into lie detection by gathering datasets and using computer models to spot lies (Hirschberg et al., 2005; P"
    },
    {
        "casual_text": "In our approach, the POS predictor adds an extra linear layer after the decoder to create the target sentence, which you can see in Figure 3. Once we're done training, we only use that POS predicting linear layer for making predictions, which helps us get better results for POS sequence prediction.",
        "formal_text": "Convert casual text to formal text: In our approach, the POS predictor adds an extra linear layer after the decoder to create the target sentence, which you see in Figure 3. Once we're done training, we only"
    },
    {
        "casual_text": "Python has some tools for checking how good text or code is, like BLEU, METEOR, and ROUGE-L. There's also something called CODE-NN, which Iyer and others talked about in 2016. It got a score of 27.",
        "formal_text": "Convert casual text to formal text: Python has some tools for checking how good text or code is, like BLEU, METEOR, and ROUGE-L. There's also something called CODE-NN, which"
    },
    {
        "casual_text": "Okay, let's say we're working on optimizing along the d-th dimension. Take a foreign sentence, f, and let's say the possible translations for it are e1, ..., eK. Remember from earlier (like in equation (1)) that the best candidate at a given  is the one that gives the highest value for the sum of M terms, where each term is m times m(ek, f). Basically, we can think of that sum as...",
        "formal_text": "Convert casual text to formal text: Okay, let's say we're working on optimizing along the d-th dimension. Take a foreign sentence, f, and let's say the possible translations for it"
    },
    {
        "casual_text": "Alright, so  is just a fancy way of saying \"stick together\" or \"combine.\" Now, let's talk about a convolution operation. It uses a filter W that’s part of the set of real numbers R hm, where h is less than or equal to k, and m is less than n. The size of this filter is determined by the window size m. When you apply this filter W to a word x ia, you get a feature map C that’s part of R pq. Here, p is calculated as k minus h plus 1, and q is n minus m plus 1. This feature map C is created by sliding the filter W over the word x ia. Next, there's a max-pooling operation. This takes the feature map C and applies the max function to it, specifically max(C st ), to create a new feature map  that’s part of R p/s q/t. Finally, all these features from different filters are fed into a sigmoid function, which looks like 1 divided by (1 plus exp(-x)). This function helps calculate the probabilities for y i.",
        "formal_text": "Convert casual text to formal text: Alright, so  is just a fancy way of saying \"stick together\" or \"combine.\" Now, let's talk about a convolution operation. It uses a filter"
    },
    {
        "casual_text": "We've got our best results for different tasks using a loss function based on uncertainty, and they're all listed in Table 1. Our best F rpS score is 0.743, and our best F e is 0.922. For utterance segmentation, we hit 0.767 on F uttSeg. Comparing our results to standard methods for disfluency detection is tricky since they use pre-segmented utterances. But our best result is just as good as Seeker's (2016) state-of-the-art models on the Switchboard data for utterance segmentation. When we look at incremental approaches, we beat Hough and Schlangen's (2017) 0.748 on end-of-utterance, and their 0.720 and 0.918 on F rpS and F e from both 2015 and 2017. Models using timing info did better than those using just lexical info on utterance segmentation and F e, but performed worse on F rpS, language modeling, and POS tagging. Our top results across all tasks came from the model with four joint tasks. Edit term detection did really well at 0.922, almost matching the state-of-the-art on Switchboard, which is 0.938. Our lowest perplexity was 64.3. Adding more training objectives improved language modeling in all joint models, except when it was trained with disfluency detection alone. The best POS tagging result was 0.965. It's worth noting that comparing language modeling and POS tagging on this specific dataset isn't straightforward.",
        "formal_text": "Convert casual text to formal text: We've got our best results for different tasks using a loss function based on uncertainty, and they're all listed in Table 1. Our best F rpS score is 0.743"
    },
    {
        "casual_text": "A more advanced way to use machine learning could be to figure out how well words translate from one language to another, instead of just using the translation probabilities from a bilingual dictionary. This would lead to a fully personalized translation. Similar ideas have been used in the learning-to-rank field for single-language information retrieval (like Bendersky et al., 2010), but not for translating between multiple languages. Another cool idea would be to use the same approach to translate answers back into the question's language (not just translating the question itself). This would help us understand the meaning of each answer better, since we've talked about how just picking one translation can miss out on a lot of important details.",
        "formal_text": "Convert casual text to formal text: A more advanced way to use machine learning could be to figure out how well words translate from one language to another, instead of just using the translation probabilities from a bilingual dictionary. This would lead to"
    },
    {
        "casual_text": "Alright, so current methods for embedding graphs that use complex numbers have a couple of problems. First, while working only in complex vector spaces gives you good interpretability for different types of relationships, it’s not great at capturing all the nuances because it relies on basic operations like adding or multiplying complex numbers. To fix this, QuatE (Zhang et al., 2019) came up with a solution using quaternion hypercomplex vector spaces, which helps with semantic matching. But this comes with a catch: it makes things harder to interpret and adds more computational work, and even then, the improvement isn’t huge. Second, there’s the issue of embedding ambiguity. This happens when different entities end up with very similar embeddings, and existing methods like TransE and RotatE can’t really handle this well. This problem mostly comes from trying to optimize things for one-to-many relationships by applying a translation function, which ends up causing similar embeddings for different entities.",
        "formal_text": "Convert casual text to formal text: Alright, so current methods for embedding graphs that use complex numbers have a couple of problems. First, while working only in complex vector spaces gives you good interpretability for different types of"
    },
    {
        "casual_text": "Sequence labeling models have been a big deal in basic NLP tasks like POS tagging, chunking, and named entity recognition (NER). Back in the day, people used statistical methods like Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with manually designed features and task-specific resources. But with the rise of deep learning, neural models have been crushing it on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are now encoded using distributed representations (Mikolov et al., 2013), and sentence-level features are learned automatically during end-to-end training. A lot of the current top-performing neural sequence labeling models use word-level Long Short-Term Memory (LSTM) structures to handle global sequence information and a CRF layer to deal with dependencies between labels (Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). On the other hand, Convolutional Neural Networks (CNN) (LeCun et al., 1989) have also been used because they can do parallel computing, making training and decoding faster and more efficient.",
        "formal_text": "Convert casual text to formal text: Sequence labeling models have been a big deal in basic NLP tasks like POS tagging, chunking, and named entity recognition (NER). Back in the day, people used statistical"
    },
    {
        "casual_text": "CDSP methods use different types of encoders: one for handling utterances and another for managing MRs (meaning representations). Utterance encoders create neural representations for both the current and past utterances, whereas MR encoders focus on building neural representations using historical MRs.",
        "formal_text": "Convert casual text to formal text: CDSP methods use different types of encoders: one for handling utterances and another for managing MRs (meaning representations). Utterance encoders create neural representations"
    },
    {
        "casual_text": "BART, which stands for Bidirectional and Auto-Regressive Transformers, was trained using stuff like rotating documents, shuffling sentences, filling in missing text, and hiding or deleting words (Lewis et al., 2019). For our tests, we went with the larger version of BART, called BART-Large.",
        "formal_text": "Convert casual text to formal text: BART, which stands for Bidirectional and Auto-Regressive Transformers, was trained using stuff like rotating documents, shuffling sentences, filling in missing text, and hiding or deleting"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: \"Self-Training (Artetxe et al., 2017) works by finding a shared linear space. Kernelized Sorting (Quadrianto et al., 2009) is another approach that uses HSIC (a way to measure statistical dependence) to pair up different types of data points. Self-Training (Artetxe et al., 2017) is a pretty cutting-edge method that alternates between figuring out an orthonormal transformation and using nearest neighbor (NN) matching.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: \"Self-Training (Artetxe et al., 2017) works by finding a shared"
    },
    {
        "casual_text": "Earlier studies on the BASIL corpus divided the data by splitting sentences into training, development, and test sets (Fan et al., 2019). This method separates target sentences from other sentences in the same article and even from other articles about the same event. But this kind of split goes against what we're trying to do here, which is to look at sentences in the context they're in. Plus, spreading sentences from the same article across training and test data can be seen as a problem. Oh, and here's Table 3, showing how adding direct text context works with a Sequential Sentence Classifier (SSC, from Cohan et al., 2019) and with or without a window, using a max sequence length of 5 or 10.",
        "formal_text": "Convert casual text to formal text: Earlier studies on the BASIL corpus divided the data by splitting sentences into training, development, and test sets (Fan et al., 2019). This method separates target sentences from"
    },
    {
        "casual_text": "In Weighted Set Cover, each set has a number attached to it, like a cost or weight. The main idea is to find the smallest or largest total cost of sets that can cover everything in the universe U.",
        "formal_text": "Convert casual text to formal text: In Weighted Set Cover, each set has a number attached to it, like a cost or weight. The main idea is to find the smallest or largest total cost of sets that can"
    },
    {
        "casual_text": "In real-world situations, we usually estimate this gradient using just one Monte Carlo sample from a training example and subtract a baseline reward to lower the variance in our gradient estimate.",
        "formal_text": "Convert casual text to formal text: In real-situations, we usually estimate this gradient using just one Monte Carlo sample from a training example and subtract a baseline reward to lower the variance our gradient estimate. Convert casual text"
    },
    {
        "casual_text": "The Student-adapted system uses the same reinforcement learning algorithm as the Lecturer-adapted system. The main difference is in the reward function. The reward function for training is pretty similar to the one used in the Lecturer-adapted system. It was created by tweaking student ratings from an earlier experiment and figuring out the weights using linear regression, just like Walker et al. (1997) and Rieser et al. (2010) did.",
        "formal_text": "Convert casual text to formal text: The Student-adapted system uses the same reinforcement learning algorithm as the Lecturer-adapted system. The main difference is in the reward function. The reward function for training is pretty similar to the one used in"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. The findings are all laid out in Table 2. Applying INLP has a mixed effect on how well the main task performs: it boosts BOW by 1.9%, but brings BWV down by 5.1% and BERT by 5.51%. Now, GAP TPR, RMSg, which measures how close the true positive rates are for male and female classifiers, takes a noticeable hit. It shows that the rates are getting closer on average: - In BOW, it drops from 0.203 to 0.124, a 38.91% decrease. - In BWV, it goes from 0.184 to 0.089, a 51.6% drop. - In BERT, it falls from 0.184 to 0.095, a 48.36% decrease. We also looked at the correlation between GAP TPR for each profession and the percentage of women in those professions. Here's what we found: - In BOW, the correlation went from 0.894 before INLP to 0.670 after, a 33.4% decrease. - In BWV, it dropped from 0.896 to 0.425, a 52.5% decrease. - In BERT, it fell from 0.883 to 0.470, a 46.7% decrease (check Figure 4b for more details). Interestingly, De-Arteaga et al. (2019) found a correlation of 0.71 for BWV when they used a \"scrubbed\" version of the biographies, meaning they removed all pronouns and names. INLP does way better than that baseline, even while keeping all the gender markers intact. So, the big question is: How does trying to make things fair affect how much importance the classifier gives to different words in the biographies? That's what we're analyzing here.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. The findings are all laid out in Table 2. Applying INLP has a mixed effect on how well the main task performs"
    },
    {
        "casual_text": "Alright, so here's the deal: If both C1 and C2 can be learned in a structured way (PAC-learnable), and h01 and h02 are kinda helpful in predicting f1 and f2 (weakly useful predictors), plus  is on point with D, f1, f2, h01, and h02, and has a discrimination score that’s high enough—like at least 4(|Y|  1)2 for 0/1 loss or  4|V|2(|Y|  1)2 for Hamming loss—and h01 and h02 aren’t related to each other (uncorrelated), then C1 and C2 can also be learned with two-sided hints.",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal: If both C1 and C2 can be learned in a structured way (PAC-learnable), and h01 and h02"
    },
    {
        "casual_text": "These spaces might mix and match different types of homogeneous spaces we talk about in this work. We're not diving into explaining other parts of how the model works, like why a specific class got a certain probability or why a particular neuron fired in a certain way. Also, for the foil, we just look at one other class, not a group of them. The fact we're focusing on doesn't have to be the model's prediction; it could be something like the model's probability instead. Another option is to swap out the candidate factor for other causal factors. We'll save comparing amnesic and non-amnesic interventions in detail for later work.",
        "formal_text": "Convert casual text to formal text: These spaces might mix and match different types of homogeneous spaces we talk about in this work. We're not diving into explaining other parts of how the model works, like why a specific"
    },
    {
        "casual_text": "So, N E is the total number of pattern evaluators. S(I) is the overall score for each aspect, which is like a summary of how well everything is doing.  j (I) is a way to adjust or weigh how important each pattern evaluator is for that specific aspect. And s j (I) is the score for each individual pattern evaluator, which is based on how likely the pre-trained model thinks it is.",
        "formal_text": "Convert casual text to formal text: So, N E is the total number of pattern evaluators. S(I) is the overall score for each aspect, which is like a summary of how well everything is doing."
    },
    {
        "casual_text": "So, S(y_i) is the score for the i-th tree shown on the beam, and S(y) is calculated as the sum of P(i, j, k) squared times y_s for all i, j, k. S-DIORA uses this loss along with the reconstruction loss, which is the original goal of DIORA.",
        "formal_text": "Convert casual text to formal text: So, S(y_i) is the score for the i-th tree shown on the beam, and S(y) is calculated as the sum of P(i, j,"
    },
    {
        "casual_text": "Attended-Value Probes for POS, NER, and CON tasks are all about identifying and labeling specific parts of a sentence. For POS, it's just a single word, but for NER and CON, it's a sequence of words in a row. In the case of CON, these sequences can even overlap with each other. To tackle these tasks, we’ve come up with a fresh probing method, outlined in Algorithm 1. This method predicts the label for each span by looking at its representation.",
        "formal_text": "Convert casual text to formal text: Attended-Value Probes for POS, NER, and CON tasks are all about identifying and labeling specific parts of a sentence. For POS, it's just"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: We're looking for the best combination of items (let's call them \"s\") from a set (S) that gives us the highest score. The score is calculated by taking the relevance of \"s\" to a query (q) and subtracting the similarity between \"s\" and other items in the set. The whole thing is weighted by a factor (), which decides how much we care about relevance versus similarity. So, it's like saying: Find the item that's most related to the query, but also make sure it's not too similar to other items we've already picked.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: We're looking for the best combination of items (let's call them \"s\") from a set (S) that"
    },
    {
        "casual_text": "We used HuggingFace's Transformers (Wolf et al., 2020) to build our GAML-BERT and other baseline methods. All our experiments were run on a single Nvidia V100 16GB GPU.",
        "formal_text": "Convert casual text to formal text: We used HuggingFace's Transformers (Wolf et al., 2020) to build our GAML-BERT and other baseline methods. All our experiments were run"
    },
    {
        "casual_text": "Limitations and Future Plans. Our paper mainly focuses on unsupervised summarization because it's super important for situations where we don't have a lot of data. One thing we haven't done yet is get solid results for supervised summarization, even though our model might work there too. The reason for this is that past studies on supervised summarization don't clearly categorize summaries by length (Yang et al., 2020), which makes comparing results tricky and not really fair (Schumann et al., 2020). This issue is also pointed out by Su et al. (2021), who noticed that the same model can score a few ROUGE points differently depending on the length of the summary it generates. Despite this, we did compare our work with Su et al. (2021) under our conditions and showed that our NAUS model comes out on top when the comparison is fair. For future work, we plan to dive into supervised summarization once we set up a proper and rigorous testing environment, but that's something we'll tackle in another paper.",
        "formal_text": "Convert casual text to formal text: Limitations and Future Plans. Our paper mainly focuses on unsupervised summarization because it's super important for situations where we don't have a lot of data. One thing we"
    },
    {
        "casual_text": "In this paper, we’ve added a straightforward alignment algorithm to Anymalign to help us figure out where it’s currently falling short. These new alignments make Anymalign’s phrase tables much better, enough to get top-notch results. Along the way, we also came up with a quick and easy method to calculate ITG alignments using any word-level association scores.",
        "formal_text": "Convert casual text to formal text: In this paper, we’ve added a straightforward alignment algorithm to Anymalign to help us figure out where it’s currently falling short. These new alignments make Anymalign’s phrase"
    },
    {
        "casual_text": "There's been a lot of talk about how tricky adjectives and comparing things can be. People like Cresswell (1976), Kennedy (1997), Heim (2000), and Lassiter (2017) have all weighed in on this topic.",
        "formal_text": "Convert casual text to formal text: There's been a lot of talk about how tricky adjectives and comparing things can be. People like Cresswell (1976), Kennedy (1997), Heim (2000), and Lassi"
    },
    {
        "casual_text": "In step (6a), the strategy of transforming to normal form means making separate copies for each branch, which ends up duplicating parts of the tree. On the other hand, the tableau procedure takes advantage of the idea of sharing structures instead.",
        "formal_text": "Convert casual text to formal text: In step (6a), the strategy of transforming to normal form means making separate copies for each branch, which ends up duplicating parts of the tree. On the other hand, the tableau procedure takes"
    },
    {
        "casual_text": "Sure! Here's the informal version: **RQ1:** How do different types of mismatches in a parallel text mess with the translation quality of a machine translation (whether it's SMT or NMT) system that’s trained on that text?",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: **RQ1:** How do different types mismatches in a parallel text mess with the translation quality of a machine translation (whether it"
    },
    {
        "casual_text": "The really high CV * for the baseline system might be because of a problem with the evaluation code—it seems like they used macro-F1 instead of weighted-F1, as mentioned by Bestgen (Section 3.2, first paragraph), Caines and Buttery (Section 2.5, second-to-last paragraph), and Huber and öltekin (Section 3.2, second paragraph). If you multiply the evaluation scores by the same number, the CV * stays the same.",
        "formal_text": "Convert casual text to formal text: The really high CV * for the baseline system might be because of a problem with the evaluation code—it seems like they used macro-F1 instead of weighted-F1, as mentioned by"
    },
    {
        "casual_text": "So, based on the stuff we just talked about and letting t get super huge, equations (3) and (4) turn into...",
        "formal_text": "Convert casual text to formal text: So, based on stuff we just talked about and letting t get super huge, equations (3) and (4) turn into... Convert casual text to formal text: So, based on"
    },
    {
        "casual_text": "WER, or word error rate, is a way to measure how accurate a speech recognition system is. It basically tells you how many mistakes (like replacing words, adding extra ones, or leaving some out) it takes to change the generated sentence to match the correct one. The idea is that the fewer mistakes, the better the system is. So, a lower WER is always better.",
        "formal_text": "Convert casual text to formal text: WER, or word error rate, is a way to measure how accurate a speech recognition system is. It basically tells you how many mistakes (like replacing words, adding extra ones, or"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way: So, we're dealing with some kind of system or process here. There are different parts labeled with codes like *c12, *c13, and so on. These parts seem to be interacting with each other in a specific order. For example, there's something called OBJ2 that's connected to *c12 and *c13. Then, there's another part called FAf that's linked to *el3 and *el40BL. It looks like FAf is doing something with these parts, maybe controlling or processing them. Next, there's FA which is working with *c14, *ell, and *oont. It seems like FA is handling these parts and might be outputting something (maybe \"nil\" is the result here). Then, there's FAOJP' which is connected to *clO, *ell, and (*gf *cont). This part seems to be doing something with these connections and then passing it on to *gf. Finally, there's a part labeled *i0 and *10 that's linked to -VP\". This part is working with *cI0, *ell, *cont, and *outpxcomp. It seems like it's processing these parts and then passing them to NP (*el0 *ell *ontpnp). At the end, there's a - lET that's connected to *el0, *cii, and *ontpdet. This part is working with N (*outpdet *outpnp), which seems to be the final output or result of the whole process. In short, this is a bunch of interconnected parts doing different tasks, and they all seem to be working together to produce some kind of output.",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in a simpler way: So, we're dealing with some kind of system or process here. There are different parts labeled with codes like *c"
    },
    {
        "casual_text": "To optimize this objective function while considering the constraints, we need to pick vectors a and b in a way that makes a_i * b_j high when C_xy_ij is also high. Basically, each non-zero entry in the crosscovariance matrix limits the options for the projection directions we can choose. This usually isn't a big deal if the training data is clean, but that's pretty rare, especially with high-dimensional data like text documents. Plus, natural languages are inherently ambiguous, so there's a good chance any document will have some noisy words. Every time a noisy word pops up, it adds to the covariance matrix, making it denser and harder to find the right projection directions.",
        "formal_text": "Convert casual text to formal text: To optimize this objective function while considering the constraints, we need to pick vectors a and b in a way that makes a_i * b_j high when C_x"
    },
    {
        "casual_text": "Kind of like what Erkan and Radev did back in 2004, we can think of each group of tweets, called a cluster, as a full graph where each tweet is a node. The connection between tweet i and tweet j gets a weight, which helps us figure out how related they are.",
        "formal_text": "Convert casual text to formal text: Kind of like what Erkan and Radev did back in 2004, we can think of each group of tweets, called a cluster, as a full graph where each tweet is a node"
    },
    {
        "casual_text": "Alright, so if RSTj is \"Unknown\" and tbr j is greater than i, then PO%_t is, and RSI y is also \"Unknown.\" If that's not the case, then we move on to the next part (18). Basically, the final answer comes from the single-ner tagger that doesn't give an \"Unknown\" result and has the longest input.",
        "formal_text": "Convert casual text to formal text: Alright, so if RSTj is \"Unknown\" and tbr j is greater than i, then PO%_t is, and RSI"
    },
    {
        "casual_text": "In this part, we’re using the same approach to predict power dynamics that we talked about in our earlier work. So, let’s say we have a thread t and two people (p1, p2) involved in it. We want to figure out if p1 has more power than p2, or vice versa, automatically. We’re using a supervised learning system based on SVM (Support Vector Machine) that can predict whether p1 is superior or subordinate to p2 based on how they interact in the thread. The order of p1 and p2 is set so that p1 is the one who sends the first message in the thread. The system we built uses ClearTK, which is a wrapper for SVMLight, to handle the SVM stuff. It uses a quadratic kernel to look at how different features interact, which is super important, as we’ll see in Sections 5 and 6. For our experiments, we’re using the Train, Dev, and Test subsets from the APGI part of our dataset. We train our models using the pairs of people interacting in the threads from the Train set and fine-tune them using the Dev set. Finally, we test how well everything works on both the Dev and Test sets and report the results.",
        "formal_text": "Convert casual text to formal text: In this part, we’re using the same approach to predict power dynamics that we talked about in our earlier work. So, let’s say we have a thread t and two people ("
    },
    {
        "casual_text": "At test time, this version not only considers the document-level context but also taps into the GPT-2 model for step-level ingredient predictions (as mentioned in section 3.2) to create an ingredient prompt.",
        "formal_text": "Convert casual text to formal text: At test time, this version considers the document-level context but also taps into the GPT-2 model for step-level ingredient predictions (as mentioned in section 3.2 to create an ingredient"
    },
    {
        "casual_text": "Alright, let's kick things off by pulling out phrases from the parallel text. We're talking about a sequence of phrases, like  K 1.",
        "formal_text": "Convert casual text to formal text: Alright, let's kick things things by pulling out phrases from the parallel text. We're talking about a sequence of phrases, like  K 1. We're talking about a"
    },
    {
        "casual_text": "(3) Our CZSL-Adv model totally crushes the current top models in zero-shot and few-shot learning tests. It's way better. We also did a bunch of extra tests to break it down and analyze everything in detail.",
        "formal_text": "Convert casual text to formal text: (3) Our CZSL-Adv model totally crushes the current top models in zero-shot and few-shot learning tests. It's way better. We also did a bunch of extra"
    },
    {
        "casual_text": "We add e_t in Eq. 8 to handle cases where N_g(t) is empty, making it possible to compute the context representation. Think of it as a way to smooth things out when calculating context. After that, we figure out the distance between the head-relation context of t and the orthogonal transform representation of a triple (h, r, t), like this.",
        "formal_text": "Convert casual text to formal text: We add e_t in Eq. 8 to handle cases where N_g(t) is empty, making it possible to compute the context representation. Think of it as a way to"
    },
    {
        "casual_text": "Our approach falls into the third category of studies, but it’s a bit different because we combine two types of features in a unique way. In our method, internal component features are more important. We’ll show that a model using just these features can work really well on its own. The other type of features is like a helpful extra that can slightly improve the results for certain words. We connect the two models by giving each part-of-speech (POS) guess from the first model a credibility score. If a result has a lower credibility score, we flag it and run it through a second step that uses global context to reconsider it.",
        "formal_text": "Convert casual text to formal text: Our approach falls into the third category of studies, but it’s a bit different because we combine two types of features in a unique way. In our method, internal component features are more important"
    },
    {
        "casual_text": "TextRCNN is a mix of TextCNN and TextRNN. It starts by using a single-layer Bi-LSTM with 64 cells, combining outputs from both directions at each timestep. Then, it uses convolution kernels with widths of 2, 3, and 4 (three for each size) and adds a max-pooling layer. This approach has hit the top performance on RCV1 for both flat and hierarchical categorization, according to Zhou et al. in 2020.",
        "formal_text": "Convert casual text to formal text: TextRCNN is a mix of TextCNN and TextRNN. It starts by using a single-layer Bi-LSTM with 64 cells, combining outputs from both directions at"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We've got three main categories here: 1. **Strongly successful**: This is where things went really well. 2. **Strongly consistent**: This is where things stayed steady and reliable. 3. **Strongly unsuccessful**: This is where things didn't work out at all. Now, when we compare two things (let's call them (28) and (29)), we can see some interesting stuff. There are some implications that come from this comparison, but the reverse isn't always true. In other words, just because one thing happens doesn't mean the opposite will happen too.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. We've got three main categories here: 1. **Strongly successful**: This is where things went really well. 2."
    },
    {
        "casual_text": "Following this idea of dealing with weird or incomplete stuff, our grammar has a bunch of categories (like feature-trees) that wouldn’t be considered normal grammar categories in regular English. For example, we’ve got special rules for phrases that include things like names or car registration numbers, which aren’t usually treated as part of standard syntax. Our parser is more focused on finding phrases that describe events, not just full sentences. So, we don’t always stick to the usual structure where everything is covered by a single \"S\" label for the whole input message.",
        "formal_text": "Convert casual text to formal text: Following this idea of dealing with weird or incomplete stuff, our grammar has a bunch of categories (like feature-trees) that wouldn’t be considered normal grammar categories in regular English. For"
    },
    {
        "casual_text": "For this MDP, we need a policy network to help the agent pick the right action depending on the situation it's in.",
        "formal_text": "Convert casual text to formal text: For this MDP, we need a policy network to help the agent pick the right action depending on the situation it's in. Convert casual text to formal text: For this MDP,"
    },
    {
        "casual_text": "The ANTS framework offers a bunch of cool ways to speed up, enhance, and expand the work of social science researchers. With deep learning tools becoming available on cheap or even free platforms like Colab, OpenAI, and HuggingFace, researchers will be able to create better datasets using bigger, more varied, and more nuanced text sources. We’re hoping that ANTS can help make this happen, making deep learning more accessible and opening up new possibilities in unexpected ways.",
        "formal_text": "Convert casual text to formal text: The ANTS framework offers a bunch of cool ways to speed up, enhance, and expand the work of social science researchers. With deep learning tools becoming available on cheap or even free platforms like Co"
    },
    {
        "casual_text": "Sure! Here's a more casual version: The type of hardware you're using can totally mess with how accurate your system is. Liu and his crew noticed back in 2018 that training a system for NER tasks works way better on a GPU than on a CPU. Plus, the speed at which everything runs is super dependent on the hardware you've got.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: The type hardware you're using can totally mess with how accurate your system is. Liu and his crew noticed back in 2018 that training a"
    },
    {
        "casual_text": "Chinese abbreviations are made by picking out key characters from the full name. Like with \"\" (Peking University), the abbreviation is \",\" which comes from the first and third characters. You can think of it as a sequence labeling problem. Check out Table 1 for an example: the abbreviation \"\" from the full form \"\" (Peking University).",
        "formal_text": "Convert casual text to formal text: Chinese abbreviations are made by picking out key characters from the full name. Like with \"\" (Peking University), the abbreviation is \",\" which comes from the"
    },
    {
        "casual_text": "For instance, in the top graph in Figure 5, there's an edge connecting a vowel to a Hebrew character with a really high weight. But when you look at the bottom graph, which shows the results after discriminative training, that same edge now has a weight of zero.",
        "formal_text": "Convert casual text to formal text: For instance, in the top graph in Figure 5, there's an edge connecting a vowel to a Hebrew character with a really high weight. But when you look at the bottom graph"
    },
    {
        "casual_text": "When we check out the different text levels, we can see that the number of errors per word helps us figure out the best level for general MT scoring. To find this, we take the total errors and divide them by the number of words (for each rater and system for every text). This shows that the systems work best with texts that are around level two because the errors per word are lower here, at 0.209, compared to the next closest level, which is 0.251. Lastly, the error rate at level T0 tells us that general MT systems might struggle with simpler texts that have basic grammar and fewer complex words.",
        "formal_text": "Convert casual text to formal text: When we check out the different text levels, we can see that the number of errors per word helps us figure out the best level for general MT scoring. To find this, we take the total errors"
    },
    {
        "casual_text": "For the Recipes dataset, we're using the same setup as in (Zhang et al., 2020), where the main task is to figure out how the ingredients move around during the cooking process. To check how well our models are doing, we look at precision, recall, and F1 scores.",
        "formal_text": "Convert casual text to formal text: For the Recipes dataset, we're using the same setup as in (Zhang et al., 2020), where the main task is to figure out how the ingredients move around during the"
    },
    {
        "casual_text": "Another cool tweak to PDP sampling was introduced in a paper by Chen et al. in 2011. They added another helper variable called a \"table indicator.\" This indicator tells us for each data point z_i whether it's the \"head of its table.\" Remember, the n_k data points are split into t_k tables, and each table has exactly one \"head.\" If z_i is the head of its table, we set r_i = 1; otherwise, it's zero. Following this \"table\" idea, the number of tables for n_k should match the number of data points z_i that are also table heads.",
        "formal_text": "Convert casual text to formal text: Another cool tweak to PDP sampling was introduced in a paper by Chen et al in 2011. They added another helper variable called a \"table indicator.\" This indicator tells us for each"
    },
    {
        "casual_text": "Turns out, even in the gold standard, those pairs aren't as similar as they're supposed to be. Here are some cosine similarity scores to give you an idea:",
        "formal_text": "Convert casual text to formal text: Turns out, even in the gold standard, those pairs aren't as similar as they're supposed to be. Here are some cosine similarity scores to give you an idea:"
    },
    {
        "casual_text": "We use the pipe symbol (|) to represent the element-wise absolute value of a vector. Basically, the idea is that if the (featurized) states of two domain experts are very different from each other, we don't need to spend much time explaining or clarifying them.",
        "formal_text": "Convert casual text to formal text: We use the pipe symbol (|) to represent the element-wise absolute value of a vector. Basically, the idea is that if the (featurized) states of two domain"
    },
    {
        "casual_text": "In other areas, we see similar patterns, but to keep things short, we're just showing the ROC results.",
        "formal_text": "Convert casual text to formal text: In other areas, we see similar patterns, but to keep short, we're just showing the ROC results. Convert casual text to formal text: In other areas, we see similar patterns,"
    },
    {
        "casual_text": "Also, we check out how our method stacks up against CTC beam search (Graves et al., 2006). Usually, a non-autoregressive model trained with CTC can be decoded in two ways: either greedily or using beam search. Greedy decoding just picks the most likely token at each step, like w*i = argmax wi P(wi|x), and then squeezes those tokens into a sentence using .",
        "formal_text": "Convert casual text to formal text: Also, we check out how our method stacks up against CTC beam search (Graves et al., 2006). Usually, a non-autoregressive model trained"
    },
    {
        "casual_text": "In Table 3, we’ve got the results from our pairwise alignment algorithm, comparing it to some baseline methods using 200 text-text recipe pairs from Common Crawl that were aligned by humans. Unlike the text-video alignments, we noticed that the uniform alignment baseline didn’t do better than the textual similarity baselines. This seems to be because the different ways text-text recipe pairs are reordered make alignment a bit trickier. When it comes to the textual similarity baselines, RoBERTa came out on top, just like in the text-video alignment. We think this is because text recipes often use similar vocabulary, which makes it easier to spot matching words between two sets of instructions. On the other hand, video narrators usually use more casual language than the folks writing text recipes, so it’s harder to align things based on word similarities. Something cool we found is that both BM25 and RoBERTa have higher recall than our best HMM+IBM1 model, but they fall short in precision. This means retrieval models are great at finding more alignments, even if they’re not always super precise. Our unsupervised HMM+IBM1 model, though, still crushed all the baselines when it came to the F1 score (p  0.001). When we broke down the HMM+IBM1 model, we saw that using all the words to learn alignments gave us the best results.",
        "formal_text": "Convert casual text to formal text: In Table 3, we’ve got the results from our pairwise alignment algorithm, comparing it to some baseline methods using 200 text-text recipe pairs from Common Crawl that were aligned by"
    },
    {
        "casual_text": "We’ve got two types of contraction rules: weak contraction for proofs, which is defined in (16), and t-contraction for lambda-terms, which is in (17).",
        "formal_text": "Convert casual text to formal text: We’ve got two types of contraction rules: weak contraction for proofs, which is defined in (16), and t-contraction for lambda-terms, which is in (17)."
    },
    {
        "casual_text": "Setting up an open-source project mostly takes time and effort, not a ton of money. For Adobe, it took about the same amount of work as one engineer putting in three months of work to get the Moses open-source package ready for us to create high-quality, large-scale engines. The hardware we started with wasn’t fancy—just a regular Mac running OSX, with a few tweaks to make it work for our setup.",
        "formal_text": "Convert casual text to formal text: Setting up an open-source project mostly takes time and effort, not a ton of money. For Adobe, it took about the same amount of work as one engineer putting in three months of"
    },
    {
        "casual_text": "Instead of using a simple swap rule that just switches neighboring phrases, we can go for more advanced grammar tweaks that let us skip over chunks of symbols. Our first version of these jump rules is based on Vilar et al. (2010), but we've made it more flexible, allowing for jumping across any number of blocks in a sentence.",
        "formal_text": "Convert casual text to formal text: Instead of using a simple swap rule that just switches neighboring phrases, we can go for more advanced grammar tweaks that let us skip over chunks of symbols. Our first version of these jump rules"
    },
    {
        "casual_text": "So, the k-th rebuttal argument span is labeled as  b k. Based on that, the argument pairs pulled from Y v, r k can be written like this:",
        "formal_text": "Convert casual text to formal text: So, the k-th rebuttal argument span is labeled as  b k. Based on that, the argument pairs pulled from Y v, r"
    },
    {
        "casual_text": "Parse tree-based reordering rules aim to rearrange Farsi sentences by shifting around phrases to match the structure of the target language. These rules work with the input parse tree, which is written in bracket notation, and then spit out the reordered sentence.",
        "formal_text": "Convert casual text to formal text: Parse tree-based reordering rules aim to rearrange Farsi sentences by shifting around phrases to match the structure of the target language. These rules work with the input parse"
    },
    {
        "casual_text": "We split sentences into individual words and picked the top N most common words for our vocabulary. We didn’t use any pre-trained word vectors, so we just randomly set up the embedding layer. The same goes for the CSVs—we randomly initialized those too. We didn’t mess around with tweaking any fancy settings or trying to get the absolute best accuracy. Our main goal was to show that our method works just as well, or even better, than the black-box approach, while also offering clearer explanations than other methods out there. The word embedding, semantic vector, and feature vector (at the final layer) all have 128 dimensions. For training, we used the Adam optimizer (thanks, Kingma and Ba, 2017!) with a batch size of 64 and a learning rate of 0.0001. We also added dropout with a 50% chance to prevent overfitting.",
        "formal_text": "Convert casual text to formal text: We split sentences into individual words and picked the top N most common words for our vocabulary. We didn’t use any pre-trained word vectors, so we just randomly set up the embedd"
    },
    {
        "casual_text": "• Hansards (200 sentences): This is a set of sentences that aren't related to the main topic. They're just random ones picked from a standard test set of Canadian Hansards.",
        "formal_text": "Convert casual text to formal text: • Hansards (200 sentences): This is a set of sentences that't related to the main topic. They're just random ones picked from a standard test set of Canadian Hans"
    },
    {
        "casual_text": "In this part, we’re looking into how each layer figures out what’s “local.” First, we took a look at how the window size changes as we move through the layers. After that, we dug into the behavior of the first word embedding layer, which turned out to be a bit different from the rest.",
        "formal_text": "Convert casual text to formal text: In this part, we’re looking into how each layer figures out what’s “local.” First, we took a look at how the window size changes as we move through the layers. After"
    },
    {
        "casual_text": "Since the way the 6 images are arranged and where the target is placed isn’t the same for every target-context pair, it could mess with how the reference resolution model works. To fix this, we mix up the images in the context for all parts of the data at the start of each training round. For the generation models, we just do this shuffling once at the very beginning of training for all parts.",
        "formal_text": "Convert casual text to formal text: Since the way the 6 images are arranged and where the target is placed isn’t the same for every target-context pair, it could mess with how the reference resolution model works. To"
    },
    {
        "casual_text": "We also noticed that the automatic brand identification worked pretty well, which shows that CBA has potential for finding hidden brand info in different types of text online. While the predictions for brand-satisfaction and brand-gender-satisfaction aren't as strong, there's definitely room to make things better. For instance, adding syntactic, semantic, and metadata features could help improve the model. Plus, using higher-order n-gram features might make the data analysis even better. But since this paper is mainly about proving the concept of Laplacian structured sparsity models and computational branding analytics, we haven't looked into different multiview representations to boost our model yet.",
        "formal_text": "Convert casual text to formal text: We also noticed that the automatic brand identification worked pretty well, which shows that CBA has potential for finding hidden brand info in different types of text online. While the predictions for brand-satisfaction"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We want to figure out how well this step is doing, so we’re creating a function to measure precision. But we’re also considering two important things: 1. Some mistakes are worse than others. For example, if the system says \"most\" instead of \"all,\" that’s not great, but it’s not the end of the world. However, if it says \"some\" when it should have said \"all,\" that’s a bigger deal. 2. Mistakes that could lead to wrong conclusions should be heavily punished. For instance, saying \"all dogs are black\" is way worse than saying \"some dogs are mammals.\" The first one could make you think wrong things about individual dogs, while the second one is actually true, even if it sounds a bit weird. Oh, and here’s Table 7, which shows how we’re measuring the differences in the way the system handles these natural language quantifiers.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. We want to figure out how well this step is doing, so we’re creating a function to measure precision. But we"
    },
    {
        "casual_text": "(a) RAM is 256GB, d=768 (b) RAM is 128GB, d=768 (c) RAM is 64GB, d=768 (d) RAM is 32GB, d=768 Figure 9 shows the latency distribution with d=768.",
        "formal_text": "Convert casual text to formal text: (a) RAM is 256GB, d=768 (b) RAM is 128GB, d=768 (c) RAM is 64GB, d=768"
    },
    {
        "casual_text": "Clause 8 is about making sure the right context matches and also checking that the left context doesn't have any of the specific strings we're looking for. Basically, for the morphology clauses to work both when we're analyzing and generating, the negation has to be set up in a way that makes sense. This means the arguments for the negation need to be solid. Logically safe negation means we wait to evaluate the negation until all the arguments are properly grounded.",
        "formal_text": "Convert casual text to formal text: Clause 8 is about making sure the right context matches and also checking that the left context doesn't have any of the specific strings we're looking for. Basically, for the morphology"
    },
    {
        "casual_text": "Alright, so to wrap things up, our study shows that NMT systems follow a power law scaling pattern, but we’re not diving into why that happens or what’s causing it. We also didn’t look into the exact training factors that influence the scaling exponent, but we think that’s a cool area for future research. Now, let’s talk about the figures: **Figure 5:** This one shows the same data as Figure 2 (on the left), but the translations have been cleaned up by removing BPE encoding and tokenization. We also used Sacrebleu (Post, 2018) to calculate the BLEU score. This process added a bit of noise, but it didn’t mess with the overall exponential relationship between cross-entropy and BLEU. **Figure 6:** Here, we see that the number of unique words seen during training drops sharply around 5 MB of data when using a Byte Pair Encoding (BPE) of size 30k. But when we use a smaller BPE of size 2k, the number of unique words stays steady.",
        "formal_text": "Convert casual text to formal text: Alright, so to wrap things up, our study shows that NMT systems follow a power law scaling pattern, but we’re not diving into why that happens or what’s causing it"
    },
    {
        "casual_text": "As more and more video content gets uploaded online every day, learning stuff from videos, especially for tasks like \"How to,\" has become a big part of our daily lives and work. But watching long videos can be super time-consuming. Right now, there are two main ways tech is trying to make videos easier to handle: video summarization, which cuts long videos into shorter ones, and (dense) video captioning, which creates a text summary of the important parts in the video. For really long videos, dense video event captioning breaks things down into smaller, detailed captions for all the key events. This helps people quickly scan through the video and opens up cool possibilities like creating video chapters or making it easier to search within a video.",
        "formal_text": "Convert casual text to formal text: As more and more video content gets uploaded online every day, learning stuff from videos, especially for tasks like \"How to,\" has become a big part of our daily lives and work. But watching long"
    },
    {
        "casual_text": "Even though this approach might seem like it’s not following the usual rules of compositionality, it’s actually pretty straightforward to create a function that combines the meanings of smaller parts to give you the meanings of all these sentences. We can just set it up with different rules for different cases: for example, \"departsfrom/\" means \"connect,\" \"departs/from/on\" means \"dday,\" and so on. Hirst tweaks the definition of compositionality to say that \"the meaning of a whole is a systematic meaning of the parts\" (from his work, page 27, with our emphasis added), but he doesn’t really explain what \"systematic\" means.",
        "formal_text": "Convert casual text to formal text: Even though this approach might seem like it’s not following the usual rules of compositionality, it’s actually pretty straightforward to create a function that combines the meanings of smaller parts to give"
    },
    {
        "casual_text": "In English, the subject usually gets to be the main idea of a sentence because the word order is pretty strict.",
        "formal_text": "Convert casual text to formal text: In English, the subject usually gets the main idea of a sentence because word order pretty strict. Convert casual text to formal text: In English, the subject usually gets the main idea of a"
    },
    {
        "casual_text": "Kurohashi and Nagao wrote the JUMAN Manual, version 3.4, in Japanese. It was published by Nagao Lab at Kyoto University in 1997.",
        "formal_text": "Convert casual text to formal text: Kurohashi and Nagao wrote the JUMAN Manual, version 3.4, in Japanese. It was published by Nagao Lab at Kyoto University in 1997."
    },
    {
        "casual_text": "MHA: Our model uses a hierarchy with multiple loss functions and an attenuation factor.",
        "formal_text": "Convert casual text to formal text: MHA: Our model uses a hierarchy with multiple loss functions and attenuation factor. Convert casual text to formal text: MHA: Our model uses a hierarchy with multiple loss functions"
    },
    {
        "casual_text": "A predefined kernel function is just a fancy way of saying a function that's already set up to do something specific. Meanwhile, (x) is a function that takes a vector x and transforms it into a higher-dimensional space.",
        "formal_text": "Convert casual text to formal text: A predefined kernel function is just a fancy way of saying a function that's already set up to do something specific. Meanwhile, (x) is a function that takes"
    },
    {
        "casual_text": "Fluency and Style in Summaries. Based on our human evaluation in Section 6.2, our model's summaries aren't as fluent as the human-written ones or the baseline. To dig deeper into fluency and style across different models on a larger scale, we followed previous research (Liu et al., 2018) and trained a neural language model (LM) specifically for radiology summaries. The idea is that summaries that are more fluent and stylistically similar to human writing should have a lower perplexity when tested with this LM, while less fluent ones would have a higher perplexity. To make this happen, we gathered all the human-written summaries from the training and development sets of both datasets, which added up to around 222,000 summaries. We then trained a powerful Mixture of Softmaxes LM (Yang et al., 2018) on this collection and used it to evaluate the perplexity of the summaries generated by all the models on the test set.",
        "formal_text": "Convert casual text to formal text: Fluency and Style in Summaries. Based on our human evaluation in Section 6.2, our model's summaries aren't as fluent as the human-written ones or the"
    },
    {
        "casual_text": "GCSum grabs sentences that give helpful background info about the topics related to the internal node we're looking at. We skip sentences that just talk about general stuff without mentioning the authors' work. So, we only feed GCSum sentences where the authors aren't the main focus.",
        "formal_text": "Convert casual text to formal text: GCSum grabs sentences that give helpful background info about the topics related to the internal node we're looking at. We skip sentences that just talk about general stuff without mentioning the authors'"
    },
    {
        "casual_text": "We’ve noticed that some parts of morphology don’t really fit well with the transformation-based approach we’re using here. Infixation and templatic morphology are good examples. Even agglutinative systems, which seem like they’d work with transformation rules because they add stuff to the ends of words, can get tricky when you start dealing with more complex morphological processes. For example, in Turkish, the suffixes -lar and -ler seem like they’d need two separate transformation rules, but they’re actually just one morpheme that changes depending on vowel harmony. This isn’t a huge deal for breaking words into their parts (morphological segmentation) because -lar and -ler show up a lot and we can still recognize them as separate suffixes. But to really cover all the different ways languages work, we’re going to need a more solid way of representing these processes. We’ll save that for later research, though.",
        "formal_text": "Convert casual text to formal text: We’ve noticed that some parts of morphology don’t really fit well with the transformation-based approach we’re using here. Infixation and templatic morphology are good"
    },
    {
        "casual_text": "We're using the 460-hour LibriTTS clean sets (Zen et al., 2019) for training our text-to-speech (TTS) model. This dataset is a multispeaker collection of English speech from audiobooks on the LibriVox project. We've trimmed the audio waveforms following the ESPnet recipe (Watanabe et al., 2018). To check the Word Error Rate (WER), we're using the open-source ASR model wav2vec 2.0 CTC 6. For evaluating how natural the synthetic speech sounds, we're relying on the NISQA-TTS model (Mittag and Möller, 2020), which is also open-source.",
        "formal_text": "Convert casual text to formal text: We're using the 460-hour LibriTTS clean sets (Zen et al., 2019) for training our text-to-speech (TTS) model"
    },
    {
        "casual_text": "Back in high school, when we had to parse sentences by hand, like in English or Latin class, we kinda got a sneak peek at what literal movement grammars (LMG) are all about. Unlike the old-school way of thinking that a phrase just has one main boss (the head), LMG says that sentences are made up of different parts, each with its own important bits. Take a verb phrase, for instance—it’s not just the main verb, but it also has stuff like objects hanging around. And guess what? These key parts can actually move around. When one of these important bits ends up outside the chunk it’s supposed to be in, LMG has this neat trick to send it down the tree of the sentence. There, it gets grabbed by the part of the sentence that left a little \"trace\" for it.",
        "formal_text": "Convert casual text to formal text: Back in high school, when we had to parse sentences by hand, like in English or Latin class, we kinda got a sneak peek at what literal movement grammars (LMG)"
    },
    {
        "casual_text": "Sure! Here's a more casual version: You can check out the Text-to-Text Transfer Transformer (T5) project on GitHub here: [google-research/text-to-text-transfer-transformer](https://github.com/google-research/text-to-text-transfer-transformer). And if you're interested in TensorFlow Research Cloud (TFRC), you can find more info here: [TensorFlow Research Cloud](https://www.tensorflow.org/tfrc).",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: You can check out the Text-to-Text Transfer Transformer (T5) project on GitHub here: [google-research/"
    },
    {
        "casual_text": "Basically, it's about breaking down the title and seeing how each word connects to the main ideas in the body of the text. Here's how it works:",
        "formal_text": "Convert casual text to formal text: Basically, it's about breaking down the title and seeing how each word connects to the main ideas in the body of the text. Here's it works: Break down the title and seeing"
    },
    {
        "casual_text": "de Melo and Bansal (2013) came up with a way to use MILP to maximize this goal, and we tried it out in our experiments. Just a heads-up, though: they included synonym info from WordNet in their ranking method, but we didn't use that part of their approach.",
        "formal_text": "Convert casual text to formal text: de Melo and Bansal (2013) came up with a way to use MILP to maximize this goal, and we tried it out in our experiments. Just a heads-up, though:"
    },
    {
        "casual_text": "• No NONE, but with DP: We're only considering candidates that aren't seen as non-epistemic, but we're still including any disagreements that carried over from Task 1.",
        "formal_text": "• No NONE, but with DP: We're only considering candidates that't seen as non-epistemic, but we're still including any disagreements that carried over from Task 1. • No NONE,"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way. First part: The subject (Sbj) is connected to the verb and the object (Obj). It's like saying, \"the subject does something to the object.\" So, it's like: Subject  Verb  Object Or in short: S  V  O Second part: Now, the object is connected to the subject through the verb. It's like saying, \"the object is affected by the subject doing something.\" So, it's like: Object  Verb  Subject Or in short: O  V  S Basically, it's just showing how the subject, verb, and object are all linked together in different ways.",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in a simpler way. First part: The subject (Sbj) is connected to the verb and the object (Obj). It's like"
    },
    {
        "casual_text": "Okay, let's break this down in simpler terms: First, \"D i, j\" just means there's a connection between words \"w i\" and \"w j\" in the sentence's dependency tree. \"Sim(•)\" is how we measure the similarity between words. If there's no similarity, we just set \"Sim(•)\" to 0. \" i, j\" is a tweak factor that deals with how the sentiment (like happy or sad) of an image part doesn't match the sentiment of a word in the text. \"(w i )\" is the emotional weight of word \"w i,\" which we get from a thing called SenticNet. If the word isn't in SenticNet, we just set its weight to 0. \"|•|\" is just the absolute value, like turning -5 into 5. \"a j\" and \"o j\" are the attribute and object of a box in the image, kind of like describing what's inside the box. Now, inspired by some smart people (Kipf and Welling), we made a graph that connects different types of info (like text and images) without any direction, so it's like A i, j = A j, i. Each part of the graph also has a loop to itself, so A i, i = 1. The reason we made this graph (see Equations 7 and 9) is two-fold: 1) Sometimes, sarcasm in text can be spread across multiple words, like \"wonderful weather\" when it's actually bad. So, we added connections based on the sentence's structure to help understand these situations better. 2) We also made a special factor, \" i, j,\" that adjusts how much we care about opposite sentiments. \"\" is a number we can change to control how much we focus on these opposite sentiments, and we usually set it to be more than 1.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in simpler terms: First, \"D i, j\" just means there's a connection between words \"w i\" and \"w"
    },
    {
        "casual_text": "First, let’s quickly go over Procrustes Analysis (PA), which is a method to find the best orthogonal mapping between two languages. Then, we’ll explain our approach, called Multi-Pairwise Procrustes Analysis (MPPA), which builds on PA and works for multiple languages. Imagine we have d-dimensional word embedding data from two languages, along with a dictionary that pairs up corresponding words. Mikolov et al. (2013) found that there’s a strong linear relationship between the vector spaces of two languages, and surprisingly, a simple linear mapping works just as well as a more complicated nonlinear one. Xing et al. (2015) took this a step further and showed that making sure the linear mappings are orthogonal (like rotation matrices) helps prevent overfitting and improves results. To find this orthogonal mapping, T, we can minimize this cost function:",
        "formal_text": "Convert casual text to formal text: First, let’s quickly go over Procrustes Analysis (PA), which is a method to find the best orthogonal mapping between two languages. Then, we’ll explain our"
    },
    {
        "casual_text": "Here's how you can build and use the hierarchical attention as outlined in Algorithm 1:",
        "formal_text": "Convert casual text to formal text: Here's how you build and use the hierarchical attention as outlined in Algorithm 1: Hierarchical Attention. Here's how you build and use the hierarchical attention"
    },
    {
        "casual_text": "To come up with a bunch of different ways to say the same thing, we came up with a system to use different sets of paraphrasing tricks. As mentioned in Section 2, paraphrasing can be split into two main types: simplification, which makes things easier to understand, and diversity, which creates more variety. Since simplification can clear up any confusion in the sentence, we do that first and then move on to diversity paraphrasing. We think this approach will make the diversity part work better because there’ll be less room for misunderstanding. Out of the four groups of patterns we have, group (1) is for simplification, and the rest are for diversity. Here’s how we apply these different groups to a single sentence.",
        "formal_text": "Convert casual text to formal text: To come up with a bunch of different ways to say the same thing, we came up with a system to use different sets of paraphrasing tricks. As mentioned in Section 2, paraphra"
    },
    {
        "casual_text": "\"\"\"\"\"\"\"\",1,,\"\",\"\":agt()ent(),\"\"\"\"\"\"manr() ,,, ,1,\"\",\"\",\"\",\"\",",
        "formal_text": "\"\"\"\"\"\"\"\",1,,\"\",\"\":agt()ent(),\"\"\""
    },
    {
        "casual_text": "The definition isn’t part of the same sentence or document as the context. It’s not right to just put the context as the first sentence, then the definition as the second sentence, separated by [SEP] for BERT. In this part, as you can see in Figure 2, we suggest a method to combine the context and definition with BERT so the model understands that the definition is mostly about the idiom. We feed the context, the idiom, and the definition all at once. For instance, we input the context \" (they hope they can achieve greater success)\", the idiom \" (make still further progress)\", and the definition \" (an outstanding achievement)\" together as \"  The Multi-Head Attention works differently for the context and the definition. Specifically, the Multi-Head Attention for the context is:",
        "formal_text": "Convert casual text to formal text: The definition isn’t part of the same sentence or document as the context. It’s not right to just put the context as the first sentence, then the definition as the second sentence, separated"
    },
    {
        "casual_text": "The CoFEE framework does need a cluster size K for generating pseudo labels. You might be curious if picking the right K really makes a big difference in the end. To check this out, we tested K from 4 to 90 and looked at the F1 score for CoFEE-MRC on the E-commerce dev set. As you can see in Figure 2(d), the best results came when K matched the number of fine-grained entity types mentioned in the queries (which is 23). This shows that our CoFEE pre-training can use this info as helpful prior knowledge. Thanks to the self-supervised learning setup, even when we changed K from 3 to 90, the model kept a stable F1 score and wasn’t too picky about the exact K value. This also means the framework works well when dealing with a new type of named entity where we don’t know the number of fine-grained types beforehand. We can just pick a bigger K than we think we need, and the model still performs solidly.",
        "formal_text": "Convert casual text to formal text: The CoFEE framework does need a cluster size K for generating pseudo labels. You might be curious if picking the right K really makes a big difference in the end. To check this"
    },
    {
        "casual_text": "[4] Before looking at a new sentence, the system guesses a rough idea of the plan the person asking for information is thinking about.",
        "formal_text": "Convert casual text to formal text: [4] Before looking at a new sentence, the system guesses rough idea the plan the person asking for information is thinking about. [5] Before looking at a new sentence, the"
    },
    {
        "casual_text": "To make things sound more natural and varied, we have real people manually turn graph queries into regular questions. We’ve got two ways of doing this, which you can check out in Figure 5. Each query gets sent to a few different people to rewrite it in their own words. Plus, we use different names or ways of saying things for the same entity, which we get from FACC1, to mix it up even more. We’ve got a list of common ways to say each entity, along with how often they’re used. For instance, for \"UnitedStatesOfAmerica,\" you’ve got options like \"us\" (108M times), \"united states\" (44M), \"usa\" (22M), and so on. Lastly, we automatically convert these graph queries into SPARQL queries to find the answers.",
        "formal_text": "Convert casual text to formal text: To make things sound more natural and varied, we have real people manually turn graph queries into regular questions. We’ve got two ways of doing this, which you can check out in Figure 5. Each query"
    },
    {
        "casual_text": "We’ve added our stuff to our Lee-BERT framework, and here’s the gist of it:",
        "formal_text": "casual text to formal text: Convert casual text to formal text: We’ve added our stuff to our Lee-BERT framework, and here’s the gist of it: Convert casual text to formal text:"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way: The \"bag-of-words\" classifier is a method some researchers use to tell apart text written by humans and text generated by machines, specifically using things like logistic regression. Solaiman et al. (2019) used a basic model that takes a document, turns it into a tf-idf vector (which basically means it looks at the words and how often they appear), and then runs it through a logistic regression model to see if it can tell if a piece of text came from a website or was generated by GPT-2. They tested different versions of GPT-2, like the 117M, 345M, 762M, and 1542M models, which vary in size (and number of parameters). They also tried different ways of generating text, like pure sampling, top-k sampling, and top-p (nucleus) sampling. What they found was that the bigger GPT-2 models were harder to detect than the smaller ones, meaning the larger the model, the more human-like the text it generates. Specifically, top-k samples were easier to spot as fake, while nucleus samples were trickier. This is because top-k sampling tends to overuse common words, which makes it easier for the detector to pick up on the patterns. On top of that, Solaiman et al. (2019) also fine-tuned GPT-2 on Amazon reviews. They found that the text generated by this fine-tuned model was even harder to detect, showing that domain-specific models (like the one trained on reviews) are more human-like than the general GPT-2 model. Lastly, Tay et al. looked into detecting the specific machine configuration, but that part isn’t detailed here.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way: The \"bag-of-words\" classifier is a method some researchers use to tell apart text written by humans and"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. The text you've provided seems to be a mix of random characters and numbers, possibly part of a code or encryption. It doesn't make much sense as a readable message. However, the part at the end, \"rC = 0.75,\" suggests that there's a value assigned to \"rC,\" which is 0.75. So, in a more casual tone: \"This jumble of letters and numbers looks like some kind of code or maybe a mix-up. But at the end, it says 'rC = 0.75,' which means 'rC' is equal to three-quarters or 0.75. That's about it!\"",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. The text you've provided seems to be a mix of random characters and numbers, possibly part of a code or encryption"
    },
    {
        "casual_text": "So, we've got a bunch of features to help figure out predicates and arguments, and to handle all that, we're using a classification model to estimate probabilities. Specifically, we're using the Adaboost classifier (Freund and Schapire, 1996), which is a pretty smart way to combine a lot of \"simple\" or \"weak\" classifiers. Each of these weak classifiers isn't super accurate on its own, but when you put them together, they create a really accurate overall classifier. The process works by picking weak classifiers one by one, focusing on the ones that can fix mistakes made by the classifiers we've already chosen. Each weak classifier gets a weight (k) that shows how much it helps reduce the overall error. This helps us figure out the probability of...",
        "formal_text": "Convert casual text to formal text: So, we've got a bunch of features to help figure out predicates and arguments, and to handle all that, we're using a classification model to estimate probabilities."
    },
    {
        "casual_text": "Mixture Decoder: This setup uses a hard-MoE with K decoders, where each decoder gets an equal share (called hMup in Shen et al. (2019)). It does parallel greedy decoding, meaning all decoders work at the same time. All the decoders have the same parameters, but they each use a different starting token embedding.",
        "formal_text": "Convert casual text to formal text: Mixture Decoder: This setup uses a hard-MoE with K decoders, where each decoder gets an equal share (called hMup in Shen"
    },
    {
        "casual_text": "Pruning is a way to tackle this issue by making models smaller by getting rid of certain parts based on specific rules. The tricky part is figuring out which parts to remove without messing up the model's performance. For language models, there's a smart algorithm that uses something called relative entropy, as mentioned in studies by Seymore and Rosenfeld (1996), Stolcke (1998), and Moore and Quirk (2009). This method uses KL divergence as a guide to decide whether higher-order n-grams should be kept in the model, only if they add enough extra value compared to the lower-order n-grams. More recently, this idea has been used for pruning translation models (Ling et al., 2012; Zens et al., 2012). The results show that this approach gives a better balance between the size of the phrase table and the quality of translations compared to older methods. For example, the method by Johnson et al. (2007) uses Fisher's exact test to check how well a phrase pair is backed by data, but the newer pruning method seems to work better.",
        "formal_text": "Convert casual text to formal text: Pruning is a way to tackle this issue by making models smaller by getting rid of certain parts based on specific rules. The tricky part is figuring out which parts to remove without messing"
    },
    {
        "casual_text": "In our setup, we're trying to create a semantic parser that works well with very few training examples in a bunch of new target domains. Let's say we have a set of N target domains, which we can call T = D T 1, . . . , D T N . At the same time, we have some training data from other domains, called source domains, that we can use to help build models for these new target domains. We'll call the set of M source domains S = D S 1, . . . , D S M . For each source domain d in S, we’ve got a set of annotated training data to work with.",
        "formal_text": "Convert casual text to formal text: In our setup, we're trying to create a semantic parser that works well with very few training examples in a bunch of new target domains. Let's say we have"
    },
    {
        "casual_text": "On Wikipedia, \"category\" basically means tags or topics that group together things related to the same subject. For example, Yao Ming is in the category \"Olympic basketball players of China,\" which tells you what he's about, not what kind of thing he is. Categories can be helpful for figuring out what something is if you don't already know.",
        "formal_text": "Convert casual text to formal text: On Wikipedia, \"category\" basically means tags or topics that group together things related to the same subject. For example, Yao Ming is in the category \"Olympic basketball players"
    },
    {
        "casual_text": "If we don't consider the LM score, we can calculate everything exactly. We can sort the different translation options for each hyperedge based on their costs. For hypernodes without any predecessors (basically, just words), we can generate these in a straightforward, step-by-step manner. This setup lets us, with the right approach, generate all the options for each hypernode in order of increasing cost.",
        "formal_text": "Convert casual text to formal text: If we don't consider the LM score, we can calculate everything exactly. We can sort the different translation options for each hyperedge based on their costs. For hypernodes without any"
    },
    {
        "casual_text": "The release of plWordNet 3.0 is kind of a big deal, but language resources don’t really stay stable for long. This wordnet is like a super detailed description of the Polish language, and it’s on a scale that’s way bigger than anything we’ve seen in other big, single-language dictionaries before.",
        "formal_text": "Convert casual text to formal text: The release of plWordNet 3.0 is kind a big deal, but language resources don’t really stay stable for long. This wordnet is like a super detailed description of the"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. We're looking at sentences that describe what someone did, because those actions can show how much they affected the case. We pick out sentences for each person involved and remove any that don't have verbs, since verbs are what tell us what happened. Then, we use two special models called bidirectional LSTMs (kind of like fancy tools for understanding text) to analyze these sentences and the overall case description. After that, we bring in another tool called match-lstm to combine the results from the LSTMs. This helps us figure out how much each person's actions influenced the case. Think of it like this: the overall case description is like the big picture (the hypothesis), and the sentences about what each person did are like the details (the premise). The match-lstm combines these to give us something called behavioral semantic features, which basically tells us how each person's actions fit into the whole case.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in simpler terms. We're looking at sentences that describe what someone did, because those actions can show how much they affected the case. We pick out sentences"
    },
    {
        "casual_text": "In this paper, we’re building on an idea we came up with earlier for figuring out new words in text (Murawaki and Kurohashi, 2008). Unlike languages like Chinese or Thai, Japanese has a lot of grammar, so we can use that to tell if a word we don’t know is actually a new word we should learn. This usually works great, but there’s one tricky part: noun phrases. In Japanese, there’s no special marker to show when nouns are joined together to make a phrase, so it’s hard to tell them apart from single nouns. We used to rely on a kind of guesswork to handle this, but now we’ve got a new statistical method that makes it way easier to deal with.",
        "formal_text": "Convert casual text to formal text: In this paper, we’re building on an idea we came up with earlier for figuring out new words in text (Murawaki and Kurohashi, 2008). Unlike languages like"
    },
    {
        "casual_text": "Commonsense knowledge is often used through explicit relationships, like where things are or what they look like, which are stored in knowledge graphs or as simple pairs of connected entities. Some well-known databases that do this include Never Ending Language Learner (NELL) (mentioned by T. Mitchell in 2015), ConceptNet (created by Liu and Singh in 2004), and WebChild (developed by Tandon et al. in 2017).",
        "formal_text": "Convert casual text to formal text: Commonsense knowledge is often used through explicit relationships, like where things are or what they look, which are stored in knowledge graphs or as simple pairs of connected entities. Some well-known databases that"
    },
    {
        "casual_text": "The rest of the paper goes like this: Section 2 talks about other work and articles that touch on some of the ideas we used in our design process. Section 3 explains how we designed the corpora. Section 4 tests a few tools on some parts of our corpora. And the last section looks back at what we learned from creating a gold standard for creative works.",
        "formal_text": "Convert casual text to formal text: The rest of the paper goes like this: Section 2 talks about other work and articles that touch on some of the ideas we used in our design process. Section 3 explains how we designed the corpor"
    },
    {
        "casual_text": "We've built a parsing system for LGTs. The core of this system is an abstract machine with one focus register and two stacks. These stacks keep track of the left and right contexts of the word we're focusing on, as described by Nelimarkka et al. in 1985. The system makes sure that the focused word can only connect with words from the top of either stack—with the left stack getting priority. It also includes instructions for checking the context, which can look into the stacks. In the grammar description, which is written in FUhDPL, there are three main sections. First, you declare the data types. Then, you describe the valid binary dependency relations. For each relation, you define which word pairs are allowed by using morphological or lexical attributes. The notation makes it easy to combine these attributes with boolean operations, keeping everything concise.",
        "formal_text": "Convert casual text to formal text: We've built a parsing system for LGTs. The core of this system is an abstract machine with one focus register and two stacks. These stacks keep track of the left"
    },
    {
        "casual_text": "It'll be a basic search tool for regular users, but behind the scenes, it uses fancy semantic annotations to find audiovisual files more accurately and smartly.",
        "formal_text": "Convert casual text to formal text: It'll a basic search tool for regular users, but behind the scenes, it uses fancy semantic annotations to find audiovisual files more accurately and smartly. Convert casual text to formal text"
    },
    {
        "casual_text": "Alright, let's break down what Table 7 is showing us about the comment generation dataset. The table has a few key stats: 1. The total number of samples. 2. The average number of subtokens in the code. 3. The percentage of samples where the code has fewer than 100, 150, or 200 subtokens. 4. The average number of subtokens in the comments. 5. The percentage of samples where the comments have fewer than 20, 30, or 50 subtokens. Figure 5 gives us a visual representation of how the number of subtokens in the code (on the x-axis) and the number of subtokens in the comments (on the y-axis) are distributed.",
        "formal_text": "Convert casual text to formal text: Alright, let's break down what Table 7 is showing us about the comment generation dataset. The table has a few key stats: 1. The total number of samples. 2. The average number"
    },
    {
        "casual_text": "Figure 1 shows a summary of our kanji distance metrics, specifically the l1 norm. It worked well, but it had a bit lower precision when dealing with pairs that were very similar.",
        "formal_text": "Convert casual text to formal text: Figure 1 shows a summary of our kanji distance metrics, specifically the l1 norm. It worked well, but it had a bit lower precision when dealing with pairs that were very similar"
    },
    {
        "casual_text": "So, we've got a single-layer feedforward network here, and \" R 2d\" just means it's working with 2D data. The double bars \"||\" are used to show that we're combining things, like putting them together. Now, these attention scores are kind of like how much each part matters. We use something called a softmax function to make sure these scores add up to one, which helps us figure out the \" ij\" scores for all the connections in a local area. These normalized attention scores are then used to calculate the output features for a node in a graph. Basically, we're taking all the nodes in the nearby area and combining them in a way that gives us the new features for that node, like this:",
        "formal_text": "Convert casual text to formal text: So, we've got a single-layer feedforward network here, and \" R 2d\" just means it's working with 2D data. The double bars \"||\""
    },
    {
        "casual_text": "Okay, so ATGs G1 and G2 are considered equivalent if you can map the edge labels of G1 to the edge labels of G2 using a function that covers all of G2's labels. Basically, you're just reshuffling the labels from G1 to match G2.",
        "formal_text": "Convert casual text to formal text: Okay, so ATGs G1 and G2 are considered equivalent if you can map the edge labels of G1 to the edge labels of G2 using a function that covers all of G"
    },
    {
        "casual_text": "• Gujarati and English are pretty different, so there’s not a lot of data available—whether it’s bilingual or just in one language. For each task, we check how well the unsupervised approach works compared to supervised and semi-supervised methods. Plus, we play around with the size and type of the monolingual data to test more situations and see when unsupervised NMT struggles.",
        "formal_text": "Convert casual text to formal text: • Gujarati and English are pretty different, so there’s not a lot of data available—whether it’s bilingual or just in one language. For each task, we check how well the"
    },
    {
        "casual_text": "For UDA, we're using a PyTorch-based reimplementation. We kept the batch size the same as MixText and used the hyperparameter settings suggested by Xie et al. (2020). We went with an exponential schedule for training signal annealing and set the learning rate to 2 • 10 5. We also tweaked the number of training steps for each task and dataset size, testing values from 500 up to 10,000 in increments of 500.",
        "formal_text": "Convert casual text to formal text: For UDA, we're using a PyTorch-based reimplementation. We kept the batch size the same as MixText and used the hyperparameter settings suggested"
    },
    {
        "casual_text": "Before this project, no one had connected this phenomenon to exposure bias, so people came up with different solutions to fix it. Holtzman and his team in 2020 suggested using nucleus sampling to sample from the language model. Welleck and others in 2020 proposed training neural language models with something called unlikelihood as a way to keep things in check. Li and his team in 2019 took that idea and used it for dialogue tasks. Now that we’ve figured out the connection between exposure bias and text degeneration, we might find that new methods targeting exposure bias could work really well for fixing text degeneration issues in the future.",
        "formal_text": "Convert casual text to formal text: Before this project, no one had connected this phenomenon to exposure bias, so people came up with different solutions to fix it. Holtzman and his team in 2020 suggested using nucleus sampling to"
    },
    {
        "casual_text": "When you run two NLP systems on the same data, you’d expect their results to follow certain rules or patterns. This is like a built-in assumption we have about how they should work. We’re suggesting a method where we use this idea to make one of the systems perform way better. The main concept is to focus on improving it only where the outputs match those expected patterns.",
        "formal_text": "Convert casual text to formal text: When you run two NLP systems on the same data, you’d expect their results to follow certain rules or patterns. This is like a built-in assumption we have about how they should work"
    },
    {
        "casual_text": "In this paper, we’re suggesting three different ways to automatically break words into syllables using a bunch of text data. The first method is rule-based, the second one uses sequence labeling, and the third one combines the rule-based approach with sequence labeling.",
        "formal_text": "Convert casual text to formal text: In this paper, we’re suggesting three different ways to automatically break words into syllables using a bunch of text data. The first method is rule-based, the second one"
    },
    {
        "casual_text": "While training, if a zero pronoun candidate matches something in the same spot in the annotated training data (whether it's anaphoric or not), we create a positive example. If it doesn't match anything, we make a negative example. When testing, each zero pronoun candidate gets checked by the zero pronoun detector to see if it actually is a zero pronoun.",
        "formal_text": "Convert casual text to formal text: Convert casual text to formal text: While training, if a zero pronoun candidate matches something in the same spot in the annotated training data (whether it's anaphor"
    },
    {
        "casual_text": "We pick the features from the development set. Here's a link to the word2vec project: https://code.google.com/archive/p/word2vec/",
        "formal_text": "Convert casual text to formal text: We pick the features from the development set. Here's a link to the word2vec project: https://code.google.com/archive/p/word2ve"
    },
    {
        "casual_text": "We used the SRILM toolkit (Stolcke, 2002) to estimate a Kneser-Ney smoothed trigram language model. The model was trained on Tweets from the Edinburgh Twitter corpus, but we only included tweets with no out-of-vocabulary (OOV) words except for hashtags and mentions (following Han et al., 2013). This language model was used for both training and decoding. Sometimes, during training, we come across contexts where the trigram tn, tn1, tn2 hasn't been seen in the language model data. When that happens, we don't consider the features from those trigrams when calculating the weight gradients.",
        "formal_text": "Convert casual text to formal text: We used the SRILM toolkit (Stolcke, 2002) to estimate a Kneser-Ney smoothed trigram language model. The model was trained on Tweet"
    },
    {
        "casual_text": "Once we've finished generating the current event, we toss the freshly created event sequence—basically, the extracted arguments—back into the memory storage.",
        "formal_text": "Convert casual text to formal text: Once we've finished generating the current event, we toss the freshly created event sequence—basically, the extracted arguments—back into the memory storage. Convert casual text to formal text"
    },
    {
        "casual_text": "We picked the sequence generation method in Step 1 of Procedure 1 for a couple of reasons—both practical and intuitive. We tested a bunch of different ways to generate sequences, like just splitting a word into individual letters, grouping the longest stretches of consonants together while treating each vowel separately, and doing the opposite—grouping vowels but keeping consonants apart. We also tried the method described in Step 1 of Procedure 1. Turns out, the one in Step 1 gave us the best results. On a more intuitive level, the reason it works so well is that by grouping the longest stretches of vowels or consonants (but not mixing them), each symbol in the sequence keeps the phonetic info of the word. And that phonetic info is super important when it comes to fixing spelling mistakes.",
        "formal_text": "Convert casual text to formal text: We picked the sequence generation method in Step 1 of Procedure 1 for a couple of reasons—both practical and intuitive. We tested a bunch of different ways to generate sequences, like just splitting"
    },
    {
        "casual_text": "This module lets the model work with both conditional and non-conditional pairs. It also helps make better predictions about the cause-and-effect relationship between two things, especially when there's a specific situation involved.",
        "formal_text": "Convert casual text to formal text: This module lets the model work with both conditional and non-conditional pairs. It also helps make better predictions about the cause-and-effect relationship between two things, especially when there's"
    },
    {
        "casual_text": "The usual ways to combine features in neural networks are adding them together, sticking them side by side (concatenation), or projecting them into a new space. These methods all happen after the features are processed. But for tasks like reading comprehension, where we need to think more interactively, we’d rather combine features earlier on. This way, each part can use what the others have figured out. For the QCP model, as we talked about earlier, we only use [CLS] to represent the overall meaning (holistic semantic encoding) of the QCP, not the whole input sequence. There are two reasons for this. First, we believe that focusing on the big picture helps with the interaction between questions and paragraphs. Second, we want to steer clear of the attention deconcentration problem we mentioned before.",
        "formal_text": "Convert casual text to formal text: The usual ways to combine features in neural networks are adding them together, sticking them side by side (concatenation), or projecting them into a new space. These methods all happen after the"
    },
    {
        "casual_text": "We're using a single Bi-LSTM layer for words, with hidden dimensions set to 128 for CONLL and 256 for ONTONOTES. Both models have a character embedding size of 25, and the forward and backward character LSTMs have hidden dimensions of 50. To avoid overfitting, we add a dropout mask (thanks, Srivastava et al., 2014) with a probability of 0.5 on the input and output vectors of the Bi-LSTM layer. For both datasets, we set the capitalization embeddings to 25 dimensions and trained the models for up to 50 epochs.",
        "formal_text": "Convert casual text to formal text: We're using a single Bi-LSTM layer for words, with hidden dimensions set to 128 for CONLL and 256 for ONTONOTES. Both models have a character embed"
    },
    {
        "casual_text": "Sure! Here's a more casual version: B RND = n;  n is randomly picked from (0, 1), and  n is less than or equal to  (6) B ENT = b 1, b 2, . . . , b N  (7)",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: B RND = n;  n is randomly picked from (0, 1), and  n is less than or equal"
    },
    {
        "casual_text": "Alright, so, inspired by linguistics, we realized that the idea of branching (mentioned in studies by Berg et al., 2011, and Payne, 2006) can help us understand the issue. We ran a third set of experiments to see how language branching affects accuracy. We noticed that for right-branching languages like English, the accuracy for words on the left side is usually higher than for the right side, whether we're using left-to-right or right-to-left NMT models. On the flip side, for left-branching languages like Japanese, the accuracy for the left side is usually lower than for the right side, no matter the model. The simple explanation is that right-branching languages, like English, have a clearer structure on the left side of the sentence, which is easier to predict because the main subject is usually on the left. To back this up, we looked at two things: n-gram statistics (which include n-gram frequency and conditional probabilities) and dependency parsing statistics. For right-branching languages, we found higher n-gram frequency/conditional probabilities and more dependencies on the left side compared to the right. The opposite was true for left-branching languages.",
        "formal_text": "Convert casual text to formal text: Alright, so, inspired by linguistics, we realized that the idea of branching (mentioned in studies by Berg et al., 2011, and Payne, 2006)"
    },
    {
        "casual_text": "The rest of the AdaBoost process for updating instance weights and figuring out the final combination parameters is pretty much the same as the standard AdaBoost for binary classification. The whole procedure is laid out in Algorithm 1. Using this approach, we're hoping to combine the benefits of subjectivity analysis data from different languages and improve how well we can classify subjectivity.",
        "formal_text": "Convert casual text to formal text: The rest of the AdaBoost process for updating instance weights and figuring out the final combination parameters is pretty much the same as the standard AdaBoost for binary classification. The whole procedure is laid"
    },
    {
        "casual_text": "There are two main ways to create adversarial examples, depending on how much access you have to the target model: white-box and black-box settings (Xu et al., 2019; Wang et al., 2019). In the white-box scenario, you can see everything about the model—its structure, parameters, and how it processes inputs. This extra info usually makes white-box attacks more successful because you can use what you know about the model to craft better adversarial examples. On the other hand, black-box attacks don’t need any inside knowledge about the model, which makes them more practical for real-world situations. These attacks can also be split into two types: targeted and non-targeted. A targeted attack aims to mess up a specific part of the model’s output, like a particular subtree in a phrase-level attack. A non-targeted attack, like a sentence-level one, just tries to cause any kind of error without aiming for a specific outcome.",
        "formal_text": "Convert casual text to formal text: There are two main ways to create adversarial examples, depending on how much access you have to the target model: white-box and black-box settings (Xu et al.,"
    },
    {
        "casual_text": "Alright, so we're working with random chunks of the WMT14 En-Fr dataset in different sizes: 1 million, 2.5 million, 5 million, 10 million, 20 million, and 30 million sentence pairs. In Sections 4 and 7, we'll share what we got when we trained the model on just the 1 million pair subset. Then, in Section 6, we'll dive into how the results change based on how much training data we use.",
        "formal_text": "Convert casual text to formal text: Alright, so we're working with random chunks of the WMT14 En-Fr dataset in different sizes: 1 million, 2.5 million, 5 million, 10 million, 20 million,"
    },
    {
        "casual_text": "Okay, so let’s say X is the English embedding space and Y is the Chinese one. To make these two work together in a multilingual way, we figure out a way to map X to Y using a matrix W that’s d by d. We do this with a small dictionary of m pairs, and we tweak things to make it all line up nicely.",
        "formal_text": "Convert casual text to formal text: Okay, so let’s say X is the English embedding space and Y is the Chinese one. To make these two work together in a multilingual way, we figure out"
    },
    {
        "casual_text": "If a language limits how its possible LGTs can be structured, LGTs become way more efficient and simpler to work with. The issue we talked about earlier doesn't happen with constituent grammars and phrase structure rules because these rules show hierarchy just by the names they give to the parts.",
        "formal_text": "Convert casual text to formal text: If a language limits how its possible LGTs can be structured, LGTs become way more efficient and simpler to work with. The issue we talked about earlier doesn't happen with constituent grammar"
    },
    {
        "casual_text": "Okay, so EWD is the array where English words are stored after looking them up in the dictionary, and ENG is where they're stored during the analysis. The first five spots in the VOC table hold the vowels U, O, I, E, and A. Now, if the first letter of the noun we're looking at, which is stored in EWD(1, N), starts with a consonant, then in statement 1476, K will be zero, and in statement 1477, K-1 will be negative. If the first letter is U, K-1 will be zero, and if it's any other vowel, K-1 will be positive. Basically, for any noun that doesn't start with U, statement 1477 decides whether to use \"a\" or \"an\". But if the noun starts with U, the choice depends on the third letter. Statement 1910 checks that: if the third letter is a vowel, it picks \"a\", and if it's a consonant, it picks \"an\". Of course, this system still needs some tweaking for cases where the word starts with \"un\" followed by a vowel. Now, about machine-word parts: 4.1. CSW0 (which stands for \"compose word\").",
        "formal_text": "Convert casual text to formal text: Okay, so EWD is the array where English words are stored after looking them up in the dictionary, and ENG is where they're stored during the analysis. The first five spots in the V"
    },
    {
        "casual_text": "With CKA, we can check how similar the representations are across different layers of the same model or even between different models. In our analysis, we focus on the CLS token's representations—that's the token where the final layer's output gets passed to the task's output head. We calculate CKA using the validation examples for each task.",
        "formal_text": "Convert casual text to formal text: With CKA, we can check how similar the representations are across different layers of the same model or even between different models. With CKA, we can check how similar the representations are across different"
    },
    {
        "casual_text": "So, here's the thing: if we really dive into all the legal stuff in the civil law system and use it to the fullest, how much better off can we actually be?",
        "formal_text": "Convert casual text to formal text: So, here's the thing: if we really dive into all legal stuff in the civil law system and use it to the fullest, how much better off can we actually be? Convert"
    },
    {
        "casual_text": "The negative boxes overlap first. There's a new method for smoothing the loss that just came out (Li et al., 2019), but we used a much simpler solution we came up with earlier. When we paired it with a different way of setting things up, it fixed the box crossing issue.",
        "formal_text": "Convert casual text to formal text: The negative boxes overlap first. There's a new method for smoothing the loss that just came out (Li et al., 2019), but we used a much simpler solution"
    },
    {
        "casual_text": "Basically, the model that scores higher on these metrics is the one that's doing a better job.",
        "formal_text": "Convert casual text to formal text: Basically, the model that scores higher on these metrics is one that does a better job. Convert casual text to formal text: Basically, the model that scores higher on these metrics is the"
    },
    {
        "casual_text": "Okay, let’s break this down in a simpler way. First, we’re looking at some results from different models: - **BERT 6 (PKD-Last)**: Got 91.9% accuracy. For MRPC, it scored 85.1% on the main task and 79.5% on the matched task. For QQP, it scored 70.5% and 88.9%. For MNLI-m and MNLI-mm, it got 80.9% and 81.0%. QNLI was 88.2%, and RTE was 65.0%. - **BERT 6 (PKD-Skip)**: Did slightly better with 92.0%. MRPC was 85.0% and 79.9%. QQP was 70.7% and 88.9%. MNLI-m and MNLI-mm were 81.5% and 81.0%. QNLI was 89.0%, and RTE was 65.5%. The idea here is that the student model (KD) keeps learning from the teacher and getting better, but eventually, it starts to level off. Now, for the **MRPC dataset**, there’s a theory that the vanilla KD (a simpler version) did better than our model because there aren’t enough training samples. This could cause overfitting, where the model performs well on the training data but not as well on new data. To check this, we ran the experiments three times and averaged the results. Fine-tuning got 82.23% on average, vanilla KD got 82.84%, and our method (Patient-KD) got 83.46%. So, our method did slightly better, but it might have overfitted a bit because of the small amount of training data. This overfitting thing can also be seen in the difference between the teacher and student models on the RTE dataset (check Table 5), which also has a small training set.",
        "formal_text": "Convert casual text to formal text: Okay, let’s break this down in a simpler way. First, we’re looking at some results from different models: - **BERT 6 (PKD-Last)"
    },
    {
        "casual_text": "He's probably going to marry you, unless I'm totally off base. Considering how broad this idea is, it seems a bit too simple to say Austin's two \"ifs\" are two separate uses based on the situation of the speech act, instead of just two different meanings of \"if.\"",
        "formal_text": "Convert casual text to formal text: He's probably going to marry you, unless I'm totally off base. Considering how broad this idea is, it seems a bit too simple to say Austin's two \"if"
    },
    {
        "casual_text": "The question will be represented by the controller's state at t = 1. To generate the answer, we'll use a softmax function on a fixed answer vocabulary. This approach makes sense for factoid and list questions since the possible values for a given variable are usually known. For Yes/No and Indefinite knowledge questions, we add Yes, No, Maybe to the output options. Following the approach in (Weston et al., 2014), a list-task answer is treated as a single item in the answer set, similar to how count questions are handled. An alternative could be swapping the softmax activation function at the MemN2N output for a logistic one and using categorical cross-entropy loss. However, this would require cross-validating a decision threshold to pick the right answers, which is a bit of a hassle. For count questions, the numbers found in the training set are included in the vocabulary.",
        "formal_text": "Convert casual text to formal text: The question will be represented by the controller's state at t = 1. To generate the answer, we'll use a softmax function on a fixed answer vocabulary. This approach makes sense"
    },
    {
        "casual_text": "In this paper, we're looking at how to model zero pronouns using related text. To do this, we’ve added a cool new self-attentive mechanism that helps our model pay attention to different parts of the text, which is super helpful for understanding zero anaphors. Plus, we use an attention-based method to handle candidate modeling, so our model can focus on the most important bits of the mentions. All of this makes our approach really effective for resolving zero pronouns.",
        "formal_text": "Convert casual text to formal text: In this paper, we're looking at how to model zero pronouns using related text. To do this, we’ve added a cool new self-attentive mechanism that helps"
    },
    {
        "casual_text": "We also tried using bigrams (n = 2) on the best settings for unigrams, meaning no stopword removal or stemming. The results are in the last two rows of Table 2. It looks like bigrams gave us a noticeable boost in MRR (0.733 vs. 0.699) but slightly dropped the MAP, though not by much. The improvement seems to come from bigrams doing a better job with word order and term overlap. However, using even longer n-grams (n  3) didn’t really improve things much more.",
        "formal_text": "Convert casual text to formal text: We also tried using bigrams (n = 2) on the best settings for unigrams, meaning no stopword removal or stemming. The results are in the last two rows of Table"
    },
    {
        "casual_text": "Alright, so to break down a word w, you just pick the split (r, s, t) that gives you the highest P (r, s, t).",
        "formal_text": "Convert casual text to formal text: Alright, so to break down a word w, you just pick the split (r, s, t) that gives you the highest P (r, s, t"
    },
    {
        "casual_text": "In the word-based segmenter, we use a statistical n-gram language model and aim to improve the language modeling score along with a word insertion penalty, as mentioned in Equation 4. K is a per-word penalty that we set beforehand using 10-fold cross-validation on the SIGhan PKU training set. We train a Kneser-Ney backoff language model using the training data and pull a dictionary of words from the same data to create the search space. Our initial tests showed that a bigram language model works just fine for this task.",
        "formal_text": "Convert casual text to formal text: In the word-based segmenter, we use a statistical n-gram language model and aim to improve the language modeling score along with a word insertion penalty, as mentioned in Equation"
    },
    {
        "casual_text": "Sure, it's pretty straightforward to see that the same kind of formulas work for the relational and Frobenius models too.",
        "formal_text": "Convert casual text to formal text: Sure, it's pretty straightforward to see that the same kind of formulas work for the relational and Frobenius models too."
    },
    {
        "casual_text": "Generative models tend to handle overfitting better, while discriminative models are better at ignoring irrelevant features. Both have shown solid accuracy on their own (Raymond and Riccardi, 2007), but they work in pretty different ways and encode prior knowledge in totally different styles. So, creating models that combine the strengths of both approaches seems like a really smart move.",
        "formal_text": "Convert casual text to formal text: Generative models tend to handle overfitting better, while discriminative models are better at ignoring irrelevant features. Both have shown solid accuracy on their own (Raymond and Riccardi, 2007"
    },
    {
        "casual_text": "• MT-CNN: A model that uses CNNs and shares a lookup table between different domains to create better word embeddings (Collobert and Weston, 2008).",
        "formal_text": "• MT-CNN: A model that uses CNNs and shares a lookup table between different domains to create better word embeddings (Collobert and Weston, 2008). Convert casual text to formal"
    },
    {
        "casual_text": "First off, we’ll talk about the dataset and how we’re measuring success. Then, we’ll go over the settings we used for our experiments. After that, we’ll compare how our method stacks up against other approaches, both feature-based and neural-based ones. Oh, and we also did a case study showing that our SHTCNN is pretty good at pulling out better semantic features.",
        "formal_text": "Convert casual text to formal text: First off, we’ll talk about the dataset and how we’re measuring success. Then, we’ll go over the settings we used for our experiments. After that, we’ll compare"
    },
    {
        "casual_text": "Alright, let's start by looking at how the model spreads out its predictions across all the different possible sequences.",
        "formal_text": "Convert casual text to formal text: Alright, let's start by looking at how the model spreads out its predictions across all the different possible sequences. Let's start by looking at how the model spreads out its predictions"
    },
    {
        "casual_text": "We tried adding direct textual context by comparing two methods for classifying sentences one after the other (called SSC and the new WinSSC) to the best baseline sentence classifier we had. Turns out, adding this context didn’t help—in fact, it made things worse (check out Table 3). Even when we doubled the sequence length from 5 to 10, it didn’t improve performance for either the regular SSC or the WinSSC model (F1 scores went from 38.19 to 38.22 for SSC, and 38.67 to 37.44 for WinSSC). We think this might be because there just isn’t enough data to work with. When we did 10-fold cross-validation with sequences of up to 5 sentences, we got around 1654 sequences per training iteration on average. But when we bumped it up to sequences of 10 sentences, things got messy. Oh, and Table 5 shows how RoBERTa performed with different adaptations for the domain and task.",
        "formal_text": "Convert casual text to formal text: We tried adding direct textual context by comparing two methods for classifying sentences one after the other (called SSC and the new WinSSC) to the best baseline sentence classifier we had."
    },
    {
        "casual_text": "For this project, we're mainly looking at verbal predicates because they're well-labeled and consistent in the PropBank releases. Once the ongoing work to consistently annotate noun predicates, light verb constructions, and adjectives (thanks to Bonial et al., 2014) is done, we plan to dive into SRL for those other types of predicates too.",
        "formal_text": "Convert casual text to formal text: For this project, we're mainly looking at verbal predicates because they're well-labeled and consistent in the PropBank releases. Once the ongoing work to consistently annot"
    },
    {
        "casual_text": "We looked at how unsupervised parsing works compared to few-shot parsing. Turns out, just a few labeled examples usually help most models perform better than fully unsupervised methods. Plus, models that are fine-tuned with a tiny number of labeled examples—like 15—can perform almost as well as those fine-tuned with 1,700 examples. So, we don’t need nearly as many labeled examples as existing unsupervised approaches to get pretty similar results.",
        "formal_text": "Convert casual text to formal text: We looked at how unsupervised parsing works compared to few-shot parsing. Turns out, just a few labeled examples usually help most models perform better than fully un"
    },
    {
        "casual_text": "In our test, we’re looking at prediction scores for individual sentences (called 1-block) and also for groups of five sentences (5-block). For the 5-block part, we group five sentences together, moving one sentence at a time, kind of like how we did it during training. The reason we’re doing this is to use the extra context from the surrounding sentences to help classify the main sentence in the middle. This helps with shorter sentences that don’t have much info on their own but are still part of the section we’re interested in. By doing this, we’re making sure those sentences get classified correctly when we’re trying to spot HCD sections in the model’s output. It’s all about making the most of the limited data we have.",
        "formal_text": "Convert casual text to formal text: In our test, we’re looking at prediction scores for individual sentences (called 1-block) and also for groups of five sentences (5-block). For the 5-block part, we group five sentences together"
    },
    {
        "casual_text": "Here, c_i represents the feature map from a 1-D convolution, f is the ReLU function (a non-linear thingy), and b_c is just a bias term. Equation 2 creates m filters, each with its own kernel size. These filters are made using different kernels by the first set of convolutional layers, and then they're all combined (concatenated) together.",
        "formal_text": "Convert casual text to formal text: Here, c_i represents the feature map from a 1-D convolution, f is the ReLU function (a non-linear thingy), and b_c is just"
    },
    {
        "casual_text": "Once you have z and y from equation 2, the loss for the whole model is set up like this:",
        "formal_text": "Convert casual text to formal text: Once you have z and y from equation 2, the loss for the whole model is set like this: Once you have z and y from equation 2, the loss for the whole model is set"
    },
    {
        "casual_text": "1. A way to think about how we come up with a conclusion based on some starting ideas. 2. Two different but helpful methods that help us figure out what the conclusion should be, using the information we already have. 3. Real-world proof that focusing on what we're aiming for when making a conclusion is really important.",
        "formal_text": "Convert casual text to formal text: 1. A way to think about how we come up with a conclusion based on some starting ideas. 2. Two different but helpful methods that help us figure out what the conclusion should be, using the information"
    },
    {
        "casual_text": "Detecting clickbait automatically is super important for websites to clean up their content and make things better for users. The usual way people have been doing this is by using specific features they’ve picked out manually to describe webpages. For example, Chen and his team in 2015 came up with a way to describe news articles using things like how the language sounds (like suspenseful words or numbers being overused), how the sentences are structured (like forward references), and even how the images look (like where they’re placed or if they seem emotional). They also looked at how people interact with the news, like how long they read it, if they share it, or if they comment. They used models like Naive Bayes and SVM to figure out if something was clickbait based on these features. Another group, Biyani and friends in 2016, took a different approach. They focused on features from the webpage’s content, like patterns in the words (n-grams), how positive or negative the tone is, the parts of speech used, and how many numbers are there. They also checked how similar the title was to the first few sentences of the article using TF-IDF. Plus, they considered other stuff like how casual the title sounds, if it uses forward references, and even the URL as extra clues.",
        "formal_text": "Convert casual text to formal text: Detecting clickbait automatically is super important for websites to clean up their content and make things better for users. The usual way people have been doing this is by using specific features they’ve picked"
    },
    {
        "casual_text": "We came up with a new statistical model that links how easy a document is to read with the typical age people learn certain words. Building on what others have done before, we used a logistic Rasch model and the key points in the age-learning curve to create a model for measuring how readable a document is. Then, we used data we gathered by searching the web to figure out when different words are usually learned.",
        "formal_text": "Convert casual text to formal text: We came up with a new statistical model that links how easy a document is to read with the typical age people learn certain words. Building on what others have done before, we used a logistic"
    },
    {
        "casual_text": "We used a big chunk of text in the same language for this project, and we didn’t use any language models from other bilingual datasets. The results seemed okay, and since this is focused on phrase tables, we made this choice after some testing that we’re not going into here. We also tried adding the English parts of the parallel corpora to see if it helped, but it didn’t improve the BLEU score. To stick with the materials provided by WMT11, we didn’t use the corpora from LDC.",
        "formal_text": "Convert casual text to formal text: We used a big chunk of text in the same language for this project, and we didn’t use any language models from other bilingual datasets. The results seemed okay, and since this is focused"
    },
    {
        "casual_text": "Alright, let's tweak the starting setup by tackling this constrained optimization problem:",
        "formal_text": "Convert casual text to formal text: Alright, let's tweak the starting setup by tackling this constrained optimization problem: Alright, let's tweak the starting setup by tackling this constrained optimization problem: Alright,"
    },
    {
        "casual_text": "The data works really well, especially on stuff that’s not from the same domain. Our segmentation model is pretty straightforward but super effective—it’s hitting top-notch accuracy on standard tests. Plus, it’s handy for other NLP tasks when you’ve got a little labeled data but a ton of unlabeled stuff. You can grab our code over at https://github.com/zhouh/WCC-Segmentation.",
        "formal_text": "Convert casual text to formal text: The data works really well, especially on stuff that’s not from the same domain. Our segmentation model is pretty straightforward but super effective—it’s hitting top-notch accuracy on standard tests."
    },
    {
        "casual_text": "We’ve built an anaphor resolver that uses a bunch of cool stuff like Local Constraints, Case Role Semantic Constraints, Pre/Postcondition Constraints, Case Role Persistence, Intersentential Recency Preference, and Syntactic Topicalization Preference. This resolver is part of the Universal Parser (UP) project [6, 16] happening at the Center for Machine Translation at Carnegie Mellon University. The UP uses a modified version of lexical-functional grammar [3] to combine syntactic and semantic info and fully parse each sentence. The anaphor resolver works after the fact, using the already-processed semantic case frames and syntactic trees. It tries to figure out what anaphors in the latest sentence refer to by looking at earlier sentences (both semantic and syntactic) for possible candidates. We think this resolver will be a big part of our multilingual machine translation project. To find possible referents, the resolver grabs noun phrases from the most recent sentences it’s processed. We can adjust how many sentences it looks at, which leaves room to add more discourse-related stuff later to narrow down which sentences it checks for candidates.",
        "formal_text": "Convert casual text to formal text: We’ve built an anaphor resolver that uses a bunch of cool stuff like Local Constraints, Case Role Semantic Constraints, Pre/Postcondition Cons"
    },
    {
        "casual_text": "• If the thing being talked about (the predicate nominal) is referring back to the subject, then they're connected. Think of it like they're pointing to the same thing. This part is still being worked on, though.",
        "formal_text": "Convert casual text to formal text: • If the thing being talked about (the predicate nominal is referring back to the subject, then they're connected. Think of it like they're pointing to the same thing."
    },
    {
        "casual_text": "So, we've got two sets of vectors: l and r, each with n elements. For each pair of corresponding vectors h_il and h_ir, we combine them to get a final feature vector h_if. The way we do this is by using the formula: h_if = tanh(W_l h_il + W_r h_ir + b).",
        "formal_text": "Convert casual text to formal text: So, we've got two sets of vectors: l and r, each with n elements. For each pair of corresponding vectors h_il and h"
    },
    {
        "casual_text": "To check how well we can spot mistakes in captions (like extra or missing info), we picked a random bunch of data and went through it by hand to find the real extra stuff (called E C) and the actual missing bits (called O C) in each caption. We then used a method based on the average co-2 https: //github. com/tylin/coco-caption sine similarity to compare the errors our system found (a C i &g i) with the actual errors. Figure 3 shows two examples of this process. In the examples, red highlights (like \"on a table\") mark extra details that shouldn’t be in the caption, while green highlights (like \"mashed potatoes\") show things that are missing from the caption but are in the image and the reference caption. We noticed that the errors our system found are pretty similar to the real ones in both examples (with a similarity score of 0.65 or higher). This tells us that our method can effectively catch both extra and missing information in image captions.",
        "formal_text": "Convert casual text to formal text: To check how well we can spot mistakes in captions (like extra or missing info), we picked a random bunch of data and went through it by hand to find the real extra stuff (called E"
    },
    {
        "casual_text": "Alright, so first, we figure out how much attention each token should get by doing a dot product between the query and the key for that token (that's what Equation 8 is about). This gives us a score, a_ih, which basically tells us how confident the model is that the token in position i should have label h. Now, since we have these scores for different labels (or \"heads\"), we need to turn them into probabilities. That's where the softmax function comes in (Equation 9)—it squishes those scores into a nice probability distribution. Finally, we mash all these scores together, normalize them across all the heads, and get a vector t_i with H elements. This vector is what the model spits out as its token-level prediction, and we use it both to tweak the model and to see how well it's doing as a token-level tagger.",
        "formal_text": "Convert casual text to formal text: Alright, so first, we figure out how much attention each token should get by doing a dot product between the query and the key for that token (that's what Equation 8 is"
    },
    {
        "casual_text": "We picked these two datasets to start with because they involve dealing with tricky questions and a ton of entities, many of which aren’t seen during training. Just a heads-up: the parameters in the logical forms match up with the ones in the input sentences—every open-ontology parameter value has to show up in the sentence exactly as it is. Table 2 compares our dataset to the Overnight dataset (from Wang et al., 2015) and the ATIS dataset (Dahl et al., 1994), which some earlier work translated into other languages (Sherborne et al., 2020). Our Schema2QA dataset is bigger, has more variety in terms of language, and has way more possible values for each property. In the hotels domain, there are 443 and 528 examples in the validation and test sets, and in the restaurants domain, there are 528 and 524 examples in those same splits.",
        "formal_text": "Convert casual text to formal text: We picked these two datasets to start with because they involve dealing with tricky questions and a ton of entities, many of which aren’t seen during training. Just a heads-up"
    },
    {
        "casual_text": "First off, when we train on PS M, doing full coreference resolution really messes up the performance for both models. This makes us think that in real-world situations, where the training data isn't perfectly balanced (like in the shuffled set we used, which you can check out in Table 3), doing full coreference resolution isn't a smart move. It basically makes the accuracy not much better than just guessing randomly (which is 50%). Also, when we look at training on PS BL, full coreference resolution acts differently on the two datasets. For Earthquakes, our model crushes B&L's, but for Accidents, the improvement isn't really noticeable. This is probably because of how entities are handled in these datasets. As Barzilay and Lapata pointed out back in 2008, in Earthquakes, entities often get referred to using pronouns later on, but in Accidents, it's more about repeating the exact same words.",
        "formal_text": "Convert casual text to formal text: First off, when we train on PS M, doing full coreference resolution really messes up the performance for both models. This makes us think that in real-world situations, where the training data"
    },
    {
        "casual_text": "He walked across the road. Corrections: he, walked, across, th er  the, r oad  road Normalization: he walked across the road LexiClean handles this by: 1. Letting you split up those squished-together words by adding extra spaces. 2. Fixing messed-up tokenization using a tool that lets you adjust how the text is broken down and tweak its parts (check out Figure 3(d)).",
        "formal_text": "Convert casual text to formal text: He walked across the road. Corrections: he, walked, across, th er  the, r oad  road Normalization"
    },
    {
        "casual_text": "The translation system we used for the experiments in this paper is based on Apertium, which is an open-source rule-based machine translation toolkit.",
        "formal_text": "Convert casual text to formal text: The translation system we used for the experiments in this paper is based on Apertium, which is an open-source rule-based machine translation toolkit. Convert casual text to formal text"
    },
    {
        "casual_text": "When you have different ways of ranking question-answer pairs, you usually need to combine them after getting the results. Each method gives you a list of answers in order, and then you mix them all together to make one final list. This kind of combining approach has been used in a lot of QA research. For example, in IR systems, they often merge results at the document level (like in Tsai et al., 2008), and it’s also helped improve multilingual QA (Garca-Cumbreras et al., 2012). A lot of QA systems mix answers from different versions of their models (like Brill et al., 2001 for monolingual, or Ko et al., 2010a; Ko et al., 2010b for multilingual QA). But we haven’t seen any previous work that looks at merging answers from language-specific ranking models. While our experiments didn’t show it makes things better, we think it’s an interesting new way to think about the problem.",
        "formal_text": "Convert casual text to formal text: When you have different ways of ranking question-answer pairs, you usually need to combine them after getting the results. Each method gives you a list of answers in order, and then you mix them"
    },
    {
        "casual_text": "KWPSI looks pretty similar to the traditional SI, at least when it comes to its overall appearance.",
        "formal_text": "Convert casual text to formal text: KWPSI looks pretty similar to the traditional SI, at least it comes to its overall appearance. Convert casual text to formal text: KWPSI looks pretty similar to the traditional SI,"
    },
    {
        "casual_text": "First, let's talk about how the GTAHS attaches language-specific function words to a main word. This can cause some overlap between word alignments and the way sentences are structured because many of these function words are already connected to a main word within a part of the sentence (like how the English word \"the\" is part of a noun phrase, or NP). But here's a bigger problem: the GTAHS can create fake ambiguities. We know that words can have multiple meanings when translating, like how the English word \"bank\" can mean different things and each meaning has a different Chinese word. But the GTAHS adds extra confusion that, in our opinion, could mess up Machine Translation (MT) if it tries to use these as translation rules. Take this example: the Chinese word  is linked to six different English phrases (the connected parts are underlined):",
        "formal_text": "Convert casual text to formal text: First, let's talk about how the GTAHS attaches language-specific function words to a main word. This can cause some overlap between word alignments and the way sentences are structured"
    },
    {
        "casual_text": "We've got a pseudo-algorithm in Algorithm 1 that checks if a hypothesis has odd repetitions that aren't backed up by the reference. When we're looking at repeated n-grams, we ignore punctuation and conjunctions. The REPEATED function just checks if an n-gram shows up more than once in either the hypothesis (h) or the reference (r).",
        "formal_text": "Convert casual text to formal text: We've got a pseudo-algorithm in Algorithm 1 that checks if a hypothesis has odd repetitions that aren't backed up by the reference."
    },
    {
        "casual_text": "Okay, so CRF is a supervised learning model, which means it needs training data that’s been manually labeled. On the other hand, the rule-based Semantic Parser doesn’t have that requirement. Instead, it uses linguistic patterns and dependency trees to work. This makes it more flexible and easier to apply to different problems or products with just a little tweaking. Since big service providers often deal with around 7-8k products, this flexibility is super handy. Now, when it comes to symptoms, the CRF tends to do better if there’s a lot of training data available. But for the other two attributes, the Semantic Parser edges out the CRF slightly.",
        "formal_text": "Convert casual text to formal text: Okay, so CRF is a supervised learning model, which means it needs training data that’s been manually labeled. On the other hand, the rule-based Semantic Par"
    },
    {
        "casual_text": "So, h(l) is just the combined hidden stuff at layer l of the GCN, and f is... well, you know, some function or something.",
        "formal_text": "Convert casual text to formal text: So, h(l) is just the combined hidden stuff at layer l of the GCN, and f is... well, you know, some function or something. Convert casual text"
    },
    {
        "casual_text": "We also made a new 2-hop dataset. The datasets we mentioned earlier don’t have ground-truth labels for checking how well aggregation works at each step, and most of them are 1-hop, meaning the premise and hypothesis only differ by one small change, like adding or removing a word. In our 2-hop dataset, the premise and hypothesis differ by two changes—like inserting, deleting, or swapping words or phrases. This dataset gives us ground-truth aggregation outputs  z 1, . . . , z j, . . . z n  to test how well models handle natural logic and figure out their decision-making process. Creating this 2-hop dataset involved three main steps: (a) figuring out the type of edit for each example in MED and identifying the logic relationships; (b) adding one more layer of relation, making it 2-hop; (c) recording the ground-truth aggregation labels at each step and the final NLI labels based on MacCartney’s natural logic rules. We manually checked some of the data and found that over 96% of the examples were correct. You can find more details about how we built the dataset in Appendix A.",
        "formal_text": "Convert casual text to formal text: We also made a new 2-hop dataset. The datasets we mentioned earlier don’t have ground-truth labels for checking how well aggregation works at each step, and most of"
    },
    {
        "casual_text": "2. ELMo uses max-pooling to reduce the size of the feature maps from its convolutional layers. But this method isn't great for picking up new morphological patterns from other languages because it tends to ignore patterns that aren't from English.",
        "formal_text": "ELMo uses max-pooling to reduce the size of the feature maps from its convolutional layers. But this method isn't great for picking up new morphological patterns from other languages because it tends to ignore patterns"
    },
    {
        "casual_text": "GIGABERT-JOINT: This is basically the same as MBERT-JOINT, but instead of using MBERT as the encoder, it uses GIGABERT.",
        "formal_text": "Convert casual text to formal text: GIGABERT-JOINT: This is basically the same as MBERT-JOINT, but instead of using MBERT as the encoder, it uses GIGABERT."
    },
    {
        "casual_text": "We highlight how crucial it is to calculate the CHAIR metric for both models and human-generated text. We then use this metric to do a deeper, more qualitative analysis to really get a handle on what's going on with the results.",
        "formal_text": "Convert casual text to formal text: We highlight how crucial it is to calculate the CHAIR metric for both models and human-generated text. We then use this metric to do a deeper, more qualitative analysis to really get"
    },
    {
        "casual_text": "Deep reinforcement learning (RL) has been used in a bunch of cool stuff, like computer games, text-based games, and controlling robots (Mnih et al., 2015; Narasimhan et al., 2015; Kimura, 2018). But there’s a catch: these methods need a ton of practice runs to figure out the best moves, and even then, the final strategy is super hard for humans to understand. Why? Because all the smart decisions are locked away in a deep neural network, which is kind of like a black box—you know it works, but you can’t see how. This becomes a big deal when someone wants to use RL for real-world problems and actually check if the rules make sense. If the rules were clear and easy to tweak, people could adjust them or set limits as needed. Representing the rules in a symbolic or logical way could help with both understanding and faster training, but traditional training methods struggle to learn these logical rules effectively.",
        "formal_text": "Convert casual text to formal text: Deep reinforcement learning (RL) has been used in a bunch of cool stuff, like computer games, text-based games, and controlling robots (Mnih et al.,"
    },
    {
        "casual_text": "Alright, so here's what we did: 2) We picked 50 business pitches written by students at random. Using what we learned in step 1, we made a set of rules and guidelines to help us tag the argument structures in these pitches. 3) We tested and tweaked these guidelines with three native speakers during five workshops. This helped us clear up any confusion about how to apply the rules. 4) Finally, we used our 26-page guideline to tag a bigger set of 200 student pitches. In total, we tagged 3,207 sentences. And that's the gist of it!",
        "formal_text": "Convert casual text to formal text: Alright, so here's what we did: 2) We picked 50 business pitches written by students at random. Using what we learned in step 1, we made a set of rules and guidelines to"
    },
    {
        "casual_text": "So, the vector h n+3 is used as the representation for classifying stuff.",
        "formal_text": "Convert casual text to formal text: So, the vector h n+3 is used the representation for classifying stuff."
    },
    {
        "casual_text": "In this paper, we contribute to the research on interpretable word embeddings by introducing a new way to use informative priors. This helps create predefined, interpretable dimensions, making use of the probabilistic framework's ability to express and generalize well.",
        "formal_text": "Convert casual text to formal text: In this paper, we contribute to the research on interpretable word embeddings by introducing a new way to use informative priors. This helps create predefined, interpretable dimensions, making"
    },
    {
        "casual_text": "(3) Here, f stands for a simplified version of the function that Lattice-LSTM uses to update its memory.",
        "formal_text": "Convert casual text to formal text: (3) Here, f stands for a simplified function that Lattice-LSTM uses to update its memory. Convert casual text to formal text: (3) Here, f stands for"
    },
    {
        "casual_text": "The rest of the paper is divided into a few sections. First, this section will cover some background stuff and give an overview of the phrase-based model. Then, in Section 2, we’ll talk about the changes we made to speed up decoding. Section 3 will explain how we set up the experiments, and Section 4 will show the results. Finally, the last section will wrap things up and talk about what we might do next.",
        "formal_text": "Convert casual text to formal text: The rest of the paper is divided into a few sections. First, this section will cover some background stuff and give an overview of the phrase-based model. Then, in Section 2, we’"
    },
    {
        "casual_text": "3. German-English (de-en): The training text has 100,269 German-English sentence pairs. On average, each German sentence is 22 words long.",
        "formal_text": "Convert casual text to formal text: 3. German-English (de-en): The training text has 100,269 German-English sentence pairs. On average, each German sentence is 22 words long."
    },
    {
        "casual_text": "We looked at how different paraphrasing methods create diverse paraphrases and shared the results for two key datasets in Table 3. (We’ve put all the other datasets in Appendix B because there wasn’t enough space here.) For each method and dataset, we calculated some metrics using unlabeled sentences and their paraphrases. Now, when it comes to measuring how diverse the paraphrases are, the usual BLEU score used in Neural Machine Translation isn’t great for this purpose (Bawden et al., 2020). So, we went with the bi-gram diversity (dist-2) metric suggested by Ippolito et al. (2019). This metric counts how many unique 2-grams there are and divides that by the total number of tokens. We also checked the average similarity within each set of sentences using the Universal Sentence Encoder as a separate way to compare sentences. The results showed that paraphrases made with back-translation were way too similar to each other, meaning high sentence similarity but low bi-gram diversity. On the flip side, DBS created more diverse sentences with less similarity. And guess what? Our masking strategies made this even better, boosting the diversity even more. The diversity we measured here was closely linked to how well the intent detection task performed on average, as shown in Table 4.",
        "formal_text": "Convert casual text to formal text: We looked at how different paraphrasing methods create diverse paraphrases and shared the results for two key datasets in Table 3. (We’ve put all the other datasets in Appendix"
    },
    {
        "casual_text": "We wanted to see how well BERT can predict transitive IS-A relationships, so we tested it on a bunch of cases. Imagine we have three word senses: A, B, and C, where A is a B, and B is a C. We checked how often BERT could correctly predict that A is a C, if it already got A is a B and B is a C right. We looked at 666 of these transitive triples and put the results in Table 2. In the table, p(AB) shows the percentage of times BERT correctly predicted A is a B out of the 666 pairs. Same goes for p(BC) and p(AC). p(AC|AB, BC) tells us how often BERT got A is a C right, given that it already predicted A is a B and B is a C correctly. The table shows that when BERT says A is a B and B is a C, it’s right about A is a C 82.4% of the time. But since it doesn’t always get A is a C right, even when it’s right about the other two, it seems like BERT isn’t always making logically consistent predictions.",
        "formal_text": "Convert casual text to formal text: We wanted to see how well BERT can predict transitive IS-A relationships, so we tested it on a bunch of cases. Imagine we have three word senses: A, B, and"
    },
    {
        "casual_text": "For every pattern we create, there should be at least one example that matches it—the sentence we used to make the pattern in the first place. But we don’t know how accurate each pattern is. That’s why we need an extra step between making the patterns and using them: pattern evaluation. This idea isn’t new—it’s been talked about in research, especially when it comes to bootstrapping (like in Ravichandran and Hovy’s work from 2002). The main goal of this step is to check how well each pattern works by looking at the data we have about questions and their correct answers. We want to see how often the pattern gives the right or wrong answer. This info gets saved with the pattern, and something called \"pattern precision\" (basically, how good the pattern is) can be used later when we’re trying to find answers. In our case, pattern precision is calculated as:",
        "formal_text": "Convert casual text to formal text: For every pattern we create, there should be at least one example that matches it—the sentence we used to make the pattern in the first place. But we don’t know how accurate each pattern is"
    },
    {
        "casual_text": "With the data x and the correct answers y, supervised learning models try to figure out the best parameters  that make the log-likelihood as big as possible.",
        "formal_text": "Convert casual text to formal text: With the data x and the correct answers y, supervised learning models try to figure out the best parameters  that make the log-likelihood big possible. Convert casual text"
    },
    {
        "casual_text": "Singlish, as mentioned by Leimgruber in 2009, is basically like this: He’s curious to see how we speak.",
        "formal_text": "Convert casual text to formal text: Singlish, as mentioned by Leimgruber in 2009, is basically like this: He’s curious to see how we speak. Convert casual text to formal text: Singl"
    },
    {
        "casual_text": "In this paper, we’re testing out both methods using the same semantic space. This space is built from a corpus with 120 million tokens. The rows in this space represent word forms, and the columns represent word lemmas found in the corpus. We’re following the parameters from (Mitchell and Lapata, 2010) for our semantic space: a window size of 10 and a dimension size of 2000 (basically, the 2000 most common lemmas). We’re not removing stop words because, as shown in (Bullinaria and Levy, 2007), they can actually be helpful for different semantic similarity tasks. We’re also using positive point-wise mutual information to calculate the values of the vector components, which, as mentioned in (Bullinaria and Levy, 2007), has been shown to work well.",
        "formal_text": "Convert casual text to formal text: In this paper, we’re testing out both methods using the same semantic space. This space is built from a corpus with 120 million tokens. The rows in this space represent word forms,"
    },
    {
        "casual_text": "We're mainly focusing on evaluating different methods using linked mentions because detecting NIL (No Information Link) isn't our main goal. However, we do use NIL mentions in Section 5.6 to create a full entity linking system. Check out Table 3 for the entity linking accuracy percentages across different business categories and feature types. In the \"Features\" column, \"C,\" \"S,\" and \"L\" stand for conventional, social, and location features, respectively. \"All\" refers to all categories combined, which is basically the entire test set. The \"pose\" part is interesting and worth looking at separately from the usual entity linking problem. The accuracy of Di-rectLink shows that around 67% of mentions in Yelp-EL just point to the businesses being reviewed. But don't think this makes our problem easier than traditional entity linking. Using just the popularity of entities can already get you about 82% accuracy in that task (Pan et al., 2015).",
        "formal_text": "Convert casual text to formal text: We're mainly focusing on evaluating different methods using linked mentions because detecting NIL (No Information Link) isn't our main goal. However, we do use NIL"
    },
    {
        "casual_text": "We're suggesting a method that uses data from the web to figure out p(i), p(c), and p(r). If you have domain knowledge, you can use that too, but it's not necessary. We'll start by looking at the probability of entities, p(e), but we're leaving out literals for now. If people mention an entity more often, it's more likely to show up in a question. We're using a big dataset called FACC1 (from Gabrilovich et al., 2013) that has around 10 billion mentions of entities from Freebase in over 1 billion web documents. The linking accuracy is around 80-85% for precision and 70-85% for recall. If an entity e has n(e) mentions, then p(e) is calculated as n(e) divided by the total number of mentions for all entities. For a class c, the probability p(c) is higher if it has more entities that are frequently mentioned. If e is an instance of c, then p(c) is the total mentions of entities in c divided by the total mentions of all entities in all classes. Estimating p(r) is tricky because it involves extracting relations from text. We simplify this by saying if (e1, r, e2) is a fact in the knowledge base, we add 1 to n(r) whenever e1 and e2 appear together in a document. This helps us tell common relations from rare ones. Then, p(r) is just n(r) divided by the total n(r) for all relations. Finally, we use frequency data from the knowledge base to smooth out the probabilities, like avoiding zeros. The probabilities for literals are based only on the frequency data from the knowledge base. Check out Appendix C for the final probability distributions.",
        "formal_text": "Convert casual text to formal text: We're suggesting a method that uses data from the web to figure out p(i), p(c), and p(r). If you have domain knowledge, you can use"
    },
    {
        "casual_text": "This tool creates a short, clear, and useful summary of a scientific paper, covering the main points. You can either ask it to focus on a specific question or just get a general overview. Scientific papers can be tricky—they're usually long, organized in different ways, and often use different language in different parts, like the introduction versus the experiment section. To make sure we give each part the attention it deserves, we decided to make a separate summary for each section. This way, we’re summarizing smaller, more focused pieces of text, and it’s easier for you to follow along since you can see the paper’s structure. Finally, all these section summaries are put together to give you a complete summary of the whole paper.",
        "formal_text": "Convert casual text to formal text: This tool creates a short, clear, and useful summary of a scientific paper, covering the main points. You can either ask it to focus on a specific question or just get a"
    },
    {
        "casual_text": "We came up with a way to clean up short social media posts using paraphrasing. We collected a bunch of tweets and their cleaned-up versions from parallel data on microblogs, all using machine translation techniques. After that, we created two models to learn how to normalize these posts—one works on phrases and the other on individual characters. Finally, we made a decoder that uses both models together during the process. The fact that our method improved several machine translation systems shows it actually works.",
        "formal_text": "Convert casual text to formal text: We came up with a way to clean up short social media posts using paraphrasing. We collected a bunch of tweets and their cleaned-up versions from parallel data on microblogs"
    },
    {
        "casual_text": "Alright, so we're working with a gold standard that includes all 5 quantified classes from QMR, but we've taken out the KIND items. We're using the majority opinion for the annotations, which gives us 6156 instances. The natural language quantifiers are turned into numbers—you can check out the details in section 4. With these numbers, we calculated the average Spearman rank correlation between the three annotators, and it came out to 0.63.",
        "formal_text": "Convert casual text to formal text: Alright, so we're working with a gold standard that includes all 5 quantified classes from QMR, but we've taken out the KIND items. We're using the majority"
    },
    {
        "casual_text": "If mention i isn't a split-antecedent anaphor or if Y(j) (the possible antecedents) doesn't include any mentions from GOLD(i), then we just set GOLD(i) to be empty, like  .",
        "formal_text": "Convert casual text to formal text: If mention i isn't a split-antecedent anaphor or if Y(j) (the possible antecedents) doesn't include any mentions from"
    },
    {
        "casual_text": "This perspective also shows that if the goal is to make labels for a bunch of sentences without any labels, you don't need to go through the whole dataset. For instance, if we're okay with kind of rough or not-so-perfect labels for 4,000 Hinglish tweets, the labels we get would still be about 90% accurate.",
        "formal_text": "Convert casual text to formal text: This perspective also shows that if the goal is to make labels for a bunch of sentences without any labels, you don't need to go through the whole dataset. For instance, if we"
    },
    {
        "casual_text": "1. f 1 and e 1 2. f 2 and e 1 3. f 3 4. f 3 and e 5 5. f 5 and e 4 6. f 2 7. f 6 8. f 6 and e 2 9. f 6 and e 3 10. f 5",
        "formal_text": ""
    },
    {
        "casual_text": "We use the SRL model, which relies on a bidirectional Long Short-term Memory (LSTM) network for sequential labeling. This LSTM thing was introduced by Hochreiter and Schmidhuber back in 1997, and it's been updated over the years by folks like Graves and Schmidhuber in 2005 and Graves et al. in 2013.",
        "formal_text": "Convert casual text to formal text: We use the SRL model, which relies on a bidirectional Long Short-term Memory (LSTM) network for sequential labeling. This LSTM thing was introduced by Hochreiter"
    },
    {
        "casual_text": "There are three main early exiting strategies for BERT: BranchyNet, FastBERT, and Dee-BERT. These methods use the entropy of the prediction probability distribution to measure the confidence of the exiting classifiers, allowing the model to exit early. On the other hand, Shallow-Deep Nets and RightTool use the softmax scores of the predictions. If a particular class has a dominant and high enough score, the model exits. More recently, PABEE introduced a patience-based strategy, similar to early stopping during model training. Basically, if the model's predictions stay the same for a set number of times (called patience), it stops the inference and exits. PABEE has been getting state-of-the-art results for BERT early exiting.",
        "formal_text": "Convert casual text to formal text: There are three main early exiting strategies for BERT: BranchyNet, FastBERT, and Dee-BERT. These methods use the entropy of the prediction probability distribution to"
    },
    {
        "casual_text": "We tell our annotators to always connect slot fillers to the EXPERIMENT mention that’s closest in the sentence. If an experiment description is spread across multiple clauses, we use the \"same exp\" relation to link the two EXPERIMENTs together. For experiments done on the same cell but with slightly different conditions, we use \"exp variation\" to link them. The \"exp variation\" link can also connect two frame-evoking elements that talk about measurements on different materials or cells but under the same conditions. These elements often suggest a comparison, like \"increase\" or \"reach from...to...\".",
        "formal_text": "Convert casual text to formal text: We tell our annotators to always connect slot fillers to the EXPERIMENT mention that’s closest in the sentence. If an experiment description is spread across multiple clauses, we use"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. One way to train models is by using extra bits of info about the article, like who wrote it, when it was made, and where it came from. You add this info as extra tokens at the start of the article before training the model. These tokens give the model more context about the article, helping it understand how the extra info connects to the main content. After training, you can control the model by giving it the same kind of extra info that you want. The first model that does this is called GROVER. It can make a news article based on details like the headline, author, and date. The tricky part is, GROVER can create really convincing fake news that's harder for people to spot than fake news written by humans. This could be a big problem. Another model, called CTRL, works similarly but uses control codes found naturally in the text, like the URL of a news article. These codes help the model adjust the style, content, and even how it performs specific tasks, like answering questions or translating text. For example, it can switch between writing about sports or politics, or even between different sources like FOX Sports or CNN Sports.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. One way to train models is by using extra bits of info about the article, like who wrote it, when it was made,"
    },
    {
        "casual_text": "1 Since we have to keep things confidential, all the customer feedback examples in this paper are made up by us and are just for showing you how it works.",
        "formal_text": "Convert casual text to formal text: 1 Since we have to keep things confidential, all the customer feedback examples in this paper are just for showing you it works."
    },
    {
        "casual_text": "Word representations, or word embeddings as they're often called (Turian et al., 2010), play a big role in a bunch of NLP tasks, including Named Entity Recognition (NER) (Collobert et al., 2011). Since there isn't a lot of labeled data for named entities, embeddings are used to enhance, not replace, manually created features to get the best performance possible (Lample et al., 2016). Lately, some studies (Yang et al., 2017; Sgaard and Goldberg, 2016) have been looking into ways to give deep sequential taggers extra features on top of the usual embeddings. Peters et al. (2017) and Tran et al. (2017) tried using special embeddings pulled from a neural language model (LM) trained on a huge dataset. These LM embeddings pick up on context-dependent meanings by looking at both the words that come before (backward LM) and after (forward LM) a word. When you add this info to the standard features, it really boosts NER performance. Plus, Chiu and Nichols (2016) found that external knowledge resources, like gazetteers, are super important for NER. Gazetteer features basically check if certain word n-grams are in predefined lists of named entities.",
        "formal_text": "Convert casual text to formal text: Word representations, or word embeddings as they're often called (Turian et al., 2010), play a big role in a bunch of NLP tasks"
    },
    {
        "casual_text": "To dig deeper into our model, we did some ablation studies, and the results are in Table 4.",
        "formal_text": "Convert casual text to formal text: To dig deeper into our model, we did some ablation studies, and the results are in Table 4. Convert casual text to formal text: To dig deeper into our model, we did some ablation"
    },
    {
        "casual_text": "Alright, during the machine translation (MT) part, users can keep an eye on, and if needed, fix one really important thing: how the words are chosen or translated, which is called lexical disambiguation.",
        "formal_text": "Convert casual text to formal text: Alright, during the machine translation (MT) part, users can keep an eye on, and if needed, fix one really important thing: how the words are chosen or translated, which is called"
    },
    {
        "casual_text": "Alright, let's dive into the model, the tasks it can handle, and the different ways you can tweak its settings.",
        "formal_text": "Convert casual text to formal text: Alright, let's dive into the model, the tasks it can handle, and the different ways you tweak its settings. Convert casual text to formal text: Alright, let's dive"
    },
    {
        "casual_text": "Okay, so, based on what Ramshaw and Marcus said back in 1995. These symbols tell us if a word is part of an argument (inside the brackets), not part of an argument (outside the brackets), or right where two arguments meet (on the boundary). Check out Table 1 for more details. With this setup, figuring out the arguments is all about finding the best sequence of tags for the input, like in Equation 3. We’re trying to estimate the probability of that happening.",
        "formal_text": "Convert casual text to formal text: Okay, so, based on what Ramshaw and Marcus said back in 1995. These symbols tell us if a word is part of an argument (inside the brackets), not part of"
    },
    {
        "casual_text": "Figure 4 shows how skipping chunks of Transformer layers affects task performance. The Y-axis and X-axis represent the first and last layers being skipped, and there's no additional fine-tuning. Generally, performance goes down when more layers are skipped, but skipping just one layer usually doesn't hurt much, except for the very first layer. Like the results we saw earlier, skipping some of the higher layers doesn't seem to mess things up too much.",
        "formal_text": "Convert casual text to formal text: Figure 4 shows how skipping chunks of Transformer layers affects task performance. The Y-axis and X-axis represent the first and last layers being skipped, and there'"
    },
    {
        "casual_text": "This subroutine lets you switch \"O\"-bits to \"1\"-bits and the other way around. It works kind of like this: J = LBIT(WORD, N). The result stored in J will be \"1\" if a \"0\" was changed to \"1\", and \"0\" if a \"1\" was changed to \"0\".",
        "formal_text": "Convert casual text to formal text: This subroutine lets you switch \"O\"-bits to \"1\"-bits and the other way around. It works kind of like this: J = LBIT(WORD,"
    },
    {
        "casual_text": "When we need to, we use the MLP with 'softmax loss' model checkpoint from Drozdov et al. (2019a). S-DIORA tweaks DIORA in a meaningful way, but it uses the exact same parameters, so it's super easy to load a pretrained DIORA model for S-DIORA. We’ve got our version of S-DIORA, the best model checkpoints, training scripts, and all the parsing output online for you to check out. Oh, and there are 7 extra training details in Appendix A.1 if you're interested.",
        "formal_text": "Convert casual text to formal text: When we need to, we use the MLP with 'softmax loss' model checkpoint from Drozdov et al. (2019a). S-DIORA tweaks DI"
    },
    {
        "casual_text": "The last method we're looking at is from Lin (1998), which we'll call the \"lin\" metric. He came up with a way to measure semantic similarity using a formula based on information theory. This method is sometimes referred to as a universal semantic similarity measure because it’s supposed to work across different applications, domains, and resources. The similarity score according to this method is calculated using Equation 5.",
        "formal_text": "Convert casual text to formal text: The last method we're looking at is from Lin (1998), which we'll call the \"lin\" metric. He came up with a way to measure semantic similarity using"
    },
    {
        "casual_text": "The main idea here is to use conceptual or terminological knowledge to guide how language works in general. We call this \"performance control.\" Basically, we're building a kind of grammar that can be adjusted based on this conceptual knowledge. The actual work is done through something we call \"terminological anchors,\" which connect different concepts to the general rules of language. What makes our approach different is that we don’t need a special bridge between different types of knowledge. Instead, we use the same tool, called the ALEP formalism, to model everything. The cool thing about this is that if the conceptual knowledge can’t help with something—like when there’s ambiguity in the subject itself—we still have the full grammar to fall back on.",
        "formal_text": "Convert casual text to formal text: The main idea here is to use conceptual or terminological knowledge to guide how language works in general. We call this \"performance control.\" Basically, we're building a kind of grammar that can"
    },
    {
        "casual_text": "We looked at some of the best-performing systems that either hit the top scores (state-of-the-art) or came close to it, and we were able to get their outputs for the CNNDM dataset. For extractive summarization, we checked out systems like CNN-LSTM-BiClassifier (CLSTM-SL; Kedzie et al., 2018), Latent (Zhang et al., 2018), BanditSum (Dong et al., 2018), REFRESH (Narayan et al., 2018), NeuSum, HIBERT (Zhang et al., 2019b), Bert-Sum-Ext (Liu and Lapata, 2019a), and MatchSum (Zhong et al., 2020). That’s a total of 11 extractive systems for each document in the CNNDM test set. For abstractive summarization, we used systems like pointer-generator+coverage (See et al., 2017), fastAbsRL (Chen and Bansal, 2018), fastAbsRLrank (Chen and Bansal, 2018), Bottom-up (Gehrmann et al., 2018), T5 (Raffel et al., 2019), Unilm-v1 (Dong et al., 2019), Unilm-v2 (Dong et al., 2019), twoStageRL (Zhang et al., 2019a), pre-SummAbs (Liu and Lapata, 2019b), preSummAbsext (Liu and Lapata, 2019b), BART (Lewis et al., 2019), and Semsim (Yoon et al., 2020). That gives us 14 abstractive system outputs for each document in the CNNDM test set.",
        "formal_text": "Convert casual text to formal text: We looked at some of the best-performing systems that either hit the top scores (state-of-the-art) or came close to it, and we were able to get their outputs"
    },
    {
        "casual_text": "When you're recording your speech, make sure to address the points brought up in the government's opening speech. Basically, just interact with them the way you would in a British Parliamentary debate or any other academic debate format.",
        "formal_text": "Convert casual text to formal text: When you're recording your speech, make sure to address the points brought up in the government's opening speech. Basically, just interact with them the way you would in a British Parliamentary"
    },
    {
        "casual_text": "When we used the best majority filtering method on the data with a 50% injection ratio, we found a really strong correlation.",
        "formal_text": "Convert casual text to formal text: When we used the best majority filter method on the data with a 50% injection ratio, we found a really strong correlation. Convert casual text to formal text: When we used the best majority filter"
    },
    {
        "casual_text": "We’ve talked about the LMG framework, how it works in practice, and what we know about its complexity, especially for a specific subset of cases. Take Example 2.9, for instance—it shows how LMG can neatly explain movement patterns. The complexity result in section 3.5 is mostly there to give us an idea of how tough it is to work with LMG compared to regular context-free grammars. Just to be clear, though, the result we’re discussing here only applies to non-combinatorial LMGs, so it doesn’t cover things like the grammar in Example 2.9 as it’s presented.",
        "formal_text": "Convert casual text to formal text: We’ve talked about the LMG framework, how it works in practice, and what we know about its complexity, especially for a specific subset of cases. Take Example 2.9, for instance"
    },
    {
        "casual_text": "Don’t share any personal info. Stick to your role, meaning talk like the user or the assistant.",
        "formal_text": "Convert casual text to formal text: Don’t share any personal info. Stick to your role, meaning talk like the user or the assistant. Stick to your role, meaning talk like the user or the assistant. Convert casual text to"
    },
    {
        "casual_text": "But neural models usually need a ton of really good training data, and that’s not something we have for lexical normalisation. Even though there are a bunch of open-source tools for token-level annotation (like the ones by Stenetorp et al., 2012; Yimam et al., 2013; Yang et al., 2017; Kummerfeld, 2019), they don’t really help with lexical normalisation much.",
        "formal_text": "Convert casual text to formal text: But neural models usually need a ton of really good training data, and that’s not something we have for lexical normalisation. Even though there are a bunch of open-source"
    },
    {
        "casual_text": "The features we've discovered are way easier to understand compared to what Roller et al. (2014) found. They didn't spot any H-features in their analysis of one classifier. Their model was based on bag-of-words distributional vectors, which don't work as well for this task. They also looked at lexical entailment classifiers and noticed some weak signals, like \"such\" and \"of,\" showing up a lot in their classifier. This gave a little hint that there might be H-feature detectors, but not nearly as strong as what we're seeing here. The key difference is that they focused on a classifier trained with high-dimensional, sparse vectors, instead of context embeddings like we did. Using those sparse vectors, their model couldn't handle similar contexts very well. Plus, their model didn't use collapsed dependencies, so features like \"such\" weren't as strong indicators of entailment, making them less noticeable in their analysis.",
        "formal_text": "Convert casual text to formal text: The features we've discovered are way easier to understand compared to what Roller et al. (2014) found. They didn't spot any H-features in their analysis of one classifier"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Table 1 shows how well people agree on different types of annotations. It turns out that marking events and time expressions is easier, but figuring out the relationships between them is harder, which is why the agreement scores are lower for those. This makes sense because relationships involve two things, while events and time expressions are just one thing each. Interestingly, even something that seems simple, like picking the main event in a sentence, has a surprisingly low agreement score. One reason for this could be that in languages like Chinese, which aren't as strict with grammar, it's not always clear which verb is the main one, especially if the annotation tool doesn't show the sentence structure. Another reason is that sometimes, the people doing the annotations just don't agree on what the main event should be. Here's what each part in Table 1 means: - **event-extent**: This is about marking the part of the text that describes an event. - **timex-extent**: This is about marking the part of the text that mentions a time expression. - **tlinks-main-event**: This is about figuring out the relationship between the main events in a sentence. - **tlinks-dct-events**: This is about the relationship between an event and when the document was created. - **tlinks-e-t**: This is about the relationship between an event and a time expression. - **tlinks-sub-e**: This is about the relationship between one event that's part of another event (like a sub-event).",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. Table 1 shows how well people agree on different types of annotations. It turns out that marking events and time expressions is easier"
    },
    {
        "casual_text": "Once we've got both the supervised loss and the unsupervised loss, we mix them together to get the final loss. We use something called a loss annealing scheduler (check out Equation 5) to do this. Basically, it slowly adds more of the unsupervised loss as the training goes on.",
        "formal_text": "Convert casual text to formal text: Once we've got both the supervised loss and the unsupervised loss, we mix them together to get the final loss. We use something called a loss annealing scheduler (check"
    },
    {
        "casual_text": "Hallucinations aren't just a problem in Vision & Language; they also mess with Multimodal Machine Translation systems. Lala and Specia (2018) pointed out the challenges of translating ambiguous or polysemic words when you have a visual context to deal with. Rohrbach et al. (2018) looked into object hallucination in image captioning, which is pretty similar to what we're working on. They came up with a new metric called CHAIR to measure how much machine-generated captions include made-up stuff. They figured out that relying too much on language patterns was a big reason for these hallucinations. Plus, they noticed that models with better visual understanding tend to hallucinate less, which shows how important it is to process visual input well. We're using the CHAIR metric to test different models and see how different visual representations play a role. There's a recent study by Xiao and Wang (2021) that connects hallucinations with predictive uncertainty in image captioning and data-to-text generation. They found that higher uncertainty increases the chance of hallucinating entities. We're planning to dive into that kind of analysis later on.",
        "formal_text": "Convert casual text to formal text: Hallucinations aren't just a problem in Vision & Language; they also mess with Multimodal Machine Translation systems. Lala and Specia (2018) pointed out the challenges of translating"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: a__i = sum from j=1 to m of _ij * h__j (1) _ij = exp( * sim(v_i, h__j)) / sum from k=1 to m of exp( * sim(v_i, h__k)) (2)",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: a__i = sum from j=1 to m of _ij * h_"
    },
    {
        "casual_text": "But maybe the coolest thing about Europe right now is how many companies are popping up that focus on software localization. These guys are getting really good at using tools and machine translation systems, like Logos, Metal, and XL8. To help everyone share ideas and set some standards, the Localisation Industry Standards Association was created back in 1990. They even put out a newsletter called the LISA Forum and made a CD-Rom with all kinds of info about products, standards, and methods called the LISA Showcase. Ireland is a big player in this space—since 1994, they’ve had their own Software Localisation Group, which hosts conferences and workshops. They’ve also just launched a Localisation Resources Center with help from the Irish government and the EU.",
        "formal_text": "Convert casual text to formal text: But maybe the coolest thing about Europe right now is how many companies are popping up that focus on software localization. These guys are getting really good at using tools and machine translation systems, like Logos"
    },
    {
        "casual_text": "Since the term \"syntax\" alone isn't specific enough, they added a part that really limits how the parser builds structures. This part, which is based on Lexical Phonology, sets rules for the order in which affixes and stems can be combined. As a result, it limits what kind of stems an affix or another stem can connect to, or how complex that stem can be. This helps avoid mistakes like assigning the wrong word class or incorrectly splitting up words, and can even lead to rejecting structures that don’t work.",
        "formal_text": "Convert casual text to formal text: Since the term \"syntax\" alone isn't specific enough, they added a part that really limits how the parser builds structures. This part, which is based on"
    },
    {
        "casual_text": "Basically, in terminography, like the EIRETERM database, they’re all about concepts and how they’re expressed in specific terms, which come from texts (this is called term identification). When it comes to translation, though, it’s more about the act of creating something—it’s a dynamic process where you move from one language’s text to another. Within this process, there’s a step where you match up the meaning units from one culture with those from another, before figuring out how to express them in the right way for the text and situation. But when it comes to terminology, these meaning units aren’t really the focus because they’re just temporary, random groupings of concepts that an author puts together. Translation needs to deal with concepts and terms in their actual context, while terminology takes terms out of context (decontextualization) and links them directly to concepts. So, it’s more about matching a term to a concept versus matching textual units through concepts.",
        "formal_text": "Convert casual text to formal text: Basically, in terminography, like the EIRETERM database, they’re all about concepts and how they’re expressed in specific terms, which come from texts (this is called term identification"
    },
    {
        "casual_text": "Theorem G. 9 only works if we make some pretty reasonable assumptions. Remember, we started with a set of graph constants C, which picks source names from a set S, along with a set of types  and a set of edge labels L. Now, let’s spell out a few key assumptions about how these things are connected:",
        "formal_text": "Convert casual text to formal text: Theorem G. 9 only works if we make some pretty reasonable assumptions. Remember, we started with a set of graph constants C, which picks source names from a set"
    },
    {
        "casual_text": "To test out the solutions we talked about in section 4, we ran some experiments on different text classification tasks.",
        "formal_text": "Convert casual text to formal text: To test out the solutions we talked about in section 4, we ran some experiments on different text classification tasks."
    },
    {
        "casual_text": "In multiple-distinct coordination, each part of the sentence gets its own clause, but any repeated stuff across those clauses gets cut out based on the extended directionality rules talked about in Section 5.4. This works because it uses the parallel structure you can see in the sentence.",
        "formal_text": "Convert casual text to formal text: In multiple-distinct coordination, each part of the sentence gets its own clause, but any repeated stuff across those clauses gets cut out based on the extended directionality rules talked about in Section"
    },
    {
        "casual_text": "So, there’s not a lot of research on how people pitch business models using language, and what little there is, is still pretty new (Ducasse, 2020). It’s no big surprise, then, that no one’s made a collection of these pitches with special notes about how the arguments are structured (Lawrence and Reed, 2019). We thought, “Hey, why not create a new way to label these pitches so we can see the different parts of the arguments, how they connect, and how strong they are?” We used Toulmin’s model (1984) as our guide. To make this happen, we followed a four-step plan: first, we looked at research and theories about argument structures and models in different kinds of texts.",
        "formal_text": "Convert casual text to formal text: So, there’s not a lot of research on how people pitch business models using language, and what little there is, is still pretty new (Ducasse, 2020). It’s no"
    },
    {
        "casual_text": "A way to fix this issue is to stop a single daughter from sharing more than one of its branches with its mom. But we’re not doing that because it could still lead to trees where every branch is the same length, which would break the rule about having only a limited number of branches that keep growing. Figure 4 shows how this can happen by demonstrating what happens when you use the production mentioned earlier.",
        "formal_text": "Convert casual text to formal text: A way to fix this issue is to stop a single daughter from sharing more than one of its branches with its mom. But we’re not doing that because it could still lead to trees where every"
    },
    {
        "casual_text": "To create the candidate generation function, we start by aligning each dataset using GIZA++. Then, we build a bilingual lexicon with probabilities based on maximum likelihood (P mle ) from the aligned data. After cleaning up the lexicon by removing less frequent and less significant entries, we keep the top 200 translations for each word in the source language, ranked by P mle (t|s) • P mle (s|t). These translations are considered the candidate word pairs. We also use these word alignments to train the BNN models. Each alignment link is treated as a training example, and we don’t do anything special for words that don’t align or for cases where one word aligns with multiple words.",
        "formal_text": "Convert casual text to formal text: To create the candidate generation function, we start by aligning each dataset using GIZA++. Then, we build a bilingual lexicon with probabilities based on maximum likelihood (P"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: 1. **, , g,  |= X: Y: P** means that if  ((Y )) is in (P ) and (X) *  (Y ), then it's true. 2. **, , g,  |= ARG S (X, a)** means there's an i in S where (X) • i is in D( ) and  ((X) • i) equals g(a). 3. **, , g,  |= ARG S (X, Y )** means there's an i in S where (X) • i is in D( ) and (X) • i equals (Y ). 4. **, , g,  |= X * Y** means (X) * (Y ) is true. 5. **, , g,  |= X =/ = Y** means (X) is not equal to (Y ). 6. **, , g,  |= v 1 =/ = v 2** means g(v 1 ) is not equal to g(v 2 ). 7. **, , g,  |= P Q** means (P ) is a subset of (Q). In simpler terms, these are rules that explain how different things relate to each other based on certain conditions.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: 1. **, , g,  |= X: Y: P** means that if"
    },
    {
        "casual_text": "The models have a lot of parameters, and there are many different ways to train them. This makes trying out every possible combination really time-consuming—like, it could take 2 to 3 months on a regular computer.",
        "formal_text": "Convert casual text to formal text: The models have a lot of parameters, and there are many different ways to train them. This makes trying out every possible combination really time-consuming—like, it could take 2 to 3 months on"
    },
    {
        "casual_text": "For the QE models in SyntheticQE, we basically followed what Kepler et al. (2019) did and used a BERT-based model for both sentence-level and word-level tasks. We fine-tuned these models on the synthetic data. For the optimizer, we went with Adam (Kingma and Ba, 2015) and set 1 to 0.9, 2 to 0.999, and  to 10-8. We also set the batch size to 128, the max sequence length to 256, the number of training steps to 100,000, the learning rate to 510-5, and the dropout rate to 0.1. We checked how our model was doing every 1,000 steps and picked the one that performed best on the development set for actual use. We also adjusted the threshold on the development set to get the best word-level performance.",
        "formal_text": "Convert casual text to formal text: For the QE models in SyntheticQE, we basically followed what Kepler et al. (2019) did and used a BERT-based model for both sentence-level"
    },
    {
        "casual_text": "We've come up with the idea of Multiple Tasks Integration, which is all about how tasks work together without just sharing the same weights. The key here is to handle the input at the same time but on different levels, making decisions based on everything we've figured out so far, without sticking to a specific order. This lets the tasks really connect and work together. To make this system learn, we suggest using reinforcement learning, which helps it figure out the best way to handle the order on its own.",
        "formal_text": "Convert casual text to formal text: We've come up with the idea of Multiple Tasks Integration, which is all about how tasks work together without just sharing the same weights. The key here is to handle the input at the same"
    },
    {
        "casual_text": "We've set some limits on how sentences can be matched up. Basically, we think each sentence should only be part of one \"sentence bead\" and the order of things should stay the same when aligning sentences. Let's say Pi = (ai, xi; bi, Yi), those rules would look like this:",
        "formal_text": "Convert casual text to formal text: We've set some limits on how sentences can be matched up. Basically, we think each sentence should only be part of one \"sentence bead\" and the order of things should"
    },
    {
        "casual_text": "At low recall values, the baselines are pretty small, which means the model is very confident in its predictions. This makes sense because our joint model can adjust how much it relies on direct sentence and relation paths, and it tends to trust the Text Encoder more when the confidence is high. As recall goes up, our models show bigger improvements compared to CNN, percentage-wise. This happens because sometimes CNNs struggle to pull out useful info from direct sentences, but our approach fixes that by looking at more details from inference chains, so we still keep high precision.",
        "formal_text": "Convert casual text to formal text: At low recall values, the baselines are pretty small, which means the model is very confident in its predictions. This makes sense because our joint model can adjust how much it relies on direct sentence and"
    },
    {
        "casual_text": "Alright, so we start with a raw text and a dictionary. We use exact string matching to create entity labels, including ones we don't recognize. If there are any conflicts, we handle them by picking the option that matches the most words overall. This method has been used by folks like Etzioni et al. in 2005, Hanisch et al. in 2005, Lin et al. in 2012, and He in 2017.",
        "formal_text": "Convert casual text to formal text: Alright, so we start with a raw text and a dictionary. We use exact string matching to create entity labels, including ones we don't recognize. If there are any conflicts, we"
    },
    {
        "casual_text": "So, after generating possible words, each one gets checked in a dictionary to see if it actually exists. If it does, it might be a correct spelling. But this method has a couple of issues. First, it only works for certain types of spelling checks based on \"edit distance.\" Second, it can take a lot of time if the edit distance is more than 2 and there are a lot of possible characters, like in Unicode for many Asian languages. Hulden (2009) came up with a Finite-State-Automata (FSA) algorithm to speed things up by finding similar words quickly, even if they're misspelled. Other people have used FSA too, but these methods are still just approximations for finding the closest match. More recently, de Amorim and Zampieri (2013) suggested another way to cut down on the number of checks needed by using something called \"anomalous pattern initialization\" and \"partition around medoids.\"",
        "formal_text": "Convert casual text to formal text: So, after generating possible words, each one gets checked in a dictionary to see if it actually exists. If it does, it might be a correct spelling. But this method has"
    },
    {
        "casual_text": "When we train models on just one dataset and then test them on other datasets, they don't perform as well, which isn't surprising. But, our method does way better at handling different datasets compared to the standard approach by Bugert et al. (2020a), even when we only train on one dataset.",
        "formal_text": "Convert casual text to formal text: When we train models on just one dataset and then test them on other datasets, they don't perform as well, which isn't surprising. But, our method does way better at handling"
    },
    {
        "casual_text": "Instead, I focus on stable features, which changes how we handle domain adaptation later on. The idea is to use unlabeled data from the target language to boost prediction accuracy. I call this \"target language adaptation\" (TLA). The cool thing about this approach is that we can use a totally different set of features to adapt to the target language. Once we’ve bridged the language gap, there’s no need to stick with the restrictions we had for stable features. In fact, we can use any language-independent feature for this part. The assumption here is that the method I talked about in Section 3.2 gives us a decent result, but it can be improved, especially since it’s still not as good as working with just one language. The decent, though not perfect, labeling we get for the target language texts can help us improve accuracy even more.",
        "formal_text": "Convert casual text to formal text: Instead, I focus on stable features, which changes how we handle domain adaptation later on. The idea is to use unlabeled data from the target language to boost prediction accuracy. I call this \""
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. First, we start with a question, which we represent as q0. This q0 is basically the average of all the word embeddings of the question (shown as the gray vector). Next, we have these \"Key memories\" (m_in_1 to m_in_k), which are the purple vectors. These are filled with the k most relevant sentences we found earlier, using the average of their word embeddings. Then, there are \"Value memories\" (m_out_1 to m_out_k), which are the green vectors. Each of these contains the average embedding of all the characters mentioned in the corresponding sentence. If there are no characters mentioned, we just use a padding vector. We also have \"Candidate embeddings\" (c1 to cn), which are the orange vectors. These represent the embeddings of every character in the current book. Now, the model goes through multiple \"hops\" (t = 1 to h) over these memories. In each hop, qt (the current question vector) is passed through a linear layer (Rt) and then compared to all the key memories. The attention weights (a1 to ak) are calculated using a method called sparsemax (Martins and Astudillo, 2016). These weights are then used to get an output vector (ot) by taking a weighted average of the value memories. This process is repeated h times, and the final output is passed through another linear layer (C). After that, it's compared to all the candidate vectors using a dot product to get the final prediction. The whole model is trained using something called negative log-likelihood.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way. First, we start with a question, which we represent as q0. This q0 is basically the average of all"
    },
    {
        "casual_text": "We wanted to see how different factors in the model affected things, so we did a bunch of simulations tweaking the chances of diffusion (how features spread) and internal change (how features evolve on their own). We tried four different setups, mixing low and high values for these parameters. You can check out the exact numbers for each setup in Table 1. For each setup, we ran ten separate simulations and then averaged the results. We let the simulations run for 750 timesteps to make sure the starting conditions didn’t mess up the results. To analyze what we got, we used some measures from research papers that look at how similar languages are in terms of family (genealogical) and location (areal). We’ll explain these metrics and what they showed in the next sections. Basically, they help us understand how diffusion and internal change shape the patterns we see in terms of geography and language family.",
        "formal_text": "Convert casual text to formal text: We wanted to see how different factors in the model affected things, so we did a bunch of simulations tweaking the chances of diffusion (how features spread) and internal change (how features evolve on"
    },
    {
        "casual_text": "The word order in spoken Chinese is pretty flexible. In the paraphrase corpus, a lot of paraphrasing is done by just rearranging the words. We looked at paraphrase pairs where the original sentence and the paraphrase have the same number of words, and each word from the original sentence shows up in the paraphrase and vice versa. There's an example in section 3-1.",
        "formal_text": "Convert casual text to formal text: The word order in spoken Chinese is pretty flexible. In the paraphrase corpus, a lot of paraphrasing is done by just rearranging the words. We looked at paraphrase pairs"
    },
    {
        "casual_text": "Alright, let’s break this down in simpler terms. Nonterminals—those are the grammar parts that can be broken down further—we’ll write them using uppercase letters like A, B, and so on. Terminals, which are the basic building blocks of the language, will be written with lowercase letters. Now, for symbols that could be either terminals or nonterminals, we’ll use uppercase letters like X, Y, Z. And to talk about sequences—which could be a mix of terminals and nonterminals, and might even be empty—we’ll use Greek letters. When we see a rule that looks like A  a, we’ll call it an A-production, and we’ll say that a is what A expands into.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in simpler terms. Nonterminals—those are the grammar parts that can be broken down further—we’ll write them using uppercase letters like A"
    },
    {
        "casual_text": "To show how the stuff talked about in the hidden conversation affects how well the model works, we trained and tested both Transformer and Transformer-IF on two versions of DailyDial: one with (1-1-1) and another with (3-1-3). The \"3-1-3\" means the past and future chats each have three turns, while the reply just has one turn. The \"1-1-1\" means everything, including the past, future, and reply, is just one turn long.",
        "formal_text": "Convert casual text to formal text: To show how the stuff talked about in the hidden conversation affects how well the model works, we trained and tested both Transformer and Transformer-IF on two versions of DailyDial: one with (1-1"
    },
    {
        "casual_text": "- METEOR is a way to measure how well your translation matches the reference, kind of like a mix of precision and recall, but it puts more emphasis on recall. (Banerjee and Lavie, 2005) - TER (Translation Edit Rate) is like a score that tells you how many changes you’d need to make to your translation to match the reference sentence exactly. (Snover et al., 2006) - TERP is an upgraded version of TER. It’s smarter because it can do things like swap out phrases, simplify words, and even check for synonyms to make the score more accurate. (Snover et al., 2009) - TERPA is a specific version of TERP that’s been tweaked to work really well for measuring how good a translation is, especially when it comes to making sure the meaning is clear.",
        "formal_text": "Convert casual text to formal text: - METEOR is a way to measure how well your translation matches the reference, kind of like a mix of precision and recall, but it puts more emphasis on recall. (Ba"
    },
    {
        "casual_text": "Once the text-image matching model is trained and the sentences are simplified, we look at each text-image pair (T i, I j ) in our task. If the score s(T i, I j ) is higher than a certain threshold T match, we consider them a match. We picked the threshold as the average matching score for positive text-image pairs in Flickr30K. But, in theory, tweaking this threshold could make the matching better for our specific task.",
        "formal_text": "Convert casual text to formal text: Once the text-image matching model is trained and the sentences are simplified, we look at each text-image pair (T i, I j ) in our task. If the score"
    },
    {
        "casual_text": "We also checked how long it took to solve the four problems where both our system and Onishi et al. (2020)'s system said \"yes\". Turns out, our system was quicker than the older logic-based one. On average, our system took 1.98 seconds, while Onishi et al.'s took 3.11 seconds.",
        "formal_text": "Convert casual text to formal text: We also checked how long it took to solve the four problems where both our system and Onishi et al. (2020)'s system said \"yes\". Turns out, our system"
    },
    {
        "casual_text": "For top-k sampling, we tried out different values—some higher and some lower than 40, which is the default they used in their GPT2 paper for text generation. We decided to go with top-p sampling (with p = 0.9) as the default instead because it doesn’t rely as much on the size of the vocabulary and has been widely used in earlier research on GPT2 (Zellers et al., 2019; Ippolito et al., 2020).",
        "formal_text": "Convert casual text to formal text: For top-k sampling, we tried out different values—some higher and some lower than 40, which is the default they used in their GPT2 paper for text generation. We decided to go with"
    },
    {
        "casual_text": "First, let's just pick a random order for a list of documents called D. After that, for each number i from 1 up to the total number of documents in D, we'll look at each mention of an entity.",
        "formal_text": "Convert casual text to formal text: First, let's just pick a random order for a list of documents called D. After that, for each number i from 1 up to the total number of documents in D, we"
    },
    {
        "casual_text": "Here, we’ll talk about the two main cutting-edge methods for pulling out bilingual word lists from similar text sources. We’ll also share how we’ve tweaked these methods to fix some of their issues. The reason we’re focusing on single-word terms (SWTs) is that they’re the basic building blocks for handling multi-word terms (MWTs) in our approach.",
        "formal_text": "Convert casual text to formal text: Here, we’ll talk about the two main cutting-edge methods for pulling out bilingual word lists from similar text sources. We’ll also share how we’ve tweaked these methods to fix"
    },
    {
        "casual_text": "EDS nodes are basically separate from the words we see on the surface, but each node is linked to parts of the sentence in a way that’s clear and flexible—meaning one node can connect to multiple parts of the sentence, and vice versa. This setup makes EDS fall under Flavor (1) in our list of different types of semantic graphs. Specifically, EDS is fully connected to the sentence but doesn’t follow a strict order. By not forcing a direct match between graph nodes and words, EDS can handle things like breaking down complex words (like comparatives), dealing with smaller bits of meaning (like morphology or syntax), and even capturing hidden meanings (like when something is left out but still implied). In the EDS example shown in Figure 1, every node clearly shows how it’s tied to parts of the input sentence, like how \"similar\" is linked to span 2:9.",
        "formal_text": "Convert casual text to formal text: EDS nodes are basically separate from the words we see on the surface, but each node is linked to parts of the sentence in a way that’s clear and flexible—meaning one no"
    },
    {
        "casual_text": "After that not-so-great run with Eurotra, the European Commission shifted its focus. They started putting more effort into creating useful tools for translators and building up important language resources. We’ll talk more about these projects later. One big one was the TWB (Translator's Work Bench) project, which was part of the ESPRIT program. It kicked off in 1989 and wrapped up in 1994. The project was led by Triumph-Adler and had 10 partners from companies and universities. They looked into what translators really needed and came up with a bunch of features that are now standard in translator workstations. This included stuff like a multilingual editor, document converters, access to dictionaries and terminology databases (like Eurodicautom), access to machine translation systems (like METAL), tools for creating term banks, pre-translation and translation memory, and a special toolkit called System Quirk. This toolkit helped analyze texts and build lexical resource databases (like thesauri and knowledge bases) using corpora and term banks. (If you’re curious, there’s a detailed write-up of the TWB project by Kugler et al. from 1995.)",
        "formal_text": "Convert casual text to formal text: After that not-so-great run with Eurotra, the European Commission shifted its focus. They started putting more effort into creating useful tools for translators and building up important language resources."
    },
    {
        "casual_text": "• The question pattern prediction score, which is the score given by either the retrieval-based method or the generation-based method;",
        "formal_text": "• The question pattern prediction score, which is the score given by either the retrieval-based method or the generation-based method; Convert casual text to formal text: • The question pattern prediction score, which is the score given by either"
    },
    {
        "casual_text": "To save time and avoid asking people the same question over and over, we assume that if we get an answer for a specific part of a sentence and one of its possible translations, we can use that same answer for all other possible translations of that part. So, for each sentence, we create a little database with entries like source part, target part, judgment>. The judgment can be YES, NO, or NOT SURE. This way, we don’t have to keep asking about the same source part with different target options.",
        "formal_text": "Convert casual text to formal text: To save time and avoid asking people the same question over and over, we assume that if we get an answer for a specific part of a sentence and one of its possible translations, we"
    },
    {
        "casual_text": "To understand how our model works, we're doing some visualization analysis from two angles: bi-attention visualization and semantic matching representation visualization. This will help us uncover key details about its inner workings.",
        "formal_text": "Convert casual text to formal text: To understand how our model works, we're doing some visualization analysis from two angles: bi-attention visualization and semantic matching representation visualization. This will help us uncover key details its inner workings."
    },
    {
        "casual_text": "In this paper, we dive deeper into the challenge of summarizing conversations, specifically for dialogue summarization. As online chats on platforms like Messenger, WhatsApp, and WeChat become more common, summarizing chats between a few people is turning into an exciting area of research. To help with this, we made the SAMSum Corpus 1, which has over 16,000 chat dialogues with human-written summaries. The dataset is open for anyone in the research field to use 2.",
        "formal_text": "Convert casual text to formal text: In this paper, we dive deeper into the challenge of summarizing conversations, specifically for dialogue summarization. As online chats on platforms like Messenger, WhatsApp, and WeChat become more common"
    },
    {
        "casual_text": "A hypothesis gets penalized by the distance-based model not just for straying away from the monotonic translation path, but also for going back to it. Let’s look at this example coverage vector to see what I mean:",
        "formal_text": "Convert casual text to formal text: A hypothesis gets penalized by the distance-based model not just for straying away from the monotonic translation path, but also for going back to it. Let’s look at this example coverage"
    },
    {
        "casual_text": "There are some pretty big practical and theoretical issues when canonical forms end up in the grammar like we talked about in Section 4.1. First, there's the problem of how we learn words when the categories they belong to get really weird. Second, with those complicated relationships between syntax and meaning, as we mentioned in Section 4.2, it's tough to imagine how these systems could be used for anything other than the exact purposes they were designed for. For example, it's hard to see how systems that work across multiple languages could connect their grammars when each one is so heavily shaped by the random quirks of how that language processes things. It's also tricky to figure out how a generation system could work with these grammars, since the rules for mapping things would likely be super complicated and only work in one direction.",
        "formal_text": "Convert casual text to formal text: There are some pretty big practical and theoretical issues when canonical forms end up in the grammar like we talked about in Section 4.1. First, there's the problem of how we learn words when"
    },
    {
        "casual_text": "To figure out how well each system performed, we averaged the scores from different raters and categories. The results show that the systems can be ranked from best to worst: S3, S2, S1 (check out Table 4 and Figure 4). From these numbers and charts, it's clear that S1 is the weakest, which makes sense since it's a lexical transfer system. S2 and S3 are pretty close, which reflects the fact that both are mainly syntactic transfer systems. Looking back at the 1994 DARPA MT evaluation scores for S2 and S3 (White & O'Connell, 1994) for the language pairs, we see that the scores here (Table 5) match up pretty well. S3 did slightly better than S2 in terms of adequacy back then (the 1994 scores were on a 0 to 1 scale). S1 didn't take part in the 1994 DARPA evaluation. So far, the answer to the first question, \"Can this method be used to rank the systems effectively?\" is a big yes. The method does a good job ranking the three systems, and this ranking matches up with an independent ranking of two of them. In the next section, we'll take a closer look at the ratings for text type T3, since at this level, S2 and S3 switch places in their ranking.",
        "formal_text": "Convert casual text to formal text: To figure out how well each system performed, we averaged the scores from different raters and categories. The results show that the systems can be ranked from best to worst: S3, S2,"
    },
    {
        "casual_text": "Basically, let's say we're trying to build a system that can tell the difference between different types of things based on some data. We have a bunch of examples where each one is a pair (x, y), and y tells us what category or class that example belongs to. The classes are labeled V. The typical way to train this system is by trying to make it as accurate as possible, so we minimize something called NLL for each example in our training data.",
        "formal_text": "Convert casual text to formal text: Basically, let's say we're trying to build a system that can tell the difference between different types of things based on some data. We have a bunch of examples where each"
    },
    {
        "casual_text": "The next word distribution might have multiple peaks. But using a bunch of softmaxes makes things way more complicated because we have to calculate the dot product between each facet and every word in our vocabulary. Based on our analysis, we thought of a way to simplify this: we can divide all the words in the vocabulary into different groups and use different facets for each group. For instance, if we put words like \"queen,\" \"man,\" \"woman,\" and \"king\" into one group and the rest of the words into another, we won't end up with weird combinations like \"queenking\" being the same as \"womanman\" in either group. With this approach, each word is only in one group, so we only need to calculate one dot product per word. The only extra work we have to do is setting up the extra linear projections for the facets.",
        "formal_text": "Convert casual text to formal text: The next word distribution might have multiple peaks. But using a bunch of softmaxes makes things way more complicated because we have to calculate the dot product between each facet and every word"
    },
    {
        "casual_text": "The University Bookstore stays open until 4:30 PM. I'm not sure if they sell floppy disks, though. But they do have a bunch of other stuff that's useful for school, so it might be worth checking out.",
        "formal_text": "Convert casual text to formal text: The University Bookstore stays open until 4:30 PM. I'm not sure if they sell floppy disks, though. But they do have a bunch of other stuff that's useful"
    },
    {
        "casual_text": "To see how POS Embedding improves how tokens are represented, we looked at some stats from the SQuAD 1.1 development set. Specifically, we checked: 1) the POS types of the boundary words in the predicted spans, which you can see in Table 5; 2) how well POI-Net and its baseline, ALBERT base, did on POS classification, and that's shown in Figure 3.",
        "formal_text": "Convert casual text to formal text: To see how POS Embedding improves how tokens are represented, we looked at some stats from the SQuAD 1.1 development set. Specifically, we checked: 1) the"
    },
    {
        "casual_text": "We tested both BERT and GPT-2 embeddings, and both models behaved pretty similarly, which we talk about in the paper. To keep things clear, the paper only shows the BERT results in the figures, but we've added the GPT-2 versions of those same figures for comparison. So, Figure 4 is the GPT-2 version of Figure 1, Figure 5 is the GPT-2 version of Figure 2, and Figure 6 is the GPT-2 version of Figure 3. Figure 3 from the main paper is about GPT-2 embeddings. As you can see from the dashed line being close to random, in cases where the words don't give away the grammar, you can't really pull out grammatical info from sentences that have been shuffled around locally.",
        "formal_text": "Convert casual text to formal text: We tested both BERT and GPT-2 embeddings, and both models behaved pretty similarly, which we talk about in the paper. To keep things clear, the paper only shows the B"
    },
    {
        "casual_text": "To give a higher preference value to lemmas that show up more often and closer together, we use a formula to calculate the total score. This score is based on how many times a noun phrase with the same lemma as our main noun phrase appears in the ten sentences before it.",
        "formal_text": "Convert casual text to formal text: To give a higher preference value to lemmas that show up more often and closer together, we use a formula to calculate the total score. This score is based on how many times"
    },
    {
        "casual_text": "When it comes to measuring agreement in studies, there are a few different methods out there. Sure, there's the basic one that just checks if annotators pick the same value for something. But then there are more advanced stats that tweak this idea by considering other stuff, like Cohen's  (from Cohen et al., 1960), the G-index score (Holley and Guilford, 1964), or Krippendorff's  (Krippendorff, 2004). Most of these, though, can be thrown off by things like how labels are spread out, missing data, and other tricky situations. The method we used in this paper doesn't get as easily messed up by these things but still gives us a solid idea of how much agreement there is.",
        "formal_text": "Convert casual text to formal text: When it comes to measuring agreement in studies, there are a few different methods out there. Sure, there's the basic one that just checks if annotators pick the same value for something"
    },
    {
        "casual_text": "In a tree, every node (except the root) has only one incoming edge. But in semantic graphs, nodes can have two or more incoming edges, which means they can share arguments. This is called reentrancy in the graph. Unlike trees, general digraphs can have cycles, where a path leads from a node back to itself. Trees also have the property that they’re all connected, meaning you can always find a path between any two nodes, even if it’s not a direct one. Semantic graphs, on the other hand, don’t always have to be connected like that.",
        "formal_text": "Convert casual text to formal text: In a tree, every node (except the root) has only one incoming edge. But in semantic graphs, nodes can have two or more incoming edges, which means they can"
    },
    {
        "casual_text": "You can solve this stuff pretty easily with the Viterbi algorithm, which was introduced by Forney back in 1973.",
        "formal_text": "Convert casual text to formal text: You solve this stuff pretty easily with the Viterbi algorithm, which was introduced by Forney back in 1973. Convert casual text to formal text: You solve this stuff pretty easily with the Vit"
    },
    {
        "casual_text": "Step 4: Check out the telops in TV images. Figure 5 has a bunch of different info that telops show during TV news (thanks to Watanabe in '96 for pointing that out). Watanabe also came up with a way to analyze the meaning behind these telops, and it worked pretty well—92% accurate. We’re using that same method to figure out what each telop is actually saying.",
        "formal_text": "Convert casual text to formal text: Step 4: Check out the telops in TV images. Figure 5 has a bunch of different info that telops show during TV news (thanks to Watanabe in"
    },
    {
        "casual_text": "Right now, we're using a basic beam search to find a sequence of parts that covers the entire input. These sequences can have gaps, meaning they don't have to be connected, but they can't overlap. The algorithm likes shorter sequences. To figure out the length of a sequence, we add up the lengths of the parts in it, but we give some parts more weight than others. We tried two ways to assign weights and lengths to these parts. In the first method, each part gets a length of 1, but we multiply that by a factor based on how \"good\" the part is. Sequences can be extended by adding gaps that cover one input token at a time, and these gaps get a weight of 3. Parts created by rules that have OUTPUT> on the left side are considered the best and get a weight of 1. Other parts can also be added to the sequence and get a weight of 1.5. For example, if we have a sequence with an OUTPUT> part covering tokens 1 to 3, a gap covering tokens 4 and 5, and a Vl> part covering tokens 6 to 10, the total length would be 1 (for the OUTPUT> part) + 6 (for the gap) + 1.5 (for the Vl> part) = 8.5. This algorithm really prefers sequences that are connected and calculates lengths based on the number of parts in the sequence, not their actual length.",
        "formal_text": "Convert casual text to formal text: Right now, we're using a basic beam search to find a sequence of parts that covers the entire input. These sequences can have gaps, meaning they don't have to be connected"
    },
    {
        "casual_text": "We've been looking at four languages so far: three that branch to the right (English, German, Chinese) and one that branches to the left (Japanese). To make sure our results aren’t skewed or random, we added another translation task: English to Turkish (En-Tr). Turkish is also a left-branching language, so it’s a good comparison. We calculated the BLEU scores for the left and right halves of the translations, both when translating left-to-right and right-to-left, just like we did in Sections 3.1 and 3.2. You can find the results in Table 8. For the left-to-right decoding, the left half of the translation is more accurate than the right half when doing normal translation. But when we use teacher forcing, the right half becomes more accurate. This shows that English-Turkish translation behaves kind of like English-Japanese translation, where the right half is more accurate. However, unlike what we saw with Japanese, Turkish shows a different pattern: the effect of language branching isn’t as strong as the impact of error propagation.",
        "formal_text": "Convert casual text to formal text: We've been looking at four languages so far: three that branch to the right (English, German, Chinese) and one that branches to the left (Japanese). To make sure our"
    },
    {
        "casual_text": "Alright, let's dive into defining the first version of the transition system for our parser. Basically, this parser constructs a dependency tree from the top down and uses parser configurations to keep track of parsing decisions while making sure everything stays well-typed.",
        "formal_text": "Convert casual text to formal text: Alright, let's dive into defining the first version of the transition system for our parser. Basically, this parser constructs a dependency tree from the top down and"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. When we have a sentence, we look for the shortest path between two key words (called nominals) in the sentence. This path can go in two directions—one from the subject to the object and the other way around. We then use our model to make predictions for both directions. If both predictions come back as \"other\" (meaning no specific relation), we just go with \"other.\" But if one of them is a specific relation, we pick the one with the highest confidence score. Ideally, our model should give the correct label for the right direction and \"other\" for the wrong one. Finally, we check how well our model did by using a macro-averaged F1 score with the official evaluation tool.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in simpler terms. When we have a sentence, we look for the shortest path between two key words (called nominals) in the sentence. This"
    },
    {
        "casual_text": "In simpler terms, when we're dealing with things like how computers or AI systems understand language, they rely on special databases that store information about language and the world. These databases can be structured in different ways—like word lists, memory models, or networks—depending on what the system is trying to do, whether it's explaining something, making inferences, or simulating how understanding works. But here's the thing: the information these systems use is kind of limited in two ways. First, a lot of it comes from people's own thoughts or guesses (introspection), so it's not really backed up by any concrete, testable methods. And even when it seems like there are methods involved, those methods are usually just showing the results of simple, straightforward connections in very clear situations, not actually proving anything. Most of the time, the data these systems use comes from the people who create them—like the researchers or system designers—or maybe from some experts they ask. This data is then organized into structures that represent ideas or concepts (like lists, networks, etc.) and is usually focused on things that can be logically pieced together. But the way this data is put together often feels a bit random and is mostly limited to representing clear, logical statements.",
        "formal_text": "Convert casual text to formal text: In simpler terms, when we're dealing with things like how computers or AI systems understand language, they rely on special databases that store information about language and the world. These databases can be structured in"
    },
    {
        "casual_text": "For drafting stuff: People write something in their own language and then use a machine to translate it, so they can get a document in a different language than the one they usually speak.",
        "formal_text": "Convert casual text to formal text: For drafting stuff: People write something in their own language and then use a machine to translate it, so they can get a document in a different language than the one they usually speak."
    },
    {
        "casual_text": "This part talks about the basic extractive summarization models and the specific settings for their parameters.",
        "formal_text": "Convert casual text to formal text: This part talks the basic extractive summarization models and the specific settings their parameters. Convert casual text to formal text: This part talks the specific settings their parameters. Convert casual text to formal"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. First, we start by setting 'a' to 0.5. Then, we tweak the topic number K, changing it from 2 to 20, but we do it in steps of 2. If you check out Figure 7 (1) and Figure 7 (2), you'll see that the ROUGE score hits its highest point when K is around 12. Next, we lock K at 12 and adjust the value of 'a' from 0 to 1, stepping it up by 0.1 each time. When 'a' is set to 0, the model basically becomes a one-layer graph ranking thing, ignoring any topic clustering info. Looking at Figure 7 (3) and Figure 7 (4), the ROUGE scores peak around 0.6 and then start to drop after that. So, for our test dataset, we go with K set to 12 and 'a' set to 0.6.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. First, we start by setting 'a' to 0.5. Then, we tweak the topic number K, changing it"
    },
    {
        "casual_text": "We built a website kinda like Wikipedia, called the GMB Explorer, where people can check out the Groningen Meaning Bank. It does three main things: lets you browse and search through the documents, shows you the different layers of annotations, and lets you fix any mistakes in those annotations. We'll go over these features in a bit.",
        "formal_text": "Convert casual text to formal text: We built a website kinda like Wikipedia, called the GMB Explorer, where people can check out the Groningen Meaning Bank. It does three main things: lets you browse and search through"
    },
    {
        "casual_text": "We grab football game transcripts by downloading YouTube videos from regular folks who post stuff on YouTube. We search YouTube for channels that focus on old football games. YouTube often adds captions to these videos, so we can easily get the text from 601 NFL games and 854 NCAA games. Next, we figure out which teams are playing and the year of the game by checking the video titles. If the title isn’t clear, we label it ourselves. After getting the videos, we break down the transcripts into smaller pieces using spaCy. Since spaCy isn’t great at tagging the text in our transcripts, we switch to ARK TweetNLP POS tagger (Owoputi et al., 2013). This tool works better with messy, fragmented text, like TV subtitles (Jrgensen et al., 2016). We also use phrasemachine (Handler et al., 2016) to pick out all the noun phrases in the text. Finally, we spot player mentions in the transcripts by matching first, last, and full names to player rosters we found online. These rosters also tell us the player’s position. At first, we were worried about how well player names were transcribed, but we found very few mistakes, especially with common names. Even less common names were usually spelled and capitalized correctly. We’ll save a more detailed check for later.",
        "formal_text": "Convert casual text to formal text: We grab football game transcripts by downloading YouTube videos from regular folks who post stuff on YouTube. We search YouTube for channels that focus on old football games. YouTube often adds captions to these videos,"
    },
    {
        "casual_text": "In short, our calculation shows that  S, T is the exact highest probability for max b: |b|=S, | (b)|=T P (b|x) when we're looking at a sequence of length T with S generation slots. Our algorithm, without merging repeating tokens, finds the B S, T that gives the highest P (b|x) under the same conditions, which wraps up the proof for Part (1).",
        "formal_text": "Convert casual text to formal text: In short, our calculation shows that  S, T is the exact highest probability for max b: |b|=S, | (b)|=T P (b|x"
    },
    {
        "casual_text": "When talking about precision, we usually look at things like the mean, standard deviation with 95% confidence intervals, the coefficient of variation (CV), and the percentage of values that fall within a certain number of standard deviations. We prefer using the CV because it’s a universal measure—it’s not tied to the units of the measurements, unlike the mean and standard deviation. This makes it a good way to compare precision (how consistent results are) across different studies (Ahmed, 1995, p. 57). The percentage within n standard deviations is also useful, but it’s not as commonly used and might be harder to understand for some people. In studies trying to reproduce results in NLP/ML, the sample sizes are often really small. For example, in Table 6, there’s only 8 samples—one original study and 7 reproductions. Because of this, we need to use special methods to get more accurate estimates. We use the unbiased sample standard deviation, which we call s*, and calculate confidence intervals using a t-distribution. We also estimate the standard error of the unbiased sample standard deviation based on the standard error of the unbiased sample variance (se(s2), from se, 1973). If we assume the values we’re measuring are normally distributed, we calculate the standard error of the sample variance the usual way: se(s2) = 24/(n1). Lastly, since our sample size is small, we apply a small sample correction to the CV. We call this corrected version CV*: CV* = (1 + 1/(4n)) * CV. This adjustment helps make the CV more accurate for smaller samples (Sokal and Rohlf, 1971).",
        "formal_text": "Convert casual text to formal text: When talking about precision, we usually look at things like the mean, standard deviation with 95% confidence intervals, the coefficient of variation (CV), and the percentage of values that fall within a certain"
    },
    {
        "casual_text": "In our tests, we stop the iteration if the difference between f_ki and f_k+1i is 0.0001 or less (for all i from 1 to N) and the same goes for g_ki and g_k+1i (for all i from 1 to T). There are two TAC datasets used for update summarization tasks. The summarization for docset-A can be considered as the query-focused summarization task mentioned in this paper.",
        "formal_text": "Convert casual text to formal text: In our tests, we stop the iteration if the difference between f_ki and f_k+1i is 0.0001 or less (for all i"
    },
    {
        "casual_text": "Neural extractive summarization is a method that creates short summaries by picking out key sentences from a document. It's been pretty effective in real-world situations, like summarizing news articles (Cheng and Lapata, 2016; Nallapati et al., 2017) and scientific papers (Cohan et al., 2018; Xiao and Carenini, 2019). Usually, these summarizers figure out which sentences are the most important by looking at how relevant, informative, and non-repetitive they are. But when it comes to specific fields, like news, the summarizer can pick up on certain patterns in the data. One well-known example is the \"lead bias\" in news articles (Nenkova et al., 2011; Hong and Nenkova, 2014). This bias suggests that the first few sentences of a news story often contain the most important information. So, it's no surprise that neural extractive summarizers for news tend to focus more on where a sentence is placed in the article rather than its actual content (Jung et al., 2019; Grenander et al., 2019; Zhong et al., 2019a, b).",
        "formal_text": "Convert casual text to formal text: Neural extractive summarization is a method that creates short summaries by picking out key sentences from a document. It's been pretty effective in real-world situations, like"
    },
    {
        "casual_text": "Graph neural networks (GNNs), like graph convolutional networks (GCNs) (Kipf and Welling, 2017) and graph attention networks (GATs) (Velickovic et al., 2018), have been doing really well in a bunch of recent studies. They’ve shown great results in areas like visual representation learning (Wu et al., 2019; Xie et al., 2021), text representation learning (Yao et al., 2019; Lou et al., 2021; Liang et al., 2021b, 2022), and recommendation systems (Ying et al., 2018; Tan et al., 2020). Plus, some researchers have been looking into using graph models for multi-modal tasks, such as multi-modal sentiment detection (Yang et al., 2021), multi-modal named entity recognition, cross-modal video moment retrieval (Zeng et al., 2021), multi-modal neural machine translation (Yin et al., 2020), and multimodal sarcasm detection (Liang et al., 2021a).",
        "formal_text": "Convert casual text to formal text: Graph neural networks (GNNs), like graph convolutional networks (GCNs) (Kipf and Welling, 2017) and graph attention networks (GATs) ("
    },
    {
        "casual_text": "When we plug this IF into the mapper, it spits out the FS you see in Figure 7. Basically, it's like saying \"for whom=us\" and \"room-spec=room\" with a quantity of 4.",
        "formal_text": "Convert casual text to formal text: When we plug this IF into the mapper, it spits out the FS you see in Figure 7. Basically, it's like saying \"for whom=us\" and \""
    },
    {
        "casual_text": "With RoBERTa and for TroFi and VUA Verbs, we notice some crazy jumps in performance in those final layers.",
        "formal_text": "Convert casual text to formal text: With RoBERTa and for TroFi and VUA Verbs, we notice some crazy jumps performance in those final layers. Convert casual text to formal text: With RoBERTa and"
    },
    {
        "casual_text": "The metrics we're about to talk about are pretty efficient, running in linear time based on the length of the permutation. Each of these metrics focuses on a specific part of PETs/PEFs: how much they can be broken down (|PET|), how flexible they are with brackets (#PETs), and the highest number of inputs they can handle (MAX |Op|). You can check out these example metrics in Figure 7.",
        "formal_text": "Convert casual text to formal text: The metrics we're about to talk about are pretty efficient, running in linear time based on the length of the permutation. Each of these metrics focuses on a specific part of PET"
    },
    {
        "casual_text": "Alright, so when you're dealing with this kind of task, a good starting point is to just throw those police fatality documents into some ready-made event extraction tools that are already out there. These tools can help spot killing events in the text. In the world of NLP, a common way to handle event extraction is by creating a detailed list of event types, labeling a small set of documents, and then using that to train a model. Here's a quick look at how two tools—SEMAFOR and RPI-JIE—performed, along with some rules we've got (R1-R3). We've got their precision, recall, and F1 scores for the test data in Table 6.",
        "formal_text": "Convert casual text to formal text: Alright, so when you're dealing with this kind of task, a good starting point is to just throw those police fatality documents into some ready-made event extraction tools that are already out"
    },
    {
        "casual_text": "The same process can be applied to the other trees, which results in a (deterministic linear complete) embedded tree transducer. Here's the set of equations that define it:",
        "formal_text": "Convert casual text to formal text: The same process can be applied to the other trees, which results in a (deterministic linear complete) embedded tree transducer. Here's the set equations that define it: Here'"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. The vector representation of X is written as V(X), and it's equal to the sum of V(xi), where each V(xi) is a vector itself. Specifically, V(xi) is made up of Mu(xi, o1) all the way to Mu(xi, om).",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. The vector representation of X is written as V(X), and it's equal to the sum of V(xi"
    },
    {
        "casual_text": "You can check out the code and stuff for the custom intent engines from June 2017 here: [nlubenchmark/2017-06-custom-intent-engines](https://github.com/snipsco/nlubenchmark/tree/master/2017-06-custom-intent-engines).",
        "formal_text": "Convert casual text to formal text: You can check out the code and stuff for the custom intent engines from June 2017 here: [nlubenchmark/2017-06-custom-intent-engines](https://git"
    },
    {
        "casual_text": "For our experiments with the TaLAPi corpus, we looked at how well the Isolated, Cascade, and Joint models performed. We could have done the same thing with the ORCHID corpus, but since we're short on time and space, we're only showing the results from the TaLAPi experiments in Table 6.",
        "formal_text": "Convert casual text to formal text: For our experiments with the TaLAPi corpus, we looked at how well the Isolated, Cascade, and Joint models performed. We could have done the same thing with the OR"
    },
    {
        "casual_text": "So, to effectively debug models with human input, it's crucial to understand these dimensions (we call them features). We use an explanation method to get this understanding. There are various ways to explain predictions from text classifiers—like natural language explanations, rules (Ribeiro et al., 2018), extracted rationales (Lei et al., 2016), and attribution scores (Lertvittayakumjorn and Toni, 2019). Some methods, like LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee, 2017), work with any model and don't need access to the model's inner workings. On the other hand, methods like DeepLIFT (Shrikumar et al., 2017) and LRP (layer-wise relevance propagation) (Bach et al., 2015; Arras et al., 2016) dive into the model's architecture and parameters to generate explanations. In our work, we use LRP to explain the learned features, not just the predictions, to help humans understand the model's behavior and make better debugging decisions.",
        "formal_text": "Convert casual text to formal text: So, to effectively debug models with human input, it's crucial to understand these dimensions (we call them features). We use an explanation method to get this understanding. There are various ways to explain"
    },
    {
        "casual_text": "We create meaning representations for hypothesis sentences using the CCG derivation tree and Neo-Davidsonian Event Semantics (as described by Parsons in 1990). To get these meaning representations (which are basically FOL formulas) from hypothesis sentences, we use a tool called ccg2lambda. This tool was developed by Mineshima and others in 2015 and later improved by Martnez-Gómez and team in 2016. It works with CCG and -calculus to do the job.",
        "formal_text": "Convert casual text to formal text: We create meaning representations for hypothesis sentences using the CCG derivation tree and Neo-Davidsonian Event Semantics (as described by Parsons in 1990). To get these meaning representation"
    },
    {
        "casual_text": "In Experiment 1 with the Amazon Products dataset, which is a multiclass classification thing, we tweaked the user interface a bit, as shown in Figure 9. We didn’t include the options for “mostly” and “partly” relevant because that would’ve added up to nine choices per question, which felt way too overwhelming for the participants. Instead, with the setup in Figure 9, we assigned a score to feature f_i based on what the participant picked. Here’s how it worked: we took the values in the i-th column of W, used min-max normalization to scale them between 0 and 1, and then gave the normalized value of the selected class as the score for feature f_i. If someone chose “None,” that feature got a score of zero. Figure 10 shows the distribution of the average feature scores for this task, using one CNN.",
        "formal_text": "Convert casual text to formal text: In Experiment 1 with the Amazon Products dataset, which is a multiclass classification thing, we tweaked the user interface a bit, as shown in Figure 9. We didn’t include"
    },
    {
        "casual_text": "To make lexical normalization work as a token prediction thing, we brought up the idea of a new special token called [SPACE] in section 4.2.1. We want our normalization model to be able to predict this token. So, we added a new label to our WordPiece vocabulary and also a new vector to the last softmax layer. We did this by just tacking on a vector to the output matrix, which we got by sampling from a normal distribution. Moving on to the next part, the [MASK] predictor. As we mentioned, alignment sometimes needs [MASK] tokens added to the source sequence based on the gold sequence. To deal with the differences between training and testing, we added an extra token classification module to the BERT architecture. This module looks at the last hidden state of each WordPiece token from BERT and predicts how many [MASK] tokens are needed. At test time, we first figure out how many [MASK] tokens we need to add to the noisy sequence. Then, we predict the normalized tokens using the full sequence.",
        "formal_text": "Convert casual text to formal text: To make lexical normalization work as a token prediction thing, we brought up the idea of a new special token called [SPACE] in section 4.2.1. We want our normalization"
    },
    {
        "casual_text": "To look at this, we’ll check out how many content words are in the sentences. Content words can help us see things like when people shorten their sentences by leaving out stuff like \"that\"—which is a sign of more packed information (Levy and Jaeger, 2006). Haber and his team (2019) noticed that in the later stages of the PhotoBook games, all types of sentences had more content words. We see the same thing in our chains of referring sentences, and the ReRef and Copy models show a similar pattern: the follow-up sentences have way more nouns and adjectives than the first descriptions. Figure 3b highlights this, especially for nouns, which are the most common type of content word in our data.",
        "formal_text": "Convert casual text to formal text: To look at this, we’ll check out how many content words are in the sentences. Content words can help us see things like when people shorten their sentences by leaving out stuff like \"that\""
    },
    {
        "casual_text": "In simpler terms, the meaning of sentence (4) comes from combining the features of the words listed in the table. The words that depend on the main word 'wet|' have now become part of its meaning.",
        "formal_text": "Convert casual text to formal text: In simpler terms, the meaning of sentence (4) comes from combining the features of the words listed in the table. The words that depend on the main word 'wet|' have now"
    },
    {
        "casual_text": "NESPOLE! focuses on the Travel & Tourism area, which is all about getting things done. The machine translation project uses something called an interlingua, which focuses on what the speaker wants to do rather than just the exact words they use. The IF (Intermediate Form) is a way to represent the meaning of a piece of speech based on tasks. Since the system deals with spoken conversations, these pieces are called Spoken Dialogue Units (SDUs). They can be as short as a single word like \"hello\" or as long as a whole sentence like \"I'd like to reserve a room.\" The IF is built around specific actions related to the domain, called Domain Actions (DAs), which come with parameters. Each DA has a tag showing who’s speaking, and it might also include a speech act, followed by a bunch of concepts and sometimes extra arguments. Basically, DAs can be thought of like this (according to Levin et al., 2003):",
        "formal_text": "Convert casual text to formal text: NESPOLE! focuses on the Travel & Tourism area, which is all about getting things done. The machine translation project uses something called an interlingua, which focuses on what"
    },
    {
        "casual_text": "We coded our algorithm in Python 3.7.3 and used the PyTorch 1.2.0 library. For the baseline methods, we used the code shared by the original authors. We set the hyperparameters for these baselines based on the values the authors suggested and tweaked them a bit using 10-fold cross-validation on the development set. In our method, we set the output dimension of the SetConv layer to 128, the support set size to 64 (which is N1 + N2), the post-training subset size to 1000, the learning rate to 0.01, and the Adam optimizer parameters to 1 = 0.9 and 2 = 0.999. The input dimension for the SetConv layer is the same as the dimension of the input data for each dataset. You can check out the sensitivity analysis for the post-training subset size in Section 4.6.",
        "formal_text": "Convert casual text to formal text: We coded our algorithm in Python 3.7.3 and used the PyTorch 1.2.0 library. For the baseline methods, we used the code shared by the original authors. We set the hyperparameter"
    },
    {
        "casual_text": "Translators typically rely on commercial tools for their work, including both translation and post-editing tasks. According to the 2018 Language Industry Survey 1, which was put together by EUATC, Elia, FIT Europe, GALA, and LINDWeb, SDL Trados 2 was the most popular tool, used by over half of the market. It was followed by MemoQ, Memsource, Wordfast, and Across. But here’s the thing: these post-editing tools have their limitations. Since they’re proprietary, they’re not super flexible or easy to tweak. Plus, they don’t usually offer much data on the translator’s activity, which could be useful for studying how much effort goes into post-editing. On the flip side, there are open-source CAT tools like OmegaT that have been modified for data collection purposes. Moran and others explored this in a 2014 study.",
        "formal_text": "Convert casual text to formal text: Translators typically rely on commercial tools for their work, including both translation and post-editing tasks. According to the 2018 Language Industry Survey 1, which was put together by EUATC, Eli"
    },
    {
        "casual_text": "We need to create some solid computational tools that can take these source sentences, even if they're super unclear, and turn them into clear descriptions.",
        "formal_text": "Convert casual text to formal text: We need to create some solid computational tools that can take these source sentences, even if they're super unclear and turn clear descriptions. Convert casual text to formal text: We need to create some"
    },
    {
        "casual_text": "Getting better results with more info. We looked at how the number of extra facts, M, affects things and put the findings in Figure 2. Basically, the more facts we add, the better it gets. But it levels off at M = 20 and starts to drop if we add even more, probably because we’re getting too much random or irrelevant stuff.",
        "formal_text": "Convert casual text to formal text: Getting better results with more info. We looked at how the number of extra facts, M, affects things and put the findings in Figure 2. Basically, the more facts we add, the better"
    },
    {
        "casual_text": "For each ambiguous name and feature model, we figured out the best stop-threshold for agglomerative clustering and listed the results in Table 1, Table 2, and Table 3. But for the most reliable feature model, which is Bagga + summarized bnp + document entities, we picked a fixed stop-threshold for agglomerative clustering based on the development data—0.089 for English and 0.078 for Chinese. You can check out the results in Table 4.",
        "formal_text": "Convert casual text to formal text: For each ambiguous name and feature model, we figured out the best stop-threshold for agglomerative clustering and listed the results in Table 1, Table 2, and Table"
    },
    {
        "casual_text": "We don’t use language models like ELMo and BERT because our main goal is to improve the pooling technique, not to compete with the latest and greatest methods out there.",
        "formal_text": "Convert casual text to formal text: We don’t use language models like ELMo and BERT because our main goal, not to compete the latest and greatest methods out."
    },
    {
        "casual_text": "Anaphoricity determination, which is super useful for figuring out coreference resolution, has been looked at a lot in research. It can be broken down into three main types: rule-based methods (like Paice and Husk in 1987, Lappin and Leass in 1994, Kennedy and Boguraev in 1996, Denber in 1998, and Vieira and Poesio in 2000), statistical approaches (like Bean and Riloff in 1999, Cherry and Bergsma in 2005, and Bergsma et al in 2008), and learning-based techniques (like Evans in 2001, Ng and Cardie in 2002, Ng in 2004, Yang et al in 2005, and Denis and Balbridge in 2007).",
        "formal_text": "Convert casual text to formal text: Anaphoricity determination, which is super useful for figuring out coreference resolution, has been looked at a lot in research. It can be broken down into three main types: rule-based"
    },
    {
        "casual_text": "The Attention Layer uses the attention mechanism in both ways—from the prompt to the response and from the response back to the prompt. This gives both sides extra info that helps each other out.",
        "formal_text": "Convert casual text to formal text: The Attention Layer uses the attention mechanism in both ways—from the prompt to the response and from the response back to the prompt. This gives both sides extra info that helps each out."
    },
    {
        "casual_text": "We used WordNet (Miller, 1992) to look at how words in the two arguments are related and how similar they are. To do this, we looked at things like antonyms, synonyms, and hypernyms from WordNet. Every noun and verb we were working with got assigned to its specific meaning (synset) using the Lesk algorithm, which Banerjee and Pedersen (2002) adapted for WordNet. We turned relationships like antonyms into categories, each with its own synset. To measure how similar the arguments are, we calculated the shortest-path scores between all their synsets and normalized the results to create a feature.",
        "formal_text": "Convert casual text to formal text: We used WordNet (Miller, 1992) to look at how words in the two arguments are related and how similar they are. To do this, we looked at things like antonyms,"
    },
    {
        "casual_text": "Using counterfactual reasoning in NLP usually involves defining a relationship between x and x, and then generating x based on that relationship. This has led to different counterfactual generators being designed for various tasks, each focusing on specific subsets of x that are relevant to the job. For instance, human annotators create counterfactuals like \"It is great for kids\" to help with model training and evaluation.",
        "formal_text": "Convert casual text to formal text: Using counterfactual reasoning in NLP usually involves defining a relationship between x and x, and then generating x based on that relationship. This has led to different counter"
    },
    {
        "casual_text": "Language models (LMs) try to figure out how likely a sequence of symbols x t  T t=0 is, by looking at the joint probability.",
        "formal_text": "Convert casual text to formal text: Language models (LMs) try to figure out how likely a sequence of symbols x t  T t=0 is, by looking at the joint probability. Convert casual"
    },
    {
        "casual_text": "Once we’ve done a DEL or INS, we go through the same steps again but with the updated mt. There are three possible outcomes when decoding:",
        "formal_text": "Convert casual text to formal text: Once we’ve done a DEL or INS we go through the same steps again but with the updated mt. There are three possible outcomes when decoding: Convert casual text to"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: You have something like this: = x 0 | x 1 | x 2 | and so on. Then there's an equation that looks like this: = q( f (n) (x 1, ..., x n )) =  (n) Here,  (n) is a real number, and it can be written as: = g (m) ( (n) 1, ...,  (n) m ) | q j (x i ) where i is between 1 and n. Basically, it's a bunch of math stuff with functions and numbers, but I tried to make it a bit easier to understand.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: You have something like this: = x 0 | x 1 | x 2 | and so on. Then there"
    },
    {
        "casual_text": "• **Highlight tough words:** You can ask our system to automatically mark tricky words for you. This is super handy if you’ve tweaked the original text but aren’t sure if your changes actually made it easier to understand. After the system points out some tricky words or phrases, you can still decide if you want to simplify them even more. • **See instructions and original text:** The instructions (check out Figure 2) pop up at the start of each task. Once you accept the task, the instructions disappear automatically so you can focus on simplifying the text without distractions. While working, if you need to check the instructions again, you can bring them up below the text editor. Plus, you can always compare your simplified version with the original text to make sure you’re on the right track.",
        "formal_text": "Convert casual text to formal text: • **Highlight tough words:** You can ask our system to automatically mark tricky words for you. This is super handy if you’ve tweaked the original text but aren’t"
    },
    {
        "casual_text": "You can simplify complex distributions and make them less dependent and multimodal by either using autoregressive factorization or adding more variance info as input. This helps with the over-smoothing issue and makes the generated voice sound better. FastSpeech 2, for example, is great because it’s non-autoregressive, so it generates stuff quickly by providing more variance details.",
        "formal_text": "Convert casual text to formal text: You can simplify complex distributions and make them less dependent and multimodal by either using autoregressive factorization or adding more variance info as input. This helps with the over-smoothing"
    },
    {
        "casual_text": "We're treating the relation classification task as a multi-class classification problem, so our training goal is to minimize the cross-entropy error. To prevent overfitting, we use dropout (Srivastava et al., 2014) on the feature vector we get from the convolution layer. We also add an l2 regularizer to the fully connected layer to keep things in check.",
        "formal_text": "Convert casual text to formal text: We're treating the relation classification task as a multi-class classification problem, so our training goal is to minimize the cross-entropy error. To prevent overfitting, we use drop"
    },
    {
        "casual_text": "Alright, let's talk about log-likelihood training. Imagine we have a training example with features x and label y, and the class z can either be 0 or 1. Now, we can think of a generative classifier like this.",
        "formal_text": "Convert casual text to formal text: Alright, let's talk about log-likelihood training. Imagine we have a training example with features x and label y, and the class z can either be"
    },
    {
        "casual_text": "Like always, when we're dealing with limited resources, the improvements we see on the test and dev sets don't really match up. We can clearly see better results on the test set with the BTEC stuff, but when we apply SBR, the score on the dev set actually goes down.",
        "formal_text": "Convert casual text to formal text: Like always, when we're dealing with limited resources, the improvements we see on the test and dev sets don't really match up. We can clearly see better results on the test set with"
    },
    {
        "casual_text": "To check how well the models are doing, we calculate the cosine similarity between the model's output and every possible ground truth vector. During training, we have 3701 word vectors to compare with, and during testing, we have 4284 (which includes the 3701 from training plus 583 new ones). With these word vectors, we figure out the Recall@k (R@k), which tells us how often the ground truth vector is among the top k most similar vectors to the model's output. If the ground truth is the most similar to the model's output, that counts for R@1. Similarly, if the ground truth is in the top 5 (or top 10) most similar words to the output, it counts for R@5 (or R@10).",
        "formal_text": "Convert casual text to formal text: To check how well the models are doing, we calculate the cosine similarity between the model's output and every possible ground truth vector. During training, we have 3701 word vectors"
    },
    {
        "casual_text": "Okay, let’s break this down in simpler terms: If the subtree doesn’t fit with the LP constraints, we have to toss it out. The LP constraints for the local tree, where C is the main category, are a mix of different LPc(i, j) for 1  i  j  n, plus the evaluation of the LP constraints for the subtree where the daughter is the root category. Basically, LPc(0) = LPc(i, j) for 1  i  j  n plus eval_lp(LPc(i)). Now, for a projection created by a lexical rule, the LP constraints are empty (LPc(0) = ), because lexical rules only involve one daughter, which is just a wordform.",
        "formal_text": "Convert casual text to formal text: Okay, let’s break this down in simpler terms: If the subtree doesn’t fit with the LP constraints, we have to toss it out. The LP constraints for"
    },
    {
        "casual_text": "Infrequent names tend to be more similar to themselves within each layer of different language models, but they don’t match well across layers, especially when compared to their initial representation. This suggests that these names don’t generalize well to new contexts and are kind of stuck in the specific situations they’ve been seen in before. Table 4 shows that in models like BERT, GPT-2, T5, and XLNet, the Spearman’s  values are negative, ranging from -0.523 to -0.763, with super tiny p-values (less than 10-263). Most of the least negative correlations happen in the top layers, which might be because upper layers in these models are really directional, as noted by Ethayarajh (2019). The strongest link between frequency and self-similarity within a layer is found in the first layer of GPT-2 and XLNet. Table 5 highlights that names broken into multiple tokens are more similar to themselves within a layer compared to names that are just one token. Common names and names of people from majority groups are better represented in language models than rare names or names of minority groups. Table 5 also confirms that multiply tokenized names have higher self-similarity. The embeddings for infrequent names don’t match their initial representation well, showing that these names are poorly contextualized and overfit to the contexts they’ve been seen in. On the other hand, common and singly tokenized names have more detailed representations in the embedding matrix.",
        "formal_text": "Convert casual text to formal text: Infrequent names tend to be more similar to themselves within each layer of different language models, but they don’t match well across layers, especially when compared to their initial representation. This suggests that these"
    },
    {
        "casual_text": "Okay, so we've got some weights here: v_a is a vector, W_a is a matrix, and U_a is a bigger matrix. The hidden state h_j comes from the encoder and is made up of two parts: the forward hidden state (let's call it   h j) and the backward hidden state (  h j). Basically, h_j is just the combination of these two states.",
        "formal_text": "Convert casual text to formal text: Okay, so we've got some weights here: v_a is a vector, W_a is a matrix, and U_a is a bigger matrix. The hidden"
    },
    {
        "casual_text": "For the human evaluation part, we only picked annotators who fit these requirements:",
        "formal_text": "Convert casual text to formal text: For the human evaluation part, we only picked annotators that fit these requirements: For the human evaluation part, we only picked annotators that fit these requirements: For the human evaluation part, we"
    },
    {
        "casual_text": "We’re specifically looking at three precomputed measures of language distance from the URIEL typological database (Littell et al., 2017). First, there’s phylogenetic distance (PHY), which comes from the idea of a family tree showing how languages are related. Then, there’s typological distance (TYP), which is based on how similar languages are in terms of their grammar, using data from the WALS database (Dryer and Haspelmath, 2013). Lastly, geographic distance (GEO) is about how far apart languages are spoken geographically. For more info on this, check out Littell et al. (2017).",
        "formal_text": "Convert casual text to formal text: We’re specifically looking at three precomputed measures of language distance from the URIEL typological database (Littell et al., 2017). First, there’s"
    },
    {
        "casual_text": "There are a few cool directions we could explore for future work. First off, a simple way to make these unsupervised representations better is by using structured knowledge from things like knowledge graphs or Open Information Extraction systems. We could just feed this structured knowledge as extra input to the autoencoder. Another idea is to label relation vectors, basically identifying parts of the vector space that match specific relation types (like hypernymy, for example). Another cool thing we could do is improve SeVeN by combining relation vectors along paths in the graph. This way, we might be able to predict missing connections (or smooth out relation vectors that were based on too few or not-so-useful sentences), kind of like how random walk strategies work for filling in gaps in traditional semantic networks and knowledge graphs (Gardner et al., 2014).",
        "formal_text": "Convert casual text to formal text: There are a few cool directions we could explore for future work. First off, a simple way to make these unsupervised representations better is by using structured knowledge from things like knowledge graphs or"
    },
    {
        "casual_text": "This part looks at the stuff that impacts how well PLM works for automatically assigning ICD codes.",
        "formal_text": "Convert casual text to formal text: This part looks at the stuff impacts the stuff that impacts how well PLM works for automatically assigning ICD codes. Convert casual text to formal text: This part looks at the stuff impacts the stuff"
    },
    {
        "casual_text": "Alright, let’s break down how we measure the connection between two sentences using word embeddings. There are three main methods for doing this: 1. **Greedy Metric**: This one matches words in both sentences based on how similar their embeddings are (using cosine similarity). It then averages all these similarity scores to get a final number. This method was introduced by Rus and Lintean in 2012. 2. **Average Metric**: Instead of matching words one by one, this method just averages all the word embeddings in each sentence to create a single vector for each sentence. Then, it checks how similar these two vectors are using cosine similarity. Mitchell and Lapata came up with this idea in 2008. 3. **Extreme Metric**: This one is a bit different. It looks at all the word embeddings in a sentence and picks out the most extreme values (like the highest or lowest ones). Then, it uses these extreme values to create a vector for each sentence and compares them using cosine similarity. Forgues and his team introduced this method in 2014. So, these are the three ways we can measure how semantically connected two sentences are using word embeddings.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break down how we measure the connection between two sentences using word embeddings. There are three main methods for doing this: 1. **Greedy Metric"
    },
    {
        "casual_text": "Matrix T (l) is super important for building the low-rank version of the off-diagonal parts of the attention matrix. Think of the ij-th block in the simplified attention matrix at level-1, and that's what we're talking about.",
        "formal_text": "Convert casual text to formal text: Matrix T (l) is super important for building the low-rank version of the off-diagonal parts of the attention matrix. Think of the ij-th block in the simplified"
    },
    {
        "casual_text": "Crowdsourcing has made it super easy to gather tons of data in no time. But when it comes to annotations, crowdsourced ones need way more quality control compared to the stuff done in-house. That's mainly because it's hard to guarantee that the people doing the work actually know what they're doing when it comes to linguistic annotation.",
        "formal_text": "Convert casual text to formal text: Crowdsourcing has made it super easy to gather tons of data in no time. But when it comes to annotations, crowdsourced ones need way more quality control compared to the stuff done in-"
    },
    {
        "casual_text": "Since mention representations in coreference can be pretty different, we did a bunch of tests to see which features really matter. This should help guide future work—check out Table 4 for the details.",
        "formal_text": "Convert casual text to formal text: Since mention representations in coreference can be pretty different, we did a bunch of tests to see which features really matter. This should help guide future work—check out Table 4 for the details."
    },
    {
        "casual_text": "At ACL 2020, Amigó and his team (2020) came up with a new measure for OC, called the Closeness Evaluation Measure (CEM ORD), and talked about its rules and properties. They mainly tested it by comparing it to other measures to see how well it matched up with a set of \"gold standard\" measures they chose. Their results showed that CEM ORD was pretty similar to these gold measures, but the outcome could change if they picked a different set of gold measures. This is similar to what Sakai and Zeng (2019) found when they showed that a related meta-evaluation method called unanimity (Amigó et al., 2018) really depends on which gold measures you use. Also, while Amigó and his team (2020) said that CEM ORD works well in terms of keeping system rankings consistent across different data (which they call \"robustness\"), they didn't give many details about their experiments in their paper. So, to add more to their work, this study does a lot of thorough and reproducible experiments on OC measures. We tested nine different measures, including MAE M, MAE , and CEM ORD, as part of our OC meta-evaluation experiments.",
        "formal_text": "Convert casual text to formal text: At ACL 2020, Amigó and his team (2020) came up with a new measure for OC, called the Closeness Evaluation Measure (CEM ORD), and talked about its"
    },
    {
        "casual_text": "Sure! Here's the informal version: A string is basically a list of symbols, like s = (s1, ..., sl), where each symbol comes from an alphabet . l is the group of all strings that are exactly l symbols long, and * is the collection of all possible strings, which we can also call the language. A subsequence u of s is just some symbols from s, picked based on their positions in the string.",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: A string is basically a list of symbols, like s = (s1, ..., sl), where each symbol comes from an alphabet"
    },
    {
        "casual_text": "To tweak the settings for our CNN, we’re following the advice from Zhang and Wallace (2015). We’re using word embeddings with 300 dimensions and starting with pre-trained word2vec vectors from Mikolov et al. (2013). For words that aren’t in the pre-trained set, we randomly initialize them using a uniform distribution between -0.25 and 0.25. We’ve set the filter window sizes to 3, 4, and 5, each with 128 filters, which gives us a hidden layer size of 384 (1283). For training, we’re using the Adam optimizer, as suggested by Kingma and Ba (2015).",
        "formal_text": "Convert casual text to formal text: To tweak the settings for our CNN, we’re following the advice from Zhang and Wallace (2015). We’re using word embeddings with 300 dimensions and starting with pre-trained word2"
    },
    {
        "casual_text": "Basically, the decision is always made using the formula in Eq. 1. The difference between models comes down to how they represent things and the weight vectors they use.",
        "formal_text": "Convert casual text to formal text: Basically, the decision is always made using the formula in Eq. 1. The difference between models down down to ."
    },
    {
        "casual_text": "In this paper, we looked at how SABLE, a user-friendly system for building translation dictionaries, works for finding domain-specific word translations using small, specialized text collections. When tested on a really small corpus (around 400,000 words), the system did pretty well. It can suggest possible single-word translations by automatically removing common words. After filtering those out, the system gets a precision of up to 89% for domain-specific terms, with a recall that’s estimated to be between 30-40%, though this is a very cautious estimate.",
        "formal_text": "Convert casual text to formal text: In this paper, we looked at how SABLE, a user-friendly system for building translation dictionaries, works for finding domain-specific word translations using small, specialized text collections."
    },
    {
        "casual_text": "During the training phase, we optimize all the exits together using a combined loss function. Basically, we follow what Huang et al. (2017) did, and our loss function is a weighted average of the cross-entropy (CE) losses.",
        "formal_text": "Convert casual text to formal text: During the training phase, we optimize all the exits together using a combined loss function. Basically, we follow what Huang et al. (2017) did, and our loss function is"
    },
    {
        "casual_text": "We're curious about which kind of words we can tweak to pull off something like the attack mentioned in (Hashemi and Hwa, 2016). In this experiment, we only replaced words that were part of the same part of speech. We also tried messing with prepositions, which wasn’t allowed in the earlier experiments. From Table 3, it’s clear that prepositional, verbal, and adverbial phrases are especially vulnerable. No big surprise, most of the issues happen with structures that are naturally tricky to handle in dependency parsing.",
        "formal_text": "Convert casual text to formal text: We're curious about which kind of words we can tweak to pull off something like the attack mentioned in (Hashemi and Hwa, 2016). In this experiment, we only replaced"
    },
    {
        "casual_text": "We also link mentions of the same entity across different parts of the text. Take Figure 2, for instance: the entity \"bones\" shows up in two separate sentences, each represented by a node. We connect these nodes so that information can flow between them when we're doing our graph-based reasoning.",
        "formal_text": "Convert casual text to formal text: We also link mentions of the same entity across different parts of the text. Take Figure 2, for instance: the entity \"bones\" shows up in two separate sentences, each represented by a no"
    },
    {
        "casual_text": "CAPWAP compared to other stuff: Check out Table 1, it shows how our setup stacks up against regular captioning and visual question answering (VQA). Both regular captioning and CAPWAP give you one caption per image, but CAPWAP doesn’t compare its output to any references. Both VQA and CAPWAP use QA data for training and testing, but CAPWAP doesn’t need a question before it starts generating stuff. VQA models just spit out a single answer, while CAPWAP gives you these anticipatory contexts.",
        "formal_text": "Convert casual text to formal text: CAPWAP compared to other stuff: Check out Table 1, it shows how our setup stacks up against regular captioning and visual question answering (VQA). Both regular captioning and CAP"
    },
    {
        "casual_text": "But we gotta be mindful when setting the frequency limits. See, case slots spotted by DBA or marked as obligatory case> by SBA are more likely to be, well, obligatory. On the flip side, stuff like modification> or time> cases? Those should always be optional. Taking all that into account, we’ve set the thresholds for obligatory cases, and you can check them out in Table 3.",
        "formal_text": "Convert casual text to formal text: But we gotta be mindful when setting the frequency limits. See, case slots spotted by DBA or marked as obligatory case> by SBA are more likely to be,"
    },
    {
        "casual_text": "Figure 5 shows the real and computer-made mel-spectrograms from different ways of making data simpler.",
        "formal_text": "Convert casual text to formal text: Figure 5 shows the real and computer-made mel-spectrograms from different ways to making data simpler."
    },
    {
        "casual_text": "We started working on classifying words to help model a morpho-syntactic alternation in Nêhiyawêwin verbs. Inspired by Arppe et al. (2008), we came up with a hypothesis that both the semantic classes of the verbs and their noun arguments could play a role in this alternation. Because of time limits, we looked into ways to automatically classify both verbs and nouns in Nêhiyawêwin. While our main goal is still to use statistical modeling, semantic or thematic classifications can be super helpful for language learners and revitalization efforts, especially in online dictionaries. Instead of just looking up English translations, you could see all the words related to a specific theme, which is way more useful.",
        "formal_text": "Convert casual text to formal text: We started working on classifying words to help model a morpho-syntactic alternation in Nêhiyawêwin verbs. Inspired by Arppe"
    },
    {
        "casual_text": "Here are some examples of titles generated using rejection sampling. In Figure 7, you'll see titles that were picked and ones that got rejected. As you can tell, the rejected titles usually aren't as good.",
        "formal_text": "Convert casual text to formal text: Here are some examples of titles generated using rejection sampling. In Figure 7, you'll see titles that were picked and ones that got rejected. As you can tell, the rejected titles usually aren't"
    },
    {
        "casual_text": "Step Two. Now, we need to make a ranked list of types for all the terms we found in the Terminology extraction step (Section 3.1). But, we're only talking about the terms that weren't already included in the list of is-a relationships we got from either the Pattern based or Semantic Web based methods.",
        "formal_text": "Convert casual text to formal text: Step Two. Now, we need to make a ranked list of types for all the terms we found in the Terminology extraction step (Section 3.1). But, we're only"
    },
    {
        "casual_text": "First, I need to get a sense of what’s going on with them. . . Let me dive into their experiences. (Question) Can you tell me why you’re feeling frustrated?",
        "formal_text": "Convert casual text to formal text: First, I need to get a sense what’s going with them. . . Let me dive into their experiences. (Question) Can you tell me why you’re feeling"
    },
    {
        "casual_text": "Word embeddings are super cool because they can capture the subtle meanings of words, which is why people often use them to check how similar words are in terms of meaning. Usually, this is done by using a measure like cosine similarity to compare how close words are in a vector space and see how well it matches what humans think. But, there are other ways to measure similarity too, like Weighted Overlap or Tanimoto Distance, which some researchers have talked about. What we’re suggesting is a bit different. Instead of just trying to make individual word representations better, we want to improve their meanings by adding extra info through something called relation vectors and looking at how words are connected to others. There are lots of ways to do this, but we’re keeping it simple for now. We focus on finding the closest neighbors of two words, w1 and w2. The idea is that if w1 and w2 are similar, they should also be connected to similar words. So, we start by finding the best match between the neighbors of w1 and the neighbors of w2.",
        "formal_text": "Convert casual text to formal text: Word embeddings are super cool because they can capture the subtle meanings of words, which is why people often use them to check how similar words are in terms of meaning. Usually, this"
    },
    {
        "casual_text": "In this paper, we take a look at patents and how they affect machine translation performance. We focus on two main things: the topic, which is the technical field the patent covers, and the structure, which refers to the different sections of the patent text.",
        "formal_text": "Convert casual text to formal text: In this paper, we take a look at patents and how they affect machine translation performance. We focus on two main things: the topic, which is the technical field the patent covers, and the"
    },
    {
        "casual_text": "One interesting thing to note from these early and not-so-complete results of relational analysis and semantic classification is that even though the categories 'receiver' and 'beneficiary' overlap in the data, they don’t seem to cause confusion. This is because, in any given example, the specific role of an item that could fit into either category is decided by the classification of the direct object. Basically, if the direct object is in group 2' or 3', the dative object acts as the 'receiver'. But if the direct object is in group 5', the dative object takes on the role of 'beneficiary'. Traditionally, grammars just grouped words based on general syntactic roles or how they look morphologically. For teaching people or for basic linguistics, this worked fine. But with the rise of computers, things got more complicated. Machines don’t have any real-world experience, so they need a more detailed grammar to understand natural language. Correlational grammar is an attempt to address this. The paper talks about parts of correlational syntax and shows how a really detailed syntax can help define word classes in a way that makes sense semantically. It uses this approach to look at two areas of grammar: predicative adjectives and transitive verbs. The goal of this classification is to clear up any confusion and prevent computers from misinterpreting sentences.",
        "formal_text": "Convert casual text to formal text: One interesting thing to note from these early and not-so-complete results of relational analysis and semantic classification is that even though the categories 'receiver' and 'beneficiary"
    },
    {
        "casual_text": "In this paper, we dive into a framework for efficiently tracking the whole relaxation path of constrained max-entropy problems. We kick things off in Section 2 by generalizing the problem: we look at finding a distribution that minimizes the relative entropy to a given prior distribution while meeting max-norm constraints based on an observed distribution. In Section 3, we take on this challenge by introducing a re-parametrization that boils the unknown distribution down to a single scalar. Then, in Section 4, we describe a homotopy between the relaxation parameter and the distribution characterizing parameter. This setup also highlights a cool symmetry between the prior distribution and the observed one. Using this reformulated problem, we outline space and time efficient algorithms for tracking the entire relaxation path in Sections 5-6. Our approach is based on a neat geometric view of the relaxation path as a piecewise linear function in a two-dimensional space of relaxation-characterization parameters. Unlike typical homotopy methods for the Lasso (as Osborne et al. did in 2000), our method for tracking the max-ent homotopy ends up with surprisingly low complexity bounds, making it practical for large alphabets. We share some early experimental results with Zipf distributions in Section 8, showing the strengths of our approach. Finally, in Section 9, we wrap up with a quick look at what’s next.",
        "formal_text": "Convert casual text to formal text: In this paper, we dive into a framework for efficiently tracking the whole relaxation path of constrained max-entropy problems. We kick things off in Section 2 by generalizing the problem: we"
    },
    {
        "casual_text": "c. GloVe: We use GloVe embeddings (Pennington et al., 2014) that we trained on a collection of 3 million words from a mix of recipe texts and video transcriptions. For any given instruction, we take the GloVe embeddings (Pennington et al., 2014) of the nouns and verbs and average them to get a single vector representing the instruction. When comparing two instructions, we measure how similar they are by looking at the cosine similarity between their vectors.",
        "formal_text": "Convert casual text to formal text: c. GloVe: We use GloVe embeddings (Pennington et al., 2014) that we trained on a collection of 3 million words from"
    },
    {
        "casual_text": "We tried this method: first, we ranked all the candidates using a similarity function and picked the top 3. Then, we only kept those 3 if they were within the last 4 sentences. Without worrying about their semantic class, this bumped up the precision to 41% (it was 30% if we limited the distance earlier, or 39% without limiting it). If we added a filter based on semantic classes (so only keeping the top 3 if they matched the right class and were in the last 4 sentences), we got a much better precision of 53%, with a decent recall of 57.8%. Compared to the similarity-list approach, we got way better precision than other methods with similar recall (like the one with the 100 most similar items, which has 44% precision, or the one with 50 items and two-way matching, which has 46% precision).",
        "formal_text": "Convert casual text to formal text: We tried this method: first, we ranked all the candidates using a similarity function and picked the top 3. Then, we only kept those 3 if they were within the last 4 sentences"
    },
    {
        "casual_text": "One cool thing about our discourse-enhanced VAE is that once it’s trained, we can get a discourse score from the extra layer it has (check out Equation 4). This score helps us figure out how good a story is. In Table 5, you can see the predicted discourse scores for a bunch of generated stories. All these stories were made by randomly picking points in the latent space. From what we’ve seen, stories with high discourse scores usually make sense and flow well, while those with low scores often have issues like logic problems or repeating stuff. To put a number on it, we calculated the average discourse score for the test stories and their negative samples, and the averages came out to 0.75 and 0.25, respectively. In Figure 3, you’ll find a look at the trade-offs between quality and diversity for generated sentences across three datasets. For both quality and diversity, a lower score is better, and the curve closest to the axes is the one that performs the best overall.",
        "formal_text": "Convert casual text to formal text: One cool thing about our discourse-enhanced VAE is that once it’s trained, we can get a discourse score from the extra layer it has (check out Equation 4)."
    },
    {
        "casual_text": "The big challenge here is figuring out how to add an adaptive paraphrasing model to a text simplification tool. The goal is to see if we can improve the quality of the tool's natural language processing (NLP) by using adaptive learning with real-world usage data. This text simplification tool, called Par4Sim, has a few different parts working together. First, it looks for complex or tricky words and phrases (let’s call them CPs) based on some research by Yimam et al. (2017). Once it spots these CPs, the next step is to come up with possible alternatives using different paraphrase resources. Any suggestions that don’t make sense in the sentence are tossed out based on a language model score. Finally, an adaptive ranking model sorts the remaining options by how simple they are and shows them to the user in an easy-to-use writing tool.",
        "formal_text": "Convert casual text to formal text: The big challenge here is figuring out how to add an adaptive paraphrasing model to a text simplification tool. The goal is to see if we can improve the quality of the tool"
    },
    {
        "casual_text": "In Figures 1, 2, 3, and 4, the X-axis represents similarity. In each figure and Table 7, there are 24 sets that can be correctly clustered in 'Dis', while 21 sets can be clustered in 'Freq'. Looking at the results in Figure 3, 'BVG' and 'HRD' are correctly classified as 'food restaurant' and 'market news', respectively. But in 'Freq' (Figure 1), they're classified incorrectly. This shows that 'Dis' can tell apart different meanings of words like 'BVG' and 'HRD'. In Table 9, for instance, 'security' is used a lot and in the sense of 'being secure' in 'BVG', but in 'HRD', 'security' means 'certificate of creditorship'. One reason 'Freq' might not work as well as 'Dis' is that it doesn't recognize the different meanings of high-frequency words.",
        "formal_text": "Convert casual text to formal text: In Figures 1, 2, 3, and 4, the X-axis represents similarity. In each figure and Table 7, there are 24 sets that can be correctly clustered in 'Dis'"
    },
    {
        "casual_text": "If we’re just filtering and not using a specific query, the keyphrases from the paper are pulled out and treated like a query instead. In this situation, all the keywords in the generated query are considered equally important.",
        "formal_text": "Convert casual text to formal text: If we’re just filtering and not using a specific query, the keyphrases from the paper are pulled out and treated like a query instead. In this situation, all keywords in the"
    },
    {
        "casual_text": "Okay, so if (i0, j0) is in A sub and (i1, j1) is also in A sub, then do this: First, calculate x by multiplying (i1 - i0) by (j1 - j0). Then, if x is greater than 0, do whatever comes next.",
        "formal_text": "Convert casual text to formal text: Okay, so if (i0, j0) is in A sub and (i1, j1) is also in A sub, then do this: First, calculate x by multiply"
    },
    {
        "casual_text": "Subjectivity is basically about how different people experience things based on their physical, political, and cultural backgrounds (Ellis and Flaherty, 1992). This idea can help us grasp emotions as something everyone feels, but in their own unique way. For instance, there’s a lot of research that talks about \"universal\" emotions. Ekman and Friesen (1971) came up with the idea that certain emotions are the same everywhere, like anger, fear, happiness (or joy), disgust, sadness, and surprise. Plutchik (1980) added trust and anticipation to that list. According to these big theories in psychology and sociology, these emotions show up across cultures (Ekman and Keltner, 1997). They also think these emotions have similar evolutionary and biological roots. For example, fear helps us react to danger, surprise kicks in for sudden changes, and emotions like happiness, joy, and trust help us communicate good feelings, while anger and sadness signal negative ones. Disgust helps us avoid harmful stuff, like germs or bad food (Pessoa and Adolphs, 2010). So, in a way, these emotions are like a \"universal language.\" The whole idea of subjectivity we talked about earlier also mentions \"cultural diversity,\" which ties into how different cultures experience these emotions.",
        "formal_text": "Convert casual text to formal text: Subjectivity is basically about how different people experience things based on their physical, political, and cultural backgrounds (Ellis and Flaherty, 1992). This idea can help us grasp emotions as"
    },
    {
        "casual_text": "Hey, just a heads-up: we're using the same dataset for both training the discourse parser (MEGA-DT) and evaluating sentiment (Yelp'13). To keep things fair, we're only using the training part of the dataset to train the discourse parser. This means the development and test documents haven't been seen at all during the training process.",
        "formal_text": "Convert casual text to formal text: Hey, just a heads-up: we're using the same dataset for both training the discourse parser (MEGA-DT) and evaluating sentiment (Yelp'13"
    },
    {
        "casual_text": "The reference translation is the smoothest overall. Makes sense since it's written by humans. On average, the reference translation scored the same in terms of adequacy compared to the others. Our reference was collected from the web, so it has some issues, as mentioned in section 3.3. One big problem with the reference translation is that it focuses on the whole document, which makes it less adequate for sentence-by-sentence translation. This is especially noticeable in conversational texts, where the reference was translated based on the entire session (like a talk or vlog). Here's an example: Source: \"-Nope, they're shutting us down.\" Ref: \"-Tidak, misi ditunda.\" Ours: \"-Tidak, mereka menutup kita.\" Google Translate: \"-Tidak, mereka menutup kita.\"",
        "formal_text": "Convert casual text to formal text: The reference translation is the smoothest overall. Makes sense since it's written by humans. On average, the reference translation scored the same in terms of adequacy compared to the"
    },
    {
        "casual_text": "This paper talks about evaluating how well different groups of words or tweets fit together, which is kind of like judging how well-organized a bunch of topics are. People often do this for topic models, where they check how coherent automatically grouped words are (Mimno et al., 2011; Newman et al., 2010). We’re doing something similar but with tweet clusters from the Election and COVID-19 datasets. Here’s how we made the clusters: Tweets with the same keyword, posted within a certain time frame (3 hours for Election, 1 hour for COVID-19), were grouped together using a two-step clustering method by Wang et al. (2017b). This method uses two topic models (Yin and Wang, 2014) and a step where tweets are pooled together. We picked this approach because it’s been pretty effective in different tweet clustering tasks and doesn’t need a fixed number of clusters beforehand.",
        "formal_text": "Convert casual text to formal text: This paper talks about evaluating how well different groups of words or tweets fit together, which is kind of like judging how well-organized a bunch of topics are. People often do this for"
    },
    {
        "casual_text": "We take a closer look at these ideas by turning sentiment classification tasks into language modeling tasks. Specifically, as you can see in Figure 2, both ACSA and ACD are turned into sequence-to-sequence (seq2seq) tasks. In this setup, the encoder reads the input sentence, and the decoder creates a natural language sentence as the output. For ACD, the output is a simple statement saying whether a specific aspect is mentioned (like, \"The category_type category is discussed\"). For ACSA, the output tells us the sentiment polarity of a particular aspect (for example, \"The sentiment polarity of given_category is polarity_type\"). This setup is pretty similar to a denoising auto-.",
        "formal_text": "Convert casual text to formal text: We take a closer look at these ideas by turning sentiment classification tasks into language modeling tasks. Specifically, as you can see in Figure 2, both ACSA and ACD are turned into sequence-to"
    },
    {
        "casual_text": "For our experiments, we started by grabbing a sample of Wikipedia articles—specifically, every 20th one. We narrowed it down to articles that had matching versions in both English and Spanish, with at least 100 words each. This left us with 10,369 aligned documents, which we called our background collection, or B for short. We then split this collection into a training set with 9,332 documents and a test set with 1,037 documents. After that, we got rid of any words that appeared fewer than 50 times overall. This cleaned-up version of the data had 6.7 million words in English and 4.2 million words in Spanish.",
        "formal_text": "Convert casual text to formal text: For our experiments, we started by grabbing a sample of Wikipedia articles—specifically, every 20th one. We narrowed it down to articles that had matching versions in both English and Spanish"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. The probability that term ( t_j ) shows up given a document ( v_i ) and its label ( L_i ) is calculated like this: First, check if ( t_j ) is in the label ( L_i ). If it is, you multiply by ( et_jT v_i ). If it's not, you just ignore it. Then, you do the same thing for all the other terms ( t_k ) in ( L_i ), and sum up all those ( et_kT v_i ) values. Finally, you divide the first part by the sum you just calculated. So, it's like saying, \"How likely is ( t_j ) to be in this document based on its label and the other terms in that label?\"",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way. The probability that term ( t_j ) shows up given a document ( v_i"
    },
    {
        "casual_text": "When it comes to translating Lin  EBM T REC+, it generally takes less time than translating LinEBM T. However, there's a bit more work involved because you have to extract the constraints. This extra step is due to changes in the recombination matrix.",
        "formal_text": "Convert casual text to formal text: When it comes to translating Lin  EBM T REC+, it generally takes less time than translating LinEBM T. However, there's a bit more work involved because you"
    },
    {
        "casual_text": "Alright, so in the Multilingual setup (check out Figure 3c), instead of having two different random forest classifiers, we can just use one that works with both sets of features at the same time. This means each language-specific feature now has two versions. The model doesn’t know the language of the input words directly, but some features, like n-gram probabilities, are super helpful in figuring that out. The cool thing about this approach is that we only need to train one classifier, and we don’t need to label the language. But, it’s a bit more complicated because we’re feeding it more features than the Monolingual or Fragments models, which makes the whole thing a bit more complex.",
        "formal_text": "Convert casual text to formal text: Alright, so in the Multilingual setup (check out Figure 3c), instead of having two different random forest classifiers, we can just use one that works with both sets of features at the"
    },
    {
        "casual_text": "Sometimes, language models (LMs) can get confused and think \"and\" signals a similar relationship between words, but it might go beyond just the word you're looking at. For instance, if you type \"The human heart not only makes heart sounds and,\" the LM might predict \"muscle\" as one of the top suggestions, and then follow up with \"movements.\" So, it's not just matching \"sounds\" but also the whole phrase \"heart sounds\" with something like \"muscle movements.\" We didn't really focus on this in our current work, but we think making sure the predictions match the target word in terms of grammar (like being the same part of speech or having the same number) could help fix this. Plus, this opens up a cool possibility for moving beyond just single words and dealing with whole phrases instead.",
        "formal_text": "Convert casual text to formal text: Sometimes, language models (LMs) can get confused and think \"and\" signals a similar relationship between words, but it might go beyond just the word you're looking at. For instance,"
    },
    {
        "casual_text": "In Table 3, it looks like basic emotions (like joy, anger, or sadness) usually perform better than complex emotions (like positive, neutral, or negative). But in Table 1, the amount of data for basic emotions is often smaller than for complex ones. This suggests that the difference in performance is probably due to the emotional content of the labels, not the size of the data. For example, the complex emotion 'negative' (which includes things like 'hate' and 'anxious') is way more varied than the basic emotion 'sad', and that variety makes it harder to detect. Also, even though 'sad' and 'angry' are both basic emotions and have similar amounts of data, it seems way easier to spot 'sad' than 'angry'. Maybe this is because 'angry' can be caused by all sorts of different things, and it's harder to figure out and use that information. So, for emotion classification, it's important to have an encoder that can pull out the event-related info about what caused the emotion from the text. Table 4 shows how different emotion cause detection models perform, with \"Sequence\" listing the order of words each model uses. In Table 4, JMECause does better than the best pipeline model (LSTM) by 0.8% in terms of F-scores.",
        "formal_text": "Convert casual text to formal text: In Table 3, it looks like basic emotions (like joy, anger, or sadness) usually perform better than complex emotions (like positive, neutral, or negative). But in Table 1, the amount of data for"
    },
    {
        "casual_text": "CPL uses some shortcuts, but CPL-Lite is a simpler version of CPL that works in a more predictable way, kind of like PENG. In CPL-Lite, each sentence is just about one simple connection between two things. There are three kinds of connections: noun-like ones (like \"the age of x> is y>\"), verb-like ones (like \"x> causes y>\"), and ones that feel like prepositions (like \"x> is during y>\").",
        "formal_text": "Convert casual text to formal text: CPL uses some shortcuts, but CPL-Lite is a simpler version of CPL that works in a more predictable way, kind of like PENG. In CPL-Lite,"
    },
    {
        "casual_text": "3) CoFEE-MRC hits the best F1 score on all three tests, which shows that our CoFEE pre-training method really works. 4) The FET pre-training task boosts performance, proving that adding fine-grained named entity knowledge is a good move. 5) ESI pre-training gives an extra performance boost, which shows that using general-typed named entity knowledge to warm up the language model is important.",
        "formal_text": "Convert casual text to formal text: 3) CoFEE-MRC hits the best F1 score on all three tests, which shows that our CoFEE pre-training method really works. 4) The FET pre-training task"
    },
    {
        "casual_text": "We're using Transformer (from Vaswani et al., 2017) as our basic, context-free model and Transformer+HAN (by Miculicich et al., 2018) as our context-aware model. We set up our experiments following the same setup as HAN. Basically, both the sentence encoder and decoder have 6 hidden layers, and the path encoder has 2 hidden layers. We consider the three sentences before the current one as context for the current sentence. The hidden size and the size of the pointwise FFN are 512 and 2048, respectively. We set the dropout rate for all hidden states to 0.1. Both the source and target vocabularies have 30K words. During training, we use the Adam optimizer (Kingma and Ba, 2015), with batch sizes of 4096 for the context-free model and 1024 for the context-aware one. Lastly, we measure translation quality using case-sensitive BLEU (Papineni et al., 2002) and TER (Snover et al., 2006).",
        "formal_text": "Convert casual text to formal text: We're using Transformer (from Vaswani et al., 2017) as our basic, context-free model and Transformer+HAN (by Miculicich et al."
    },
    {
        "casual_text": "There's a bunch of research out there that uses masked language models (MLM) for understanding language. The main idea is to use pre-trained models and create specific prompts for language tasks. For example, Brown and his team (2020) used prompts for few-shot learning in text classification by turning inputs into fill-in-the-blank questions. Gao and colleagues (2020) took it a step further by automatically creating label words and templates. Petroni et al. (2019) figured out how to pull relationships between entities from BERT using cloze-style templates. We’re the first to try applying these methods to Aspect-Category Sentiment Analysis (ACSA), using it as a starting point. Unlike those template-based models, our final approach uses BART for text generation. BART does a better job of capturing the connection between the input and output sentences compared to BERT.",
        "formal_text": "Convert casual text to formal text: There's a bunch of research out there that uses masked language models (MLM) for understanding language. The main idea is to use pre-trained models and create specific prompts"
    },
    {
        "casual_text": "(f) If the answer has words like \"perfect\" or \"satisfied\" that show feelings, we mark it as positive. E6 is a good example of this.",
        "formal_text": "Convert casual text to formal text: (f) If the answer has words like \"perfect\" or \"satisfied\" that show feelings, we mark it as positive. E6 is a good example of this. E7"
    },
    {
        "casual_text": "Sure, I can help with that. Here's the informal version: You know how people talk about passport checks at Berwick and that whole idea of putting up a barbed wire fence along Hadrian's Wall?",
        "formal_text": "Convert casual text to formal text: Sure, I can help with that. Here's the informal version: You know how people talk about passport checks at Berwick and that whole idea putting up barbed wire fence along Hadrian'"
    },
    {
        "casual_text": "Lexical Semantics, Ambiguity, and Plausible Assignments. The accuracy numbers we’re getting from our methods are kind of \"pessimistic\" in a way, meaning they should be seen as lower limits. When we dig into the mistakes our models make, we find that a lot of the frame assignments they come up with are actually pretty reasonable, even if they don’t match the exact labels in the leave-one-out test. Take the LU \"guerrilla,\" for example. In FrameNet, it’s assigned to the frame PEOPLE BY VOCATION. Our mixed model suggests the two most similar frames are MILITARY and TERRORISM, which still make sense, right? Same thing with the LU \"caravan.\" Our model says the closest frame is VEHICLE, but in FrameNet, it’s only linked to the frame BUILDINGS. This happens because FrameNet doesn’t cover everything—some LUs aren’t fully annotated and only show up in a few of their possible frames. So, the actual accuracy of our models is probably higher than what the numbers suggest.",
        "formal_text": "Convert casual text to formal text: Lexical Semantics, Ambiguity, and Plausible Assignments. The accuracy numbers we’re getting from our methods are kind of \"pessimistic\" in a way"
    },
    {
        "casual_text": "Microsoft's statistical MT engine has two types of decoders: one that uses a parser to create dependency treelets, which is helpful for translating between languages with different word orders. Then there's a simpler string-based decoder that doesn't need any fancy linguistic info to work. This one's great for quickly training on language pairs when you don't have a parser handy.",
        "formal_text": "Convert casual text to formal text: Microsoft's statistical MT engine has two types of decoders: one that uses a parser to create dependency treelets, which is helpful for translating between languages with different word orders"
    },
    {
        "casual_text": "In this part (focused on proof search), we're looking for axiom links where both conclusions are connected by cut links. Basically, we're trying to finish up U3. Just like in the last section, we're working with equivalences, so solving equation (1) is the same as solving equation (2): 4 = r, . Y tcr. _, in Y, where X is invertible. After that, we need to solve (r3: X-1 +0\"4 so that tU3 = (ra and U a = 1.",
        "formal_text": "Convert casual text to formal text: In this part (focused on proof search), we're looking for axiom links where both conclusions are connected by cut links. Basically, we're trying to finish up U3. Just"
    },
    {
        "casual_text": "Even though humans can process natural language using these three analyzers at the same time, we’ve made some tweaks to their approach to make it way more efficient for computers to handle.",
        "formal_text": "Convert casual text to formal text: Even though humans can process natural language using these three analyzers at the same time, we’ve made some tweaks to their approach to make way more efficient for computers to handle. Convert casual"
    },
    {
        "casual_text": "We're using an RL framework to train our captioning model with QA data. For more techy stuff like hyperparameter settings and optimization choices, check out Appendix B.",
        "formal_text": "Convert casual text to formal text: We're using an RL framework to train our captioning model with QA data. For more tech stuff like hyperparameter settings and optimization choices, check out Appendix B."
    },
    {
        "casual_text": "Sheremetyeva and Nirenburg (1999) already talked about a prototype of the APTrans app, so we won't go into the technical details. Instead, we'll focus on how the app works and its developer setup, especially when it comes to using it as part of a machine translation learning environment.",
        "formal_text": "Convert casual text to formal text: Sheremetyeva and Nirenburg (1999) already talked about a prototype of the APTrans app, so we won't go into the technical details. Instead, we'll"
    },
    {
        "casual_text": "The IITB-en-hi results show the automated evaluation scores for both translation directions in IITB-hi-en, and they're listed in Table 2. For the hien direction, the ilmulti model scored higher BLEU points than previous submissions. Plus, using backtranslation gave an extra boost of +0.39 in BLEU scores. A similar improvement happened in the enhi direction when we added backtranslated data to the ilmulti model. Both directions got decent scores, though they weren't the best in category 5. For UFAL English-Tamil, we didn't do any extra training on the ilmulti model with backtranslation. When we tested it on the UFAL English-Tamil test set, the non-adapted model didn't do well in terms of BLEU scores. But after warmstarting and training with the UFAL English-Tamil dataset for a few more epochs, we got better scores in both directions. These results are also in Table 2.",
        "formal_text": "Convert casual text to formal text: The IITB-en-hi results show the automated evaluation scores for both translation directions in IITB-hi-en, and they're listed in Table 2. For the hien direction"
    },
    {
        "casual_text": "We're diving into the theoretical side of the CAN framework here. Basically, we show that CANs work by minimizing differences between multiple joint distributions of shared features and label predictions. We also explain the specific situation where the conditional domain discriminator performs its best.",
        "formal_text": "Convert casual text to formal text: We're diving into the theoretical side of the CAN framework here. Basically, we show that CANs work by minimizing differences between multiple joint distributions of shared features and label predictions."
    },
    {
        "casual_text": "The weights in the logistic regression model, as shown in Table 1, help us predict how the classifier will behave with new data. When we look at the features, specifically the one about matching syntactic paths and the one that checks if the first mention is in the subject position, we notice both have positive weights. However, the first feature (syntactic path match) is stronger than the second (subject position), which tells us that parallel roles seem to have a bigger effect than just the subject position of the antecedent. Based on this, we can guess that the Subject Assignment Strategy is at play, but it’s paired with the Parallel Function Strategy. And if the Parallel Function Strategy comes into play, it seems to have a stronger pull and might even override the Subject Assignment Strategy.",
        "formal_text": "Convert casual text to formal text: The weights in the logistic regression model, as shown in Table 1, help us predict how the classifier will behave with new data. When we look at the features, specifically the one about matching synt"
    },
    {
        "casual_text": "There are a few ways to normalize matrix D1. One common method is called Sinkhorn balancing, which was introduced by Sinkhorn in 1964. This technique turns a square matrix with non-negative elements into a doubly stochastic matrix, meaning both the rows and columns add up to 1. Sinkhorn balancing works through an iterative process. At each step, it calculates the sums of the rows and columns, then uses those sums to rescale the matrix. To balance matrix A, each iteration involves two updates: A gets multiplied by WR and then by WC. WR is a diagonal matrix with the inverse of the row sums of A, and WC is similar but with the inverse of the column sums. This algorithm converges linearly, so it might take a lot of iterations to get the job done. You can tweak it to normalize the rows and columns using any norm you like. From what we've seen, normalizing D1 using the Euclidean norm works pretty well in real-world applications.",
        "formal_text": "Convert casual text to formal text: There are a few ways to normalize matrix D1. One common method is called Sinkhorn balancing, which was introduced by Sinkhorn in 1964. This technique turns a square matrix with non"
    },
    {
        "casual_text": "Our model isn't dealing with anything sensitive, like legal or healthcare situations. The dataset we used for the experiment doesn't have any sensitive info either. One cool thing about our neuro-symbolic RL method is that it can pull out the rules it learned, which makes the model easier to understand. This means we can figure out why it made a certain decision. If the model ever gives biased results, this feature will come in handy for figuring out what’s causing those biases.",
        "formal_text": "Convert casual text to formal text: Our model isn't dealing with anything sensitive, like legal or healthcare situations. The dataset we used for the experiment doesn't have any sensitive info either. One cool thing about our neuro-s"
    },
    {
        "casual_text": "Text generation covers a bunch of different tasks, like style transfer (Liu et al., 2020a) and filling in missing text. This whole area has been around since the days of statistical machine translation (Liu et al., 2006; Galley et al., 2006), and people have been looking into predicting linguistic structures for a long time. Earlier work often focused on using syntactic structures on the decoder side, like modeling how words relate to each other over long distances using syntactic dependency trees (Wu et al., 2017), or subtly including linguistic knowledge in the decoder (Eriguchi et al., 2017) and decoding together with syntactic structures (Feng et al., 2020). In NAG, linguistic structures can still be super useful. They can act as a kind of guide for the target sentence, helping models understand word dependencies while decoding. But the methods we talked about earlier don’t really fit well with current NAG models because they were made for AG. On the other hand, POSPD is more flexible—it’s like a plug-and-play tool that uses a separate POS predictor to keep NAG models in check during inference. This way, NAG models can benefit from having syntactic structures to guide them without needing to change their original setup.",
        "formal_text": "Convert casual text to formal text: Text generation covers a bunch of different tasks, like style transfer (Liu et al., 2020a) and filling in missing text. This whole area has been around since the"
    },
    {
        "casual_text": "We're looking at how well three different types of features work: regular features, social features, and location features. Regular features are the kind you'd use in typical entity linking tasks, but social and location features are special to our specific problem.",
        "formal_text": "Convert casual text to formal text: We're looking at how well three different types of features work: regular features, social features, and location features. Regular features are the kind you'd use in typical entity linking tasks, but social"
    },
    {
        "casual_text": "Lately, people have been looking at understanding text as a kind of supervised learning problem (Kumar et al., 2015; Hermann et al., 2015). Basically, this means figuring out the probability of an answer (a) to a question (q) based on a document (d), written as p(a|d, q). To do this, you need a big dataset of examples with documents, questions, and answers. But so far, these datasets have only had a few hundred examples (Richardson et al., 2013). In simpler terms, when it comes to tracking the state of a conversation, it's about figuring out hidden values (l) connected to certain variables (v) in a conversation (d). This is done by looking at how people talk to each other and using that to make educated guesses, which can be written as p(l|d, v).",
        "formal_text": "Convert casual text to formal text: Lately, people have been looking at understanding text as a kind of supervised learning problem (Kumar et al., 2015; Hermann et al., 2015)."
    },
    {
        "casual_text": "Next, we look at how big of a deal these mistakes are for the GPT-2 model. Even though earlier parts showed that these mistakes are there from the start, they need to be significant enough to mess up GPT-2's behavior. So, we compare how GPT-2 handles the fake text versus the real stuff to see if it acts differently.",
        "formal_text": "Convert casual text to formal text: Next, we look at how big of a deal these mistakes are for the GPT-2 model. Even though earlier parts showed that these mistakes are there from the start, they need to be significant enough"
    },
    {
        "casual_text": "In the siamese network setup by Chopra et al. (2005), the weights are the same for both inputs, x_ia and x_ib. They use the 2-norm (D) between the representations R_ia and R_ib, which are generated by the shared convolutional networks for x_ia and x_ib, along with the label y_i, to train something called a contrastive loss function.",
        "formal_text": "Convert casual text to formal text: In the siamese network setup by Chopra et al. (2005), the weights are the same for both inputs, x_ia and x_ib"
    },
    {
        "casual_text": "We're using an NVIDIA GeForce RTX 2080 to train the smaller versions of GPT-2 and BERT base, and a GeForce RTX 8000 for training the medium GPT-2. For BERT large, we're using Tesla M40 GPUs. Since we're starting with pre-trained models, we can wrap up training each one in about two weeks. It takes just one GPU for GPT-2 Small, BERT base, and GPT-2 Medium, but we need four GPUs for training BERT large.",
        "formal_text": "Convert casual text to formal text: We're using an NVIDIA GeForce RTX 2080 to train the smaller versions of GPT-2 and BERT base, and a GeForce RTX 8000"
    },
    {
        "casual_text": "So, basically, DATR theories can work as a kind of dictionary for a PATR system. In this setup, the words (or lexemes) are like DATR nodes, and when you ask about them, you get back these feature structures. In a dictionary set up like this (like in example 6), you can see three types of nodes: 1. The main ones, like FROG, which are the actual words. 2. Nodes like SYNTAX or NV, which are like the attributes in the PATR system. 3. Nodes like NOUN or LEX/CAL, which are more like categories or types. The words get their general info from these type nodes, but the specific details about each word are directly linked to the main nodes.",
        "formal_text": "Convert casual text to formal text: So, basically, DATR theories can work as a kind of dictionary for a PATR system. In this setup, the words (or lexemes) are like DATR"
    },
    {
        "casual_text": "We started by recognizing that information retrieval (IR) lacks a clear understanding of meaning, so we came up with a simple way to add event-based meaning to sentence representations. To do this, we use basic parsing and some extra knowledge sources for event extraction. Then, we introduced a cool two-layer clustering method to make use of event info, along with a technique similar to LSA for reducing complexity. For sentence ordering, we used a method that balances local and global coherence to mimic how people write in chunks, and we implemented it with a greedy algorithm. The results show that our event-enhanced model works way better than the basic models, both in numbers and in real-world tests.",
        "formal_text": "Convert casual text to formal text: We started by recognizing that information retrieval (IR) lacks a clear understanding of meaning, so we came up with a simple way to add event-based meaning to sentence representations."
    },
    {
        "casual_text": "We looked at how well the Translation Similarity Measure (TSM) from Section 4 works by comparing it to the MaxEnt classifier by Munteanu and Marcu from 2005. We tested this on English-German (ende) document pairs with different levels of similarity (2:1 noise ratio, 5:1, and 10:1; check out section 5.1 for more details). For both TSM and MaxEnt (using the confidence score for the \"parallel\" label), we tried out all possible thresholds in steps of 0.01 to see which ones would mark the pairs as parallel. We picked the threshold that gave the best F1 score for each method (they weren’t the same). We tested three different scenarios. The first one (Table 4) involved calculating TSM for every possible sentence pair. The second scenario (Table 5) only used TSM on the pairs suggested by the search engine, without any filtering. For corpora that are very similar (like the 2:1 noise ratio one), filtering actually made things worse. This makes sense because filtering gets rid of a lot of the pairs the engine found. So, filtering should only be used for corpora that are less similar. To make things clearer, we also ran an experiment with a 100:1 noise ratio, which is for a corpus that’s very weakly comparable.",
        "formal_text": "Convert casual text to formal text: We looked at how well the Translation Similarity Measure (TSM) from Section 4 works by comparing it to the MaxEnt classifier by Munteanu and Marcu from 2005. We tested this"
    },
    {
        "casual_text": "Check out Figure 7 for the Kendall's tau correlation summary between various metrics and human evaluations.",
        "formal_text": "Convert casual text to formal text: Check out Figure 7 for the Kendall's tau correlation summary between various metrics and human evaluations."
    },
    {
        "casual_text": "CEMEL works on the idea that if you have a name and two posts mentioning it, the more similar the posts are, the more likely they're talking about the same thing. These related posts might have helpful clues to figure out who or what the name refers to. But, just because two posts look similar doesn't mean they're actually about the same thing. If you try to expand one of these non-related posts by looking at its context, you might end up adding unnecessary noise instead of useful information.",
        "formal_text": "Convert casual text to formal text: CEMEL works on the idea that if you have a name and two posts mentioning it, the more similar the posts are, the more likely they're talking about the same thing."
    },
    {
        "casual_text": "Okay, so our method can really cut down on the number of edges in the actual search space. One possible reason is that it hits around 1.0E+6.",
        "formal_text": "Convert casual text to formal text: Okay, so our method can really cut down on the number of edges in the actual search space. One possible reason that it hits around 1.0E+6."
    },
    {
        "casual_text": "3. An abbreviation should only come from one letter in its full form, not from a bunch of them. And a single letter in the full form shouldn't turn into multiple letters when shortened.",
        "formal_text": "An abbreviation should only come from one letter in its full form, not from a bunch of them. And a single letter in the full form shouldn't turn into multiple letters when shortened."
    },
    {
        "casual_text": "Vocabulary is something we need to keep an eye on when working with test data. TSNI, P does this by limiting both the size and the scope of the vocabulary. It tries to steer clear of words that are tricky to categorize or have multiple meanings, unless the test specifically wants to check how people handle that kind of ambiguity.",
        "formal_text": "Convert casual text to formal text: Vocabulary is something we need to keep an eye on when working with test data. TSNI, P does this by limiting both the size and the scope of the vocabulary. It"
    },
    {
        "casual_text": "Okay, so we have this thing where w = w_1, ..., w_W. At a specific spot in the text, let's call it position i, the element d_i in d = d_1, ..., d_W tells us which document has the word w_i in it. And then there's this vector z = z_1, ..., z_W that kind of does something similar.",
        "formal_text": "Convert casual text to formal text: Okay, so we have this thing where w = w_1, ..., w_W. At a specific spot in the text, let's call it position i,"
    },
    {
        "casual_text": "We want to sample from P(a_j | a_not_j, C), but to keep things simple, we're grouping all the other variables in C together. We know P(a_j | a_not_j, C) except for a scaling factor, so to sample, we need to calculate Equation 5 (or 6) for every i from 1 to l. This takes time proportional to O(l). Here, we'll show a trick using auxiliary variables that lets us do this in constant time instead.",
        "formal_text": "Convert casual text to formal text: We want to sample from P(a_j | a_not_j, C), but to keep things simple, we're grouping all the other variables in C together. We know"
    },
    {
        "casual_text": "In information retrieval (IR), it's pretty common to run into issues where the search terms and the relevant text don't match up. Latent Semantic Indexing (LSI), introduced by Deerwester et al. back in 1900, was one of the first big attempts to tackle this problem. Nowadays, a lot of research is focused on tweaking queries to make them work better. For instance, Cui et al. (2002) figured that if queries with one term often lead to documents with another term, there must be a strong connection between them. They linked query terms and document terms through user sessions, where people clicked on documents shown as results for their queries. More recently, Riezler and Liu (2010) used a Statistical Machine Translation model on data that paired user queries with snippets from clicked web documents. This helped them pull out terms that could expand the context of the queries. Our work is tackling the same core issue, but we're focusing on a different angle—instead of just looking at mismatches between query terms and document terms, we're looking at mismatches in the grammatical structure of natural language queries and the relevant text. To do this, we analyze the syntactic structure of both the queries and the relevant content using dependency paths. This approach has a strong tradition in question answering (QA). For example, Lin and Pantel (2001) developed an unsupervised algorithm to automatically find inference rules—basically paraphrases—from text.",
        "formal_text": "Convert casual text to formal text: In information retrieval (IR), it's pretty common to run into issues where the search terms and the relevant text don't match up. Latent Semantic Indexing (LSI),"
    },
    {
        "casual_text": "Alright, let’s break down the three key metrics we’re using to evaluate our model. First, we’ve got two types of words: those that need to be normalized (we call these \"need norm words\") and those that don’t need any changes—basically, the model just has to \"copy\" them as they are. We’ll call the words that our model actually normalized (meaning it gave a different prediction than the original word) \"pred need norm.\" Next, we track how many of these normalized words the model got right, which we’ll call \"TP\" for True Positives. Finally, we define two important measures: recall and precision.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break down the three key metrics we’re using to evaluate our model. First, we’ve got two types of words: those that need to be normalized (we"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Basically, we set the optimization interval to 10, took out common words like \"the\" or \"and,\" ran the training for 1,000 rounds, and set a limit of 0.05 for the document-topic stuff. Each document included the Wikipedia page title and the actual question text.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Basically, we set the optimization interval to 10, took out common words like \"the\" or \"and,\" ran the training for 1,000 rounds, and"
    },
    {
        "casual_text": "In this paper, we introduce a Parser-Independent Interactive Approach (PIIA) that lets us interact with human users and helps parsers get a better grasp of natural language (NL) questions. To make this happen, we came up with three main modules: 1. **Error Locator**: This part uses an alignment method to help parsers figure out which tokens in the NL questions are unclear or uncertain. 2. **Question Generator**: This module creates multiple-choice questions in plain language for users, making the interaction smooth and user-friendly. 3. **NL Modifier**: Based on the users' feedback, this part rewrites the NL questions to make them clearer and easier for the parser to handle. Our main contributions are:",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce a Parser-Independent Interactive Approach (PIIA) that lets us interact with human users and helps parsers get a better grasp of natural language"
    },
    {
        "casual_text": "Got a house with three bedrooms and one bathroom, and honestly, it's driving me crazy.",
        "formal_text": "Convert casual text to formal text: Got a house with three bedrooms and one bathroom, and honestly it's driving me crazy."
    },
    {
        "casual_text": "Coecke and his team (2010) used category theory to give distributional models of meaning a way to combine things, making sure that each grammatical step has a matching mathematical operation. Basically, if you have a sentence s = w1 w2 • • • wn, there's a linear map f that takes the context vectors of the individual words and combines them into a single vector representing the whole sentence.",
        "formal_text": "Convert casual text to formal text: Coecke and his team (2010) used category theory to give distributional models of meaning a way to combine things, making sure that each grammatical step has a matching mathematical operation."
    },
    {
        "casual_text": "Okay, so for each item i in T, where i is between 1 and the total number of items in T: 1. Set up the sentiment classifier SC (0) using the data from  ij, and make sure j is randomized from 1 to the total number of items in  i, with i still being between 1 and the total number in T.",
        "formal_text": "Convert casual text to formal text: Okay, so for each item i in T, where i is between 1 and the total number of items in T: 1. Okay, so for each item i in T, where i"
    },
    {
        "casual_text": "Let's dive into an example using the stuff we just talked about. We'll stick with the same lexicon from Table 3, and our goal is to create a sentence—if we can—that matches the meaning shown in the proof net from Figure 3(b).",
        "formal_text": "Convert casual text to formal text: Let's dive into an example using the stuff we just talked about. We'll stick with the same lexicon from Table 3, and our goal to create a sentence—if we can—"
    },
    {
        "casual_text": "(2) But here's the thing: on CNNDM Abs and CNNDM Mix, R-2 totally crushes the competition. On the other hand, on CNNDM Ext, none of the metrics really stand out or do better than the others. So what does this mean? Well, it looks like some metrics might be getting too used to certain datasets, which is why we need to double-check how well they work on newer stuff. Also, there’s no single metric that’s the ultimate winner across all datasets. This makes it clear that we should use different metrics depending on the dataset we're working with. For example, MoverScore works great on TAC-2008, JS-2 shines on TAC-2009, and R-2 is your go-to for CNNDM datasets.",
        "formal_text": "Convert casual text to formal text: (2) But here's the thing: on CNNDM Abs and CNNDM Mix, R-2 totally crushes the competition. On the other hand, on CNNDM Ext, none of the metrics"
    },
    {
        "casual_text": "He also thinks launching a single currency across many different countries is risky because it could mess with their domestic policies and needs. He mentioned that there might be differences too. Table 4 shows how GD and RW translated this part. RW got it right by putting \"said\" at the end, but GD missed the last verb. Still, RW starts with \"he,\" which is the first word in the original Japanese sentence. This happens because our current method of breaking up sentences doesn’t save words for later, unlike how human interpreters take notes.",
        "formal_text": "Convert casual text to formal text: He also thinks launching a single currency across many different countries is risky because it could mess with their domestic policies and needs. He mentioned that there might be differences too. Table 4 shows how"
    },
    {
        "casual_text": "You can find the source code and pre-trained models over at https://github.com/Receiling/ENPAR.",
        "formal_text": "Convert casual text to formal text: You can find the source code and pre-trained models over at https://github.com/Receiling/ENPAR."
    },
    {
        "casual_text": "..             ,   (- , ),          ,      ,   -        ,     -                       ,",
        "formal_text": "Convert casual text to formal text:       .."
    },
    {
        "casual_text": "The agreement scores were 0.57 for extractive systems and 0.72 for abstractive systems.",
        "formal_text": "Convert casual text to formal text: The agreement scores were 0.57 for extractive systems and 0.72 for abstractive systems. Convert casual text to formal text: The agreement scores were 0.57 for extractive systems and 0.72 for abstract"
    },
    {
        "casual_text": "We'll start by looking at the frames that have been used most recently and also the ones linked to noun phrases that refer back to something earlier in the sentence.",
        "formal_text": "Convert casual text to formal text: We'll start looking at the frames that have been used most recently and also the ones linked to noun phrases that refer back to something earlier in the sentence."
    },
    {
        "casual_text": "A lot of research about evaluating machine translation (MT) focuses on identifying and categorizing MT mistakes. For example, studies like those by Vilar et al. (2006), Farrs et al. (2010), Stymne and Ahrenberg (2012), Lommel et al. (2014), and Klubika et al. (2018) have done this. However, not many papers look at how people actually perceive these errors, and none of them really nail down what exactly makes a translation good or bad. Kirchhoff et al. (2014) took a different approach by using conjoint analysis to figure out what users prefer when it comes to MT errors. They started by labeling the errors in MT translations and then showed different versions with various types of errors to people who evaluated them. The evaluators were asked to pick the MT output they liked best and explain why. One interesting finding was that the frequency of certain error types didn’t match up with what users preferred. For instance, word order errors were the least liked, even though they happened less often. After that came word sense errors (like ambiguity), then morphological errors (which were the most common). On the other hand, errors with function words were the least annoying to people.",
        "formal_text": "Convert casual text to formal text: A lot of research about evaluating machine translation (MT) focuses on identifying and categorizing MT mistakes. For example, studies like those by Vilar et al. (2006),"
    },
    {
        "casual_text": "The candidate antecedent is typically a noun phrase made up of a few words. We use NP = (np1, np2, ..., npn) to represent all the possible antecedents for a given zero pronoun wzp, and npt = (w1, w2, ..., wj) to represent one of those antecedents. First, we take the pre-trained word vectors and feed them into a 1-layer BiLSTM network to get context-aware representations for each word.",
        "formal_text": "Convert casual text to formal text: The candidate antecedent is typically a noun phrase made up of a few words. We use NP = (np1, np2, ..., npn)"
    },
    {
        "casual_text": "2. A lot of these methods give you results, but sometimes the connection between things isn't very strong. For what we're doing, it's better to focus on the ones with a really strong link.",
        "formal_text": "Convert casual text to formal text: 2. A lot of these methods give you results, but sometimes the connection between things isn't very strong. For what we're doing, it's better to focus on the ones with"
    },
    {
        "casual_text": "Why don’t we have two other subcategories: false protasis with a conditional speech act and true protasis with standard inference? Well, there’s no real-life situation where the first one would make sense. If both the writer and the reader know the protasis is false, then the whole speech act just wouldn’t fly. As for the second one, there’s already another category—non-temporal since—that does the same job.",
        "formal_text": "Convert casual text to formal text: Why don’t we have two other subcategories: false protasis with a conditional speech act and true protasis with standard inference? Well, there"
    },
    {
        "casual_text": "The INIT method starts by training a model with labeled data from the source domain. After that, it uses the learned parameters to set up a target model. Lastly, it tweaks the target model using labeled data from the target domain. The MULT method, in contrast, trains two models at the same time—one with source data and the other with target data. During this process, some parameters are shared between the two models. Figure 1 shows these two methods using the BLSTM-CRF (a bidirectional LSTM with a CRF layer) for NER. While these approaches seem to make sense, they also have their downsides.",
        "formal_text": "Convert casual text to formal text: The INIT method starts by training a model with labeled data from the source domain. After that, it uses the learned parameters to set up a target model. Lastly, it tweak"
    },
    {
        "casual_text": "The analysts looked at the average scores and how much they varied for the acceptability ratings of different tools. The scale they used for acceptability has points that are equally spaced based on real data (Means, 2006), so it’s okay to use means, standard deviations, and t-tests. The ratings went from -3 (totally unacceptable) to +3 (totally acceptable). Figure 2 shows the average scores and how much they varied for all the participants who filled out the analyst questionnaire. To figure out at what point machine translation (MT) becomes useful for analysts, they asked a final question in the Follow-on Questions part of the experiment: \"Looking at the rating scale, at what point does a document become useful to you?\" The analysts answered using the same scale from \"totally unacceptable\" to \"totally acceptable.\" This point is marked with a dotted line in Figure 2. The figure shows that human post-edited passages got ratings at or above the \"useful\" level, while machine post-edited passages scored lower.",
        "formal_text": "Convert casual text to formal text: The analysts looked at the average scores and how much they varied for the acceptability ratings of different tools. The scale they used for acceptability has points that are equally spaced based on real data ("
    },
    {
        "casual_text": "Back in the day, early research on generating text with specific controls used things like attribute label embeddings (Ficler and Goldberg, 2017) or latent variables (Hu et al., 2017; Ke et al., 2018; Zhou and Wang, 2018) to figure out how control variables and the generated text were connected. But as big, pre-trained generative models got better, it became a hassle and expensive to retrain or tweak these models on datasets with attribute labels (Keskar et al., 2019). Nowadays, people are focusing on methods that work during the decoding phase, letting pre-trained models generate text with the desired attributes right away during inference. This includes stuff like PPLM (Dathathri et al., 2020), GeDi (Krause et al., 2020), FUDGE (Yang and Klein, 2021), and DEXPERTS (Liu et al., 2021a). The thing is, these methods really depend on human evaluation because the current automated metrics, whether unsupervised or supervised, just don’t cut it for judging controlled text generation (Dathathri et al., 2020).",
        "formal_text": "Convert casual text to formal text: Back in the day, early research on generating text with specific controls used things like attribute label embeddings (Ficler and Goldberg, 2017) or latent variables (Hu e"
    },
    {
        "casual_text": "The model's performance really depends on the set of permutations used during training. We don’t know exactly how B&L came up with their permutations, but we’re guessing they were generated completely randomly.",
        "formal_text": "Convert casual text to formal text: The model's performance really depends on the set of permutations used during training. We don’t know exactly how B&L came up with their permutations, but we’re guess"
    },
    {
        "casual_text": "We need to understand x_i. So, we suggest replacing 3. This goal is a bit different from the minimum risk training method used by Li and Eisner (2009), and it's a difference that matters. In both approaches, * aims to minimize risk or expected loss. However, the expectation is calculated with respect to different distributions: in Li and Eisner (2009), it's based on the conditional distribution p(y | x), whereas in our case (1), it's based on the joint distribution p(x, y).",
        "formal_text": "Convert casual text to formal text: We need to understand x_i. So, we suggest replacing 3. This goal is a bit different from the minimum risk training method used by Li and Eisner (2009), and it's"
    },
    {
        "casual_text": "That’s how things are, and people only think differently because they’re hoping for something better or using really simple ways of thinking. Testing and disproving ideas in linguistics, or any other science, isn’t as straightforward as high school makes it seem. Still, just because it’s tricky to disprove something doesn’t mean it’s impossible to test or that it becomes like math, where it’s all about proof.",
        "formal_text": "Convert casual text to formal text: That’s how things are, and people only think differently because they’re hoping for something better or using really simple ways of thinking. Testing and disproving ideas in linguistics, or any other"
    },
    {
        "casual_text": "3) Our method works better than the current best ones, according to tests on a standard dataset. To help you understand how our model works, we also show how tree-based position features affect relation classification.",
        "formal_text": "Convert casual text to formal text: 3) Our method works better than the current best ones, according to tests on a standard dataset. To help you understand how our model works, we also show how tree-based position features affect relation"
    },
    {
        "casual_text": "Using sampling for decoding shows that our models know way more words than when we use beam search. Our models learn way over 212 words, which is way more than the ResDAVEnet-VQ model, which only learns around 279 words (check out Table 6 and Harwath et al., 2020 for the details). We think this happens because training a model to create spoken captions makes it learn a lot more words compared to just training it to match captions to images. Plus, beam search tries to pick the most likely caption, so it sticks to a smaller set of words and doesn’t show off all the words the model knows.",
        "formal_text": "Convert casual text to formal text: Using sampling for decoding shows that our models know way more words than when we use beam search. Our models learn way over 212 words, which is way more than the ResDAVEnet"
    },
    {
        "casual_text": "We use a Transition-based dependency parser, which is based on Nivre's work from 2008. This parser has two main parts: a transition mechanism that turns a phrase into a dependency tree, and a machine learning classifier that figures out the next step for each structure it processes. With these two parts, dependency parsing becomes a straightforward search through the transition system, guided by the classifier. The parser works with three things: a stack for partially processed words, an input buffer for the remaining words, and a set of arcs that show the partially built dependency tree. This system has four possible transitions, but it can only handle projective dependency trees.",
        "formal_text": "Convert casual text to formal text: We use a Transition-based dependency parser, which is based on Nivre's work from 2008. This parser has two main parts: a transition mechanism that turns a"
    },
    {
        "casual_text": "Aujourd'hui, il pleuvait à verse. Jean a jeté un coup d'il par la fenêtre. Marie n'était pas là. Il a enfilé son imperméable et est sorti.",
        "formal_text": "Convert casual text to formal text: Aujourd'hui, il pleuvait à verse. Jean a jeté un coup d'il par la fenêtre. Marie n'était pas là. Il a"
    },
    {
        "casual_text": "U has to be greater than or equal to zero. The derivative of L(U) with respect to U, which we can write as U L(U), is equal to: -2XVHT + 2UHVT V HT + 21UUT U - 21U + 2C_u U - 2C_u U0 + 2D_u U - 2W_u U + .",
        "formal_text": "Convert casual text to formal text: U has to be greater than or equal to zero. The derivative of L(U) with respect to U, which we can write as U L(U), is equal to: -2"
    },
    {
        "casual_text": "We're sharing the macro-averaged F1 scores for pronoun translation, which we got using a simplified version of AutoPRF (a method by Hardmeier and Federico from 2010). Basically, for each sentence in our test set, we look at the pronouns in the system's translation and compare them to the ones in the reference translation. We adjust the counts based on what's in the reference, and then use those numbers to calculate precision, recall, and F1 scores.",
        "formal_text": "Convert casual text to formal text: We're sharing the macro-averaged F1 scores for pronoun translation, which we got using a simplified version of AutoPRF (a method by Hardmeier and Federico from"
    },
    {
        "casual_text": "• Users. This tab lets the project manager create accounts for people and assign them to specific tasks. Each person will only see the tasks they’ve been assigned to. Users can’t see other people’s annotations unless they’re in \"revision mode,\" which shows an existing annotation for them to review.",
        "formal_text": "Convert casual text to formal text: • Users. This tab lets the project manager create accounts for people and assign them to specific tasks. Each person will only see the tasks they’ve been assigned to. Users can’t see other people"
    },
    {
        "casual_text": "To start recording an opposing speech, the debaters first had ten minutes to look over the background info for the motion, just like when they recorded a supporting speech. After that, they listened to a supporting speech made by another debater and then recorded their own counter speech, keeping it about the same length. Depending on the debate style they were used to, some debaters made clear, direct counter speeches, while others went for more subtle, implied ones. To speed things up towards the end, a few opposing speeches were recorded without making the debater respond to a specific supporting speech. Instead, they were told to come up with their own supporting arguments in their head and then respond to those. In the end, they recorded a total of 1887 opposing speeches: 348 were explicit counters, 1389 were implicit, and 150 didn’t directly counter any supporting speech. The full instructions the debaters followed are in the Appendix.",
        "formal_text": "Convert casual text to formal text: To start recording an opposing speech, the debaters first had ten minutes to look over the background info for the motion, just like when they recorded a supporting speech. After that, they"
    },
    {
        "casual_text": "Figuring out how well machines can understand human language has been a big deal in artificial intelligence for a long time. One way to test this is through machine reading comprehension (MRC), where a machine reads and tries to understand text that isn’t organized in a specific way. This seems like a pretty good way to evaluate natural language understanding (NLU) since it’s pretty broad and flexible (Chen, 2018). Lately, there have been a bunch of huge datasets created for this, and some deep learning systems have even managed to perform at a human level on certain ones.",
        "formal_text": "Figuring out how well machines can understand human language has been a big deal in artificial intelligence for a long time. One way to test this is through machine reading comprehension (MRC), where a machine reads and tries to understand"
    },
    {
        "casual_text": "The second thing we tried was removing our segment pooling method. Instead, we used the approach suggested by Zhang et al. (2020). They used label attention and made individual predictions for each segment, then combined those predictions using max-pooling. As you can see in row (b), switching to their method made things worse. This shows that our segment pooling is better at putting all the segment info together.",
        "formal_text": "Convert casual text to formal text: The second thing we tried was removing our segment pooling method. Instead, we used the approach suggested by Zhang et al. (2020). They used label attention and made individual predictions for each"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. We're talking about a word in a specific position (let's call it \"\") that has the letters \"xa t(x) y ctype(a, y, t) ya t(y)\". The part-of-speech code for this word can be one of these: N (nothing), U (uppercase), L (lowercase), D (digit), S (symbol). Now, we're looking at the letter \"y\" in this word. Its position within the word can be N (nothing), H (at the start), T (at the end), I (somewhere in the middle). Finally, \"char(a, y, t)\" just refers to the character \"y\" in the word at position \"t\".",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in simpler terms. We're talking about a word in a specific position (let's call it \"\") that has the letters \"x"
    },
    {
        "casual_text": "Let's say the degree of an extended label is just how many CCG categories are in it. Take our earlier example: the phrase \"sugar in your\" has an extended CCG label with a degree of three, while \"cream and\" has a degree of two. Now, for the whole system that uses these extended CCG labels, its degree is the highest degree found in any of the labels in the model.",
        "formal_text": "Convert casual text to formal text: Let's say the degree of an extended label is just how many CCG categories are in it. Take our earlier example: the phrase \"sugar in your\" has an extended CCG label"
    },
    {
        "casual_text": "We think this \"bridge language\" method will become more popular over time, especially as languages get less well-known and have fewer speakers. But the bridge language idea will work best for languages that are widely spoken around the world. For example, Arabic can act as a bridge language in the Middle East, North Africa, and many countries with lots of Muslims. French, Spanish, and Portuguese can do the same in their old colonies. Chinese can be used across a big chunk of Southeast Asia, and Russian can work in the countries that used to be part of the Soviet Union.",
        "formal_text": "Convert casual text to formal text: We think this \"bridge language\" method will become more popular over time, especially as languages get less well-known and have fewer speakers. But the bridge language idea will work best for languages that are"
    },
    {
        "casual_text": "For ACSA, we make templates by hand. These templates have one spot for the category and another for the sentiment type (like positive or negative). We have a set of category words, A = a1, ..., a|C|, where |C| is the number of category types (for example, a1 could be \"price\"). We also have a set of polarity words, P = p1, ..., p|L|, where |L| is the number of polarity types (like p1 being \"positive\"). We use these words to create templates like \"The sentiment polarity of price is positive\". The basic template is \"The sentiment polarity of ai is pk\". So, for any category ai, we can make a bunch of these templates.",
        "formal_text": "Convert casual text to formal text: For ACSA, we make templates by hand. These templates have one spot for the category and another for the sentiment type (like positive or negative). We have a set of category words, A ="
    },
    {
        "casual_text": "Okay, let me break this down in simpler terms: x, y, and s IRF( ) are just the weight matrix, bias vector, and squashing function for the IRF layer at specific time steps x and y. Meanwhile, g 0x, y ( ) =  is just saying that  is equal to itself. Now, w IRF(1) and b IRF(1) are the initial weight and bias vectors that are applied globally for the first layer of the IRF. These vectors are used to transform the scalar , and each of them is adjusted based on its own random effects. W just refers to the whole thing.",
        "formal_text": "Convert casual text to formal text: Okay, let me break this down in simpler terms: x, y, and s IRF( ) are just the weight matrix, bias vector, and squashing function for the I"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We're trying to define a relationship (we'll call it ) between two types of sequences: one made up of DATR descriptors (let's call them DESC*) and the other made up of atoms, which are basically values (let's call them ATOM*). When we write   a, it means that the sequence of descriptors  evaluates to the sequence of atoms a. Based on the DATR/ theory we're working with, we'd expect things like: - Dog: (cat)  noun, which means \"Dog: (cat)\" evaluates to \"noun.\" - Dog: (root) Noun: (surf)  dog s, which means \"Dog: (root) Noun: (surf)\" evaluates to \"dog s.\" And there are probably other examples like these.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. We're trying to define a relationship (we'll call it ) between two types of sequences: one"
    },
    {
        "casual_text": "Word form: i, j, ds(i), ds(j), h(j/i), h(h(j/i)) Lemma (if available): i, j, ds(i), ds(j), h(j/i), h(h(j/i))",
        "formal_text": "Convert casual text to formal text: Word form: i, j, ds(i), ds(j), h(j/i), h(h(j/i)) Lemma"
    },
    {
        "casual_text": "For future projects, we can use our task to see how different ways of asking questions affect learning. By combining Multi-Question Generation with reading comprehension exercises, we can see how students respond to questions that are rephrased. This can really help test how well students understand the material and if they can apply what they’ve learned in different contexts. The pipeline we’ve made available can create multiple versions of the same questions, which could be a big help in making educational resources better and more varied on a larger scale.",
        "formal_text": "Convert casual text to formal text: For future projects, we can use our task to see how different ways of asking questions affect learning. By combining Multi-Question Generation with reading comprehension exercises, we can see how students respond to"
    },
    {
        "casual_text": "2. The car's going at 30 km/h. This can be written clearly and without any confusion in CPL-Lite: 3. Someone's driving a car.",
        "formal_text": "Convert casual text to formal text: 2. The car's going at 30 km/h. This can be written clearly and without any confusion in CPL-Lite: 3. Someone's driving a car. 4. Convert casual text"
    },
    {
        "casual_text": "In 2007, the Document Understanding Conference (DUC) challenge was all about answering 45 natural language questions by creating summaries from groups of 10 documents taken from the AQUAINT English news corpus (Graff, 2002). The reference summaries had to be between 230 and 250 words long. For testing, we worked with 30 topics, splitting them into 10 for training and 5 for validation under FSL.",
        "formal_text": "Convert casual text to formal text: In 2007, the Document Understanding Conference (DUC) challenge was all about answering 45 natural language questions by creating summaries from groups of 10 documents taken from the AQUAINT English news corpus ("
    },
    {
        "casual_text": "Plugging equation (3) into the simplified version of the Lagrangian gives us this dual problem.",
        "formal_text": "Convert casual text to formal text: Plugging equation (3) into the simplified version of the Lagrangian gives us this dual problem."
    },
    {
        "casual_text": "You can limit your search in different ways, like choosing a specific area to look in. And when you get the results, you can focus on certain parts of the information by picking specific fields from EURODICAUTOM.",
        "formal_text": "Convert casual text to formal text: You can limit your search in different ways, like choosing a specific area to look in. And when you get the results, you can focus on certain parts of the information by picking specific fields from EURO"
    },
    {
        "casual_text": "We also tried out some cutting-edge transformer-based seq2seq models, like Bert-SumExtAbs (Liu and Lapata, 2019) and BART (Lewis et al., 2019). Bert-SumExtAbs needs the encoder to be fine-tuned and the decoder to be trained from scratch, while BART just fine-tunes both the encoder and decoder. We only used AMI data for training and fine-tuning, and the results from these models are shown in the bottom two rows of Table 6. Even though our hier2hier t-learn model only needs the decoder to be fine-tuned and uses hierarchical attention, the more complex semi-supervised training of both the encoder and decoder in BART, along with its much larger size (100x), gives better performance. But, if you're working with limited memory, like on some mobile devices, our model might be a better choice. Also, even though Bert-SumExtAbs has a pre-trained encoder, training a big decoder from scratch with just a small AMI dataset led to overfitting, which resulted in lower scores.",
        "formal_text": "Convert casual text to formal text: We also tried out some cutting-edge transformer-based seq2seq models, like Bert-SumExtAbs (Liu and Lapata, 2019) and B"
    },
    {
        "casual_text": "Let’s say in the sentence \". . . apple juice. . . \", the unsupervised method tags \"apple\" as \"food. fruit\" and the supervised model tags it as \"food. generic\". Based on the hybrid approach, it would go with \"food. fruit\" as per the rule mentioned. But if the unsupervised method tags \"apple\" as \"org. company\", the hybrid approach would then pick \"food. generic\" because the two tags from the unsupervised and supervised methods don’t match up.",
        "formal_text": "Convert casual text to formal text: Let’s say in the sentence \". . . apple juice. . . \", the unsupervised method tags \"apple\" as \"food. fruit\" and the supervised"
    },
    {
        "casual_text": "MMI decoding uses a backward seq2seq model to rerank the responses, as described by Li et al. (2016a). They suggest setting the hyperparameter  to 0.5, so that's what we did. For each context, we sample 200 candidates to re-rank. For Diverse Sampling, we used the diverse beam search strategy from Vijayakumar et al. (2018). This method balances exploring and exploiting the search space. We set the number of groups to 5,  to 0.3, and used the Hamming diversity penalty function, just like in the paper.",
        "formal_text": "Convert casual text to formal text: MMI decoding uses a backward seq2seq model to rerank the responses, as described by Li et al. (2016a). They suggest setting the hyper"
    },
    {
        "casual_text": "The third example is about DeReKo data. Let's say it's the 2019 edition of the Mannheimer Spezielle Zeitung. It was added to the collection, but then someone filed a legal complaint. Since this newspaper is super popular in Germany and is a favorite among linguists for studying word usage, we made a new version of it that’s as close to the original as we could get. This should help keep things reproducible while staying within the law. Just to clarify, this new version isn’t considered an OAIS version because it wasn’t created through a migration process. But we think the regular, everyday meaning of \"version\" fits what we’re doing here.",
        "formal_text": "Convert casual text to formal text: The third example is about DeReKo data. Let's say it's the 2019 edition of the Mannheimer Spezielle Zeitung. It was added to the collection, but then someone"
    },
    {
        "casual_text": "At every step t during response generation, the decoder starts by calculating a self-attention h r t based on the words it has already produced, y 1: t1.",
        "formal_text": "Convert casual text to formal text: At every step t during response generation, the decoder starts by calculating a self-attention h r t based on the words it has already produced, y 1"
    },
    {
        "casual_text": "We trained our classifiers using three different methods: MLE, L1, and PosCal. MLE is just a standard maximum likelihood estimation approach where we minimize cross-entropy loss. L1 is similar but adds an L1 regularizer to the mix. PosCal is the method we came up with, where we minimize L_PosCal (check out Eq 1 for details). For PosCal, we use Kullback-Leibler divergence to calculate L_cal. We also looked at ECE with temperature scaling (tScal), which is basically the best post-calibration method out there right now, according to Guo et al. (2017).",
        "formal_text": "Convert casual text to formal text: We trained our classifiers using three different methods: MLE, L1, and PosCal. MLE is just a standard maximum likelihood estimation approach where we minimize cross-entropy loss."
    },
    {
        "casual_text": "The questions and scoring guidelines for the Amazon Products dataset, which is a multiclass classification problem, are a bit different. Check out Appendix B for more info.",
        "formal_text": "Convert casual text to formal text: The questions and scoring guidelines for the Amazon Products dataset, which is a multiclass classification problem are a bit different. Check out Appendix B for more info."
    },
    {
        "casual_text": "Basically, we used three different methods (MUSE, VecMap, and XLM-R) to train cross-lingual models. For MUSE and VecMap, we got the cross-lingual mapping from monolingual embeddings, like we mentioned earlier. This gave us thirteen models for each of those two methods. On the other hand, we only made one model using XLM-R for the third approach, which we’ve already explained. We take the last layer of the XLM-R model to create representations for each token.",
        "formal_text": "Convert casual text to formal text: Basically, we used three different methods (MUSE, VecMap, and XLM-R) to train cross-lingual models. For MUSE and VecMap, we got the"
    },
    {
        "casual_text": "Step 3 is all about checking if the marker from the prediction tree matches up with the elementary tree for \"open.\" To make parsing PLTAG easier and more efficient, Demberg and his team (2013) came up with the idea of \"fringes.\" Fringes help us understand that when building a tree step by step, you can only connect a prefix tree to an elementary tree at certain spots. For example, in Figure 3, the prefix tree has two places where you can make a substitution, one for B and one for C. But if you try to substitute into B, you get a valid new prefix tree. If you substitute into C, you end up with the tree shown in Figure 3b, which isn't a valid prefix tree—it’s like trying to build the tree in a way that doesn’t work step by step.",
        "formal_text": "Convert casual text to formal text: Step 3 is all about checking if the marker from the prediction tree matches up with the elementary tree for \"open.\" To make parsing PLTAG easier and more efficient, Demberg and his"
    },
    {
        "casual_text": "To figure out the ECE scores, we gotta break the model's predictions into M bins based on the confidence levels. Then, we calculate the weighted average of the difference between confidence and accuracy for each bin, using the number of samples in each bin as the weight. That gives us the ECE scores.",
        "formal_text": "Convert casual text to formal text: To figure out the ECE scores, we gotta break the model's predictions into M bins based on the confidence levels. Then, we calculate the weighted average of the difference"
    },
    {
        "casual_text": "So, we're talking about a triplet (a, b, c), where a and b are in the same category, but c is in a different one. The function s is just the cosine similarity between these entities in some space. The set E l is a smaller group of entities from the bigger set E that have already been assigned categories, either by some initial process (S1) or by a person (S2). Basically, entities from different categories should be far enough apart in this space that their similarity is at least a certain margin M away from any pair of entities that are in the same category.",
        "formal_text": "Convert casual text to formal text: So, we're talking about a triplet (a, b, c), where a and b are in the same category, but c is in a different one"
    },
    {
        "casual_text": "So, we're using STL as our starting point, and the results for each task are in Tables 7 to 11. In each table, the first column has the labels sorted by how often they appear in the test set. The second column shows the average results from three runs for STL, which is our baseline. The other columns show how BERT and MTL models performed, with their differences from the STL baseline in parentheses (green means better, red means worse). Usually, BERT scores are way lower than STL, which makes sense since it wasn't fine-tuned for the task. MTL scores are also mostly lower than STL, suggesting that working on multiple tasks at once might be causing some interference. Here's a quick look at some numbers: - **LS**: 100.00, 100.00(-0.00), 100.00(-0.00), 100.00(-0.00), 100.00(-0.00), 100.00(-0.00) - **AFX**: 100.00, 100.00(-0.00), 100.00(-0.00), 100.00(-0.00), 100.00(-0.00), 100.00(-0.00) - **FRAG**: 83.33, 100.00(+16.67), 100.00(+16.67), 94.44(+11.11), 94.44(+11.11), 88.89(+5.56), 100.00(+16.67) - **SBAR**: 100.00, 100.00(-0.00), 100.00(-0.00), 100.00(-0.00), 100.00(-0.00) Basically, the results show that while some models improved over the baseline, others didn't do as well, especially BERT and MTL, which is expected given how they were set up.",
        "formal_text": "Convert casual text to formal text: So, we're using STL as our starting point, and the results for each task are in Tables 7 to 11. In each table, the first column has the labels sorted by how often"
    },
    {
        "casual_text": "Another way to check if questions are shortcut-proof is by looking at how humans would answer them, especially when we tweak or remove some of their main parts. We think that if humans can still answer a question even after we take out the specific features we're testing, then those features probably aren’t that important for understanding the question. For example, if we replace pronouns with random nouns and people can still answer, maybe the question doesn’t really need you to figure out who the pronouns are referring to. Even if we can’t perfectly pinpoint which features are super important, if we can figure out some of them in a bunch of questions, we can still feel good that those questions are testing the skills we want them to test. Kind of like how Geirhos and his team (2020) say that a dataset is only helpful if it actually reflects the ability you’re trying to measure.",
        "formal_text": "Convert casual text to formal text: Another way to check if questions are shortcut-proof is by looking at how humans would answer them, especially when we tweak or remove some of their main parts. We think that if humans can still"
    },
    {
        "casual_text": "One way to deal with those extra length limits is to just not worry about them while training. Instead, we focus on making sure the model sticks to those limits when it's actually generating text. For length constraints, we can do this by tweaking how likely the model is to end a sentence. First, we need to make sure the model doesn't end the sentence too early, before it reaches the target length J. To do that, we can set the probability of ending the sentence to zero for all the positions before J. Then, we just adjust the other probabilities to make sure everything adds up to 100%.",
        "formal_text": "Convert casual text to formal text: One way to deal with those extra length limits is to just not worry about them while training. Instead, we focus on making sure the model sticks to those limits when it's actually generating text."
    },
    {
        "casual_text": "You can check out the prototype tool and the models over here: https://github.com/aakorolyova/DeSpin.",
        "formal_text": "Convert casual text to formal text: You can check out the prototype tool and the models over here: https://github.com/aakorolyova/DeSpin."
    },
    {
        "casual_text": "Hyp-Negation. This idea pops up when the hypothesis has a word like \"no\" in it. Basically, if this concept is around, it strongly suggests that an NLI model will predict the answer to be \"contradiction,\" no matter what else is going on in the NLI stuff (Gururangan et al., 2018).",
        "formal_text": "Convert casual text to formal text: Hyp-Negation. This idea pops up when the hypothesis has a word like \"no\" in it. Basically, if this concept is around, it strongly suggests"
    },
    {
        "casual_text": "Alright, so here's the deal: with the zero-one loss (01), the risk R(g) is calculated using  + F N F N +T P +   F P T N +F P. During training, we use something called \"sig,\" which is basically a smoother version of this formula that works better for backpropagation. In real-world training, we take the average of these losses and optimize them using methods like batched gradient descent or something similar.",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal: with the zero-one loss (01), the risk R(g) is calculated using  + F N F N +T P +"
    },
    {
        "casual_text": "We used the discourse parser from Wang et al. (2017) to train on our newly created MEGA-DT corpus, along with the Yelp13-DT and the original RST-DT. Here's how it went: - **Micro-averaged precision results:** - Original Parseval method (Par.) - RST Parseval (R-Par.) The inter-domain results were averaged over 10 runs, and for models with random parts, we averaged over 3 different runs. The best results for each section are in bold. *Some results are from Morey et al. 2017. These are statistically significant with a p-value  .05 compared to the best inter-domain baseline (adjusted with Bonferroni). -These are unpublished values. These combinations weren’t possible. We also tested how well the training data could help the parser work across different domains. For example, we trained it on Yelp reviews (MEGA-DT) and tested it on news articles (RST-DT). Then, we compared these results to the simpler intra-domain setup, where training and testing happen within the same domain.",
        "formal_text": "Convert casual text to formal text: We used the discourse parser from Wang et al. (2017) to train on our newly created MEGA-DT corpus, along with the Yelp13-DT and the original"
    },
    {
        "casual_text": "You can see that if we don’t use our inter-sentence relation graph (w/o ISRG), the F1 score takes a big hit, dropping by 3.75%. If we go a bit further and remove the in-passage edges (w/o IPE), the F1 score drops by 1.28%, showing how important it is to connect sentences that are close to each other. On the other hand, adding cross-passage edges to MGF (w/o CPE) gives a bigger boost to the F1 score, improving it by 2.07%. This is because cross-passage edges help model relationships between sentences in different passages, which makes it easier to spot argument pairs that work together. F-score (c). Check out Figure 4 for more details on how these graph settings affect things.",
        "formal_text": "Convert casual text to formal text: You can see that if we don’t use our inter-sentence relation graph (w/o ISRG), the F1 score takes a big hit, dropping by 3.75%."
    },
    {
        "casual_text": "Before adding any new rule to the system, we always check if it helps improve how well the grammar covers everything. Even though the final phonological system does a great job at handling loanwords from English to Japanese, it's still interesting to look at the mistakes it makes. These mistakes aren’t just about the system’s flaws—they also tell us something about how Japanese loanword phonology works in general.",
        "formal_text": "Convert casual text to formal text: Before adding any new rule to the system, we always check if it helps improve how well the grammar covers everything. Even though the final phonological system does a great job at handling loanword"
    },
    {
        "casual_text": "• If our system says \"true,\" it means entailment. • If it says \"false,\" that's a contradiction. • And if it doesn't say either, then it's neutral.",
        "formal_text": "Convert casual text to formal text: • If our system says \"true,\" it means entailment. • If it says \"false,\" that's a contradiction. • And if it doesn't say"
    },
    {
        "casual_text": "Keeping an eye on social media across a whole continent, like Europe, means dealing with a bunch of different languages. Twitter data is messy, and the way people use words can be way different from regular texts in the same language. Things get even trickier when you're trying to handle multiple languages at once.",
        "formal_text": "Convert casual text to formal text: Keeping an eye on social media across a whole continent, like Europe, means dealing with a bunch of different languages. Twitter data is messy, and the way people use words can be way different"
    },
    {
        "casual_text": "In Dyer et al. (2016), they only have one CLOSE action, but Choe and Charniak (2016) go a step further and add nonterminals to their CLOSE(X) actions. We're presenting the more flexible version here.",
        "formal_text": "Convert casual text to formal text: In Dyer et al. (2016), they only have one CLOSE action, but Choe and Charniak (2016) go a step further and add nonterminals to their CLOSE"
    },
    {
        "casual_text": "We looked at three different setups for our system. The first one, called iSRL, uses all semantic roles for each PLTAG lexicon entry, runs the PLTAG parser (IRPA), and uses both classifiers to handle identification and disambiguation, just like we explained in Section 4. The second setup, Majority-Baseline, skips the classifiers and deals with argument identification and role disambiguation in a different way. For identification, we used some heuristics based on Lang and Lapata's work (2014), which rely on gold syntactic dependency info from CoNLL input. For disambiguation, we just picked the most common role based on the gold standard dependency relation label for that specific argument. Keep in mind, these dependencies were made looking at the whole sentence, not step by step. We used MaltParser, a top-notch shift-reduce dependency parser, to get labeled syntactic dependencies, following Nivre et al. (2007). Based on Beuck et al. (2011), we tweaked the parser to give us intermediate output after each word by showing the current state of the dependency graph before each shift. We trained MaltParser with the arc-eager algorithm (which worked better than the other options available) on the CoNLL dataset, and it got an 89.66% labeled dependency accuracy on section 23.",
        "formal_text": "Convert casual text to formal text: We looked at three different setups for our system. The first one, called iSRL, uses all semantic roles for each PLTAG lexicon entry, runs the PLTAG par"
    },
    {
        "casual_text": "Okay, so here's a real-life example taken from a study by Mgrquez and Padr6 back in 1997. There are two taggers being compared: T1 and T2. T1 only uses bigram information, and it gets a performance score of 0.9135, which is like 96.86% accuracy overall when dealing with ambiguous words. T2, on the other hand, uses trigrams plus some automatically learned context rules, and it scores a bit better with an accuracy of 0.9282, or 97.39% overall. Both of these taggers were tested on a specific corpus (let's call it wsa) where the estimated error rate is about 0.03 for both T1 and T2. The average number of possible tags for each ambiguous word in this corpus is 2.5 tags per word. The error rate for the wsa corpus is calculated across all the words in it.",
        "formal_text": "Convert casual text to formal text: Okay, so here's a real-life example taken from a study by Mgrquez and Padr6 back in 1997. There are two taggers being compared:"
    },
    {
        "casual_text": "To check how well the model does with just a few examples, we randomly pick 5 different ways to split the training and development data and see how it performs on average across these splits. We train the vision-language models for 200 epochs in this few-shot setup and pick the best version based on how it does on the development set. For the NoCaps task, since there's no training data, we use the training data from COCO captioning instead. We test the model on the VQAv2 validation set, GQA test-dev, OK-VQA test set, the test set from Karpathy's split for Flickr30k captioning, and the NoCaps validation set. For VQA datasets and miniImageNet, we use accuracy as the measure, and for captioning, we use CIDEr and SPICE as our evaluation metrics.",
        "formal_text": "Convert casual text to formal text: To check how well the model does with just a few examples, we randomly pick 5 different ways to split the training and development data and see how it performs on average across these splits. We"
    },
    {
        "casual_text": "In Figure 1, you can see the probabilistic structure created by the grammar-based translation method, as explained earlier. Basically, each English word in the question gets turned into a structure that includes Chinese words along with their probabilities. These probabilities show how much importance the method assigns to each specific word. The same kind of structures are also generated by the other three translation methods.",
        "formal_text": "Convert casual text to formal text: In Figure 1, you can see the probabilistic structure created by the grammar-based translation method, as explained earlier. Basically, each English word in the question gets turned into a structure that includes Chinese"
    },
    {
        "casual_text": "For the agree/disagree/none relationships, we used an 11-point scale from -5 to 5 to rate them. A \"-5\" means strong disagreement, \"0\" means no relation, and \"5\" means a strong argumentative connection. Each turn in a pt, ct> pair also got a label: Sarcasm (S) or Non-Sarcasm (N S). Table 2 has all the stats for argumentative relations (A/D/N) and sarcasm (S/N S). We divided the dataset into training (80%; 7,982 turn pairs), test (10%; 999 turn pairs), and dev (10%; 999 turn pairs) sets. Each set has the same proportion of instances. For example, 80% of the 315 sarcastic turns (S) with an agree label (A) are in the training set. The dev set is for tweaking parameters.",
        "formal_text": "Convert casual text to formal text: For the agree/disagree/none relationships, we used an 11-point scale from -5 to 5 to rate them. A \"-5\" means strong disagreement, \"0\""
    },
    {
        "casual_text": "A similar approach to what we're doing here can be found in the work by Zhao et al. (2004). They used each source sentence to create a query and pulled similar sentences from a bigger corpus. Then, they trained a specific language model (LM) and blended it with a general LM to translate the original sentence. In another study by Lü et al. (2007), they used each sentence to find similar data in the same corpus using TF-IDF, and then prepared specific LMs and translation models (TMs) to be mixed together. Yamamoto and Sumita (2007) took a different route: they grouped bilingual corpora into clusters to reduce entropy in each group. They trained separate LMs and TMs from these smaller groups and combined them during translation based on domain prediction. In contrast, our method is a bit different. We get the final combination of target LMs by reusing the weights we estimated by maximizing the probability of generating the source sentence. This is done through linear interpolation of source sub-LMs.",
        "formal_text": "Convert casual text to formal text: A similar approach to what we're doing here can be found in the work by Zhao et al. (2004). They used each source sentence to create a query and pulled similar sentences from"
    },
    {
        "casual_text": "The main idea behind their model is that a summary's value comes from adding up the importance of all the unique ideas it has. This automatically takes care of avoiding repetition at a smaller, word-by-word level: a summary gets the most value by including each idea just one time.",
        "formal_text": "Convert casual text to formal text: The main idea behind their model is that a summary's value comes from adding up the importance of all the unique ideas it has. This automatically takes care of avoiding repetition at a smaller,"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Let's say we have a cost function set up so that c(y, y) represents the cost of predicting y when the actual output is y. Our main aim is to figure out  (some parameters) that will help us make predictions with low expected cost, especially on data we haven't seen before. To make this happen, we typically train linear models by working on a problem that looks something like this:",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Let's say we have a cost function set up so that c(y, y) represents the cost of predicting y"
    },
    {
        "casual_text": "To figure out if PLMs actually have generalizable metaphorical knowledge, we test them in situations where the test data and training data come from different sources. We look at how well they can transfer knowledge between languages and across different datasets as two main ways to check this. We'll explain each of these in the next sections.",
        "formal_text": "Convert casual text to formal text: To figure out if PLMs actually have generalizable metaphorical knowledge, we test them in situations where the test data and training data come from different sources. We look at how well they"
    },
    {
        "casual_text": "There's been a lot of buzz around crosslingual transfer learning for SRL lately. A bunch of researchers have been diving into this topic, like Padó and Lapata (2009), van der Plas et al. (2011), Kozhevnikov and Titov (2013), Tiedemann (2015), Zhao et al. (2018), Chen et al. (2019), Aminian et al. (2019), and Fei et al. (2020). Most of the work so far has been split between two main approaches: annotation projection and model transfer.",
        "formal_text": "Convert casual text to formal text: There's been a lot of buzz around crosslingual transfer learning for SRL lately. A bunch of researchers have been diving into this topic, like Padó and Lapata (2009), van der"
    },
    {
        "casual_text": "There are different ways to answer questions: 1. You can pick an answer straight from the text (this is called answer extraction). An example of this is NewsQA (Trischler et al., 2017). 2. You can choose from a list of possible answers (multiple choice). MCTest (Richardson et al., 2013) is an example of this. 3. You can write a free-form answer (like a description). NarrativeQA (Koisk et al., 2018) is an example of this. Some datasets also let you answer with just a simple yes or no (like BoolQ).",
        "formal_text": "Convert casual text to formal text: There are different ways to answer questions: 1. You can pick an answer straight from the text (this is called answer extraction). An example of this is NewsQA (Trischler et al."
    },
    {
        "casual_text": "To tackle these problems, we suggest adding some adjustable weights to our loss function. These weights will be updated using gradient descent, just like the model's parameters. We'll assign a weight ( w_i ) for each classification loss part and ( w_m,t ) for the distillation loss part where exit ( m ) learns from exit ( t ). This way, our overall loss function will look like",
        "formal_text": "Convert casual text to formal text: To tackle these problems, we suggest adding some adjustable weights to our loss function. These weights will be updated using gradient descent, just like the model's parameters. We'll assign a"
    },
    {
        "casual_text": "The table also shows the results for the LDA model. This model lowers perplexity when using an additive composition function, but it doesn't do as well as the n-gram model with a multiplicative function. To give you a better idea, Figure 1 shows how the perplexity changes for both the combined LDA and n-gram models as the number of topics increases. More topics usually mean higher-dimensional representations, which should be more detailed and better at making predictions. This holds true for the additive model, but interestingly, the multiplicative model's perplexity actually goes up with more topics, suggesting it becomes less predictive.",
        "formal_text": "Convert casual text to formal text: The table also shows the results for the LDA model. This model lowers perplexity when using an additive composition function, but it doesn't do as well as the n-gram model"
    },
    {
        "casual_text": "A good guess might be that one of the points made in an argument is actually the main idea we're supposed to take away. So, our first step is to figure out which of those points is most likely to be the main idea. Check out Figure 2: It shows how often the main idea matches one of the points in the argument, based on either how much they overlap in words (solid lines) or how similar they are in meaning (dashed lines).",
        "formal_text": "Convert casual text to formal text: A good guess might be that one of the points made in an argument is actually the main idea we're supposed to take away. So, our first step is to figure out which of those points is"
    },
    {
        "casual_text": "In this part, we run a bunch of tests to check how well our framework works using data from an actual online education platform. We're trying to figure out two things: (1) Does our model do better than other common models? and (2) How much does each part of our framework help improve the results?",
        "formal_text": "Convert casual text to formal text: In this part, we run a bunch of tests to check how well our framework works using data from an actual online education platform. We're trying to figure out two things: (1) Does our model"
    },
    {
        "casual_text": "Here,  represents the threshold for how similar the generated question in a chunk is to the sub-questions in PHQ-9. Meanwhile, I[] is just a function that gives a 0 or 1 depending on whether a condition  is met (you can find the scores in Table 3).",
        "formal_text": "Convert casual text to formal text: Here,  represents the threshold for how similar the generated question in a chunk is to the sub-questions in PHQ-9. Meanwhile, I[] is just a function that"
    },
    {
        "casual_text": "The rank of (A + ) won't be more than d_k. If you've got a bunch of vectors in a linear space, any combination of those vectors should still be in the same space. So, the rows we made up for A are part of LN(T). That means there's an  that proves the whole thing about A being unidentifiable.",
        "formal_text": "Convert casual text to formal text: The rank of (A + ) won't be more than d_k. If you've got a bunch of vectors in a linear space, any combination of those"
    },
    {
        "casual_text": "Alright, so here's the deal with error analysis. The main issues come from not properly adding the right information to the system. Check out Figure 6, which shows two examples of how our system handles things. In Figure 6a, the system managed to make a connection because the link between \"spouse\" and \"husband\" was pretty strong (score of 0.747). That's why it added the \"HUSBAND\" function during the knowledge injection. But in Figure 6b, it didn't add the \"WIN\" function because the connection between \"award\" and \"win\" wasn't strong enough (score of 0.336). Also, even though we made the original model checking program faster, some tests still took way too long to finish. Table 6 shows the average and longest times (in seconds) for checking the model with and without the optimization.",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal with error analysis. The main issues come from not properly adding the right information to the system. Check out Figure 6, which shows two examples of how our system handles"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. We're dealing with a random variable called Z, which represents a binary protected attribute—like race, for example. Z can take two values, which we'll call z and z (just different versions of the same attribute). Y and  are variables that represent the correct class (what the actual outcome should be) and the predicted class (what the model thinks it is), respectively. Now, let's talk about the experiment. We're starting with a controlled setup where we can adjust the proportion of protected attributes (like race) within each main-task class (like sentiment). We're following the work of Elazar and Goldberg (2018), who used a Twitter dataset collected by Blodgett et al. in 2016. In this dataset, each tweet is tagged with information about the user's race and their sentiment, which was determined based on the emojis they used. The thing is, if there's a strong connection between the protected attribute (like race) and the main class (like sentiment), it might make the model unfair because it could start relying too much on the protected attribute to make predictions. To test this, we're looking at how well the model predicts sentiment for different race groups—specifically, African American English (AAE) speakers and Standard American English (SAE) speakers. We're doing this under different conditions where the data might be imbalanced, and we're also testing with and without a special \"classifier debiasing\" procedure we have. We're measuring something called TPR-GAP to see how fair the model is in these different setups.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in simpler terms. We're dealing with a random variable called Z, which represents a binary protected attribute—like race, for example. Z can"
    },
    {
        "casual_text": "We're hoping our comparison of different methods will give you some fresh ideas about their pros and cons. Plus, we’ve set up some standardized experiments to make it easier for anyone to evaluate unsupervised constituency parsing in the future. You can check out our pre/post-processing and evaluation code on GitHub at https://github.com/i-lijun/UnsupConstParseEval.",
        "formal_text": "Convert casual text to formal text: We're hoping our comparison of different methods will give you some fresh ideas about their pros and cons. Plus, we’ve set up some standardized experiments to make it easier for anyone evaluate unsupervised"
    },
    {
        "casual_text": "Since CDRNN is made for scientific modeling, the main thing we care about is the IRF itself and what it can tell us about how our brains work, not how well it does on some task (like predicting how fast people read or fMRI results, which aren’t really the focus here). But, being able to predict stuff can help us trust the IRF estimates. So, as a quick check, this section looks at how well it predicts human data compared to other methods. The results might look like those \"bake-off\" competitions in machine learning (and yeah, CDRNN does beat all the other models), but the real goal here is to make sure the CDRNN estimates are reliable because they actually describe what we’re interested in and work well on new, unseen data. The baseline models, including CDR, are included in the results. While CDR kinda struggles compared to GAMs on the Dundee dataset, CDRNN has narrowed the gap. This is interesting because Shain and Schuler (2021) thought CDR’s weaker performance on Dundee might be partly due to not handling non-linear effects, which GAMs can do but CDR can’t. CDRNN’s performance seems to back this up: once it can deal with those non-linearities, it beats the GAMs.",
        "formal_text": "Convert casual text to formal text: Since CDRNN is made for scientific modeling, the main thing we care about is the IRF itself and what it can tell us about how our brains work, not how well it does on some"
    },
    {
        "casual_text": "Neural methods use neural networks to turn natural language (NL) sentences into meaning representations (MRs) without relying on traditional grammar rules. Instead, they treat semantic parsing like a machine translation task, where NL is the input language and the formal language of MRs is what we're aiming for.",
        "formal_text": "Convert casual text to formal text: Neural methods use neural networks to turn natural language (NL) sentences into meaning representations (MRs) without relying on traditional grammar rules. Instead, they treat semantic parsing like"
    },
    {
        "casual_text": "After figuring out the causal graph (Section 6.2), we move on to calculate the ACE of word type affecting both semantic change and frequency shift, using do-calculus (Section 6.3).",
        "formal_text": "Convert casual text to formal text: After figuring out the causal graph (Section 6.2), we move on to calculate the ACE of word type affecting both semantic change and frequency shift, using do-calculus ("
    },
    {
        "casual_text": "To really understand how well the (He et al. , 2010) recommendation system works, we need to do some user studies to see if the systems built using automatic evaluation metrics match up with what real people think. Our experiments back up the idea of using an automatic evaluation metric (TER) to estimate post-editing effort in the translation recommendation model from (He et al. 2010). The model scores over 90% precision and more than 75% recall when compared to the opinions of professional human post-editors.",
        "formal_text": "Convert casual text to formal text: To really understand how well the (He et al. , 2010) recommendation system works, we need to do some user studies to see if the systems built using automatic evaluation metrics match up"
    },
    {
        "casual_text": "Here,  represents a fixed version of . This training method doesn't add any extra parameters to the model. The only extra cost for supervised learning is running a training pass and a generation pass for each unlabeled sentence. As the main goal, we train using a combination of the supervised loss from Equation 1 and the unsupervised loss from Equation 3, with some weights applied to balance them.",
        "formal_text": "Convert casual text to formal text: Here,  represents a fixed version of . This training method doesn't add any extra parameters to the model. The only extra cost for supervised learning is running a training pass"
    },
    {
        "casual_text": "NMT is kind of the big thing right now, but not much has been done specifically for translating between Bengali and English. Dandapat and Lewis (2018) worked on creating a general-purpose NMT model for Bengali-English using sentences from comparable corpora. They dealt with the lack of training examples by using data augmentation and back-translation (Sennrich et al., 2016). Hasan et al. (2019) and Mumin et al. (2019a) also showed that even with limited parallel data available online, NMT improved translations for the Bengali-English pair.",
        "formal_text": "Convert casual text to formal text: NMT is kind of the big thing right now, but not much has been done specifically for translating between Bengali and English. Dandapat and Lewis (2018) worked on creating a general-purpose"
    },
    {
        "casual_text": "Dealing with complex human language, like soccer commentaries, is super tough at every level. It gets even trickier when you zoom in—that’s where the data is super sparse, and tiny mistakes are harder to smooth over. Our work is going to kickstart some fresh research into this tricky but awesome challenge.",
        "formal_text": "Convert casual text to formal text: Dealing with complex human language, like soccer commentaries, is super tough at every level. It gets even trickier when you zoom in—that’s where the data is super sparse,"
    },
    {
        "casual_text": "We tried out two different ways to line up all the books and novels. The first one was Bleualign, which was created by Sennrich and Volk back in 2010. It's used for matching sentences in parallel texts, like those used to train machine translation models. Bleualign works by comparing the modified BLEU score between sentences in the original language and those translated into the target language. This method of matching sentences within the same language works perfectly for what we needed, especially when dealing with simplified language. So, in those cases, we didn't have to bother with the translation part. The other tool we used was CATS-Align. This one was made specifically for aligning simplification datasets, with Newsela being a prime example. It has a bunch of options for how to measure similarity, what level to align at, and other settings. We went with the character trigram similarity strategy, which uses log TF-IDF weighting and compares vectors using cosine similarity. We also chose to align at the paragraph level and used the closest similarity alignment strategy.",
        "formal_text": "Convert casual text to formal text: We tried out two different ways to line up all the books and novels. The first one was Bleualign, which was created by Sennrich and Volk back in 2010. It's used for matching"
    },
    {
        "casual_text": "Since samples from the policy  p  usually give low or even zero rewards, the estimator (5) is famous for having a really high variance. A lot of research has been done on trying to reduce this variance by using things like baselines or control-derivative methods (Rennie et al., 2016).",
        "formal_text": "Convert casual text to formal text: Since samples from the policy  p  usually give low or even zero rewards, the estimator (5) is famous for having a really high variance. A lot of research has been"
    },
    {
        "casual_text": "But, the trace in COMP is figured out (using Bounding Theory or subjacency conditions, which limit how far apart the landing and extraction sites of movement can be) because \"who\" is in a COMP position and needs to connect with a variable.",
        "formal_text": "Convert casual text to formal text: But, the trace in COMP is figured out (using Bounding Theory or subjacency conditions, which limit how far apart the landing and extraction sites of movement can be). Convert casual"
    },
    {
        "casual_text": "For the second version of our model, we’ve added a new approach called the COUPLING heuristic. Instead of just boosting the importance of the main word when it’s very concrete, we focus on encouraging rules that could have created certain phrases based on the argument-predicate connections we pull from the visual data. Basically, we’re looking to reward phrases that include both an argument and a predicate, with the predicate being the key part. Take this caption, for example: *A girl is eating a slice of cheesecake.* If the image gives us semantic role labels like *agent: woman*, *activity: eat*, and *item: cake*, COUPLING steps in. It sets  c to 0 in Equation 2. To get those semantic role labels, we use a VLP model (from Zhou et al., 2020) that’s been fine-tuned on parts of MSCOCO and imSitu (from Yatskar et al., 2016). You can check out Figure 3 for more details. In Equation 2, s i represents the span created by rule r i, and V is the group of spans we’re rewarding. The process for picking these rewarded spans is explained below.",
        "formal_text": "Convert casual text to formal text: For the second version of our model, we’ve added a new approach called the COUPLING heuristic. Instead of just boosting the importance of the main word when it’"
    },
    {
        "casual_text": "Breaking down activities into smaller parts, we noticed that they can be divided into two groups to help us understand the issue better. The first group includes actions or steps that happened before the problem showed up. For example, in the sentence - \"We tried to upgrade the DB2 AESE ON RHEL from 10.5.5 to 11.1 and in the upgrade check, we got orphan rows,\" the part about \"trying to upgrade DB2\" isn't something done to fix the problem; it's actually what caused the error. The second group consists of actions taken to fix the problem (like in figure 4). We definitely don't want to leave out documents or procedures that mention the first group from our results. The tricky part is that both groups often use very similar language. To properly tell them apart, we might need to look at where the symptoms are mentioned in the text.",
        "formal_text": "Convert casual text to formal text: Breaking down activities into smaller parts, we noticed that they can be divided into two groups to help us understand the issue better. The first group includes actions or steps that happened before the problem showed up."
    },
    {
        "casual_text": "For phrase-level attacks, we wanted to see if tweaking a part of a sentence (the source subtree) could mess with the prediction for another part (the target subtree). Check out Figure 2 for an example. We tested two scenarios: one where the source and target subtrees are at least one word apart (k  1), and another where they just can't overlap (k  0). In the k  0 case, we found 1420 sentences in the test set that fit the bill. For k  1, there were 1340 valid examples we could use for these attacks. The test set has a total of 2416 sentences, by the way. Oh, and all the subtrees we looked at had between 4 and 12 words. For each source-target pair, we could change up to 3 words in the source subtree. Sometimes, just swapping one or two words was enough to create an adversarial example for some sentences.",
        "formal_text": "Convert casual text to formal text: For phrase-level attacks, we wanted to see if tweaking a part of a sentence (the source subtree) could mess with the prediction for another part (the target subtre"
    },
    {
        "casual_text": "The vector s, which is part of the set R S, holds overall, population-wide estimates for the parameters that describe how we predict things. In this study, we assume that these predictions follow a normal (bell-shaped) distribution. So, s includes the average prediction (, also known as the intercept) and the spread or variability of those predictions (2, which is the variance).",
        "formal_text": "Convert casual text to formal text: The vector s, which is part of the set R S, holds overall, population-wide estimates for the parameters that describe how we predict things. In this study, we assume that these predictions follow"
    },
    {
        "casual_text": "The experiments show that older methods don't work as well on microblog posts compared to longer texts. Both CEMEL and GMEL do a lot better than the basic methods, which means using extra posts can really help improve how well an entity linking system works on microblogs. Plus, GMEL works way better than CEMEL.",
        "formal_text": "Convert casual text to formal text: The experiments show that older methods don't work as well on microblog posts compared to longer texts. Both CEMEL and GMEL do a lot better than the basic methods, which"
    },
    {
        "casual_text": "• Baseline: The training data stays in its original order, and during decoding, it lets you make small changes (like moving up to 5 words around within a 5-word window), as explained in the paper.",
        "formal_text": "Convert casual text to formal text: • Baseline: The training data stays in its original order, and during decoding it lets you make small changes (like moving up to 5 words around within a 5-word window), as explained in"
    },
    {
        "casual_text": "First off, we took a look at how well 13 top-notch open-source biomedical word embeddings, which Schulz and Juric (2020) tested on some existing datasets, perform on our dataset. The datasets we’re talking about are PMC, PM, PP, and PPW from Pyysalo et al. (2013), ASQ from Kosmopoulos et al. (2016) and Zhang et al. (2019), and MIM(IC) along with its M(odel) version. We skipped the sentence embeddings Schulz and Juric tested because they didn’t do so well on the existing datasets. In Table 5, you can see how each embedding did on EHR-RelB, measured by Spearman's correlation. We only used a subset of 3350 out of the total 3630 concept pairs that all embeddings could handle. For each embedding, we used two ways to measure similarity: fuzzy Jaccard similarity (Zhelezniak et al., 2019) and the standard average cosine. Turns out, fuzzy Jaccard similarity gave better results across the board. LTL30 came out on top with a correlation of 0.49. It beat all but two embeddings—it couldn’t top ASQ or extr. Schulz and Juric (2020) pointed out that most existing datasets are too small to really tell the difference between embeddings. But our results show that EHR-RelB is a pretty solid new benchmark that’s big enough to spot those differences.",
        "formal_text": "Convert casual text to formal text: First off, we took a look at how well 13 top-notch open-source biomedical word embeddings, which Schulz and Juric (2020) tested on some existing dataset"
    },
    {
        "casual_text": "• Coreference for base noun phrases: This is the trickiest and most confusing type of coreference in the MUC coreference task. Even though it’s a big part of the task, it’s really hard to come up with rules to handle it. In our system, we only use one simple rule: If the anaphor and the potential antecedent match exactly in wording and both have at least two words (not counting determiners), then they’re considered coreferential throughout the whole document.",
        "formal_text": "Convert casual text to formal text: • Coreference for base noun phrases: This is the trickiest and most confusing type of coreference in the MUC coreference task. Even though it’s a big part of the"
    },
    {
        "casual_text": "In this project, we're using a sequence-learning method for captioning sketchy scenes, which you can see in Figure 1. Our approach combines two main parts: the Sketchy Scene Encoder, which takes a sketchy scene and turns it into a high-level, unique visual representation, and the Sketchy Scene Decoder with Spatial Visual Attention, which helps in picking up more details from the sketch while creating the description. This is a fresh take on generating detailed descriptions for sketchy scenes using sequence learning.",
        "formal_text": "Convert casual text to formal text: In this project, we're using a sequence-learning method for captioning sketchy scenes, which you can see in Figure 1. Our approach combines two main parts: the Sketchy Scene En"
    },
    {
        "casual_text": "We use precision (P), recall (R), and the F-measure (F) to check how well things are working. Table 4 shows that just looking at how similar strings are doesn't do a great job of finding similar ideas between cells. The F-measure here is 55.50%. Table 5 adds more semantic stuff, like categories of named entities, but it doesn't really help much. The performance only gets a tiny boost. The main issue is that table descriptions often leave out key words like \"pm/am,\" \"$,\" or \"%,\" which are important for things like dates, times, money, and percentages. Table 6, however, shows that using number categories boosts the F-measure to 86.50%, which is a big improvement over Tables 4 and 5.",
        "formal_text": "Convert casual text to formal text: We use precision (P), recall (R), and the F-measure (F) to check how well things are working. Table 4 shows that just looking at how similar strings are doesn't do"
    },
    {
        "casual_text": "In the second experiment, we looked at multilingual word translation across six European languages: English, German, French, Spanish, Italian, and Portuguese (Lample et al., 2018). We compared our MPPA method to MAT+MPSR (Chen and Cardie, 2018). Since MAT+MPSR is an unsupervised approach, we swapped out the MPSR part with our MPPA algorithm to make it MAT+MPPA for a fair comparison. We ran 5 refinement epochs after the MAT step, which is the default setting in the MAT+MPSR source code. The MPPA training phase is about 10 times faster than the MPSR equivalent, though both methods have hyperparameters that need tuning. We also tested UMH (Alaux et al., 2019) on this benchmark. The precision@1 results are in Table 2. MPPA performed similarly to UMH, and MPSR did slightly better. One thing to note is that the MPSR mapping matrices weren’t exactly orthogonal. They had a smaller mean-square error (MSE) on the training data compared to our method, which was restricted to be orthogonal. This suggests that the orthogonality constraint, especially when combined with transitivity constraints in multilingual settings, might be too strict.",
        "formal_text": "Convert casual text to formal text: In the second experiment, we looked at multilingual word translation across six European languages: English, German, French, Spanish, Italian, and Portuguese (Lample et al., 2018). We"
    },
    {
        "casual_text": "Since tables made from plain text are pretty basic, they don’t have a lot of fancy features or options.",
        "formal_text": "Convert casual text to formal text: Since tables made from plain text are pretty basic, they don’t have a lot of fancy features or options. Convert casual text to formal text: Since tables made from plain text are pretty basic"
    },
    {
        "casual_text": "The paper is organized like this: In Section 2, we talk about the new dataset we created, how we made it, and how we checked and cleaned it. You can also find a quick overview of the baselines we used for summarization in Section 3. Section 4 explains our experimental setup and the model parameters. In Sections 5 and 6, we share the results—both the automatic evaluation using the ROUGE metric and the linguistic evaluation. Section 7 gives examples of what the models produced and some mistakes they made. Finally, Sections 8 and 9 cover the discussion, conclusions, and ideas for future work.",
        "formal_text": "Convert casual text to formal text: The paper is organized like this: In Section 2, we talk about the new dataset we created, how we made it, and how we checked and cleaned it. You can also find a quick overview of"
    },
    {
        "casual_text": "We’re kicking off our analysis with bilingual lexicon induction (BLI), which is a cross-lingual task that’s gotten a lot of attention lately. Specifically, it’s often used as a case study to see how differences between languages affect performance (Artetxe et al., 2018). It’s pretty popular because it’s a straightforward task and doesn’t need a ton of resources, so it’s super practical for lots of language pairs (Ruder et al., 2019b).",
        "formal_text": "Convert casual text to formal text: We’re kicking off our analysis with bilingual lexicon induction (BLI), which is a cross-lingual task that’s gotten a lot of attention lately. Specifically"
    },
    {
        "casual_text": "With this scalable solution, we can now also consider extra properties, like nuclearity, in our analysis. The cool thing about creating discourse trees with nuclearity is that it really helps us understand the \"importance\" of different parts of the text. This idea comes from RST (Mann and Thompson, 1988), where nuclearity tells us how important something is in a local context. There are three types: Nucleus-Satellite (N-S), Satellite-Nucleus (S-N), and Nucleus-Nucleus (N-N). N-S and S-N show which part is more important, while N-N means both parts are equally important (Morey et al., 2018). This idea of importance is super useful for tasks like summarization and text categorization (e.g., Marcu (2000); Ji and Smith (2017); Shiv and Quirk (2019)). To make this work, we add the nuclearity attribute to the tree-generation process. We assign each subtree one of the three nuclearity classes (N-S, S-N, or N-N) based on the attention values (a_cl and a_cr) that show how important each node is. We start with the attention values from the leaf nodes, which come from MILNet, and then move through the tree structure using equation (1). If a_cl is bigger than a_cr, we label it N-S; if it's the other way around, we go with S-N.",
        "formal_text": "Convert casual text to formal text: With this scalable solution, we can now also consider extra properties, like nuclearity, in our analysis. The cool thing about creating discourse trees with nuclearity is that it really helps us understand the \""
    },
    {
        "casual_text": "We're looking at three ways to add the vector v to the single-mode setup we talked about in Section 3. First up, there's the encdecinit method, which basically starts off by using the visual features to set the initial states for both the encoder (h0) and the decoder (s0).",
        "formal_text": "Convert casual text to formal text: We're looking at three ways to add the vector v to the single-mode setup we talked about in Section 3. First up, there's the encdecinit method, which"
    },
    {
        "casual_text": "Alright, so we've got: - **Hotel set**: V_D - **Review set**: V_R - **Term set**: V_T - And some normalized transition probability matrices: D_HR, D_RR, D_RH, D_RT, D_TT, D_TR.",
        "formal_text": "Convert casual text to formal text: Alright, so we've got: - **Hotel set**: V_D - **Review set**: V_R - **Term set**: V_T"
    },
    {
        "casual_text": "The network we're suggesting has three main parts: 1. **Modality Encoders (ME)**: This part takes in the single-type features we got earlier and turns them into individual encodings for each type. 2. **Triplet Attention Subnetwork (TAS)**: This includes attention for the same type, between different types, and between different tasks. 3. **Classification Layer**: This layer gives us the results for both tasks, DAC and ER.",
        "formal_text": "Convert casual text to formal text: The network we're suggesting has three main parts: 1. **Modality Encoders (ME)**: This part takes in the single-type features we got earlier and turns them into individual"
    },
    {
        "casual_text": "So, basically, if we say: F is the forest made up of all the syntactic trees (T i ) from the considered lexical entries, plus the output (the type we're trying to get), then the generation problem (check out figure 4) is about finding a matching M for the atomic formulas in F.",
        "formal_text": "Convert casual text to formal text: So, basically, if we say: F is the forest made up of all the syntactic trees (T i ) from the considered lexical entries, plus the output ("
    },
    {
        "casual_text": "(2) Cool thing: we noticed that some of the tricky annotation issues mentioned in the PDTB (by Webber et al., 2019) are popping up in our own data too. One of those tricky bits is figuring out the difference between Comparison. Contrast and Comparison. Concession, specifically when dealing with \"Despite what\" and \"What is contrasted with.\" In our manually checked data sample, we found 3 cases where QADiscourse and PDTB annotations got mixed up.",
        "formal_text": "Convert casual text to formal text: (2) Cool thing: we noticed that some of the tricky annotation issues mentioned in the PDTB (by Webber et al., 2019) are popping up in our own data too. One"
    },
    {
        "casual_text": "• Nave Bayes: We're comparing our model to the classic NB method. We're also checking it against a really strong version of NB called NBSVM, which was introduced by Wang and Manning in 2012. NBSVM has two versions: one uses just single words (NBSVM-uni) and the other uses pairs of words (NBSVM-bi).",
        "formal_text": "Convert casual text to formal text: • Nave Bayes: We're comparing our model to the classic NB method. We're also checking it against a really strong version of NB called NBSVM"
    },
    {
        "casual_text": "Here, we’ll talk about some background stuff on syntactic and semantic dependency parsing, as well as mixture models. We’ll also give a quick overview of syntax-aware SRL methods that use SSDP info and compare the method we’re proposing to what’s been done before.",
        "formal_text": "Convert casual text to formal text: Here, we’ll talk about some background stuff on syntactic and semantic dependency parsing, as well as mixture models. We’ll also give a quick overview of syntax-a"
    },
    {
        "casual_text": "It seems like the text you provided is a mix of symbols and characters that don't form coherent words or sentences. It looks like it might be a random string of characters or possibly encrypted data. Could you clarify or provide more context so I can assist you better?",
        "formal_text": "Convert casual text to formal text: It seems like the text you provided is a mix of symbols and characters that don't form coherent words or sentences. It looks like it might be a random string of characters or possibly encrypted data"
    },
    {
        "casual_text": "We ran experiments on the CoNLL-2009 shared task datasets for Semantic Role Labeling (SRL) and the SemEval-2010 Task 8 dataset for Relation Classification (RC). For SRL, our models did way better than earlier methods across different languages. For RC, our model also performed really well, almost on par with the best models out there. Our main new contributions are:",
        "formal_text": "Convert casual text to formal text: We ran experiments on the CoNLL-2009 shared task datasets for Semantic Role Labeling (SRL) and the SemEval-2010 Task 8 dataset for Relation Classification"
    },
    {
        "casual_text": "The denominator represents all the possible word pairs in the path P. Basically, if a partial path follows the same order of translating words as other system outputs, it gets a boost. This cost function is then multiplied by a weight that's set manually and added into the log-linear cost model framework.",
        "formal_text": "Convert casual text to formal text: The denominator represents all the possible word pairs in the path P. Basically, if a partial path follows the same order of translating words as other system outputs, it gets"
    },
    {
        "casual_text": "In an SOV-SVO setup, some ways to guess words that aren't seen before are suggested to deal with this issue. Matsubara and his team (1999) try to figure out the English verb in the target sentence and fit it into the sentence structure. Grissom II and others (2014) predict the last verb in the source sentence and use reinforcement learning to decide when to use that predicted verb.",
        "formal_text": "Convert casual text to formal text: In an SOV-SVO setup, some ways to guess words that aren't seen before are suggested to deal with this issue. Convert casual text to formal text: Convert casual text"
    },
    {
        "casual_text": "In subfigure (c), M-CAN+2R o gives us the best attention weights. The attention for the aspect \"food\" is pretty much at right angles to the attention for \"service.\" The \"food\" aspect focuses on the first half of the sentence, while \"service\" zeroes in on the second half. Also, the words \"outstanding\" and \"tops\" get the most attention in their respective aspects.",
        "formal_text": "Convert casual text to formal text: In subfigure (c), M-CAN+2R o gives us the best attention weights. The attention for the aspect \"food\" is pretty much at right angles to the attention for"
    },
    {
        "casual_text": "The only reason mistakes happen is because of issues with the annotations or problems in how we handle pronouns. In the next section, we'll compare the results we get from the extraction process using semantic annotations to the ones we get when we only use syntactic annotations.",
        "formal_text": "Convert casual text to formal text: The only reason mistakes happen is because of issues with the annotations or problems in how we handle pronouns. In the next section, we'll compare the results we get from the extraction process"
    },
    {
        "casual_text": "Given these constraints, the current methods out there just can't handle those entity-concept pairs that don't show up together, which leaves us with a concept graph that's missing some pieces.",
        "formal_text": "Convert casual text to formal text: Given these constraints, the current methods out there just can't handle those entity-concept pairs that don't show together, which leaves us a concept graph that's missing some pieces."
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. We're dealing with something called H(y, z | x), which is a combination of sequence entropy and intent entropy. The goal here is to make calculating this stuff easier and more practical. Back in 2007, Mann and McCallum, along with Hernando et al. in 2005, came up with a neat way to efficiently calculate what's called true sequence entropy in linear-chain CRFs. We're taking that method and using it to create Algorithm 2, which helps us figure out the true Joint Entropy Equation (9) for our specific model.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. We're dealing with something called H(y, z | x), which is a combination of sequence entropy"
    },
    {
        "casual_text": "Basically, a lot of argumentative stuff, like articles that take a side on an issue, usually don’t include arguments against their own viewpoint. This could be because the writer just doesn’t know about those opposing arguments, maybe because they weren’t brought up when the article was written. Or, the writer might know about them but chooses to ignore them because they don’t want to mess with their main point. Either way, this leaves readers in the dark. Without hearing both sides, people might make decisions or form opinions based on incomplete or biased info. So, it makes sense to create a system that can automatically find and show those opposing arguments.",
        "formal_text": "Convert casual text to formal text: Basically, a lot of argumentative stuff, like articles that take a side on an issue, usually don’t include arguments against their own viewpoint. This could be because the writer just doesn"
    },
    {
        "casual_text": "Temporal adverbials help us figure out when something happens in relation to other events in the text. They can vary based on how they describe that time and how they connect to earlier parts of the text. For example, words like \"tomorrow\" or \"last week\" set up different time frames with different rules about what comes before or after, but they both tie the time of evaluation to the moment of speaking. On the other hand, \"the next day\" or \"the previous week\" work the same way in terms of time but require the evaluation to be linked to a past point in time or perspective. Then there are other types of adverbials. For instance, \"this day\" sets a general time frame, while \"at three o'clock\" marks a specific moment. Words like \"then\" or \"afterwards\" just show a connection between events without setting a specific time. The whole system for handling this is built in PRO-LOG. Right now, we're focusing on two parts: the Composer and the Resolver, which deal with how tenses are managed in these modules. The inference machine and knowledge base aren't ready yet.",
        "formal_text": "Convert casual text to formal text: Temporal adverbials help us figure out when something happens in relation to other events in the text. They can vary based on how they describe that time and how they connect to earlier"
    },
    {
        "casual_text": "So, the non-terminals are on the left side (LHS) and also in parentheses on the right side (RHS). Each row has a role template (from Melamed et al., 2004) that explains how the non-terminals on the RHS are arranged and how close they are to each other. For instance, in the first row of 5 r, [1, 2] means the two non-terminals are in a straight order. In the last row of 2 r, [2, 1] shows that the order of the two non-terminals is flipped. 5 r and 6 r are basically the straight rule and inverted rule in MEBTG. In the last row of 7 r, [2, 1, 2] tells us that the second non-terminal comes before and after the first one. The (\"join\") operator then shuffles the non-terminals in each language based on their role template.",
        "formal_text": "Convert casual text to formal text: So, the non-terminals are on the left side (LHS) and also in parentheses on the right side (RHS). Each row has a role template (from Mela"
    },
    {
        "casual_text": "Here's the informal version: Seq2Seq-QG on SQuAD got 12.28, MS MARCO scored 10.46, and WikiQA had 2.04. Table 3 shows the QG results using the original training sets.",
        "formal_text": "Convert casual text to formal text: Here's the informal version: Seq2Seq-QG on SQuAD got 12.28, MS MARCO scored 10.46, and WikiQA had 2.04. Table 3"
    },
    {
        "casual_text": "Looking at the context sentences manually, we noticed that a big reason for missing full evidence is the way literary language can be vague. Authors often avoid repeating stuff or directly mentioning character names, so it’s super important to catch accurate paraphrases and figure out coreference. We think commonsense knowledge is really key to making BookQA better. While testing our system, we kept finding situations where the model messed up because it missed important details that weren’t spelled out. Some common examples we saw were: i) Character relationships that were obvious to readers but never actually explained (like, “Who did Mark’s best friend marry?”); ii) A character’s feelings about something (like, “Who was mad about the school’s policy?”); iii) The order of events (like, “Who did Marriat talk to after the big fight?”). Adding commonsense knowledge to a QA system is still a tricky problem in general, and that includes BookQA.",
        "formal_text": "Convert casual text to formal text: Looking at the context sentences manually, we noticed that a big reason for missing full evidence is the way literary language can be vague. Authors often avoid repeating stuff or directly mentioning character names,"
    },
    {
        "casual_text": "Alright, let's dive deeper into this. Instead of looking at languages as a whole, we're zooming in on individual nouns. The big question here is whether there's any rhyme or reason to the mistakes we see. Are they just random, or do certain types of nouns give us more trouble than others?",
        "formal_text": "Convert casual text to formal text: Alright, let's dive deeper into this. Instead of looking at languages as a whole, we're zooming in on individual nouns. The big question here is whether there'"
    },
    {
        "casual_text": "The main idea behind event argument detection is to connect argument parts of a sentence with the predicate, which is like the event trigger. Nowadays, the best methods for sentence-level SRL work in an end-to-end way, like span-based models (He et al., 2018; Ouchi et al., 2018) and sequence labeling models (He et al., 2017; Shi and Lin, 2019). But span-based models struggle with arguments that cross sentence boundaries because they have to deal with a lot of span candidates, making the process really complex. On the other hand, traditional sequence labeling models are faster but not as flexible when it comes to tricky situations like overlapping mentions or multiple roles for a single mention. In this project, we’re taking a two-step approach to break down the problem into smaller parts. We think that focusing on head-words can help us capture the information we need from the mention spans. Figure 1 shows the three main parts of our model: 1) a BERT-based Encoder, 2) an Argument Head-Word Detector, and 3) a Head-to-span Expander.",
        "formal_text": "Convert casual text to formal text: The main idea behind event argument detection is to connect argument parts of a sentence with the predicate, which is like the event trigger. Nowadays, the best methods for sentence-level SRL work in"
    },
    {
        "casual_text": "Check out the results for the English-Turkish dataset in Table 3. We ran some specific experiments on it, which you can see labeled as \"Experimental results on EN-TR dataset.\" The stuff we looked at includes indexing time (IT), computation time (CT), and the average time it took for each sentence (AT).",
        "formal_text": "Convert casual text to formal text: Check out the results for the English-Turkish dataset in Table 3. We ran some specific experiments on it, which you can see labeled as \"Experimental results on EN-TR"
    },
    {
        "casual_text": "In this paper, we’ve introduced a fresh approach that’s definitely worth checking out by the machine translation crowd and exploring more in other areas.",
        "formal_text": "Convert casual text to formal text: In this paper, we’ve introduced a fresh approach that’s definitely worth checking out by the machine translation crowd and exploring more in other other areas. Convert casual text to formal text: In"
    },
    {
        "casual_text": "Over the last three decades, there’s been a lot of work to create a strong and growing collection of language resources for different linguistic and NLP tasks (Bowman et al., 2015; Guzmán et al., 2019). For example, gold-standard datasets have been made for things like sentiment analysis and emotion detection (Wiebe et al., 2005; Thelwall et al., 2010). People have also looked into how personality traits show up in text (Luyckx and Daelemans, 2008). More recently, researchers have been building corpora to study depression and cyberbullying, including annotating texts where people share personal stuff that could lead to bullying (Rakib and Soon, 2018). There are also datasets for modeling empathy and distress (Buechel et al., 2018).",
        "formal_text": "Convert casual text to formal text: Over the last three decades, there’s been a lot of work to create a strong and growing collection of language resources for different linguistic and NLP tasks (Bowman et"
    },
    {
        "casual_text": "Barzilay and Lapata (2008) tested their entity-based model with two tasks: sentence ordering and rating the coherence of summaries.",
        "formal_text": "Convert casual text to formal text: Barzilay and Lapata (2008) tested their entity-based model with two tasks: sentence ordering and rating the coherence of summaries. Convert casual text to formal text: Barzi"
    },
    {
        "casual_text": "Another big part of FunLines is tweaking regular headlines to make them funny. We’re aiming for humor that’s tightly controlled to make machine learning and humor analysis easier. In FunLines, players take a normal headline from a news source and change just one word or thing to make it funny. This gives us really useful data for figuring out the exact moment when a headline goes from serious to funny. This is different from Unfun.me, another game that also creates pairs of funny and unfunny headlines (West and Horvitz, 2019). In Unfun.me, players start with a funny headline and turn it into a serious one. Their changes aren’t limited, but they’re encouraged to keep them small. Since FunLines starts with regular headlines, there’s a ton of raw material to work with. FunLines’ editing tasks are also similar to projects that help people create humor, like HumorTools (Chilton et al., 2016) and Libitum (Hossain et al., 2017).",
        "formal_text": "Convert casual text to formal text: Another big part of FunLines is tweaking regular headlines to make them funny. We’re aiming for humor that’s tightly controlled to make machine learning and humor analysis easier. In Fun"
    },
    {
        "casual_text": "Basically, we use a special thing called \"[AM]\" to show the AM query, which is like a way to find all the arguments, like A a = arg a k .",
        "formal_text": "Convert casual text to formal text: Basically, we use a special thing called \"[AM]\" to show the AM query, which like a way to find all the arguments, like A a = arg"
    },
    {
        "casual_text": "A typical way to add semantic meaning to language models is to check how similar a word is to its context and then tweak the probabilities from an n-gram model based on that. This helps the n-gram model, which usually focuses on short-term connections, also consider longer-term, more meaningful relationships. A lot of earlier research followed this idea (like Bellegarda in 2000, Coccaro and Jurafsky in 1998, and Wandmacher and Antoine in 2007), often using LSA to handle the semantic parts for individual words. Some researchers (Coccaro and Jurafsky, Wandmacher and Antoine) used the idea of a \"vector centroid\" to represent the context, while others (Bellegarda, Deng and Khundanpur) went with a \"pseudodocument\" approach, which comes from the relationship between documents and words in LSA. All of them calculate the probability of a word based on its context using cosine similarity, but they need to adjust it to get proper probability values. Gildea and Hofmann (1999) took a different route, using pLSA to create representations that already have a clear probabilistic meaning. This makes it easier to directly calculate the probability of a word based on its context without needing extra adjustments.",
        "formal_text": "Convert casual text to formal text: A typical way to add semantic meaning to language models is to check how similar a word is to its context and then tweak the probabilities from an n-gram model based on that. This"
    },
    {
        "casual_text": "Sure! Here's a more casual version: You might get some info about the restaurants the user has been to, including details like what they offer (called \"slots\") and their specific features. For example, \"(3) parking lot\" means the user has visited three places with parking. This part about their past visits might or might not be included. If it is, your job is to make good use of it—just click \"Use Fact\"—to help give them a great recommendation based on where they've been before.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: You might get some info about the restaurants the user has been to, including details like what they offer (called \"slots\") and their specific"
    },
    {
        "casual_text": "We used LM as the starting point for our retrieval tests. To find the best LM setup, we tried out different values for the  parameter, ranging from 0 to 1. Turns out, setting  to 0.99 gives us the best results, with a MAP of 0.593. This shows that idf doesn't really matter for ASM. Normalizing tf (like tf (t, D)/|D|) is enough to get close to LS, as you can see in Equation 2.",
        "formal_text": "Convert casual text to formal text: We used LM as the starting point for our retrieval tests. To find the best LM setup, we tried out different values for the  parameter, ranging from 0 to 1. Turn"
    },
    {
        "casual_text": "But, when we're making predictions,  t is figured out based on the stuff that's already been generated.",
        "formal_text": "Convert casual text to formal text: But, when we're making predictions,  t is figured out based on the stuff that's already generated. Convert casual text to formal text: But, when we'"
    },
    {
        "casual_text": "The inference model can be any ready-to-use language model or one that’s been fine-tuned for the specific task. We’re not doing any extra fine-tuning with knowledge prompting.",
        "formal_text": "Convert casual text to formal text: The inference model can be any ready--use language model or one that’s been fine-tuned for the specific task. We’re not doing any extra fine-tuning"
    },
    {
        "casual_text": "2. How well does the other side stick to this guideline? (Rate them 1-5):",
        "formal_text": "Convert casual text to formal text: 1. How well the other side sticks to this guideline (Rate them 1-5): 2. How well the other side sticks to this guideline? (Rate them 1-5): 2. How well does"
    },
    {
        "casual_text": "We fine-tuned the settings for a single GPU with 12GB of memory, which works well in most average industrial or academic setups. The rest of the settings are just the standard ones recommended by XLM.",
        "formal_text": "Convert casual text to formal text: We fine-tuned the settings for a single GPU with 12GB of memory, which works well in most average industrial or academic setups. The rest of the settings are just the standard ones"
    },
    {
        "casual_text": "After that, one of the authors went through the explanations the experts gave and put them together.",
        "formal_text": "Convert casual text to formal: After that, one of the authors went through the explanations the experts gave and put them together. Convert casual text to formal: After that, one of the authors went through the explanations the experts gave"
    },
    {
        "casual_text": "So,  is the Gram-Schmidt process (check out Section 3.3 for more info) applied to the square matrix M r (i). The result, (M r (i)), is an orthogonal matrix that comes from M r (i). And  t is basically the combination of all the sub-vectors  t (i) from Equation 1, like, you know, putting them all together.",
        "formal_text": "Convert casual text to formal text: So,  is the Gram-Schmidt process (check out Section 3.3 for more info) applied to the square matrix M r (i). The result, (M"
    },
    {
        "casual_text": "When people tackle reading comprehension, they usually have their own ways of doing it. Some folks like to read the question first and then skim the paragraph to find the answer, using what they know about the question to guide them. We call this the Q2P mode. Others prefer to read the paragraph first and then answer the question based on what they remember from it. That’s the P2Q mode. Then there’s another approach where you read both the question and the paragraph at the same time and do some comparing and thinking to figure things out. We call that the QCP mode. The Q2P mode works well when the answer is buried somewhere in the paragraph, and having the question in mind helps you zero in on the right spot. The P2Q mode is great for tricky questions that need the whole paragraph as context to make sense of. And the QCP mode is useful when you need to piece together different parts of the paragraph to understand the main idea. So, these three ways of tackling reading comprehension are all different and can be super helpful in different situations. That’s why our model is going to mix all three of these approaches. This way, we can handle reading comprehension in all kinds of scenarios by using different methods depending on what’s needed.",
        "formal_text": "Convert casual text to formal text: When people tackle reading comprehension, they usually have their own ways of doing it. Some folks like to read the question first and then skim the paragraph to find the answer, using what they know about the"
    },
    {
        "casual_text": "It says that the tokens linked to ReplaceOld get deleted, and the tokens linked to ReplaceNew take their spot. Usually, there's no overlap between the tokens linked to ReplaceOld and ReplaceNew. For example, if you're replacing B with C in \"C old =AB\" to make \"C new =AC,\" the related edit would look like:",
        "formal_text": "Convert casual text to formal text: It says that the tokens linked to ReplaceOld get deleted, and the tokens linked to ReplaceNew take their spot. Usually, there's no overlap between the tokens linked to"
    },
    {
        "casual_text": "Sure! Here's a more casual version: \"For a negative sample of a document, you swap its original context with a random one taken from other documents. But you keep the ECP the same.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual text: \"For a negative sample of a document, you swap its original context with a random one taken from other documents. But you keep the"
    },
    {
        "casual_text": "To kick off our walk, we set v 0 so that all the seed nodes have the same chance of being picked. As we move through the graph, there's a tiny chance, , that we'll jump back to the seed nodes. This keeps us from wandering too far from where we started. We keep going through the graph until things settle down, then we look at the final numbers to rank the mentions. For our tests, we used a  of 0.01.",
        "formal_text": "Convert casual text to formal text: To kick off our walk, we set v 0 so that all the seed nodes have the same chance of being picked. As we move through the graph, there's a tiny chance"
    },
    {
        "casual_text": "When the best possible distribution for the next word in the output space has multiple options, GPT-2 struggles to pick the right ones across all those options. This happens because the single embedding in the softmax layer, which is used by almost all current language models, limits how well they can predict the next or masked word. To fix this issue caused by the way things are set up, we came up with something called multifacet softmax (MFS). In our tests, we found that MFS works way better than the standard softmax layer and does a better job than the mixture of softmax (MoS) at solving the problem in models like GPT-2.",
        "formal_text": "Convert casual text to formal text: When the best possible distribution for the next word in the output space has multiple options, GPT-2 struggles to pick the right ones across all those options. This happens because the single embedding in the"
    },
    {
        "casual_text": "Alright, let’s start with the basic SoftTFIDF. Imagine we have two sets of feature vectors, S and T, where S = (s1, ..., sn) and T = (t1, ..., tm). In this case, si (for i = 1 to n) and tj (for j = 1 to m) are just little pieces of text, or \"tokens.\" Now, let’s define CLOSE(; S; T) as the group of tokens w in S that have something similar in T.",
        "formal_text": "Convert casual text to formal text: Alright, let’s start with the basic SoftTFIDF. Imagine we have two sets of feature vectors, S and T, where S = (s1, ..., sn"
    },
    {
        "casual_text": "In this paper, we take a closer look at how one of the cool new things in SEAL—using character-level methods to spot potential regular structures, or wrappers, in web pages—affects performance. Back in the day, some early systems for analyzing web pages, like WIEN (Kushmerick et al., 1997) and DIPRE (Brin, 1998), did work at the character level, but more recent methods for set expansion have mostly focused on tokenized or parsed free text (Carlson et al., 2009; Talukdar et al., 2006; Snow et al., 2006; Pantel and Pennacchiotti, 2006) or used tricks to take advantage of HTML structures that often contain lists and tables (Nadeau et al., 2006; Etzioni et al., 2005).",
        "formal_text": "Convert casual text to formal text: In this paper, we take a closer look at how one of the cool new things in SEAL—using character-level methods to spot potential regular structures, or wrappers, in web pages—"
    },
    {
        "casual_text": "Okay, so here's how we handle the recipe rewrite task in a straightforward way: we just look up the dish online and find a version that matches the dietary restriction. Starting with the original recipe, we figure out what dish it’s for, then search through all the recipes we have (training, dev, and test) to find one for the same dish that fits the dietary requirement.",
        "formal_text": "Convert casual text to formal text: Okay, so here's how we handle the recipe rewrite task in a straightforward way: we just look up the dish online and find a version that matches the dietary restriction. Starting"
    },
    {
        "casual_text": "Alright, so you need to figure out the AI sequence for the main element on the source side. For example, if the IP node is labeled as \"(1 2) (3 4)\", that's what you're looking at.",
        "formal_text": "Convert casual text to formal text: Alright, so you need to figure out the AI sequence for the main element on the source side. For example, if the IP node is labeled as \"(1 2) (3"
    },
    {
        "casual_text": "Different Numbers of Shots. Looking at the number of shots (K) and comparing them based on the few-shot results in Table 2 and Figure 2, it's clear that using buckets generally boosts the model's performance on most tasks (like MLDoc, MARC, POS, and NER) compared to zero-shot results. This matches what previous studies found (Lauscher et al., 2020; Hedderich et al., 2020) and aligns with the success of methods that use bootstrapped data (Chaudhary et al., 2020).",
        "formal_text": "Convert casual text to formal text: Different Numbers of Shots. Looking at the number of shots (K) and comparing them based on the few-shot results in Table 2 and Figure 2, it's clear that using bucket"
    },
    {
        "casual_text": "Another way to make sure the questions are more meaningful is to connect them so that answering one correctly depends on getting some others right. Like, for example, Dalvi and his team (2018) made a dataset that needs you to really understand scientific stuff step by step.",
        "formal_text": "Convert casual text to formal text: Another way to make sure the questions are more meaningful is to connect them so that answering one correctly depends on getting some others right. Like, for example, Dalvi and his team (2018) made a dataset"
    },
    {
        "casual_text": "Unlike other types of spam, like web spam (Martinez-Romo and Araujo, 2009; Castillo et al, 2006) or email spam (Chirita et al, 2005), deceptive opinion spam is tricky. It’s not something people can just ignore or easily spot (Ott et al, 2011). So, there’s been more focus on creating automated, often machine learning-based, tools to help people figure out which reviews are fake (check out Section 2 for more on that). But even with these methods, it’s not perfect. Sometimes, they might mistake real reviews for fake ones, which could annoy honest reviewers and push them away.",
        "formal_text": "Convert casual text to formal text: Unlike other types of spam, like web spam (Martinez-Romo and Araujo, 2009; Castillo et al, 2006) or email spam (Chi"
    },
    {
        "casual_text": "Okay, so let’s break this down in a simpler way. We’re looking at the equation i \"+1 = 1. The integral here is basically O/(r -O), which is the most important part on the left side. So, we can rewrite the left side as )d(1 -a)/a. Earlier, it was said that only a combined local field can go beyond a certain limit, which means external input. e(1 -a)  U. So, we can safely assume A   Aa, and this is fine because a is small. Next, we’re trying to figure out a threshold value that helps with storage capacity. This is done using signal-to-noise ratio analysis, following references [1] and [2], as N and p get really large. Time-related effects are ignored because they affect both signal and noise equally. Also, it’s assumed that there’s external input, which lets us study critical noise effects. In this model, the external input synapses don’t add extra noise—they only show the strength of the incoming signal. Now, let’s say the system is in a state S = . The signal here is the part of the input that activates the current pattern.",
        "formal_text": "Convert casual text to formal text: Okay, so let’s break this down in a simpler way. We’re looking at the equation i \"+1 = 1. The integral here is basically O/(r -O),"
    },
    {
        "casual_text": "We've added a basic negation feature that looks for the 'un-' prefix and some common negation words like 'not'. It also tells you which part of the sentence it found the negation in.",
        "formal_text": "Convert casual text to formal text: We've added a basic negation feature that looks for the 'un-' prefix and some common negation words like 'not' It also tells you which part of the"
    },
    {
        "casual_text": "Learning to rank has been a hot topic in the information retrieval field lately. There are different methods like pointwise, pairwise, and listwise approaches that have been studied a lot (Liu, 2009). Some of these methods have been used for document summarization, such as SVR (Ouyang et al., 2007), classification SVM (Wang et al., 2007), and RankNet (Svore et al., 2007). However, no one has really compared these ranking algorithms in a systematic way. As far as we know, this is the first time a listwise learning-to-rank algorithm called ListNet (Cao et al., 2007) is being used for document summarization in this paper. Plus, pairwise and listwise learning-to-rank algorithms have never been applied to concept ranking for extractive summarization before.",
        "formal_text": "Convert casual text to formal text: Learning to rank has been a hot topic in the information retrieval field lately. There are different methods like pointwise, pairwise, and listwise approaches that have been studied a lot (L"
    },
    {
        "casual_text": "Nie et al. (2020) came up with a thing called SA-NER that uses some fancy methods to deal with data sparsity issues. Basically, they grab extra semantic info from a huge collection of text and then use two cool tools: an attentive semantic augmentation module and a gate module. These tools help them organize and combine all that extra info.",
        "formal_text": "Convert casual text to formal text: Nie et al. (2020) came up with a thing called SA-NER that uses some fancy methods to deal with data sparsity issues. Basically, they grab extra semantic info"
    },
    {
        "casual_text": "Hiersum: This is a method suggested by Haghighi and Vanderwende in 2009. It uses LDA (a topic model) to figure out the distribution of single words (unigrams), as shown in Equation 12.",
        "formal_text": "Convert casual text to formal text: Hiersum: This is a method suggested by Haghighi and Vanderwende in 2009. It uses LDA (a topic model) to figure out the distribution of single words (unigrams"
    },
    {
        "casual_text": "2. For the second set, make sure to include all the correct answers to the question in one OR operator. This way, there's a much better chance that the evaluation set will have some valid answer sentences.",
        "formal_text": "Convert casual text to formal text: 2. For the second set, make sure to include all the correct answers to the question in one OR operator. This way, there's a much better chance that the evaluation set will have some valid"
    },
    {
        "casual_text": "The studies show that using semi-automated tools can be really helpful for writers and people reading or reviewing medical papers. These tools could make the articles better and make it easier to analyze the content.",
        "formal_text": "Convert casual text to formal text: The studies show that using semi-automated tools can be really helpful for writers and people reading or reviewing medical papers. These tools could make the articles better and make it easier to analyze the content."
    },
    {
        "casual_text": "Okay, so \"beantworten\" means \"to answer,\" and \"stellen\" means \"to ask.\" Then we've got 20% with 0, 15 and 0, 37. Next, \"Mensch\" is \"human,\" and \"sein\" is \"to be.\" There's 16% with 0, 09 and 0, 03. Finally, \"Zeuge\" means \"witness.\"",
        "formal_text": "Convert casual text to formal text: Okay, so \"beantworten\" means \"to answer,\" and \"stellen\" means \"to ask.\" Then we've got 20% with 0, 15 and 0, 37. Next, \"M"
    },
    {
        "casual_text": "The other 3 sub-categories under B2 are all connected in a few ways: the protasis can be a phrase (in two cases—restrictive and concessive use—it HAS to be a phrase). While the protasis is up in the air, the apodosis part is usually true, with just one small exception. This exception is hinted at in the protasis, but it’s not a sure thing. In the restrictive use (like \"if not\"), the writer is suggesting there might be an extra limit to one of the apodosis features. In the use where it questions a presupposition, the apodosis kind of oversteps in one of its assumptions. And in the concessive use, it’s not as strong as overstepping a presupposition, but more like a likely meaning or suggestion from a phrase in the apodosis. There’s a bit of a pattern here with the B2 category, but we think it’s pretty open-ended. You’ll probably find more creative uses of \"if\" here than in the other two categories.",
        "formal_text": "Convert casual text to formal text: The other 3 sub-categories under B2 are all connected in a few ways: the protasis can be a phrase (in two cases—restrictive and"
    },
    {
        "casual_text": "From what we've seen so far, getting rid of unhelpful examples in the training data seems to work well for making model responses less generic. But, like anything, it has its downsides. We noticed that the model starts making up more stuff when we do this. So, we need to figure out how to fix that and make sure the responses are more accurate.",
        "formal_text": "Convert casual text to formal text: From what we've seen so far, getting rid of unhelpful examples in the training data seems to work well for making model responses less generic. But, like anything, it has its downsides"
    },
    {
        "casual_text": "Argumentation Mining (AM) is all about spotting the parts of arguments, like claims and premises, and figuring out how they support or attack each other to show how arguments are structured. This has been done in a bunch of areas recently, like legal documents (Mochales Palau and Ieven, 2009), news articles (Deng and Wiebe, 2015; Sardianos et al., 2015), and user-generated content (Wachsmuth et al., 2014; Habernal and Gurevych, 2015). The goal is to automatically find arguments in messy text by sorting out what's argumentative and what's not, and then pulling out the key components and how they connect. Lately, there's been more interest in making tools that use AM to help people with their arguments, especially for students who need feedback on their writing (Song et al., 2014; Stab and Gurevych, 2014a, b; Wambsganss et al., 2020b). These tools give personalized tips on how to improve their argumentation. But, using this tech in schools hasn't really taken off yet (Stab and Gurevych, 2017b; Lawrence and Reed, 2019; Rosé et al., 2008), mostly because there aren't many datasets with student-written texts that have been labeled for argumentation (Lawrence and Reed, 2019; Wambsganss et al., 2020c).",
        "formal_text": "Convert casual text to formal text: Argumentation Mining (AM) is all about spotting the parts of arguments, like claims and premises, and figuring out how they support or attack each other to show how arguments are structured. This"
    },
    {
        "casual_text": "Alright, so this just gives us the axiom links. We still need to figure out the word order so that none of the axiom links cross. Turns out, this can be done in less than quadratic time—it’s basically a bracketing problem.",
        "formal_text": "Convert casual text to formal text: Alright, so this just gives us the axiom links. We still need to figure out the word order so that none of the axiom links cross. Turns out, this"
    },
    {
        "casual_text": "To check how well our methods work, we did an ablation study using the full test set from MIMIC-3. The results are in Table 5. We used a pretrained RoBERTa-base model as our PLM and fine-tuned it for ICD coding. As you can see in row (a), the performance drops a bit when we remove the domain-specific pretraining. This shows that domain-specific pretraining helps boost the performance.",
        "formal_text": "Convert casual text to formal text: To check how well our methods work, we did an ablation study using the full test set from MIMIC-3. The results are in Table 5. We used a pretrained RoBERTa-"
    },
    {
        "casual_text": "The 1 2 thing just shows if the matching part is in the foreign sentence or the English one.",
        "formal_text": "Convert casual text to formal text: The 1 2 thing just shows if the matching part in the foreign sentence or English one."
    },
    {
        "casual_text": "This approach results in 20,000 entries and 50,000 word senses, which seemed pretty handy at first. But there were two big issues that stopped us from using it again. First, to make it easy to switch between the database and rtf formats, the dictionary entry structure can't be too complicated. Second, when someone edits an entry using Word, it's super hard to keep the syntax in check, even with some Word macros that help a bit.",
        "formal_text": "Convert casual text to formal text: This approach results in 20,000 entries and 50,000 word senses, which seemed pretty handy at first. But there were two big issues that stopped us from using it again. First, to make it easy"
    },
    {
        "casual_text": "We're using spaCy, which is pretty awesome. It has near-state-of-the-art accuracy for part-of-speech tagging at 97.8% and for named entity recognition at 89.8% when tested on the OntoNotes dataset.",
        "formal_text": "Convert casual text to formal text: We're using spaCy, which is pretty awesome. It has near-state-of-the-art accuracy for part-of-speech tagging at 97.8%"
    },
    {
        "casual_text": "A PATR system needs to use lexical info, which is stored in feature structures. These structures are basically a bunch of attribute-value pairs. If we take the DATR theory stuff we talked about earlier (1), here's how it would look when written in feature structures:",
        "formal_text": "Convert casual text to formal text: A PATR system needs to use lexical info, which is stored in feature structures. These structures are basically a bunch of attribute-value pairs. If we take the DATR theory stuff"
    },
    {
        "casual_text": "In our experiments, we went with k=1, which turned out to be the best performer based on our tests. Figure 2 shows the active features in the example, using a graph representation. We call this feature extraction method \"All-Features\" (AF for short), and we define it as an operator AF:",
        "formal_text": "Convert casual text to formal text: In our experiments, we went with k=1, which turned out to be the best performer based on our tests. Figure 2 shows the active features in the example, using a graph representation"
    },
    {
        "casual_text": "The language stuff we're talking about here comes from basic grammar rules that are pretty common and not just specific to Hindi.",
        "formal_text": "Convert casual text to formal text: The language stuff we're talking about here comes from basic grammar rules that are pretty common and not just specific to Hindi. Convert casual text to formal text: The language stuff we're talking about"
    },
    {
        "casual_text": "Alright, let's break down the process shown in Figure 1, which outlines our Graph2Tree framework. First off, Graph2Tree starts by using a BiLSTM to encode the MWP text input. At the same time, it builds two graphs: the Quantity Cell Graph and the Quantity Comparison Graph. The BiLSTM gives us word-level representations, which we use as the nodes for these graphs. Next, these node representations, along with the two graphs, are fed into a graph transformer. This transformer has a modified multiGCN component that focuses on learning the graph representation based on the Quantity Cell Graph and Quantity Comparison Graph. This tweak helps the model understand the relationships between quantities and their numerical qualities, making the final graph representation richer. After that, pooling is applied to combine all the nodes into a single graph embedding vector, which is the output of the graph transformer. Finally, this graph representation, along with the updated node representations, is passed to a tree-structure decoder. The decoder then works to figure out the final solution expression tree.",
        "formal_text": "Convert casual text to formal text: Alright, let's break down the process shown in Figure 1, which outlines our Graph2Tree framework. First off, Graph2Tree starts by using a Bi"
    },
    {
        "casual_text": "Some research has looked at compositionality from different angles. Ruis et al. (2020) took a deeper dive into compositionality by expanding on Lake and Baroni's (2018) work, connecting language to grid world environments. Hupkes et al. (2020) took a broader approach, testing neural models on their compositional skills using a bunch of task-agnostic tests, like an artificial translation task. The main takeaway from this research is that doing well on one specific task—even if it’s super compositional—isn’t enough to show that a model really gets compositionality. This paper builds on that idea by tweaking those more detailed compositionality tests and applying them to image captioning tasks, where the language is tied to visuals.",
        "formal_text": "Convert casual text to formal text: Some research has looked at compositionality from different angles. Ruis et al. (2020) took a deeper dive into compositionality by expanding on Lake and Baroni's (2018) work"
    },
    {
        "casual_text": "So, our model takes the key info from each candidate based on the zero pronoun (zp). Then, we create these representative vectors for the candidates, like v np 1, v np 2, . . . , v npn . Next, we plug these vectors and the zero pronoun into a two-layer neural network, which spits out a resolution score for each zero pronoun-candidate pair. After that, we get the probability for each candidate. The one with the highest probability wins and is considered the final result. Overall, our model has three main parts: a zero pronoun encoder, a candidate antecedent encoder, and a feed-forward neural network that figures out the scores for each candidate.",
        "formal_text": "Convert casual text to formal text: So, our model takes the key info from each candidate based on the zero pronoun (zp). Then, we create these representative vectors for the candidates, like v"
    },
    {
        "casual_text": "In Section 2, they talked about a word-based method. Basically, they used something called ILP to pick the best group of words from a sentence.",
        "formal_text": "Convert casual text to formal: In Section 2, they talked about a word-based method. Basically, they used something called ILP to pick the best group of words from a sentence."
    },
    {
        "casual_text": "Reporting verbs like \"insist\" get a lot of their structure from their semantic type, which is called LCP REPORTING VERB. The semantic type is what really gives the basic structure, while the individual verbs just add some extra details on top of that.",
        "formal_text": "Convert casual text to formal text: Reporting verbs like \"insist\" get a lot of their structure from their semantic type, which is called LCP REPORTING VERB. The semantic type is what really gives"
    },
    {
        "casual_text": "Our new model, sec-bert (last zone), which was trained on SEC reports, does better than the regular bert and fin-bert models when we don’t use any numeric pseudo-tokens. But, sec-bert still falls behind bert when numeric pseudo-tokens are added (75.7 vs. 78.3 and 79.4 on the test -F1 score). It struggles with breaking numbers into smaller parts, averaging 2.4 subwords per gold tag span. Without pseudo-tokens, sec-bert also doesn’t perform as well as the bilstm with word embeddings (75.7 vs. 77.3 -F1, check Table 3). But when we use the pseudo-tokens we suggested, secbert-num and sec-bert-shape really shine, hitting the best scores overall with test -F1 scores of 80.4 and 82.1, respectively. This shows that teaching the model how to handle numeric expressions during its initial training is a smarter move than trying to learn that stuff only during fine-tuning.",
        "formal_text": "Convert casual text to formal text: Our new model, sec-bert (last zone), which was trained on SEC reports, does better than the regular bert and fin-bert models when we don’t use any numeric pseudo"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: 1. The new topic score vector, which we'll call g k+1, is calculated like this: - Start with a times the old score vector P g k. - Then, add half of (1  a) times the sum of  T and W, multiplied by f k. 2. After that, make sure the score for the query in f k+1 is set to 1.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: 1. The new topic score vector, which we'll call g k+1, is calculated like this: -"
    },
    {
        "casual_text": "You can find the MELD dataset here: https://github.com/SenticNet/MELD, and the IEMOCAP dataset here: https://sail.usc.edu/iemocap/iemocap_release.htm. Basically, A is a matrix in R wdq, where w stands for the total number of window segments.",
        "formal_text": "Convert casual text to formal text: You can find the MELD dataset here: https://github.com/SenticNet/MELD, and the IEMOCAP dataset here: https://sail.usc"
    },
    {
        "casual_text": "Check out the appendix for the PPLM details. And yeah, the CTRL details are in there too. For the implementation, we're using the one from this GitHub link: https://github.com/gooppe/transformer-summarization.",
        "formal_text": "Convert casual text to formal text: Check out the appendix for the PPLM details. And yeah, the CTRL details are in there too. For the implementation, we're using the one from this GitHub link:"
    },
    {
        "casual_text": "This method can be adjusted using parameters to find connections at different levels of detail. But we’ve noticed that some groups break apart for unclear reasons, like the \"MARRIEDTO\" category splitting into two groups when k is higher.",
        "formal_text": "Convert casual text to formal text: This method can be adjusted using parameters to find connections at different levels of detail. But we’ve noticed that some groups break apart for unclear reasons, like the \"MARRIEDTO\" category splitting into"
    },
    {
        "casual_text": "Pretty much all the successful uses of MT and in-house systems rely on controlling the input texts. There's been a lot of research in Europe lately on this whole area of \"language engineering.\" For example, the SECC project in Leuven, which is backed by Siemens Nixdorf, Cap Gemini Innovation, and Sietec, is using MT methods to create a tool for writing in Simplified English, a controlled language. The Metal system is basically translating regular English into this controlled version.",
        "formal_text": "Convert casual text to formal text: Pretty much all the successful uses of MT and in-house systems rely on controlling the input texts. There's been a lot of research in Europe lately on this whole area of \"language"
    },
    {
        "casual_text": "So, we’ve changed the dialogue task to focus on creating responses based on specific scenarios, not just the conversation history. To make the scenarios richer, we use both past and future conversations to figure out the hidden context. This helps add more meaning to the dialogue and guides how the responses should be made. We’ve also come up with a new model to handle this kind of training data, where we pair up the hidden scenario with the response.",
        "formal_text": "Convert casual text to formal text: So, we’ve changed the dialogue task to focus on creating responses based on specific scenarios, not just the conversation history. To make the scenarios richer, we use both past and future conversations to"
    },
    {
        "casual_text": "Like FastText, the Form-Context Model (Schick and Schütze, 2018) mixes together both the word's shape and its context to create a better vector. There are two versions of this model, and both of them figure out the vector v(w, C) for rare words using:",
        "formal_text": "Convert casual text to formal text: Like FastText, the Form-Context Model (Schick and Schütze, 2018) mixes together both the word's shape and its context to create a better vector. There are"
    },
    {
        "casual_text": "We can make the ATN registers work like logical variables and add a unification step to them, which will give us a similar level of neatness as the DCG example.",
        "formal_text": "Convert casual text to formal text: We can make the ATN registers work like logical variables and add a unification step to them, which will give us a similar level of neatness the DCG example. Convert"
    },
    {
        "casual_text": "Okay, so let's break this down in a simpler way. There are a bunch of models and methods mentioned here, like CAGE, Latcinnik and Berant, DynaGen, Self-talk, and others. These are all different approaches to generating knowledge or making predictions using models that have been tweaked for specific tasks. Here's a quick rundown of the terms: - **Knowledge Generator**: This is a model that's been fine-tuned to create knowledge specifically for a task. There are different ways it can be done: - **Template-prompted**: Using a ready-made language model and getting it to generate knowledge by feeding it some templates. - **Demonstration-prompted**: Using a ready-made language model and getting it to generate knowledge by showing it a few examples (few-shot learning). - **Inference Model**: This is the model that uses the generated knowledge to make predictions. It can be: - **0-shot**: Just using a standard, off-the-shelf model that hasn't been fine-tuned for the task. - **Task-finetuned**: A model that's been fine-tuned using task-specific data, but without any extra knowledge. - **Joint-finetuned**: A model that's been fine-tuned using both task-specific data and the generated knowledge. Now, about the knowledge statements: We know some of them might not be perfect grammatically. If you can still figure out what the statement is trying to say, even if it's not a complete sentence or a bit off grammatically, just go ahead and choose the \"ungrammatical but understandable\" option.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in a simpler way. There are a bunch of models and methods mentioned here, like CAGE, Latcinnik and Berant, Dyna"
    },
    {
        "casual_text": "When figuring out vocabulary sizes and word stats, we usually clean up the text by making everything lowercase and getting rid of any characters that aren't letters or numbers.",
        "formal_text": "Convert casual text to formal text: When figuring out vocabulary sizes and word stats, we usually clean the text by making everything lowercase and getting rid of any characters that aren't letters or numbers. Convert casual text to"
    },
    {
        "casual_text": "When you’ve got the annotated document ready, it’s pretty straightforward to use it to figure out p(t a |t b ). The document should have all the info you need, even if someone forgot to add some tags. Here’s the formula we use to estimate p(t a |t b ) based on how often tags and words show up together:",
        "formal_text": "Convert casual text to formal text: When you’ve got the annotated document ready, it’s pretty straightforward to use it to figure out p(t a |t b ). The document should have all the"
    },
    {
        "casual_text": "In both GROBID and LaTeX parses, each bibliography entry is connected to the papers in the corpus that are most similar to it. To do this, we calculate a similarity score for each pair of a bibliography entry and a paper cluster, based on how alike their titles are. First, we clean up the titles by removing extra spaces, making everything lowercase, and getting rid of special characters. Then, we turn each title into a set of 3-character chunks (called 3-grams). The similarity score, called S_title, is a mix of two things: the Jaccard index (which measures how much two sets overlap) and a containment metric (which checks how much one set is included in the other). We combine these two using something called the harmonic mean.",
        "formal_text": "Convert casual text to formal text: In both GROBID and LaTeX parses, each bibliography entry is connected to the papers in the corpus that are most similar to it. To do this, we calculate"
    },
    {
        "casual_text": "1) So, the project manager had everyone go around and re-introduce themselves. 2) Then, the industrial designer talked about how a remote works on the inside, and the team brainstormed ideas for batteries and infra-red signals.",
        "formal_text": "Convert casual text to formal text: 1) So, the project manager had everyone go around and re-introduce themselves. 2) Then, the industrial designer talked about how a remote works on the inside, and the team brainstorm"
    },
    {
        "casual_text": "Okay, so we're keeping an eye on how  changes as  goes up, and at the same time, the relaxation parameter 1/ goes down. The whole setup with l 0 stays the same until l 0 crosses one of the lines l j, where j is between 1 and n (including negative values). To figure out which line l 0 is going to hit, we need to find where they intersect by calculating the points ( j,  j ) where l 0 and l j meet.",
        "formal_text": "Convert casual text to formal text: Okay, so we're keeping an eye on how  changes as  goes up, and at the same time, the relaxation parameter 1/ goes down. The whole setup with l"
    },
    {
        "casual_text": "We used the Moses baseline system from WMT11 (the Workshop on Statistical Machine Translation in 2011) and the same corpora they used for their evaluation. The only tweaks we made were to make sure the scripts ran smoothly. We also followed the same setup for training and dev sets as WMT11. So, the results here match the WMT11 setup exactly, except for the timing—our training and tuning took way longer. We grabbed a snapshot of Moses (from Koehn et al., 2007) on August 3, 2011. This was paired with the latest versions of SRILM (Stolcke, 2002, v. 1.5.12), GIZA++ (Och and Ney, 2000, v1.0.5), and the mteval script used in WMT11 (mteval-v11b.pl).",
        "formal_text": "Convert casual text to formal text: We used the Moses baseline system from WMT11 (the Workshop on Statistical Machine Translation in 2011) and the same corpora they used for their evaluation. The only tweaks we made were to"
    },
    {
        "casual_text": "This project sits right where discourse parsing and sentiment analysis meet, and it’s heavily inspired by four main areas of research:",
        "formal_text": "Convert casual text to formal text: This project sits right where discourse parsing and sentiment analysis are heavily inspired by four main areas research: Convert casual text to formal text: This project sits right where discourse parsing and"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: W c l  O c l = W c  l  the sum of (|a j |  1) from j=1 to l1  (O c + n  l) This is greater than or equal to (|t|  1)  n  the sum of (|a j |  1) from j=1 to l1 Which is also greater than or equal to the sum of (|a j |  1) from j=l to n And this is greater than or equal to |a l |  1. Basically, it's a bunch of math stuff showing how one part is bigger than another, step by step.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: W c l  O c l = W c  l  the sum of (|"
    },
    {
        "casual_text": "• Data Sets: As mentioned in Section 3, the annotated question-answer pairs are from three different areas. For each area, we divided the data into a training set (80% of each category) and a test set (20% of each category). On top of that, we kept 10% of the training set aside as development data to tweak the parameters.",
        "formal_text": "Convert casual text to formal text: • Data Sets: As mentioned in Section 3, the annotated question-answer pairs are from three different areas. For each area, we divided the data into a training set (80% of"
    },
    {
        "casual_text": "We picked Japanese literature because the researchers in that area really wanted a better system. The usual keyword-based systems just didn’t cut it for literature stuff.",
        "formal_text": "Convert casual text to formal text: We picked Japanese literature because the researchers in that area really wanted a better system. The usual keyword-based systems just didn’t cut for literature stuff. So we picked Japanese literature because the researchers in"
    },
    {
        "casual_text": "Elle a passé du temps en prison dans une cellule du commissariat local avant d'être jugée. D'après ce qu'on dit, elle était en vacances dans la région de Krabi, au sud de la Thalande.",
        "formal_text": "Convert casual text to formal text: Elle a passé du temps en prison dans une cellule du commissariat local avant d'être jugée. D'après ce qu'on dit, elle"
    },
    {
        "casual_text": "When we use ROUGE-L to check how well both the synthesized and machine-translated (MT) articles compare to human-written ones, we notice that the synthesized articles usually get better scores than the MT ones. You can see the results in Table 3.",
        "formal_text": "Convert casual text to formal text: When we use ROUGE-L to check how well both the synthesized and machine-translated (MT) articles compare to human-written ones, we notice that the synthesized articles"
    },
    {
        "casual_text": "For most datasets, the PRED and PRED-CCA methods boosted the F1 score compared to just using the \"target-only\" approach (CRF-TGT). The only exceptions were the MIT Restaurant and MIT Movie datasets, where the F1 scores for CRF-TGT, PRED, and PRED-CCA were pretty much the same. On the CADEC dataset, though, the F1 scores dropped when using 20 or 50 sentences.",
        "formal_text": "Convert casual text to formal text: For most datasets, the PRED and PRED-CCA methods boosted the F1 score compared to just using the \"target-only\" approach (CRF-TGT). The"
    },
    {
        "casual_text": "We’re also taking all four distance scores (Eq. 3, Eq. 5, Eq. 9, and Eq. 11) we talked about earlier and combining them into one final distance score. This score is what we use for the graph contextual orthogonal transform embedding (GC-OTE) when we’re training and making predictions.",
        "formal_text": "Convert casual text to formal text: We’re also taking all four distance scores (Eq. 3, Eq. 5, Eq. 9, and Eq. 11) we talked about earlier and combining them into one final distance score"
    },
    {
        "casual_text": "In this part, we'll start by looking at the basic pLSI and LDA models. After that, we'll introduce our own twist on LDA, which we call cross-collection LDA (ccLDA).",
        "formal_text": "Convert casual text to formal text: In this part, we'll start by looking at the basic pLSI and LDA models. After that, we'll introduce our own twist on LDA, which we call cross-"
    },
    {
        "casual_text": "Alright, here's the informal version: \"Top 20 words related to the topic 'players' in a bag-of-words format: [PAD], of, the, players, is, goal, a, ball, and, to, team, on, it, each, in, score, points, most, field.\"",
        "formal_text": "Convert casual text to formal text: Alright, here's the informal version: \"Top 20 words related to the topic 'players' in a bag-of-words format: [PAD], of, the"
    },
    {
        "casual_text": "We're using  to draw a comparison to inside probability—well, more specifically, the Viterbi approximation of inside probability. This is because we're focusing on maximizing instead of summing up all the possible parses.",
        "formal_text": "Convert casual text to formal text: We're using  to draw a comparison to inside probability—well, more specifically, the Viterbi approximation of inside probability. This is because we're focusing"
    },
    {
        "casual_text": "Classification systems, whether they're basic logistic regression or fancy neural networks, usually predict the likelihood of different classes and pick the one with the highest probability. We then check how well these predictions match the actual labels (called ground-truth labels) on new, unseen data to see how good the model is. But sometimes, we need to pay close attention to how confident the model is in its predictions, not just whether it got the right answer. For example, if the model's confidence levels are accurate, it can help us figure out if a tool predicting someone's likelihood of reoffending is fair (Chouldechova, 2017) or decide the best number of labels for medical diagnoses (Kavuluru et al., 2015). Guo et al. (2017) pointed out that even if a model is really good at getting the right class, it doesn't always mean it's good at estimating how sure it is about that prediction. To fix models that aren't great at showing their confidence, we use calibration methods (like the ones from Zadrozny and Elkan, 2001; Platt et al., 1999; Guo et al., 2017; Kumar et al., 2019). These methods tweak the probabilities the model gives after it's been trained. They basically retrain the model a bit on a separate validation set to get a better idea of how confident it should be, and then apply that to new test data. The problem is, this can mess up the consistency of the model's predictions across different data sets. Since the data is split into fixed parts, the calibration doesn't really adapt to how well the model is doing, making it kind of rigid and not very flexible.",
        "formal_text": "Convert casual text to formal text: Classification systems, whether they're basic logistic regression or fancy neural networks, usually predict the likelihood of different classes and pick the one with the highest probability. We then check how well these predictions match the"
    },
    {
        "casual_text": "We built our model using mention-level classifiers that work with probabilities. Think of it like this: a single person might be mentioned multiple times (like their name popping up in different sentences throughout our data). For each mention i in a sentence x i, our model decides if the person is described as having been killed by the police, where z i = 1 means yes, using a simple yes-or-no logistic model.",
        "formal_text": "Convert casual text to formal text: We built our model using mention-level classifiers that work with probabilities. Think of it like this: a single person might be mentioned multiple times (like their name popping up in different sentences"
    },
    {
        "casual_text": "To make sure the slot values stay the same, we treat each span as a single word. No changes can be made within that span. So, SD augmentation doesn't mess with the original meaning or labels of the sentence, meaning y stays as y.",
        "formal_text": "Convert casual text to formal text: To make sure the slot values stay the same, we treat each span as a single word. No changes can be made within that span. So, SD augmentation doesn't mess with the original"
    },
    {
        "casual_text": "When looking at how each class performs, we see a similar pattern across all the models. They tend to do best on the neutral class, but their performance drops for the pro-Ukrainian and pro-Russian classes. Interestingly, all the models struggle the most with the pro-Russian class, which could be because it has the smallest number of examples in the dataset. Oh, and by the way, the AUC (Area Under the Curve) was calculated using the trapezoidal rule, which is a method available in the sklearn package (thanks to Pedregosa et al., 2011). The precision-recall curves and confusion matrices were made by combining the test sets from all 10 data splits.",
        "formal_text": "Convert casual text to formal text: When looking at how each class performs, we see a similar pattern across all the models. They tend to do best on the neutral class, but their performance drops for the pro-Ukrai"
    },
    {
        "casual_text": "Wikipedia is the biggest multilingual source on the internet, but most articles aren't direct or close translations of each other. Still, we can't just ignore such a huge collection of parallel texts. Wikimatrix (Schwenk et al., 2019) pulled out bitexts from Wikipedia for 1,620 language pairs, including Bengali-English. But we noticed problems like foreign text, messed-up sentence splits, and alignment issues. So, we went back to the original source and only aligned sections that were very similar. We translated Bengali articles into English using an NMT model trained on our other data and compared each section of a Bengali article to its English version. We used the SacreBLEU (Post, 2018) score to measure similarity and only kept sections with a score above 20. Then, we used our filtered ensemble on the matching sections we found.",
        "formal_text": "Convert casual text to formal text: Wikipedia is the biggest multilingual source on the internet, but most articles aren't direct or close translations of each other. Still, we can't just ignore such a huge collection of"
    },
    {
        "casual_text": "We've got some exciting plans to make SWSD even better for subjectivity and sentiment analysis. First, we'll handpick and label a decent number of words that show up a lot but are super tricky to understand. Plus, we're going to tweak the SWSD system to take into account the vibe of the words around them. And hey, we’ve only just started with some basic, easy-to-get rules in this paper, but there are way cooler, more advanced methods we can try to really boost how SWSD works.",
        "formal_text": "Convert casual text to formal text: We've got some exciting plans to make SWSD even better for subjectivity and sentiment analysis. First, we'll handpick and label a decent number of words that show up a lot"
    },
    {
        "casual_text": "After the first translation is done, we tweak function R using that translation, and then we go through the whole search process again. This keeps happening, with R getting updated each time, until R stops changing between rounds (that's when it converges) or we hit the max number of rounds we set.",
        "formal_text": "Convert casual text to formal text: After the first translation is done, we tweak function R using that translation, and then we go through the whole search process again. This keeps happening, with R getting updated each time, until R stops changing"
    },
    {
        "casual_text": "It's pretty cool to see which tech and online stuff people are into. In the UK, Twitter and Facebook are big deals, but in India, Orkut used to be the go-to. Over in Singapore, blogging platforms like Wordpress are super popular.",
        "formal_text": "Convert casual text to formal text: It's pretty cool to see which tech and online stuff people are into. In the UK, Twitter and Facebook are big deals, but in India, Orkut used to be the go-to"
    },
    {
        "casual_text": "Looking at Table 5, the NB classification model throws in some random hashtags that don't really fit. On the other hand, LDA, being a generative model, leans towards suggesting broader hashtags like \"Information News\", \"mobile phone\", or \"Technology leaders\". But it misses out on more specific ones like \"WWDC\" or \"MAC OS Lion\". The IBM1 method also suggests some hashtags that have nothing to do with the topic, like \"2012 Jinshan Inc cloud computing\" or \"2012 spring and summer men's week\", just because they both have \"2012\" in them. In contrast, TSTM does a better job by suggesting specific hashtags that actually relate to the tweet's topic.",
        "formal_text": "Convert casual text to formal text: Looking at Table 5, the NB classification model throws in some random hashtags that don't really fit. On the other hand, LDA, being a generative model, leans towards"
    },
    {
        "casual_text": "We're using the \"bert-base-multilingual-cased\" model, which is pretty cool. It has 12 Transformer blocks, each with 768 hidden dimensions. Each block also has 12 self-attention heads. This model was trained on a mix of Wikipedia data from 104 different languages. It has around 179 million parameters. For whatever task we're doing, we add a linear output layer. If the task's output dimension is m, like m = 2 for PAWSX, the total number of parameters becomes 179 million + 768m + m.",
        "formal_text": "Convert casual text to formal text: We're using the \"bert-base-multilingual-cased\" model, which is pretty cool. It has 12 Transformer blocks, each with 768 hidden dimensions. Each block also has 12"
    },
    {
        "casual_text": "For translating between Japanese and English, we used the NTCIR-7 PAT-MT data (Fujii et al., 2008). For German to English, we trained on the Europarl v7 corpus (Koehn, 2005) and used the WMT 08 and WMT 09 test sets for development and testing, respectively. We stuck to the default settings in MOSES for PB SMT (Koehn et al., 2007), but for Japanese-English, we set the distortion-limit to 12 to match a recent baseline (Isozaki et al., 2012). We tuned the parameters on the development set using MERT (Och, 2003) and evaluated the translations on the test sets with BLEU (Papineni et al., 2002). To check for statistical significance, we used bootstrap sampling (Koehn, 2004) with the bleu kit.",
        "formal_text": "Convert casual text to formal text: For translating between Japanese and English, we used the NTCIR-7 PAT-MT data (Fujii et al., 2008). For German to English, we trained on"
    },
    {
        "casual_text": "Okay, so when it comes to describing images with words, there are three main approaches: template-based, retrieval-based, and sequence-learning-based. The template-based method works by first identifying the important objects, their features, and how they relate to each other in the image. Then, it fills in a pre-made sentence template with this info to create a full description (Elliott and Keller, 2013). The retrieval-based method, on the other hand, finds a similar image to the one you're looking at and uses its description as the description for your image (Karpathy et al., 2014). But both of these methods have a downside—they can only produce pretty standard, fixed sentences, and they heavily depend on the dataset of image descriptions they're given. Now, with the rise of deep learning, we have the sequence-learning-based method. This approach uses a Convolutional Neural Network (CNN) (He et al., 2016) to turn the image into a high-level representation, and then a Recurrent Neural Network (RNN) to translate that representation into a sentence. Vinyals et al. (2015) were among the first to use a specific type of CNN called Inception (Ioffe and Szegedy, 2015) to encode the image into a fixed-length vector. Then, they used a Long Short-Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) network to generate the caption. Xu et al. also contributed to this area.",
        "formal_text": "Convert casual text to formal text: Okay, so when it comes to describing images with words, there are three main approaches: template-based, retrieval-based, and sequence-learning-based. The template-based method works"
    },
    {
        "casual_text": "So, in short, we’re suggesting a neat, compact way to represent words that’s calculated beforehand, so it doesn’t slow things down when we’re actually using it. This method captures how likely a word is to belong to a certain category, which is something basic binary lists can’t do. Plus, it works well with other features that have been successful in older systems (shoutout to Ratinov and Roth, 2009, and Luo et al., 2015). Another cool thing is that since WiFiNE does a good job with entity types, the word representations are pretty solid. This approach can handle less common words and isn’t too bothered by the noise that comes with distant supervision.",
        "formal_text": "Convert casual text to formal text: So, in short, we’re suggesting a neat, compact way to represent words that’s calculated beforehand, so it doesn’t slow things down when we’re actually using it. This"
    },
    {
        "casual_text": "Basically, the core SIGN feature structure, especially the conceptual ones, should be consistent across all languages. Based on this idea, the only part that might need tweaking when creating a grammar for English synthesis is the syntactic feature structure.",
        "formal_text": "Convert casual text to formal text: Basically, the core SIGN feature structure, especially the conceptual ones, should be consistent across all languages. Based on this idea, the only part that might need tweaking when creating a grammar for"
    },
    {
        "casual_text": "Paracrawl has over 100 million sentences. We trimmed it down to 22.2 million by applying some basic filtering methods.",
        "formal_text": "Convert casual text to formal text: Paracrawl has over 100 million sentences. We trimmed it down to 22.2 million by applying some basic filtering methods."
    },
    {
        "casual_text": "We need to use both the grammar stuff and the meaning details from DICT for this process.",
        "formal_text": "Convert casual text to formal text: We need both the grammar stuff and the meaning details from DICT for this process."
    },
    {
        "casual_text": "The accuracy difference between the test set and dev set could be because we're only using a part of ConceptNet. This subset was picked based on the vocabulary from the training and development data. But the test data might have words that aren’t in this subset. So, there might be hardly any or even no connections for the test data in the chosen subset. That’s why the accuracy for the test data in the model without Q is pretty similar to the accuracy for the dev data in the model without P A Rel and QA Rel.",
        "formal_text": "Convert casual text to formal text: The accuracy difference between the test set and dev set could be because we're only using a part of ConceptNet. This subset was picked based on the vocabulary from the training and development"
    },
    {
        "casual_text": "It's tricky for the paraphrase identification module to catch all the event arguments without any context clues. So, we decided to throw in some argument role info to help out, especially when the event arguments in the sentence are way off from the actual event mentions or don’t match up well between different event pairs. Here’s how we do it: First, we run SRL on the two sentences we’re looking at. Then, we mix the token embeddings we get from the BERT encoder with the argument label embeddings. This combo gets fed into a semantic integration module to create a joint embedding. Finally, we use pooling strategies to pull out the event embeddings e_i and e_j, and use those to figure out the event semantic similarity.",
        "formal_text": "Convert casual text to formal text: It's tricky for the paraphrase identification module to catch all the event arguments without any context clues. So, we decided to throw in some argument role info to help out, especially when the event"
    },
    {
        "casual_text": "(1) Data-Sensitive Convolution. Using SetConv, each input sample gets linked to a set of weights that are calculated based on how it relates to the minority class. This data-sensitive convolution lets the model tailor the feature extraction for each sample, which can make the model perform better.",
        "formal_text": "Convert casual text to formal text: (1) Data-Sensitive Convolution. Using SetConv, each input sample gets linked to a set of weights that are calculated based on how it relates to the"
    },
    {
        "casual_text": "We should mention that our model is a bit unusual because it doesn't really care if die 4 and Frau 5 stay next to each other or not. It only focuses on whether their order gets flipped.",
        "formal_text": "Convert casual text to formal text: We should mention that our model a bit unusual because it doesn't really care if die 4 and Frau 5 stay next to each other or not. It only focuses on whether their order gets"
    },
    {
        "casual_text": "So, you have this thing called h_t, which is made up of h_1_t through h_K_t. Each of these h_t things is a matrix with K rows and p columns, where p is just d divided by K. Then, there's this matrix W, which is a collection of smaller matrices W_1 to W_K.",
        "formal_text": "Convert casual text to formal text: So, you have this thing called h_t, which is made up of h_1_t through h_K_t. Each of these h_t things is"
    },
    {
        "casual_text": "Here, h(L) represents the output word representations generated by the L-layer GCN network, and f is a maxpooling function that converts the input into the subtree vector, h_subtree. On top of the subtree representation, we also got a representation for the trigger, h_trg, and one for the entity, h_ent.",
        "formal_text": "Convert casual text to formal text: Here, h(L) represents the output word representations generated by the L-layer GCN network, and f is a maxpooling function that converts the input into the sub"
    },
    {
        "casual_text": "For each t j, we grab the related event E j by looking at t j and the event entities found in the closest Seg p and Seg q that have entities. Take this example: [moving, [people, Jamaica, Dominican Republic, west, Mexico] ] From this, it's simple to spot the two events in sentence 1a) based on the terms \"killed\" and \"moving\". Unlike the triplets (two named entities and a connector) used in (Filatova and Hatzivassiloglou 2003), our model allows for events to have as many event entities as needed, which is more realistic. Plus, we can see that \"killing\" involves \"people\", \"storm\", \"Jamaica\", and so on, while \"moving\" involves \"Jamaica\", \"Dominican Republic\", and so forth.",
        "formal_text": "Convert casual text to formal text: For each t j, we grab the related event E j by looking at t j and the event entities found in the closest Seg p and Seg q that have entities."
    },
    {
        "casual_text": "We owe a big thanks to the program committee and the secondary reviewers they brought in for some papers. With so many submissions this time, their workload was way bigger than usual. We’re super grateful for their help, especially since most of them managed to get everything done on time (though we did have to give a few gentle reminders after the deadline). We hope the reviewers’ feedback was helpful and constructive for everyone—whether it was to boost your chances for a future submission or to polish your paper if it got accepted. We know the timeline was tight, so thanks to all the authors for getting your final versions in on time.",
        "formal_text": "Convert casual text to formal text: We owe a big thanks to the program committee and the secondary reviewers they brought in for some papers. With so many submissions this time, their workload was way bigger than usual. We"
    },
    {
        "casual_text": "In real-world use, we went with a bigram model because it was quick to implement and performed pretty well in our tests.",
        "formal_text": "Convert casual text to formal text: In real-use, we went with a bigram model because it was quick to implement and performed pretty well in our tests. Convert casual text to formal text: In real-use, we"
    },
    {
        "casual_text": "For WMT15, they used the German to English dataset (DeEn) with 4.5 million sentences (Sennrich et al., 2016). They did 32,000 merge operations and shared the vocabulary between the two languages.",
        "formal_text": "Convert casual text to formal text: For WMT15, they used the German to English dataset (DeEn) with 4.5 million sentences (Sennrich et al., 2016). They did 32,000 merge operations"
    },
    {
        "casual_text": "When creating a document using this model, you start by picking a collection c (which you can see in the data). Next, you select a topic z and toss a virtual coin x to decide if you should grab words from the shared topic-word distribution or the collection-specific one for that topic. The chance of x being 1 or 0 is determined by a Beta distribution (kind of like a two-part version of the Dirichlet distribution). Oh, and you can also think about how long the document will be, but that part doesn’t really change how the model works, so we’ll skip over it here and in other places.",
        "formal_text": "Convert casual text to formal text: When creating a document using this model, you start by picking a collection c (which you can see in the data). Next, you select a topic z and toss"
    },
    {
        "casual_text": "The system needed to be tweaked for different customer data, but the topics in the documents were all over the place—covering stuff like fashion, retail, and computer jargon. So, the translation system had to be big enough to handle all these areas and give accurate translations for each one.",
        "formal_text": "Convert casual text to formal text: The system needed to be tweaked for different customer data, but the topics in the documents were all over the place—covering stuff like fashion, retail, and computer jargon. So,"
    },
    {
        "casual_text": "The original paper by Boratko et al. (2020) didn't take into account how often answers appear during fine-tuning. Basically, they treated all answers equally, whether they were the most common or the least common ones for each question. When we tested this on our development set, we noticed that giving each answer a weight based on the square root of its frequency worked better than either treating all answers the same way (like in the original paper) or just using the raw frequency of the answers. So, we decided to use this square root weighting method for fine-tuning all our models.",
        "formal_text": "Convert casual text to formal text: The original paper by Boratko et al. (2020) didn't take into account how often answers appear during fine-tuning. Basically, they treated all answers equally"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. First, to create a summary of the conversation, we need to organize the statements by their numbers, which means ranking them based on their scores. You can check out Table 4 for how this ranking turned out. Now, let’s say we have a compression rate called COM P, which can be anywhere from 1 to 100. The number of statements that get picked as \"relevant\" using a specific scoring method (P N r) depends on the total number of statements in the conversation. The formula for this is: P N r = (COM P / 100) * Number total. Once we decide on a compression rate (COM P), the top-ranked P N r statements will automatically be marked as relevant. Going back to the example in Table 1, the summaries we get from this process are shown in Table 5.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. First, to create a summary of the conversation, we need to organize the statements by their numbers, which means ranking them"
    },
    {
        "casual_text": "Another thing to point out is that terms are a super concise way to show a bunch of equations, which is pretty cool. They naturally make you think of using \"and\" statements, which is another neat feature of CLG. Plus, these \"and\" statements are a great way to keep things tidy when dealing with stuff that's not fully clear. They kind of keep the confusion in one place.",
        "formal_text": "Convert casual text to formal text: Another thing to point out is that terms are a super concise way to show a bunch of equations, which is pretty cool. They naturally make you think of using \"and\" statements, which"
    },
    {
        "casual_text": "Let's break this down in simpler terms. First, let's call the previous memory graph G x1 and the updated one Gx j. The R-GCN layer updates each entity's hidden state by looking at the hidden states of its neighbors, but it does this separately for each type of connection. After that, it adds up all these updates across different types and then runs the result through a GELU activation function (Hendrycks and Gimpel, 2016). To calculate the hidden state of an entity e j in the next layer (l + 1), we use a residual connection (He et al., 2016) to keep the original information of the entity, not just its neighbors, and then we apply layer normalization.",
        "formal_text": "Convert casual text to formal text: Let's break this down in simpler terms. First, let's call the previous memory graph G x1 and the updated one Gx j. The R-GCN layer updates"
    },
    {
        "casual_text": "Imitation Learning is all about learning skills by watching demos. It's shown some cool potential in areas like fixing the exposure bias issue in structured prediction (Zhang et al., 2019b), helping non-autoregressive translation models by sharing knowledge (Gu et al., 2018; Wei et al., 2019), and even teaching dialogue systems how to reward themselves (Li et al., 2019b). In our project, we have a regular dialogue model acting as a student, trying to copy a scenario-based dialogue model, not just in the final output but also in the middle layers.",
        "formal_text": "Convert casual text to formal text: Imitation Learning is all about learning skills by watching demos. It's shown some cool potential in areas like fixing the exposure bias issue in structured prediction (Zhang et al.,"
    },
    {
        "casual_text": "In these rules, each part of the tree has information that’s basically a statement or idea, kind of like a sentence. It’s written in a special format called an LT(L ') wff, which is just a fancy way of saying it’s a formula or expression used in logic.",
        "formal_text": "Convert casual text to formal text: In these rules, each part of the tree has information that’s basically a statement or idea, kind of like a sentence. It’s written in a special format called an LT"
    },
    {
        "casual_text": "The main way people try to solve this problem is through something called multi-domain text classification (MDTC) (Li and Zong, 2008). MDTC works well when you have labeled data from multiple domains, but not enough of it to train a really good classifier. Deep learning models have been pretty successful in this area (Wu and Guo, 2020; Wu et al., 2021). Most recent MDTC methods use a shared-private approach. This means they split the data into two parts: one part is shared across all domains to capture general, domain-invariant knowledge, and the other part is specific to each domain to focus on domain-specific details. To make sure these shared and domain-specific spaces don’t overlap too much, they use something called adversarial training (Goodfellow et al., 2014). This helps prevent domain-specific features from sneaking into the shared space, which would just create extra, unnecessary information (Liu et al., 2017). In adversarial training, a special domain discriminator is trained to work against a shared feature extractor. The goal is to minimize differences between domains. Once the domain discriminator and the shared feature extractor balance each other out, the shared features are considered domain-invariant and can be used for classification.",
        "formal_text": "Convert casual text to formal text: The main way people try to solve this problem is through something called multi-domain text classification (MDTC) (Li and Zong, 2008). MDTC works well when you have labeled"
    },
    {
        "casual_text": "Lucy is a prototype for a natural language interface, created by the Lingo group at MCC (Rich et al. 1987). One of the main goals was to make an interface system that can work across different applications, so modularity was a big deal in the design. Figure 3 shows the basic setup—it’s a typical conduit model where control moves from syntax to semantics, then to discourse and pragmatics.",
        "formal_text": "Convert casual text to formal text: Lucy is a prototype for a natural language interface, created by the Lingo group at MCC (Rich et al. 1987). One of the main goals was to make an"
    },
    {
        "casual_text": "We got people from Amazon Mechanical Turk to chat with our models. The online tests happened on the same platform where we collected the data, but this time, the supporter role was played by a model. Each person chatted with two different models, and we mixed up the order randomly to make sure no one got used to one model over the other. After the chats, we asked the participants to compare the two models by answering some questions.",
        "formal_text": "Convert casual text to formal text: We got people from Amazon Mechanical Turk to chat with our models. The online tests happened on the same platform where we collected the data, but this time, the supporter role was played by a model"
    },
    {
        "casual_text": "We’ve introduced ORB, an open reading benchmark that’s meant to thoroughly test reading comprehension systems. It checks how well these systems can handle different types of natural language, make reliable predictions, and answer questions that aren’t from the same source as the training data. As more cool and useful reading comprehension datasets come out, this benchmark will keep growing. We’re hoping it’ll push research toward building better, more general reading systems.",
        "formal_text": "Convert casual text to formal text: We’ve introduced ORB, an open reading benchmark that’s meant to thoroughly test reading comprehension systems. It checks how well these systems can handle different types of natural language, make reliable predictions, and"
    },
    {
        "casual_text": "Right now, we're working on a version of our model that we can run on super powerful computers called massively parallel machines. Specifically, we're looking at IXM and SNAP, which are two examples of these kinds of machines.",
        "formal_text": "Convert casual text to formal text: Right now, we're working on a version of our model that we can run on super powerful computers called massively parallel machines. Specifically, we're looking at IXM and"
    },
    {
        "casual_text": "The N-of-AD. I algorithm, which you can find explained in section 6, is a function that helps figure out the main noun, with a certain property P, that an adjective is talking about. It's written in a special language used for defining lexicons and is also built into the lexicon system as a handy tool. When rules use this function, they can make the translation of an adjective depend on the noun phrase it modifies having specific traits. Oh, and by the way, the N-of-ADJ function can also find the noun when the adjective is used as a pronoun-like modifier or when it's in the VP predicative form.",
        "formal_text": "Convert casual text to formal text: The N-of-AD. I algorithm, which you can find explained in section 6, is a function that helps figure out the main noun, with a certain property P, that an adjective"
    },
    {
        "casual_text": "We made these tables by running Anymalign for the same amount of time in every setup. That's why bigger length parameter values result in smaller tables—check out the specifics in (Lardilleux et al., 2011b).",
        "formal_text": "Convert casual text to formal text: We made these tables by running Anymalign for the same amount of time in every setup. That's why bigger length parameter values result in smaller tables—check out the specifics in (Lard"
    },
    {
        "casual_text": "Simile processing is all about three main areas: spotting similes, creating similes, and understanding what they mean. Most of the work in this field has been about finding similes and figuring out their parts (Niculae, 2013; Niculae and Danescu-Niculescu-Mizil, 2014; Liu et al., 2018; Zeng et al., 2020). Lately, there's been more focus on turning regular sentences into similes (Chakrabarty et al., 2020b). Chakrabarty et al. (2021b) looked at how well pre-trained language models (PLMs) can handle recognizing things like textual entailment related to similes. When it comes to understanding similes, studies like Qadir et al. (2016), Xiao et al. (2016), Bar et al. (2020), and Zheng et al. (2019) have used statistical methods and embedding similarities to rank properties that often go with simile components. Chakrabarty et al. (2021a) took a different approach, using PLMs to either pick or create continuations for stories that involve similes. Unlike these studies, we're looking at how well PLMs can figure out the shared properties in similes.",
        "formal_text": "Convert casual text to formal text: Simile processing is all about three main areas: spotting similes, creating similes, and understanding what they mean. Most of the work in this field has been about finding simile"
    },
    {
        "casual_text": "When we look at the mistakes in the same sentences as before, we see that some types of errors keep popping up in both cases, like issues with coordination, connecting conjunctions, modifiers, and determiners. But, training with constituency trees has its own unique set of problems. First, possessive constructions often mess up because the possessor (the thing owning something) doesn't get attached to the possessed (the thing being owned) as often as it should. This might be because the genitive possessor isn't linked to the possessed in the constituency treebank, so the parser doesn't learn how to handle this relationship. Second, it's a bit harder to figure out the arguments (things related to the verb) when there are at least two verbal elements in a clause. This is especially tricky with adverbial participles and infinitives. In Figure 6, you can see how the trees differ. The noun \"pecsenyéjükkel\" (which means \"with their roast\") is correctly linked to the adverbial participle in one analysis, but it's connected to the main verb in the other. Third, finding the root node of the sentence can also be a challenge in this setup. It's been noted that preconversion can work better for finding the root node in English, so this might be a language-specific thing, showing an interesting difference between English and Hungarian. Despite these issues, training on constituency trees does help with identifying multiword named entities.",
        "formal_text": "Convert casual text to formal text: When we look at the mistakes in the same sentences as before, we see that some types of errors keep popping up in both cases, like issues with coordination, connecting conjunctions, modifiers, and determine"
    },
    {
        "casual_text": "We looked at how different sentence lengths affected the results to see how length-related noise injection played a role. Short sentences might be more affected by length constraints, and the opposite might be true for longer ones. We left out the longest group (over 80 tokens) because it had three sentences and some serious length issues. In Table 3, our method with LDPE and a noise window of [2, 2] did way better than the basic Transformer by 3.22 points (50.81 vs. 47.59) in BLEU for the shortest sentences (1 to 10 tokens). Other setups also did better than the basic Transformer, though the differences weren’t huge. One thing that stood out was that the basic Transformer tended to produce really short translations for long sentences, while LDPE and LRPE gave longer outputs. This is good for avoiding under-translation issues in NMT.",
        "formal_text": "Convert casual text to formal text: We looked at how different sentence lengths affected the results to see how length-related noise injection played a role. Short sentences might be more affected by length constraints, and the opposite might be true for"
    },
    {
        "casual_text": "The two kids are represented using two separate sets of weight matrices.",
        "formal_text": "Convert casual text to formal text: The two kids are represented using two separate sets of weight matrices. Convert casual text to formal text: The two kids are represented using two separate sets of weight matrices."
    },
    {
        "casual_text": "To help the entity-pair embeddings remember the original attribute features, we add shortcuts from the input features to the output layer in the GNN model. Basically, we make sure F = F, meaning the input and output sizes for each GNN layer are the same. We throw in a shortcut connection between the input and output layers, and to get the final node representation, we just add h0i and hLi together, element by element.",
        "formal_text": "Convert casual text to formal text: To help the entity-pair embeddings remember the original attribute features, we add shortcuts from the input features to the output layer in the GNN model. Basically, we make sure"
    },
    {
        "casual_text": "In this part, we're planning to create a basic discourse parser using the representation we came up with. This parser will take a sentence and spit out QA pairs for all the discourse relations in that sentence. We'll train it on the data we collected. Just like in earlier work on discourse parsing (check out Section (1)), our parser will have three steps: (i) predicting the question prefix, (ii) generating the actual question, and (iii) coming up with the answer.",
        "formal_text": "Convert casual text to formal text: In this part, we're planning to create a basic discourse parser using the representation we came up with. This parser will take a sentence and spit out QA"
    },
    {
        "casual_text": "Here,  represents the chance that the community will switch emotions between two time periods, t and t + 1. The letter b stands for the total number of times the emotion state changes across all the time intervals from 1 to T.",
        "formal_text": "Convert casual text to formal text: Here,  represents the chance that the community will switch emotions between two time periods, t and t + 1. The letter b stands for the total number of times the emotion state changes across"
    },
    {
        "casual_text": "SST-2, also known as the Stanford Sentiment Treebank, has around 70,000 sentences tagged as either positive or negative (thanks to Socher et al. in 2013). We took the pre-trained RoBERTa base model and tweaked it for this sentiment classification task. As a result, we got an accuracy of 94.5 on the development set.",
        "formal_text": "Convert casual text to formal text: SST-2, also known as the Stanford Sentiment Treebank, has around 70,000 sentences tagged as either positive or negative (thanks to Socher et al in 2013). We"
    },
    {
        "casual_text": "Alright, let’s break it down and make it simple. To write a line of iambic pentameter, you need to arrange words so that they have 10 syllables, with the pattern switching between stressed and unstressed. Like this: 010 1 0 10 101. For instance, \"Attending on his golden pilgrimage\" follows that pattern. Now, following what Ghazvininejad and Knight said in 2015, we use 0 for unstressed syllables and 1 for stressed ones. So, the structure of a Shakespearean sonnet is basically ((01) 5 ) 14. To figure out the stress patterns for individual words, we use the CMU pronunciation dictionary, but we treat primary and secondary stresses the same. For example:",
        "formal_text": "Convert casual text to formal text: Alright, let’s break it down and make it simple. To write a line of iambic pentameter, you need to arrange words so that they have 10 s"
    },
    {
        "casual_text": "Another party got wild, and this time it wasn't just Bobby or some guy named Eddie messing around. Someone got careless, put their finger on the trigger, and things got sloppy.",
        "formal_text": "Another party got wild, and this wasn't just Bobby or some guy named Eddie messing around. Someone got careless, put their finger on the trigger, and things got sloppy. Convert casual text: Another party got wild"
    },
    {
        "casual_text": "EMOTyDA has dialogues with two people talking (dyadic) and groups of people talking (multi-party). So, we ran experiments splitting these conversations into dyadic, multi-party, and the whole dataset for our multi-task framework, which also uses different types of data (modalities). We also tested the framework with different attention methods to see which ones work best for the whole EMOTyDA dataset. Plus, we included results from basic baselines like combining features at different levels—feature level, hidden state level, and hypothesis level. The main goal here is to see how emotions affect deciding the dialogue act (DA) of a sentence when looking at multiple types of data. We’re not focusing on improving or analyzing the emotion recognition (ER) part; we see it as a side task helping the main one, which is dialogue act classification (DAC). So, the results we’re reporting are all about the DAC task and its different setups. Table 3 shows how all the models performed. As you can see, the text-only model did the best among the single-modality setups. Adding audio or visual features separately made it better, but combining visual and text features gave the best results overall across all the dataset combinations.",
        "formal_text": "Convert casual text to formal text: EMOTyDA has dialogues with two people talking (dyadic) and groups of people talking (multi-party). So, we ran experiments splitting these conversations into dya"
    },
    {
        "casual_text": "The adversarial detector is basically a machine-learning classifier. It looks at how the model reacts, using something called WDR(x, f), and then decides if the input x is an adversarial attack or not. To train this model, we follow a multi-step process:",
        "formal_text": "Convert casual text to formal text: The adversarial detector is basically a machine-learning classifier. It looks at how the model reacts, using something called WDR(x, f), and then decides if"
    },
    {
        "casual_text": "We looked at our baselines under these conditions: 1. **No-Reg**: This is when we don't use any regularizer at all. 2. **Dropout**: Here, we apply the dropout technique to the final linear layer (the classification layer) in E. 3. **L2-norm**: In this case, we add an L2-norm regularizer as weight decay to the optimizer.",
        "formal_text": "Convert casual text to formal text: We looked at our baselines under these conditions: 1. **No-Reg**: This is when we don't use any regularizer at all. 2. **Dropout**: Here, we"
    },
    {
        "casual_text": "When you're dealing with long chunks of text that might be part of a bigger story, like ongoing news updates, creating these \"lexical chains\" gets complicated. Plus, just counting how many times words repeat doesn't really help with understanding synonyms or how words are connected in meaning. This can make it tricky to figure out if different parts of the text actually fit together, especially if they don't share many words.",
        "formal_text": "Convert casual text to formal text: When you're dealing with long chunks of text that might be part a bigger story, like ongoing news updates, creating these \"lexical chains\" gets complicated. Plus, just counting how many"
    },
    {
        "casual_text": "Lemmatization is all about figuring out the base form (lemma) of a word from its inflected form. For example, turning \"atmest\" into \"atmen.\" This process is pretty straightforward for languages with simple grammar, like English, but it gets way more complicated with languages like Finnish. Essentially, it's like doing the opposite of what happens when you add endings to words to change their form. This idea has been talked about in a few studies (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where they look at how you can generate an inflected word by starting with the base form and adding the right ending.",
        "formal_text": "Convert casual text to formal text: Lemmatization is all about figuring out the base form (lemma) of a word from its inflected form. For example, turning \"atmest\" into \""
    },
    {
        "casual_text": "Collapsed sampling tends to take a long time to get to the right answer. This is a big issue for us, especially since we start with a segmentation provided by a morphological analyzer. The analyzer works by splitting text based on fixed rules, so the initial segmentation it gives is pretty consistent. However, this means that errors, especially with unknown words, happen in a predictable way. Basically, we start off close to the best possible solution, but not quite there. The problem is that the collapsed Gibbs sampler can get stuck in this not-quite-right starting point. That's why we often just randomly pick the initial segmentation instead. Sentence-based block sampling can also get stuck if it starts with a consistent segmentation (Liang et al., 2010).",
        "formal_text": "Convert casual text to formal text: Collapsed sampling tends to take a long time to get to the right answer. This is a big issue for us, especially since we start with a segmentation provided by a"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. First, we can use a function  that takes pairs from sets  and  and maps them to binary numbers of length n. This can be done using a linear transformation with a matrix A, which is a big table of numbers. Each row in this matrix represents how a specific pair (, ) should be mapped to a binary number of length n. Now, let’s prove this works step by step. We’ll use induction to show that at each step, the model can figure out the next state and update the stack based on the input it gets. Imagine we have a sequence of inputs x1, x2, ..., xn. At each step t, the model has a hidden state vector ht, which is made up of two parts: qt, which is a one-hot encoding of the current state, and t, which is a representation of the stack, kind of like a Cantor set. When the model gets a new input xt+1, it uses this to calculate the next hidden state ht+1, which includes the new state qt+1 and the updated stack representation t+1. After processing all the inputs, if the final state qn is one of the accepted states in the set F, the sequence is accepted. If not, it’s rejected. So, in short, the model keeps track of the state and stack, updates them based on each input, and decides whether to accept or reject the sequence at the end.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. First, we can use a function  that takes pairs from sets  and  and maps them to binary numbers"
    },
    {
        "casual_text": "In our model, we need a really solid way to represent entities in a Knowledge Graph (KG). To do this, the KG embedding model has to not only capture typical relationship patterns in the KG but also handle semantic hierarchies, which are super common in real-world situations. So, we decided to use Hierarchy-Aware Knowledge Graph Embedding (HAKE) to figure out the hidden structure of the KG. HAKE works by converting the KG into a polar coordinate system. The \"modulus\" part helps represent entities at different levels in the hierarchy, while the \"phase\" part deals with entities that are at the same level. For a given triple (h, r, t), here's how HAKE works:",
        "formal_text": "Convert casual text to formal text: In our model, we need a really solid way to represent entities in a Knowledge Graph (KG). To do this, the KG embedding model has to not only capture typical relationship"
    },
    {
        "casual_text": "The 21st century Sejong Project is a Korean information initiative led by the Ministry of Culture and Tourism. It's named after King Sejong the Great, who created Hangeul. (http://www.sejong.or.kr/)",
        "formal_text": "Convert casual text to formal text: The 21st century Sejong Project is a Korean information initiative led by the Ministry of Culture and Tourism. It's named after King Sejong the Great, who created Hangeul"
    },
    {
        "casual_text": "The new setup lets the model keep track of how many target words are left to go at each step of decoding. Unlike the basic model, which only trims the end of sentences, this one can start shortening things right from the beginning.",
        "formal_text": "Convert casual text to formal text: The new setup lets the model keep track of how many target words are left to go at each step of decoding. Unlike the basic model, which only trims the end of sentences, this one"
    },
    {
        "casual_text": "Sure! Here's the informal version: \"Check out this link to the Hugging Face Transformers repository on GitHub. It's the one from commit c439752482759c94784e11a87dcbf08ce69dccf3.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: \"Check out this link the Hugging Face Transformers repository on GitHub. It's the one from commit c439752482759c"
    },
    {
        "casual_text": "We've come up with a way to handle multitask learning for two main parts of community Question Answering: figuring out if questions are related and picking the best answers. We also threw in a third, extra task: spotting good comments in a question-comment thread. Our approach has two steps: first, we use deep neural networks and structured conditional models, along with a feedforward neural network to create task-specific embeddings. Then, we use these embeddings in a pairwise CRF as part of a multitask model that tackles all three tasks together.",
        "formal_text": "Convert casual text to formal text: We've come up with a way to handle multitask learning for two main parts of community Question Answering: figuring out if questions are related and picking the best answers. We also"
    },
    {
        "casual_text": "2. Updating an entity-event database: Besides D (test), let’s say we also have access to a historical database of killings E (train) and a collection of news articles D (train) from before the time T. This kind of setup is pretty common in real-world scenarios and is what this paper is mainly about. It lets us use distantly supervised learning methods, which is pretty cool. The task itself is really important for society, but the NLP research community might also want a more scientific reason to care. We think police fatalities are a great test case for event extraction research. Fatalities are a specific type of event with clear meanings, so they avoid some of the trickier issues in this field (Hovy et al., 2013). Plus, this task ties into a lot of existing research on building knowledge bases (like Craven et al. (1998)). Lastly, we believe that NLP should, whenever possible, work on applications that matter to the public. Previous research showed that news text is valuable for this problem, but using computational methods could make it way less labor-intensive than doing everything manually.",
        "formal_text": "Convert casual text to formal text: 2. Updating an entity-event database: Besides D (test), let’s say we also have access to a historical database of killings E (train) and a collection"
    },
    {
        "casual_text": "There are two situations where COs (Content Objects) might change, but only one of them is actually a big deal. The other one is pretty straightforward and not really an issue. The easy case happens when a resource gets updated, like when it has more annotations or fixes some errors, but the old version stays available. In this situation, the long-term archive just creates a new version of the CO, and the old one stays as it is, still accessible. Now, there’s a bit of a conflict here. On one hand, users might find it useful to know there’s a new version of a CO. On the other hand, updating the metadata to reflect this new version would mess with the whole idea of long-term archival, which is supposed to keep things stable. We think the second point is way more important: there’s no need to link to the new version in the long-term archive or change the metadata. That said, it’s totally fine, from a long-term archival standpoint, to link from the new version to the old ones—as long as the old versions are archived first, which usually happens. To make things easier for users, the archive’s presentation layer can flip these links around without actually changing the metadata itself.",
        "formal_text": "Convert casual text to formal text: There are two situations where COs (Content Objects) might change, but only one of them is actually a big deal. The other one is pretty straightforward and not really an issue."
    },
    {
        "casual_text": "Sure! Here's a more casual version: While the average effect is helpful, it’s kind of a rough estimate. By averaging over random state vectors, we treat all features as if they’re the same. But what if we want to look at how Q affects things along certain specific directions, like breaking it down dimension by dimension?",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: While the average effect is helpful, it’s kind of a rough estimate. By averaging over random state vectors, we treat all"
    },
    {
        "casual_text": "Sure! So, while making D(P p ||P) as small as possible would give us the best results, it's way too hard to do that on a computer. So instead, we just look for close-to-best options for each pair of phrases.",
        "formal_text": "Convert casual text to formal text: Sure! So, while making D(P p ||P) as small as possible would give us the best results, it's way too hard that on a computer. So instead,"
    },
    {
        "casual_text": "In a more advanced grammar, we can make things more flexible by just adding a simple rule that lets us switch stuff around.",
        "formal_text": "Convert casual text to formal text: In a more advanced grammar, we can make things more flexible by just adding a simple rule to switch stuff around. Convert casual text to formal text: In a more advanced grammar, we"
    },
    {
        "casual_text": "Let’s break this down in a simpler way. We’re adding some depth maps to Figure 2 to emphasize this point. In Figure 1, you can see an example of APE (Argument Pair Extraction). On the left, there’s a review passage, and on the right, its rebuttal. Each sentence is labeled as Sent-i, where i is the sentence number. Similarly, Rev: Arg-i or Rep: Arg-i shows the i-th argument in the review or rebuttal. An argument can be made up of one or more sentences. In this example, two argument pairs are highlighted in green and blue. Now, the current method for finding these argument pairs isn’t the best. It combines the results of two smaller tasks, but it doesn’t directly model how the arguments in the pairs interact with each other. Plus, these two smaller tasks might not work well together. When people do this task, they usually start by picking an argument from the review. Then, they look for the matching argument in the rebuttal to form a pair. You could also start with the rebuttal and find the corresponding argument in the review. We took inspiration from this process and created a mutual guidance framework (MGF) to handle APE better. Our approach first uses a non-guided sequence tagger to identify the arguments in both the review and rebuttal.",
        "formal_text": "Convert casual text to formal text: Let’s break this down in a simpler way. We’re adding some depth maps to Figure 2 to emphasize this point. In Figure 1, you can see an example of APE (Argu"
    },
    {
        "casual_text": "In this project, we're trying to figure out how a word's type directly affects how its meaning and how often it's used changes over time. To prove this effect exists and to know what other factors to consider, we're using causal discovery algorithms. Along with the word's type, we're also looking at its frequency, how many meanings it has (polysemy), and its part of speech (POS) in our analysis. To build our causal graph, we're using the PC-stable algorithm (Colombo and Maathuis, 2014), which is a version of the PC algorithm (Spirtes et al., 2000) that doesn't depend on the order of variables. You can find more details about this in Appendix D.1. We're working with a mixed graphical model (Lauritzen, 1996; Lee and Hastie, 2015) that includes both continuous data (like frequency) and categorical data (like the word's type). That's why we're using constraint-based algorithms—they let us adjust the tests for conditional independence based on the different types of data we have.",
        "formal_text": "Convert casual text to formal text: In this project, we're trying to figure out how a word's type directly affects how its meaning and how often it's used changes over time. To prove this effect exists and"
    },
    {
        "casual_text": "Teams could keep submitting their work, but only the last successful upload to CodaLab during the evaluation period would count for the official ranking. Participants were encouraged to use the same parsing system for all inputs, but since it's kind of tricky to define what exactly counts as \"one\" parser, this wasn't strictly required.",
        "formal_text": "Convert casual text to formal text: Teams could keep submitting their work, but only the last successful upload to CodaLab during the evaluation period would count for the official ranking. Participants were encouraged to use the same parsing system for"
    },
    {
        "casual_text": "Okay, so here's the deal with how we define an \"effective question.\" It all revolves around this thing called the reference set, which we call RS. For every question q t, we figure out what RS is. At the very beginning, before any conversation starts, RS(q 0 ) includes every single object in the image. Basically, it’s a list of all the objects that are in the dataset and were given to the Oracle model. But here’s the thing: the human Oracles didn’t get this list. Now, as the conversation goes on, at each turn t, RS(q t ) is made up of the objects that were in RS(q t1 ) and have the same answer A for question q t as the one for the referent r. All these answers A are calculated using the Oracle we have, which is 79% accurate on the test set. So, in short:",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal with how we define an \"effective question.\" It all revolves around this thing called the reference set, which we call RS. For every question q"
    },
    {
        "casual_text": "Like other pre-training models (like UNILM, BERT, and XLM), we also use the multi-layer Transformer (Vaswani et al., 2017) as the main tool to encode sentences and get contextual representations for each word in a sentence. The multi-layer Transformer works by:",
        "formal_text": "Convert casual text to formal text: Like other pre-training models (like UNILM, BERT, and XLM), we also use the multi-layer Transformer (Vaswani et al., 2017"
    },
    {
        "casual_text": "That movie was awesome! It took a bit to get into at first, but it totally hooked me and kept me glued to the screen. I thought it was super good, like, really good. Just saying, there was one weird review that didn't make sense—like, it talked about something totally unrealistic.",
        "formal_text": "Convert casual text to formal text: That movie was awesome! It took a bit to get into at first, but it totally hooked me and kept me glued to the screen. I thought it was super good, like, really good"
    },
    {
        "casual_text": "There's some noise in how crossentropy and BLEU scores relate to each other at this scale, as shown in Appendix Figure 8. This makes the predictions a bit less precise and reliable. In real-life situations, you can usually gather small amounts of data in chunks and re-evaluate your predictions before deciding to move forward. These predictions give you a rough idea of the effort involved in improving low-resource machine translation. When building a real-world system, it's important to think carefully about whether it's better to just get more data and expect steady improvements, or go for more complex and unpredictable approaches.",
        "formal_text": "Convert casual text to formal text: There's some noise in how crossentropy and BLEU scores relate to each other at this scale, as shown in Appendix Figure 8. This makes the predictions a bit less precise"
    },
    {
        "casual_text": "• Adding event phrases from ConceptNet boosted the model's accuracy on the dev set by 9%. This is a good sign that using different Knowledge Graphs and Corpora (like we talked about in the intro) is working out. As Natural Language Understanding keeps getting better, this method of using commonsense relations should improve too.",
        "formal_text": "Convert casual text to formal text: • Adding event phrases from ConceptNet boosted the model's accuracy on the dev set by 9%. This is a good sign that using different Knowledge Graphs and Corpora"
    },
    {
        "casual_text": "We're suggesting a straightforward yet effective way to bring word lexicons into character-based representations for Chinese NER.",
        "formal_text": "Convert casual text to formal text: We're suggesting a straightforward yet effective way to bring word lexicons into character-based representations for Chinese NER. Convert casual text to formal text: We're suggesting"
    },
    {
        "casual_text": "• Unsupervised: the mapping that the unsupervised method figured out, as described in the paper by Artetxe et al. (2018).",
        "formal_text": "Convert casual text to formal text: • Unsupervised: the mapping that the unsupervised method figured out, as described in the paper by Artetxe et al (2018). Convert casual text to formal text: Con"
    },
    {
        "casual_text": "Feature frequency: This is another key factor that affects how we rank features. It's been talked about in studies by Hu and Liu (2004) and Blair-Goldensohn et al. (2008). Basically, if feature f1 shows up more often than feature f2 in opinion-related documents, we think f1 is more important. In real-world situations, it makes sense to give higher priority to features that pop up a lot. The reason? Missing a feature that’s mentioned all the time in opinion mining is a bigger deal than missing something that hardly ever comes up.",
        "formal_text": "Convert casual text to formal text: Feature frequency: This is another key factor that affects how we rank features. It's been talked about in studies by Hu and Liu (2004) and Blair-Goldensohn et"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. W represents the n-gram, which is just a fancy way of saying a sequence of words. The function f(.) counts how many times something happens, and f_match(.) looks at the biggest number of n-grams that show up in both the summary you made (let's call it C) and the real, expert-made summaries (we'll call those S). Now, if your summary C has n words and the gold summary S has u sentences, ROUGE-L is a way to measure how well your summary matches up with the real one.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in simpler terms. W represents the n-gram, which is just a fancy way of saying a sequence of words. The function f("
    },
    {
        "casual_text": "Paraphrasing has been around for a while and has a bunch of uses in different areas of natural language processing, like summarizing text (Cao et al., 2016), understanding sentences (Berant and Liang, 2014), and answering questions (Yu et al., 2018). Back in the day, people mostly used rule-based methods or statistical machine translation systems to create paraphrases (McKeown, 1980; Meteer and Shaked, 1988; Bannard and Callison-Burch, 2005).",
        "formal_text": "Convert casual text to formal text: Paraphrasing has around for a while and has a bunch of uses in different areas of natural language processing, like summarizing text (Cao et al., 2016"
    },
    {
        "casual_text": "When you're trying to figure out \"vp(John, Pl),\" the first \"wp\" part should work fine because \"loves\" is a transitive verb. Just make sure you don't mix up the variables.",
        "formal_text": "Convert casual text to formal text: When you're trying to figure out \"vp(John, Pl),\" the first \"wp\" part should work fine because \"loves\" is transitive verb. Just make sure"
    },
    {
        "casual_text": "• If the name field has two words, go with the second word if there's a comma, or the first word if there isn't. • If the name field has three words and a comma, grab the second and third words (probably the first and middle name). If it has three words without a comma, take the first and second words (again, likely the first and middle name). • If the name field has an email address, snag the part from the start up to a '.', ' ', or '-'. If the email is in camel case, take the part from the start up to the first capital letter.",
        "formal_text": "Convert casual text to formal text: • If the name field has two words, go with the second word if there's a comma, or the first word if there isn't. • If the"
    },
    {
        "casual_text": "Except for maybe trying to speed up the calculations, which isn't what this paper is about. This was from the 16th EAMT Conference, happening from May 28-30, 2012, in Trento, Italy.",
        "formal_text": "Convert casual text to formal text: Except for maybe trying to speed up the calculations, which isn't what this paper is about. This was from the 16th EAMT Conference, happening from May 28-30, 2012, in"
    },
    {
        "casual_text": "Based on what this paper found, we think it would be cool for future experiments to test how well the model does with lemmata it hasn't seen during training. Also, instead of counting triples, it might be better to count the number of unique lemmata when figuring out the size of the data set. Oh, and about that encoder-decoder model with hard monotonic attention—let's see how it performs!",
        "formal_text": "Convert casual text to formal text: Based on what this paper found, we think it would be cool for future experiments to test how well the model does with lemmata it hasn't seen during training. Also, instead of"
    },
    {
        "casual_text": "When we mix in back-off sampling (+BM) with type-based sampling (+TB) and hybrid type-based sampling (+HTB), it actually boosts the accuracy compared to models without mixing. On the other hand, direct mixing (+DM) messes things up and lowers the accuracy a lot. We found out that if the main text is way smaller than the annotated text, the importance of individual words in the main text gets overlooked. Interestingly, models with collapsed sampling (+CL) and mixing (+DM/+BM) did better than the basic model. But, the results from type-based sampling (+TB) make us think that if we keep running it for way more iterations, the models might end up in not-so-great places.",
        "formal_text": "Convert casual text to formal text: When we mix in back-off sampling (+BM) with type-based sampling (+TB) and hybrid type-based sampling (+HTB), it actually boosts the accuracy compared to models without mixing"
    },
    {
        "casual_text": "To see how well our new framework works, we ran some tests and compared the results to what was done before.",
        "formal_text": "Convert casual text to formal text: To see how well our new framework works, we ran some tests and compared the results done before. Convert casual text to formal text: To see how well our new framework works, we ran some"
    },
    {
        "casual_text": "But when we throw the LM score into the mix, things get a bit more complicated. The total cost of a derivation isn't just the sum of the costs from the previous hypernodes and the grammar rule anymore. Now, we need to tack on the cost from the language model, which is calculated based on the parts of the target language we're dealing with. This LM score is called a \"combination cost\" because it impacts how we combine hypernodes. The problem is, this score is kind of a pain to calculate—it depends on everything involved in the combination (the predecessor hypernodes and the translation rule). And because of that, the sorting strategy we talked about earlier can't guarantee that we'll always generate derivations in a nice, neat, monotonic order.",
        "formal_text": "Convert casual text to formal text: But when we throw the LM score into the mix, things get a bit more complicated. The total cost of a derivation isn't just the sum of the costs from the"
    },
    {
        "casual_text": "Machine reading comprehension (MRC) is all about finding the right answers in a passage based on a specific question (Devlin et al., 2019; Wen et al., 2021). In recent years, turning NLP tasks into MRC tasks has become pretty popular. For example, things like dependency parsing (Gan et al., 2021), relation extraction (Levy et al., 2017), named entity recognition (Li et al., 2020), and sentiment analysis (Chen et al., 2021; Mao et al., 2021) have all been approached this way. Unlike those earlier studies, we’re using an MRC framework to dig into the complex argumentative connections between two super-long documents.",
        "formal_text": "Convert casual text to formal text: Machine reading comprehension (MRC) is all about finding the right answers in a passage based on a specific question (Devlin et al., 2019; Wen e"
    },
    {
        "casual_text": "From what we know, Reinforcement Learning (RL) was first used to tackle this problem (Li et al., 2017). They combined RL with a GAN, where the discriminator's score was used as feedback to train the generator, creating a hybrid model. However, to train this RL part, Li et al. (2017) had to make some compromises. They used two methods to calculate rewards at each step when selecting words: one was a Markov Chain Monte Carlo (MCMC) sampling approach, and the other was a partial utterance scoring method. They mentioned that the MCMC method takes a lot of time, and the partial utterance scoring approach doesn't work as well because it leads to overfitting when too many partial sentences are added to the training set. But we also think that RL isn't the best option here, even aside from the MCMC's time issue. As we showed in our experiments in Section 5.1, a more straightforward, end-to-end differentiable GAN design can really boost performance in this text generation task.",
        "formal_text": "Convert casual text to formal text: From what we know, Reinforcement Learning (RL) was first used to tackle this problem (Li et al., 2017). They combined RL with a GAN, where"
    },
    {
        "casual_text": "Following Li et al. (2020), we used H as input for two binary classifiers to figure out where arguments start and end in the sentences. Once we had all the start and end positions, we ran another binary classifier to check if each pair of start and end positions (basically, every possible combination) could form a valid answer span. Just to clarify, the span classifier takes the start and end sentence parts from H and combines them as its input.",
        "formal_text": "Convert casual text to formal text: Following Li et al. (2020), we used H as input for two binary classifiers to figure out where arguments start and end in the sentences. Once we had all the start and end"
    },
    {
        "casual_text": "We brought on two professional annotators who are really good with English. Both of them have been doing English syntactic annotation for over ten years. One of them even taught English writing professionally for two years, giving high school and college students feedback, which is pretty similar to what we're doing here.",
        "formal_text": "Convert casual text to formal text: We brought on two professional annotators who are really good with English. Both of them have done English syntactic annotation for over ten years. One of them even taught English writing professionally for"
    },
    {
        "casual_text": "2. Next, arrange the groups in G j from highest to lowest based on their L 2 -norms. Then, look at the L 2 -norms of the B j -th and B j+1 -th items in the list. Take the average of these two and divide it by  t to get  jt.",
        "formal_text": "Convert casual text to formal text: 2. Next, arrange the groups in G j from highest to lowest based on their L 2 -norms. Then, look at the L 2 -norms"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: We start with some function q A x 0 applied to  A (x 1, x 2, x 3 ), which simplifies to A / 0 (1 B(a), 2 C(3 D(A *))). This is just another way of writing A(1 B(a), 2 C(3 D(A *))). Next, we replace 1 B(a) with q B B / 0 (a) (x 1 ), so now we have A(q B B / 0 (a) (x 1 ), 2 C(3 D(A *))). Then, we simplify q B B / 0 (a) (x 1 ) to q B B(a) (x 1 ), so it looks like A(q B B(a) (x 1 ), 2 C(3 D(A *))). Finally, we break down 2 C(3 D(A *)) into q C C(q D D(x 0 ) (x 3 )) (x 2 ), so the whole thing becomes A(q B B(a) (x 1 ), q C C(q D D(x 0 ) (x 3 )) (x 2 )). And that's it! It's just a series of steps where we keep simplifying and breaking things down until we get to the final expression.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: We start with some function q A x 0 applied to  A (x 1, x 2, x"
    },
    {
        "casual_text": "In this part, we'll check out how well the method we came up with works for classifying sentiment in QA-style stuff.",
        "formal_text": "Convert casual text to formal text: In this part, we'll check how well the method we came up with works for classifying sentiment in QA-style stuff. Convert casual text to formal text. Convert casual text to"
    },
    {
        "casual_text": "But this feature doesn't really help us pick between a doctor and a surgeon, because surgeons still study medicine too. If we run the experiment again, but this time focus on surgeons, the top result switches to \"patients died.\" This bigram seems to be a better way to tell the difference between those two groups in the BIOS model we trained.",
        "formal_text": "Convert casual text to formal text: But this feature doesn't really help us pick between a doctor and a surgeon, because surgeons still study medicine too. If we run the experiment again, but this time focus on surgeons"
    },
    {
        "casual_text": "Pronouns are kind of a big deal in sentences because they help writers spice up their writing with a bigger vocabulary and make more complex sentences. Basically, pronouns are like stand-ins in the text—they can take the place of a subject or an object, show ownership, point to places, or even refer back to people or things mentioned earlier. If you just ignore pronouns and don’t replace them with something useful, you might miss out on important stuff, like grammar or meaning. When we read, our brain automatically figures out what the pronoun is pointing to, so we understand the full picture. In NLP (Natural Language Processing), they do something similar with anaphora resolution. So, our plan is to use this info that would normally get lost and put it to work for AKE (Automated Knowledge Extraction).",
        "formal_text": "Convert casual text to formal text: Pronouns are kind of a big deal in sentences because they help writers spice up their writing with a bigger vocabulary and make more complex sentences. Basically, pronouns are like"
    },
    {
        "casual_text": "We're working with the gold standard dataset from Ling et al. (2013), which has 2581 English-Mandarin microblog sentence pairs. We randomly picked 1290 pairs for development and 1291 for testing. The normalizer model was trained on the corpora we extracted and filtered in section 3, using a total of 1.3 million normalization pairs. We tested the normalization of the sentences using four different setups. The first one just keeps the input sentence as it is, which we call \"No Norm.\" The second one uses a phrase-based model to normalize the sentence, and we'll call this \"Norm+phrase.\" The third setup uses a character-based model to create lattices, then decodes them with the phrase-based model, which we'll call \"Norm+phrase+char.\" Finally, we tested the same model but added training data from monolingual documents, and we'll refer to this as \"Norm+phrase+char+mono.\"",
        "formal_text": "Convert casual text to formal text: We're working with the gold standard dataset from Ling et al. (2013), which has 2581 English-Mandarin microblog sentence pairs. We randomly picked 1290 pairs for development"
    },
    {
        "casual_text": "Keep in mind that a single sample can cover multiple intents and slots. From what we've seen, we usually pick about 10 different samples for each intent and slot, as shown in Table 2.",
        "formal_text": "Convert casual text to formal text: Keep in mind that a single sample can cover multiple intents and slots, as shown in Table 2. From we've seen, we usually pick about 10 different samples for each intent and slot, as"
    },
    {
        "casual_text": "8. Right away, throw in the rice, green onions, and soy sauce, then give it a good stir to mix everything together.",
        "formal_text": "Convert casual text to formal text: 8. Right away, throw in the rice, green onions, and soy sauce. Then give a good stir to mix everything together. 8. Right away, throw in the rice, green onions,"
    },
    {
        "casual_text": "A lot of NLP methods treat words as the main building blocks. One big deal was when continuous representations of words came along (thanks, Bengio et al., 2003). These word embeddings are now the go-to in NLP. But, it's not so clear how to best handle a whole sentence, which has all kinds of tricky syntax and meaning stuff going on. In a sentence, you might have short-term and long-term connections between words. Right now, the usual approach is to see a sentence as a list of tokens (like characters or words) and run them through a recurrent neural network (RNN). The tokens are usually processed one by one, from left to right, and the RNN is supposed to \"remember\" the whole sequence in its internal states. The most popular and effective type of RNN is definitely the LSTM.",
        "formal_text": "Convert casual text to formal text: A lot of NLP methods treat words as the main building blocks. One big deal was when continuous representations of words came along (thanks, Bengio et al., 2003). These"
    },
    {
        "casual_text": "In Bangladesh, digital books aren't really taking off for a bunch of reasons, like the high cost of e-book readers and not enough people wanting them.",
        "formal_text": "Convert casual text to formal text: In Bangladesh, digital books aren't really taking off for a bunch of reasons, like the high cost of e-book readers and not enough people wanting them. Convert casual text to"
    },
    {
        "casual_text": "The comprehension task results showed that people were really paying attention to the story being told, with an average accuracy of 90% in their responses. The variation in scores was pretty small, with a standard deviation of 3.7%.",
        "formal_text": "Convert casual text to formal text: The comprehension task results showed that people were really paying attention to the story being told, with an average accuracy of 90% in their responses. The variation in scores was pretty small, with a standard deviation of"
    },
    {
        "casual_text": "9,“”407000",
        "formal_text": "Convert casual text: 9,“”407000  Convert casual text: 9,“”407000  Convert casual text:"
    },
    {
        "casual_text": "The Tanglish tweets mix both Tamil and English words. We changed the Tamil words into English using a Tamil to English mapping corpus. We used the NLTK library in Python to clean up the data. For any classification task, pre-processing is super important because it helps the classifier work better. We did some cleaning steps like removing stop words, lemmatization, and getting rid of special characters. For example, after cleaning, the tweet \"@Bala sundar ayyo sorry. . . antha line ah clarify pannama vittutu irukan[: drowsy]ok na solran( en appavum indha grant work ku vanthurukkaru, neenga en appava paakala pola. . . . en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaru[: yeah][: yeah] ' chiiiii karumam podinnngggg. . . asingama vaila vanthurum. . . . \" becomes \"bala sundar ayyo sorry antha line ah clarify pannama vittutu irukandrowsyok na solran en appavum indha grant work ku vanthurukkaruneenga en appava paakala pola en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaruyeahyeah chi karumam poding asingama vaila vanthurum. \"",
        "formal_text": "Convert casual text to formal text: The Tanglish tweets mix both Tamil and English words. We changed the Tamil words into English using a Tamil to English mapping corpus. We used the NLTK library in Python to clean"
    },
    {
        "casual_text": "News image captioning can be tricky because it needs to use both the picture and the text to create detailed, well-organized captions. Check out Table 4 for a human evaluation of the captions we made. The best model is marked in bold.",
        "formal_text": "Convert casual text to formal text: News image captioning can be tricky because it needs to use both the picture and the text to create detailed, well-organized captions. Check out Table 4 for a human evaluation of the captions"
    },
    {
        "casual_text": "The paper proves that dealing with uncertainties can be done in a natural, straightforward, and intuitive manner.",
        "formal_text": "Convert casual text to formal text: The paper proves that dealing with uncertainties can be a natural, straightforward, and intuitive manner."
    },
    {
        "casual_text": "The offline model is based on the idea that we start with a phonemic representation before words are formed, and then connect that to a meaning representation. This model doesn't include any written forms at the early stages. It works by processing one phoneme at a time. Once it's done with all the phonemes in a sequence, it does a linear transformation on the output from an LSTM layer that has 400 units. This leads to a fully connected layer with 400 neurons, which then connects to the output layer. The output layer has 300 units and uses a tangent activation function.",
        "formal_text": "Convert casual text to formal text: The offline model is based on the idea that we start with a phonemic representation before words are formed, and then connect that to a meaning representation. This model doesn't include any written"
    },
    {
        "casual_text": "Basically, we're looking at all the possible slots and intents for a given sequence. This approach helps us figure out how to \"encode\" the joint distribution of (y, z), which you can think of as the model's best guesses for slots and intents. In reality, though, the number of possible labels shoots up quickly as the sequence gets longer, so the combinations of intents and labels become way too many to handle. Back in 2006, Kim and team used something called N-best Sequence Entropy (NSE), which is all about the entropy of the N most likely interpretations. In our case, we can tweak it a bit and use it like this:",
        "formal_text": "Convert casual text to formal text: Basically, we're looking at all the possible slots and intents for a given sequence. This approach helps us figure out how to \"encode\" the joint distribution of (y,"
    },
    {
        "casual_text": "(2) Most triggers are used without any extra stuff to make them stronger or weaker; and (3) punctuation marks are actually pretty helpful for showing where something starts and ends, like when someone is directly quoting someone else or talking about a holder. The reasons people disagree about meaning come down to: (1) triggers that are nouns and have important grammatical roles, (2) triggers that describe a state or condition, (3) triggers that show someone’s opinion or how they know something, and (4) triggers that have a ton of different meanings.",
        "formal_text": "Convert casual text to formal text: (2) Most triggers are used without any extra stuff to make them stronger or weaker; and (3) punctuation marks are actually pretty helpful for showing where something starts and ends, like when someone is directly"
    },
    {
        "casual_text": "We ran all our experiments with the Moses phrase-based system (Koehn et al., 2007), including lexicalized reordering. To make things faster, we used cube pruning with a pop limit of 1,000 during both tuning and evaluation. For the symmetrised alignments, we went with the grow-diag-final-and heuristic.",
        "formal_text": "Convert casual text to formal text: We ran all our experiments with the Moses phrase-based system (Koehn et al., 2007), including lexicalized reordering. To make things faster,"
    },
    {
        "casual_text": "We looked at eight different ways to check how well two texts match up, specifically comparing the system summary to the reference summary. BERTScore (BScore) checks for soft overlap by looking at the contextual BERT embeddings of tokens in both texts (Zhang et al., 2020). MoverScore (MScore) uses a distance measure on contextualized BERT and ELMo word embeddings (Zhao et al., 2019). Sentence Mover Similarity (SMS) finds the minimum distance matching between texts based on sentence embeddings (Clark et al., 2019). Word Mover Similarity (WMS) measures similarity by finding the minimum distance matching between texts represented as bags of word embeddings (Kusner et al., 2015). JS divergence (JS-2) measures the Jensen-Shannon divergence between the bigram distributions of the two texts (Lin et al., 2006). ROUGE-1 and ROUGE-2 measure the overlap of unigrams and bigrams, respectively (Lin, 2004). ROUGE-L looks at the overlap of the longest common subsequence between the two texts (Lin, 2004). We used the recall version of all these metrics (since the Pyramid method for human evaluations is based on recall) except for MScore, which doesn't have a specific recall version.",
        "formal_text": "Convert casual text to formal text: We looked at eight different ways to check how well two texts match up, specifically comparing the system summary to the reference summary. BERTScore (BScore) checks for soft overlap by looking at"
    },
    {
        "casual_text": "This paper talks about experiments done on tweet datasets in five languages: English, French, Spanish, German, and Italian. The basic idea is pretty straightforward: instead of sticking with a single-language model, we went with a multilingual one. We trained it on a big collection of English tweets, the original non-English tweets, and their automatic translations. We picked the XLM-RoBERTa model, which is a multilingual transformer model from Lample and Conneau (2019), and used a data-augmentation trick with machine translation. We looked into how pre-training with English data and data-augmentation affected things. We also compared how our multilingual models performed against their single-language versions for French (Martin et al., 2020) and English (Liu et al., 2019) and found some cool improvements.",
        "formal_text": "Convert casual text to formal text: This paper talks about experiments done on tweet datasets in five languages: English, French, Spanish, German, and Italian. The basic idea is pretty straightforward: instead of sticking with a single-language"
    },
    {
        "casual_text": "In this part, we’re looking at two ways to handle word ambiguity. The first one treats words as having distinct meanings and needs labeled data (like WordNet). The second one uses continuous meaning spaces (like BERT) and lets us work with languages that don’t have as many resources.",
        "formal_text": "Convert casual text to formal text: In this part, we’re looking at two ways to handle word ambiguity. The first one treats words as having distinct meanings and needs labeled data (like WordNet). The second"
    },
    {
        "casual_text": "Speed was a big deal when building the database and when using it. Usually, it takes about 20 seconds to create or rebuild a database, like when you're testing new rules or updated versions of rules. The system can also handle around 25 phoneme-to-grapheme conversions per second. This whole phoneme-to-grapheme thing is connected to a diphone speech synthesis system that was made at IPO. Right now, they're testing the system with a list of about 4000 single-morpheme words.",
        "formal_text": "Convert casual text to formal text: Speed was a big deal when building the database and when using it. Usually, it takes about 20 seconds to create or rebuild a database, like when you're testing new rules or updated"
    },
    {
        "casual_text": "• Entertainment News (Wikipedia) is all about celebrity stuff from early 2020.",
        "formal_text": "• Entertainment News (Wikipedia) is about celebrity stuff from early 2020. Convert casual text to formal text: • Entertainment News (Wikipedia) is about celebrity stuff from early 2020."
    },
    {
        "casual_text": "The basic RE system is trained with examples from the target language and then tested on different examples in the same language. This method, though expensive in terms of labeling, gives us an idea of the best possible performance we can expect.",
        "formal_text": "Convert casual text to formal text: The basic RE system is trained with examples from the target language and then tested on different examples in the same language. This method, though expensive in terms of labeling, gives us an idea of the best"
    },
    {
        "casual_text": "2) Text-based methods: These models focus solely on text data. Some examples include TextCNN (Kim, 2014), which is a deep learning model using CNNs for text classification; Bi-LSTM, a bidirectional LSTM network designed for text classification; SIARN (Tay et al., 2018), which uses inner-attention for detecting sarcasm in text; SMSD (Xiong et al., 2019), a self-matching network that identifies incongruity in text; and BERT (Devlin et al., 2019), the standard pre-trained BERT-base model that takes '[CLS] text [SEP]' as its input format.",
        "formal_text": "Convert casual text to formal text: 2) Text-based methods: These models focus solely on text data. Some examples include TextCNN (Kim, 2014), which is a deep learning model using CNNs for text classification;"
    },
    {
        "casual_text": "Once we finished all the preprocessing, we calculated features using the original post and question text, along with their translations. To create the training data, we had annotators go through all the sentences from the top 200 documents Indri pulled for each collection (for each question). Since retrieval tasks often have more negative examples than positive ones, we made sure to balance the data by splitting it into equal parts (each with the same positive labels) and trained multiple classifiers, which we’ll call Eng, Arz, and Cmn for short. When it comes to making predictions, we just go with the majority vote.",
        "formal_text": "Convert casual text to formal text: Once we finished all the preprocessing, we calculated features using the original post and question text, along with their translations. To create the training data, we had annotators go through all the"
    },
    {
        "casual_text": "Looks like we gotta normalize those regression labels, otherwise the performance might not be as good as it could be.",
        "formal_text": "Convert casual text to formal text: Looks like we gotta normalize those regression labels, otherwise the performance might not be as good as it could be. Convert casual text to formal text: Looks like we gotta normalize"
    },
    {
        "casual_text": "After round 22, we'll pick a smaller group of data, called D*, from the main training set D. This D* will be what the model uses for the next round of training. To choose D*, we can use different methods, like weighting (Liang et al., 2016; Zhou et al., 2020), sampling (Zhou et al., 2021), or batching (Yong Jae Lee and Grauman, 2011).",
        "formal_text": "Convert casual text to formal text: After round 22, we'll pick a smaller group of data, called D*, from the main training set D. This D* will be what the model uses for the next round of training."
    },
    {
        "casual_text": "In this part, we’re talking about the second layer of the model. We’re looking at different networks and suggesting a hierarchical model called CAN-GRU, along with three versions that build on it. So, CAN-GRU: In real-life conversations, when we try to figure out the emotion of what someone just said, we usually look at what was said before. That’s why our model uses GRU to handle the sequence of things said in a conversation. GRU is good at remembering and passing along information from earlier in the conversation. GRU (from Cho et al., 2014) is a better version of the original recurrent neural networks. It’s efficient and doesn’t require a lot of complicated math. At each point in time (timestep t), it uses something called the reset gate (R_t) and the update gate (Z_t) to figure out the current hidden state (S_t). It does this by combining the input from the current utterance (e_ut) with the hidden state from the previous step (s_t-1).",
        "formal_text": "Convert casual text to formal text: In this part, we’re talking about the second layer of the model. We’re looking at different networks and suggesting a hierarchical model called CAN-GRU, along with three"
    },
    {
        "casual_text": "Since our probability model isn't based on the noisy channel idea, we don't call our search module a \"decoder,\" like most statistical MT systems do. Instead, if you're an English speaker who doesn't know Chinese, you can think of it as an \"encoder\" (or encryptor), which lines up perfectly with our direct model. Given a fixed parse-tree  *, we're looking to find the best derivation with the highest probability. This can be done by doing a simple top-down traversal (or depth-first search) starting from the root of  *. At each node  in  *, we try out each possible rule r where the English-side pattern t(r) matches the subtree  *  rooted at . Then, we recursively visit each descendant node  i in  *  that corresponds to a variable in t(r). After that, we gather all the resulting target-language strings and plug them into the Chinese-side s(r) of rule r, which gives us a translation for the subtree  * . Finally, we pick the best translation out of all the options. With the extended LHS of our transducer, there might be multiple rules that can apply at a single tree node. For example, take a look at the VP subtree in Fig. 2 (c), where both r 3 and r 6 could be used. This means the number of possible derivations grows exponentially with the size of the tree, since there are exponentially many ways to break down the tree with a given set of rules. To handle this, we use memoization (Cormen et al., 2001): we store each subtree we've already visited in a cache, so that every tree node is only visited once.",
        "formal_text": "Convert casual text to formal text: Since our probability model isn't based on the noisy channel idea, we don't call our search module a \"decoder,\" like most statistical MT systems do. Instead,"
    },
    {
        "casual_text": "We're talking about Release 2 of the European Language Grid (ELG) cloud platform. This scalable system is designed to become the main platform for Language Technology (LT) in Europe. You can check out a screencast demo video here: https://youtu.be/LD6QadkkZiM. The ELG aims to be a single platform for all LT developments from Europe, including both research and industry. This addresses a long-standing need that the European LT community has been highlighting for years (see references like Rehm and Uszkoreit, 2013; Rehm et al., 2016b; STOA, 2017; Rehm, 2017; Rehm and Hegele, 2018; European Parliament, 2018). ELG is supposed to act as a virtual hub and marketplace for all products, services, and organizations involved in LT in Europe (Rehm et al., 2020a). Everyone—stakeholders, researchers, companies—can use the platform to showcase, share, and distribute their tools, services, and resources. By the end of the ELG EU project (2019–2022), which will set up a legal entity in early 2022, the platform will offer access to around 1,300 commercial and non-commercial tools and services for all European languages, along with thousands of language resources (LRs). ELG will allow the European LT community to upload their technologies and datasets and deploy them through the grid. It's also meant to promote digital language equality across Europe (STOA, 2017; European Parliament, 2018), meaning it'll help ensure equal access to language technologies for everyone.",
        "formal_text": "Convert casual text to formal text: We're talking about Release 2 of the European Language Grid (ELG) cloud platform. This scalable system is designed to become the main platform for Language Technology (LT) in Europe. You can"
    },
    {
        "casual_text": "2) We use an inverted index designed for the knowledge graph entities to find potential matches for each predicted entity span. Then, 3) we narrow down the list of candidates by matching them with the predicted entity types. After filtering, we just pick the first entity on the list as the correct one.",
        "formal_text": "Convert casual text to formal text: 2) We use an inverted index designed for the knowledge graph entities to find potential matches for each predicted entity span. Then, 3) we narrow down the list of candidates by matching them with the predicted"
    },
    {
        "casual_text": "Alright, let's make this sound more chill. 11. Take it off the heat and mix in the sesame oil until everything's all blended together. 9. Add the soy sauce, salt, and pepper to the rice, and keep cooking until the rice is nice and hot. Oh, and check out Figure 2: It shows how the steps in three written recipes and two video recipes line up for making fried rice. The same colored text boxes in the written recipes and the matching borders in the video recipes show which steps are basically the same.",
        "formal_text": "Convert casual text to formal text: Alright, let's make this sound more chill. 11. Take it off the heat and mix in the sesame oil until everything's all blended together. 9. Add the soy sauce,"
    },
    {
        "casual_text": "The number of errors dropped from 380 to 292, even though the word count stayed about the same. The main reason for the improvement seems to be that there are more closed-class words and repeated words now. These types of words make up a bit over 50% of the total word count. The percentage of lexical errors also went down—from 67% to 62%, according to Table 12. Meanwhile, both the number and percentage of syntactic errors decreased across all systems, including the lexical transfer system. There are a couple of reasons for this improvement mentioned earlier, like better phrasal lexicons and syntactic similarities. But another possible reason is something called an \"error attribution effect.\" This happens when it’s tricky to figure out exactly why an error occurred, which isn’t just a problem in language learning or machine translation evaluation—it’s been noted by a bunch of researchers over the years (Schwind, 1995; Holland & Kaplan, 1995; Heift, 1998; Knight, 2000; Halliday & Briss, 1971; van Slype, 1979; Falkedal, 1994; Balkan et al., 1994; Taylor & White, 1998). Looking at how the judges scored things, it seems like they tended to blame errors on the lexicon being difficult. When there were multiple errors in a sentence, they often marked them as lexical errors.",
        "formal_text": "Convert casual text to formal text: The number of errors dropped from 380 to 292, even though the word count stayed about the same. The main reason for the improvement seems to be that there are more closed-class words and repeated"
    },
    {
        "casual_text": "So, how do we use this system to deal with the issue of making clauses and texts make sense together? The first thing we did was to categorize the different ways that semantic coherence works and then fit them into our system. Basically, semantic coherence happens through things like: 1. Proforms 1.1.",
        "formal_text": "Convert casual text to formal text: So, how do we use this system to deal with the issue of making clauses and texts make sense together? The first thing we did was to categorize the different ways that semantic coherence"
    },
    {
        "casual_text": "Here, we'll talk about the different datasets we used for our experiments, the steps we took to get the data ready, and how we added extra info like automatically generated group labels and word forms.",
        "formal_text": "Convert casual text to formal text: Here, we'll talk about different datasets we used for our experiments, the steps we used to get the data ready, and how we added extra info like automatically generated group labels and word forms."
    },
    {
        "casual_text": "To figure out how related each sentence is to all the other sentences:",
        "formal_text": "Convert casual text to formal text: To figure out how related each sentence to all the other sentences: To figure out how related each sentence to all the other sentences: To figure out how related each sentence to all the other sentences: To figure"
    },
    {
        "casual_text": "Alright, so first, we take the word w i from the sentence and tag it with its emotional info, which we'll call emo i. To keep things simpler, we only use three categories to label each word: positive, negative, or neutral.",
        "formal_text": "Convert casual text to formal text: Alright, so first, we take the word w i from the sentence and tag it with its emotional info, which we'll call emo i. To keep things simpler"
    },
    {
        "casual_text": "From Figure 3, you can see that the error rate goes up as the compound gets longer (meaning more atoms in the compound). For the shortest compounds, which are just a determiner and a noun, only 4.50% are translated wrong. But when the compound has 3 atoms, like \"the smart lawyer,\" the error rate jumps to 13.72%.",
        "formal_text": "Convert casual text to formal text: From Figure 3, you can see that the error rate goes up as the compound gets longer (meaning more atoms in the compound). For the shortest compounds, which are just a determiner and"
    },
    {
        "casual_text": "Okay, so here's what's going on: 1. Bob remembers the current conversation, and he's talking about his positive opinion on Thai food. He specifically mentions Basil. 2. Bob also remembers the current conversation and brings up how affordable Basil is. 3. Bob has a memory from the past where he visited the Seas. He remembers that it was affordable, and he's referring to three different options: (a) G0, (b) G1, and (c) G2.",
        "formal_text": "Convert casual text to formal text: Okay, so here's what's going on: 1. Bob remembers the current conversation, and he's talking about his positive opinion on Thai food. He specifically mentions Basil. 2."
    },
    {
        "casual_text": "Table 4 shows that the BT language model gives way more probability to naturally occurring text, Y, compared to the OP language model (82.2 vs 57.4 perplexity). This means BT's outputs are way closer to natural text than OP's. We think this difference, which is seen in a language model trained on system outputs and tested on Y, might partly explain why people prefer BT translations over OP's.",
        "formal_text": "Convert casual text to formal text: Table 4 shows that the BT language model gives way more probability to naturally occurring text, Y, compared to the OP language model (82.2 vs 57.4 perplexity"
    },
    {
        "casual_text": "To boost performance, some methods use entity attributes or names in knowledge graphs (KGs). JAPE, for example, uses a Skip-Gram model to embed attributes and capture how they relate to each other in KGs. GCN-Align (from Wang et al., 2018) incorporates attribute info into entity embeddings using Graph Convolutional Networks (GCNs). MultiKE (Zhang et al., 2019) takes a more holistic approach, combining entity names, relations, and attributes to create embeddings for aligning entities. CEA (Zeng et al., 2020) goes a step further by mixing structural, semantic, and string features of entities, with weights that are adjusted dynamically.",
        "formal_text": "Convert casual text to formal text: To boost performance, some methods use entity attributes or names in knowledge graphs (KGs). JAPE, for example, uses a Skip-Gram model to embed attributes and capture how they"
    },
    {
        "casual_text": "Alright, let's break down how we built the groundtruth dataset. Here's what we did: 1. **Xtrain**: We started with 10,000 resumes, each represented as a 9054-dimension vector. These vectors are basically one-hot encoded versions of the resumes. The 9054 dimensions cover all the words in all the resumes after cleaning them up and getting rid of common stop words. 2. **Ytrain**: Next, we created 10,000 output vectors, each with 50 dimensions. To do this, we picked a random job description and used the same method we talked about in the white-box approach (check out Section 5.1 for details). This method helped us identify the 50 most important words from the job description. Now, we assumed the recruitment algorithm uses something called USE embedding, but the attacker could use other text embedding methods or even just go with the most common words. So, in reality, the attacker doesn't need to know exactly how the recruitment algorithm works. We chose 50 words to match our white-box attack setup and to give us plenty of options. But, this number can be adjusted depending on the attack. Once we had the important words for the job description, we added each of these words to every resume in our training set. Then, we checked with the black-box algorithm to see if adding these words improved the resume's position. If it did, we marked that word and resume combination as a \"1\" in Ytrain; if not, it was a \"0\". So, the output labels are one-hot encoded vectors.",
        "formal_text": "Convert casual text to formal text: Alright, let's break down how we built the groundtruth dataset. Here's what we did: 1. **Xtrain**: We started with 10,000 resumes, each represented as"
    },
    {
        "casual_text": "Assumption 5: There's no limit on what graph constants you can assign to a specific word.",
        "formal_text": "Convert casual text to formal text: Assumption 5: There's no limit what graph constants you assign a specific word. Convert casual text to formal text: Assumption 5: There's no limit"
    },
    {
        "casual_text": "A big problem in machine translation is dealing with the way words are ordered differently in different languages. Usually, this reordering stuff makes more sense when you look at things like part-of-speech tags. In fact, some earlier research, like Tomas and Casacuberta back in 2003, already looked at using these tags to handle reordering before the actual translation happens.",
        "formal_text": "Convert casual text to formal text: A big problem in machine translation is dealing with the way words are ordered differently in different languages. Usually, this reordering stuff makes more sense when you look at things like part-of-"
    },
    {
        "casual_text": "Conversational tech is a pretty cool way to help with mental health care. When people talk to these chatbots, there are little clues in the way they speak that can show if they're feeling stressed or not. Things like how often they pause, how fast they talk, and other speech patterns can give it away (Gratch et al., 2014). Even in regular doctor-patient chats, how messy someone's speech is can tell you if they're taking their meds properly (Howes et al., 2012). And weird speech habits can also help spot cognitive issues (Rohanian et al., 2020).",
        "formal_text": "Convert casual text to formal text: Conversational tech is a pretty cool way to help with mental health care. When people talk to these chatbots, there are little clues in the way they speak that can show if they"
    },
    {
        "casual_text": "We've included some extra tables and figures to give you a better idea of how our experiments turned out and to back up what we found.",
        "formal_text": "Convert casual text to formal text: We've included some extra tables and figures to give you a better idea how our experiments turned out and to back up what we found."
    },
    {
        "casual_text": "Alright, so in the last section, we talked about how tensor-based models handle feature inclusion in a general sense. But now, we're diving into the specifics. The way these models actually include features can vary depending on how they're built, and it’s usually more complex than just straightforward intersection or union-based methods. Here, we're going to take a closer look at a few tensor-based models, figure out how they deal with feature inclusion, and see what we can learn about their properties.",
        "formal_text": "Convert casual text to formal text: Alright, so in the last section, we talked about how tensor-based models handle feature inclusion in a general sense. But now, we're diving into the specifics."
    },
    {
        "casual_text": "API stands for Application Programming Interface, and SDK means Software Development Kit.",
        "formal_text": "Convert casual text to formal text: API stands for Application Programming Interface, and SDK means Software Development Kit. Convert casual text to formal text: API stands for Application Programming Interface, and SDK means Software Development Kit. Con"
    },
    {
        "casual_text": "For this project, we created Spo-kenCOCO, which is basically a spoken version of the MSCOCO captioning dataset (Lin et al., 2014). It has 742 hours of audio from 2,532 speakers, and we got it by using Amazon Mechanical Turk. We just showed people the text and asked them to read it out loud. You can find more info about the dataset in the appendix, Section A. Oh, and just so you know, we also used the speech version (Lin et al., 2014), Flickr8k (Rashtchian et al., 2010), and Places (Zhou et al., 2014).",
        "formal_text": "Convert casual text to formal text: For this project, we created Spo-kenCOCO, which is basically a spoken version of the MSCOCO captioning dataset (Lin et al., 2014). It has 7"
    },
    {
        "casual_text": "• P(D): The domain priors try to figure out how much useful data the learning system is picking up. We could do this using smaller parts of the text (phrase-level), but we'd rather use whole sentences (sentence-level): 6",
        "formal_text": "Convert casual text to formal text: • P(D): The domain priors try to figure out how much useful data the learning system is picking up. We could do this using smaller parts of the text (phrase-level), but"
    },
    {
        "casual_text": "We checked how well the human experts agreed on whether something was acceptable, just like we did for figuring out if it was a spelling variant. Turns out, spelling variants are way trickier for people to judge compared to other types of answers. The agreement score was only .60 for spelling variants, while it was .83 for everything else (check out Table 3). Even with regular training, clear guidelines, and lots of pre-testing, human raters still had some inconsistencies, which matches what Buck (2001) found.",
        "formal_text": "Convert casual text to formal text: We checked how well the human experts agreed on whether something was acceptable, just like we did for figuring out if it was a spelling variant. Turns out, spelling variants are way trick"
    },
    {
        "casual_text": "In the study by Spitkovsky et al. (2011), they looked at how well a state-of-the-art system could handle punctuation in different setups. In one case, they used monosemous induced tags, which gave them an accuracy of 58.2% (-0.2 from the baseline). Then, they tried context-sensitive induced tags, which bumped the accuracy up to 59.1% (+0.7). These results are from testing on Section 23 of the Wall Street Journal, covering all the sentences. The table below shows the specific numbers for these experiments.",
        "formal_text": "Convert casual text to formal text: In the study by Spitkovsky et al. (2011), they looked at how well a state-of-the-art system could handle punctuation in different setups. In"
    },
    {
        "casual_text": "A lot of research on breaking down text into topics has focused on single-level segmentation. Nowadays, most methods work by spotting changes in vocabulary to figure out when a new topic starts (Youmans, 1991). There are two main types of approaches for this: local and global models. Local algorithms only look at small parts of the text. For instance, TextTiling (Hearst, 1997) uses a sliding window to check how similar chunks of text are. When it finds drops in similarity, it marks those as topic changes. More recently, Marathe (2010) used lexical chains, and Blei and Moreno (2001) used Hidden Markov Models. These methods are usually pretty fast, but they can get confused by small tangents in the text.",
        "formal_text": "Convert casual text to formal text: A lot of research on breaking down text into topics has focused on single-level segmentation. Nowadays, most methods work by spotting changes in vocabulary to figure out when a new topic starts ("
    },
    {
        "casual_text": "In 2017, some folks looked into how to handle deleting stuff in long-term archives (LTA). They came up with the idea of a \"tombstone\" for objects that had to be removed for legal reasons. Systems like Fedora Commons or DSpace let you delete resources and leave behind these tombstone objects. We’ll talk about how our approach compares to theirs later. Another thing to consider is how to represent data efficiently in corpora that get updated and released often. Objects might be referenced in different releases or belong to multiple collections. In projects where corpora are constantly updated and released frequently, there’s a lot of overlap with unchanged objects across releases. Generally, a long-term digital archive tries to avoid storing the same object multiple times. Keeping just one copy of each object helps avoid confusion about its state and saves storage space, especially for large files. The last point is about long-term preservation. You can’t predict if or when a file format might become outdated. But when that happens, the archive will need to convert those files to a new format and make sure both the new and original versions are accessible. This kind of goes against the original idea that the new version of the information replaces the old one (as mentioned in CCSDS, 2012, p. 1-11).",
        "formal_text": "Convert casual text to formal text: In 2017, some folks looked into how to handle deleting stuff in long-term archives (LTA). They came up with the idea of a \"tombstone\" for objects that had to be"
    },
    {
        "casual_text": "Sure! Here's a more casual version: The link to the GitHub repo for the CCP NLP Pipelines is here: https://github.com/UCDenver-ccp/ccp-nlp-pipelines. For the OBO NCBI Taxonomy, you can check it out here: http://www.obofoundry.org/ontology/ncbitaxon.html. And if you're using PyARC, you can find it on PyPI: https://pypi.org/project/pyarc/.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: The link to the GitHub repo for the CCP NLP Pipelines is here: https://github.com/UCDenver"
    },
    {
        "casual_text": "As mentioned in (Sukhbaatar et al., 2015), we set aside 10% of the data to create a validation set for tweaking hyperparameters. For encoding the utterances, we went with the Temporal Encoding method. Reading tasks need some sense of time, so to help the model handle them, we tweak the memory vector like this: m_i = _j Ax_ij + T_A(i), where T_A(i) is the i-th row of a special matrix T_A that deals with temporal info. The output embedding gets a similar treatment with a matrix T_C (e.g., c_i = _j Cx_ij + T_C(i)). Both T_A and T_C are learned as part of the training process and follow the same sharing rules as A and C. The embedding matrices A and B are started off using the GoogleNews word2vec model (Mikolov et al., 2013). Also following (Sukhbaatar et al., 2015), we index the utterances in reverse order, so x_1 is the last sentence of the conversation, showing its distance from the question. We also used the adjacent weight tying approach. The learning rate  starts at 0.005 and halves every 25 epochs until we hit 100 epochs. After that, we switch to the linear start method, as suggested by (Sukhbaatar et al., 2015). Specifically, we remove the softmax function from each memory layer at the beginning and add it back after 20 epochs.",
        "formal_text": "Convert casual text to formal text: As mentioned in (Sukhbaatar et al., 2015), we set aside 10% of the data to create a validation set for tweaking hyperparameters. For"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: 1. First, we create a new graph G′ by using a function called Lattice on graph A. 2. Then, we go through each vertex v′ in this new graph G′. 3. For each vertex v′, we check if there isn't already a vertex v in the original graph G that has the same label as v′. 4. If we find that such a vertex v doesn't exist in G, we add v′ to the vertex set of G. 5. And that's it! We keep doing this until we've checked all the vertices in G′.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: 1. First, we create a new graph G′ by using a function called Lattice on graph A. 2. The"
    },
    {
        "casual_text": "- It works seamlessly with MULTITERM, so you can look up terminology as you go. - Plus, it comes with extra features like concordance and coverage analysis.",
        "formal_text": "Convert casual text to formal text: - It works seamlessly with MULTITERM, so you can look up terminology as you go. - Plus, it comes with extra features like concordance and coverage analysis."
    },
    {
        "casual_text": "Hey there, Just wanted to let you know that Prof. Jan Hajic from Charles University in the Czech Republic and Prof. Junichi Tsujii from Microsoft Research in China are the co-chairs of the COLING 2014 Program Committee. This was announced on July 8, 2014. Cheers!",
        "formal_text": "Convert casual text to formal text: Hey there, Just wanted to let you know that Prof. Jan Hajic from Charles University in the Czech Republic and Prof. Junichi Tsujii from Microsoft Research in China are co"
    },
    {
        "casual_text": "In this research, we’ve also demonstrated how to do web-scale, real-time adaptive data collection using Amazon MTurk, a crowdsourcing platform. MTurk is usually used for simpler tasks, like identifying names or medical terms in text, filtering spam, adding captions to images, and stuff like that. But with MTurk's external HITs, we were able to show that it can handle more complex NLP tasks too, like text simplification with a writing aid tool. Normally, this kind of thing is only done in a lab setting, but we proved it works on MTurk as well.",
        "formal_text": "Convert casual text to formal text: In this research, we’ve also demonstrated how to do web-scale, real-time adaptive data collection using Amazon MTurk, a crowdsourcing platform. MTurk is usually"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Basically, the idea is to find ways to break down a sentence into smaller parts (clauses) by looking at simple, surface-level clues. Now, this doesn’t work for every single sentence, but it’s clear that in any language, there’s a big chunk of sentences where this kind of method can work. For Czech, for example, you could use pattern-matching techniques to spot certain structures, like: - A comma followed by a conjunction (and, but, or, etc.). - A semicolon. - A dash. Once you recognize these patterns, you can easily split the sentence into clauses.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Basically, the idea is to find ways to break down a sentence into smaller parts (clauses) by looking at simple, surface-"
    },
    {
        "casual_text": "We tried using universal regularisation and data augmentation on the HATM to see if it would improve how well it detects things. From Table 3 and Figure 4, it’s clear that adding universal regularisation to the prompt attention mechanism boosted the F 0.5 score. Both the structured techniques and the machine translation (MT) prompt data augmentation helped improve the baseline HATM with regularisation. MT gave a bigger boost, but the structured technique also worked well and seemed to complement the MT approach. We also tested data augmentation on the SGM. This actually caused the F 0.5 score to drop significantly, likely because the SGM was overfitting to the training data.",
        "formal_text": "Convert casual text to formal text: We tried using universal regularisation and data augmentation on the HATM to see if it would improve how well it detects things. From Table 3 and Figure 4, it’s clear that adding"
    },
    {
        "casual_text": "You can tackle optimization 4 using supervised learning. But here's the thing: getting training data for the grounding problem is a pain, especially when you need it at different resolutions. Plus, the tricky language in this area means you'd end up with a lot of gaps in your data.",
        "formal_text": "Convert casual text to formal text: You can tackle optimization 4 using supervised learning. But here's the thing: getting training data for the grounding problem is a pain, especially when you need it at different resolutions. Plus"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. For each sample m i, s i, m j, s j, r, we use BERT (Devlin et al., 2019) to encode a pair of sentences (s i, s j ) and figure out their paraphrase features. BERT also helps us pick up on important semantic stuff through its self-attention mechanism. To make BERT work smoothly, we add [CLS] and [SEP] tokens to connect the two sentences. To make sure the model focuses on the specific event structure, we use a simple trick: we wrap the event mentions in sentences with the special token [COREF] 2. For instance, if we have the event pair (strike, hits) and the sentences (\"A powerful quake strikes the Indonesian province of Aceh\", \"Dozens injured as the earthquake hits Aceh\"), we’ll transform them like this: Imagine the input sequence T is a bunch of tokens, T = t 0, t 1, . . . t n . Following Devlin et al. (2019), we take the token embedding of [CLS], which is t 0, as the representation for the sentence pair. Next, we run it through a fully connected layer, then a non-linear activation, and finally another fully connected layer to get the binary event-specific paraphrase vector S para (m i, m j ) for m i and m j.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. For each sample m i, s i, m j, s j, r"
    },
    {
        "casual_text": "Here, \"where\" refers to the length of the system's output sentence, and it's talking about the n-gram position difference between the words in the output and the reference sentences. Each word from both the output and the reference should only be matched once (one-to-one matching). It doesn't matter if the words are uppercase or lowercase. If there's no match for a word, it'll just be counted as zero by default.",
        "formal_text": "Convert casual text to formal text: Here, \"where\" refers to the length of the system's output sentence, and it's talking about the n-gram position difference between the words in the output and the reference sentences"
    },
    {
        "casual_text": "Check out Table 2 for the stats on transcribed and conceptually annotated data.",
        "formal_text": "Convert casual text to formal text: Check out Table 2 for stats on transcribed and conceptually annotated data. Convert casual text to formal text: Check out Table 2 for stats on transcribed and conceptually"
    },
    {
        "casual_text": "In this part, we're using the type system from Groschwitz's 2019 work. In this system, types are represented as Directed Acyclic Graphs (DAGs), with the nodes being sources and the edges showing requests.",
        "formal_text": "Convert casual text to formal text: In this part, we're using the type system from Groschwitz's 2019 work. In this system, types are represented as Directed Acyclic Graphs (DAGs"
    },
    {
        "casual_text": "(6) We can calculate the parameters for this heuristic function ahead of time, before the actual translation starts. They're saved in ARPA format, just like a regular language model. This means we can use the same code we already have for dealing with language models.",
        "formal_text": "Convert casual text to formal text: (6) We can calculate the parameters for this heuristic function ahead of time, before the actual translation starts. They're saved in ARPA format, just like a regular language model."
    },
    {
        "casual_text": "There are a few areas we can focus on for future improvements. One is coming up with a better architecture that makes good use of structured metadata. Another idea is ditching the two-stage process and switching to a multi-task generation model. This new model would be able to both figure out the most helpful context for the task and handle the text generation at the same time.",
        "formal_text": "Convert casual text to formal text: There are a few areas we can focus on for future improvements. One is coming up with a better architecture that makes good use of structured metadata. Another idea is ditching the two-stage process"
    },
    {
        "casual_text": "We add info about how many target words are left at each spot in the sentence. First, this helps make sure we don’t lose the length details while decoding. Second, by using smaller numbers (which show up more often in sentences) toward the end of the sentence, we don’t have to worry too much about weird sentence lengths being rare. Basically, at each step j in the decoder, the model starts with the word embedding of the last target word, y j1. In the original Transformer setup (Vaswani et al., 2017), positional encoding is added to that embedding to create the first hidden representation.",
        "formal_text": "Convert casual text to formal text: We add info about how many target words are left at each spot in the sentence. First, this helps make sure we don’t lose the length details while decoding. Second, by using smaller numbers"
    },
    {
        "casual_text": "First, let's look at how plain versions of depLCNN (which considers both dependency directions and labels), depCNN (only looks at dependency directions), MVRNN, and CNN compare. All of these models operate in a 2K+1 style. We can see that both our depCNN and depLCNN beat MVRNN and CNN by at least 2.2%. This shows that our approach is better at capturing syntactic structures for relation extraction compared to previous methods. Also, note that depLCNN, which takes dependency labels into account, performs even better than depCNN. This suggests that dependency labels provide more useful, distinctive information that helps with the relation extraction task.",
        "formal_text": "Convert casual text to formal text: First, let's look at how plain versions of depLCNN (which considers both dependency directions and labels), depCNN (only looks at dependency directions), MVRNN, and"
    },
    {
        "casual_text": "Sure! Here's a more casual version: \"This link takes you to the page about the speech-to-text feature from Microsoft Azure's Cognitive Services. It's basically a tool that turns spoken words into written text.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: \"This link takes you the page about the speech-to-text feature from Microsoft Azure's Cognitive Services. It's basically a tool"
    },
    {
        "casual_text": "Alright, so Table 5 shows how four different unit types performed across three sets of metrics. Let’s break it down. First off, when we compare word-based and unit-based evaluations, we see that VQ3, VQ2, and WVQ rank pretty much the same across BLEU-4, METEOR, and ROUGE for the SAT models. But here’s the weird part: VQ3 with RLE gets super high scores on these metrics, even though it just spits out boring captions for every image, as you can see in Table 3. Why? Well, the unit \"32\" has figured out how to represent stuff like silence, which happens a lot at the start and end of sentences. Without RLE, you get a bunch of \"32\" units in a row in both the generated captions and the real ones, which makes the scores look better than they should. The only exception is CIDEr, which uses TF-IDF to weigh things differently and doesn’t fall for these repetitive patterns. Still, when you compare SAT and SAT-FT with VQ3 units, CIDEr doesn’t rank them the same way as the other word-based metrics do.",
        "formal_text": "Convert casual text to formal text: Alright, so Table 5 shows how four different unit types performed across three sets of metrics. Let’s break it down. First off, when we compare word-based and unit-based evaluations"
    },
    {
        "casual_text": "The second part is a regularization term that makes sure the inner product between the poisoning loss gradient and the fine-tuning loss gradient isn't negative. The  is just a number that shows how strong this regularization is. We named this method \"Restricted Inner Product Poison Learning\" (RIPPLe). Now, in a situation where the environment changes, we don't actually know the true fine-tuning loss, so the attacker has to use a stand-in, like L FT, to guess what it might be. Later on, we'll show that even a rough guess—like using a dataset from a totally different area—can work well enough for the RIPPLe attack to do its thing.",
        "formal_text": "Convert casual text to formal text: The second part is a regularization term that makes sure the inner product between the poisoning loss gradient and the fine-tuning loss gradient isn't negative. The  is just"
    },
    {
        "casual_text": "Check out Table 2 for their results. Here's what stood out: the model without domain adaptation (Noadapt) is already pretty strong, mostly because we've got plenty of training data. But, if you just throw all the queries from different domains into one model (Union), it messes things up big time. This happens because the same query can be labeled differently depending on the domain and the context. Plus, the amount of training data varies a lot between domains. For instance, slots like app names in other domains get overshadowed by slots in the PLACES domain, like place names, since PLACES has the most slot types and is the second biggest in terms of dataset size. Lastly, the feature augmentation method from Daumé III (2009)—you can see the F1 scores for that and different LSTM model versions across seventeen personal assistant domains in Table 3.",
        "formal_text": "Convert casual text to formal text: Check out Table 2 for their results. Here's what stood out: the model without domain adaptation (Noadapt) is already pretty strong, mostly because we've got plenty of training data. But"
    },
    {
        "casual_text": "We're running the model on an NVIDIA GeForce RTX 2080 Ti GPU. Since the GPU's RAM is limited, we're using gradient accumulation during training. The system runs on Ubuntu 18.04, and we're using PyTorch 1.4.0 and Transformers 2.4.1 to build our model. To speed things up, we're also using mixed precision training with NVIDIA Apex 0.1. On average, each epoch takes about 42 hours, but the model gets its best results after 10 epochs.",
        "formal_text": "Convert casual text to formal text: We're running the model on an NVIDIA GeForce RTX 2080 Ti GPU. Since the GPU's RAM is limited, we're using gradient accumulation during training. The"
    },
    {
        "casual_text": "The Tanzil dataset is a collection of Quran translations, and it has a noticeable difference in sentence length between English and Indonesian. As you can see in Table 2, Indonesian sentences in this dataset are about 50% longer on average compared to English ones. On top of that, in most sentence pairs, one sentence is usually twice as long as the other. Despite this, we decided to include this dataset to avoid overfitting, since the other datasets we have are all related to Christianity. Another cool thing about this religious dataset is how names are localized. For example, names like David become Daud, Mary becomes Maryam, and Gabriel becomes Jibril. In other domains, names usually stay the same. We also noticed that some Indonesian translations from JW300 are missing the period (.) at the end of the sentence, even though the English versions have it. Lastly, there are some inconsistencies in how words are transliterated. For instance, \"praying\" can be written as either \"salat\" or \"shalat,\" and \"repentance\" can be \"tobat\" or \"taubat.\"",
        "formal_text": "Convert casual text to formal text: The Tanzil dataset is a collection of Quran translations, and it has a noticeable difference in sentence length between English and Indonesian. As you can see in Table 2, Indonesian sentences in"
    },
    {
        "casual_text": "The data also has instances of 'real-word' mistakes. For instance, 13% of the time, \"occidental\" is mistakenly typed instead of \"accidental,\" while only 89% of \"accidential\" (which isn't even a real word) is flagged as incorrect. There are tons of examples of words that wouldn't normally be in a standard dictionary, like made-up words (using \"mulitplayer\" instead of \"multiplayer\"), brand names (typing \"Playstaton\" instead of \"Playstation\"), proper names (misspelling \"Schwarznegger\" as \"Schwarzenegger\"), and even internet domain names (writing \"mysapce. com\" instead of \"myspace. com\").",
        "formal_text": "Convert casual text to formal text: The data also has instances of 'real-word' mistakes. For instance, 13% of the time, \"occidental\" is mistakenly typed instead of \"accidental,\""
    },
    {
        "casual_text": "Even though events seem like a pretty straightforward and useful concept, they’re not used as much as terms or entities in summarization. Back in 2004, Filatova and Hatzivassiloglou introduced \"atomic events\" as a way to represent ideas in MDS content selection. Then, Li et al. (2006) took it a step further by treating event terms and named entities as nodes in a graph for their PageRank algorithm. Yoshioka and Haraguchi (2004) also used an event-based approach for MDS content selection, specifically for Japanese articles. While their model included \"sentence reordering,\" it was pretty basic—just following the text and chronological order. Not many studies out there actually use event information to figure out the order of sentences in MDS.",
        "formal_text": "Convert casual text to formal text: Even though events seem like a pretty straightforward and useful concept, they’re not used as much as terms or entities in summarization. Back in 2004, Filatova and Hatzivassilog"
    },
    {
        "casual_text": "• As you get more and more data, self-training tends to go downhill.",
        "formal_text": "• As you get more and more data, self-training tends downhill. Convert casual text to formal text: • As you get more and more data, self-training tends to downhill. Convert casual text to formal"
    },
    {
        "casual_text": "In this paper, we suggest training the model with the dual MLR combined with the shrinking method. It's nearly four times quicker than the primal MLR (which is also called MaxEnt) and uses way less memory. For really huge datasets, the data might not even fit into memory, and training with primal MLR would take forever because of all the disk swapping. In that case, using dual MLR is a big help. This method is also handy for a lot of classification tasks in natural language processing that need to work with massive amounts of data.",
        "formal_text": "Convert casual text to formal text: In this paper, we suggest training the model with the dual MLR combined with the shrinking method. It's nearly four times quicker than the primal MLR (which is also called MaxEnt"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: P(e, f, a) is like saying, \"Hey, we're looking at these things, e and f, and how they relate to a.\" It's calculated by multiplying a couple of other probabilities: PG(; p$) and P(a|e, f). Now, PM(e, f) is just a way of saying, \"How likely are e and f to happen together?\" And this is made up of two parts: - p times N(e, f), which is like saying, \"There's a chance p that this normal thing N(e, f) happens.\" - (1 - p) times J(e, f), which is like saying, \"And if the normal thing doesn't happen, there's a chance (1 - p) that this other thing J(e, f) happens instead.\" So, it's all about figuring out how likely things are to happen together and how that connects to something else, a.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: P(e, f, a) is like saying, \"Hey, we're looking at these things"
    },
    {
        "casual_text": "This feature is based on the method described in the paper by Fattah and Ren from 2008. We don’t usually include super short sentences in the final summary because they often don’t have much info. But longer Punjabi sentences? They can pack a lot of information. To calculate this feature, you just divide the number of words in a sentence by the word count of the longest sentence. The result will always be 1 or less.",
        "formal_text": "Convert casual text to formal text: This feature is based on the method described in the paper by Fattah and Ren from 2008. We don’t usually include super short sentences in the final summary because they often don’t have much"
    },
    {
        "casual_text": "Alright, let me break it down in simpler terms. First, we turn each word in the sentence into a d-dimensional vector using a special table (called an embedding matrix) that's part of the model. This table is created as the model learns, so it’s not fixed. Here, V is just the total number of words we have in our vocabulary. Next, we take all these word vectors, which we can think of as a sequence x1, ..., xT, and feed them into something called an LSTM. The LSTM gives us outputs h1, ..., hT, and we use the last one, hT, as a summary of the whole sentence. This summary is also a d-dimensional vector. Finally, we use some extra parameters, W and b, to help us figure out the slots. W is a matrix that connects our summary vector to the different slot types, and b is just a bias term. The number k here represents how many different slot types we’re dealing with.",
        "formal_text": "Convert casual text to formal text: Alright, let me break it down in simpler terms. First, we turn each word in the sentence into a d-dimensional vector using a special table (called an embedding matrix"
    },
    {
        "casual_text": "Let's take a look at how well the different systems perform when tested on a bunch of different languages. For this, we used part of the Multext-East corpus (7,000 sentences), which has parallel translations of Orwell's \"1984\" [en]. We also added a 7,000-sentence version of the WSJ corpus [wsj-s] to see if the results differ based on the size of the corpus or the language/domain. For the WSJ corpora, we tried two commonly used tagsets: the original PTB 45-tag gold standard and a simpler 17-tag set that some researchers have used for unsupervised POS tagging (Smith and Eisner, 2005; Goldwater and Griffiths, 2007; Johnson, 2007). For the Multext-East corpus, we only had a 14-tag tagset. To make things fair and easier to compare, we also made a 13-tag version of the WSJ corpus, keeping the genre the same but adjusting the size and tag set. Figure 3 shows how well the systems handle different types of English text. When we compare the results for the Multext-East English corpus and the smaller WSJ corpus with 13 tags (basically, trying to control for corpus size and tag set), we find that the systems actually do better on Multext-East, even though they were developed using WSJ. This is a good sign because it means the methods and settings of the algorithms aren't too dependent on the WSJ data.",
        "formal_text": "Convert casual text to formal text: Let's take a look at how well the different systems perform when tested on a bunch of different languages. For this, we used part of the Multext-East corpus (7,000 sentences"
    },
    {
        "casual_text": "Our more precise rule, called \"name-and-location,\" works by using the fact that the Fatal Encounters database also includes the location of the incident. It needs both the name and the location to match: basically, the name(i) should match name(e), and the location(e) should be part of x i.",
        "formal_text": "Convert casual text to formal text: Our more precise rule, called \"name-andlocation,\" works by using the fact that the Fatal Encounters database also includes the location of the incident. It needs both the name and the location"
    },
    {
        "casual_text": "Ciobanu and Dinu (2014) wanted to see if they could use character alignments from Longest Common Subsequence alignments to help spot cognates between pairs of words. They used a binary SVM classifier and fed it multi-gram character alignments as features for four pairs of Romance languages: Romanian-French, Romanian-Italian, Romanian-Spanish, and Romanian-Portuguese. They discovered that the SVM classifier, which was trained on these character alignments, worked better than other methods like Edit distance, Longest Common Subsequence Ratio, and counting common bigrams.",
        "formal_text": "Convert casual text to formal text: Ciobanu and Dinu (2014) wanted to see if they could use character alignments from Longest Common Subsequence alignments to help spot cognates between pairs of words. They"
    },
    {
        "casual_text": "After cleaning up the text by making everything lowercase, getting rid of weird symbols and punctuation, we turn each protocol into a bag-of-words (BOW) type of thing, ignoring any fancy structures. Then, we use TF-IDF to adjust the importance of each word based on how relevant it is compared to others.",
        "formal_text": "Convert casual text to formal text: After cleaning up the text by making everything lowercase, getting rid of weird symbols and punctuation, we turn each protocol into a bag-of-words (BOW) type of thing"
    },
    {
        "casual_text": "We're working on abstractive summarization, which is basically turning a long news sentence into a shorter summary. It's a sequence generation task. We're using the Gigaword dataset, which has around 3.8 million training pairs. If you want the specifics on the GRU-based RNN model, check out the supplementary material in section A.2. Table 10 shows the ROUGE F1 scores for sentences generated from left to right and right to left in this summarization task. ROUGE-N refers to the N-gram based ROUGE F1 score, while ROUGE-L is for the longest common subsequence based ROUGE F1 score. \"Full\" just means the whole translation sentence.",
        "formal_text": "Convert casual text to formal text: We're working on abstractive summarization, which is basically turning a long news sentence into a shorter summary. It's a sequence generation task. We're using the"
    },
    {
        "casual_text": "For multi-label evaluation, let's say each edit ( e_i ) in the set ( E ) has a bunch of relevant categories, which we'll call ( y_i ) and it's part of the bigger set ( C ). Then, we also have the categories that our model predicts for that edit, which we'll call ( h(e_i) ).",
        "formal_text": "Convert casual text to formal text: For multi-label evaluation, let's say each edit ( e_i ) in the set ( E ) has a bunch of relevant categories,"
    },
    {
        "casual_text": "On average, 27.31% of compounds get translated wrong when looking at individual cases. But if we combine all five contexts, 61.62% of compounds have at least one incorrect translation. This shows that even a really good NMT model struggles with translating compounds, even though all the parts of these compounds are super common in the training data. We also noticed that PP compounds have a much higher error rate—37.72%—compared to the other two types, which are around 21.94% and 22.25%. We'll dive into this more in the next section.",
        "formal_text": "Convert casual text to formal text: On average, 27.31% of compounds get translated wrong when looking at individual cases. But if we combine all five contexts, 61.62% of compounds have at least one incorrect translation. This shows"
    },
    {
        "casual_text": "The Austronesian Basic Vocabulary Database 3 has word lists for 210 concepts across 378 languages. Each word in the database also has a cognacy judgment. But the problem is, the database isn’t in a consistent format. So, we did some semi-automatic work to clean it up and converted a chunk of 100 languages into the uniform ASJP alphabet. From the cleaned-up data, we pulled out a total of 525,941 word pairs, and out of those, 167,676 are cognates.",
        "formal_text": "Convert casual text to formal text: The Austronesian Basic Vocabulary Database 3 has word lists for 210 concepts across 378 languages. Each word in the database also has a cognacy judgment. But the problem is"
    },
    {
        "casual_text": "Unsupervised Constituency Parsing is all about figuring out the structure of a sentence by building a tree where the words are the leaves and the non-leaf parts (called nonterminal nodes) represent phrases. It's usually seen as trickier than unsupervised dependency parsing because, in addition to figuring out the connections (edges), you also have to figure out what the different parts (nodes) of the tree are. That's why, over the last decade, there have been way more papers on unsupervised dependency parsing than on constituency parsing. But things are changing! In the past couple of years, people have started getting more interested in unsupervised constituency parsing, and there have been some cool new ideas popping up. Even though this paper is mainly about unsupervised dependency parsing, a lot of what we talk about—like different methods and recent trends—can also be applied to unsupervised constituency parsing.",
        "formal_text": "Convert casual text to formal text: Unsupervised Constituency Parsing is all about figuring out the structure of a sentence by building a tree where the words are the leaves and the non-leaf parts (called non"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: There are workers from different countries, and they're grouped by the language they speak. Here's how it looks: **English (en):** - 9 countries: USA (15), Australia (1), Canada (1), UK (1), Malaysia (1), Philippines (1), North Macedonia (9), China (2), Netherlands (2), Romania (2), Japan (2) - 11 countries: USA (5), Australia (1), Japan (1), North Macedonia (1), Bangladesh (1), Cameroon (1), Singapore (1) - 4 countries: Pakistan (2), Australia (1), Bangladesh (1), Cameroon (1), Maldives (1) **Arabic (ar):** - 11 countries: Jordan (12), Egypt (8), USA (7), Tunisia (3), Lebanon (3), Saudi Arabia (2), North Macedonia (6), Tunisia (3), Jordan (3), Egypt (2), USA (2), Bangladesh (2), North Macedonia (3), Egypt (2), Pakistan (2), China (1), Algeria (1), UK (1), Morocco (2), Algeria (1), Kuwait (1), UAE (1), Oman (1) - 15 countries: UAE (2), UK (2), Algeria (1), China (1), Spain (1), Maldives (1), Lebanon (1), Tunisia (1), UAE (1), USA (1) - 10 countries: Romania (1), Oman (1), Saudi Arabia (1) **Spanish (es):** - 8 countries: Spain (5), Mexico (4), USA (4), Colombia (2), Argentina (1), Guatemala (1) - 5 countries: North Macedonia (7), Spain (2), USA (1), Bangladesh (1), Romania (1) - 7 countries: USA (2), Bahamas (1), Spain (1), Portugal (1), North Macedonia (1), Pakistan (1), Uruguay (1), Venezuela (1) - 4 countries: Romania (1) **French (fr):** - 4 countries: USA (5), France (3), Canada (1), Cameroon (1) - 5 countries: North Macedonia (6), USA (1), Cameroon (1), Netherlands (1), Romania (1) - 8 countries: North Macedonia (3), Pakistan (3), France (2), Romania (2), Canada (1), Cameroon (1), Netherlands (1), USA (1) **Hindi (hi):** - 2 countries: India (30), USA (1) - 4 countries: India",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: There are workers from different countries, and they're grouped by the language they speak. Here's how it looks: **"
    },
    {
        "casual_text": "A good way to organize sentences is by making sure they flow well together, because when they don't, things get messy. Lately, most research has been about local coherence, looking at stuff like how words connect (Conroy et al., 2006) or how ideas move from one sentence to the next (Barzilay and Lapata, 2008). But no one’s really paying much attention to global coherence, which is about how groups of sentences work together as a whole. Not many people are trying to figure out how to measure coherence beyond just words or ideas.",
        "formal_text": "Convert casual text to formal text: A good way to organize sentences is by making sure they flow well together, because when they don't, things get messy. Lately, most research has been about local coherence, looking at"
    },
    {
        "casual_text": "So, the Cb(Ui+l) is important. Basically, how things are ranked in the Cf is super important for the model. Grosz and his crew (1995) and Brennan's group (1987) use grammar stuff to rank the Cf (like subject before object and so on), but they also mention that other things might matter too.",
        "formal_text": "Convert casual text to formal text: So, the Cb(Ui+l) is important. Basically, how things are ranked in the Cf is super important for the model. Grosz and his crew (1995"
    },
    {
        "casual_text": "Sure! Here's the informal version: - [sacrebleu](https://github.com/mjpost/sacrebleu) - [METEOR](https://www.cs.cmu.edu/alavie/METEOR/) - [InfECE](https://github.com/shuo-git/InfECE) - [tercom](http://www.cs.umd.edu/snover/tercom/)",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: - [sacrebleu](https://github.com/mjpost/sacrebleu) - [METEOR"
    },
    {
        "casual_text": "But, there aren’t many datasets out there that work well for creating adaptive argumentative writing systems for teaching purposes. As far as we know, there are only two datasets from the education field that use student-written texts and have been annotated to show argumentative structures. These are the ones by Stab and Gurevych (2017a) and Wambsganss et al. (2020c).",
        "formal_text": "Convert casual text to formal text: But, there aren’t many datasets out there that work well for creating adaptive argumentative writing systems for teaching purposes. As far as we know, there are only two datasets from the education"
    },
    {
        "casual_text": "The work we're talking about here focuses on two main areas related to making things more robust. First, we're looking at using features from both syntactic and semantic processing that can handle changes in genre and language without breaking down. We're aiming to move beyond the simple \"bag of words\" approach used in older systems (like those by Bagga & Baldwin, 1998; Gooi & Allan, 2004; Pedersen et al., 2005) that didn't really try to dig into deeper semantic features, which are tricky to pull out. We're also trying to avoid relying too much on specific biographical details (like what Mann & Yarowsky, 2003, did) that you don't see often in the kind of documents search engines usually pull up—stuff like place of birth or family relationships. The second part of our work involves applying these techniques to both English and Chinese news collections. Turns out, the methods work well for both languages, but when we look closely at the errors, we find some cool differences between the two.",
        "formal_text": "Convert casual text to formal text: The work we're talking about here focuses on two main areas related to making things more robust. First, we're looking at using features from both syntactic and semantic processing that can"
    },
    {
        "casual_text": "In this experiment, we looked at how having more strong leakage cue categories in a dataset affects how well a model works when you train it on one dataset and test it on another. Following the idea from (Chen et al., 2020), we measure inter-dataset validity by looking at the F1 drop, which is F1 in minus F1 cross. A smaller F1 drop means the model is more consistent across datasets, so it's better. From Figure 2, we can see that for each model, the F1 drop is smaller in the black cluster compared to the others. This suggests that training on datasets with lots of strong leakage cues (like BOL, RLT, LIAR, and OD) leads to better or even improved performance when testing on other datasets. In other words, it shows good inter-dataset validity. So, if you want to build a model that works well across different datasets, it's a good idea to use a lie detection dataset that has plenty of strong leakage cue categories.",
        "formal_text": "Convert casual text to formal text: In this experiment, we looked at how having more strong leakage cue categories in a dataset affects how well a model works when you train it on one dataset and test it on another."
    },
    {
        "casual_text": "The three annotation projects (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011) each used different tagsets and also varied in how they handled tokenization and a bunch of other linguistic choices. To make things consistent, we converted all three datasets to the universal tagset from Petrov et al. (2012) and treated stuff like numbers and URLs the same way across all the data. Like Foster et al. (2011), we tagged URLs, usernames, and hashtags as NOUNs. We didn’t mess with the tokenization.",
        "formal_text": "Convert casual text to formal text: The three annotation projects (Ritter et al., 2011; Gimpel et al., 2011; Foster et al., 2011) each used different tagse"
    },
    {
        "casual_text": "But another way of defining it could work too, depending on what you choose as the basic unit.",
        "formal_text": "Convert casual text to formal text: But another way defining it could work too, depending on what you choose as the basic unit. Convert casual text to formal text: But another way defining it could work too, depending on what"
    },
    {
        "casual_text": "WINDOW makes it easy to grow the interleaved corpus without repeating stuff. It randomly picks abstracts from an element, E, and keeps adding fresh abstracts to the next element as it moves along. On top of that, it randomly selects sentences from the chosen abstracts. This means the text-summary pairs in the corpus are always changing. Next, we'll talk about the two main parts of the INTERLEAVE algorithm: Selection and Interleaving.",
        "formal_text": "Convert casual text to formal text: WINDOW makes it easy to grow the interleaved corpus without repeating stuff. It randomly picks abstracts from an element, E, and keeps adding fresh abstracts to the"
    },
    {
        "casual_text": "You can find more info here: https://github.com/dbiir/UER-py/wiki/Modelzoo. Here's a quick rundown of some results: - Kim (2014): 42.53, 53.64, 14.51, 51.07, 66.29 - RCNN (Lai et al., 2015): 41.21, 68.52, 50.89, 63.54 - BiLSTM (Liu et al., 2016): 41.17, 57.13, 47.66, 65.69 - BiLSTM + Attention (Zhou et al., 2016): 39.97, 59.91, 47.44, 63.94 - FastText (Joulin et al., 2017): 40.61, 66.26, 50.12, 63.72 - DPCNN (Johnson and Zhang, 2017): 42.46, 63.25, 50.76, 66.32 - Transformer (Vaswani et al., 2017): 42.60, 64.71, 51.24, 66.18 - BERT-tiny (Jiao et al., 2019): 47.56, 53.38, 48.91, 66.21 - BERT-small (Turc et al., 2019): 47.29, 56.21, 51.21, 70.78 - BERT-base (Devlin et al., 2019): 47.60, 56.64, 51.61, 70.94 And for comparison: - Rabiner and Juang (1986): 22.19, 7.43, 11.04 - CRF (Lafferty et al., 2001): 28.56, 6.11, 10.01 - BiLSTM (Huang et al., 2015): 31.21, 1.64, 3.09 - BiLSTM-CRF (Lample et al., 2016): 30.33, 9.81, 14.48 - BERT-tiny (Jiao et al",
        "formal_text": "Convert casual text to formal text: You can find more info here: https://github.com/dbiir/UER-py/wiki/Modelzoo. Here's a quick rundown"
    },
    {
        "casual_text": "An investigation ruled that he killed himself, but some people aren't so sure and think it might have been an accident.",
        "formal_text": "Convert casual text to formal text: An investigation ruled that he killed himself, but some people ... Convert casual text to formal text: An investigation ruled that he killed himself, but some people ... Convert casual"
    },
    {
        "casual_text": "This paper talks about tackling two main issues: polysemy (when a word has multiple meanings) and having a phrasal lexicon. Like Guthrie and Yuasa, we use a vector representation, meaning each article is represented as a vector. But here's the twist: instead of assigning each coordinate of the vector to every word in the article, we focus on a single word (specifically, a noun) whose meaning we’ve figured out. Our way of figuring out the meaning of a word is based on Niwa's method, which compares the similarity between two sentences—one that has a noun with multiple meanings and another that’s like a dictionary definition. To handle Walker's problem, after we’ve sorted out the meanings, we calculate how related the words are in terms of meaning and group the ones that are related together.",
        "formal_text": "Convert casual text to formal text: This paper talks about tackling two main issues: polysemy (when a word has multiple meanings) and having a phrasal lexicon. Like Guthrie and"
    },
    {
        "casual_text": "Since the title words are already part of the nodes in X, we can just add another type of edge, like we did in section 3.2.4, to have three types of edges for the same pair of nodes: edges from X, R, and the title itself. For the edges from X, we use the proximity of the title words based on their positions to determine the edge weights. For the title edges, we use the context from the title to update the contextualized representations, kind of like what we did in equations (3, 9). The results are in Table 4.",
        "formal_text": "Convert casual text to formal text: Since the title words are already part of the nodes in X, we can just add another type of edge, like we did in section 3.2.4, to have three types of edges for the same"
    },
    {
        "casual_text": "Even though we've made a lot of progress in figuring out how sentences work for things like natural language processing, and it’s helped us in a lot of ways, there haven’t been many studies looking at how context plays a role. Plus, there are still a bunch of issues we haven’t quite figured out yet.",
        "formal_text": "Convert casual text to formal text: Even though we've made a lot of progress in figuring out how sentences work for things like natural language processing, and it’s helped us in a lot of ways, there haven"
    },
    {
        "casual_text": "Our framework has two main settings: the number of search sentences and the threshold for the final output. We picked these two settings by testing them on a development set. Figures 2 and 3 show how the F1 score changes with different numbers of search sentences and thresholds. On the ERD development set, we got better results when the search number was between 600 and 800, and the threshold was either 0.55 or 0.6. For the GERDAQ development set, better results came when the search number was between 700 and 1000, and the threshold was between 0.45 and 0.5. Based on these results, in our experiment, we set the number of sentences to 700 and the threshold to 0.56 for the ERD dataset. For the GERDAQ dataset, we went with 800 sentences and a threshold of 0.48, all based on the F1 scores we got from the development set.",
        "formal_text": "Convert casual text to formal text: Our framework has two main settings: the number of search sentences and the threshold for the final output. We picked these two settings by testing them on a development set. Figures 2 and 3 show how"
    },
    {
        "casual_text": "We decided to test out five different models in our experiment: PRPN 1 (from Shen et al., 2018), URNNG 2 (by Kim et al., 2019b), CCM 3 (Klein and Manning, 2002), CCL 4 (Seginer, 2007), and DIORA 5 (Drozdov et al., 2019). We used the open-source versions of each model, making sure they could replicate the results from the original papers.",
        "formal_text": "Convert casual text to formal text: We decided to test out five different models in our experiment: PRPN 1 (from Shen et al., 2018), URNNG 2 (by Kim et al., 2019"
    },
    {
        "casual_text": "We looked at how the number of labeled troll accounts impacts how well a classifier works. We also found that basic methods relying only on content features don’t make the most of a lot of training data compared to methods that also use style and behavior features. Plus, we used a straightforward but super effective semi-supervised approach to boost performance when there’s hardly any data to work with.",
        "formal_text": "Convert casual text to formal text: We looked at how the number of labeled troll accounts impacts how well a classifier works. We also found that basic methods relying only on content features don’t make the"
    },
    {
        "casual_text": "We also use gradual fine-tuning for an event extraction task to demonstrate how versatile this approach is. Event extraction means identifying event triggers, the arguments involved, and their roles. We tested this by working with the ACE 2005 dataset, treating Arabic as the main language we're focusing on and English as a helpful secondary language.",
        "formal_text": "Convert casual text to formal text: We also use gradual fine-tuning for an event extraction task to demonstrate how versatile this approach is. Event extraction means identifying event triggers, the arguments involved, and their roles. We tested"
    },
    {
        "casual_text": "Both methods for making inferences in DATR are designed to create a system that can formally figure out what follows from the rules in a given DATR theory, T. The main focus is on figuring out the values linked to specific node/path pairs defined in the theory. However, the proof rules mentioned in (Evans and Gazdar, 1989a) aren't broad enough to handle all the necessary inferences, and it's not clear if the approach can be expanded to cover all DATR constructs. One particular issue is DATR's idea of nonlocal or global inheritance. The value expressed by a global inheritance descriptor depends on more than just the properties of the nodes specified in the theory's definitions. In reality, it only makes sense to discuss the value of a global descriptor in relation to a specific evaluation context, or global context. Since the proof rules from (Evans and Gazdar, 1989a) only deal with DATR sentences and don't explicitly mention context, it's tricky to provide a solid explanation of how the global inheritance mechanism works.",
        "formal_text": "Convert casual text to formal text: Both methods for making inferences in DATR are designed to create a system that can formally figure out what follows from the rules in a given DATR theory, T. The"
    },
    {
        "casual_text": "We're mainly focused on this problem: when you have a compound noun, how do you figure out which way it could be understood, from the most likely to the least likely? This problem can be thought of using something called unification. Unification is a neat way to describe how different pieces of information come together to make sense of things. It helps with figuring out the different meanings of words and structures, as well as how those meanings fit together. One cool thing about unification is that it doesn't have a favorite order for putting things together. It doesn't automatically prefer one way over another. This is really useful when you're trying to decide which meaning of a compound noun is the most likely. When dealing with compound nouns, you have to figure out the different possible meanings (lexical ambiguity), how those meanings fit together in a sentence (structural ambiguity), and how everything adds up to make sense (semantic composition). It's not a great idea to focus too much on just one part of this process because everything is connected. For example, the way you pick a \"frame\" (like a mental outline of what's happening) can change how you understand the words (lexical ambiguity). Also, choosing a frame and deciding who is doing what (case/role binding) are part of the same process of making sense of the whole thing (semantic composition). And how you resolve structural ambiguity often depends on how you're putting the meanings together.",
        "formal_text": "Convert casual text to formal text: We're mainly focused on this problem: when you have a compound noun, how do you figure out which way it could be understood, from the most likely to the least likely? This"
    },
    {
        "casual_text": "LetsMT! uses Moses as its go-to language-independent SMT solution and plugs it into the LetsMT! online platform as a cloud-based service. The Moses toolkit has been tweaked to work smoothly in a fast-paced environment where training, updating, and interactive access are key. The Moses SMT training process is made up of several steps, each needing a different program to run. Within the LetsMT! setup, this whole process is simplified and can be automatically adjusted based on user inputs, like training corpora, language model data, dictionaries, and tuning sets.",
        "formal_text": "Convert casual text to formal text: LetsMT! uses Moses as its go-to language-independent SMT solution and plugs it into the LetsMT! online platform as a cloud-based service. The Moses tool"
    },
    {
        "casual_text": "Got the \"BMES\" word sets for each character, right? Now, we need to crunch those word sets into a set of fixed-size vectors. In this project, we tried out two different ways to make that happen.",
        "formal_text": "Convert casual text to formal text: Got the \"BMES\" word sets for each character, right? Now, we need to crunch those word sets into a set of fixed-size vectors. In this project, we tried"
    },
    {
        "casual_text": "This paper talks about a single neural model that works for both Semantic Role Labeling (SRL) and Relation Classification (RC). It makes good use of the three types of features we mentioned earlier. Our model uses bidirectional LSTMs to grab global context and syntactic path features. We're particularly interested in SRL because, in our view, it's more complex and challenging. SRL is a task where you predict structures with some specific rules. To handle these rules, we added a post-processing step using Integer Linear Programming (ILP) for SRL. Plus, our combined model links SRL and RC, opening the door for multi-task learning. We found that sharing knowledge between RC and SRL can really boost SRL performance.",
        "formal_text": "Convert casual text to formal text: This paper talks about a single neural model that works for both Semantic Role Labeling (SRL) and Relation Classification (RC). It makes good use of the three types of features"
    },
    {
        "casual_text": "Distractor Selection. To pick the toughest distractors from the bunch, we suggest comparing how similar the original sentence with the correct answer is to the one with a distractor. Basically, the closer they are, the trickier the distractor. Figure 3 shows an example of how this works. Starting with the original sentence or a new one where the correct answer is swapped with a distractor, we use RoBERTa LARGE to grab two kinds of features. One is the context embedding, which is the sentence embedding from the [CLS] token, and the other is the word embedding, which is the token embedding for the answer or the distractors. Then, we mash these two embeddings together to figure out the cosine similarity between the sentences with the answer and a distractor. Lastly, we grab the top 3 distractors that are the most similar.",
        "formal_text": "Convert casual text to formal text: Distractor Selection. To pick the toughest distractors from the bunch, we suggest comparing how similar the original sentence with the correct answer is to the one with a distractor. Basically"
    },
    {
        "casual_text": "In FGD, role labels like ACT (for the actor), PAT (patient), ADDR (addressee), ORIG (origin), and EFF (effect) show which \"participant\" roles are involved in a verb's basic structure. These labels are more about the positions in the verb's \"valency frame\" than their names might make you think. They’re kind of like numbered argument spots in other systems. The PTG annotations are based on a machine-readable valency lexicon (created by Ureová et al. in 2016). The frame values on the verbal nodes in Figure 2 point to specific verb meanings in that lexicon.",
        "formal_text": "Convert casual text to formal text: In FGD, role labels like ACT (for the actor), PAT (patient), ADDR (addressee), ORIG (origin), and EFF (effect)"
    },
    {
        "casual_text": "Basically, we can figure out how MeSH terms are represented by looking at how words are represented.",
        "formal_text": "Convert casual text to formal text: Basically, we can figure out how MeSH terms represented by looking at how words represented. Convert casual text to formal text: Basically, we can figure out how MeSH terms represented by looking"
    },
    {
        "casual_text": "Check out the results for these models in Table 3. There's no clear-cut baseline for summarizing dialogues, which is kind of tricky. We thought Lead-3 would perform pretty poorly since the start of conversations usually just has greetings, not the main stuff. But it turns out, in our dataset, greetings often come with questions or sharing info—sometimes they're even skipped. So, Lead-3 did better than the MIDDLE baseline, which grabs lines from the middle of the dialogue. Still, the LONGEST-3 model ended up being the best for summarizing dialogues.",
        "formal_text": "Convert casual text to formal text: Check out the results for these models in Table 3. There's no clear-cut baseline for summarizing dialogues, which is kind of tricky. We thought Lead-3 would perform pretty poorly since the"
    },
    {
        "casual_text": "As shown in Figure 1, TTOS has three main parts: a teacher network focused on the knowledge base (T KB) that grabs entities from the KB, another teacher network (T DP) that learns dialogue patterns, and a student network (S) that aims to pull out accurate KB entities and create natural-sounding responses. All three networks have the same basic setup but are trained differently.",
        "formal_text": "Convert casual text to formal text: As shown in Figure 1, TTOS has three main parts: a teacher network focused on the knowledge base (T KB) that grabs entities from the KB, another teacher network (T"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. So,  y i, y i+1 is the chance of moving from one label (y i) to the next one (y i+1).  is like a grid (a matrix) that’s (k + 2) by (k + 2) big, where k is the total number of different labels we have. We add two extra labels, \"start\" and \"end,\" which are only used in this special layer (called CRF) to mark the beginning and end of a sequence. Normally, the CRF layer tries to find the most likely sequence of labels. But with this modified IOBES system, a sentence can have more than one correct label sequence, as you can see in Figure 1. So, we’ve tweaked the CRF layer to work a bit differently—we call it a \"fuzzy CRF model.\" Instead of just picking one sequence, it looks at all the possible sequences, including all the IOBES tags and the different entity types, and tries to maximize the total probability of all of them. We’ve written this goal mathematically in Equation 2.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. So,  y i, y i+1 is the chance of moving from one label (y i) to"
    },
    {
        "casual_text": "So, like, the PAIIO-SPANAM thing [Vasconcellos 1985] shows that when translating from Spanish to English, you can get decent results even if you skip the step where you figure out all the grammar stuff in the original sentence. English speakers can still catch mistakes and fix them. But, when it comes to Japanese and English, it’s a whole different story. Since these two languages have super different ways of ordering words, you really need to understand the full grammar of the Japanese sentence to translate it properly. Also, in Japanese-English translation, how you interpret the grammar of the original sentence can totally change the translation. So, you can’t just assume that the same kind of grammar stuff that works in one language will work in the other. The METAl group [Beneht 1985] says they usually just pick the best grammar option and translate that, and sometimes different grammar choices end up with the same translation. But, honestly, that doesn’t happen much when translating between Japanese and English. Plus, some grammar ideas that work fine in most European languages just don’t work well in Japanese. For example: 1. Japanese doesn’t really use stuff like \"the\" or \"a\" to show if a noun is specific or not. 2. And there’s no clear way to mark definite or indefinite things in Japanese like there is in English.",
        "formal_text": "Convert casual text to formal text: So, like, the PAIIO-SPANAM thing [Vasconcellos 1985] shows that when translating from Spanish to English, you can get decent results even if you skip"
    },
    {
        "casual_text": "Check out Table 1 for all the datasets and their splits we used in the experiments. We'll break down each one in the upcoming sections. For every classification task, we tested and tweaked three models, each with different random seeds, and then averaged the results from those three runs. As for the models themselves, we used 1D CNNs with the same setup across all tasks and datasets. The convolution layer had three filter sizes—2, 3, and 4—with 10 filters for each size, so that's a total of 30 filters (d = 10  3 = 30). We used ReLU as the activation function everywhere except for the output layer, which had softmax. The input documents were adjusted to have exactly 150 words (L = 150). We also used pre-trained 300-dim GloVe vectors (from Pennington et al., 2014) as non-trainable weights in the embedding layers. All the models were built using Keras and trained with the Adam optimizer. We used iNNvestigate (Alber et al., 2018) to run LRP on the CNN features, specifically using the LRP-propagation rule to keep the relevance scores stable (with a value of 107). Lastly, we got crowdsourced responses for selecting features to disable by using Amazon Mechanical Turk (MTurk). Each question was answered by ten workers, and we either took the majority vote or averaged the scores, depending on the type of question (we'll explain that next).",
        "formal_text": "Convert casual text to formal text: Check out Table 1 for all the datasets and their splits we used in the experiments. We'll break down each one in the upcoming sections. For every classification task, we tested and tweak"
    },
    {
        "casual_text": "Figuring out the best way to do this is tricky because the model only gets rewarded for giving right (or kind of right) answers, which doesn’t happen much at first. Using G  (y|x) from regular captioning data can be a good starting point, though. From there, our approach follows these steps:",
        "formal_text": "Convert casual text to formal text: Figuring out the best way to do this is tricky because the model only gets rewarded for giving right (or kind of right) answers, which doesn’t happen much at first. Using G"
    },
    {
        "casual_text": "And mechanics might be the easiest of these five traits to grade consistently. So, it makes sense that humans seem to struggle more with creativity than with mechanics.",
        "formal_text": "Convert casual text to formal text: And mechanics might be the easiest of these five traits to grade consistently. So, it makes sense that humans seem to struggle more with creativity than with mechanics. So, it makes sense that humans seem"
    },
    {
        "casual_text": "So, N Y (W x i ) is just the neighborhood connected to W x i in Y. In our approach, for a word w i, we pick the J Chinese words with the lowest CSLS scores as possible translations. We call the group of these translation options T (w i ).",
        "formal_text": "Convert casual text to formal text: So, N Y (W x i ) is just the neighborhood connected to W x i in Y. In our approach, for a word w i,"
    },
    {
        "casual_text": "So, the language models we've been talking about are generative, meaning they focus on modeling the joint distribution of a sequence of symbols (like in Equation (1)). Most researchers are busy trying to improve PPL (e.g., Jozefowicz et al., 2016), but not much attention has been given to how well these models can actually distinguish between useful and not-so-useful sentences when used in real tasks like ASR and MT (Li and Khudanpur, 2008). On the other hand, discriminative language modeling is all about boosting performance in practical applications. For instance, some studies (Roark et al., 2004; Roark et al., 2007) aim to enhance ASR accuracy. The main idea is that the model should be able to tell the difference between \"good\" and \"bad\" sentences for a specific task, rather than just focusing on grammatically correct ones. The usual approach (Dikic et al., 2013) is to create a binary classifier using hand-crafted features from sentences. But the problem is, it's not clear how these methods can make use of large, unannotated datasets that are often readily available. Plus, those hand-crafted features are kind of random and might not give the best results.",
        "formal_text": "Convert casual text to formal text: So, the language models we've been talking about are generative, meaning they focus on modeling the joint distribution of a sequence of symbols (like in Equation (1)). Most researchers are busy trying"
    },
    {
        "casual_text": "We're suggesting a method called co-training for Chinese word segmentation, which is useful when you don’t have much labeled data (data that’s already been segmented by humans) but you have a lot of unsegmented data in the same domain. Here’s how it works: we divide the features into two groups—character-level features and word-level features. Then, we create two separate segmenters: one that uses character-level features and another that uses word-level features, both trained on the small amount of labeled data we have. These two segmenters then help each other improve by working with the large amount of unsegmented data. Finally, we mix the character-level and word-level features together and use a more advanced segmenter to boost the performance of the co-training process. Our tests show that when we use just 10% of the data as labeled data and the remaining 90% as unsegmented data, co-training gives us a 20% performance boost compared to supervised training using all the labeled data in the SIGHAN 2005 PKU corpus. In the CU corpus, it gives us a 31% improvement.",
        "formal_text": "Convert casual text to formal text: We're suggesting a method called co-training for Chinese word segmentation, which is useful when you don’t have much labeled data (data that’s already been segmented"
    },
    {
        "casual_text": "Alright, so when you round down, you get the index of the last singular value that's considered important. This number is basically seen as the effective number of dimensions, or the rank, of the matrix X. Now, if d is the number of dimensions in the space where X lives, and we think that the number of word vectors in X is usually way bigger than d, then this idea makes sense (and you can check out Roy and Vetterli's work from 2007 for more details).",
        "formal_text": "Convert casual text to formal text: Alright, so when you round down, you get the index of the last singular value that's considered important. This number is basically seen as the effective number of dimensions, or the rank, of"
    },
    {
        "casual_text": "Lately, there's been a lot of buzz about neural unsupervised constituency parsing (check out Shen et al., 2018a; Drozdov et al., 2019; Kim et al., 2019b, among others). These methods have shown they can outperform basic baselines by a long shot. But here's the catch: many of these approaches rely on having the correct parse trees for all sentences in a development set. They use this data either to decide when to stop training early (Shen et al., 2018a; Shen et al., 2019; Drozdov et al., 2019, etc.) or to tweak hyperparameters (Kim et al., 2019a). On the flip side, models that don't use any labeled data at all for training or tuning (like Kim et al., 2019b; Peng et al., 2019) just don't perform as well.",
        "formal_text": "Convert casual text to formal text: Lately, there's been a lot of buzz about neural unsupervised constituency parsing (check out Shen et al., 2018a; Drozdov et"
    },
    {
        "casual_text": "In the unsupervised scenario, we don't have parallel sentence pairs (x, y). Instead, we've got a bunch of sentences that are labeled with their style attribute, like sentiment, and we'll refer to them as...",
        "formal_text": "Convert casual text to formal text: In the unsupervised scenario, we don't have parallel sentence pairs (x, y). Instead, we've got a bunch of sentences that are labeled with their style attribute,"
    },
    {
        "casual_text": "In this part, we're checking out how our system performs in two main areas: 1. We look at how well the Semantic Parser and CRF do in terms of precision, recall, and the F1 score for the stuff they pull out. 2. We compare how relevant the results are when we use a full query as a baseline versus when we use the Semantic Parse-based query.",
        "formal_text": "Convert casual text to formal text: In this part, we're checking out how our system performs in two main areas: 1. We look at how well the Semantic Parser and CRF do in terms of precision,"
    },
    {
        "casual_text": "We made a dataset using Yelp to show how helpful this problem can be and to use it as a standard for comparing different methods.",
        "formal_text": "Convert casual text to formal text: We made a dataset using Yelp to show how helpful this problem and to use it a standard for comparing different methods. Convert casual text to formal text: We made a dataset"
    },
    {
        "casual_text": "To make training faster, we're using the linear-time L1 ball projection method from Duchi et al. (2008) in our setup.",
        "formal_text": "Convert casual text to formal text: To training faster, we're using the linear-time L1 ball projection method from Duchi et al (2008) in our setup. Convert casual text to formal text: To training faster,"
    },
    {
        "casual_text": "If we take a closer look at the setup, the users, and the text, we can see that the users who got the most out of using MT were the ones who post-edited Text A. On the other hand, U4, who didn't see much of an increase in productivity, post-edited Text B, just like U2 and U6. These two actually saw their output decrease when they did the post-editing.",
        "formal_text": "Convert casual text to formal text: If we take a closer at the setup, the users, and the text, we can see that the users who got the most out of using MT were the ones who post-edited"
    },
    {
        "casual_text": "Automatic metrics usually don’t do a great job of understanding the meaning of sentences or how they’re structured. They also struggle when there’s only one correct answer to compare against. In fact, these metrics don’t match up with how humans judge things in tasks like fixing grammar mistakes (Napoles et al., 2015) or generating dialogue (Liu et al., 2016). Since we’re working on a new task and haven’t really tested how these metrics perform in this context, we think it’s important to get human feedback and see if these metrics line up with what people think is good.",
        "formal_text": "Convert casual text to formal text: Automatic metrics usually don’t do a great job of understanding the meaning of sentences or how they’re structured. They also struggle when there’s only one correct answer to compare against. In fact"
    },
    {
        "casual_text": "The main thing we used for this experiment was the MULTEXT-EAST multilingual parallel and comparative corpus, which Dimitrova and others talked about in 1998. We took some random sentences from the corpus to train with, and the rest were for testing. The same sentences were picked for every language we worked with.",
        "formal_text": "Convert casual text to formal text: The main thing we used for this experiment was the MULTEXT-EAST multilingual parallel and comparative corpus, which Dimitrova and others talked about in 1998. We took"
    },
    {
        "casual_text": "Alright, so using the rules [E and E, we can figure out that \"Mary likes John\" is a sentence like this: 7Mary likes John.",
        "formal_text": "Convert casual text to formal text: Alright, so using the rules [E and E, we can figure out that \"Mary likes John\" is a sentence like this: 7Mary likes John. 7"
    },
    {
        "casual_text": "An information-theory-based approach can tackle this problem (Voita and Titov, 2020) by treating probing like a data transmission issue. They look at how much effort it takes to pull out linguistic knowledge, not just the final performance of the probe. Turns out, this method gives more useful and reliable results compared to regular probing techniques. In our work, we use both edge probing and MDL probing.",
        "formal_text": "Convert casual text to formal text: An information-theory-based approach can tackle this problem (Voita and Titov, 2020) by treating probing like a data transmission issue. They look at how much"
    },
    {
        "casual_text": "But because word-based methods are so free-form, they often end up creating sentences that don't make much sense. If you only pick function words from a bunsetsu, the part of the compressed sentence it creates will sound really awkward. Take the sentence in Figure 1, for example. It has the bunsetsu \"kanada no\" (Canada's), but word-based methods don't really pay attention to that. If you just grab the word \"kanada\" (Canada) and leave out \"no\" (the genitive marker), the compressed sentence won't make any sense. So, as I mentioned, it's pretty tough to create sentences that sound right using word-based methods.",
        "formal_text": "Convert casual text to formal text: But because word-based methods are so free-form, they often end up creating sentences that don't make much sense. If you only pick function words from a bunsetsu, the"
    },
    {
        "casual_text": "In METAL, doing a morphological analysis is like breaking down words by looking at them from left to right and figuring out their parts. It’s a back-and-forth process where you try to find the smaller pieces that make up the word. This gives you a bunch of possible ways to understand the word, based on the morphemes (those smaller bits) that the system knows about.",
        "formal_text": "Convert casual text to formal text: In METAL, doing a morphological analysis is like breaking down words by looking at them from left to right and figuring out their parts. It’s a back-and-for"
    },
    {
        "casual_text": "It's worth mentioning that none of these grammars have empty productions or cycles, which can mess things up for algorithms dealing with left recursion. It's pretty straightforward to turn any CFG into an equivalent grammar that doesn't have these tricky issues. Initially, the PT grammar had some cycles, but we got rid of them by adding 78 more productions and 89 extra symbols to the grammar. Other than that, there were no empty productions or cycles in the original grammars. Paull's Algorithm, which is used to eliminate left recursion in CFGs, works by turning indirect left recursion into direct left recursion through an iterative process. It then uses a subprocedure to handle the direct left recursion. Some might recognize this as the first step in the textbook algorithm for converting CFGs to Greibach normal form (Greibach, 1965). The subprocedure for eliminating direct left recursion does a specific transformation (Hopcroft and Ullman, 1979, p. 96), which is part of the full algorithm (Aho et al., 1986, p. 177).",
        "formal_text": "Convert casual text to formal text: It's worth mentioning that none of these grammars have empty productions or cycles, which can mess things up for algorithms dealing with left recursion. It's pretty straightforward to turn"
    },
    {
        "casual_text": "Noun-Verb Ambiguity: In Hindi, a lot of nouns can also act like verbs (and sometimes even when they're changed to show tense) and the other way around. For example, verbs can turn into nouns in their infinitive form and still work as nouns. But even when they're doing that, they keep some of their original verb-like qualities. When they're being nouns, they only show up in the singular, oblique and singular, direct cases, and they change the same way other masculine nouns ending in // do in the language.",
        "formal_text": "Convert casual text to formal text: Noun-Verb Ambiguity: In Hindi, a lot of nouns can also act like verbs (and sometimes even when they're changed to show tense) and"
    },
    {
        "casual_text": "In this part, we’ll start by talking about the scenario-based dialogue model. Next, we’ll explain the imitation learning framework you can see in Figure 2. And lastly, we’ll go over the training objective.",
        "formal_text": "Convert casual text to formal text: In this part, we’ll start by talking about the scenario-based dialogue model. Next, we’ll explain the imitation learning framework you can see in Figure 2. And lastly, we’ll"
    },
    {
        "casual_text": "So, let's break this down in a simpler way. Each part of the text is talking about different models and how they perform on tasks like summarizing articles from CNN or DailyMail. Here's the info in a more casual tone: - **BART (Lewis et al., 2020)**: This model has an encoder-decoder setup with 12 layers each and a size of 1024. On the CNN/DailyMail task, it takes 2.4 seconds to process and gives a speedup of 7.7x compared to a baseline. - **DistilBART (Wolf et al.)**: This one is similar to BART but smaller—12 layers for the encoder and 6 for the decoder, still 1024 in size. It's a bit faster at 3.4 seconds and offers a speedup of 5.4x. - **ProphetNet (Qi et al., 2020)**: Another encoder-decoder model with 12 layers each and 1024 size. It takes 2.8 seconds and gives a speedup of 3.8x. - **T5 (Raffel et al., 2020)**: This model has an interesting setup—12 layers for the decoder and none for the encoder, with a size of 768. It takes 3.0 seconds and offers a speedup of 5.5x. - **UniLM (Dong et al., 2019)**: This one is an encoder-only model with 12 layers and a size of 768. It's the fastest at 1.7 seconds and gives a huge speedup of 9.6x. So, basically, these models are all trying to summarize articles quickly, and each has its own way of doing it, with different speeds and efficiencies.",
        "formal_text": "Convert casual text to formal text: So, let's break this down in a simpler way. Each part of the text is talking about different models and how they perform on tasks like summarizing articles from CNN or DailyMail."
    },
    {
        "casual_text": "Neural sequence generation models are super popular for stuff like machine translation (MT) (Bahdanau et al., 2015; Vaswani et al., 2017) and fixing up automatically translated stuff (Junczys-Dowmunt and Grundkiewicz, 2016; Chatterjee et al., 2016; Correia and Martins, 2019; Lopes et al., 2019). Basically, these models spit out one word at a time and depend on a certain setup. *Note: This was partly worked on during a research trip to NYU.",
        "formal_text": "Convert casual text to formal text: Neural sequence generation models are super popular for stuff like machine translation (MT) (Bahdanau et al., 2015; Vaswani et al., 2017)"
    },
    {
        "casual_text": "Machine learning has some really cool tools that help systems work in complicated situations, but these tools rely on having the right way to describe the important parts of the problem. To make learning work well, you usually need to design this description using knowledge about the specific area you're working in. This can be tricky because it requires someone who knows both machine learning and the area they're working on. Often, experts in the field share their knowledge through text, like guides or explanations meant to help people understand the topic. In this paper, we figured out a way to use that kind of expert-written text to create a better description for learning. We call this idea \"reading to learn.\"",
        "formal_text": "Convert casual text to formal text: Machine learning has some really cool tools that help systems work in complicated situations, but these tools rely on having the right way to describe the important parts of the problem. To make learning work well, you"
    },
    {
        "casual_text": "In our setup using the URN architecture, we're sticking with real numbers, so Q(x) is basically an orthogonal matrix. We'll keep using that term going forward.",
        "formal_text": "Convert casual text to formal text: In our setup using the URN architecture, we're sticking with real numbers, so Q(x) is basically an orthogonal matrix. We'll keep using that term going forward. Con"
    },
    {
        "casual_text": "What we're talking about here is the kind of discourse structure you find in the Penn Discourse Treebank (Miltsakaki et al., 2004). This style of annotation can help with figuring out temporal relations between sentences in a couple of ways. First off, the Penn Discourse Treebank marks the relationship between two sentences that are right next to each other. This relationship is between abstract things like events or ideas. If there's a relationship between two events, the timing of those events might also be something we care about when we're doing temporal annotation. The idea is that the way a document is structured can show us important timing stuff too. Here's an example from the Penn Discourse Treebank. There's a relationship between the events \"dropped\" and \"fell,\" and it's marked by the phrase \"in particular.\" If we were annotating the main events between two sentences, the timing between these events would be something we'd want to know. In this example (9), the parts that aren't important to the relationship are left out, and the key parts are highlighted in italics and bold. (9) Meanwhile, the average yield on taxable funds dropped nearly a tenth of a percentage point, the largest drop since midsummer.",
        "formal_text": "Convert casual text to formal text: What we're talking about here is the kind of discourse structure you find in the Penn Discourse Treebank (Miltsakaki et al., 2004). This style of annotation"
    },
    {
        "casual_text": "We ran some bootstrap tests (shoutout to Berg-Kirkpatrick et al., 2012) with a 95% confidence level to check for statistical significance.",
        "formal_text": "Convert casual text to formal text: We ran some bootstrap tests (shoutout to Berg-Kirkpatrick et al., 2012) with a 95% confidence level to check for statistical significance."
    },
    {
        "casual_text": "Sure! Here's the informal version: The link to the Sussex Informatics research group on NLP and morphology is [here](http://www.informatics.sussex.ac.uk/research/groups/nlp/carroll/morph.html). And the link to the LDC2006T13 readme file is [here](http://www.ldc.upenn.edu/Catalog/docs/LDC2006T13/readme.txt).",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: The link to the Sussex Informatics research group on NLP and morphology is [here](http://www.informatics."
    },
    {
        "casual_text": "Manish Shrivastava and his team came up with a system in 2008 that uses something called a Hidden Markov Model for Hindi. They used a stemmer as a kind of preprocessor to figure out the root of words. This system was built with 18 different parts of speech tags and managed to get an accuracy of 93.12%. Then, in 2011, Sanjeev Kumar Sharma and his group created a system using the same Hidden Markov Model to make the Punjabi Part of Speech tagger more accurate. They made a module that takes the output from an existing POS tagger and assigns the right tag to words that could have more than one tag. They tested it on a big chunk of text with 26,479 words and got an accuracy of 90.11%.",
        "formal_text": "Convert casual text to formal text: Manish Shrivastava and his team came up with a system in 2008 that uses something called a Hidden Markov Model for Hindi. They used a stemmer as a kind of"
    },
    {
        "casual_text": "We found that names from minority groups show up way less often than names from majority groups in the training data for neural language models. Because of this, these names are less likely to be treated as single tokens, get less context, and end up with overfit representations in the models.",
        "formal_text": "Convert casual text to formal text: We found that names from minority groups show up way less often than names from majority groups in the training data for neural language models. Because of this, these names are less likely to be treated as single token"
    },
    {
        "casual_text": "We tackled the challenge of automatically updating an existing programming comment when the code it’s tied to changes. To do this, we came up with a fresh approach that focuses on linking changes in the code with corresponding updates to the comment. This method generates a series of edit actions to guide how the comment should be adjusted. Our model did really well—it beat several rule-based systems and other comment generation models, both in terms of automated tests and feedback from people.",
        "formal_text": "Convert casual text to formal text: We tackled the challenge of automatically updating an existing programming comment when the code it’s tied to changes. To do this, we came up with a fresh approach that focuses on linking changes in"
    },
    {
        "casual_text": "For supervised constituency parsing, we use the pre-built parser by Kitaev and Klein (2018) as a starting point to compare it with DIORA and S-DIORA. We train it using the parse trees from the training set of the Penn Treebank (PTB) and test it on the validation data. To prepare the ground truth, we binarize it using the Stanford parser (Manning et al., 2014) and train for 10 epochs. The results for both binary trees and the original n-ary trees (ignoring labels in both cases) are in Table 1. Both DIORA and S-DIORA are trained from scratch using the structured SVM loss from Kitaev and Klein (2018).",
        "formal_text": "Convert casual text to formal text: For supervised constituency parsing, we use the pre-built parser by Kitaev and Klein (2018) as a starting point to compare it with DIORA and S-"
    },
    {
        "casual_text": "BigCloneBench. Our model does a bit better in precision compared to the baselines, showing that our approach with contrastive learning and structure info can make up for the lack of data. But, our recall isn't as good as GraphCodeBERT's, mainly because they've been trained on huge datasets with code graphs. We think it's important to expand our Java pre-training dataset for better code clone detection, and we're planning to work on that in the future.",
        "formal_text": "Convert casual text to formal text: BigCloneBench. Our model does a bit better in precision compared to the baselines, showing that our approach with contrastive learning and structure info can make up for the lack of"
    },
    {
        "casual_text": "UCCA, which stands for Universal Cognitive Conceptual Annotation (created by Abend and Rappoport in 2013), is rooted in cognitive linguistics and typological theories, mainly drawing from Basic Linguistic Theory by Dixon (published in 2010/2012). The shared task here focuses on the foundational layer of UCCA, which deals with argument structure, meaning it looks at how predicates (which can be verbs, nouns, adjectives, or other types) work. This broad level of semantic analysis has been found to stay consistent across different translations (as shown by Sulem et al. in 2015). It’s also been useful for enhancing text simplification (Sulem et al., 2018c) and has been applied to evaluating various text-to-text generation tasks (Birch et al., 2016; Sulem et al., 2018a; Choshen and Abend, 2018).",
        "formal_text": "Convert casual text to formal text: UCCA, which stands for Universal Cognitive Conceptual Annotation (created by Abend and Rappoport in 2013), is rooted in cognitive linguistics and typological theories,"
    },
    {
        "casual_text": "We can tweak different versions of the architecture by adjusting the  weights in the loss function, which lets us decide which parts are turned on. We tried out these different versions of the model:",
        "formal_text": "Convert casual text to formal text: We can tweak different versions of the architecture by adjusting the  weights in the loss function, which lets us decide which parts are turned on. We tried out these different versions of the model:"
    },
    {
        "casual_text": "Also, a bunch of studies looking at search engine query logs have found that a lot of people keep asking the same questions over and over (Silverstein et al., 1999; Spink et al., 2001). Apparently, the top 25 most common queries on AltaVista make up 1.5% of all searches, even though they’re just 0.00000016% of unique queries (Silverstein et al., 1999). This means that what people have searched for before could be really useful info. So, technologies that focus on common user behavior, like collaborative filtering (or recommendations), might be a smarter way to handle web searches. That’s why it’s worth talking more about whether using individual user data is a good idea.",
        "formal_text": "Convert casual text to formal text: Also, a bunch of studies looking at search engine query logs have found that a lot of people keep asking the same questions over and over (Silverstein et al.,"
    },
    {
        "casual_text": "Like earlier studies found, and like what we saw too, getting data for this kind of thing is pretty tough, especially when you’re trying to label whole speeches on a big scale. It’s a lot of work. To make it easier for people to study this, we changed the task a bit and set it up as a debate. We also made a special dataset for it. We gathered and are sharing over 3,600 debate speeches that are all labeled for this task.",
        "formal_text": "Convert casual text to formal text: Like earlier studies found, and like what we saw too, getting data for this kind of thing is pretty tough, especially when you’re trying to label whole speeches on a big scale. It’"
    },
    {
        "casual_text": "The translation you got for that word is basically right, but it doesn't fit the situation you're using it in (it's the wrong meaning of the word).",
        "formal_text": "Convert casual text to formal text: The translation you got for that word basically right, but it doesn't fit the situation you're using it in (it's the wrong meaning of the word). Convert casual text to formal"
    },
    {
        "casual_text": "(1) I really dug this movie. I can't put my finger on exactly why, but when I first watched it, it felt kinda weird, almost quirky, but I got used to that pretty fast because I actually love quirky movies. This one's from the early 80s. Perfect for a rainy, gloomy night. It's got a lot of suspense and some seriously strange stuff going on. You should totally add it to your collection. Oh, and about those reviews—out of the 73 we looked at, 27 were super short, with an average of 24 words. We’ve got a couple examples below.",
        "formal_text": "Convert casual text to formal text: (1) I really dug this movie. I can't put my finger on exactly why, but when I first watched it, it felt kinda weird, almost quirky, but I got used to that"
    },
    {
        "casual_text": "More than five campers have either gotten sunburned or caught a cold. So, it's possible that more than five campers have caught a cold too.",
        "formal_text": "More than five campers have either gotten sunburned or caught a cold. So, it's possible that more than five campers have caught a cold too. Convert casual text to formal text: More than five"
    },
    {
        "casual_text": "Alright, so we use data with RLE-ed unit sequences as input for all the systems, including VQ3 and VQ3  RLE. Both of these systems share the same U2S model. The original audio sample rates for LJSpeech and VCTK are 22050Hz and 48kHz, respectively. To keep things consistent and work with the spectrogram-to-waveform model, we down-sample the VCTK audio to 22050Hz. Following the Tacotron2 setup, we calculate an 80-dimensional Mel spectrogram for each audio file. We use a frame hop of 256 samples (about 11.6ms), a frame size of 1024 samples (around 46.4ms), and a Hann window function. Any utterances longer than 8 seconds get tossed out during training to avoid running into GPU memory issues. For LJSpeech, we stick to the data splits provided in the Tacotron2 GitHub repo. As for the multi-speaker VCTK dataset, we randomly pick 2.5% of the utterances from each speaker for validation.",
        "formal_text": "Convert casual text to formal text: Alright, so we use data with RLE-ed unit sequences as input for all the systems, including VQ3 and VQ3  RLE. Both of these systems share"
    },
    {
        "casual_text": "Okay, so basically, we're looking at how to find the value of q(t_i | s_a_i) by using something called \"align\" for each t_i in U(s_a_i) except t_i itself. The formula is: q(t_i | s_a_i) = align(s_a_i, t_i) / sum of align(s_a_i, t_j) for all t_j in U(s_a_i) In simpler terms, we're taking the alignment score between s_a_i and t_i, and then dividing it by the total alignment scores for all other t's in U(s_a_i).",
        "formal_text": "Convert casual text to formal text: Okay, so basically, we're looking at how to find the value of q(t_i | s_a_i) by using something called \"align\" for each"
    },
    {
        "casual_text": "We tried out a bunch of optimization techniques that are pretty common in NLP. We broke them down into three groups: margin, likelihood, and perceptron-like methods. For each one, we tweaked the loss function to fit the specific task we were working on. Mostly, we went with online methods because they're way faster than stuff like LBFGS (from Liu and Nocedal, 1989) or batch Exponentiated Gradient (by Collins et al., 2008).",
        "formal_text": "Convert casual text to formal text: We tried out a bunch of optimization techniques that are pretty common in NLP. We broke them down into three groups: margin, likelihood, and perceptron-like methods. For each one"
    },
    {
        "casual_text": "When dealing with word sense discrimination, we usually have a ton of contexts (let's call that number N) to group together. Trying out every possible number of clusters (k) from 1 all the way up to N would be super time-consuming. As k gets bigger, the quality of our grouping doesn't really get better—it kind of levels off. So, we figure out a limit for k, which we call deltaK, by looking at where the quality stops improving much as k grows.",
        "formal_text": "Convert casual text to formal text: When dealing with word sense discrimination, we usually have a ton of contexts (let's call that number N) to group together. Trying out every possible number of clusters ("
    },
    {
        "casual_text": "We picked the dataset from Tzioumis (2018) because it covers a lot of ground—about 85.6% of the U.S. population, according to Tzioumis. It’s also anonymized in an ethical way, has at least 30 observations for 91.2% of the names included, and uses first names based on self-identification, which Larson (2017) called the \"gold standard\" for figuring out demographics, especially when it comes to gender labeling. The dataset uses the same racial categories as the U.S. census for surnames, so it fits well with our cultural context. One thing to note is that some names can belong to more than one racial or gender group, which could add some noise to our analysis. But, over 80% of the names we looked at have a self-identification rate of at least 70% with a single racial group in Tzioumis’ dataset. From the Social Security Administration (SSA) data, we found that 31.3% of the names in our study are only used by males, 38.6% are only used by females, and 30.1% are used by both. For names that appear in both genders, 88% have at least 70% of their occurrences tied to one gender. So, labeling a name based on the group it’s most common in should help us see which groups are most affected by our findings and also highlight the linguistic clues about race and gender that make first names a good way to study this stuff.",
        "formal_text": "Convert casual text to formal text: We picked the dataset from Tzioumis (2018) because it covers a lot of ground—about 85.6% of the U.S. population, according to Tzioumis. It"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. Basically, (d) is like a big table or matrix that shows how important different words are in a document (d). Each row in the table represents a different word, and each column represents a different document. The numbers in the table are calculated using something called \"tf-idf,\" which stands for \"term frequency-inverse document frequency.\" This helps figure out how important a word is in a specific document compared to all the other documents. So, (d) looks something like this:  [ tf-idf(b1) ] [ ... ] [ tf-idf(bN) ]  And then, we take this table and multiply it by the tf-idf values for the specific document (d) we're looking at. This multiplication helps us focus on the words that are most important in that particular document. In short, (d) = XT • tf-idf(d), where XT is the table we talked about, and tf-idf(d) is the importance of words in document d.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way. Basically, (d) is like a big table or matrix that shows how important different words are in a document"
    },
    {
        "casual_text": "We all know that better evaluation metrics can lead to better machine translations, right? (Liu et al., 2011). So, this paper introduces a new automatic evaluation metric called LEPOR. It uses a bunch of extra factors to make its results pretty close to what humans would say. From what we’ve seen, LEPOR works better than other top metrics out there, like BLEU, TER, Meteor-1.3, and even the newer ones like AMBER and MP4IBM1. LEPOR performs really well across different languages, especially for Czech-to-English, Spanish-to-English, and English-to-Spanish translations. It also has a great mean correlation score without needing any extra tools or data. Plus, if you tweak some parameters—like adjusting the weights for recall and precision, or changing how many word matches you use—you can probably make LEPOR even better.",
        "formal_text": "Convert casual text to formal text: We all know that better evaluation metrics can lead to better machine translations, right? (Liu et al., 2011). So, this paper introduces a new automatic evaluation"
    },
    {
        "casual_text": "Alright, so once the skeleton is done, we move on to creating the DRUs. These DRU nodes are linked only to the (S)DRS nodes in the skeleton. So, we make DRU nodes one at a time, following the (S)DRS nodes in the skeleton. For each DRU, we deal with two kinds of symbols: one for the relations and another for the variables. First, we create all the relations, and then we add the variables for each relation step by step. This way, we end up with a sequence of...",
        "formal_text": "Convert casual text to formal text: Alright, so once the skeleton is done, we move on to creating the DRUs. These DRU nodes are linked only to the (S)DRS nodes in the"
    },
    {
        "casual_text": "On January 19th, 2010, just a week after the earthquake, we got an email from a colleague who was helping with the relief efforts. They asked if we could create a translation tool for Haitian Creole. Within a few hours, we pulled together a small group of developers, testers, and computational linguists to figure out how to make this happen. Here’s the thing: none of us knew anything about Creole. No native speakers, no background in the language (except maybe some basic stuff we learned in college), no clue about its grammar, writing system, or how it’s used in different situations. We also had no idea about things like how literate the people speaking it were or the differences between formal and informal speech. On top of that, we didn’t have any data in Haitian Creole, no documents, and no resources about the language. Basically, we were starting from scratch.",
        "formal_text": "Convert casual text to formal text: On January 19th, 2010, just a week after the earthquake, we got an email from a colleague who was helping with the relief efforts. They asked if we could create a translation tool"
    },
    {
        "casual_text": "Autoregressive generation (AG) models work by creating sequences step by step, moving from left to right. As you can see in Figure 1, if you have a source sequence X, the model generates a target sequence Y of length T by figuring out each element one after the other, using the stuff it’s already generated to decide what comes next.",
        "formal_text": "Convert casual text to formal text: Autoregressive generation (AG) models work by creating sequences step by step, moving from left to right. As you can see in Figure 1, if you have a source sequence X"
    },
    {
        "casual_text": "(2) A lot of the questions generated have super intense language because Reddit is a pretty casual place, and that can be a big deal, especially when we're talking about mental health. For example, something like \"Did you f***ing realize that f***ing people are f***ing too?\" (a generated question) was found to be really close to \"What do you think makes you a failure?\". So, T5 and its versions need to figure out what the user already knows or has said in their post. They should check which PHQ-9 questions can already be answered based on the post before coming up with more questions to avoid repeating stuff.",
        "formal_text": "Convert casual text to formal text: (2) A lot of the questions generated have super intense language because Reddit is a pretty casual place, and that can be a big deal, especially when we're talking about mental health."
    },
    {
        "casual_text": "Lately, beam search has been doing a great job in making NLP parsing tasks more efficient (Mabona et al., 2019; Fried et al., 2017; Dyer et al., 2016; Vinyals et al., 2015). Inspired by that, we came up with a new beam-search method that can automatically create discourse trees with structure and nuclearity details for documents of any length.",
        "formal_text": "Convert casual text to formal text: Lately, beam search has been doing a great job in making NLP parsing tasks more efficient (Mabona et al., 2019; Fried et al."
    },
    {
        "casual_text": "Bleu seems to struggle with telling the difference between random translation variations, which makes you wonder if it really matches how humans judge translation quality in some situations. The more identical scores you get from different versions, the less likely it is that all of them would feel equally good to a human. This is more of a theoretical issue, since the examples are made up, but it does show that Bleu isn't exactly a super accurate way to measure translation quality. There are a few key reasons why Bleu isn't that great:",
        "formal_text": "Convert casual text to formal text: Bleu seems to struggle with telling the difference between random translation variations, which makes you wonder if it really matches how humans judge translation quality in some situations. The more identical scores you get from different versions"
    },
    {
        "casual_text": "After a product has been around for a while, companies often come up with different versions of it to keep their spot in the market. They do this to attract customers who have specific needs. A common example is \"light\" versions of food brands. Usually, these new versions have names that are similar to the original brand but with a little extra word to show what kind of variation it is, like \"mini babybel\" or \"philadelphia light.\" We went through and listed out 11 common prefixes or suffixes and checked how often they show up with different food items.",
        "formal_text": "Convert casual text to formal text: After a product has been around for a while, companies often come up with different versions of it to keep their spot in the market. They do this to attract customers who have specific needs. A"
    },
    {
        "casual_text": "How many countries and workers from each country are taking part in each evaluation task.",
        "formal_text": "Convert casual text to formal text: How many countries and workers from each country are taking part in each evaluation task."
    },
    {
        "casual_text": "Generalization across different attacks: The results show that the model handles other text attacks pretty well (check rows 7-9 in each table) and even performs better against TextFooler. On the flip side, the baseline FGWS really struggles with more complex attacks, like BAE, which creates context-aware changes.",
        "formal_text": "Convert casual text to formal text: Generalization across different attacks: The results show that the model handles other text attacks pretty well (check rows 7-9 in each table) and even performs better against TextFooler On the flip"
    },
    {
        "casual_text": "(3) We tested conditional significance pruning in two different situations, one where we sort of treat the whole corpus like it's part of the phrase table. This doesn't mess up the translation quality, even though it means we have to get rid of all the phrase pairs that only show up once. There's a lot of proof that this could be a smart move for big datasets, though it can cause issues with smaller ones.",
        "formal_text": "Convert casual text to formal text: (3) We tested conditional significance pruning in two different situations, one where we sort of treat the whole corpus like it's part of the phrase table. This doesn't mess up the translation quality"
    },
    {
        "casual_text": "We gathered comments from different time periods: March 27, 2018, to March 29, 3018, for English, September 2018 to March 2018 for French, and November 2017 to March 2018 for Japanese. The big gap in the collection times is because the number of comments and the amount of noise (stuff that's not helpful) varies a lot between these languages.",
        "formal_text": "Convert casual text to formal text: We gathered comments from different time periods: March 27, 2018, to March 29, 3018, for English, September 2018 to March 2018 for French, and November 2017 to March 2018 for Japanese. The big gap"
    },
    {
        "casual_text": "h gives a clear and right explanation of an idiom in a specific situation by picking bits from the idiom itself, its meaning, and how the words are connected. It keeps the good and important stuff while leaving out the wrong or not-so-important parts.",
        "formal_text": "Convert casual text to formal text: h gives a clear and right explanation of an idiom in a specific situation by picking bits from the idiom itself, its meaning, and how the words are connected. It keeps"
    },
    {
        "casual_text": "Fourth, our parser creates a clear syntactic tree for the input, but it also throws in some semantic details. Instead of jumping straight to a full semantic representation, it takes this approach.",
        "formal_text": "Convert casual text to formal text: Fourth, our parser creates a clear syntactic tree for the input, but it also throws in some semantic details. Instead of jumping straight to a full semantic representation,"
    },
    {
        "casual_text": "Figure 3 shows how our systems perform at different points, and Table 3 gives the results for our best setups on the ENNI development and test sets. Interestingly, it turns out that neither the feature set we use nor whether mazes are included really impacts the system's performance. This is quite different from Microsoft Word's grammar checker, which doesn't work well when mazes are part of the data. The system by Morley et al. (2013) can handle mazes, but it still doesn't perform as well as the one we're proposing.",
        "formal_text": "Convert casual text to formal text: Figure 3 shows how our systems perform at different points, and Table 3 gives the results for our best setups on the ENNI development and test sets. Interestingly, it turns out that neither the feature"
    },
    {
        "casual_text": "A QA sample has a question, a passage, and an answer span. Sometimes, there might be more than one part of the passage that matches the answer. To keep things simple, most baseline codes just use the first match they find for training. But, when you think about the context and the meaning of the question and answer, some parts of the passage are more likely to be the right answer than others. To find the best possible answer span, we use something called a universal sentence encoder (Cer et al., 2018) to turn the question and the sentences in the passage into fixed-size vectors. We then pick the part of the passage that’s most similar to the question based on cosine similarity and call it the golden span. In our tests, this method helped improve performance on some datasets, but on others, it actually made things worse.",
        "formal_text": "Convert casual text to formal text: A QA sample has a question, a passage, and an answer span. Sometimes, there might be more than one part of the passage that matches the answer. To keep things simple, most"
    },
    {
        "casual_text": "The graph transformer starts by using graph convolution networks (GCNs), which were introduced by Kipf and Welling in 2017, to figure out the features of the graph nodes. When dealing with multiple graphs, we set up a K-head graph convolution system. This setup is kind of like the transformer model from Vaswani et al. in 2017, where they use K different graph convolution networks, combine them, and then add a residual connection afterward.",
        "formal_text": "Convert casual text to formal text: The graph transformer starts by using graph convolution networks (GCNs), which were introduced by Kipf and Welling in 2017, to figure out the features of the graph nodes. When dealing with"
    },
    {
        "casual_text": "Sure! Here's a more casual version: \"Check out Grammarly here: https://www.grammarly.com/12 and Microsoft Word 2013 here: https://products.office.com/en-us/microsoft-word-2013.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: \"Check out Grammarly here: https://www.grammarly.com/12 and Microsoft Word 2013 here: https://products.office."
    },
    {
        "casual_text": "Since people are pretty predictable when it comes to asking for clarification, we can use that to our advantage. This gives us a bunch of sneaky ways to handle how users talk next. Turns out, \"recipient design\" is a pretty cool trick that, if we do it right, can really help make our dialogue system smarter and more user-friendly.",
        "formal_text": "Convert casual text to formal text: Since people are pretty predictable when it comes to asking for clarification, we can use that to our advantage. Since people are pretty predictable when it comes to asking for clarification, we can use that to our advantage"
    },
    {
        "casual_text": "We ran some experiments to figure out if a piece of text is satirical or not using the SaRoCo dataset. To check how well we did, we looked at precision and recall for both types of text. We also used macro F1 and micro F1 (which is basically accuracy) to combine these scores. In Table 5, you can see how our two basic models performed on the SaRoCo validation and test sets. It turns out that both models are better at spotting satire than regular news when it comes to precision. For regular news, there's a bit of a trade-off where recall is higher, meaning they catch more of those but might be less accurate. Since both models act the same way, we think this behavior is probably due to the nature of the satire detection task itself.",
        "formal_text": "Convert casual text to formal text: We ran some experiments to figure out if a piece of text is satirical or not using the SaRoCo dataset. To check how well we did, we looked at precision and"
    },
    {
        "casual_text": "Donald Trump, who's running for president, thinks it's totally nuts and not cool at all to suggest using Syrian refugees to fix up Detroit. He says it's not fair to the American workers already living there who are struggling to find jobs.",
        "formal_text": "Donald Trump, who's running for president, thinks it's totally nuts and not cool at all to suggest using Syrian refugees to fix up Detroit. He says it's not fair to the American workers already living there who are struggling"
    },
    {
        "casual_text": "Out of all the people who answered, three had some experience with MT and post-editing. They all worked with the French-English pair at least, and two of them also did French-German. Two others worked with French-Spanish, one did French-Italian, and one did French-Russian.",
        "formal_text": "Convert casual text to formal text: Out of all the people who answered, three had some experience with MT and post-editing. They all worked with the French-English pair at least, and two of them also did French-"
    },
    {
        "casual_text": "Most automatic summarization methods these days are extractive, meaning they just pull out sentences from the original text based on things like word patterns or sentence structure. They rank or score the sentences and then grab the top ones, with not much done to tweak them afterward (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). But this approach has its limits compared to abstraction, which can create new sentences that capture the main ideas better (Carenini and Cheung, 2008).",
        "formal_text": "Convert casual text to formal text: Most automatic summarization methods these days are extractive, meaning they just pull out sentences from the original text based on things like word patterns or sentence structure. They rank or score the sentences and then"
    },
    {
        "casual_text": "We've updated the neural L-PCFG model to include the idea of concreteness by adding a couple of concrete-related features. Figure 3b shows the source (captions) and target (semantic role labels) for the alignment method we talked about in Figure 3a. Words in parentheses are the stop words we took out from the original caption. We used SpaCy's default list of stop words for English (Honnibal et al., 2020). The labels are arranged with the first term being the predicted activity, followed by the entities involved in that activity (Yatskar et al., 2016). The alignment pairs are listed from the highest score to the lowest.",
        "formal_text": "Convert casual text to formal text: We've updated the neural L-PCFG model to include the idea of concreteness by adding a couple of concrete-related features. Figure 3b shows the source (captions) and target"
    },
    {
        "casual_text": "Alright, so here's the deal: The output variable for the NP subgoal is the same as the one for the main goal. But the subgoal S\" has different controller lists. The  thing shows how the box around the S-node works—basically, no controllers are coming in, so the retards can't find their controlee within the S-procedure. The only controller that gets into the S goal is the one that’s introduced below the NP node, which is connected to the root S. It’s clear that the output variable for S has to be nil. There are some rules that let certain controllers pass through a boxed node—like the rule mentioned in (13) by BresnaKaplan.",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal: The output variable for the NP subgoal is the same as the one for the main goal. But the subgoal S\" has different controller"
    },
    {
        "casual_text": "Key Point Analysis (KPA) was introduced by Bar-Haim et al. (2020a, b) as a tough NLP task that’s closely connected to Computational Argumentation, Opinion Analysis, and Summarization. It has a lot of practical uses too (Bar-Haim et al., 2021). The idea is to take a big pile of short, opinionated texts about a specific topic and figure out the most important key points (KPs) that come up, along with how often they appear. So, the result is kind of like a bullet-point summary, but with a focus on how common each point is. If we can nail KPA, it could help us better understand public opinions from places like social media or surveys. This could create a new way for decision-makers to connect with people who might be affected by their choices.",
        "formal_text": "Convert casual text to formal text: Key Point Analysis (KPA) was introduced by Bar-Haim et al. (2020a, b) as a tough NLP task that’s closely connected to Co"
    },
    {
        "casual_text": "Besides using neural models that work with word sequences, adding dependency trees to these models has been shown to help with relation extraction by better capturing relationships that are far apart. Xu et al. (2015b) took the idea of dependency path kernels and used a LSTM network on the shortest path between entities in the tree. Liu et al. (2015) started by using a recursive network on the subtrees connected to the words in the dependency path, and then applied a CNN to the path itself. Miwa and Bansal (2016) used a Tree-LSTM (a more general version of LSTM designed for dependency trees) for both entity and relation extraction. They found it worked best when applied to the subtree rooted at the lowest common ancestor of the two entities.",
        "formal_text": "Convert casual text to formal text: Besides using neural models that work with word sequences, adding dependency trees to these models has been shown to help with relation extraction by better capturing relationships that are far apart. Xu e"
    },
    {
        "casual_text": "What’s really interesting is that push-down store grammars are actually a smaller group within something called linear bounded automata. These linear bounded automata are part of a bunch of different types of automata that sit somewhere between Turing machines and finite automata. A lot of people have been looking into these recently because Turing machines are way too abstract to be super useful in real life, while finite automata are too limited for what we need. This whole area of study started with Myhill, but it’s still pretty new, kind of like how McNaughton described many other types of automata in his review. We’re still figuring out how all these models connect to language, but based on what little we know so far, it seems like almost all of them will eventually turn out to be relevant in some way.",
        "formal_text": "Convert casual text to formal text: What’s really interesting is that push-down store grammars are actually a smaller group within something called linear bounded automata. These linear bounded automata are part of a"
    },
    {
        "casual_text": "SpeechT5 uses a bunch of unlabeled speech data to figure out general ways to understand and create speech. It's trained on two main tasks: one where it tries to fill in missing parts of speech (bidirectional masked prediction) and another where it turns one speech sequence into another (sequence-to-sequence generation).",
        "formal_text": "Convert casual text to formal text: SpeechT5 uses a bunch of unlabeled speech data to figure out general ways to understand and create speech. It's trained on two main tasks: one where it tries to fill"
    },
    {
        "casual_text": "We tested the FCN model on two things: question classification and sentiment analysis. We used classification accuracy as the way to measure how well it did.",
        "formal_text": "Convert casual text to formal text: We tested the FCN model on two things: question classification and sentiment analysis. We used classification accuracy as the way to measure how well it did."
    },
    {
        "casual_text": "This project has a lot of room for expansion. Here's what we're thinking: (i) We want to look into more functions for  to improve how we balance exploration and exploitation. (ii) We need to figure out better ways to assign nuclearity, especially since our evaluation showed too much N-N classification. (iii) We plan to test our approach on more sentiment datasets, like the one from Diao et al. (2014), and build even bigger treebanks. (iv) Our scalable solution could be expanded to predict discourse relations in addition to structure and nuclearity. (v) We’re thinking of combining our large-scale treebank with a neural discourse parser, like the one by Yu et al. (2018), to really make the most of data-driven discourse parsing. (vi) With the new MEGA-DT corpus, we want to revisit discourse-guided sentiment analysis to improve systems, especially for longer documents. (vii) Long-term, we’re interested in exploring other tasks like summarization, question answering, and machine translation for distant supervision of discourse. There’s a lot of annotated data available for these tasks (e.g., Nallapati et al. (2016); Cohan et al. (2018); Rajpurkar et al. (2016, 2018)). We’re also doing a qualitative analysis of the generated discourse trees to see how they’re shaping up.",
        "formal_text": "Convert casual text to formal text: This project has a lot of room for expansion. Here's what we're thinking: (i) We want to look into more functions for  to improve how we balance exploration and"
    },
    {
        "casual_text": "Dense video event captioning (Krishna et al., 2017) and multi-modal video event captioning (Iashin and Rahtu, 2020b) are about creating a series of captions for all events in a video, whether it's just from the video itself (uni-modality) or from both the video and speech (multi-modality). Figure 1 shows an example of how tricky this task can be, especially when dealing with both visual and speech elements. For the visual part, recognizing small or detailed objects can be really tough because of things like ambiguity, being hidden (occlusion), or changes in the object's state. In the example from Figure 1, the object \"dough\" is hidden in event 1, making it hard to spot just from the video. But if you look at the previous video frame, it's easier to recognize because it's clear and visible there. When it comes to speech, even though it provides useful semantic info (Shi et al., 2019; Iashin and Rahtu, 2020b), it also brings its own challenges. Speech can be informal, leading to issues like co-reference and ellipsis (where words are left out). In Figure 1, for event 3, the word \"dough\" is missing in the speech text, but it can still be figured out by looking at the context from other events, like event 1 in this case. In short, both the immediate surrounding clips and the overall context between events are super important for creating captions that make sense and don't repeat the same stuff over and over.",
        "formal_text": "Convert casual text to formal text: Dense video event captioning (Krishna et al., 2017) and multi-modal video event captioning (Iashin and Rahtu, 2020b)"
    },
    {
        "casual_text": "In collapsed Gibbs sampling, the sampler keeps going through all the possible spots where a boundary could be, based on what's happening in the rest of the text. It randomly decides if that part should be one word, like w1, or split into two words, like w2 w3 (where w1 = w2.w3). The chances of these decisions happening can be figured out using equation (1).",
        "formal_text": "Convert casual text to formal text: In collapsed Gibbs sampling, the sampler keeps going through all the possible spots where a boundary could be, based on what's happening in the rest of the text. It randomly"
    },
    {
        "casual_text": "We used Ted Pedersen's (2002) semantic similarity tool for these experiments. We picked five different methods that all use WordNet as their foundation and were created for word sense disambiguation tasks. The first one is Leacock and Chodorow's (1998) Normalized Path Length, which we'll just call lch for short. The way it calculates the semantic similarity between two words, w1 and w2, is laid out in Equation 1.",
        "formal_text": "Convert casual text to formal text: We used Ted Pedersen's (2002) semantic similarity tool for these experiments. We picked five different methods that all use WordNet as their foundation and were created for word sense disambiguation"
    },
    {
        "casual_text": "There’s a lot of research out there about how gender affects language, especially in the workplace. Some studies focus on this specifically (like Kendall and Tannen, 1997; Holmes and Stubbe, 2003; Kendall, 2003; Herring, 2008). We can’t go into all of it here because, well, there’s just too much. But we do want to highlight one paper that’s been really influential for us. Holmes and Stubbe (2003) did these two case studies that didn’t compare how male and female managers communicate differently. Instead, they looked at how female managers communicate in environments that are mostly female versus those that are mostly male. They found that even though female managers often break the stereotypes about “feminine” communication, they still use different strategies to connect with employees and show their authority depending on the gender makeup of the workplace. This got us thinking about how we could study this idea by adding “Gender Environment” to our research. By figuring out the ratio of men to women in a discussion, we can see if the way people communicate changes based on whether the group is mostly male or mostly female. This idea of a “gender environment” is also supported by some recent Twitter-based research on gender identity and how people use language differently (Bamman et al., 2014). One of the key takeaways from their work is that how people communicate in a gendered way depends on a bunch of factors, including who they’re talking to.",
        "formal_text": "Convert casual text to formal text: There’s a lot of research out there about how gender affects language, especially in the workplace. Some studies focus on this specifically (like Kendall and Tannen, 1997; Holmes and Stubb"
    },
    {
        "casual_text": "There are definitely a bunch of ways we could make our algorithm better. One idea is to mix it with fuzzy matching methods, like the ones mentioned in those papers by Cui and team from 2004 and 2005. That’s something we could explore in the future. We also know that if we want to use this algorithm on a bigger scale, with real people in real situations, we’d need way more training data. We could get that data semi-manually, maybe by using crowd-sourcing. Or, we’re also considering fully automated ways to gather it, or even using indirect human input, like what people click on in search engine results. Usually, when someone clicks on a search result, they only see the title and a short summary. So, it’s possible to imagine using a bunch of these summaries, paired with the search queries, as training data.",
        "formal_text": "Convert casual text to formal text: There are definitely a bunch of ways we could make our algorithm better. One idea is to mix it with fuzzy matching methods, like the ones mentioned in those papers by Cui and team from 2004 and"
    },
    {
        "casual_text": "In NLP, figuring out if two words are related because they come from the same ancestor is called automatic identification of cognates. To do this, NLP uses things like how many bigrams they share, the minimum edit distance, and the length of the longest common subsequence as features. These features help a linear classifier or a sequence labeler, which is trained on examples of related and unrelated word pairs. Once trained, this classifier can then be used to check new word pairs. The features for this classifier include string similarity scores, as mentioned by Hauer and Kondrak (2011) and Inkpen et al. (2005).",
        "formal_text": "Convert casual text to formal text: In NLP, figuring out if two words are related because they come from the same ancestor is called automatic identification of cognates. To do this, NLP uses things like how many"
    },
    {
        "casual_text": "We wanted to see if the deeper layers of a model that’s been fine-tuned for a specific task actually make a big difference in how well it performs. So, we came up with a straightforward experiment: we took the representations (basically, the outputs) from an intermediate layer of a fine-tuned model and fed them directly into the task’s output head, skipping the later layers entirely. We call these \"truncated models.\" We tried three different setups: (a) **UNTUNED**: We took the intermediate layer’s outputs from a fine-tuned model and plugged them straight into the already fine-tuned task output head, no extra fine-tuning needed. (b) **TUNED**: Here, we fine-tuned just the output head while keeping the intermediate layer’s outputs the same. (c) **TUNEDORIG**: We used the intermediate layer’s outputs from the base model (not fine-tuned for the task) but fine-tuned the output head. The **UNTUNED** truncated models show how well an intermediate layer’s representation can replace the final layer’s representation without any extra work. The **TUNED** and **TUNEDORIG** models give us an idea of the best performance we can get using the CLS representation from a specific layer—either from a fine-tuned encoder or the original, non-fine-tuned one.",
        "formal_text": "Convert casual text to formal text: We wanted to see if the deeper layers of a model that’s been fine-tuned for a specific task actually make a big difference in how well it performs. So"
    },
    {
        "casual_text": "I labeled the synthetic and DBDC3 dialogues myself, using the ISO 24617-2 annotation scheme to mark the dialogue control functions. Since this project is all about finding common patterns in dialogue sequences, I mixed different types of conversations—like task-focused ones and casual chats—as well as different kinds of participants, such as human-to-human and human-to-machine interactions. Check out Figure 1 for the process I used to figure out dialogue patterns and create natural-flowing conversations for DMS.",
        "formal_text": "Convert casual text to formal text: I labeled the synthetic and DBDC3 dialogues myself, using the ISO 24617-2 annotation scheme to mark the dialogue control functions. Since this project is all about finding common patterns in"
    },
    {
        "casual_text": "Alright, so when we have a new sentence, the system needs to understand it and update the conversation model (DM). There are some rules and methods, like plan inference rules [A1179] and constraint satisfaction [LA87, Car87], that help figure out what the sentence might be part of and suggest a sequence of actions. Plus, there are some tricks called forecasting heuristics [Car87, SidSl] that help prioritize these actions to make the conversation flow smoothly. For instance, the semantic representation of sentence (1) would look something like this:",
        "formal_text": "Convert casual text to formal text: Alright, so when we have a new sentence, the system needs to understand it and update the conversation model (DM). There are some rules and methods, like plan inference rules [A11"
    },
    {
        "casual_text": "This idea is backed up by some research done by Pollack, Hirsehberg, and Webber in 1982. They looked at real conversations between experts and beginners and thought that these chats could be seen as a kind of negotiation. In these talks, they found that people not only work towards a solution that everyone agrees on but also get a better grasp of the words and ideas being used by everyone involved. The context model is part of IP's beliefs, just like her belief that it really shows what the plan being built by IS is all about.",
        "formal_text": "Convert casual text to formal text: This idea is backed up by some research done by Pollack, Hirsehberg, and Webber in 1982. They looked at real conversations between experts and beginners and thought that these chat"
    },
    {
        "casual_text": "Different setups for experiments can make a big difference in how well the same model performs. Even different ways of measuring performance can give you very different results. With the exact same outputs, you might see scores that vary by more than 20 F1 points.",
        "formal_text": "Convert casual text to formal text: Different setups for experiments can make a big difference in how well the same model performs. Even different ways of measuring performance can give you very different results. With the exact same outputs, you"
    },
    {
        "casual_text": "We talked about a parallel incremental model for natural language generation, specifically made for the speech-to-speech dialog translation system called DMDIALOG. We showed that using a parallel marker-passing scheme is a cool way to explore the natural parallelism in sentence production. This method covers all kinds of tree expansion and handles activations and selections of syntactic structures and words in a consistent way. Another thing worth mentioning is that our model is psychologically plausible, which is kind of a big deal since most research in natural language generation doesn’t really consider psychological studies. We think our parallel incremental generation model could be a solid step forward for developing interpreting telephony, especially for situations where simultaneous interpretation is needed.",
        "formal_text": "Convert casual text to formal text: We talked about a parallel incremental model for natural language generation, specifically made for the speech-to-speech dialog translation system called DMDIALOG. We showed that using a parallel"
    },
    {
        "casual_text": "So, Devlin and his team (2014) came up with this thing called a neural network joint model (NNJM). It’s like an upgraded version of the n-gram neural network language model (NNLM), but with a little extra window of source words to help it out, as you can see in Figure 1a. While this model works pretty well, it’s super expensive to use for big tasks like large-vocabulary SMT because you have to calculate probabilities for the whole vocabulary. To fix this, Devlin and his crew figured out a way to train the NNJM so it can normalize itself and skip that expensive step during decoding. But here’s the catch: this self-normalization trick kind of messes with the accuracy of the neural network. Plus, training this self-normalized version takes forever, just like regular maximum likelihood estimation (MLE).",
        "formal_text": "Convert casual text to formal text: So, Devlin and his team (2014) came up with this thing called a neural network joint model (NNJM). It’s like an upgraded version of the n-gram neural"
    },
    {
        "casual_text": "TREC (Voorhees and Tice, 2000). We're working with the 6-class version of this dataset for question classification. The dataset includes open-domain, facet-based questions. For training and testing, there are 5,452 and 500 samples, respectively.",
        "formal_text": "Convert casual text to formal text: TREC (Voorhees and Tice, 2000). We're working with the 6-class version of this dataset for question classification. The dataset includes open-domain, facet-"
    },
    {
        "casual_text": "Both versions of chrF show that models trained on filtered data perform way better than the baseline model, which was trained on all the data. When we look at chrF scores for the real target text, the improvements over the baseline are smaller. But when we check chrF scores for the source text, the improvements are much bigger for all the models we tested. This makes the model outputs much closer to what humans would write.",
        "formal_text": "Convert casual text to formal text: Both versions of chrF show that models trained on filtered data perform way better than the baseline model, which was trained on all the data. When we look at chrF scores for"
    },
    {
        "casual_text": "The hybrid change scores are still hanging out between the slang and non-slang groups, with an average of 0.621  0.073. A permutation test shows there's a noticeable difference in how meaning changes between hybrid and slang words (p  0.001) and also between hybrid and non-slang words (p  0.05).",
        "formal_text": "Convert casual text to formal text: The hybrid change scores are still hanging out between the slang and non-slang groups, with an average of 0.621  0.073. A permutation test shows there'"
    },
    {
        "casual_text": "Hey, so there's this thing called \"Dipl. -Nath. Gisela Schlotter\" and it's about processing non-numeric data. Specifically, it talks about Part I: Subroutines (PI-17).",
        "formal_text": "Convert casual text to formal text: Hey, so there's this thing called \"Dipl. -Nath. Gisela Schlotter\" and it's about processing non-numeric data"
    },
    {
        "casual_text": "Sure! Here's the informal version: In our dataset, every speech is tagged with the person who gave it. This could be pretty cool for looking at how we figure out who said what, especially since the speeches are full of persuasive language—kind of like opinion pieces or stuff you see on social media. Plus, we’ve got both the audio recordings and the written transcripts for all the speeches, so you can also try out fancy multi-modal methods for this kind of task.",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: In our dataset, every speech is tagged with the person who gave it. This could be pretty cool for looking at how we figure out who said what,"
    },
    {
        "casual_text": "From the pairs p EDU, a EDU  given to the leaf nodes, you can figure out the sentiment polarity p and attention score a for any internal node in a random constituency tree by working your way up from the bottom. Among all the possible ways to combine these from Huber and Carenini (2019), the one that works the best is:",
        "formal_text": "Convert casual text to formal text: From the pairs p EDU, a EDU  given to the leaf nodes, you can figure out the sentiment polarity p and attention score a for any internal"
    },
    {
        "casual_text": "2. Once you're done analyzing utterance U 5, take out any discourse entities from the S-list that aren't actually mentioned in U.",
        "formal_text": "Convert casual text to formal text: 2. Once you're done analyzing utterance U 5, take out any discourse entities from the S-list that aren't actually mentioned in U. Take out any discourse entities from the S"
    },
    {
        "casual_text": "We're using RankNet (Burges et al., 2005) to train our ranking model. We calculate scores for both semantic and statistical factors, then combine them with weights to get the final probability of something being a principal. All the scoring parts are made up of simple linear functions.",
        "formal_text": "Convert casual text to formal text: We're using RankNet (Burges et al., 2005) to train our ranking model. We calculate scores for both semantic and statistical factors, then combine them with weights"
    },
    {
        "casual_text": "The situations where a word or phrase shows up can give us clues about what it means. Erk (2007) and Erk and his team (2010) looked at how words are used by seeing which other words appear alongside them. They figured out how similar two words are by comparing the patterns of words that come up with each one. Erk and his team (2010) shared the best method they had, which we used as our starting point, called the EPP baseline.",
        "formal_text": "Convert casual text to formal text: The situations where a word or phrase shows up can give us clues about what it means. Erk (2007) and Erk and his team (2010) looked at how words are used by seeing which other"
    },
    {
        "casual_text": "• Compare I with a (local) translation memory service and grab any possible matches with weights, like tm1 to tmm.",
        "formal_text": "Convert casual text to formal text: • Compare I with a (local) translation memory service and grab any possible matches with weights, like tm1 to tmm."
    },
    {
        "casual_text": "Alright, now we're going to take a closer look at these setups and show how they back up our second idea.",
        "formal_text": "Convert casual text to formal text: Alright, now we're going to a closer look at these setups and show how they back up our second idea. Convert casual text to formal text: Alright, now we'"
    },
    {
        "casual_text": "We take the samples that didn’t make it into the test sets and use them for training or tweaking models with noisy data. We use a regular expression to automatically break comments into sentences by spotting sentence endings. Then, we match up the source and target sentences. If this matching process doesn’t work (like when the source comment has a different number of sentences compared to the target comment after splitting), we just keep the whole comment without cutting it up. For the training data, we don’t check the translations as carefully as we do for the test data. Lastly, here’s a quick breakdown of the samples and tokens for each language pair: - **en-fr**: 36,058 samples, 841k source tokens, 965k target tokens - **fr-en**: 19,161 samples, 661k source tokens, 634k target tokens - **en-ja**: 5,775 samples, 281k source tokens, 506k target tokens - **ja-en**: 6,506 samples, 172k source tokens, 128k target tokens We also set aside around 900 samples for validation in each direction. You can check out the size of the data in Tables 1, 2, and 3 for the test, training, and validation sets, respectively. For tokenizing, we use the Moses tokenizer for English and French data, and Kytea for Japanese before counting the tokens in each dataset.",
        "formal_text": "Convert casual text to formal text: We take the samples that didn’t make it into the test sets and use them for training or tweaking models with noisy data. We use a regular expression to automatically break comments into sentences by"
    },
    {
        "casual_text": "START takes the parts of a sentence and turns them into something called embedded ternary expressions, or T-expressions for short. It connects the three main parts of a sentence: the subject (who or what the sentence is about), the object (what the subject is doing something to), and the relation (what's happening between them). So, for example, the sentence \"Gabriella might buy some stickers\" would become the T-expression Gabriella buy stickers>. Other bits like adjectives, possessive words, or phrases with prepositions can also make extra T-expressions, where prepositions and some special words act as the \"relation\" part.",
        "formal_text": "Convert casual text to formal text: START takes the parts of a sentence and turns them into something called embedded ternary expressions, or T-expressions for short. It connects the three main parts of a sentence"
    },
    {
        "casual_text": "Likewise, you can get all the argument pairs that the RVAG tagger Prvag predicts using the same method.",
        "formal_text": "Convert casual text to formal text: Likewise, you can get all argument pairs that the RVAG tagger Prvag predicts using the same method. Likewise, you can get all argument pairs that the RVAG tagger Pr"
    },
    {
        "casual_text": "Alright, to check how well our method works, we tested it on the Snips dataset (thanks to Coucke et al. in 2018). Snips is a dataset for personal voice assistants and covers 7 different areas with a total of 39 slots. Some of these slots are used across multiple areas, while others are only for specific ones. In Table 1, we’ve got all the details about the Snips dataset, like how many samples are in each area, which slots are shared between areas, and which ones are unique to a specific area. To test our framework, we picked one area at a time to be the target, and the other six areas became the source. This way, we could see how well it performs in different scenarios.",
        "formal_text": "Convert casual text to formal text: Alright, to check how well our method works, we tested it on the Snips dataset (thanks to Coucke et al. in 2018). Snips is a dataset"
    },
    {
        "casual_text": "Alright, so we're using something called knowledge graphs to rephrase some words in a list (we call these \"keys\"). Then, we figure out how related each word in the keys is to each word in another sentence (let's call it the \"hypothesis sentence\"). If the relationship score is higher than 0.5, we treat the word from the hypothesis sentence as a function, and it works in the same \"world\" as the word from the keys. To do this, we use a standard knowledge graph called ConceptNet (created by Liu and Singh in 2004). ConceptNet helps us find out how related the words from the keys and the hypothesis sentence are. Oh, and there's this thing in the knowledge base called a derivation tree, which is like a diagram showing how Bryce Dallas Howard has two kids. There's also a \"True\" predicate, which is just a fancy way of saying something is always true, no matter what. And \"BRYCE\" is short for \"BRYCE_DALLAS_HOWARD_\".",
        "formal_text": "Convert casual text to formal text: Alright, so we're using something called knowledge graphs to rephrase some words in a list (we call these \"keys\"). Then, we figure out how related each"
    },
    {
        "casual_text": "The process has two main parts. First up is fragment extraction:",
        "formal_text": "The process has two main parts. First up is fragment extraction: The process has two main parts. First up is fragment extraction: The process has two main parts. First up is fragment extraction: The process has two main parts. First up is"
    },
    {
        "casual_text": "Besides event extraction, GCN has been pretty effective for relation extraction, as shown in a paper by Zhang et al. in 2018. Instead of just grabbing tokens from the shortest dependency path, the authors tweaked things to create a pruned subdependency tree that also includes extra info from off-path stuff, like negation words. Out of the related work mentioned, JMEE is the one that’s most similar to what we’re doing—event extraction at the sentence level.",
        "formal_text": "Convert casual text to formal text: Besides event extraction, GCN has been pretty effective for relation extraction, as shown in a paper by Zhang et al. in 2018. Instead of just grabbing tokens from the shortest"
    },
    {
        "casual_text": "So, we've got  = a, b, c, and _n(•) and _e(•) are the node and edge factors, right? And Z(•) is just this big normalization thing that makes everything work out. We're working with log-linear factors here, so keep that in mind.",
        "formal_text": "Convert casual text to formal text: So, we've got  = a, b, c, and _n(•) and _e(•) are the node and"
    },
    {
        "casual_text": "Length-Control Inference. Summarization naturally involves controlling how long the output is, like when you need a short news headline for a mobile screen. Plus, Schumann and his team (2020) found that the main evaluation metric, ROUGE (Lin, 2004), is affected by how long the summary is—longer summaries usually get better ROUGE scores. So, it’s super important to keep the summary length in check for a fair comparison.",
        "formal_text": "Convert casual text to formal text: Length-Control Inference. Summarization naturally involves controlling how long the output is, like when you need a short news headline for a mobile screen. Plus, Schumann and his"
    },
    {
        "casual_text": "In step 2, we figure out how well ED-ITS and VENSES perform on individual linguistic phenomena, as well as on groups of these phenomena. Table 2 breaks down how often each phenomenon shows up in the dataset, based on the number of positive and negative pairs created for each one. You can see that some phenomena, like coreference and general inference, pop up way more than others. Also, some phenomena only let us create positive or negative examples, while others can be used for both. Since the datasets we used are pretty small, some phenomena don’t show up much, so the accuracy for those might not be super reliable.",
        "formal_text": "Convert casual text to formal text: In step 2, we figure out how well ED-ITS and VENSES perform on individual linguistic phenomena, as well as on groups of these phenomena. Table 2 breaks down how often each phenomenon shows"
    },
    {
        "casual_text": "For students trained with PKD and CKD, they also use a third loss along with L and L KD. Just like the other losses, this third one gets multiplied by a weight () to factor in its influence during training. In this setup,  = (1    ), with  = 0.1 and  = 0.7. The big value of  compared to the other weights highlights how important intermediate KD is for deep models. All these values were figured out through some testing to help reduce the final loss in translation engines.",
        "formal_text": "Convert casual text to formal text: For students trained with PKD and CKD, they also use a third loss along with L and L KD. Just like the other losses, this third one gets multiplied by"
    },
    {
        "casual_text": "To compare how important a word is for different classifiers, we adjust their weight vectors so they're all on the same scale. These attribute: value classifiers give us the most detailed alignments, which helps us connect one word to a specific attribute of any event.",
        "formal_text": "Convert casual text to formal text: To compare how important a word is for different classifiers, we adjust their weight vectors so they're all on the same scale. These attribute: value classifiers give us the most"
    },
    {
        "casual_text": "Alright, so we’ve looked at the same countries from two angles—how travelers see them and how locals see them. It’d be cool to compare the two and see what’s different or similar. Do people see themselves the way outsiders do? And are locals into the same stuff as tourists?",
        "formal_text": "Convert casual text to formal text: Alright, so we’ve looked at the same countries from two angles—how travelers see them and how locals see them. It’d be cool to compare the two and see what’s"
    },
    {
        "casual_text": "Here's Figure 7, showing how our E2E-UMGR model figures out which items are most important for recommendations during a conversation. The darker the color, the more relevant the items are at that point in the chat. Each row represents a different turn in the conversation.",
        "formal_text": "Convert casual text to formal text: Here's Figure 7, showing how our E2E-UMGR model figures out which items are most important for recommendations during a conversation. The darker the color, the more relevant the items are at"
    },
    {
        "casual_text": "Grammatical Error Correction (GEC) is all about spotting and fixing grammar mistakes in a sentence. With more and more people learning English these days, there's been a lot of focus on English GEC over the past few years.",
        "formal_text": "Convert casual text to formal text: Grammatical Error Correction (GEC) is about spotting and fixing grammar mistakes in a sentence. With more and more people learning English these days, there's been a"
    },
    {
        "casual_text": "Alright, so when you're trying to figure out what's going on with someone's problem, it's a good idea to ask questions that help them explain things better. Open-ended questions work great for this because they let the person talk more freely and really get into what's bothering them.",
        "formal_text": "Convert casual text to formal text: Alright, so when you're trying to figure out what's going with someone's problem, it's a good idea to ask questions that help them explain things better. Open-"
    },
    {
        "casual_text": "Glosses are basically short definitions, and they're a big deal in plWordNet. They help users get what the network is about and make it easier for the editors to work efficiently.",
        "formal_text": "Convert casual text to formal text: Glosses are basically short definitions, and they're a big deal in plWordNet. They help users get what the network is about and make it easier the editors work efficiently"
    },
    {
        "casual_text": "Inspired by the main question: \"With all the fast advancements in summarization models, do we need to rethink how we evaluate them?\" we took the human judgments we gathered and used them to assess current evaluation methods from four different angles. We wanted to see how well these methods can: (1) evaluate all types of systems.",
        "formal_text": "Convert casual text to formal text: Inspired by the main question: \"With all the fast advancements in summarization models, do we need rethink how we evaluate them?\" we took the human judgments we gathered and used"
    },
    {
        "casual_text": "Okay, let’s break this down in simpler terms. In natural language generation (NLG), things start with a \"world state,\" which basically means the situation or context provided by some application, like an expert system. This system needs to create a natural language text, so it kicks off the process. The end result is, of course, a natural language text. The generation process itself has a few steps: 1. Figuring out what the text should actually say (delimiting the content). 2. Deciding how the text will be organized (planning its structure). 3. Choosing the right words, grammar, and sentence order to make that structure work. 4. Finally, putting it all together into the actual text. In more advanced NLG systems, these steps aren’t just done all at once. Instead, they’re split into separate parts or modules that work together. Researchers play around with different ways to organize these modules and how they communicate with each other. For example, they might experiment with how control flows between the modules (like McKeown in 1985, Hovy in 1987, or Meteer in 1989). But no matter how the modules are set up or how they talk to each other, you still need to define how the system processes information and shares it between the different parts. This is where knowledge structures come in—they help everything run smoothly and make sure the modules can work together effectively.",
        "formal_text": "Convert casual text to formal text: Okay, let’s break this down in simpler terms. In natural language generation (NLG), things start with a \"world state,\" which basically means the situation or context provided by some application,"
    },
    {
        "casual_text": "P K3 uses three different k values to try and spot a point where the criterion function goes up and then suddenly drops. Basically, for each k, we check how its criterion function compares to the ones right before and after it.",
        "formal_text": "Convert casual text to formal text: P K3 uses three different k values to try and spot a point where the criterion function goes up and then suddenly drops. Basically, for each k, we check how"
    },
    {
        "casual_text": "In Table 3, we check out how MFS and Softmax perform on GPT-2 Small. Then, in the first two columns of Table 7, we look at examples from models based on GPT-2 Medium, trained on OpenWebText and Wikipedia 2021. The pattern looks pretty similar. The embedding for the right answer is different from the other options, so Softmax gives the correct answer lower probabilities, but MFS handles it way better. This makes us think that a bigger model like GPT-2 Medium also runs into the same issue with Softmax.",
        "formal_text": "Convert casual text to formal text: In Table 3, we check out how MFS and Softmax perform on GPT-2 Small. Then, in the first two columns of Table 7, we look at examples from models based on GPT"
    },
    {
        "casual_text": "This paper introduces a way to measure how similar two TV show summaries are by looking at the relationships between the nouns they use, like cause-and-effect or \"is a\" relationships. These connections are pulled automatically from the web, so even if the summaries don’t share a lot of words, we can still figure out how similar they are. Our approach creates a kind of map, where TV shows and nouns are the points, and the nouns are linked based on their relationships. To measure how similar two summaries are, we use a random walk algorithm to see how close their points are on this map. We tested this method on TV shows from NHK’s on-demand service and found that it works better than other methods we tried. It gives results that match more closely with what people think, compared to the other approaches we compared it to.",
        "formal_text": "Convert casual text to formal text: This paper introduces a way to measure how similar two TV show summaries are by looking at the relationships between the nouns they use, like cause-and-effect or \"is"
    },
    {
        "casual_text": "First off, we came up with a straightforward but adaptable ontology for a user memory graph, which is explained in Section 4. From there, we cooked up a model called the end-to-end user memory graph reasoner (E2E-UMGR) that handles two main tasks: updating the graph and creating dialog policies. The first task is treated like a graph generation problem (Li et al., 2018c), where it figures out new info from random utterances and adds it to the user memory graph. The second task tackles the issue of open-ended dialog policies by keeping the graph's structure intact while generating the necessary policies.",
        "formal_text": "Convert casual text to formal text: First off, we came up with a straightforward but adaptable ontology for a user memory graph, which is explained in Section 4. From there, we cooked up a model called the end"
    },
    {
        "casual_text": "Yeah, there are some downsides too. For one, the whole thing is pretty small—around 500 million tokens in total. Plus, most languages only have one version of the New Testament, which is about 200,000 tokens each. The focus is pretty narrow, and there’s a lot of overlap with names and stuff. This can cause weird issues if you use random word combinations, because the model might just spit out a bunch of random names instead.",
        "formal_text": "Convert casual text to formal text: Yeah, there are some downsides too. For one, the whole thing is pretty small—around 500 million tokens in total. Plus, most languages only have one version of the New Testament, which"
    },
    {
        "casual_text": "Sure! Here's a more casual version: You can find the data set over at http://homepages.inf.ed.ac.uk/gcong/qa/.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: You can find the data set over at http://homepages.inf.ed.ac.uk/gcong/q"
    },
    {
        "casual_text": "The subsumption relationship works in one direction, meaning that just because t a  s t b, it doesn't mean t b  s t a. Let me give you an example: \"literature\" can include \"chineseliterature\" because any document tagged with \"chineseliterature\" can also be tagged with \"literature.\" But if we flip it around, it doesn't work the same way.",
        "formal_text": "Convert casual text to formal text: The subsumption relationship works in one direction, meaning that just because t a  s t b, it doesn't mean t b  s"
    },
    {
        "casual_text": "The PosEdiOn analyzer can run either in text command mode or in a graphical mode. To launch the graphical user interface (shown in Figure 2), you can simply run the program without any parameters or use the --gui parameter. If you don’t include any parameters or only include some, the GUI will start up (check out Figure 3). If you want to use it in command line mode, you’ll need to add the right set of parameters. You can check what parameters are available by using the --h option.",
        "formal_text": "Convert casual text to formal text: The PosEdiOn analyzer can run either in text command mode or in a graphical mode. To launch the graphical user interface (shown in Figure 2), you can simply run"
    },
    {
        "casual_text": "When it comes to orthonormalized topics, using SVD, we can easily figure out the following:",
        "formal_text": "Convert casual text to formal text: When it comes to orthonormalized topics, using SVD, we easily figure out the following: Convert casual text to formal text: When it comes to orthonormalized topics, using SV"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: - \"semclass+gend (2)\" means we're looking at something related to semantic classes and gender, and it's the second instance of that. - \"Lin similarity (2)\" is about how similar things are based on Lin's method, and again, it's the second time we're mentioning it. - \"Lin (2) +sem+gend\" is just saying we're combining Lin's method with semantics and gender, and it's the second time we're doing that. - \"TheY+sem+gend\" is another way of referring to something related to \"TheY,\" semantics, and gender. - \"Lin+Bnd PL03+Bnd\" is talking about Lin's method with some boundaries (Bnd) and PL03 with boundaries as well. - \"GWN5Web\" means GWN5 comes before or is related to \"Web.\" - \"GWN5TheY+s+g\" means GWN5 also comes before or is related to \"TheY\" with semantics and gender. So, it's all about combining Lin's method with semantics and gender, and showing how different things like GWN5 and \"TheY\" fit into that.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: - \"semclass+gend (2)\" means we're looking at something related to semantic classes and gender,"
    },
    {
        "casual_text": "Subjectivity analysis has been getting more attention in the field of natural language processing (NLP) lately (Banea et al., 2010; Alm, 2011). Basically, subjectivity is all about expressing emotions, sentiments, opinions, beliefs, speculations, evaluations, and other personal feelings (Banfield, 1982; Wiebe, 1994). The goal of subjectivity classification is to figure out if a piece of text is subjective or objective (Wiebe and Cardie, 2005; Banea et al., 2008, 2010). This area has been studied a lot, especially because of the growing need for opinion-related applications. For example, it’s used to mine opinions from product reviews (Pang et al., 2002; Hu and Liu, 2004) or political news (Abbott et al., 2011), and to recognize stances in online debates (Wiebe, 2009, 2010). Plus, many NLP tasks use subjectivity analysis as an extra step to filter data. This has helped improve research in areas like conversation summarization (Seki et al., 2005; Carenini et al., 2008), information extraction (Riloff et al., 2005), text semantic analysis (Wiebe and Mihalcea, 2006), and question answering (Li et al., 2008; Yu and Hatzivassiloglou, 2003).",
        "formal_text": "Convert casual text to formal text: Subjectivity analysis has been getting more attention in the field of natural language processing (NLP) lately (Banea et al., 2010; Alm, 2011). Basically,"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way: **Golden Answer:** The London Exhibition. **Imprecise Answer 1:** The London Exhibition in 1862. **Imprecise Answer 2:** Exhibited at the London Exhibition. Now, let’s talk about how different tasks work, like the RACE (multi-choice) and SQuAD 2.0 (extractive) tasks. The stuff in **bold** is super important or tricky. In 2017 and later (like Trischler et al., 2017; Yang et al., 2018), extractive tasks focused on pulling out specific bits of info from the text. On the other hand, multi-choice tasks (Huang et al., 2019; Khashabi et al., 2018) needed to pick the right answer from a list of options. Even though both types of tasks are trying to figure out the right answer, they focus on different things. Multi-choice tasks usually need to look at the whole passage and pull together all the important details. Extractive tasks, though, are more about pinpointing exactly where the answer is in the text—they’re good at finding the right spot, even if it’s just a small part of the passage. You can see this difference in Table 1.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way: **Golden Answer:** The London Exhibition. **Imprecise Answer 1:** The London Exhibition in 1862"
    },
    {
        "casual_text": "Hey, so here's a quick rundown of some #MH17 updates: - A pro-Russia separatist mentioned that the bodies found in the Ukraine field weren't fresh, implying they were already dead before the plane took off. - #MH17 is back in the news: 1. #Kolomoisky admits to being involved (link provided) 2. He also got $1.8 billion from Ukraine's bailout funds. - Pro-Russia and pro-Ukraine sides are still at it. - Russia is now saying that #MH17 was taken down by an air-to-air missile, but of course, it wasn't Russian-made. Classic. - And Moscow's latest claim is that a Ukrainian fighter jet shot down #MH17.",
        "formal_text": "Convert casual text to formal text: Hey, so here's a quick rundown of some #MH17 updates: - A pro-Russia separatist mentioned that the bodies found in the Ukraine field weren't fresh,"
    },
    {
        "casual_text": "To show that CoRA can handle long-tail relations really well, we did a test just on those long-tail relations. We used the same setup as before, where Hits@K (Macro) measures how well it performs on these long-tail relations. From Table 2, you can see we compared CoRA to other models and our own base models. It turns out that CoRA boosts performance on long-tail relations by a big margin and sets a new record. When we look at previous models (like PCNN+HATT/+KATT) that also use relation hierarchies, our relation-augmented attention (Base) without any hierarchy still does pretty well, even without fancy pre-trained graph embeddings like PCNN+KATT uses. If we compare our base model with selective attention (PCNN+ATT), the big difference in performance shows how our framework is better at dealing with both wrong labels and long-tail relations. Lastly, as you can see in the last row of the table, if we take out the sent2rel attention we proposed, the performance drops a lot, which really highlights how important it is for handling long-tail relations.",
        "formal_text": "Convert casual text to formal text: To show that CoRA can handle long-tail relations really well, we did a test just on those long-tail relations. We used the same setup as before, where Hits@K ("
    },
    {
        "casual_text": "Graph embedding is all about creating continuous representations of nodes or edges in a graph, based on its structure. There are three main types of graph embedding methods, according to Goyal and Ferrara (2017): factorization-based (like Roweis and Saul, 2000; Belkin and Niyogi, 2001), random walk-based (e.g., Perozzi et al., 2014; Grover and Leskovec, 2016), and deep learning-based. Among these, random walk-based methods are pretty straightforward to understand and do a good job of preserving the importance and similarity of nodes. Two popular examples of random walk-based methods are Deepwalk (Perozzi et al., 2014) and node2vec (Grover and Leskovec, 2016). The basic idea behind Deepwalk is to treat random walk paths like sentences and then use a general word embedding model to process them. node2vec is similar but uses a biased random walk on graphs, which often leads to more efficient paths.",
        "formal_text": "Convert casual text to formal text: Graph embedding is all about creating continuous representations of nodes or edges in a graph, based on its structure. There are three main types of graph embedding methods, according"
    },
    {
        "casual_text": "Noun compounds can mean different things based on the words that make them up. For example, \"morning coffee\" is coffee you drink in the morning, and \"brick house\" is a house made of bricks. For (1c), the most reasonable way to understand it is as a relationship where students are the ones eating food.",
        "formal_text": "Convert casual text to formal text: Noun compounds can mean different things based on the words that make them up. For example, \"morning coffee\" is coffee you drink in the morning, and \"brick house\" is"
    },
    {
        "casual_text": "Since the examples were originally in German, we went ahead and translated them into English for this paper.",
        "formal_text": "Convert casual text to formal text: Since the examples were originally in German, we went ahead and translated them into English for this paper."
    },
    {
        "casual_text": "Smoothing isn’t exactly groundbreaking—it’s been around for a while. For example, people have been looking into smoothing techniques and functions for count-based language modeling for decades (Jelinek and Mercer, 1980; Katz, 1987; Church and Gale, 1991; Kneser and Ney, 1995; Chen and Goodman, 1996). What’s kind of cool is that when it comes to training neural networks (NNs), smoothing takes on a fresh approach. Instead of the usual methods, it’s now applied to the empirical one-hot target distributions.",
        "formal_text": "Convert casual text to formal text: Smoothing isn’t exactly groundbreaking—it’s around for a while. For example, people have been looking into smoothing techniques and functions for count-based language modeling for decades (Je"
    },
    {
        "casual_text": "We're testing the server load fault this Friday morning, PST time, and will let Bob know how it goes. The Fluency score is 2 because the sentence is a bit off—it starts with an \"ing\" verb and doesn't follow the usual structure. But the Completeness score is 4 since it covers the context and has all the important keywords. We set the commitment classifier confidence threshold to 0.9 and ended up with 29,000 potential To-Do items. Out of these, we randomly picked 12,000 for annotation. Here's how we did it: For each email (e_c) and the previous email in the thread (e_p, if there was one), we grabbed metadata like \"From,\" \"Sent-To,\" \"Subject,\" and \"Body.\" We highlighted the commitment sentence in e_c and asked annotators to create a To-Do item using all the info from both e_c and e_p. We made a detailed guideline to help annotators understand how to write these To-Do items, including what they are, how they should be structured, and some examples. Annotators were told to stick closely to the words and phrases in the emails and only add new vocabulary when necessary. Each email was reviewed by two annotators. In total, we got 9,349 emails with To-Do items, all double-checked by two annotators. The To-Do items have a median length of 9 tokens and an average length of 9.71. About 60.42% of the time, both annotators agreed that the subject line was useful for writing the To-Do item.",
        "formal_text": "Convert casual text to formal text: We're testing the server load fault this Friday morning, PST time, and will let Bob know how it goes. The Fluency score is 2 because the sentence is a bit off—it"
    },
    {
        "casual_text": "Sure! Here's the informal version: \"Check out this link for more info: http://alaginrc.nict.go.jp/hyponymy/index.html. It's the same site: http://alaginrc.nict.go.jp.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: \"Check out this link for more info: http://alaginrc.nict.go.jp/hyponymy/index.html"
    },
    {
        "casual_text": "The experiments show that getting rid of that noise helps our model work better.",
        "formal_text": "Convert casual text to formal text: The experiments show that getting rid of that noise helps our model work better."
    },
    {
        "casual_text": "Around 1% of BCCWJ is considered the \"core data,\" and this core data is manually annotated with extra linguistic details. The part of the core data that includes annotations for dependency structures, coordinate structures, coreference relations, and predicate argument structures is specifically called BCCWJ-DepParaPAS (Ueda et al., 2015; Maekawa et al., 2014). BCCWJ-DepParaPAS follows the annotation style of the NAIST Text Corpus (Iida et al., 2007b), where three key arguments—ga (nominative), wo (accusative), and ni (dative)—are marked at the SUW level.",
        "formal_text": "Convert casual text to formal text: Around 1% of BCCWJ is considered the \"core data,\" and this core data is manually annotated with extra linguistic details. The part of the core data that includes annotations for dependency"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: \"Censoring PU learning is a simpler way to approach the Document Set Expansion task. In this scenario, the labeled positive (LP) data comes from the same distribution as the unlabeled data, which is called censoring PU learning. To set up this situation, you can follow these steps:\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: \"Censoring PU learning is a simpler way to approach the Document Set Expansion task. In this scenario"
    },
    {
        "casual_text": "Another thing worth mentioning is the TSNLP project at Essex University. They're working on creating test suites for natural language tools, including machine translation (MT). Oh, and let's not forget that some of the most detailed reviews and evaluations of MT systems and translation tools have come from European organizations. Most recently, there were two reports from Ovum Ltd. in London—one about the global market and future outlook, and another about the current translation technology out there.",
        "formal_text": "Convert casual text to formal text: Another thing worth mentioning is the TSNLP project at Essex University. They're working on creating test suites for natural language tools, including machine translation (MT). Oh, and let's"
    },
    {
        "casual_text": "So, to start with, we're using a basic system for figuring out which words refer to the same thing in a text. First, the text goes through some automatic processing steps, like finding where sentences start and end, tagging parts of speech, recognizing names and other entities, and breaking the text into chunks of phrases. This setup is similar to what Soon et al. did back in 2001. For the named entity recognition, part-of-speech tagging, and noun phrase chunking, they all use the same Hidden Markov Model (HMM) engine, which learns from mistakes to get better over time, as described by Su in 2000. When training the system, for every pronoun or anaphor (words like \"he,\" \"she,\" \"it\"), we pair it with its closest possible antecedent (the word it's referring to) to create a positive example. Then, we also pair it with all the other possible candidates that aren't the right antecedent to create negative examples. Using these training examples, we train a binary classifier with a specific learning algorithm. In this case, we're using SVMLight, which was developed by Joachims in 1998. When the system is actually trying to figure out the antecedents in new text, it pairs each anaphor with every possible antecedent that comes before it to create test examples. These are then fed into the classifier, which gives a confidence score for each candidate, showing how likely it is to be the right antecedent. The candidate with the highest score is picked as the antecedent. As a basic rule, we also filter out noun phrases that don't match in number, person, or gender. On average, each anaphor has around 7 possible antecedent candidates to choose from.",
        "formal_text": "Convert casual text to formal text: So, to start with, we're using a basic system for figuring out which words refer to the same thing in a text. First, the text goes through some automatic processing steps,"
    },
    {
        "casual_text": "Civilization really took off because of the extra food we could grow through farming. It gave us the chance to do more than just survive, like building bigger societies and all that. Now, the focus is shifting to new kinds of gasoline that aren’t fully developed yet, but the oil industry is really pushing them as a way to clean up the air in cities, which is getting pretty bad from car pollution.",
        "formal_text": "Civilization really took off because of the extra food we could grow through farming. It gave us the chance to do more than just survive, like building bigger societies and all that. Now, the focus is shifting to new kinds of gasoline that are"
    },
    {
        "casual_text": "De Amorim and Zampieri (2013) grouped words in a dictionary using a method called \"anomalous pattern initialization\" and \"partition around medoids.\" In this method, the medoids (kind of like central words) represent the clusters, and the quality of a cluster is judged by how close a misspelled word is to the medoid. This setup helps cut down on the number of distance calculations needed. Once the right clusters are picked, all the words in those clusters are considered for further checks. On average, their method only needs to do 3,251.4 distance calculations for a dictionary with 57,046 words, which is a 0.057 reduction in the number of calculations. They also got an 88.42% success rate on a test dataset called the Birkbeck spelling error corpus. But here's the thing: they define \"success\" pretty loosely. A success is when one of the selected clusters either has the correct spelling or has a word that's closer to the misspelled word than the correct one. In Section 4, we'll define a stricter and more natural way to measure success for our own work. Because of this difference in how success is defined and the different approaches we're taking, it's not totally fair to compare their method directly with ours.",
        "formal_text": "Convert casual text to formal text: De Amorim and Zampieri (2013) grouped words in a dictionary using a method called \"anomalous pattern initialization\" and \"partition around medoids.\" In this"
    },
    {
        "casual_text": "For instance, if something bad happens in the community, people are probably going to feel really sad all at once, but they're not likely to suddenly feel super happy. If you check out Table 3, you'll see that our method works better than the four other ones we compared it to. The main reason for this is that our burst detection method looks at how emotions are spread out across the community over time. Treating community emotions as a distribution just works better than trying to deal with a bunch of different emotion types separately.",
        "formal_text": "Convert casual text to formal text: For instance, if something bad happens in the community, people are probably going to feel really sad all at once, but they're not likely to suddenly feel super happy. If you check out Table"
    },
    {
        "casual_text": "For languages that aren’t as high-priority or when the source language isn’t English, and especially for apps where the quality expectations are a bit lower, we’re focusing on more budget-friendly solutions. Like we mentioned, Adobe’s started looking into the Moses open-source tool, and we’ve also been talking to some vendors to address the issues with those one-size-fits-all, free online services.",
        "formal_text": "Convert casual text to formal text: For languages that aren’t as high-priority or when the source language isn’t English, and especially for apps where the quality expectations are a bit lower, we’re"
    },
    {
        "casual_text": "We can build the transition sequence that Theorem G.9 is asking for by using C LT L over and over on a starting configuration, let's call it c. Lemma G.15 says that when you use C LT L on a configuration, you either end up with a goal configuration or you add one more FINISH transition. Lemma G.14 also tells us there's a limit to how many times we can keep adding FINISH transitions in a valid sequence. Since C LT L only gives us valid sequences, we'll definitely hit a goal configuration after applying it a certain number of times.",
        "formal_text": "Convert casual text to formal text: We can build the transition sequence that Theorem G.9 is asking for by using C LT L over and over on a starting configuration, let's call it c. Le"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: L_pe = sum of [log of the probability of _b,r_i given B and X_v] + sum of [log of the probability of _v,r_i given V and X_b] (18) Here, _v,r_i and _b,r_i are just parts of the equation, and we're dealing with probabilities based on B, V, X_v, and X_b.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: L_pe = sum of [log of the probability of _b,r_i given B and X_"
    },
    {
        "casual_text": "Check out the sample output in Table 7—it’s pretty similar to the human timeline in Table 1. Also, using a dynamic  i seems to make sense. The burstiness thing is kind of a big deal and deserves more focus. On days when nothing new happens, fewer sentences are picked. Cool stuff, right? We also noticed that humans tend to create timelines with some biases. For one, they like focusing on local events, and two, they have different writing styles. For example, US news outlets often summarize what the US government is saying, while UK sites focus on British stuff. Some editors love stats, others go for storytelling. Some timelines are super detailed, while others are super short—like, just two sentences per entry. Our system-generated timelines vary a lot compared to the \"golden standards.\" Maybe we need a new way to measure how good human-made timelines are to account for these biases. Another interesting thing: different subjects have different patterns. Like, H1N1 started slow but then got really active. BP Oil had a sudden start but quickly faded. Obama’s timeline is different—it’s steady and spread out over time.",
        "formal_text": "Convert casual text to formal text: Check out the sample output in Table 7—it’s pretty similar to the human timeline in Table 1. Also, using a dynamic  i seems to make sense. The burstiness thing"
    },
    {
        "casual_text": "Most regular search engines usually give the same results for the same search, no matter who’s searching. But, turns out, most searches are pretty vague (Cronen-Townsend et al., 2002) and way too short (Silverstein et al., 1999) to really nail down what people are looking for. Different people can have totally different reasons for searching the same thing (Jansen et al., 2000). Like, if someone searches for \"Java,\" they could be after info on the programming language or maybe just looking to learn about coffee.",
        "formal_text": "Convert casual text to formal text: Most regular search engines usually give the same results for the same search, no matter who’s searching. But, turns out, most searches are pretty vague (Cronen-Townsend"
    },
    {
        "casual_text": "We rely on the ROUGE evaluation metrics, which have consistently matched up well with human-rated summarization scores (Lin, 2004). To be more precise, we use ROUGE-L, ROUGE-1, and ROUGE-2 to assess and compare how good the summaries our system generates are. ROUGE-N looks at n-gram overlaps, while ROUGE-L measures the quality of the summary by checking the longest common subsequence. ROUGE-N, where N is the n-gram order, is calculated like this:",
        "formal_text": "Convert casual text to formal text: We rely on the ROUGE evaluation metrics, which have consistently matched up well with human-rated summarization scores (Lin, 2004). To be more precise, we use ROUG"
    },
    {
        "casual_text": "The semantic guesses made by the SEMANT[CS module and sent through the Speech System's blackboard—which are basically predictions based on meaning—are compared with the chart. This comparison helps create new tasks, which might then lead to more word and phrase ideas. All this is done to expand and connect the current pieces of information.",
        "formal_text": "Convert casual text to formal text: The semantic guesses made by the SEMANT[CS module and sent through the Speech System's blackboard—which are basically predictions based on meaning—are compared with the chart."
    },
    {
        "casual_text": "Okay, so we're looking at how the source affects the target at each step, right? For every generation step, we check out how much the source is contributing overall. And here's a cool thing: this is basically the same as checking how much the prefix is contributing because, you know, R t (prefix) is just 1 minus R t (source). This is all explained in Section 2.3, by the way.",
        "formal_text": "Convert casual text to formal text: Okay, so we're looking at how the source affects the target at each step, right? For every generation step, we check out how much the source is contributing overall. And here's"
    },
    {
        "casual_text": "Sure! Here's a more casual version: For text classification, it assigns a label to each piece of text by looking at the embedding of the first token. This first token is usually some kind of special marker, like a symbol or something.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: For text classification, it assigns a label to each piece of text by looking at the embedding of the first token. This first token"
    },
    {
        "casual_text": "All parsing methods end up with the same number of nodes overall, but they just build those nodes at different times. Usually, left-branching ways of doing things get nodes done earlier compared to right-branching ones. But when it comes to right-adjunction, both left and right-branching methods hold off on making a lot of nodes until the right-adjunct is processed. That’s not true for revealing derivations, though—they’re made to handle right-adjuncts more flexibly and don’t delay like the others.",
        "formal_text": "Convert casual text to formal text: All parsing methods end up with the same number of nodes overall, but they just build those nodes at different times. Usually, left-branching ways of doing things get no"
    },
    {
        "casual_text": "So, back in the day, saying \"hopefully\" meant the train was expected to arrive on time and in good shape. But these days, \"hopefully\" is more about how the person talking feels, not the train itself.",
        "formal_text": "Convert casual text to formal text: So, back in the day, saying \"hopefully\" meant the train was expected to arrive on time and in good shape. But these days, \"hopefully\" is more about how the"
    },
    {
        "casual_text": "A cool way to figure out new relationships in a knowledge base is something called random walk inference, which was introduced by Lao and Cohen back in 2010. This method, known as the Path Ranking Algorithm (PRA), turns the knowledge base into a graph and uses random walks to find routes between the starting and ending points of a relationship. These routes are then used as features for a logistic regression model that predicts new instances of that relationship. Each route can be seen as a kind of logical rule using the knowledge base's relationships as conditions, so PRA is like a smart, trained way of doing logical reasoning.",
        "formal_text": "Convert casual text to formal text: A cool way to figure out new relationships in a knowledge base is something called random walk inference, which was introduced by Lao and Cohen back in 2010. This method, known as the Path Ranking Al"
    },
    {
        "casual_text": "Following what Fan et al. (2019) noticed, we found that token classification doesn’t do as well as sentence classification. Specifically, the F1 score for token classification is 29.86, while for sentence classification, it’s 42.16 (using RoBERTa on the Story split).",
        "formal_text": "Convert casual text to formal text: Following what Fan et al. (2019 noticed, we found that token classification doesn’t do as well as sentence classification. Specifically, the F1 score for token classification is 29.86"
    },
    {
        "casual_text": "Alright, let’s break down the different approximation methods we tested. We looked at four of them: standard LSI, ON-Eigen (Equation 1), Explicit LSI (Equation 2), and L-Solve (Equation 3). All these methods were tested on the same small dataset. To make things easier to understand, we plotted an approximation rate, which is either K or N1, depending on the method. At K = 500 and N1 = 500, the approximations become exact, and you can see this in Figure 1. We also noticed how the performance changes as we increase the computational effort. The orthonormal eigenvector method (Equation 1) and L-Solve (Equation 3) turned out to be pretty similar in terms of how well they approximate. On the other hand, both the explicit LSI method (Equation 2) and the standard LSI method didn’t perform as well for most approximation levels. Explicit LSI was especially worse because it first maps the documents into a K-dimensional LSI topic space and then maps them back into an N-dimensional explicit space. As you’d expect, this double mapping introduces more error, so it performs worse than standard LSI, except when K is really high. Interestingly, even the least approximated method (orthonormalization) improved upon the (CL-)ESA baseline, which was already low due to the small number of documents. For the rest of this section, we’ll focus on the L-Solve method because it performed really well and is less computationally expensive than ON-Eigen.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break down the different approximation methods we tested. We looked at four of them: standard LSI, ON-Eigen (Equation 1),"
    },
    {
        "casual_text": "The idea behind this approach is to concentrate on the tricky examples where the learner isn't doing so great, so the loss is higher. The concept of \"hardness\" isn't fixed; something that seems hard can become easier as you figure it out. Following Zhou and the team from 2020, we use a dynamic measure of hardness based on the average of how hard things seem right now, which we calculate using the difference in loss between two quick training rounds. Let’s say (x i, p i ) is the ith pair of image and program in our training set, with the correct answer a i. The \"instantaneous hardness\" r t (i) for (x i, p i ) at a specific time-step t is defined like this:",
        "formal_text": "Convert casual text to formal text: The idea behind this approach is to concentrate on the tricky examples where the learner isn't doing so great, so the loss is higher. The concept of \"hardness\" isn't"
    },
    {
        "casual_text": "Up until now, most discourse annotation has been done by experts who use detailed linguistic guidelines. Examples of this are the PDTB (Prasad et al., 2008; Webber et al., 2019) and RST (Mann and Thompson, 1988; Carlson et al., 2002) frameworks. But this kind of annotation is time-consuming and expensive. Using crowdsourcing to label discourse relations instead of relying on experts could be a great way to get more training data for discourse parsers on a larger scale.",
        "formal_text": "Convert casual text to formal text: Up until now, most discourse annotation has been done by experts who use detailed linguistic guidelines. Examples of this are the PDTB (Prasad et al., 2008"
    },
    {
        "casual_text": "The main idea here is to guess what a person will buy next based on what they just bought. We use a pre-trained BERT model and tweak it a bit to work for this specific task. We treat the item they just bought as \"sentence A\" and the one we're trying to predict as \"sentence B.\" Both item names are combined and cut short if they’re too long, keeping the total under 128 tokens, with special markers like [CLS] and [SEP] included. For the item they just bought, we find items they also bought in the same shopping session to use as positive examples. For negative examples, we just pick some random items. With these positive and negative sets, we calculate the cross-entropy loss to help the model learn to predict what comes next.",
        "formal_text": "Convert casual text to formal text: The main idea here is to guess what a person will buy next based on what they just bought. We use a pre-trained BERT model and tweak it a bit to work"
    },
    {
        "casual_text": "We use a margin threshold to figure out if x and y are mutual translations. Research shows it works better than the usual cosine similarity for identifying correct translation pairs (Artetxe and Schwenk, 2019a).",
        "formal_text": "Convert casual text to formal text: We use a margin threshold to figure out if x and y are mutual translations. Research shows it works better than the usual cosine similarity for identifying correct translation pairs ("
    },
    {
        "casual_text": "We're using dictionaries from MUSE, which is a well-known benchmark for bilingual lexicon induction (BLI) and was created by Conneau et al. in 2018. The setup is pretty standard: we train on 5,000 source word translations and test on 1,500 words for BLI. For each language, we train three different projection-based cross-lingual word embeddings (CLWE): one method is canonical correlation analysis (CCA), which was introduced by Faruqui and Dyer in 2014. In the results, we show accuracy for document classification on the left and the unlabeled attachment score (UAS) for dependency parsing on the right. Compared to the original embeddings (shown in gray), retrofitting with the training dictionary (in pink) boosts the average performance on downstream tasks, which means fully using the training dictionary really helps. Adding a synthetic dictionary (in orange) also improves test accuracy for some languages.",
        "formal_text": "Convert casual text to formal text: We're using dictionaries from MUSE, which is a well-known benchmark for bilingual lexicon induction (BLI) and was created by Conneau et al"
    },
    {
        "casual_text": "The second part is about showing the context around the word we found in a more general way. This is actually already done while we're working on the first part. The BiLSTM hidden states can be used as context vectors because, at every step, they keep adding up information from the words around them.",
        "formal_text": "Convert casual text to formal text: The second part is about showing the context around the word we found in a more general way. This is actually already done while we're working on the first part. The BiLSTM hidden states"
    },
    {
        "casual_text": "For our next steps, we're planning to explore some areas we had to skip over in this project. First, we want to look at more suffixoid candidates and also expand our work to include prefixoids. Second, we're interested in dealing with affixoids that show up in complex adjective forms. Lastly, once we have more annotated data on additional affixoid candidates, we'd like to see if we can figure out how to spot morphemes that act like affixoids based on things like their formation patterns and how often they appear. We also want to figure out how to tell them apart from morphemes that only work in regular compounds. Since affixoids have mostly been identified by people thinking about them, we don't really know how many there are in German or other languages.",
        "formal_text": "Convert casual text to formal text: For our next steps, we're planning to explore some areas we had to skip over in this project. First, we want to look at more suffixoid candidates and also expand our work"
    },
    {
        "casual_text": "The GMB's current version has all its documents in English, and they come from four main places: (i) Voice of America (VOA), an online newspaper run by the U.S. government; (ii) the Manually Annotated Sub-Corpus (MASC) from the Open American National Corpus (Ide et al., 2010); (iii) country descriptions from the CIA World Factbook (CIA) (Central Intelligence Agency, 2006), specifically the Background and Economy sections; and (iv) a bunch of Aesop's fables (AF). All these documents are in the public domain, so they can be shared freely, unlike the WSJ data used in the Penn Treebank (Miltsakaki et al., 2004), which isn't.",
        "formal_text": "Convert casual text to formal text: The GMB's current version has all its documents in English, and they come from four main places: (i) Voice of America (VOA), an online newspaper run by the U.S"
    },
    {
        "casual_text": "For example, to figure out what's dairy-free, we checked out this link: https://en.wikipedia.org/wiki/Dairy_product.",
        "formal_text": "Convert casual text to formal text: For example, to figure out what's dairy-free, we checked out this link: https://en.wikipedia.org/wiki/Dairy_product."
    },
    {
        "casual_text": "From earlier studies (like Das et al. in 2018 and Lin et al. in 2018), we know that even though Knowledge Graph Embedding (KGE) models aren’t super easy to interpret, they often perform better than multi-hop reasoning models when it comes to most Knowledge Graphs (KGs). This difference is even clearer on sparse KGs (check out Table 3 for the results). KGE models handle these better because they don’t depend on how connected the KG is. Taking this into account, we came up with a new idea called \"dynamic anticipation.\" The plan is to bring in the prediction info from embedding-based models and use it to guide multi-hop reasoning models during training. Here’s how it works: for a given triple query (e_s, r_q, ?), we use pretrained KGE models to figure out the probability of each entity being the tail entity. This gives us a probability vector, let’s call it p, which is a list of probabilities for all entities in the graph. The i-th value in this vector p represents the probability that entity e_i is the correct tail entity.",
        "formal_text": "Convert casual text to formal text: From earlier studies (like Das et al. in 2018 and Lin et al. in 2018), we know that even though Knowledge Graph Embedding (KGE) models are"
    },
    {
        "casual_text": "Alright, let's break this down in a more casual way. For the Paraphrase Identification task, we used the PAWS-X dataset, which was introduced by Yang et al. in 2019. This dataset has around 24,000 human-translated pairs for evaluation and about 296,000 machine-translated pairs for training, covering 7 languages: English, Spanish, French, German, Japanese, Chinese, and Korean. However, we didn't use the Korean part because, during our initial experiments, we couldn't get the same results as in the original paper by Yang et al. We think there might have been an issue with the encoding when using the BERT multilingual model for Korean. Moving on to Sequence Tagging, we ran some experiments on Named Entity Recognition (NER) using the CoNLL 2002 and CoNLL 2003 datasets, which were created by Tjong Kim Sang in 2002 and Tjong Kim Sang and De Meulder in 2003, respectively. Following the approach of Rahimi et al. in 2019, we combined these two datasets into one, covering 4 languages: English, Spanish, German, and Dutch. The combined dataset has 51,821 sentences for training, 11,344 for validation, and 13,556 for testing. Each sentence is labeled with entities like Person, Location, Organization, and Miscellaneous.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a more casual way. For the Paraphrase Identification task, we used the PAWS-X dataset, which was introduced by Yang et al"
    },
    {
        "casual_text": "Unlike earlier features, lexical vectors are calculated offline and don’t get updated during training. In our experience, it’s helpful to apply a MinMax scaler that scales the values to a range of [-1, +1] for each LS vector we create. So, something like [. . , 0.095, . . , 0.20, . . , 0.76, . . ] gets transformed into [. . , -1, . . , -0.67, . . , 1, . . ].",
        "formal_text": "Convert casual text to formal text: Unlike earlier features, lexical vectors are calculated offline and don’t get updated during training. In our experience, it’s helpful to apply a MinMax scaler that scales"
    },
    {
        "casual_text": "Here’s what we’ve done: (i) We came up with a general approach and a training method to make summarization models more accurate by using reinforcement learning (RL) to optimize multiple goals at once. (ii) We tested this method on radiology reports and saw that it actually improves how factually correct the summaries are. (iii) We had radiologists check our system, and they found that the summaries it generates are pretty close to what a human would write, clinically speaking. As far as we know, this is the first time someone’s tried directly optimizing a neural summarization system to focus on factual correctness using RL.",
        "formal_text": "Convert casual text to formal text: Here’s what we’ve done: (i) We came up with a general approach and a training method to make summarization models more accurate by using reinforcement learning (RL) to"
    },
    {
        "casual_text": "Here, we’ll start by explaining two tasks: sentiment classification and sentence segmentation. After that, we’ll give you a quick rundown of our joint segmentation and classification model (JSC) for sentiment analysis. In Section 4, we’ll talk about the segmentation candidate generation model and the segmentation ranking model. Then, in Section 5, we’ll dive into the specifics of the sentiment classification model.",
        "formal_text": "Convert casual text to formal text: Here, we’ll start by explaining two tasks: sentiment classification and sentence segmentation. After that, we’ll give you a quick rundown of our joint segmentation and classification model (J"
    },
    {
        "casual_text": "b. Not everyone slept.  Everyone was awake. c. Three boys got £10.  Three boys got £10 each. Looking at the examples in (5), we’re kind of back to (1), and it might seem like any unclear parts in the data are just seen as different possible meanings. But even if we ignore the issue with Reflexivity, this idea still can't be right because it messes up the obvious and logical conclusion in (6). (6) If the students get £10, they’ll buy books.",
        "formal_text": "Convert casual text to formal text: b. Not everyone slept.  Everyone was awake. c. Three boys got £10.  Three boys got £10 each. Looking at the examples in (5), we’re"
    },
    {
        "casual_text": "In simpler terms, a sentence like (la) is written as (lb), and (2a) turns into (2b) if the negation is understood with a narrow scope. But if the negation has a wide scope, (2a) is shown as (2c).",
        "formal_text": "Convert casual text to formal text: In simpler terms, a sentence like (la) is written as (lb), and (2a) turns into (2b) if the negation is understood with a narrow scope. But"
    },
    {
        "casual_text": "We're using the micro-averaged F1 score to measure how well we're doing. Along with that, we're also showing precision and recall. The numbers you see are the average results from five separate runs.",
        "formal_text": "Convert casual text to formal text: We're using the micro-averaged F1 score to measure how well we're doing. Along with that, we're also showing precision and recall. The numbers you see are the average"
    },
    {
        "casual_text": "When working on rumor detection, the final state of the top layer in an event-level BiLSTM can be seen as a summary of all the information gathered from the posts.",
        "formal_text": "Convert casual text to formal text: When working on rumor detection, the final state of the top layer in an event-level BiLSTM can be seen as a summary of all information gathered from the posts. Convert"
    },
    {
        "casual_text": "In this project, we're diving into differentiable natural logic models that blend natural logic with neural networks. The goal is to stick to the core of inference based on natural logic but also bring in subsymbolic vector representations and some neural network elements. Mixing the strengths of neural networks with natural logic comes with a few challenges. There are two main issues we need to tackle: 1) Figuring out how (and where) to use the power of neural networks within the natural logic framework, and; 2) Dealing with the lack of intermediate supervision when training smaller parts, which can cause problems like the spurious issue in end-to-end training (as mentioned by Guu et al., 2017, and Min et al., 2019).",
        "formal_text": "Convert casual text to formal text: In this project, we're diving into differentiable natural logic models that blend natural logic with neural networks. The goal is to stick to the core of inference based on natural logic but also"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: - If you add **x1** to **C(s, t)**, you get **C(s)**. - If you add **x2** to **C(s, t)**, you get **C(t)**. - If you add **x4** to **C(s)**, you get **N**. - If you add **x5** to **C(t)**, you also get **N**. - And if you add **x2** and **x3** together, you get **x4**. So, in short: - **C(s, t) + x1 = C(s)** - **C(s, t) + x2 = C(t)** - **C(s) + x4 = N** - **C(t) + x5 = N** - **x2 + x3 = x4**",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: - If you add **x1** to **C(s, t)**, you get **C(s)"
    },
    {
        "casual_text": "Alright, so  stands for the sigmoid function applied to each component, and it's also used for element-wise multiplication.",
        "formal_text": "Convert casual text to formal text: Alright, so  stands for the sigmoid function applied to each component, and also used for element-wise multiplication. Alright, so  stands for the sigmoi"
    },
    {
        "casual_text": "The sentence generator gets its input from the internal representation and creates a series of caserole representations for the act fragments as its output. These are then sent one by one to the act generator, which turns each act fragment into a sequence of words. When everything is running, the four networks are linked together in a chain, with the output from one network becoming the input for the next one (check out figure 1). During training, each network is trained on its own using matching input and output data (see figure 6).",
        "formal_text": "Convert casual text to formal text: The sentence generator gets its input from the internal representation and creates a series of caserole representations for the act fragments as its output. These are then sent one by one to the act"
    },
    {
        "casual_text": "We start with  set to 0.5 and notice that changing this starting value doesn't really impact the final results much.",
        "formal_text": "Convert casual text to formal text: We start with  set to 0.5 and notice that changing this starting value really impact the final results much."
    },
    {
        "casual_text": "A downside of this study is that the people labeling the data shouldn’t know we’re using the random reason choices as a sign of unreliability. So, we made sure not to tell them about it and didn’t kick anyone out for picking those random reasons. For now, we’re leaving it up to future work to figure out a way to handle this situation, maybe by having the random reasons switch up automatically between tasks, so it still works even if people catch on to the trick.",
        "formal_text": "Convert casual text to formal text: A downside of this study is that the people labeling the data shouldn’t know we’re using the random reason choices as a sign of unreliability. So, we made sure not to"
    },
    {
        "casual_text": "Contexts can come in all sorts of shapes and sizes. They might be just a single passage, like in MCTest (Richardson et al., 2013), or a bunch of passages, like in HotpotQA. Sometimes, they're longer documents, as seen in CBT (Hill et al., 2016), or even cover the whole open domain, like in the work by Chen et al. (2017). Some datasets mix things up by including non-text stuff, like images, which you can find in RecipeQA (Yagcioglu et al., 2018).",
        "formal_text": "Convert casual text to formal text: Contexts can come in all sorts of shapes and sizes. They might be just a single passage, like in MCTest (Richardson et al., 2013), or"
    },
    {
        "casual_text": "This paper is about showing how a specific type of feature system, which is similar to some recent ideas in phonological features, can be represented in connectionist networks. But, since some parts, like seriality and synchronization, aren’t really covered by current connectionist networks, we’re going to use a different model that acts like a serial connectionist network. The system we describe can combine feature structures and run them as programs on the machine.",
        "formal_text": "Convert casual text to formal text: This paper is about showing how a specific type of feature system, which is similar to some recent ideas in phonological features, can be represented in connectionist networks. But, since some parts,"
    },
    {
        "casual_text": "DATR was first introduced by Evans and Gazdar (1989a; 1989b) as a straightforward, nonmonotonic language for handling lexical inheritance hierarchies. Basically, a DATR hierarchy is set up using path-value pairs. This inheritance of values helps capture useful generalizations and cuts down on repeating the same information over and over. Plus, it has a simple default system that keeps descriptions neat and tidy, but still lets you note any exceptions to the inherited info in a pretty natural way.",
        "formal_text": "Convert casual text to formal text: DATR was first introduced by Evans and Gazdar (1989a; 1989b) as a straightforward, nonmonotonic language for handling lexical inheritance hierarchies. Basically"
    },
    {
        "casual_text": "\"The Extra Girl\" was made by Mack Sennett. b. It came after some earlier movies that were about the movie business. c. Plus, it kind of set the stage for later films that talked about Hollywood.",
        "formal_text": "Convert casual text to formal text: \"The Extra Girl\" was made by Mack Sennett. b. It came after some earlier movies that were about the movie business. c. Plus, it kind of set the stage for later"
    },
    {
        "casual_text": "Our approach for decoding using parsers as language models is pretty similar to the one for decoding with ngram language models, which was explained by Wu in 1997. The idea is to create a dynamic programming chart by working from the bottom up. Each entry in the chart looks like [X, s, t, e l, e r ], where X represents a BTG nonterminal covering the span (s, t) of the input. Meanwhile, e l and e r are boundary words from the target language vocabulary (E *), which help us multiply in the ngram language model probability when we combine these entries.",
        "formal_text": "Convert casual text to formal text: Our approach for decoding using parsers as language models is pretty similar to the one for decoding with ngram language models, which was explained by Wu in 1997. The idea is to create"
    },
    {
        "casual_text": "The word \"assimilation\" shows up in 2 documents that are written at a 12th-grade level. Since these documents are pretty tough to read, it means the estimated age when people learn this word is older, and there aren’t many examples of it being used, so the guess about when people learn it is kind of shaky—you can see this in how spread out the data is. On the other hand, the word \"thought\" pops up a bunch of times across different grades. It’s first spotted in 1st grade and then again in 4th grade, so the estimated age when people learn it is somewhere in between those two grades. But because \"thought\" is used more often, the guess about when people learn it is more certain—the data is less spread out. Now, \"multitude\" is used in grades 6, 8, and 9. Compared to \"thought\" and \"assimilation,\" \"multitude\" is used less often than \"thought\" but more often than \"assimilation.\" This means the guess about when people learn \"multitude\" is less certain than for \"thought\" but more certain than for \"assimilation\"—so the data is spread out more than for \"thought\" but less than for \"assimilation.\"",
        "formal_text": "Convert casual text to formal text: The word \"assimilation\" shows up in 2 documents that are written at a 12th-grade level. Since these documents are pretty tough to read, it means the estimated age when people"
    },
    {
        "casual_text": "Sure, let’s not worry about the connection between the two words (you can pick P, V, or I). This pair could actually come in handy for putting together a general glossary of terms. Just so you know, a word pair can work in both directions.",
        "formal_text": "Convert casual text to formal text: Sure, let’s not worry about the connection between the two words (you can pick P, V, or I). This pair could actually come handy for putting together a general glossary of"
    },
    {
        "casual_text": "Here, N stands for the number of training examples, L_i is the length of the feedback, and y_m_il is 1 if the l-th word is part of modality m, and 0 if it's not. s_m_il shows how likely the modality gate thinks the word belongs to modality m. This whole thing will be trained together with the loss function from Equation 2, using a multi-task learning approach (Caruana, 1993).",
        "formal_text": "Convert casual text to formal text: Here, N stands for the number of training examples, L_i is the length of the feedback, and y_m_il is 1 if the l-th word is part of"
    },
    {
        "casual_text": "First off, some research, like Zens and Ney's from 2003, suggests that to create multiword translation units in these synchronous grammar setups, it's helpful to be able to throw in multiple terminals at once. The second point is about handling context-sensitivity. Basically, (2, 2)-BRCGs can handle translations like  a n b m c n d m, a n b m d m c n | m, n  0, which means they can translate cross-serial dependencies into nested ones. But it also means (2, 2)-BRCGs can create a wider range of alignment structures. In fact, the set of alignment structures they can produce is closed under union, meaning they can generate any alignment structure. Lastly, this is pretty practical. As you'll see, things like inside-out alignments and CDTUs, which ITGs can't handle but (2, 2)-BRCGs can, show up a lot in manually aligned parallel corpora.",
        "formal_text": "Convert casual text to formal text: First off, some research, like Zens and Ney's from 2003, suggests that to create multiword translation units in these synchronous grammar setups, it's helpful to be able"
    },
    {
        "casual_text": "In 2003, some researchers showed that both how many sounds a word has nearby (phonological neighborhood) and how many similar-looking words it has (orthographic neighborhood) can affect SWR. They found two different things happening: one is a downer (the inhibitory phonological effect), and the other is a boost (the facilitatory orthographic effect). Basically, if a word has a lot of similar sounds or similar-looking words around it, SWR either gets slowed down or sped up.",
        "formal_text": "Convert casual text to formal text: In 2003, some researchers showed that both how many sounds a word has nearby (phonological neighborhood) and how many similar-looking words it has (orthographic neighborhood) can affect SWR."
    },
    {
        "casual_text": "The correct classification is represented by y_i. The generator's loss function is the same as the one in the original paper, as shown in equation 2.",
        "formal_text": "Convert casual text to formal text: The correct classification is represented by y_i. The generator's loss function is the same one in the original paper, as shown in equation 2. The generator's loss function is the same one"
    },
    {
        "casual_text": "In the world of combinatorial optimization, problem (4), which takes input B, is called the Linear Ordering Problem. This problem pops up in all sorts of real-world situations, like in economics, sociology, graph theory, drawing graphs, archaeology, and even when scheduling tasks (Grötschel et al., 1984). Researchers often test it out on real data using \"input-output\" matrices that show how resources move between different sectors of the economy (Schiavinotto and Stützle, 2004).",
        "formal_text": "Convert casual text to formal text: In the world of combinatorial optimization, problem (4), which takes input B, is called the Linear Ordering Problem. This problem pops up in all sorts of real-world situations,"
    },
    {
        "casual_text": "Automating the creation of graph queries comes with the issue of figuring out which ones are any good and getting rid of the bad ones. Our approach deals with this by mixing structured data from a knowledge base with statistical data from the web. First, we spot parts of a graph query that are unnecessary and come up with ways to get rid of them. Then, using how often entities, classes, and relationships show up on the web, we measure how common a graph query is and toss out the really rare ones.",
        "formal_text": "Convert casual text to formal text: Automating the creation of graph queries comes with the issue of figuring out which ones are any good and getting rid of the bad ones. Our approach deals with this by mixing structured data from a knowledge"
    },
    {
        "casual_text": "Sure, this gives us a simple way to do data augmentation on any Masked Language Model setup. We’ll look into how it works beyond just lexical normalization in future research.",
        "formal_text": "Convert casual text to formal text: Sure, this gives us a simple way to do data augmentation on any Masked Language Model setup. We’ll look into how it works beyond just lexical normalization in future research."
    },
    {
        "casual_text": "We want to give a big shoutout to Steven R. Wilson for hooking us up with the Urban Dictionary data and Walter Rader for sharing a curated list of slang words from the Online Slang Dictionary. For the Twitter data, we’re super grateful to have been able to use Twitter’s Academic Research Track. Lastly, we really appreciate the feedback and helpful comments from Mario Giulianelli, Yifan Hou, Bernhard Schölkopf, and the three anonymous reviewers.",
        "formal_text": "Convert casual text to formal text: We want to give a big shoutout to Steven R. Wilson for hooking us up with the Urban Dictionary data and Walter Rader for sharing a curated list of slang words from"
    },
    {
        "casual_text": "We used rule-based metrics, which you can check out in Table 1. All the current metrics were run using the MS-COCO evaluation tool. To see how well these metrics worked, we looked at Kendall's tau () for the scoring-based datasets (like Composite and PublicSys) and checked the accuracy of the pairwise comparisons for the Pascal-50S dataset.",
        "formal_text": "Convert casual text to formal text: We used rule-based metrics, which you can check out in Table 1. All the current metrics were run using the MS-COCO evaluation tool. To see how well these metrics worked, we looked at"
    },
    {
        "casual_text": "For multi-view training, Johansson (2013) combined shared features with unique features for each treebank. Qiu et al. (2013) came up with a multi-task learning model that predicts two sets of labels for a given sentence. Their model just uses the basic features for each labelset and doesn’t add anything extra to capture how the two labelsets interact. Li et al. (2015) took this a step further by really connecting the two labelsets, treating the combination of base labels as one big labelset, and using features from both labelsets together. While this helps with capturing how labels interact, it slows things down because the search space gets way bigger. In contrast, our neural approach shares parameters in the hidden layers, which lets us model label interaction without having to directly merge the two output labelsets. This results in a simpler model that’s almost as fast as a single-label baseline.",
        "formal_text": "Convert casual text to formal text: For multi-view training, Johansson (2013) combined shared features with unique features for each treebank. Qiu et al. (2013) came up with a multi-task learning model"
    },
    {
        "casual_text": "I looked at English user reviews, which are kind of in the middle—not super formal, but not super casual either. Then, I translated them into Croatian and Serbian, which are examples of European languages with lots of grammar rules but not as widely used or studied as some others.",
        "formal_text": "Convert casual text to formal text: I looked at English user reviews, which are kind of in the middle—not super formal, but not super casual either. Then, I translated them into Croatian and Serbian, which are examples"
    },
    {
        "casual_text": "We dug into the backgrounds of all 145 key employees by doing web searches and checking out public records or articles about them. To make sure we were looking at the right person, we added the word 'Enron' to our search terms. In the public records we found for each employee, we paid attention to any clues about their gender, like pronouns (he/him/his or she/her) or titles (Mr., Mrs., Ms., Miss). Since these folks were big shots at Enron during its bankruptcy, it wasn’t too hard to find info about them. For instance, the page we found on Kay Mann clearly shows she’s a woman. Using this method, we managed to figure out the gender of all 145 employees. One cool thing about doing it this way is that it gives us 100% confident gender assignments for everyone in our data set.",
        "formal_text": "Convert casual text to formal text: We dug into the backgrounds of all 145 key employees by doing web searches and checking out public records or articles about them. To make sure we were looking at the right person, we added the word"
    },
    {
        "casual_text": "Over those twelve years, more than 1,100 journalists and media workers were killed while doing their jobs. Some were targeted because someone didn’t like what they wrote or said, while others were in the wrong place at the wrong time. Table 5 shows an example sentence in Arabic Broadcast News, with the original text in Iraqi Colloquial Arabic (ICA) and Modern Standard Arabic (MSA). It also includes translations done using rule-based MT (LFG), statistical MT (SMT), and hybrid MT (HMT).",
        "formal_text": "Convert casual text to formal text: Over those twelve years, more than 1,100 journalists and media workers were killed while doing their jobs. Some were targeted because someone didn’t like what they wrote or said, while others were in the wrong"
    },
    {
        "casual_text": "For the heuristic settings, we picked pruning methods across all tasks to get results that are close to FIXED, using the development set as a guide. In machine translation, we used newstest2017, and for semantic and syntactic parsing, we used the ATIS and Penn Treebank development sets, respectively. Like always, when choosing heuristics, there's a balance between how fast it runs and how well it performs, and we looked into that specifically for machine translation.",
        "formal_text": "Convert casual text to formal text: For the heuristic settings, we picked pruning methods across all tasks to get results that are close to FIXED, using the development set as a guide. In machine translation, we"
    },
    {
        "casual_text": "2) The basic constraint agent is used to weed out invalid antecedent candidates by checking them against general rules, like making sure words agree in form and that their meanings make sense together.",
        "formal_text": "Convert casual text to formal text: 2) The basic constraint agent is used to weed out invalid antecedent candidates by checking them against general rules, like making sure words agree in form and that their meanings make sense together."
    },
    {
        "casual_text": "Okay, so the function splitprj (P, , rx, ry, rz) takes a bunch of projects P and divides them into three groups: Px, Py, and Pz.",
        "formal_text": "Convert casual text to formal text: Okay, so the function splitprj (P, , rx, ry, rz) takes a bunch of projects P and divides them into three groups:"
    },
    {
        "casual_text": "A lot of past research has been about making FrameNet-style annotations for languages that aren't English. One popular method is using parallel corpora and copying annotations from English sentences to their translations (Padó and Lapata, 2006; Johansson and Nugues, 2006). Other studies try to improve the English FrameNet on its own by either expanding its range or generating more training data.",
        "formal_text": "Convert casual text to formal text: A lot of past research has been about making FrameNet-style annotations for languages that aren't English. One popular method is using parallel corpora and copying annotations from English sentences"
    },
    {
        "casual_text": "Basically, when we added the EG (coreference graph) to the BERT model, it boosted the F1 score by 2.6 points compared to the regular BERT baseline. And it also gave us a 0.1-point improvement over the RoBERTa baseline. So, we figured out that using both the EG and EQ as extra parts for the BERT model works really well when dealing with coreference resolution in anaphora. In simple terms, our fancy additions made the model better at understanding pronouns and stuff.",
        "formal_text": "Convert casual text to formal text: Basically, when we added the EG (coreference graph) to the BERT model, it boosted the F1 score by 2.6 points compared to the regular BERT baseline. And"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. The model uses WordNet or Wikipedia to find meanings for certain words by checking how similar they are based on BERT embeddings. It then combines this with explicit knowledge graphs (called +KG explicit) to make better sense of sentences, especially when looking at tables. For encoding, it uses RoBERTa-large, which is a fancy tool for understanding text. Most of the setup is the same as before, but we made two small changes: (1) we used a smaller batch size of 4, and (2) we only used one random seed instead of averaging three. In Table 4, you can see how well the old model (+KG) and our new system (Ours) did on different problem sets. Our system got 31% of the problems right, while the old one only got 3%. For problems where the answer is \"neutral,\" our system can guess correctly without needing to understand numbers exactly. But when it comes to \"entailment\" or \"contradiction\" problems, the old model (+KG) couldn't handle them at all. Our system, though, got 27% right, showing it's better at dealing with numerical comparisons. Here's a quick comparison: - **Less than k**: Old model: 10%, Our system: 36% - **No more than k**: Old model: 10%, Our system: 35% - **Exactly k**: Old model: 19%, Our system: 32% - **k**: Old model: 24%, Our system: 33% - **At least k**: Old model: 8%, Our system: 32% - **No less than k**: Old model: 19%, Our system: 33% - **More than k**: Old model: 17%, Our system: 35% So, our system is better at predicting \"entailment\" and \"contradiction\" labels compared to the old one.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. The model uses WordNet or Wikipedia to find meanings for certain words by checking how similar they are based on BERT embed"
    },
    {
        "casual_text": "The other stuff—like adverbs, where they go, tenses, helping verbs, active or passive voice, and negatives—gets stored in something called a \"history.\" This history has a section for each sentence that matches the T-expression we're looking at. When we look up the T-expression in our knowledge base, we link up its three parts and add the history H to it. So, you can think of the final entry in the knowledge base as a kind of \"condensed version\" of the sentence's grammar in English.",
        "formal_text": "Convert casual text to formal text: The other stuff—like adverbs, where they go, tenses, helping verbs, active or passive voice, and negatives—gets stored in something called a \""
    },
    {
        "casual_text": "Warrens (2010c) talks about another popular measure called the Odds Ratio, which is written as ad/bc. This measure is often used in Epidemiology, not in fields like Computer Science or Computational Linguistics. Another closely related concept is the Determinant of the Contingency Matrix, which is dtp = ad-bc = etp-etn. This is based on independent marginal probabilities, as used in Chi-Sqr, Cohen, and Powers methods. Both the Odds Ratio and the Determinant help figure out if the odds of positive cases are higher for the first rater (the real one) compared to the second rater (the predicted one). For the Odds Ratio, this means the value should be greater than one, and for the Determinant, it should be greater than zero. It's worth noting that if you take the logarithm of all the coefficients, the relationship stays the same. The difference of these logarithms is equal to the logarithm of the ratio, which ties it all to the information domain.",
        "formal_text": "Convert casual text to formal text: Warrens (2010c) talks about another popular measure called the Odds Ratio, which is written as ad/bc. This measure is often used in Epidemiology"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: Basically, we're looking at this equation: (x, y,   ) =  T t=1 log p(y t |x, y [t1];   ) (1) And then there's another part: L(C,   ) = (x, y)C (x, y,   ) So, it's like we're taking some values (x, y, and   ) and plugging them into these equations to get some results. The first equation is about calculating something using logs, and the second one is about summing up those results for a set of (x, y) pairs in C.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: Basically, we're looking at this equation: (x, y,   ) ="
    },
    {
        "casual_text": "Alright, let's break down how we check the quality of paper clusters. For each cluster, we look at the chosen Title and Authors fields and compare them to the title and authors listed in the selected PDF. For the Title, we say it's correct if it matches the PDF title exactly, but we give some leeway for things like different capitalizations or minor differences in how special characters are shown (like \"\" vs. \"gamma\"). We also ignore extra spaces. For the Authors, we mark it as correct if all the authors from the PDF are listed in the right order. We allow for some differences in how their names are written, like using just initials instead of full first names, or leaving out middle names or titles like \"Dr.\" or \"PhD.\" This way, we don't penalize the metadata too much for small variations.",
        "formal_text": "Convert casual text to formal text: Alright, let's break down how we check the quality of paper clusters. For each cluster, we look at the chosen Title and Authors fields and compare them to the title and authors listed"
    },
    {
        "casual_text": "Looking at how users behave, we can see they’re pretty consistent in their decisions, whether we’re comparing different people’s opinions or the same person’s views over time. One post-editor accidentally thought the recommended machine translation (MT) results were actually translation memory (TM) outputs. This shows why combining TM and MT is both useful and important.",
        "formal_text": "Convert casual text to formal text: Looking at how users behave, we can see they’re pretty consistent in their decisions, whether we’re comparing different people’s opinions or the same person’s views over time. One post"
    },
    {
        "casual_text": "When working on sarcasm detection using both text and images, researchers have tried a few different approaches. Some combine the text and image features to find sarcastic stuff (Schifanella et al., 2016). Others use something called an attention mechanism to mix the features from different sources based on extra knowledge (Cai et al., 2019; Xu et al., 2020; Pan et al., 2020). There are also those who create interactive graphs to understand how different types of data relate to each other (Liang et al., 2021a). Even though these methods have shown some success, they still have a few issues: 1) Just looking at the whole image doesn't work well because images can be really complex. Plus, only certain parts of the image might actually connect to the text. For example, in Figure 1, you can get the right answer just by focusing on the parts of the image inside the boxes. So, it's better to pick out the important visual details and ignore the rest to get a clearer picture. 2) Important visual details that match up with the sarcasm in the text might be spread all over the image (check out Figure 1 (b)). This means it's important to really dig into how the text and image emotions connect, so you can use the conflicting feelings between them to figure out the sarcasm.",
        "formal_text": "Convert casual text to formal text: When working on sarcasm detection using both text and images, researchers have tried a few different approaches. Some combine the text and image features to find sarcastic stuff (Sch"
    },
    {
        "casual_text": "In the ACE Program 2, event extraction is broken down into two main parts: (1) figuring out and categorizing event triggers, and (2) finding the pieces of information related to those triggers and labeling what role each one plays. In this project, we’re working with the Commodity News dataset, which was introduced by Lee and the team in 2021. Check out Figure 1 for an example sentence from this dataset.",
        "formal_text": "Convert casual text to formal text: In the ACE Program 2, event extraction is broken down into two main parts: (1) figuring out and categorizing event triggers, and (2) finding the pieces of information related to those triggers and"
    },
    {
        "casual_text": "We ran the model 300 times and averaged the results to get the timing. Before we started measuring, we let the GPU (a Tesla V100-SXM2 with 16GB of memory) run for 10 warm-up rounds. To make sure the timing was accurate, we used PyTorch's built-in method to sync the timing (which happens on the CPU) with whatever was going on with the GPU during training or inference. For the CPU tests, we used an Intel Xeon CPU E5-2650 v2 running at 2.60GHz, with 4 cores and 16GB of memory set aside. All the timings were done with a batch size of 16.",
        "formal_text": "Convert casual text to formal text: We ran the model 300 times and averaged the results to get the timing. Before we started measuring, we let the GPU (a Tesla V100-SXM2 with 16GB of memory)"
    },
    {
        "casual_text": "2. Repeatability, or measurement repeatability, is basically how precise a measurement is when you do it under the same conditions over and over again.",
        "formal_text": "Repeatability, or measurement repeatability, is basically how precise a measurement is when you it under the same conditions over and again. It is basically how precise a measurement is when you it under the same conditions over and again. Repeatability"
    },
    {
        "casual_text": "For our experiments, we used two special datasets that other researchers have used before to find matching terms in two languages for technical stuff.",
        "formal_text": "Convert casual text to formal text: For our experiments, we used two special datasets that other researchers before to find matching terms in two languages for technical stuff. For our experiments, we used two special datasets that other researchers before to find"
    },
    {
        "casual_text": "To use a bunch of different strategies, like the ones in this paper, you gotta know the difference between constraints (which are like rules you can't break) and preferences (which help you pick between options that meet all the rules). Preferences can either be ranked in some order (like the goal trees mentioned in [4]) or you can use a voting system where stronger preferences get more votes. If two conflicting preferences have the same voting power, it just shows that there's no clear winner, kind of like a tie.",
        "formal_text": "Convert casual text to formal text: To use a bunch of different strategies, like the ones in this paper, you gotta know the difference between constraints (which are like rules you can't break) and preferences (which help you"
    },
    {
        "casual_text": "Most of the time, all the possible options for a sentence (called beam candidates) aren’t perfect. It might help to use the fact that some of these options are better than others. We think it’s a good idea to rank them based on how close they are to the real sentence (called the ground-truth). For speech recognition (ASR), we use a measure called WER, and for machine translation (MT), we use BLEU score. We’ll call this ranking x i, 0.",
        "formal_text": "Convert casual text to formal text: Most of the time, all the possible options for a sentence (called beam candidates) aren’t perfect. It might help to use the fact that some of these options are better than others."
    },
    {
        "casual_text": "IBM constraints, which are basically an expanded version of local constraints (as mentioned by Dreyer et al., 2007), create different ways to arrange phrases that don’t follow the usual, predictable order, like r K 1: r k = k. Basically, any spot in the phrase can be picked from the first m uncovered phrases (check out Eq. (3) for more details). A common value for m is 4, as suggested by Zens and Ney in 2003. We usually refer to IBM constraints with m set to 4 as IBM(4).",
        "formal_text": "Convert casual text to formal text: IBM constraints, which are basically an expanded version of local constraints (as mentioned by Dreyer et al., 2007), create different ways to arrange phrases that don’t follow the usual"
    },
    {
        "casual_text": "The error D(f) is no more than (|S|, F, , ), which is basically 2 times |S| multiplied by the stuff inside the brackets. That stuff is the logarithm (base 2) of N(G, 2|S|, 2) plus the logarithm (base 2) of 2.",
        "formal_text": "Convert casual text to formal text: The error D(f) is no more than (|S|, F, , ), which is basically 2 times |S| multiplied by the stuff inside the"
    },
    {
        "casual_text": "GuessWhat?! (de Vries et al., 2017) is a fun cooperative game where two players chat to figure out an object in a picture. One player, the Questioner, has to guess the object by asking yes or no questions. The other player, the Oracle, knows what the object is and answers the questions. The GuessWhat?! dataset has games of all kinds of difficulty, from simple images with just the target object and one other thing, to more complex ones with up to 19 other objects. This dataset has over 150,000 human-to-human conversations with an average of 5.3 questions each, all in natural language. These were created by people playing the game using images from MS COCO (Lin et al., 2014).",
        "formal_text": "Convert casual text to formal text: GuessWhat?! (de Vries et al., 2017) is a fun cooperative game where two players chat to figure out an object in a picture. One player, the Questioner"
    },
    {
        "casual_text": "Table 5 shows that S2ORC-SCIBERT does better than SCIBERT on a bunch of tasks, even though it has a lot of non-biomedical and non-computer science stuff mixed in. Since SCIBERT's pretraining data isn't out there for everyone to use, S2ORC can be a big dataset for testing and comparing different pretraining methods on academic text. We're also putting S2ORC-SCIBERT out there as a starting point for research.",
        "formal_text": "Convert casual text to formal text: Table 5 shows that S2ORC-SCIBERT does better than SCIBERT on a bunch of tasks, even though it has a lot of non-biomedical and"
    },
    {
        "casual_text": "We're using D(iscourse) R(epresentation) T(heory) as our semantic representation, which was developed by Hans Kamp [4]. Basically, we're not going with the semantic theory for L(exical) F(unctional) C(grammar) suggested by Per-Kristian Halverson [2]. Halverson converts the functional structures of LFG into what he calls semantic structures, which are basically scyclic graphs. These semantic structures come from a translation process that connects formulas of intensional logic to the semantic forms found in the functional structure. We’re not taking this approach for a reason, which we’ll explain by laying out some requirements that a semantic representation needs to meet to handle text processing effectively. Then, we’ll prove why these requirements are super important by looking at some example sentences and discourses.",
        "formal_text": "Convert casual text to formal text: We're using D(iscourse) R(epresentation) T(heory) as our semantic representation, which was developed by Hans Kamp [4]. Basically, we'"
    },
    {
        "casual_text": "We introduced a new NLU task where the goal is to pick the best counter speech that can oppose a given input speech from a bunch of options.",
        "formal_text": "Convert casual text to formal text: We introduced a new NLU task where the goal is to pick the best counter speech that can oppose a given input speech from a bunch of options. Convert casual text to formal text"
    },
    {
        "casual_text": "Next up, we’ll give a quick rundown of how our generation system is set up and the parts that deal with building coordination. Section 3 will compare this to other work on text generation. In Section 4, we’ll talk about the semantic representation we use for coordination. Section 5 will walk you through an algorithm for segregatory coordination, complete with an example. Finally, in Section 6, we’ll look at some examples from linguistics and explain how our current algorithm handles them.",
        "formal_text": "Convert casual text to formal text: Next up, we’ll give a quick rundown of how our generation system is set up and the parts that deal with building coordination. Section 3 will compare this to other work on text generation."
    },
    {
        "casual_text": "In this paper, we're looking at a real-time setup where we handle a bunch of tasks like spotting disfluencies, breaking down speech into chunks, predicting what comes next, and tagging parts of speech. In the next section, we'll outline these tasks and then introduce a straightforward deep learning system that can do all of this at once—identify disfluencies, tag parts of speech, figure out where one speech segment ends and another begins, and guess the next words based on what it’s hearing so far.",
        "formal_text": "Convert casual text to formal text: In this paper, we're looking at a real-time setup where we handle a bunch of tasks like spotting disfluencies, breaking down speech into chunks, predicting what"
    },
    {
        "casual_text": "We did some experiments that suggested large language models (PLMs) have metaphorical knowledge, but we weren't sure if this knowledge could work outside the data they were trained on. So, to test this, we did crosslingual and cross-dataset experiments. The results were pretty good for transferring knowledge across four languages with LCC annotations. But when the definitions and annotations were all over the place in different datasets, the results weren't great.",
        "formal_text": "Convert casual text to formal text: We did some experiments that suggested large language models (PLMs) have metaphorical knowledge, but we weren't sure if this knowledge could work outside the data they were trained on. So,"
    },
    {
        "casual_text": "The LCI says that when there's an extraposed element, it needs to be attached to the first maximal projection that includes its antecedent.",
        "formal_text": "Convert casual text to formal text: The LCI says that when there's an extraposed element it needs to be attached to the first maximal projection that includes its antecedent."
    },
    {
        "casual_text": "We tested PAM on three baseline models and put the results in Table 6. Before adding PAM, \"BiLSTM + BiLSTM\" had the best F1 score compared to the other two models. This might be because the Self-Attention module works better with bigger datasets, and just combining the models doesn't give us good semantic embedding vectors. After adding PAM, the F1 scores for all three baseline models went up by an average of 3.47%. The results with p-values show that models with PAM clearly perform better than those without it.",
        "formal_text": "Convert casual text to formal text: We tested PAM on three baseline models and put the results in Table 6. Before adding PAM, \"BiLSTM + BiLSTM\" had the best F1 score compared to the other"
    },
    {
        "casual_text": "Sure! Here's the informal version: We used publicly available 300-dimensional fastText word embeddings for all the isomorphism measures (SVG, COND-HM, ECOND-HM, GH, and IS) across the languages we looked at. These embeddings were pre-trained on Wikipedia with the exact same default settings as in Bojanowski et al. (2017). We also normalized them by length and cut them down to the 200,000 most frequent words.",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: We used publicly available 300-dimensional fastText word embeddings for all the isomorphism measures (SVG, COND-HM"
    },
    {
        "casual_text": "Sure! So, pronominal anaphora (like \"he\" or \"she\") and named entities (like names of people or places) can be handled pretty well by systems, but when it comes to definite noun phrases (like \"the cat\" or \"the book\"), things get trickier. Usually, these systems can only handle cases where both mentions have the same noun, like \"the cat\" and \"the cat.\" This is what Vieira and Poesio (2000) call \"direct coreference.\" But when we're dealing with what they call \"coreferent bridging\" (which is like when the mentions don't have the same noun but still refer to the same thing), it's way harder. This is because there are way more possible candidates to consider, and you can't just rely on the words looking similar on the surface.",
        "formal_text": "Convert casual text to formal text: Sure! So, pronominal anaphora (like \"he\" or \"she\") and named entities (like names of people or places) can be handled pretty well by systems, but"
    },
    {
        "casual_text": "In the more flexible version of GPSG, they use unification to build trees. In the version from [GKPS85], the main category R of a smaller tree has to match up with a &mghter category C from a local tree (basically, R __. C and C  R).",
        "formal_text": "Convert casual text to formal text: In the more flexible version of GPSG, they use unification to build trees. In the version from [GKPS85], the main category R of a smaller tree has to match up with"
    },
    {
        "casual_text": "You can find it here: http://12r.cs.uiuc.edu/cogcomp/Data/QA/QC/definition.html",
        "formal_text": "Convert casual text to formal text: You can find it here: http://12r.cs.uiuc.edu/cogcomp/Data/QA/QC/definition.html. Convert"
    },
    {
        "casual_text": "Alright, let’s dive into this. First, we’ll take a closer look at the softmax layer in GPT-2 and explain why, even in context-aware language models, something like \"queenking = womanman\" tends to make sense. Then, we’ll get into our theoretical stuff, where we expand on the whole \"woman and king\" example. Basically, we’ll show that when you’re dealing with words in a low-dimensional space, it becomes impossible to rank some words above others.",
        "formal_text": "Convert casual text to formal text: Alright, let’s dive into this. First, we’ll take a closer look at the softmax layer in GPT-2 and explain why, even in context-aware language models"
    },
    {
        "casual_text": "Some updates have been suggested to make things run faster. Ukkonen's Enhanced Dynamic Programming Approximation algorithm (Ukkonen, 1985b) for figuring out edit distance has a worst-case complexity of O(|Q| x B), where B is the maximum possible edit distance. The cool thing is that the distance(i, j) values don’t go down along any diagonal, so we only need to calculate the highest distance(i, j) values for a given threshold k. The modified Berghel-Roach algorithm (Berghel and Roach, 1996) is like an upgrade to Ukkonen's method. It’s 42% faster than Ukkonen's approach for matching names and 79% quicker than the Wagner-Fischer algorithm. We’re using the Berghel-Roach algorithm (LD BR) to see how much faster we can get with a more efficient algorithm. For comparison, we’re also using the Wagner-Fischer algorithm (LD W F) as a baseline to measure the runtime.",
        "formal_text": "Convert casual text to formal text: Some updates have been suggested to make things run faster. Ukkonen's Enhanced Dynamic Programming Approximation algorithm (Ukkonen, 1985b) for figuring out"
    },
    {
        "casual_text": "We’ve found that an LSTM recurrent neural network can predict English article usage pretty well, even though it involves figuring out both nearby and distant clues. Even without fancy features, our model does better than older methods that only look at a small amount of context. It’s especially good at predicting when no article is needed and handling named entities. We did some digging to see if the improvements come from the usual reasons people say RNNs are great. By looking at the attention weights, we found that the model is actually picking up on complex stuff like syntactic and semantic rules, which older models had to manually code. Using pre-trained word embeddings helps a bit across different models, but not by much. The LSTM can also use long-range info when making predictions, since it can focus on any part of the sentence, no matter how far back it is from the noun. Plus, we’ve seen that LSTMs handle tricky semantic situations really well when they have a bigger context, like a few sentences.",
        "formal_text": "Convert casual text to formal text: We’ve found that an LSTM recurrent neural network can predict English article usage pretty well, even though it involves figuring out both nearby and distant clues. Even without fancy features,"
    },
    {
        "casual_text": "In the \"two-event method,\" the system doesn't mess with titles when it comes to selection. Once it grabs two events during sentence processing, it starts looking for a structure that connects these two events as their scenes.",
        "formal_text": "Convert casual text to formal text: In the \"two-event method,\" the system doesn't mess with titles when it comes to selection. Once it grabs two events during sentence processing, it starts looking for a structure that"
    },
    {
        "casual_text": "Here,  helps decide how much each part of the loss matters, and P CRF (Y|X) is explained in Equation 8.",
        "formal_text": "Convert casual text to formal text: Here,  helps decide how much each part of the loss matters, and P CRF (Y|X) explained in Equation 8. Convert casual text to formal text: Here,"
    },
    {
        "casual_text": "There are even more intricate differences to consider. For instance, Swahili has a more elaborate system with 18 classes. These systems are usually called 'noun classes' in the academic world and don't fall under the term 'grammatical gender' in this paper.",
        "formal_text": "Convert casual text to formal text: There are even more intricate differences to consider. For instance, Swahili has a more elaborate system with 18 classes. These systems are usually called 'noun classes' in the academic world"
    },
    {
        "casual_text": "Lastly, like we talked about earlier, we get rid of substitution (Joshi and Schabes, 1997, footnote 6). After making these tweaks, the sample TAG grammar and the derivation tree from Figures 1 and 4(a) could be shown using the core TAG grammar and the derivation tree from Figures 2 and 4(c).",
        "formal_text": "Convert casual text to formal text: Lastly, like we talked about earlier, we get rid of substitution (Joshi and Schabes, 1997, footnote 6). After making these tweaks, the sample TAG grammar and"
    },
    {
        "casual_text": "For our evaluation, we looked at hypernymy search and a basic edge-based distance using GermaNet. We also used a baseline method that combines simple named entity classification with GermaNet subsumption to determine semantic classes. Plus, we included an evolved version of Markert's approach. We also tried different setups for precision in coreferent bridging cases: considering candidates from the 4 preceding sentences, the 16 preceding sentences, and also checking if the anaphor is in the antecedent's similarity list, following Nissim's method from Versley's 2007 work. For methods using similarity and association measures, we ranked candidates based on their similarity or relatedness values. We also implemented an approach that uses lists of the most similar words to a given word, based on a similarity measure similar to Lin's. This method works if: (i) the candidate is in the list of words most similar to the anaphor, (ii) the anaphor is in the list of words most similar to the candidate, or (iii) the similarity lists of the anaphor and candidate share a common word. We tested different list lengths, like 15, 25, 50, and 100 items, since Gasperin and Vieira used 15.",
        "formal_text": "Convert casual text to formal text: For our evaluation, we looked at hypernymy search and a basic edge-based distance using GermaNet. We also used a baseline method that combines simple named entity classification with"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. 1. **Modernization ()**: Just think of it as making things more modern. 2. **Compounds**: These are words made up of two parts. For example: -  (huoyun) means \"obtain permission.\" -  (nisha) means \"mud.\" -  (tubian) means \"sudden change.\" 3. **Numeric Compounds**: These are numbers combined with words, like: -  (siqian riyuan) is \"four thousand Japanese yen.\" - 2003 (2003nian) is \"the year 2003.\" 4. **Reduplicated Words**: These are words where the same part is repeated, like: -  (yingbuyinggai) means \"should or should not.\" -  (chuchujinjin) means \"go in and go out.\" Now, proper names and numeric compounds are all nouns, so you don't have to guess what part of speech they are. We'll mainly focus on abbreviations, derived words, compounds, and reduplicated words.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. 1. **Modernization ()**: Just think of it as making things more modern. 2. **Compounds"
    },
    {
        "casual_text": "You can figure out what the string means by looking at the proof. Just treat /I and I as lambda-abstraction, and you'll get the term shown in (13).",
        "formal_text": "Convert casual text to formal text: You can figure out what the string means by looking at the proof. Just treat /I and I as lambda-abstraction, and you'll get term shown in (13"
    },
    {
        "casual_text": "When it comes to ranking scalar adjectives by how strong they are, most methods fall into two categories: pattern-based or lexicon-based. Pattern-based methods look at things like words or sentence structures (called \"patterns\") in big collections of text to figure out which adjectives are stronger or weaker. For instance, if you see a pattern like \"X, but not Y\" or \"not just X but Y,\" it suggests that X is a less intense adjective than Y. Researchers like Sheinman and Tokunaga (2009), de Melo and Bansal (2013), and Sheinman et al. (2013) have used this kind of approach, as well as others like Shivade et al. (2015) who focus on sentence structure.",
        "formal_text": "Convert casual text to formal text: When it comes to ranking scalar adjectives by how strong they are, most methods fall into two categories: pattern-based or lexicon-based. Pattern-based methods look at things"
    },
    {
        "casual_text": "Since unsupervised models really shine when they can be used across different stuff, we’ve been big on testing them on texts they weren’t originally trained on, especially in other languages. Turns out, Clark’s system from 2003 worked the best when we tried it on non-English languages.",
        "formal_text": "Convert casual text to formal text: Since unsupervised models really shine when they can be used across different stuff, we’ve been big on testing them on texts they weren’t originally trained on, especially in other languages. Turns out"
    },
    {
        "casual_text": "In this project, we think that adding some local, instance-level info from extra data while training can actually make the learning algorithm work better. We tested this idea on our model and did a bunch of experiments to show it works. Even though people have looked a lot at instance-based learning in AI, it’s not really been used much in deep learning for transfer learning. One thing we could do to make our approach even better is to use a smarter way to search for instances, which could help speed things up. We’ve shown that our method can reduce how much labeled data we need, and this could also be tested in an unsupervised setting. We could also improve how we modify features and combine that with the search part to make the whole thing more effective. We assumed that the datasets we used are from the same domain, but in the future, we’ll need to figure out how to handle datasets from different domains to make the system work with a wider range of data.",
        "formal_text": "Convert casual text to formal text: In this project, we think that adding some local, instance-level info from extra data while training can actually make the learning algorithm work better. We tested this idea on our model and did a bunch"
    },
    {
        "casual_text": "We're using 300-dimensional pre-trained GloVe 2 word embeddings (thanks to Pennington et al., 2014) that were trained on web data. We've got three different convolution filters—sizes 3, 4, and 5—each with 100 feature maps. The GRU's hidden states are set to 300 dimensions. For optimization, we're using the adam optimizer (props to Kingma and Ba, 2015) with an initial learning rate of 1.0  10-4. The learning rate gets cut in half every 20 epochs during training. We've also set the dropout probability to 0.3.",
        "formal_text": "Convert casual text to formal text: We're using 300-dimensional pre-trained GloVe 2 word embeddings (thanks to Pennington et al., 2014) that were trained on web data."
    },
    {
        "casual_text": "You'll find more results and analysis in Appendix B. They show that our methods can also tell us how well different target languages work with a specific source language and vice versa for the tasks we talked about in this paper.",
        "formal_text": "Convert casual text to formal text: You'll find more results and analysis in Appendix B. They show that our methods can also tell us how well different target languages work with a specific source language and vice versa for the"
    },
    {
        "casual_text": "Okay, so x_k(w) is just the number of times a word w shows up in a particular year k. This is the only way to measure how much a word's use changes over time that’s balanced, adds up correctly, and has a consistent scale (Tornqvist et al., 1985). What’s cool about this method is that it treats increases and decreases in how often a word is used equally. The average changes in how often words were used were -0.486 (plus or minus 1.644) for slang words and 0.533 (plus or minus 1.070) for non-slang words. A positive number means the word got used more often. As you can see in Figure 2, not only did more slang words go down in use compared to non-slang words, but the slang words that increased the most in frequency were also the ones that saw the biggest jumps. We also looked at the absolute value of equation (4) to see how much change there was overall, whether the word got more or less popular. And, as expected, slang words had way bigger changes in how often they were used compared to non-slang words (p  0.05). For more info, check out Appendix C.",
        "formal_text": "Convert casual text to formal text: Okay, so x_k(w) is just the number of times a word w shows up in a particular year k. This is the only way to measure how much"
    },
    {
        "casual_text": "In this process, we group the training data into smaller chunks, which we call domains. In SMT (Statistical Machine Translation), a bilingual dataset is used to build the translation model. Usually, they also use bilingual data along with extra monolingual data to create the language model. In our approach, we group both the bilingual and monolingual data. After clustering, we make domain-specific language and translation models using the data from these clusters.",
        "formal_text": "Convert casual text to formal text: Convert casual text to formal text: In this process, we group the training data into smaller chunks, which we call domains. In SMT (Statistical Machine Translation), a bilingual dataset is"
    },
    {
        "casual_text": "When dealing with less common phrases, you need to get creative to find the right way to translate them. Even super common phrases might require some thinking to make sure they don't sound too generic. For example, if you have a general adjective like \"good,\" \"great,\" or \"nice\" modifying a noun, there might be a more specific adjective in the target language that would work better with that noun. In practice, anything can be described as \"good,\" and the most basic translation for \"good\" is . But just translating \"good idea\" as  might not always be the best choice, depending on the context and style of the original text. In the Cambridge Dictionary, under the entry for \"idea,\" there are two example sentences using \"good idea,\" and both are translated as . There's another example with \"bright idea\" (which means the same as \"good idea\"), and that's translated as , which is a bit more casual. The real question is, how do we help translators think of other possible translations?",
        "formal_text": "Convert casual text to formal text: When dealing with less common phrases, you need to get creative to find the right way to translate them. Even super common phrases might require some thinking to make sure they don't sound too generic. For"
    },
    {
        "casual_text": "Also, since open scene datasets are often super noisy, just copying knowledge over isn't the best idea. To make our model tougher, we added virtual adversarial training to our algorithm for building semantic spaces. Tests show that MORE-RLL creates sharper semantic representations and works really well on actual datasets.",
        "formal_text": "Convert casual text to formal text: Also, since open scene datasets are often super noisy, just copying knowledge over isn't the best idea. To make our model tougher, we added virtual adversarial training to our"
    },
    {
        "casual_text": "This just goes to show that the dataset we came up with for testing numerical understanding is pretty tough for today's systems. We break down how our system did and where it went wrong in the fourth paragraph of this section.",
        "formal_text": "Convert casual text to formal text: This just goes to show that the dataset we came up for testing numerical understanding is pretty tough for today's systems. We break down how our system did and where it went wrong in the fourth paragraph of"
    },
    {
        "casual_text": "If you're working with documents that have formatting from a specific editing tool (like Microsoft Word or HTML editors), it can be super helpful to make changes in the same or a similar program. That's because these tools let you use format-based operations, like searching and replacing based on formatting, which can save you a lot of time and effort.",
        "formal_text": "Convert casual text to formal text: If you're working with documents that have formatting from a specific editing tool (like Microsoft Word or HTML editors), it can be super helpful to make changes in the same or a similar program."
    },
    {
        "casual_text": "This paper is all about figuring out if pre-trained models have a special kind of common sense: the ability to compare objects physically, like which one is bigger or faster. The task we set up is pretty straightforward: we give the system two words, say \"car\" and \"bike,\" and ask it to decide which one is \"faster\" based on size or speed (see section 2.1). We use a simple model—either a linear one or a one-layer neural network—that just combines (by either putting them together or subtracting one from the other) the pre-trained word embeddings of the two words we’re comparing (section 2.2). This simple setup actually works better than older methods that used extra info, like the verbs connecting the words, on a dataset called Verb Physics (Forbes and Choi, 2017) (section 3). It shows that these pre-trained models can indeed make physical comparisons. What’s cool is that this model can also handle objects it hasn’t seen during training (section 3.1) and does better than other models that rely on quirks of the dataset (section 4). We kept the model simple on purpose because more complex models make it hard to tell if the results are coming from the model itself or the embeddings (like in other studies, e.g., Liu et al. (2019)). Another big part of our work is analyzing how these models actually compare objects. The confidence scores (called logits) the model gives for each label show that it consistently ranks objects the same way, no matter which object you start with (section 4.1.1).",
        "formal_text": "Convert casual text to formal text: This paper is all about figuring out if pre-trained models have a special kind of common sense: the ability to compare objects physically, like which one is bigger or faster. The task"
    },
    {
        "casual_text": "Our model was trained with just a little bit of data and not too many features. For future work, we want to try it out with lots of monolingual data, which would let us use way more features. Right now, we tested it on a language pair (Chinese to English) where there's a ton of bilingual stuff available for the domain we used. Later, we’re thinking of looking at low-resource areas and language pairs like Urdu-English, where there’s not much bilingual data for new domains.",
        "formal_text": "Convert casual text to formal text: Our model was trained with just a little bit of data and not too many features. For future work, we want to try it out with lots of monolingual data, which would let us use way"
    },
    {
        "casual_text": "3. For each potential fixation, we check if it lasts for T seconds or more. If it does, we consider it a real fixation.",
        "formal_text": "Convert casual text to formal text: 3. For each potential fixation, we check if it lasts for T seconds or more. If it does, we consider it a real fixation. 4. For each potential fixation, we"
    },
    {
        "casual_text": "The Stanford Sentiment Treebank, created by Socher and colleagues in 2013, has sentences from movie reviews along with human-made sentiment labels. The goal is to figure out the sentiment of a sentence. Like in the GLUE setup, we're sticking to a simple positive/negative split and only looking at sentiment labels for whole sentences.",
        "formal_text": "Convert casual text to formal text: The Stanford Sentiment Treebank, created by Socher and colleagues in 2013, has sentences from movie reviews along with human-made sentiment labels. The goal is to figure out the sentiment of a sentence"
    },
    {
        "casual_text": "Basically, the Kurmanji language model features were the most important overall. But adding PanPhon features gave almost as much improvement as adding Tajik features. The best results came when we used both Kurmanji and PanPhon features together. This is great because not every language has a close relative with a clear writing system, but PanPhon features can be used for any language.",
        "formal_text": "Convert casual text to formal text: Basically, the Kurmanji language model features were the most important overall. But adding PanPhon features gave almost as much improvement as adding Tajik features. The best results came when"
    },
    {
        "casual_text": "For binary classification, we're training a 1-layer MLP with 512 hidden units. We're using Adam as the optimizer with a learning rate of 0.0001. To prevent overfitting, we're applying dropout with a probability of 0.5 at both the hidden and input layers. Additionally, we're adding some noise to the input features—specifically, isotropic Gaussian noise with a standard deviation of 0.5.",
        "formal_text": "Convert casual text to formal text: For binary classification, we're training a 1-layer MLP with 512 hidden units. We're using Adam as the optimizer with a learning rate of 0.0001. To prevent over"
    },
    {
        "casual_text": "We use the projective parser and the fixed tree parser from Groschwitz et al. (2018), focusing on the top 6 supertags. If the fixed tree parser takes more than 30 minutes to finish with k supertags, we try again with k  1 supertags. If k ends up being 0, we just go with a placeholder graph that has one node.",
        "formal_text": "Convert casual text to formal text: We use the projective parser and the fixed tree parser from Groschwitz et al. (2018), focusing on the top 6 supertags. If the fixed tree pars"
    },
    {
        "casual_text": "Our goal is to prove that second-order similarity gives better results than first-order similarity. But before we dive into that, we need to figure out the best way to combine the results in our tests. In Figure 1, we’ve plotted the recall-precision breakeven points (for the outlier class) after running 250 trials for each of our four first-order similarity methods (inverse Euclidean, inverse Manhattan, cosine, min-max) paired with four different aggregation functions (centroid, mean, k-NN mean, median). It’s pretty clear that k-NN is the top performer every time. So, to give the baseline methods a fair shot, we’ll use k-NN as our go-to aggregation function for all the experiments that follow. Now, we’re set to run our main experiment. We’ll use BOW as our feature set and k-NN for aggregation. For the impostor set, we’ve got 500 random blog posts. In Figure 2, we’ve got the recall-precision curves for outlier documents after 250 trials, comparing the four first-order similarity measures with our second-order similarity measure, which is built on each of the first-order ones. The results show that even the weakest second-order similarity method crushes all the standard first-order methods. Finally, in Figure 3, we’ve listed the breakeven values for each measure, pairing each first-order measure with the second-order one that’s based on it.",
        "formal_text": "Convert casual text to formal text: Our goal is to prove that second-order similarity gives better results than first-order similarity. But before we dive into that, we need to figure out the best way to combine the results in"
    },
    {
        "casual_text": "The research on cross-lingual text retrieval (CLTR) has some work that’s pretty similar to what we’re doing here. Lately, a lot of approaches have been focusing on using dictionaries and corpora to translate queries from one language to another, like the language of the documents you’re searching through (Oard, 1997). For instance, some methods create queries in the target language using a corpus-based technique that’s kind of like what we’re talking about. But the thing is, they don’t really try to figure out if the terms they’re using are specific to a certain field or just general words, and they don’t involve any manual work. What we’re doing here is more about semi-automating the process of building translation dictionaries that are tailored to a specific field. You could think of it as providing bilingual dictionary entries for CLTR methods, similar to what Davis did later on (Davis, 1996). In his approach, he first used a dictionary to create a query in the target language, which could be a bit unclear, and then used a corpus to figure out the right meaning.",
        "formal_text": "Convert casual text to formal text: The research on cross-lingual text retrieval (CLTR) has some work that’s pretty similar to what we’re doing here. Lately, a lot of approaches have been focusing"
    },
    {
        "casual_text": "• We're using the Transformer model (Vaswani et al., 2017) here. It's trained with the OpenNMT library. For both news and dialogues, we keep the training parameters the same, except for the minimum length of the summary we generate—35 for news and 15 for dialogues.",
        "formal_text": "Convert casual text to formal text: • We're using the Transformer model (Vaswani et al., 2017 here. It's trained with the OpenNMT library. For both news and dialogues,"
    },
    {
        "casual_text": "Even though the results from the three solvers are pretty similar (p-value > 0.4), they lead to different conclusions. The solution from GLPK, shown in Table 3, has ROUGE-1 recall scores for the first optimal solution found by each solver. The table also shows how many systems got significantly lower () or higher () scores (p-value  0.05).",
        "formal_text": "Convert casual text to formal text: Even though the results from the three solvers are pretty similar (p-value > 0.4), they lead to different conclusions. The solution from GLPK, shown in Table 3, has ROUG"
    },
    {
        "casual_text": "Once we get an embedding vector for each clause, we need to figure out the causal relationship of the input ECP in a specific context. To do this, we have to add context information to the embedding vectors of the input ECP before making any predictions. For this part, we're looking at three common methods used in text processing: explicit concatenation, implicit encoding, and the attention-based method. We'll show how well these methods work and talk about them more in Section 6.",
        "formal_text": "Convert casual text to formal text: Once we get an embedding vector for each clause, we need to figure out the causal relationship of the input ECP in a specific context. To do this, we have to add context information"
    },
    {
        "casual_text": "In this part, we'll talk about the languages we looked at and where we got our data from. After that, we'll give a quick rundown of the cross-lingual word embedding method and how we set up the classifier for gender transfer.",
        "formal_text": "Convert casual text to formal text: In this part, we'll talk about the languages we looked at and where we got our data from. After that, we'll give a quick runn of the cross-lingual word embed"
    },
    {
        "casual_text": "Alright, let’s dive into the topic of big TGMs. These TGMs can be a real problem because even someone not super tech-savvy can mess with them, like making up fake news or writing bogus product reviews. Table 1 gives a quick rundown of their main features and the risks they bring, based on what the original papers say.",
        "formal_text": "Convert casual text to formal text: Alright, let’s dive into the topic of big TGMs. These TGMs can be a real problem because even someone not super tech-savvy can mess with them, like making"
    },
    {
        "casual_text": "Another thing to think about when making summaries is figuring out what bits of the conversation are actually important. Take Dialogue 3, for example. The model called Dy-namicConv + GPT-2 emb. with sep. does a good job with the summary, but it picks a different piece of info than the one in the reference summary. Meanwhile, some other models—like Fast Abs RL enhanced—manage to grab both important bits from the conversation. But then, when it comes to summarizing Dialogue 5, the models seem to get stuck on the phrase \"it's the best place,\" which, honestly, doesn’t feel like the most crucial thing to highlight.",
        "formal_text": "Convert casual text to formal text: Another thing to think about when making summaries is figuring out what bits of the conversation are actually important. Take Dialogue 3, for example. The model called Dy-namicConv +"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. Imagine we have a situation where |S| > 2 , and there's no chance of messing up on training points. Taylor mentioned this back in 2000. Now, think about setting a limit (threshold) on a space of real-valued functions, and we have a fixed  that's a positive real number. For any probability distribution D on X, with a high chance (1  ) of getting the training set S right, any function f in F that pairs with g f in G = F  L(X ) will have a generalization error that's not too big.",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down in simpler terms. Imagine we have a situation where |S| > 2 , and there's no chance of messing up on training points"
    },
    {
        "casual_text": "Another approach to figuring out what pronouns refer to, like the one by Ge et al. (1998), uses stats to help out. They look at how far apart the pronoun and the possible word it’s referring to are to see how likely they are connected. They also check things like gender, number, and whether the word refers to a living thing to narrow it down. Plus, they use head information to set some rules and count how many times a word is mentioned.",
        "formal_text": "Convert casual text to formal text: Another approach to figuring out what pronouns refer to, like the one by Ge et al. (1998), uses stats to help out. They look at how far apart the"
    },
    {
        "casual_text": "An alignment system matches up the surface representations of two languages. (Shin, 1996) tested an expectation-maximization algorithm and got 68.7% accuracy at the phrase level. This will be added to the version 2 platform.",
        "formal_text": "Convert casual text to formal text: An alignment system matches up the surface representations of two languages. (Shin, 1996) tested an expectation-maximization algorithm and got 68.7% accuracy at the phrase level. This will"
    },
    {
        "casual_text": "Constraints b and c can be handled automatically once we have the word alignments, but when it comes to Constraint a and aligning non-terminal nodes, we need to think about language stuff. The main thing here is figuring out which non-terminal nodes are responsible for the grammar bits that the unaligned words are pointing to. This way, when we align the non-terminal nodes, they’ll naturally include those unaligned words in the right grammatical spot. When we’re picking which non-terminal nodes to align, we have to balance two rules that might seem a bit at odds with each other:",
        "formal_text": "Convert casual text to formal text: Constraints b and c can be handled automatically once we have the word alignments, but when it comes to Constraint a and aligning non-terminal nodes,"
    },
    {
        "casual_text": "But, a single Chinese character doesn’t hold much info. Just relying on characters isn’t enough for creating Chinese abbreviations. So, we came up with an MSU-based method that helps pick the most important characters by looking at the local MSU context.",
        "formal_text": "Convert casual text to formal text: But, a single Chinese character doesn’t hold much info. Just relying on characters isn’t enough for creating Chinese abbreviations. So, we came up with an M"
    },
    {
        "casual_text": "So, what we're saying here is that some parts of how we understand things depend on the language we're dealing with, and you can't really explain everything in just one language. We need to think about how we handle two languages separately from how we handle just one. In the earlier part of Section 2, we thought the transfer phase was just about figuring out how to change the source text's stuff into the target text's stuff. We thought we could take the factors we got from the analysis phase and use them to make the target text, but we realized that's not totally accurate. The transfer phase needs to do more than that. The new idea is shown in Figure 7. We're sticking with the usual three-phase setup that most systems use, but we're not saying it's the ultimate best way or that you have to do it in that exact order. Instead, we're suggesting that the 'understanding' phase could pull out not just the stuff that makes up the source text but also the stuff that will help make the target text. But even with that, we think the results from the understanding phase will still be different for each language pair, not something that works the same way for all languages.",
        "formal_text": "Convert casual text to formal text: So, what we're saying here is that some parts of how we understand things depend on the language we're dealing with, and you can't really explain everything in just one language. We"
    },
    {
        "casual_text": "QRA gives you a single score that shows how reproducible a system and its evaluation method are. It does this by looking at the scores and differences between multiple attempts to replicate the same original study. We noticed that this method helps us figure out where the variations come from in different reproductions, makes it easier to compare results from different reproducibility checks, and even suggests what changes need to be made in the system or evaluation design to make things more reproducible.",
        "formal_text": "Convert casual text to formal text: QRA gives you a single score that shows how reproducible a system and its evaluation method are. It does this by looking at the scores and differences between multiple attempts to replicate the same original study"
    },
    {
        "casual_text": "In this part, we'll look at how well our manifold ranking method (mentioned in Section 5) works for spotting fake hotels.",
        "formal_text": "Convert casual text to formal text: In this part, we'll look at how well our manifold ranking method (mentioned in Section 5) works for spotting fake hotels. Convert casual text to formal text"
    },
    {
        "casual_text": "To get a clearer idea of these issues, let’s look at a quick example of a question, paragraph, and answer combo to dive deeper into the topic.",
        "formal_text": "Convert casual text to formal text: To get a clearer idea of these issues, let’s look at a quick example of a question, paragraph, and answer combo to dive deeper into the topic. Convert casual text"
    },
    {
        "casual_text": "This approach can totally work in the food world too. For all our tests, we’re using a dataset from chefkoch.de, which is like the biggest German website for anything food-related. It has 418,558 forum pages, and we’re using that for our experiments. In Table 1, you can see how well coordination works for pulling out food items from our special food-focused data. We started with a small list of 10 common food items (like water, salt, sugar, salad, bread, meat, cake, flour, and potato) and then looked at all the combinations of words that come up together, ranking them by how often they appear. We did this both on our food-specific data and on Wikipedia. As a comparison, we also just ranked all the nouns by how often they show up in our food data. The table shows that just going by how often something appears isn’t really effective. But when we use those word combinations (conjuncts) from our food-specific data, we get much better results.",
        "formal_text": "Convert casual text to formal text: This approach can totally work in the food world too. For all our tests, we’re using a dataset from chefkoch.de, which is like the biggest German website for anything food-related"
    },
    {
        "casual_text": "CVaR is being too cautious because it considers all kinds of groups, even ones that are totally messed up and don't make sense. To fix this, we'll focus on optimizing models for actual, meaningful subgroups instead of just any random group.",
        "formal_text": "Convert casual text to formal text: CVaR is being too cautious because it considers all kinds of groups, even ones that are totally messed up and don't make sense. To fix this, we'll focus on"
    },
    {
        "casual_text": "It’s important to note that when a feature helps one class, it also takes away from the others. Let’s say f is a prediction function where the results are balanced across all classes C. Every feature you input affects the classification of a specific class, but it also impacts the other classes in the opposite way. A method that assigns positive relevance to a feature for every class doesn’t really help us understand how the model works. What we need is an explanation method that meets the Class Zero-Sum condition, meaning the total relevance of each feature x i across all classes adds up to zero.",
        "formal_text": "Convert casual text to formal text: It’s important to note that when a feature helps one class, it also takes away from the others. Let’s say f is a prediction function where the results are balanced across all"
    },
    {
        "casual_text": "You know how multiple keyphrases often come from different spots in the source text, right? (Check out Figure 1 for a visual.) Once a word has been summarized, there’s no need to focus on it again because the attention mechanism naturally zeroes in on the important parts of the source text. To deal with the coverage problem, we added a coverage mechanism (shoutout to Tu et al., 2016) to our model. This helps spread out the attention across multiple keyphrases, making sure the most important areas of the source document get noticed and turned into keyphrases.",
        "formal_text": "Convert casual text to formal text: You know how multiple keyphrases often come from different spots in the source text, right? (Check out Figure 1 for a visual.) Once a word has been summarized, there’"
    },
    {
        "casual_text": "Sure! Here's the informal version: \"Check out this combined disaster response dataset: https://appen.com/datasets/combined-disaster-response-data/3. Also, take a look at https://thedeep.io.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: \"Check out this combined disaster response dataset: https://appen.com/datasets/combined-disaster-respon"
    },
    {
        "casual_text": "@MSNBC Yeah, sure, all those jobs will be in China. In response to @realDonaldTrump: I'll be the best jobs-creating president God ever made.",
        "formal_text": "Convert casual text to formal text: @MSNBC Yeah, sure, all those jobs will be in China. In response to @realDonaldTrump: I'll be the best jobs-creating president God ever made."
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: To compare our work with previous studies (like Mullenbach et al. in 2018, Li and Yu in 2020, and Vu et al. in 2020), we tested PLM-ICD on the MIMIC-2 dataset (thanks to Saeed et al. in 2011). We stuck to the setup from Mullenbach et al. (2018), using 20,533 summaries for training and 2,282 for testing. The dataset has 5,031 labels in total.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: To compare our work with previous studies (like Mullenbach et al. in 2018, Li and Yu in 2020, and V"
    },
    {
        "casual_text": "We take each input token w i and turn it into a learned embedding, which then goes into the input layer of the LSTM. We’re looking at a few different ways to make this representation better than the basic \"vanilla\" LSTM model. First, we play around with whether or not to include POS tags as a way to add some syntactic info (+/POS). If we do include them, we stick the POS tags onto the learned embedding before sending it to the input layer. Another thing we’re thinking about is how to set up the embedding for each word-type. We try two approaches: one where we just randomly start with something, and another where we use pre-trained vectors, which come from a big external dataset. The idea behind using pre-trained vectors is that they might help the model figure out things like bridging references or when words are synonyms (like \"house\" and \"home\"). So, we’re comparing these options:",
        "formal_text": "Convert casual text to formal text: We take each input token w i and turn it into a learned embedding, which then goes into the input layer of the LSTM. We’re looking at a few"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: GCN GCN GCN refers to Graph Convolutional Networks, which are used in machine learning to work with graph-structured data. Quantity Cell Q \" Q # Q $ Q % \" # % $ is talking about different quantities or values in a cell, maybe related to data or calculations. Q \" Q # Q $ Q % BiLSTM is combining those quantities with a BiLSTM, which stands for Bidirectional Long Short-Term Memory, a type of neural network often used for sequence prediction problems. So, in short, it's about using GCNs and BiLSTMs to handle and analyze data, possibly in a structured way.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: GCN GCN refers to Graph Convolutional Networks, which are used in machine learning to work with graph-"
    },
    {
        "casual_text": "Alright, let's break it down. For each sentence, which we can think of as a group of words W = w 1, w 2, . . . , w n , we need to figure out how these words connect to the events they’re talking about.",
        "formal_text": "Convert casual text to formal text: Alright, let's break it down. For each sentence, which we can think of as a group of words W = w 1, w 2, . . ."
    },
    {
        "casual_text": "One thing to keep in mind is that this approach would mean switching between training on a specific part of the data and an expanded version of the backtranslation dataset, following our training plan. Given how big the backtranslation dataset is, this would mean a ton of extra training time.",
        "formal_text": "Convert casual text to formal text: One thing to keep in mind is that this approach would mean switching between training on a specific part of the data and an expanded version of the backtranslation dataset, following our training plan. Given how"
    },
    {
        "casual_text": "We ran three linear mixed-effects models on the reading times from the held-out set using lme4 (Bates et al., 2015). The full model had both CharWSurp and FreqWSurp as fixed effects, while the other two models only had one of them. This setup gave us two pairs of nested models, which we could compare using a likelihood ratio test (LRT). The first LRT checked how much CharWSurp contributed by comparing the full model to the one without CharWSurp. Likewise, the second LRT looked at how much FreqWSurp contributed by comparing the full model to the one without FreqWSurp.",
        "formal_text": "Convert casual text to formal text: We ran three linear mixed-effects models on the reading times from the held-out set using lme4 (Bates et al., 2015). The full model had both Char"
    },
    {
        "casual_text": "Okay, so let’s break it down in simpler terms. Let’s say  represents the overall topic distribution, and _k is the word distribution for a specific topic, say topic k. Now, for the dth tweet, _d is the distribution that decides which topic to pick when assigning hashtags. The probability of choosing topic k is just randomly picked from the available topics based on the content.",
        "formal_text": "Convert casual text to formal text: Okay, so let’s break it down in simpler terms. Let’s say  represents the overall topic distribution, and _k is the word distribution for a specific topic, say"
    },
    {
        "casual_text": "For the [T, H] RT E5-sample, the system uses a straightforward approach: if a pair has different judgment-related phenomena and one of them is linked to a contradiction with a probability greater than 50%, the pair is labeled as a contradiction. If not, it's labeled as an entailment.",
        "formal_text": "Convert casual text to formal text: For the [T, H] RT E5-sample, the system uses a straightforward approach: if a pair has different judgment-related phenomena and one of them is linked"
    },
    {
        "casual_text": "E-HowNet is like an updated version of HowNet (created by Dong & Dong in 2006) that focuses on how words and their meanings are connected. It uses a system where word meanings are explained by basic concepts and something called attribute-values, which are like relationships between those concepts. Here's a quick example to show how E-HowNet represents a word's meaning.",
        "formal_text": "Convert casual text to formal text: E-HowNet is like an updated version of HowNet (created by Dong & Dong in 2006) that focuses on how words and their meanings are connected. It uses"
    },
    {
        "casual_text": "In this part, we’re talking about our GNMTF method, which works with just a few labeled documents. For this semi-supervised version of GNMTF, we set some parameters to specific values based on experiments: Iter = 100,  1 =  2 = 2,  =  =  =  = 1, and p = 10 for both document and word spaces. We also ran the process 10 times to make sure the results weren’t affected by random starting points. Since we don’t have much space here, we’re not diving deep into how we chose these parameters. For CNMTF, we kept things fair by setting  =  = 1. We also compared our GNMTF to a couple of other semi-supervised methods mentioned in (Li et al., 2009): (1) The Consistency Method, which balances local and global consistency (Zhou et al., 2004); (2) GFHF, which uses Gaussian fields and harmonic functions for semi-supervised learning (Zhu et al., 2003). On top of that, we compared our GNMTF to a popular supervised method: the Support Vector Machine (SVM), which is commonly used for sentiment classification (Pang et al., 2002).",
        "formal_text": "Convert casual text to formal text: In this part, we’re talking about our GNMTF method, which works with just a few labeled documents. For this semi-supervised version of GNMTF, we set some"
    },
    {
        "casual_text": "In this project, we’re looking at how to figure out who the main criminals are and who’s just helping out based on the details of a crime case. The main criminal is the one who’s in charge, leads a group, or takes a big part in the crime. On the other hand, an accessory is someone who’s more in a supporting role, helping out but not the main player. As shown in Fig. 1, our goal is to take the description of the crime and a list of people involved, and then figure out who’s the main criminal and who’s just assisting.",
        "formal_text": "Convert casual text to formal text: In this project, we’re looking at how to figure out who the main criminals are and who’s just helping out based on the details of a crime case. The main criminal is"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Imagine you have a sentence like this: \"A man ate it.\" Now, when we look at this sentence, we're kind of thinking about all the different people who could have eaten something. It's like we're talking about the idea of someone eating, not just one specific person. So, it's more about the general concept of eating rather than focusing on a particular individual.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. Imagine you have a sentence like this: \"A man ate it.\" Now, when we look at this sentence, we"
    },
    {
        "casual_text": "The thing is, a lot of benchmarks don't have the right kind of knowledge bases that cover everything they need. Plus, older methods usually need special, custom supervision to work with knowledge, which can be a hassle (Mitra et al., 2019; Chang et al., 2020). This makes it tricky to quickly adapt new pretrained models for different tasks.",
        "formal_text": "Convert casual text to formal text: The thing is, a lot of benchmarks don't have the right kind of knowledge bases that cover everything they need. Plus, older methods usually need special, custom supervision to work with knowledge,"
    },
    {
        "casual_text": "ListNet, a listwise ranking algorithm, approaches learning the ranking function f(x j ) differently. Instead of looking at individual sentences or concepts, it considers a whole list of them as a single unit. Both RankNet and ListNet also account for the relationships between the sentences or concepts in the list.",
        "formal_text": "Convert casual text to formal text: ListNet, a listwise ranking algorithm, approaches learning the ranking function f(x j ) differently. Instead of looking at individual sentences or concepts, it considers a whole list"
    },
    {
        "casual_text": "A partial proof structure is basically a proof frame that has some axiomatic formulae paired up. A full proof structure is just a partial one where all the axiomatic formulae are matched, no missing pieces.",
        "formal_text": "Convert casual text to formal text: A partial proof structure is basically a proof frame that has some axiomatic formulae paired up. A full proof structure is just a partial one where all the axiomatic formula"
    },
    {
        "casual_text": "Rewriting training data doesn't just speed up simultaneous translations; it also makes batch translations better. One issue is that when translating from SOV to SVO, like from Japanese to English, the verb often gets left out because of all the shuffling around. (This happens with German too, by the way.) When the source and target languages have similar word orders, there's less need to rearrange stuff, which helps phrase-based machine translation (Xu et al., 2009). Table 3 shows how many verbs were in the translations from the test sentences, comparing GD, RW, RW+GD, and the gold reference. Both RW and RW+GD end up with more verbs (and it's a statistically significant difference), with RW+GD getting the most verbs overall.",
        "formal_text": "Convert casual text to formal text: Rewriting training data doesn't just speed up simultaneous translations; it also makes batch translations better. One issue is that when translating from SOV to SVO, like from Japanese to English,"
    },
    {
        "casual_text": "We looked into whether localness modeling needs to be applied to every layer. Since TRANSFORMER has both encoder and decoder self-attention, plus encoder-decoder attention, we wanted to see which parts of the attention networks get the most out of localness modeling. To make sure other factors didn't mess with the results, we started by testing only the encoder-side self-attention. Table 1 shows that all the window prediction methods we tried improved the model's performance compared to the basic setup, proving that localness modeling really helps in self-attention. Specifically, the layer-specific and query-specific window approaches did better than the fixed one, showing that a flexible system can adapt to different local contexts based on the layer and query. Plus, this flexible strategy doesn't rely on manually set parameters (like a preset window size), making it more versatile for different languages and NLP tasks. For speed during training, we decided to use the query-specific prediction as the standard method in our later experiments.",
        "formal_text": "Convert casual text to formal text: We looked into whether localness modeling needs to be applied to every layer. Since TRANSFORMER has both encoder and decoder self-attention, plus encoder-decoder attention, we"
    },
    {
        "casual_text": "Okay, so here's the deal: we're talking about how confident our classifier is, and we're using something called BoosTexter (created by Schapire and Singer back in 2000) to figure that out. BoosTexter is the machine learning algorithm behind all this. We measure confidence by looking at the difference between the biggest weight and the second biggest weight for the labels, which is what the BoosTexter docs recommend. We've set a threshold of 0.0008 for what we consider \"low confidence.\" Now, we're running our classifier and another system called SWSD on some data. We're comparing the original system (let's call it OS/O) to three other versions that are more \"sense-aware\": one that only uses R1, another that only uses R2, and a third that uses both R1 and R2 together (R1R2). The results are in Table 3, so check that out. The R1 version improves accuracy by 2.3 points, which means it reduces errors by 9.4%. The R2 version does even better, boosting accuracy by 3.6 points and cutting errors by 14.6%. But here's the kicker: when we use both R1 and R2 together (R1R2), we get a 5.9-point boost in accuracy, which is a whopping 24% reduction in errors. So yeah, combining both rules seems to work best.",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: we're talking about how confident our classifier is, and we're using something called BoosTexter (created by Schapire and"
    },
    {
        "casual_text": "This vector shows us a few things about horses. First, it tells us that all horses are mammals—so if you take the number of horses that are also mammals and divide it by the total number of horses, you get 1, meaning every single horse is a mammal. Second, it says that horses and things with scales don’t overlap at all—in other words, no horse has scales. We also find out that most horses have four legs, and some of them are brown.",
        "formal_text": "Convert casual text to formal text: This vector shows us a few things about horses. First, it tells us that all horses are mammals—so if you take the number of horses that are also mammals and divide it by"
    },
    {
        "casual_text": "We calculate a normalized community similarity score for pairs i, j by subtracting s i, m from s i, j, where s i, m is the score from the \"subreddit merged others.\" We look at how this score correlates with community feedback for three different models. The results are in Table 4 for thread-level analysis and Table 5 for user-level analysis. At the thread level, the hyb-500. 30 style model consistently shows a positive and statistically significant correlation between a post's stylistic similarity score and its karma. This means that adapting your language style can help your post get more upvotes. None of the other models we looked at earlier had this feature, and for the topic models, the correlation is usually negative. On the user level, all the correlations between a user's k-index and their style/topic match are statistically significant. The hyb-500. 30 style model shows a stronger positive correlation compared to the others. In both cases, the word_only model gives results that are somewhere in between the style and topic models. The hyb-15k model is similar to the word_only model, and the tag_only model mostly shows negative correlations.",
        "formal_text": "Convert casual text to formal text: We calculate a normalized community similarity score for pairs i, j by subtracting s i, m from s i, j, where s i"
    },
    {
        "casual_text": "Besides the Multi-Span Text Reading (MSTR) method that we're using as a way to quickly read through long articles, we're also giving Longformer (from Beltagy et al., 2020) a shot. Longformer is designed to handle long articles efficiently by using an attention mechanism that grows in a way that's proportional to the length of the text. This attention mechanism can be easily swapped in for the usual self-attention and works by combining a local windowed approach.",
        "formal_text": "Convert casual text to formal text: Besides the Multi-Span Text Reading (MSTR) method that we're using as a way to quickly read through long articles, we're also giving Longformer (from Beltag"
    },
    {
        "casual_text": "Check out Table 2 for examples of explicit, implicit, and false sentences. Sure, the assumption isn't perfect, but it works for about 76% of the ground truth we generated. For our evaluation, we think this is good enough because it adds some realistic noise while still clearly showing the relational content in the evaluation sets.",
        "formal_text": "Convert casual text to formal text: Check out Table 2 for examples of explicit, implicit, and false sentences. Check out Table 2 for examples of explicit, implicit, and false sentences. Convert casual text to formal text: Check out Table"
    },
    {
        "casual_text": "Adding more sentence pairs to a parallel corpus is super easy since we assume each pair is independent of the others. To make things simpler, we'll explain all the models by looking at sentences one by one.",
        "formal_text": "Convert casual text to formal text: Adding more sentence pairs to a parallel corpus is super easy since we assume each pair is independent the others. To make things simpler, we'll explain all the models by looking at sentences one"
    },
    {
        "casual_text": "To measure how different texts are from each other, we use Self-BLEU (Zhu et al., 2018). This method looks at each generated text and calculates the BLEU score by treating all the other generated texts as reference points. This helps us see how similar the texts are by looking at things like N-gram overlap. A higher Self-BLEU score means the texts are less diverse. For measuring diversity within a single text, we use Distinct-N (Li et al., 2016). This method checks how many unique N-grams are in the text compared to the total number of N-grams. It averages this out to give a final score. Following Welleck et al. (2019), we also count the total number of unique words a model produces across the whole test set. This gives us a quick way to see how varied the model's vocabulary is.",
        "formal_text": "Convert casual text to formal text: To measure how different texts are from each other, we use Self-BLEU (Zhu et al., 2018). This method looks at each generated text and calculates the BL"
    },
    {
        "casual_text": "Basically, let’s say I w i represents the spots in the text where the word w i shows up. Then, we can define it like this:",
        "formal_text": "Convert casual text to formal text: Basically, let’s say I w i represents the spots in the text where the word w i shows up. Then, we can define it like this: I w"
    },
    {
        "casual_text": "Coordination in sentences can pack a lot of info into fewer words, so a lot of text generation systems have tried to work with it, each with different levels of complexity. Back in the day, systems like EPICURE (Dale, 1992) used to create sentences with conjunctions as part of their overall strategy for optimizing at the discourse level. Nowadays, more modern systems deal with decisions about how to group things, including coordination and combining words or phrases, like turning propositions into adjectives, prepositional phrases, or relative clauses, all within a sentence planner (Scott and de Souza, 1990; Dalianis and Hovy, 1993; Huang and Fiedler, 1996; Callaway and Lester, 1997; Shaw, 1998). Some other systems have also used coordination, but their rules are pretty basic, only handling simple conjunctions within a single part of the sentence, like the subject, object, or predicate. CASPER, on the other hand, takes a more global approach. It tries to find the most concise way to coordinate across all the propositions, not just within one part of the sentence. Plus, it uses a simple trick to avoid making sentences too complicated or confusing when using coordination. CASPER also takes care of ellipsis (leaving words out) and coordination in prepositional clauses, which hadn’t really been addressed before. Another thing to think about is the order of propositions when you’re combining multiple ones. That’s a pretty interesting challenge.",
        "formal_text": "Convert casual text to formal text: Coordination in sentences can pack a lot of info into fewer words, so a lot of text generation systems have tried to work with it, each with different levels of complexity. Back in the"
    },
    {
        "casual_text": "Lately, pre-trained language models (PLMs) have made some pretty cool strides in a bunch of different tasks (shoutout to Devlin et al., 2019). Some newer studies show that these models can pick up knowledge from huge datasets all by themselves during pre-training and store that info in their parameters (Tenney et al., 2019; Petroni et al., 2019; Roberts et al., 2020). But here's the thing: because of the limited size of their vocabulary, PLMs struggle to pull out that factual knowledge, especially when it comes to rare or less common entities (Gao et al., 2019a; Wang et al., 2021a).",
        "formal_text": "Convert casual text to formal text: Lately, pre-trained language models (PLMs) have made some pretty cool strides in a bunch of different tasks (shoutout to Devlin et"
    },
    {
        "casual_text": "Next, let’s dive deeper into how changing the parameters might affect things. We’re curious to see what would happen if someone (like an adversary) picked different values for these parameters instead of sticking with the ones in Table 5. The possible values for k, p, and temperature are all laid out in Table 1. Just a quick reminder: you either use top-k or top-p sampling to decide how many words to consider when generating text. We’ve been using top-p sampling as our go-to method. When we tweak k or p, we keep the temperature at its default setting. And if we mess with the temperature, we leave p at its default value.",
        "formal_text": "Convert casual text to formal text: Next, let’s dive deeper into how changing the parameters might affect things. We’re curious to see what would happen if someone (like an adversary) picked different values for these parameters instead"
    },
    {
        "casual_text": "MORPA is a tool called a MORphological PArser, made for a Dutch text-to-speech system called SPRAAKMAKER (mentioned in a paper by van Leeuwen and te Lindeft in 1993). A key part of turning text into speech is figuring out the right way to say the words, which means creating a phonemic representation based on the written text. As you probably know, you can't just take the written words and turn them into sounds directly in Dutch because the letters don't always match up perfectly with the sounds. Plus, things like stress and most of the rules about how words sound aren't shown in the spelling. So, a text-to-speech system needs a smart way to change the written words into the right sounds. This work was done at the Phonetics Lab at Leiden University and got support from the Speech Technology Foundation, which is funded by the Netherlands Stimulation Project for Information Sciences, SPIN.",
        "formal_text": "Convert casual text to formal text: MORPA is a tool called a MORphological PArser, made for a Dutch text-to-speech system called SPRAAKMAKER (ment"
    },
    {
        "casual_text": "The Bush administration mentioned they'll attempt to bring back their plan once the House Energy and Commerce Committee starts working on a big clean-air bill.",
        "formal_text": "The Bush administration mentioned they'll attempt to bring back their plan once the House Energy and Commerce Committee starts a big clean-air bill. Convert casual text to formal text: The Bush administration mentioned they'll bring back their plan once"
    },
    {
        "casual_text": "Most clusters turned out to be pretty cohesive, but the biggest ones for VAI and VTA classes were kind of like a mix of everything. Even though the computer thought they had similar meanings, it wasn’t clear how the words in those clusters were related when you looked at them. Fixing these clusters took a lot of time and mostly involved using smaller, more organized clusters as a guide to sort the words from these messy ones. After cleaning things up, we ended up with a few more clusters in most cases. For VAIs, the number went up a lot, but for NDIs, it actually went down a little. Table 1 shows how many clusters we had before and after the cleanup.",
        "formal_text": "Convert casual text to formal text: Most clusters turned out to be pretty cohesive, but the biggest ones for VAI and VTA classes were kind of like a mix of everything. Even though the computer thought they had similar meanings"
    },
    {
        "casual_text": "Random walk inference on knowledge bases was first introduced by Lao and Cohen back in 2010. They later updated their work to include a large text corpus by turning the corpus into a graph and linking it to the knowledge base (Lao et al., 2012). Then, in 2013, Gardner and his team showed that using a latent embedding of relations instead of just the surface labels improved prediction performance. Makes sense, right? The feature space in PRA is huge, and surface relations are pretty rare. Take the relations \"[river] flows through [city]\" and \"[river] runs through [city]\"—they basically mean the same thing and should both be super helpful for predicting the RIVERFLOW-STHROUGHCITY relation. But if one shows up in the training data and the other only in the test data, neither will be much use. To fix this, Gardner et al. (2013) came up with a way to find a symbolic representation of these surface relations (like grouping them into clusters) and replaced the edge labels in the graph with these new representations. This made it more likely that the same surface relations would appear in both training and test data, which naturally boosted performance.",
        "formal_text": "Convert casual text to formal text: Random walk inference on knowledge bases was first introduced by Lao and Cohen back in 2010. They later updated their work to include a large text corpus by turning the corpus into a graph and"
    },
    {
        "casual_text": "The mapping rules work by using a method called transformation grammar to arrange the recognized parts and build an Arabic FS that matches the sentence's structure. They go through step by step, using the pattern in Table 1 to create the Arabic FS. The order of the feature-value pairs in the FS lines up with the sentence structure, which will then be used to generate the Arabic sentence.",
        "formal_text": "Convert casual text to formal text: The mapping rules work by using a method called transformation grammar to arrange the recognized parts and build an Arabic FS that matches the sentence's structure. They go through step by step, using the pattern"
    },
    {
        "casual_text": "Sure! Here's a more casual version: We’ve got a list of entity types and some context for you, along with a link to the page where you can find the relation definition and other related info. The annotators need to do the following tasks.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: We’ve got a list of entity types and some context for you, along with a link the page where you find the relation definition and"
    },
    {
        "casual_text": "Alright, in this part, we're looking at how exits learn from each other by running some small tests called pairwise mutual learning (PML). For these PML tests, we pick two exits (let's call them exit i and exit j), where exit i is closer to the surface than exit j (so, i  j). We're checking out a few different scenarios to see how things play out.",
        "formal_text": "Convert casual text to formal text: Alright, in this part, we're looking at how exits learn from each other by running some small tests called pairwise mutual learning (PML). For these PML tests, we pick"
    },
    {
        "casual_text": "In this paper, we worked on detecting off-topic responses. We started by creating a model with five main layers. Inside this model, we used a bi-attention mechanism and convolutions to identify important words in prompts and key phrases in responses. We also added a gated unit as a relevance layer to improve semantic matching and included residual connections in each layer. To better understand our model, we did some visualization analysis. Lastly, we came up with a new negative sampling method to boost our off-topic training data. We tested our approach and saw big improvements on both familiar and new test data.",
        "formal_text": "Convert casual text to formal text: In this paper, we worked on detecting off-topic responses. We started by creating a model with five main layers. Inside this model, we used a bi-attention mechanism and convolution"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. **Data Effect:** In the basic setup, we just use all the data we have to train the classifier, no questions asked. But we wanted to see if picking a smarter, language-based chunk of the data could make things better. So, we tried 10 different ways (10-fold cross-validation) to pick the best data set from seven options we talked about in Section 3.3. Turns out, if you’re training a classifier for Chinese-only question answering (QA), throwing in English or Arabic sentences is a bad move. It works better if you stick to Chinese sentences (lang=ch). For the other three tasks, the best data set to use is annot=en+consist. This choice worked well across all 10 folds, and the difference was pretty clear, except for Arabic-only. The second column in Table 2 shows the MAP (a measure of performance) when we used this data selection method before training the basic model. **Feature Effect:** Next, we wanted to check how our new features (LexCL and LexQL, explained in Section 3.2) impact things. So, we trained classifiers using either just LexCL, just LexQL, or both. The data we used here was the best subset we found earlier. The results are on the right side of Table 2. If you see single or double underlines, that means the improvement over the basic model or the model with data selection is statistically significant.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. **Data Effect:** In the basic setup, we just use all the data we have to train the classifier, no questions"
    },
    {
        "casual_text": "So, there are a few settings we need to adjust. The goal is, on one hand, to make sure we’re getting the most out of using more components in the IBM2 mixture model, but on the other hand, we don’t want the process to take forever. To balance accuracy and speed, we set up these parameters:",
        "formal_text": "Convert casual text to formal text: So, there are a few settings we need to adjust. The goal is, on one hand, to make sure we’re getting the most out of using more components in the IBM2 mixture model"
    },
    {
        "casual_text": "Lately, a new straightforward architecture called the TRANSFORMER (introduced by Vaswani et al. in 2017) has been getting a lot of buzz in the machine translation world. Unlike older models that relied on complicated recurrent or convolutional neural networks, the TRANSFORMER uses self-attention networks for both its encoder and decoder. This setup helps it capture relationships between the input and output data more effectively. By running things in parallel (using multihead attention) and stacking multiple layers, the TRANSFORMER has hit the top of the charts for performance on a bunch of translation tasks (Shaw et al., 2018; Hassan et al., 2018).",
        "formal_text": "Convert casual text to formal text: Lately, a new straightforward architecture called the TRANSFORMER (introduced by Vaswani et al. in 2017) has been getting a lot of buzz in the machine"
    },
    {
        "casual_text": "• Figure out how far apart the new document is from all the ones you've already looked at.",
        "formal_text": "Convert casual text to formal text: • Figure out how far apart the new document is all the ones you've already looked at. • Figure out how far apart the new document is all the ones you've already looked at. Con"
    },
    {
        "casual_text": "First off, it’s pretty obvious that, hey, we can and often do understand descriptions of things (like scientific theories or political ideas) even if we know or think they don’t match the facts. This makes it seem like the info we take in stays separate from what we already know until we decide to believe it’s true. Now, while our thinking patterns could be adjusted to handle this by tagging info with its source (like where we got it from), that’s kind of impractical for how most of us use language—we don’t usually know the exact source of every fact we believe.",
        "formal_text": "Convert casual text to formal text: First off, it’s pretty obvious that, hey, we can and often do understand descriptions of things (like scientific theories or political ideas) even if we know or think they don’t"
    },
    {
        "casual_text": "WordNet-based models. WordNet-based models are way better than distributional ones, and there are a few reasons why. First off, distributional models only get their info from the context in the text. Since we’re not using a FrameNet-annotated corpus, there’s no guarantee that how a LU (lexical unit) is used in the text matches its meaning in FrameNet. In cases where LUs have multiple meanings (polysemy), the text might be talking about senses that aren’t even in FrameNet. In our study, we’re not dealing with polysemy—it’s a super tricky problem in semantic spaces (check out Schütze, 1998 for more on that), and figuring out how to cluster different word senses separately is a whole other challenge. We’ll tackle that in future work. On the other hand, the WordNet-based model doesn’t struggle with polysemy as much because WordNet clearly separates and lists all the senses, even the ones tied to the FrameNet gold standard.",
        "formal_text": "Convert casual text to formal text: WordNet-based models. WordNet-based models are way better than distributional ones, and there are a few reasons why. First off, distributional models only get their info from the context"
    },
    {
        "casual_text": "Here's how we set up the adversarial training. We have a discriminator D that's trained to minimize the cross-entropy loss, which is defined in equation 2. In this case, l represents the domain category, and h is the hidden representation of both the question and the passage, which is a vector in Rd. For our experiment, we use the [CLS] token representation from BERT as h.",
        "formal_text": "Convert casual text to formal text: Here's how we set up the adversarial training. We have a discriminator D that's trained to minimize the cross-entropy loss, which is defined in equation 2. In"
    },
    {
        "casual_text": "The key to relation-driven skimming is to attach things correctly without getting bogged down in too much grammar. It’s not easy, though, because even phrases that don’t seem important can mess up the ones that do. In the next parts, we’ll break down each component, explain why it works, what it does, and show you a couple of examples to help you understand.",
        "formal_text": "Convert casual text to formal text: The key to relation-driven skimming is to attach things correctly without getting bogged down in too much grammar. It’s not easy, though, because even phrases that don’t seem important"
    },
    {
        "casual_text": "Basically, not much at all, and nothing so far when we're talking about controlled experiments. Super nested sentences just don’t show up in regular conversation and hardly ever in writing, except for those logical or math formulas. To understand their structure, you kinda need to do extra stuff, like reading them a few times and using special marks to pair together parts that belong with each other but have other stuff stuck in between. Like, take a formula—",
        "formal_text": "Convert casual text to formal text: Basically, not much at all, and nothing so far when we're talking about controlled experiments. Super nested sentences just don’t show up in regular conversation and hardly ever in writing"
    },
    {
        "casual_text": "On an IBM 370 system, they can process and create index records for scholarly paper titles at a rate of about 70,000 titles per hour.",
        "formal_text": "Convert casual text to formal text: On an IBM 370 system, they can process and create index records for scholarly paper titles at a rate about 70,000 titles per hour. Convert casual text to formal text: On an IBM"
    },
    {
        "casual_text": "A lot of approaches use something called a deep mixture of experts (MoE) (Jacobs et al., 1991; Eigen et al., 2014) to mix up the decoding process and make it more varied. For example, Yang et al. (2018) added a soft mixture of softmax on top of the output layer of an RNN language model. Shen et al. (2019) took it a step further by using a mixture of decoders with equal mixing weights to boost diversity in machine translation. The method closest to ours is the mixture decoder by Shen et al. (2019), which also uses hard-EM for training. This involves assigning the best-performing predictor to each data point, a technique also known as multiple choice learning (Guzman-Rivera et al., 2012; Lee et al., 2016). While Shen et al. (2019) turned the RNN decoder into an MoE, we did something different: we made the SELECTOR the MoE to diversify content selection, while keeping the encoder-decoder models focused on one-to-one generation. Our experiments show that our approach not only balances accuracy and diversity better but also cuts down on training time significantly.",
        "formal_text": "Convert casual text to formal text: A lot of approaches use something called a deep mixture of experts (MoE) (Jacobs et al., 1991; Eigen et al., 2014)"
    },
    {
        "casual_text": "Our A* parser keeps track of a list of parse items, which are part of the projective parser. We start by filling this list with items created by the Init rule. Then, we go through the list step by step. At each step, we grab the first item, I, from the list and try applying the Skip-L and Skip-R rules to it. We also look at all the items we’ve found so far, which are stored in something called a parse chart, and see if we can combine I with any of them using the Arc-L and Arc-R rules. Any new items we create this way get added to both the list and the chart. The parsing process stops either when we pull a goal item ([0, n + 1], r, [ ]) from the list or when the list runs out of items without finding the goal.",
        "formal_text": "Convert casual text to formal text: Our A* parser keeps track of a list of parse items, which are part of the projective parser. We start by filling this list with items created by the"
    },
    {
        "casual_text": "Basically, our extra data and training methods worked well for improving how we handle split-antecedent anaphora resolution. We'll dive deeper into this in the upcoming sections.",
        "formal_text": "Convert casual text to formal text: Basically, our extra data and training methods worked well for improving how we handle split-antecedent anaphora resolution. We'll dive deeper this in the upcoming sections."
    },
    {
        "casual_text": "We use a sequence-to-sequence BiGRU model (like the one Bahdanau et al. came up with in 2015) to handle the QP prediction. This type of model is pretty popular in neural machine translation. The encoder takes in a sequence of words from the input passage, which we can write as S = (x1, ..., x|S|), and the decoder then predicts the sequence of words for the output question pattern, Qp = (y1, ..., y|Qt|). The probability of generating that specific question pattern Qp is calculated based on this setup.",
        "formal_text": "Convert casual text to formal text: We use a sequence-to-sequence BiGRU model (like the one Bahdanau et al. came up with in 2015) to handle the QP prediction"
    },
    {
        "casual_text": "TEXAN is a system for analyzing and transferring text, focusing on how it's structured and communicated. It’s built around the idea that texts are the result of specific linguistic actions, based on speech act theory. Basically, it thinks of texts as actions that guide how words and phrases are chosen. To make this work in a computer model, TEXAN uses something called a context-free illocution grammar. This grammar handles categories of actions and the logical structure of situations, connecting them to the actual words and phrases in a language. It ties all this together with a special text-related lexicon.",
        "formal_text": "Convert casual text to formal text: TEXAN is a system for analyzing and transferring text, focusing on how it's structured and communicated. It’s built around the idea that texts are the result of"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We need to get the output distributions, which we'll call P dp and P kb, from two teachers named TDP and TKB. Got it?",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down a simpler way. We need to get the output distributions, which we'll call P dp and P kb, from two teachers"
    },
    {
        "casual_text": "We still have a lot of work to do in this area. This paper isn't about coming up with a brand-new way to evaluate part-of-speech tagging. Instead, it’s pointing out that there are some problems—like the noise in the test corpus—that haven’t gotten much attention but are actually really important.",
        "formal_text": "Convert casual text to formal text: We still have a lot of work to do in this area. This paper isn't about coming up with a brand-new way to evaluate part-of-speech t"
    },
    {
        "casual_text": "Our main aim is to figure out if non-Wikipedia noun phrases can be considered entities and then assign them semantic types to make them more useful. To get started, we're using a dataset of 15 million \"(noun phrase subject, textual relation, noun phrase object)\" statements pulled from the web by RE-VERB (Fader et al., 2011). RE-VERB already takes care of filtering out stuff like relative pronouns, WHO-adverbs, and existential \"there\" noun phrases that don't really add much meaning. Next, we use standard entity linking methods—like string matching, prominence priors (Fader et al., 2009), and context matching (Bunescu and Paşca, 2006)—to connect the noun phrase subjects to Wikipedia.",
        "formal_text": "Convert casual text to formal text: Our main aim is to figure out if non-Wikipedia noun phrases can be considered entities and then assign them semantic types to make them more useful. To get started, we're using"
    },
    {
        "casual_text": "Adjuncts are placed in the target using a similar approach where Substitution Variables for adjuncts are added to the skeleton, just like in (2).",
        "formal_text": "Convert casual text to formal text: Adjuncts are placed in the target using a similar approach where Substitution Variables for adjuncts are added to the skeleton, just in (2). Convert casual text to"
    },
    {
        "casual_text": "We tested our Chinese zero pronoun resolution model on the OntoNotes-5.0 development dataset using two setups: one without a pre-trained Chinese semantic dependency graph parser (our Baseline Model) and another with it (our Semantic-aware Model). The results show that our Semantic-aware Model improved the F-score by 0.4% compared to the baseline. It also outperformed other deep learning models out there. Looking at the results across different fields, our Semantic-aware Model scored the highest in MZ, BC, and WB. The biggest improvement was in the BC field. But in NW, BN, and TC, it didn’t do better than other models. One reason might be that the semantic dependency graph parser didn’t work as well in those fields, so it couldn’t give useful semantic info for resolving zero pronouns.",
        "formal_text": "Convert casual text to formal text: We tested our Chinese zero pronoun resolution model on the OntoNotes-5.0 development dataset using two setups: one without a pre-trained Chinese semantic dependency graph parser"
    },
    {
        "casual_text": "To get the most variety, we pick one focus from each SELECTOR expert to kind of mimic p  (m|x). If we want a fixed behavior, we use a set value (th) instead of randomly picking from a Bernoulli distribution. This gives us a bunch of focus points m 1. . . m K from K different experts. Each focus m z = (m z 1. . . m z S ) gets turned into embeddings and then gets stuck together with the input embeddings from the source sequence x = (x 1. . . x S ). Any standard generation function, like an encoder-decoder, can be used to model p(y|m, x), as long as it can handle a bunch of input embeddings. We use the same generation function but with K different focus samples to create K unique and diverse outputs. Data:",
        "formal_text": "Convert casual text to formal text: To get the most variety, we pick one focus from each SELECTOR expert to kind of mimic p  (m|x). If we want a fixed behavior, we use"
    },
    {
        "casual_text": "Natural Language Inference (NLI) is all about figuring out if a statement (the premise) means something else (the hypothesis) is true. When it comes to NLI and time-related stuff, it's super important. (1) is an example of NLI in English that deals with time expressions.",
        "formal_text": "Convert casual text to formal text: Natural Language Inference (NLI) is all about figuring out if a statement (the premise) means something else (the hypothesis is true. When it comes to NLI and time"
    },
    {
        "casual_text": "Here’s a simpler way to say that: We follow these six steps for each parent node in the source-side parse tree:",
        "formal_text": "Convert casual text to formal text: Here’s a simpler way to say that: Here’s a simpler way to say that: We follow these six steps for each parent node in the source-side parse tree"
    },
    {
        "casual_text": "SR might tweak the slot values in the translated sentences, so we use fuzzy value detection to deal with issues like similar sounds or pronunciation problems when it's pulling out slot values to get a semantic label, y. But instead of fixing the noisy value with the original one, we actually let the misrecognition happen because we're okay with that. So, y = y is totally fine. Also, we normalize numerical terms to handle spoken numbers better. Most slot values can be sorted out by our automatic detection rules. The ones that are too different to recognize just get tossed out along with their labels.",
        "formal_text": "Convert casual text to formal text: SR might tweak the slot values in the translated sentences, so we use fuzzy value detection to deal with issues like similar sounds or pronunciation problems when it's pulling out slot values to get a semantic"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. When we look at different types of repairs—like exact repeats, replacements, and deletions—we can see from Table 5 that our joint models perform better than just focusing on disfluency detection for repeats and replacements. However, when it comes to detecting deletions, the separate disfluency detection does a better job. It looks like adding other tasks to the joint models doesn't really help with spotting rare deletions, but it does improve how well they handle replacements.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. When we look at different types of repairs—like exact repeats, replacements, and deletions—we can see from Table"
    },
    {
        "casual_text": "Even though CCM and DIORA have the best results overall, it’s worth mentioning that they both rely on extra stuff to work well. CCM uses gold POS tags, and DIORA uses pretrained word embeddings. From our early tests on PTB, we noticed a big drop in performance when we ran CCM without those gold POS tags. Specifically, the Evalb F1 score went from 70.14 down to 57.29 when we tested on sentences with lengths  10 in the \"ptb len10 nopunct\" setup. Similarly, DIORA doesn’t do as well when you swap out the pretrained word embeddings for randomly initialized ones. The average Evalb F1 score drops from 49.39 to 42.63 when we evaluate it on all sentences in the \"ptb len40 nopunct\" setting.",
        "formal_text": "Convert casual text to formal text: Even though CCM and DIORA have the best results overall, it’s worth mentioning that they both rely on extra stuff to work well. CCM uses gold POS tags, and"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: \"VC\" means we're dealing with a vowel and a consonant. \"2st>\" is just a placeholder for something else. \"/. . . . . .\" means there's a pause or a break here. \"-st: >\" is a bit tricky, but it seems like it's referring to a change or something being removed. \"tORSO _ CONSO\" is like saying \"torso\" (a word) and then \"consonant,\" so maybe it's about a word ending with a consonant sound. \"+voc, -st>\" means adding a vowel sound and removing a consonant sound. \"CONSO\" is just short for \"consonant\" again. \"# #\" looks like a placeholder or a marker for something specific. So, in short, this is about how vowels and consonants interact, with some changes and markers along the way.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: \"VC\" means we're dealing with a vowel and a consonant. \"2st"
    },
    {
        "casual_text": "The \"specialize output tag\" operation works kind of like the \"new output tag\" operation, but instead of just swapping the output tag for a new one, we split it up. When the model gets trained, it will include features for both the split tag and the original, untouched tag.",
        "formal_text": "Convert casual text to formal text: The \"specialize output tag\" operation works kind like the \"new output tag\" operation, but instead just swapping the output tag for a new one, we split it up. When the model"
    },
    {
        "casual_text": "• Evaluation. Some studies give results using the average and standard deviation from different random seeds (Chiu and Nichols, 2016; Peters et al., 2017; Liu et al., 2018). Others just share the best result they got after trying different setups (Ma and Hovy, 2016), which makes it hard to compare them directly.",
        "formal_text": "Convert casual text to formal text: • Evaluation. Some studies give results using the average and standard deviation from different random seeds (Chiu and Nichols, 2016; Peters et al., 2017; Liu"
    },
    {
        "casual_text": "Lately, Multi-Task Learning (MTL) has been doing pretty well in a bunch of NLP tasks. The main idea is to use helpful information from multiple related tasks to boost the performance of all those tasks. For example, you might predict the chance of a sequence happening and also check if it has names (Cheng et al., 2015), or guess the frequency of the next word along with its part-of-speech (POS) (Plank et al., 2016). Some other tasks include predicting surrounding words and error detection with extra linguistic info (Rei, 2017; Rei and Yannakoudakis, 2017). More recently, Augenstein and Sgaard (2017) took it up a notch by using MTL for classifying keyphrase boundaries while also doing semantic super-sense tagging and spotting multi-word expressions. Sanh et al. (2018) came up with a hierarchical model that handles simpler tasks at the bottom layers and more complex ones at the top. There aren’t many studies yet that use multi-task learning specifically for AES (Cummins et al., 2016; Cummins and Rei, 2018).",
        "formal_text": "Convert casual text to formal text: Lately, Multi-Task Learning (MTL) has been doing pretty well in a bunch of NLP tasks. The main idea is to use helpful information from multiple related tasks to"
    },
    {
        "casual_text": "Alright, so following the steps in Section 3, we start by running EDITS and VENSES on the [T, H] RT E5-sample and [T, H] RT E5-mono. The accuracies for these runs are listed in Table 3.",
        "formal_text": "Convert casual text to formal text: Alright, so following the steps in Section 3, we start by running EDITS and VENSES on the [T, H] RT E5-sample and [T, H"
    },
    {
        "casual_text": "Got it! Just so you know, there can be more than one connection between two nouns. So, a single dependency path might actually represent multiple relationships.",
        "formal_text": "Convert casual text to formal text: Got it! Just so you know, there can be more than one connection between two nouns. So, a single dependency path might actually represent multiple relationships."
    },
    {
        "casual_text": "Since A( G,  G ) is empty and W c  O c is greater than or equal to zero, everything balances out. The transition CHOOSE( G, G) leads to a new setup, c 1, where A c 1 (i) is also empty, just like A( G,  G ). This means we can do a POP action, which takes us back to the original configuration, c. And since we haven’t added any edges, we’re good.",
        "formal_text": "Convert casual text to formal text: Since A( G,  G ) is empty and W c  O c is greater than or equal to zero, everything balances out. The transition CHOOSE("
    },
    {
        "casual_text": "For this evaluation, we're using four methods from O'Keefe et al. (2012). The first one is a basic rule-based approach (Rule) that picks the entity closest to the speech verb near the quotation. If there’s no speech verb, it just goes with the entity closest to the end of the quote. The second method uses a CRF, which can pick from up to 15 entities in the paragraph with the quote or any paragraph before it. The third method (No seq.) is a binary MaxEnt classifier that decides if each entity is the speaker or not, based on the highest speaker probability. In O'Keefe et al. (2012), this model did the best on direct quotes in SMHC, even though it didn’t use sequence features or decoding methods that other models had. The last method we’re looking at (Gold) uses sequence features with gold-standard labels from earlier decisions. As O'Keefe et al. pointed out, this method isn’t practical in real life, but we’re including it to see how their claims hold up when we look at direct, indirect, and mixed quotes. To keep things consistent, we’re using the same list of speech verbs from Elson and McKeown (2010) that O'Keefe et al. used in 2012. Table 7 shows how accurate the two methods are on both PARC and SMHC, broken down by the type of quote.",
        "formal_text": "Convert casual text to formal text: For this evaluation, we're using four methods from O'Keefe et al. (2012). The first one is a basic rule-based approach (Rule) that"
    },
    {
        "casual_text": "Alright, so here's the deal: we've figured out how to take sentence-level info and mash it together to create paragraph-level stuff. It works pretty well and is super easy to do. Now, let's look at some examples. Qanta, our system, did really well on the first three questions, getting them all right, which no other system could do. But the last two questions were tough cookies. They needed extra knowledge (like stuff about Freudianism) and some tricky thinking about time, which our models couldn't handle. Here's a quick rundown of the examples: 1. **Question:** Who represented the Amistad slaves and negotiated the Treaty of Ghent and the annexation of Florida from Spain? **Answers:** John Quincy Adams, Henry Clay, Andrew Jackson **Correct:** John Quincy Adams (blue) 2. **Question:** This work talks about people who knelt in hopeless cathedrals and jumped off the Brooklyn Bridge. **Answers:** *Howl*, *The Tempest*, *Paradise Lost* **Correct:** *Howl* (blue) 3. **Question:** Despite 26 martyrs being crucified here in the late 16th century, it remained the center of Christianity in its country. **Answers:** Nagasaki, Guadalcanal, Ethiopia **Correct:** Nagasaki (blue) 4. **Question:** This novel makes fun of Freudianism in a chapter about the protagonist dreaming of holding a live fish. **Answers:** *Billy Budd*, *The Ambassadors*, *All My Sons* **Correct:** *Billy Budd* (blue) 5. **Question:** A contemporary of Elizabeth I, he came to power two years before her and died two years later. **Answers:** Grover Cleveland, Benjamin Harrison, Henry Cabot Lodge **Correct:** Grover Cleveland (blue) So yeah, Qanta nailed the first three but stumbled on the last two. That's because",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal: we've figured out how to take sentence-level info and mash it together to create paragraph-level stuff. It works pretty well and"
    },
    {
        "casual_text": "No big surprise here, really. There's a bit of a balancing act going on. On one hand, we want super accurate article extraction, but on the other, focusing too much on that leaves less room for charge classification. The good news is, our model is doing a great job with article extraction, which means it’s also helping with charge prediction by giving us solid legal ground to stand on. Check out Table 3 for the performance stats (micro view) on News.",
        "formal_text": "Convert casual text to formal text: No big surprise here, really. There's a bit of a balancing act going on. On one hand, we want super accurate article extraction, but on the other, focusing"
    },
    {
        "casual_text": "People have come up with graph-based ranking methods to rank sentences or passages by looking at how they \"vote\" or \"recommend\" each other. Tex-tRank (Mihalcea and Tarau, 2005) and LexPageRank (Erkan and Radev, 2004) use algorithms similar to PageRank and HITS to figure out which sentences are important. Wan et al. (2007b) made improvements by distinguishing between links within a document and links between different documents. They also came up with a manifold-ranking method to use relationships between sentences and topics (Wan et al., 2007a). ETTS seems to be connected to a recent task called \"update summarization,\" which started in DUC 2007 and continued with TAC. But update summarization only handles one update at a time, and we’re doing something new by dealing with multi-step evolutionary updates. There’s also related work like timeline systems proposed by Swan and Allan (2000) using named entities, Allan et al. (2001) focusing on usefulness and novelty, and Chieu and Lee (2004) looking at interest and burstiness. We’ve already proposed a timeline algorithm called \"Evolutionary Timeline Summarization (ETS)\" in Yan et al. (2011b), but the refining process based on generated summaries takes a lot of time. Our goal is to find a more efficient way to summarize things.",
        "formal_text": "Convert casual text to formal text: People have come up with graph-based ranking methods to rank sentences or passages by looking at how they \"vote\" or \"recommend\" each other. Tex-tRank"
    },
    {
        "casual_text": "The first transition system picks the graph constant for a token right away and then selects outgoing APP edges that match the lexical type. But, obviously, the choices for lexical type and outgoing edges are connected. So, we also look at a different transition system where the lexical type is chosen after figuring out the outgoing edges.",
        "formal_text": "Convert casual text to formal text: The first transition system picks the graph constant for a token right away and then selects outgoing APP edges that match the lexical type. But, obviously, the choices for lexic"
    },
    {
        "casual_text": "We’ve got RuSentiment, a dataset based on public posts from VKontakte (VK), which is the biggest Russian social network with around 100 million active users every month. RuSentiment was created using some fresh, detailed guidelines that made the annotation process quick and easy while still covering a wide variety of sentiments, both obvious and subtle. The agreement between the people doing the annotations, measured by Fleiss' kappa, is 0.58. Overall, 31,185 posts were labeled, with 21,268 picked randomly (including 2,967 for the test set). Another 6,950 posts were chosen using a strategy kind of like active learning to make sure the data was diverse. This makes RuSentiment the biggest publicly available sentiment dataset for social media, and also the largest general-purpose sentiment dataset for Russian, which isn’t as widely studied as other languages.",
        "formal_text": "Convert casual text to formal text: We’ve got RuSentiment, a dataset based on public posts from VKontakte (VK), which is the biggest Russian social network with around 100 million active users"
    },
    {
        "casual_text": "Neural networks have gotten really good at handling two tasks at once: predicting charges and picking out the related legal articles. We’ve built a system that ties these two things together, using something called a two-stack attention mechanism to make sense of how the details of a case connect to the laws and charges. Here’s how it works: we use a couple of fancy tools—sentence-level and document-level Bi-GRUs (kind of like advanced Recurrent Neural Networks)—along with some attention components to figure out how words and sentences fit together. This helps us understand the big picture of the case and the important details. Once we’ve analyzed the case description, we use another set of attention components to focus on the most relevant legal articles that back up our charge prediction. We’re dealing with this in a multi-label way, meaning a case can have more than one charge. We tested this model by predicting charges for criminal cases in China. We grabbed publicly available court documents from the Chinese government website, where we could automatically pull out the case details, related laws, and charges using basic rules. Check out Figure 1 for how this works. The results? Our neural network method does a solid job of predicting the right charges for a case and also points out the legal articles that support those predictions.",
        "formal_text": "Convert casual text to formal text: Neural networks have gotten really good at handling two tasks at once: predicting charges and picking out the related legal articles. We’ve built a system that ties these two things together,"
    },
    {
        "casual_text": "We set up Direct Assessment (DA) HITs on Mechanical Turk, just like in WMT human evaluations, but with a twist: the segments were evaluated in the order they appeared in the document. We call this setup \"segment rating + document context\" (SR+DC). On top of that, we also had workers rate entire documents, which gave us a second set of results for comparison. We refer to this as \"document rating + document context\" (DR+DC). As usual in DA evaluations, translations were rated on a scale from 0 to 100, and we made sure to include quality control measures.",
        "formal_text": "Convert casual text to formal text: We set up Direct Assessment (DA) HITs on Mechanical Turk, just like in WMT human evaluations, but with a twist: the segments were evaluated in the order they appeared in the"
    },
    {
        "casual_text": "In this part, we're checking out how well our new method for creating the MEGA-DT discourse treebank works. We're testing a discourse parser trained on MEGA-DT and comparing its performance to our earlier \"silver-standard\" treebank (from Huber and Carenini, 2019) and two other popular, human-made discourse datasets.",
        "formal_text": "Convert casual text to formal text: In this part, we're checking out how well our new method for creating the MEGA-DT discourse treebank works. We're testing a discourse parser trained on MEGA-"
    },
    {
        "casual_text": "It calculates the distance between the closest points that are the furthest apart. Then, the Gromov-Hausdorff distance tries to make this distance as small as possible by considering all possible ways to match up X and Y without stretching or distorting them.",
        "formal_text": "Convert casual text to formal text: It calculates the distance between the closest points that are the furthest apart. Then, the Gromov-Hausdorff distance tries to make this distance as small possible by considering all"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. We started with a bunch of answers from different language models and combined them into one big list of answers in various languages. For the weighted method, we tested three different weight values. In Table 3, it shows that training separate classifiers for each smaller task doesn’t really help improve the main task overall. When it comes to combining strategies, the weighted method (where weights are adjusted by testing on other queries) and the \"English first\" approach worked best. But both of these are pretty much the same as just using a single classifier, so they don’t give us a big advantage. The \"English first\" method gives us the most English answers (88%), which might not be ideal depending on what we’re trying to do. If you want a better mix of languages, you can tweak the merging method. For example, the \"alternate\" and \"norm\" methods tend to balance the languages more evenly. Even though these methods give us a slightly lower score overall, Table 2 shows that customizing classifiers for each smaller task (like the Chinese responses in L2CT) can make the answers more relevant. The big question still is: how do we best combine all these results into a mixed-language list? That’s still something we’re figuring out.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. We started with a bunch of answers from different language models and combined them into one big list of answers in various languages. For"
    },
    {
        "casual_text": "We add a placeholder hypothesis with an infinite loss to each N-best list. This makes sure that n doesn't go out of bounds at line 7.",
        "formal_text": "Convert casual text to formal text: We add a placeholder hypothesis with an infinite loss to each N-best list. This makes sure that n doesn't go out of bounds at line 7. This makes sure that n doesn"
    },
    {
        "casual_text": "Figuring out how words or chunks of words (like n-grams) match up between different versions of the same text is a pretty common challenge. People have come up with lots of ways to tackle this problem, and most of these methods can be grouped into two main types. First, there's the probabilistic approach, which was introduced by Brown and his colleagues back in 1988. This method focuses on finding connections between words or groups of words in parallel sentences. It works by creating a probabilistic model based on the parallel text, and then using a big optimization process to estimate the best possible links between all the words in the source and target sentences. The goal here is to figure out the most accurate set of alignment links for every pair of sentences. The IBM models, which were developed by Brown and his team in 1993, are probably the most well-known example of this approach. These models have been tweaked and improved a lot over the years—you can check out work by Vogel (1996), Wu (1997), Deng and Byrne (2005), Liang and his team (2006), Fraser and Marcu (2007), and Ganchev and his colleagues (2008) for some of the updates. The second category deals with aligning phrases, which is way trickier than just aligning single words. According to research by Marcu and Wong (2002) and Vogel (2005), phrase alignments are usually created by combining \"oriented\" 1-n word alignments in both directions. This approach has been used by Koehn and his team (2003) and DeNero and Klein (2007), among others.",
        "formal_text": "Convert casual text to formal text: Figuring out how words or chunks of words (like n-grams) match up between different versions of the same text is a pretty common challenge. People have come up with lots of"
    },
    {
        "casual_text": "1. Basically, some cases have stricter rules about what words can go with them than others. For instance, in the accusative case, \"to subscribe\" has a tighter set of rules compared to \"to take/steal,\" which can work with a wider range of objects.",
        "formal_text": "Convert casual text to formal text: 1. Basically, some cases have stricter rules about what words can go with them than others. For instance, in the accusative case, \"to subscribe\" has a tighter set of rules"
    },
    {
        "casual_text": "There have been a bunch of question answering datasets created over the years (like the ones by Berant et al. in 2013, Joshi et al. in 2017, Trischler et al. in 2017, and Rajpurkar et al. in 2018, among others). But all of these were focused on answering single questions at a time. Saha et al. (2018) took a different approach and looked into answering a series of questions in a row, creating a dataset for that purpose. However, we’re doing things a bit differently from them in two main ways: 1) They worked with question answering based on structured knowledge bases. 2) Their dataset was kind of artificial: they had human annotators come up with templates using knowledge base predicates, and then they grouped individual questions together by predicate or subject to create sequences.",
        "formal_text": "Convert casual text to formal text: There have been a bunch of question answering datasets created over the years (like the ones by Berant et al. in 2013, Joshi et al. in 2017, Trischler"
    },
    {
        "casual_text": "The tracking stops when j moves to j + 1 because we've run out of transitions from the set I 0, which ends up being empty. For homotopy tracking with a uniform prior, it uses O(n) memory and takes O(n log(n)) operations. Plus, it's super easy to implement.",
        "formal_text": "Convert casual text to formal text: The tracking stops when j moves to j + 1 because we've run out of transitions from the set I 0, which ends up being empty. For homotopy tracking with a uniform"
    },
    {
        "casual_text": "We're comparing our method to a few other approaches: TMN stands for E2E Transformer MemNet, which was introduced by Dinan et al. in 2019. TMN uses a Transformer memory network for picking the right knowledge and a Transformer decoder with a copy mechanism to predict what to say next. The knowledge selection part is trained using just the knowledge label, not a posterior distribution. Then there's TMN BERT, which is short for TMN+BERT, created by Kim et al. in 2020. TMN BERT swaps out the Transformer memory network for a pre-trained BERT model.",
        "formal_text": "Convert casual text to formal text: We're comparing our method to a few other approaches: TMN stands for E2E Transformer MemNet, which was introduced by Dinan et al. in 2019."
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. We’ve got some results from looking at NER (Named Entity Recognition) and POS (Part-of-Speech) stuff for a bunch of languages that are pretty different from each other. In Figure 2, you can see that for languages that don’t use the Latin script (so, not like English), even though they have very little in common with English in terms of words (that’s what “lexical overlap” means) and they’re super different in how they work (check out Appendix D, Tables 9 and 14 for more on that), they actually get way bigger improvements in performance compared to languages that do use the Latin script. For example, for NER, the average gain is 6.2 for non-Latin script languages, but only 3.0 for Latin script ones. And for POS, it’s 11.4 vs. 5.4 when K = 1. This shows there’s a big gap in how much English data helps these different groups of languages. English data doesn’t help much for these non-Latin script languages, which is why their ZS-XLT scores are low. But when they get even a little bit of their own language data, their performance shoots up, which really shows how well FS-XLT works. Table 3 also shows that, besides the script type, how much the languages share words and how similar their grammar and stuff is (that’s the “lexical overlap” and “number of linguistic features”) also matters.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. We’ve got some results from looking at NER (Named Entity Recognition) and POS (Part-of-"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: The SpeechT5 model is pretty impressive. It does way better than other models like wav2vec 2.0 BASE and HuBERT BASE, as well as some strong baseline models we tested. This shows that the way we pre-trained SpeechT5 is really effective. Plus, when we add a language model (LM) during decoding, SpeechT5 gets even better results, with lower WERs (error rates) than wav2vec 2.0 BASE across all the test sets. It's basically the best-performing model right now. Oh, and we did some experiments with 960 hours of fine-tuning, but we’ll share those results in Appendix C because there’s not enough space here.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: The SpeechT5 model is pretty impressive. It does way better than other models like wav2vec 2.0 BASE and Hu"
    },
    {
        "casual_text": "In this setup, a pre-trained language model (like BERT or RoBERTa, which was introduced by Liu et al. in 2019) is tweaked to spot text that was created by itself or similar models. Unlike the zero-shot classification method, this detector needs some supervised examples to train on. The GROVER detector, proposed by Zellers et al. in 2019, uses a linear classifier on top of the GROVER model. It works better than other detectors like fastText (from Bojanowski et al. in 2017) and BERT (by Devlin et al. in 2019). This shows that the models best at generating fake info are also the best at recognizing their own creations. This finding suggests that models like GROVER and GPT-2 should be made available to the public. However, the authors didn’t test the BERT model to see if it also excels at detecting its own text, even though the BERT detector and generator share similar traits. Uchendu et al. (2020) found that the standard GROVER detector isn’t great at identifying text from other models like TGMs, not just the original GROVER. The RoBERTa detector, experimented with by Solaiman et al. in 2019, was fine-tuned for detection tasks and currently holds the top spot, accurately identifying web pages generated by the largest GPT-2 model about 95% of the time.",
        "formal_text": "Convert casual text to formal text: In this setup, a pre-trained language model (like BERT or RoBERTa, which was introduced by Liu et al. in 2019) is tweaked to"
    },
    {
        "casual_text": "Comparing our model directly to ccMix is a bit tricky since ccMix includes a model for background words, while ours assumes stop words have already been removed during preprocessing. To make them fully comparable, we set the parameter _B (which controls the chance of a word coming from the background) to 0 and gave both models the same input. We also set the parameter _C, similar to P(x = 0), to 0.6, which is the average value ccLDA learned from this data and seems pretty reasonable. Using the implementation provided by the ccMix authors, we ran the EM procedure 20 times and saved the model with the best log-likelihood.",
        "formal_text": "Convert casual text to formal text: Comparing our model directly to ccMix is a bit tricky since ccMix includes a model for background words, while ours assumes stop words have already been removed"
    },
    {
        "casual_text": "When it comes to figuring out what people are doing in a conversation—like asking questions, giving orders, or just chatting—people have come up with different ways to analyze it. For instance, Samuel and his team (1998) looked at how people talk, like the direction of the conversation, punctuation, special phrases, and n-grams to classify what’s being said. They also used things like tone, word choice, and sentence structure to help with this. More recently, Julia and Iftekharuddin (2008) and Sridhar and others (2009) did a great job using sound and tone features to classify dialogue acts. On the other hand, Louwerse and Crossley (2006) used n-grams—which can work for both spoken and written dialogue—and tested them with a dataset called the Map Task Corpus (from Anderson and others, 1991). Bangalore and his team (2006) took this a step further by using n-grams from the last 1-3 things said to figure out what the current part of the conversation is about.",
        "formal_text": "Convert casual text to formal text: When it comes to figuring out what people are doing in a conversation—like asking questions, giving orders, or just chatting—people have come up with different ways to analyze it. For instance"
    },
    {
        "casual_text": "Baseline models: We're looking at a few different models here. First up, there's the CAPT model, which uses soft attention on a pair of input images. This attention thing is kind of like what's been done in image captioning before (like in Xu et al., 2015), but instead of just one image, we're dealing with two. In the CAPT model, we don't mess with any masking—we just skip the cluster info. The goal is to create a single sentence, so it's like a regular captioning model but with attention on two images. Next, there's the CAPT-MASK model, which is pretty much the same as CAPT, but it adds a masking mechanism. This mask is made by combining all the cluster masks for the image. We also have a version of CAPT that predicts a whole multi-sentence description—we call this one CAPT-MULTI. For this, we just slap all the sentences together in any old order. We also tried a nearest neighbor approach (NN-MULTI), where we just grab the annotation from the closest matching training data point. We figure out how close they are based on the features we pull from the image pair and use the Nearest-Neighbor module from sklearn (Pedregosa et al., 2011). For the single sentence version (NN), we just randomly pick one of the sentences from the annotation. Lastly, there's a version of the DDLA model with a fixed uniform prior, and we call this one DDLA-UNIFORM.",
        "formal_text": "Convert casual text to formal text: Baseline models: We're looking at a few different models here. First up, there's the CAPT model, which uses soft attention on a pair of input images. This attention"
    },
    {
        "casual_text": "S-rules work in two ways: forward and backward. We'll quickly explain both, but this paper mainly focuses on S-rules in the backward mode because that's the one that works best for a Question-Answering natural language system.",
        "formal_text": "Convert casual text to formal text: S-rules work in two ways: forward and backward. We'll quickly explain both, but this paper mainly focuses on S-rules in the backward mode because"
    },
    {
        "casual_text": "We've noticed that when working with a large dataset like MEDIA, the basic models (FST and CRF) can be trained really well, which means fewer mistakes need to be fixed. So, our re-ranking method doesn't help the CRF much, but it does give the FST baseline a boost—improving it by 1.6 percentage points, which is a 11.7% relative improvement.",
        "formal_text": "Convert casual text to formal text: We've noticed that when working with a large dataset like MEDIA, the basic models (FST and CRF) can be trained really well, which means fewer mistakes need to be fixed"
    },
    {
        "casual_text": "We’ve also been looking into knowledge-enhanced attention neural networks. These models have been around for a while, with examples from (Chen et al., 2017; Zhou et al., 2018; Peters et al., 2019; Huang et al., 2020). But here’s the thing—they’re not focused on sentiment classification. Even though there are tons of attention models out there for sentiment analysis, like we talked about earlier, none of them actually use enhanced attention mechanisms. Most of them just rely on self-attention, which is learned within the network. Our approach is different—we’re combining self-attention with enhanced attention. We’ll explain how our attention mechanism works and give you the details on the model we’re proposing next.",
        "formal_text": "Convert casual text to formal text: We’ve also been looking into knowledge-enhanced attention neural networks. These models have been around for a while, with examples from (Chen et al., 2017;"
    },
    {
        "casual_text": "Live chat services are becoming a bigger deal, but there’s not a lot of research out there about them in the field of computational linguistics. Most of the work has been focused on dialogue and dialogue corpora, especially in spoken dialogue systems (like the one by Stolcke et al. in 2000). There’s also been some research on multimodal dialogue systems used in things like telephone support (Bangalore et al., 2006) and tutoring systems (Litman and Silliman, 2004). Spoken dialogue systems come with a bunch of challenges because of the errors that happen with speech recognition technology. On the other hand, live chats are a type of written dialogue, so you don’t have to worry about those recognition errors. But, the language used in chats is usually pretty messy, and the back-and-forth can get complicated because the interaction isn’t fully synchronous (like Werry pointed out in 1996).",
        "formal_text": "Convert casual text to formal text: Live chat services are becoming a bigger deal, but there’s not a lot of research out there about them in the field of computational linguistics. Most of the work has been focused on"
    },
    {
        "casual_text": "We picked three Machine Learning algorithms for our project: logistic regression, neural networks, and boosted decision trees. These are known to be solid choices in the AKE community (thanks to Hasan and Ng, 2014). We ran them using R software with the glm, nnet, and C5.0 libraries to train the models. No fancy adjustments were made—the neural network was just a basic Multi Layer Perceptron (MLP) with one hidden layer. After training, we ranked the keyphrases (KPs) based on the raw probability scores the algorithms gave us. Here’s a quick look at the performance of three systems (A, B, and C) on a document with 15 correct keyphrases. The numbers show Precision (P), Recall (R), F1-Score, and Mean Average Precision (MAP). A tick () means the system got the keyphrase right, and an x () means it got it wrong. | System | P | R | F1 | MAP | |--------|------|------|------|------| | A | 0.33 | 0.33 | 0.33 | 0.33 | | B | 0.33 | 0.33 | 0.33 | 0.02 | | C | 0.40 | 0.40 | 0.40 | 0.31 | Table 1: Hypothetical results for systems A, B, and C.",
        "formal_text": "Convert casual text to formal text: We picked three Machine Learning algorithms for our project: logistic regression, neural networks, and boosted decision trees. These are known to be solid choices in the AKE community (thanks to Hasan and"
    },
    {
        "casual_text": "Non-autoregressive text-to-speech (NAR-TTS) models (like the ones by Ren et al. in 2019 and 2020, Peng et al. in 2020, Vainer and Duek in 2020, acucki in 2020, and Miao et al. in 2020) are way faster than their autoregressive counterparts (Shen et al. in 2018, Ping et al. in 2018). Plus, they can match or even beat the voice quality of those autoregressive models. The whole process of turning text into speech can be thought of as a conditional distribution, P(y|x), where x is the text and y is the speech. This is a one-to-many situation because a single piece of text can lead to different speech outputs depending on things like pitch, duration, and prosody. Also, speech mel-spectrograms are closely linked in both time and frequency (check out Section 2.1 for more details). So, P(y|x) isn’t just any distribution—it’s dependent and multimodal (as explained by Ling et al. in 2013 and Zen and Senior in 2014).",
        "formal_text": "Convert casual text to formal text: Non-autoregressive text-to-speech (NAR-TTS) models (like the ones by Ren et al. in 2019 and 2020, Peng e"
    },
    {
        "casual_text": "Organizing papers for a conference and making sure sessions have related topics can be really time-consuming, especially when you have a lot of papers. Machine learning can help automate this process by using natural language processing (NLP) to figure out what each paper is about. In this case, putting together a conference schedule is kind of like a semi-supervised clustering problem. In this paper, we introduced the ADoCS system, a web app that can group papers into clusters based on how similar they are. The groups are created based on the size distribution you set up. Right now, the app is mainly designed for organizing conference papers, but it could be used for other document clustering tasks with some restrictions, thanks to its flexible interface (you can choose different metrics, use TF-IDF, etc.).",
        "formal_text": "Convert casual text to formal text: Organizing papers for a conference and making sure sessions have related topics can be really time-consuming, especially when you have a lot of papers. Machine learning can help automate this process by"
    },
    {
        "casual_text": "We're introducing AutoNER, a fresh neural model that uses a new \"Tie or Break\" approach for distantly supervised NER tasks. We've tweaked the usual NER model to create the Fuzzy-LSTM-CRF model, which works really well as a strong baseline for distantly supervised tasks. We're also looking into ways to improve distant supervision for better NER results, like adding high-quality phrases to cut down on mistakes, and we've done some experiments to check how well it works. Our tests on three popular datasets show that AutoNER performs the best when it's just using dictionaries without any extra help from humans, and it can even go head-to-head with models that use supervised benchmarks.",
        "formal_text": "Convert casual text to formal text: We're introducing AutoNER, a fresh neural model that uses a new \"Tie or Break\" approach for distantly supervised NER tasks. We've tweaked"
    },
    {
        "casual_text": "We handed this text over to the annotators to flag any problems with how clear and complete it was. You can find all the nitty-gritty details about the process in the next section.",
        "formal_text": "Convert casual text to formal text: We handed this text over to the annotators to flag any problems with how clear and complete it was. You can find all nitty-grrr details about the process in the next"
    },
    {
        "casual_text": "Sure, you can use any basic segmenter repeatedly to chop up segments into smaller pieces and build a tree structure, but that might cause some issues. Topical segmenters aren’t flawless, so doing this over and over could just make the mistakes pile up. Plus, most segmenters need you to tell them how many segments to create, which is fine for simple segmentation. But when you’re building a tree, it’s not practical to know ahead of time how many segments and sub-segments you’ll need at each level.",
        "formal_text": "Convert casual text to formal text: Sure, you can use any basic segmenter repeatedly to chop up segments into smaller pieces and build a tree structure, but that might cause some issues. Topical segmenters aren’t flawless,"
    },
    {
        "casual_text": "The whole idea of Statistical Machine Translation (SMT), which is a way of doing machine translation using pattern recognition, started back in 1993 with Brown and his team. They defined the problem of machine translation like this: you have a sentence f in one language, and you need to find a sentenceê in another language that matches it as closely as possible. This can be put into a formula like this:",
        "formal_text": "Convert casual text to formal text: The whole idea of Statistical Machine Translation (SMT), which is a way of doing machine translation using pattern recognition, started back in 1993 with Brown and his team. They defined the problem of machine"
    },
    {
        "casual_text": "To manage how much knowledge and context each layer contributes, we put a gate after the knowledge selection block. Let's say h_k is the output from the knowledge selection block and h_c is what's leftover from the last block. The output from this controller can be shown like this:",
        "formal_text": "Convert casual text to formal text: To manage how much knowledge and context each layer contributes, we put a gate after the knowledge selection block. Let's say h_k is the output from the knowledge selection block and"
    },
    {
        "casual_text": "Strube & Hahn (1996) came up with a way to rank the Cf based on how familiar or new certain things are in a conversation. I’m taking their ideas and rephrasing them using Prince’s (1981; 1992) framework. I’ve grouped expressions into three categories: things the listener already knows (OLD), things that are kind of new but can be figured out (MED), and completely new things (NEW). These groups are based on Prince’s familiarity scale (Prince, 1981, p. 245). OLD includes things that have been mentioned before (E) and things that haven’t been used yet (U), while NEW includes totally fresh stuff (BN). MED includes things that can be inferred (I), things that are connected to other inferred stuff (I c), and new things that are tied to something the listener already knows (BN A). These MED things are new but linked to something familiar (check out Figure 1 for more). I don’t think there’s a difference in how important or familiar each thing is within its group. For example, both evoked and unused things are treated the same because they’re both OLD. To make Prince’s ideas more practical, I’m saying that evoked things are expressions that refer back to something (like pronouns, previously mentioned names, or relative pronouns). Unused things are like what Hahn et al. (1996) described. For anchored brand-new things, the anchor has to be either evoked or unused.",
        "formal_text": "Convert casual text to formal text: Strube & Hahn (1996) came up with a way to rank the Cf based on how familiar or new certain things are in a conversation. I’m taking their"
    },
    {
        "casual_text": "We run some experiments to figure out: (1) How CODE-SCRIBE stacks up against the best existing methods out there? (2) How well does the design of CODE-SCRIBE actually work?",
        "formal_text": "Convert casual text to formal text: We run some experiments to figure out: (1) How CODE-SCRIBE stackss against the best existing methods out there? (2) How well does the design of CODE-SCRIBE actually"
    },
    {
        "casual_text": "Since the CoNLL-2012 dataset only tags mentions that are part of coreference chains, we start by creating a mention extraction module that’s really good at finding mentions but might not be super precise. This module mainly uses syntactic parse trees to do its job. We grab all NP nodes (noun phrases), QP nodes (like complex phrases for amounts or measurements), and any terminals with POS tags like PN (pronouns) and NR (proper nouns) to build a big list of potential mentions. After that, we use some rules to filter out mentions that probably aren’t right. For example, we remove mentions that include: 1. Measure words like \"\" (one year) or \"\" (one time). 2. Named entities that fall into categories like PERCENT, MONEY, QUANTITY, or CARDINAL. 3. Interrogative pronouns like \"\" (what) or \"\" (where).",
        "formal_text": "Convert casual text to formal text: Since the CoNLL-2012 dataset only tags mentions that are part of coreference chains, we start by creating a mention extraction module that’s really good at finding mentions but might not"
    },
    {
        "casual_text": "Can machine translation (MT) help the Department of Defense (DoD)? I think the answer is a solid, \"Yes, but with some caveats.\" We’ve shown this in a bunch of real-world cases and with tech solutions that use MT engines, which are part of several programs. Over the past week, you’ve seen and heard a lot of examples of how MT adds value to the DoD and other federal agencies. I’ll just touch on a few of the programs currently being used by the DoD. I’ll share specific examples where MT helped with intelligence work—all of it is unclassified and can be found online.",
        "formal_text": "Convert casual text to formal text: Can machine translation (MT) help the Department of Defense (DoD)? I think the answer is a solid, \"Yes, but with some caveats.\" We’ve shown this in"
    },
    {
        "casual_text": "To make URNs work better, we need to cut down on the number of parameters that go into creating the matrix. We discovered that chopping off the top k parts of the underlying anti-symmetric matrices is a pretty handy way to shrink the size of word embeddings. This not only makes the embeddings smaller but also easier to analyze formally since they can be broken down into rotations across k planes. For the tasks we looked at, this truncation didn’t mess up the performance of the TURN model too much. Kiani and his team (2022) also tried this approach on a different set of tasks and it seemed to work there too, which suggests this strategy might be pretty useful in general.",
        "formal_text": "Convert casual text to formal text: To make URNs work better, we need to cut down on the number of parameters that go into creating the matrix. We discovered that chopping off the top k parts of the underlying"
    },
    {
        "casual_text": "One last thing we want to mention is that it's super important for researchers to really get how evaluation measures work and make sure they're suitable for the task at hand. For our next steps, we plan to look into and understand evaluation measures for tasks that aren't just OC and OQ.",
        "formal_text": "Convert casual text to formal text: One last thing we want to mention is that it's super important for researchers to really get how evaluation measures work and make sure they're suitable for the task at hand. For our next steps,"
    },
    {
        "casual_text": "A convolutional neural network (CNN), which was introduced by LeCun and others in 1998, is basically a type of feed-forward neural network. It’s made up of one or more convolutional layers (usually with some pooling involved) and then one or more fully connected layers. This setup was originally designed for computer vision tasks. Over time, though, it’s been used a lot for natural language processing stuff too, like in studies by Collobert and others in 2011, Kalchbrenner and team in 2014, and Kim in 2014.",
        "formal_text": "Convert casual text to formal text: A convolutional neural network (CNN), which was introduced by LeCun and others in 1998, is basically a type of feed-forward neural network. It’s made up of one"
    },
    {
        "casual_text": "A few studies (like Lin et al. in 2020 and Agarwal et al. in 2021) have shown that context really affects how predictions turn out. Check out Table 1, which compares the in-dictionary and out-of-dictionary parts of the CoNLL 2003 baseline (from Lin et al. in 2020) when it was tested on Bert-CRF. It's clear that there's a big difference in performance between the InDict and OutDict parts.",
        "formal_text": "Convert casual text to formal text: A few studies (like Lin et al. in 2020 and Agarwal et al. in 2021) have shown that context really affects how predictions turn out. Check out Table"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: A reordering model with specific rules, like ITG constraints (Wu, 1997), IBM constraints (Berger et al., 1996), and local constraints (Kumar and Byrne, 2005), can handle the differences in syntax between languages. Research has shown (Zens and Ney, 2003; Kanthak et al., 2005; Dreyer et al., 2007) that ITG constraints work better than other methods when dealing with reordering between lots of language pairs. Earlier work on speech translation using weighted finite-state transducers (WFST), like Zhou et al., 2005; Zhou et al., 2006; Mathias and Byrne, 2006; Matusov et al., 2006; Saon and Picheny, 2007, only used IBM constraints, local constraints, or made-up rules to train the reordering model. We're going to use ITG constraints, which have only been used for text translation before, to handle the syntactic differences in cross-lingual language modeling for speech recognition.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: A reordering model with specific rules, like ITG constraints (Wu, 1997), IBM constraints (Berger"
    },
    {
        "casual_text": "Alright, so we have this setup: x_i is the input text sequence, m is the time we sample weights, p_j_i_t is the probability from the jth sampling, and E represents the expected value of all predictions. The final uncertainty is basically the total of both ends of the predicted answer span.",
        "formal_text": "Convert casual text to formal text: Alright, so we have this setup: x_i is the input text sequence, m is the time we sample weights, p_j_i_t is the probability from"
    },
    {
        "casual_text": "We've also presented and explained why using partial rewrite rules is a good idea. These rules keep the important formal stuff intact but work really well with the tech we have right now.",
        "formal_text": "Convert casual text to formal text: We've also presented and explained why using partial rewrite rules a good idea. These rules keep the important formal stuff intact but work really well with the tech we have right now."
    },
    {
        "casual_text": "Let's dive into how BioBERT, the toughest model we have, handles things when we give it a hard time. We looked at how it deals with different tricky situations in the BC5CDR dataset, which is all about chemicals and diseases. We messed with the data a bit by using synonyms, swapping words around, and even doing some keyboard-style typos to see how well BioBERT could still figure things out.",
        "formal_text": "Convert casual text to formal text: Let's dive into how BioBERT, the toughest model we have, handles things when we give it a hard time. We looked at how it deals with different tricky situations in the BC"
    },
    {
        "casual_text": "Traditionally, SemEval evaluations don’t mask entity mentions. But, as we’ll talk about in Section 6.4, this approach makes models rely too much on those mentions and doesn’t really test how well they can handle new situations. So, we’re using two different ways to evaluate: (1) with-mention, where we keep the mentions to compare with past work; and (2) maskmention, where we hide the mentions to see how well our model can generalize in a more practical scenario.",
        "formal_text": "Convert casual text to formal text: Traditionally, SemEval evaluations don’t mask entity mentions. But, as we’ll talk about in Section 6.4, this approach makes models rely too much on those mention"
    },
    {
        "casual_text": "The historical context-based projection approach, often called the standard approach (SA), has been looked at in a bunch of studies (Fung, 1995; Rapp, 1999; Chiao and Zweigenbaum, 2002; Bouamor et al., 2013; Hazem and Morin, 2016; Jakubina and Langlais, 2017). To make this approach work, we start by creating a co-occurrence matrix for both the source and target languages. Each row in this matrix represents a context vector within an n-word window. These vectors are then adjusted using Mutual Information (MI, as introduced by Fano, 1961). After that, we find the corresponding word in the target language's vector space by using a bilingual seed lexicon to project each element from the context vector. Lastly, we rank the possible translations by comparing the similarity of the projected context vector with all the context vectors in the target language. We use the Cosine similarity measure because it's the most common method in similar studies and it allows us to easily parallelize the similarity comparisons.",
        "formal_text": "Convert casual text to formal text: The historical context-based projection approach, often called the standard approach (SA), has been looked at in a bunch of studies (Fung, 1995; Rapp, 1999; Chiao and"
    },
    {
        "casual_text": "For the NIST project, we trained the TM using around 1M parallel sentence pairs (each language has about 28M words). These pairs were taken from LDC corpora for the NIST MT evaluation, but we used a sampling method from Joshua to pick them. We also used a 5-gram language model, which was trained on a dataset with 130M words from English Gigaword (LDC2007T07) and the English side of the bitext.",
        "formal_text": "Convert casual text to formal text: For the NIST project, we trained the TM using around 1M parallel sentence pairs (each language has about 28M words). These pairs were taken from LDC corpora for the N"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. 1. If c_ij equals 1, then S_ij is the similarity between i and j. If c_ij equals 0, then S_ij is just 0. 2. The C factors work similarly to handle preferences. When we run the Max-Sum algorithm on the factor graph shown in Figure 1a, it helps maximize the overall similarity between all segment centers and their child segments at every level. Figure 1b gives a closer look at the messages that need to be sent to figure out the best configuration of variables. 3. We don't actually need to send messages like , , and  directly. Their values are already included in other types of messages. So, we just need to calculate and send four types of messages: , , , and .",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way. 1. If c_ij equals 1, then S_ij is the similarity between"
    },
    {
        "casual_text": "Coreference resolution is all about finding all the mentions in a text that point to the same thing. Thanks to deep learning, neural networks have started being used for this task, like in CoNLL-2012 (Pradhan et al., 2012), and more recently (Xu and Choi, 2020; Kirstain et al., 2021). Lee et al. (2017) were the first to use an LSTM (Sak et al., 2014) network for coreference resolution, which can directly pick up on dependencies in the text. Joshi (2019) set a baseline for coreference resolution using BERT. Joshi also came up with SpanBERT (2020), which improved the performance of the PLM, especially when it comes to extracting coreferences.",
        "formal_text": "Convert casual text to formal text: Coreference resolution is all about finding all the mentions in a text that point to the same thing. Thanks to deep learning, neural networks have started being used for this task, like in CoN"
    },
    {
        "casual_text": "Alright, let’s dive into a multilingual few-shot setup. Just like before, we’re dealing with limited data in each language, but this time, we’re using that small amount of data across all languages to fine-tune. For instance, if we have 10 training examples in each language, that adds up to 90 examples when we combine them for the 9 languages we’re testing on. You can check out some of our results in Table 1 under \"multilingual few-shot,\" and the full details are in Appendix C.",
        "formal_text": "Convert casual text to formal text: Alright, let’s dive into a multilingual few-shot setup. Just like before, we’re dealing with limited data in each language, but this time, we’re using that"
    },
    {
        "casual_text": "The abstractive dialogue summarization task has been a topic in research, especially when looking at the AMI meeting corpus (McCowan et al., 2005). For example, Banerjee et al. (2015), Mehdad et al. (2014), and Goo and Chen (2018) have all talked about it. The issue is that the corpus doesn’t have many summaries—only 141 dialogues have them. So, Goo and Chen (2018) suggested using the assigned topic descriptions as a kind of reference. These are short, kind of like labels, that describe the meeting goals, like \"costing evaluation of project process\" or \"components, materials, and energy sources.\" There’s also stuff like \"chitchat.\" But these descriptions are pretty vague and don’t give much detail about the structure or who said what.",
        "formal_text": "Convert casual text to formal text: The abstractive dialogue summarization task has been a topic in research, especially when looking at the AMI meeting corpus (McCowan et al., 2005). For example"
    },
    {
        "casual_text": "It's pretty rare to see direct comparisons of different taggers using the exact same test data. Most papers these days just claim that one tagger is better than another based on indirect comparisons. But we reckon there are a bunch of factors that aren't really taken into account or controlled for, which makes those conclusions kinda shaky in a lot of cases.",
        "formal_text": "Convert casual text to formal text: It's pretty rare to see direct comparisons of different taggers using the exact same test data. Most papers these days just claim that one tagger is better than another based on indirect comparison"
    },
    {
        "casual_text": "There are a bunch of things that can make you more likely to be overweight or obese. Some of these things you can change, like your age, family history, genes, race, or gender. But there are also things you can’t really change, like eating junk food, not being active enough, having bad eating habits, not getting enough sleep, or dealing with a lot of stress. Social stuff like being in a low-income situation or living in a neighborhood that’s not safe or healthy can also play a role. Eating too much of foods with bad fats or added sugars, or just having a generally unhealthy diet, can raise your chances of becoming obese. To lower your risk, try to avoid fatty foods, eat smaller portions, and cut back on screen time. For kids, obesity is a big issue in the U.S., and some groups are more at risk than others. As you get older, the risk of gaining unhealthy weight goes up. Adults who have...",
        "formal_text": "Convert casual text to formal text: There are a bunch of things that can make you more likely to be overweight or obese. Some of these things you can change, like your age, family history, genes, race, or gender."
    },
    {
        "casual_text": "Comic books actually started way back in the 18th century in Japan and the 1830s in Europe. But they didn’t really take off until the 1930s in the U.S.",
        "formal_text": "Convert casual text to formal text: Comic books actually started way back way back in Japan and 1830s in Europe. But they didn’t really take off until the 1930s in the U.S."
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. We’re looking at errors in models that use CRF (a type of machine learning model) and focusing on words that the model either knows (in-vocabulary) or doesn’t know (out-of-vocabulary, or OOV). Following some research by Ma and Hovy (2016), we’ve split the test words into four groups: 1. In-vocabulary words (the model knows these). 2. Out-of-training-vocabulary words (OOTV) – words the model hasn’t seen during training. 3. Out-of-embedding-vocabulary words (OOEV) – words not in the word embedding (like a dictionary the model uses). 4. Out-of-both-vocabulary words (OOBV) – words that are missing from both the training and the embedding. For tasks like NER (named entity recognition) and chunking, we’re looking at entities or chunks instead of individual words. The OOV entities and chunks are grouped the same way as the words, based on Ma and Hovy’s method. In Table 7, you can see how well different models handle these OOV splits on three benchmarks. The first three rows show the performance of word-based LSTM CRF models, and after that, there are the word-based CNN CRF models. For NER, the OOEV results are 100% accurate because there are only 8 OOEV entities, and the model got all of them right. It’s clear that adding character-level LSTM or CNN representations helps a lot with OOTV and OOBV words across all three datasets. This shows that neural character sequence representations are really good at figuring out OOV words. Models using character LSTM representations perform the best on in-vocabulary (IV) words in all setups. This might be because character LSTM can be trained well on IV data, giving it useful information about character sequences. For OOV words, both character LSTM and CNN give similar results.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. We’re looking at errors in models that use CRF (a type of machine learning model) and focusing on words that"
    },
    {
        "casual_text": "(a) Create a new FST by tweaking a random candidate from the set with some random change.",
        "formal_text": "Convert casual text to formal text: (a) Create a new FST by tweaking a random candidate from the set with some random change. (b) Convert casual text to formal text. (c) Create a"
    },
    {
        "casual_text": "SMatchToPageRank, or SMatch-ToPR for short (Alshomary et al., 2021), is designed to create an embedding space where argument-KP pairs that match are closer together than those that don't. This model takes two things as input: (1) the argument itself, and (2) the combination of the Key Point (KP) and the topic. Each of these inputs is turned into tokens and then embedded using RoBERTa-large. After that, these inputs go through a siamese Sentence-BERT network that's trained using a contrastive loss.",
        "formal_text": "Convert casual text to formal text: SMatchToPageRank, or SMatch-ToPR for short (Alshomary et al., 2021), is designed to create an embedding space"
    },
    {
        "casual_text": "The superscript \"l\" here means that if we use this rule, the ith child of A should be labeled with \"l.\" But there are a couple of issues with this approach. First, writing out a grammar like this would be super tedious, and it’s basically impossible if we want to deal with trees that have any number of branches. So, instead, we can use an extended version of a CFG (Context-Free Grammar), which was introduced by Thatcher in 1967. This version allows the right-hand sides of the rules to be regular expressions. That means we can add a union operator () and a Kleene star ( * ) to the mix to make things more flexible.",
        "formal_text": "Convert casual text to formal text: The superscript \"l\" here means that if we use this rule, the ith child of A should be labeled with \"l.\" But there are a couple of issues with this"
    },
    {
        "casual_text": "Sure! Here's the informal version: 1. Check out the Transformers library here: https://pypi.org/project/transformers/. 2. The ROCStories dataset is available at this link: https://cs.rochester.edu/nlp/rocstories/. 3. You can explore the SQuAD dataset here: https://rajpurkar.github.io/SQuAD-explorer/. 4. The WMT14 translation task info is here: https://www.statmt.org/wmt14/translation-task.html.",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: 1. Check out the Transformers library here: https://pypi.org/project/transformers/. 2. The ROCStories"
    },
    {
        "casual_text": "Basically, for each step i in the process, we create a feature vector F i with four parts, using something called the H-feature detectorp i. This feature vector has: (1) how similar H i and w i are before any projection; (2) the result of applying featurep i to H i; (3) the result of applying the H-feature detectorp i to w i; and (4) the difference between parts 2 and 3.",
        "formal_text": "Convert casual text to formal text: Basically, for each step i in the process, we create a feature vector F i with four parts, using something called the H-feature detectorp i. This feature vector has"
    },
    {
        "casual_text": "Using context in semantic parsing has its ups and downs. As you can see in Table 1, one tricky part is figuring out what people are referring to, like in \"For each of those, what is the maximum age?\". This example also highlights another issue with short or incomplete sentences. Take \"What about the average age?\" for instance—it doesn’t say anything about the database table or the column pettype. To make sense of it, you need to look at the context around it. Unlike CISP, which usually assumes everything you need is in the sentence, CDSP has to deal with these context-related challenges (Liang, 2016; Zhang et al., 2019; Liu et al., 2020). But tackling these challenges also opens up opportunities to better understand how language works and affects semantic parsing. Our study on CDSP is kind of a first, since most recent surveys in this area have focused on CISP (Kamath and Das, 2018; Zhu et al., 2019).",
        "formal_text": "Convert casual text to formal text: Using context in semantic parsing has its ups and downs. As you can see in Table 1, one tricky part is figuring out what people are referring to, like in \"For"
    },
    {
        "casual_text": "The term you're looking at could be a specific example, a basic-level idea, or something in between—what we call an intermediate concept. An intermediate concept sits right in the middle, between the basic level and the more general root concepts.",
        "formal_text": "Convert casual text to formal text: The term you're looking at could be a specific example, a basic-level idea, or something in between—what we call an intermediate concept. An intermediate concept sits right in the"
    },
    {
        "casual_text": "F. 1. 3, yo, do the abstract and intro cover the main points the paper's trying to make?",
        "formal_text": "Convert casual text to formal text: F. 1. 3, yo, do the abstract and intro cover the main points the paper's trying to figure out? Do the abstract and intro cover the main points the paper's trying to"
    },
    {
        "casual_text": "In this method, we use the WordNet Similarity package (Seco et al., 2004), which works with WordNet hypernyms behind the scenes. Basically, for any two words, the package looks at how far apart they are in the WordNet network and uses that distance to figure out how similar they are. For instance, \"car\" and \"automobile\" are super similar (1.0), while \"film\" and \"audience\" are less so (0.38). For each question, we use this package to compare the main word of the question with a few key words that describe a category. These description words are usually just one to three words that give you the gist of what the category is about. For example, the description words for the category ENTY: dismed are \"diseases\" and \"medicine.\" The category that matches the main word the most is added as a feature. It's like a tiny question classifier. For instance, if the main word is \"walrus\" in the question \"What is the proper name for a female walrus,\" and it matches most closely with \"animals,\" which is a description word for the category ENTY: animal, then we add ENTY: animal to the features for that question.",
        "formal_text": "Convert casual text to formal text: In this method, we use the WordNet Similarity package (Seco et al., 2004), which works with WordNet hypernyms behind the scenes. Basically,"
    },
    {
        "casual_text": "When it comes to the joint model, recent research has mostly focused on building neural network (NN) models to take advantage of the relationship between ID and SF. This includes stuff like the slot-gated model with attention (Goo et al., 2018), joint BERT (Chen et al., 2019a), a bi-directional interrelated model (E et al., 2019), and the stack-propagation method (Qin et al., 2019). They also came up with a joint NLU model using a triangular-chain conditional random field (TriCRF), which is like a big, combined probabilistic model for two related problems. Xu and Sarikaya (2013) introduced Multitask Active Learning (MTAL), which is all about picking samples with more info to label for all the tasks. Reichart et al. (2008) took a different approach, using a rank combination protocol to rank tasks and then selecting the best samples for the chosen task. This helps keep the performance for a group of tasks similar to picking the best for each individual task. However, in multitask active learning, this method didn’t really use the connections between tasks to pick instances.",
        "formal_text": "Convert casual text to formal text: When it comes to the joint model, recent research has mostly focused on building neural network (NN) models to take advantage of the relationship between ID and SF. This includes stuff like the slot-gate"
    },
    {
        "casual_text": "Every behavior agent has a list of things it’s looking for in the input, kind of like a to-do list (Rudnicky and Xu 1999). These agents are kept in a priority queue, which means the most recently used one gets top priority. When new commands come in, like \"load water\" or \"increase volume,\" the system checks each agent one by one to see if it matches what they’re expecting. If there’s a match, that agent jumps to the top of the queue, and the message gets sent to it. This setup helps manage multiple agents working together. In the December 2002 version, each agent’s list was set in stone for every conversation, but a smarter update would be to make those lists change based on what’s happening in the conversation.",
        "formal_text": "Convert casual text to formal text: Every behavior agent has a list of things it’s looking for in the input, kind of like a to-do list (Rudnicky and Xu 1999). These agents"
    },
    {
        "casual_text": "Turns out, CNNs can automatically pull out useful stuff from sentences, like the grammar and meaning. Sometimes, we use these features in other models to tackle different tasks. In our experiment, we show how well our model's features work. Since CNN models have already nailed finding relationships within single sentences, we tried something new: figuring out the relationship between entities that weren't in the same sentence.",
        "formal_text": "Convert casual text to formal text: Turns out, CNNs can automatically pull out useful stuff from sentences, like the grammar and meaning. Sometimes, we use these features in other models to tackle different tasks. In our experiment, we show"
    },
    {
        "casual_text": "The idea of NLRS (Natural Language Report System) for really specific areas was first shown in Kukich's 1983 work about \"knowledge-based generation\" of stock market reports. Kukich's ANA system creates professional-sounding summaries of the stock market using daily data from the Dow Jones' half-hourly quotes for the market average and major indices. Both ANA and FRANA, its French counterpart (from Contant in 1986), used a phrasal lexicon approach (like the one Beeker described in 1975). This method keeps the language part limited, but it works well for small, predictable areas. The stuff I'm talking about now takes a more modular approach to NLRS and applies it to a new area.",
        "formal_text": "Convert casual text to formal text: The idea of NLRS (Natural Language Report System) for really specific areas was first shown in Kukich's 1983 work about \"knowledge-based generation\" of stock market reports"
    },
    {
        "casual_text": "Current detectors are kind of fragile, meaning their decisions can change a lot even with tiny tweaks to the text. Like, Wolff (2020) found that the RoBERTa detector can be tricked by basic stuff like swapping characters with similar-looking ones or messing up some words. These little tricks drop the detector's accuracy when checking text from TGM—from 97.44% down to 0.26% and 22.68% respectively. So, it's super important to look at all kinds of sneaky attacks, from simple ones (like typos) to more complex ones (like universal attacks, as mentioned by Wallace et al., 2019), and make examples to test how easily the detector can be fooled. The goal is to figure out where it’s weak and make it tougher against all these tricks.",
        "formal_text": "Convert casual text to formal text: Current detectors are kind of fragile, meaning their decisions can change a lot even with tiny tweaks to the text. Like, Wolff (2020) found that the RoBERTa detector can"
    },
    {
        "casual_text": "Alright, so we looked at how gender works when languages like Afro-Asiatic and Indo-European are involved, and guess what? We found out that even though these languages aren't related, they can still share some gender stuff. This kind of backs up the idea that there are some universal rules for how gender is assigned. Even when we only tested it on things that aren't alive, the system still worked better than just guessing randomly. So, it seems like these universal rules aren't just about living things, but apply to all kinds of nouns.",
        "formal_text": "Convert casual text to formal text: Alright, so we looked at how gender works when languages like Afro-Asiatic and Indo-European are involved, and guess what? We found out that even though these languages are"
    },
    {
        "casual_text": "So, basically, \"R_i\" represents the relation-oriented stuff for the i-th sentence in the review or rebuttal. And \"d_g\" is just the size of the output features that the GCN spits out.",
        "formal_text": "Convert casual text to formal text: So, basically, \"R_i\" represents the relation-oriented stuff for the i-th sentence in the review or rebuttal. And \"d_g\" is just the size"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: idf(w) = log(O_w), where O_w is calculated by looking at all the documents (d) in the collection (C) and summing up the probabilities (Pr) of the word (w) appearing in different contexts (c) within those documents. Basically, O is just a way to compare O_w with other words (w') in the vocabulary (V).",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: idf(w) = log(O_w), where O_w is calculated by looking at all the documents ("
    },
    {
        "casual_text": "Tns arguments can be the same for different predicates in a clause, so the memory is 'horizontal' and not limited at all. Plus, by sharing variables, predicates that are goals can pass information to their subgoals.",
        "formal_text": "Convert casual text to formal text: Tns arguments can be the same for different predicates in a clause, so the memory is 'horizontal' and not limited all. Plus, by sharing variables, predicates"
    },
    {
        "casual_text": "2Appelt (1982) points out some issues with the conduit model, like how it can't really explain how things like word choices and gestures work together when we're talking or thinking of what to say.",
        "formal_text": "Convert casual text to formal text: 2Appelt (1982) points out some issues with the conduit model, like how it can't really explain how things like word choices and gestures work together when we're talking or thinking of"
    },
    {
        "casual_text": "• LightConv and DynamicConv. The code for these is included in fairseq 7 (Ott et al., 2019). We train lightweight convolution models in two different ways: (1) starting from scratch to learn token representations. In this case, we use BPE tokenization with a vocabulary of 30K types, and we rely on the fastBPE implementation 8 (Sennrich et al., 2015); (2) using pre-trained language model representations to initialize the token embeddings. For the language model, we go with GPT-2 small (Radford et al., 2019).",
        "formal_text": "Convert casual text to formal text: • LightConv and DynamicConv. The code for these is included in fairseq 7 (Ott et al., 2019). We train lightweight convolution models in"
    },
    {
        "casual_text": "We got our data from three main places—two for names of people and one for brand names. For the people's names, we used databases from the U.S. and the U.K. available at https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets. The U.S. names come from birth records kept by the U.S. Social Security Administration from 1880 to 2013, and the U.K. names come from the UK Office of National Statistics, the Northern Ireland Statistics and Research Administration, and the Scotland General Register Office. After getting rid of names that could be both male and female, we had 97,102 unique English names to work with (60,984 female and 36,118 male). For brand names, we used Kantar Media's Stradegy database, which tracks U.S. advertising spending by brands across almost every product category. We removed multi-word names that were derivative brands (like Ford Escort) and a small number (66) of names that are common English words (like Coach) based on the 5,000 most frequent words in the British National Corpus. In the end, we had 1,021 brand names across 17 product categories. Each name was turned into a 16-dimensional vector based on the features we described, and we made all the data available in the supplementary materials.",
        "formal_text": "Convert casual text to formal text: We got our data from three main places—two for names of people and one for brand names. For the people's names, we used databases from the U.S. and the U.K"
    },
    {
        "casual_text": "Adversarial training isn't just used for generating stuff. It can also be applied to text classification (Chen et al., 2016; Chen and Cardie, 2018) and relation extraction (Wu et al., 2017). Plus, people are trying to use it to create language-invariant features too (Chen et al., 2016).",
        "formal_text": "Convert casual text to formal text: Adversarial training isn't just used for generating stuff. It can also be applied to text classification (Chen et al., 2016; Chen and Cardie, 2018"
    },
    {
        "casual_text": "These algorithms, along with checking how similar the extracted results are, help figure out how important the main result is.",
        "formal_text": "Convert casual text to formal text: These algorithms, along with checking how similar the extracted results are are help figure out how important the main result is. Convert casual text to formal text: These algorithms, along with checking how similar the extracted"
    },
    {
        "casual_text": "Also, a lot of previous research shows that models relying only on word-based features tend to struggle with data sparsity. The traditional bag-of-words approach doesn’t work well when there aren’t many matching words between the feature vectors. Take these two sets of words, for example: set1 = kitten, nyc and set2 = cat, new, york. There’s no overlap or similarity between the words in each set. To fix this issue, earlier studies started using word clusters from big, unannotated datasets as extra features (Ang Sun et al., 2011). These features have been proven to really help with the data sparsity problem. Taking inspiration from that, we’re planning to add these cluster-based features to our own model.",
        "formal_text": "Convert casual text to formal text: Also, a lot of previous research shows that models relying only on word-based features tend to struggle with data sparsity. The traditional bag-of-words approach doesn’t"
    },
    {
        "casual_text": "Here's the informal version: \"Check out Figure 1: These are scatter plots with lines showing the best-fit results for the SUP method in the PanLex BLI model (which is in the first column of Table 1 in the main paper). The left side shows the results for the top single isomorphism measure, which is ECOND-HM. The right side shows the results for the combined model that uses regression analysis and includes linguistic measures. Oh, and they flipped the sign of r on the right side to make the graphs easier to compare.\"",
        "formal_text": "Convert casual text to formal text: Here's the informal version: \"Check out Figure 1: These are scatter plots with lines showing the best-fit results for the SUP method in the PanLex BLI model (which is"
    },
    {
        "casual_text": "The rest of this paper goes like this: Section 2 explains the basic idea of matrix tri-factorization. Section 3 talks about our graph co-regularized non-negative matrix tri-factorization (GN-MTF) method for sentiment classification. Section 4 shows the results of our experiments. Section 5 covers related work. Finally, in Section 6, we wrap things up and talk about what we might explore in the future.",
        "formal_text": "Convert casual text to formal text: The rest of this paper goes like this: Section 2 explains the basic idea of matrix tri-factorization. Section 3 talks about our graph co-regularized non-negative matrix tri-"
    },
    {
        "casual_text": "We reached out to a researcher who specializes in Japanese literature to dig up some papers based on a pretty detailed search. One of the queries we gave him was something like, \"how fables changed from being oral stories to written forms, how they were passed around among people, and the overall environment or context of fables\" (*). He started by scanning the available works and roughly picked about ten that he figured were relevant to the topic of \"fables.\"",
        "formal_text": "Convert casual text to formal text: We reached out to a researcher who specializes in Japanese literature to dig up some papers based on a pretty detailed search. One of the queries we gave him was something like, \"how"
    },
    {
        "casual_text": "where n is the number of words in the sentence. Basically, for each episode, the way we update the parameters is based on the REINFORCE method, and we do this for every single token in the sentence.",
        "formal_text": "Convert casual text to formal text: where n is the number of words in the sentence. Basically, for each episode, the way we update the parameters is based on the REINFORCE method, and we do this for"
    },
    {
        "casual_text": "- We added early exit tricks to skip unnecessary calculations, which makes the network faster. - We ran a bunch of tests on GLUE to see how it works.",
        "formal_text": "Convert casual text to formal text: - We added early exit tricks to skip unnecessary calculations, which makes the network faster. - We ran a bunch of tests on GLUE to see how it works."
    },
    {
        "casual_text": "In the next part, we'll dive deeper into how  relates to . The pattern we uncover will be the foundation for creating efficient algorithms to track the regularization path.",
        "formal_text": "Convert casual text to formal text: In the next part, we'll dive deeper into how  relates to . The pattern we uncover will the foundation for creating efficient algorithms to track the regularization path."
    },
    {
        "casual_text": "The Leibniz Institute for the German Language (IDS) is working on setting up a long-term storage system for linguistic data. Right now, they're focusing on figuring out the best way to include their own collections of written and spoken language into this system. Both types of data can be seen as examples of big, ever-growing collections. For instance, the German Reference Corpus DeReKo (Kupietz et al., 2010, 2018) has been developed at IDS since the 1960s (Teubert and Belica, 2014). It currently holds 46.9 billion tokens, which take up 56 GB of disk space (not counting automatic annotations). This resource is used by over 40,000 German linguists worldwide, mainly through tools like COSMAS II (Bodmer, 2005) and KorAP (Baski et al., 2013). On the spoken language side, the Archive for Spoken German (AGD; Schmidt, 2017) has about 80 spoken language corpora. These are accessible through the Datenbank für Gesprochenes Deutsch (Schmidt and Gasch, 2019). While they might have fewer documents or text tokens compared to written corpora, they're still considered large because they include big audio or video files. For example, the BOLSA study (Lehr and Thomae, 1987) has 1,113 recordings, totaling 2,833 hours of audio. Stored as mono WAV files with a 48 kHz sampling rate, these recordings take up around 1 TB of disk space.",
        "formal_text": "Convert casual text to formal text: The Leibniz Institute for the German Language (IDS) is working on setting up a long-term storage system for linguistic data. Right now, they're focusing on"
    },
    {
        "casual_text": "Basically, both tasks are trained together, and we try to make the total error from each task as small as possible. To put it more formally, here's how we calculate the loss for the combined model:",
        "formal_text": "Convert casual text to formal text: Basically, both tasks are trained together, and we try to make the total error from each task as small as possible. To put it more formally, here's how we calculate the loss for"
    },
    {
        "casual_text": "Check out the results for the CRW tasks in Table 2. On the regular CRW task, hybrid methods are the clear winners, with the FCM crushing the competition even without any context sentences. This just goes to show how much useful word info is out there. On the Filtered CRW task, form-based scores drop a lot. But, as you can see from the FastText models and the Form-Context models, using form info can still give you a big edge by filling in the gaps where context info is limited. In this case, the fixed strategy used by FastText helps the selective FastText algorithm rank among the best models on the Filtered CRW task, while the original FastText algorithm kind of falls apart due to overfitting. The Neural FCM is the overall champ, topping almost every trial.",
        "formal_text": "Convert casual text to formal text: Check out the results for the CRW tasks in Table 2. On the regular CRW task, hybrid methods are the clear winners, with the FCM crushing the competition even without any context sentences."
    },
    {
        "casual_text": "We trained the system using sentences made from 17 templates listed in Table 1. These sentences had three main parts: the agent (like the person doing something), the act (basically the verb), and the patient (the thing being affected). You could add extra info about the agent or the patient using relative clauses, and these extra bits could also act as the agent or patient in the relative clause.",
        "formal_text": "Convert casual text to formal text: We trained the system using sentences made from 17 templates listed in Table 1. These sentences had three main parts: the agent (like the person doing something), the act (basically the verb), and the patient"
    },
    {
        "casual_text": "Most modern machine translation systems rely on something called log-linear models. These models are based on specific features of the translation and how important each of those features is, which is determined by weights. Basically, the model looks like this:",
        "formal_text": "Convert casual text to formal text: Most modern machine translation systems rely on something called log-linear models. These models are based on specific features of the translation and how important each of those features is, which is determined by weight"
    },
    {
        "casual_text": "Considering these challenges, this paper suggests using Graph Convolutional Networks (GCN) on a special kind of tree called a contextual sub-tree for event extraction. This contextual sub-tree is a modified version of a dependency parse tree that’s been trimmed in a unique way. It gives us not only the usual dependency path info but also extra details that aren’t on the main path. This extra info adds more context to the connection between two nodes, which is why it’s called a contextual sub-tree.",
        "formal_text": "Convert casual text to formal text: Considering these challenges, this paper suggests using Graph Convolutional Networks (GCN) on a special kind of tree called a contextual sub-tree for event extraction. This contextual"
    },
    {
        "casual_text": "The method we're suggesting isn't just good for dealing with all kinds of structures and bigger problems—it also has a solid covering number bound, which means it generalizes well.",
        "formal_text": "Convert casual text to formal text: The method we're suggesting—it also has a solid covering number bound, which means it generalizes well. Convert casual text to formal text: The method we're suggesting isn"
    },
    {
        "casual_text": "We're using LeeBERT, built on Hugging-Face's Transformers. Our experiments are running on a single Nvidia V100 GPU with 16GB of memory.",
        "formal_text": "Convert casual text to formal text: We're using LeeBERT, built on Hugging-Face's Transformers. Our experiments are running on a single Nvidia V100 GPU with 16GB memory."
    },
    {
        "casual_text": "Back-translation (BT), which has been around since at least 2011 (Bojar and Tamchyna, Sennrich et al., Poncelas et al.), is a technique used to boost the quality of neural machine translation (NMT) systems. NMT, developed by folks like Sutskever, Bahdanau, Gehring, and Vaswani, has been a game-changer in translation. Systems that use large-scale back-translation have even topped recent WMT competitions (Bojar et al.). The basic idea is to train a model that translates from the target language back to the source language. This helps create extra synthetic parallel data using just monolingual target text. The result is pairs of sentences where the source is synthetic but the target is real. These pairs are then added to the original bilingual data to train the source-to-target model. BT helps the model generalize better and can also be used to tweak models to perform well in specific test domains by incorporating relevant monolingual data.",
        "formal_text": "Convert casual text to formal text: Back-translation (BT), which has been around since at least 2011 (Bojar and Tamchyna, Sennrich et al., Poncelas et"
    },
    {
        "casual_text": "To figure out the probabilities ( p(a_i) ) and ( p(a_i, a_j) ), we just count how many n-grams have either one word or both words and then divide that by the total number of n-grams. We’re using 5-grams here, by the way. Next, we split compounds into 4 groups based on their length and check the error rates within each group. As you can see in Figure 5, the error rates for compounds with positive mean PMI scores are lower than those with negative scores across all groups, which supports our ideas. In Figure 6, you can see the error rates for different compound patterns from Table 3. The MOD atom really messes things up when it comes to translation error rates. On average, compounds with MOD have an error rate that’s 19.78% higher than those without it. On the other hand, adding an ADJ only bumps up the error rate by 2.66%. The main issue with MOD is that it causes word reordering problems. For example, you can easily translate \"the small dog\" without changing the word order. But with compounds like \"the dog he liked,\" the model needs to recognize that \"he liked\" is a MOD and translate it before \"the dog\" in Chinese. We found a lot of cases where the model didn’t reorder the words or broke the connection between the nouns and modifiers. Looking at all these groups, you’ll notice that the error rate for NP (Pattern 1.*) is usually lower than for VP (Pattern 2.*) and PP (Pattern 3.*). This difference is even clearer for patterns without MOD. The reason? Compounds in Pattern 1. just seem to be easier to handle.",
        "formal_text": "Convert casual text to formal text: To figure out the probabilities ( p(a_i) ) and ( p(a_i, a_j) ), we just count"
    },
    {
        "casual_text": "It made sense to kick things off with a small, easy-to-handle pilot project to see if the idea would work.",
        "formal_text": "Convert casual text to formal text: It made sense to kick things off with a small, easy-to-handle pilot project to see if the idea would work. Convert casual text to formal text. Convert casual text"
    },
    {
        "casual_text": "We did a deep dive into how the algorithm learns. It starts with a model trained on English data, but it struggles with sentences that have a lot of code-mixing. We found that with each round of training, the model gets better at handling sentences with more code-mixing. Eventually, it starts to get the hang of the code-mixed data.",
        "formal_text": "Convert casual text to formal text: We did a deep dive into how the algorithm learns. It starts with a model trained on English data, but it struggles with sentences that have a lot of code-mixing. We"
    },
    {
        "casual_text": "The fifth part of syntactic complexity is called transformational history. I’m not calling it a ‘measure’ right now because it’s really unclear if we can even assign any meaningful measures to this idea. No one’s really tried to do that yet, so I’m not going to dive into it any further here.",
        "formal_text": "Convert casual text to formal text: The fifth part of syntactic complexity is called transformational history. I’m not calling it a ‘measure’ right now because it’s really unclear if we can even assign any"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We're talking about extending our parser to output \"S's\" (whatever that means). The tricky part is dealing with verbs like \"raising\" and \"equi\" that take infinitives or \"that\" clauses. But don't worry, it's not too hard to figure out how to handle these. The reason it's not too tricky is because we're using LFG (Lexical Functional Grammar) as our base, not Kamp's categorial syntax. With LFG, the thematic roles in a sentence are already sorted out in the f-structure (functional structure). So, it's pretty straightforward to write rules that give us the right meanings for sentences with or without those tricky verbs. Let's look at an example: - \"John persuaded Mary to come\" - \"John persuaded Mary that she should come\" Both of these sentences mean the same thing, just structured differently. Now, let's talk about how these rules work using a familiar example: \"every man loves a woman.\" With Kamp's categorial syntax, the rules work from the top of the tree down. The order in which we handle the parts of the sentence is determined by the syntactic rules. In other words, the scope (how things relate to each other) is directly tied to how the sentence is built.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. We're talking about extending our parser to output \"S's\" (whatever that means). The tricky"
    },
    {
        "casual_text": "We've come up with three ways to figure out the best value of k by using the H2 values from 1... deltaK. Basically, we look at how H2 changes as k increases and pick the point where those changes stop being meaningful.",
        "formal_text": "Convert casual text to formal text: We've come up with three ways to figure out the best value of k by using the H2 values from 1... deltaK Basically, we look at how H2 changes as k"
    },
    {
        "casual_text": "Okay, so we have a set E with elements e1, e2, up to eM, and we want to group them into K different clusters labeled C1 to Ck.",
        "formal_text": "Convert casual text to formal text: Okay, so we have a set E with elements e1, e2, up to eM, and we want to group them into K different clusters labeled C1 to C"
    },
    {
        "casual_text": "Lately, people have been looking into using web search queries as a way to train systems and fix spelling mistakes (Yang Zhang and Li, 2007; Cucerzan and Brill, 2004). Query data is packed with examples of misspelled words and how users correct them, but it's not something we can easily access or use in our method.",
        "formal_text": "Convert casual text to formal text: Lately, people have been looking into using web search queries as a way to train systems and fix spelling mistakes (Yang Zhang and Li, 2007; Cucerzan and Brill, 2004)."
    },
    {
        "casual_text": "Alright, let’s break this down into something more understandable. Here’s the informal version: \"Okay, so we’ve got a bunch of codes here: KOIO 000951, K0ll 014-185, K012 014135, K013 040, K01.1 031, K015 031901, K016 031900, K017 020847, and then L010, EM010, EM020. It’s a mix of numbers and letters, but I’m guessing they’re some kind of reference or identifier. Maybe part of a system or something?\" Does that make more sense?",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down into something more understandable. Here’s the informal version: \"Okay, so we’ve got a bunch of codes here: KOIO"
    },
    {
        "casual_text": "Hey, do you know where the tensor-dot operation is? It's the one that calculates the product of two tensors along the K-axis, like, you know, for example.",
        "formal_text": "Convert casual text to formal text: Hey, do you know where the tensor-dot operation? It's the one that calculates the product of two tensors along the K-axis,"
    },
    {
        "casual_text": "To figure out these matrices quickly, we break down the data into smaller chunks and handle each chunk at the same time (Chen et al., 2011). There’s a simple example of a graph G, along with its affinity matrix A and Laplacian matrix L, in Figure 1. **Parameter Estimation:** When it comes to optimizing the objective function in equations (12-13), there’s a tricky part: the L1 term, which encourages sparsity, isn’t smooth or easy to work with. On the other hand, the L2 norm and the Laplacian sparsity term are smooth. If we start by taking the derivative of the smooth parts, we can get the following gradient pieces:",
        "formal_text": "Convert casual text to formal text: To figure out these matrices quickly, we break down the data into smaller chunks and handle each chunk at the same time (Chen et al., 2011). There’s"
    },
    {
        "casual_text": "We've built a bottom-up CKY-style decoder. For a given source sentence, c, we start by setting up our search space using the phrase translation table and applying the terminal rules from 1 r to 4 r. Each source phrase, whether it's contiguous or not, can be translated in two ways: as a contiguous translation, a non-contiguous one, or both. All possible translations covering the source from position i to j are stored in the chart cell for i to j. Before expanding the cell (i, j), all its sub-cells within (i, j) are already expanded. To finish the derivation for each sub-cell, we do two things. First, we use rules from 7 r to 11 r to create initial hypotheses for each sub-cell and calculate the score for the new translation by combining the scores of the two sub-derivations. Then, in each cell, we only keep the contiguous translations. Next, we use rules 5 r and 6 r for reordering, which is kind of like how BTG handles translation. Once we've covered the entire source sentence, the decoding process is done.",
        "formal_text": "Convert casual text to formal text: We've built a bottom-up CKY-style decoder. For a given source sentence, c, we start by setting up our search space using the phrase translation table"
    },
    {
        "casual_text": "To make sure we have more examples for the less common relations shown in Figure 1, we create some fake or \"silver\" data. This helps us transfer models from one language to another, usually from a language with lots of resources to one with fewer. We’ll show how this works using English and Hindi as examples.",
        "formal_text": "Convert casual text to formal text: To make sure we have more examples for the less common relations shown in Figure 1, we create some fake or \"silver\" data. This helps us transfer models from one language to another, usually from"
    },
    {
        "casual_text": "We ran QuestEval on the QAGS-XSUM data for all the different combinations of hyperparameters and models. This way, SummEval stayed completely out of the picture during testing.",
        "formal_text": "Convert casual text to formal text: We ran QuestEval on the QAGS-XSUM data for all the different combinations of hyperparameters and models. This way, SummEval stayed completely out the"
    },
    {
        "casual_text": "Like we mentioned earlier, thinking about bigger picture stuff (like topics) can really help when we're trying to rank sentences for summarizing. In our two-part graph, the top part has all the topic nodes, and the bottom part has the sentence nodes, including one special node for the query.",
        "formal_text": "Convert casual text to formal text: Like we mentioned earlier, thinking about bigger picture stuff (like topics) can really help when we're trying to rank sentences for summarizing. In our two-part graph, the top part has"
    },
    {
        "casual_text": "2. For this one, we're using all 30 OneStopQA articles to train and then we'll use the RACE dev and test sets for development and testing.",
        "formal_text": "Convert casual text to formal text: 2. For this one we're using all 30 OneStopQA articles to train and then we'll ... Convert casual text to formal text: 2. For this one we're using all"
    },
    {
        "casual_text": "The Normalized Edit Distance (NED) method calculates the edit distance for every pair of words in our dataset, following the approach by Nerbonne and Heeringa (1997). Each operation (like insertions, deletions, or substitutions) counts as one unit, except for substituting a character with itself, which costs nothing. So, NED basically tells us the minimum number of steps needed to change 'word a' into 'word b'. We use a similarity score based on NED, which is calculated as (1 - NED Score). To improve this score, we mix NED with q-gram distance (Shannon, 1948). Q-grams, or 'n-grams', are just chunks of text with a length of q. This combined measure has been used in different spelling correction methods before (like Owolabi and McGregor, 1988; Kohonen, 1978). Kanojia et al. (2019b) came up with this metric, and we’re using it to create features for their baseline approach. For any pair of words p and q, here's how it works:",
        "formal_text": "Convert casual text to formal text: The Normalized Edit Distance (NED) method calculates the edit distance for every pair of words in our dataset, following the approach by Nerbonne and Heeringa (1997). Each operation"
    },
    {
        "casual_text": "We ran some experiments to see how BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) handle masking. You can check out the details in Appendix A.",
        "formal_text": "Convert casual text to formal text: We ran some experiments to see how BART (Lewis et al., 2020) and T5 (Raffel et al., 2020) handle masking. You"
    },
    {
        "casual_text": "To calculate the SARI score, we use the code from (Xu et al., 2016) that's available on GitHub at https://github.com/cocoxu/simplification/blob/master/SARI.py.",
        "formal_text": "Convert casual text to formal text: To calculate the SARI score, we use the code from (Xu et al., 2016) that's available on GitHub at https://github.com/cocox"
    },
    {
        "casual_text": "We can find the projection directions by solving this generalized eigenvalue problem:",
        "formal_text": "Convert casual text to formal text: We can find the projection directions by solving this generalized eigenvalue problem: We can find the projection directions by solving this generalized eigenvalue problem: We can find the projection directions by"
    },
    {
        "casual_text": "It makes sense that the feelings of users with more influence in a community should carry more weight when figuring out the overall mood of the group. So, we try to gauge the community's emotion by considering each user's \"emotional authority.\" This is based on the idea that different people in the community have different levels of emotional influence.",
        "formal_text": "Convert casual text to formal text: It makes sense that the feelings of users with more influence in a community should carry more weight when figuring out the overall mood of the group. So, we try to gauge the community's emotion"
    },
    {
        "casual_text": "To meet the needs of our customers, we tweaked the SAIC MT system to work with their data and added some features that let us integrate translation memory. Over time, this has really boosted the quality of the MT output, which means less work for humans to do in post-editing. Plus, we fixed some issues with formatting and how the MT output looked, which helped cut down on post-editing costs even more for the TSP. Here's how this paper breaks down: In Section 2, we'll explain the SAIC hybrid MT system and its key features. Section 3 will show how we integrated translation memory (TM) into the system using special flags. These flags make sure that full TM matches are always picked, and partial matches are preferred during translation. Section 4 talks about a method we used for offline domain adaptation. This let us create a customized baseline system for the customer. We also have an online adaptation method that allows us to make updates to the system as new TM entries or other parallel data come in. In Section 5, we'll show how a few simple but effective techniques improved the readability of the MT output for humans. These included tweaking punctuation spacing, capitalization, and making adjustments for differences between European and Canadian French. Finally, Section 6 presents the experimental results. We'll show how the adaptation affected automatic MT metrics on different test sets, both in-domain and out-of-domain.",
        "formal_text": "Convert casual text to formal text: To meet the needs of our customers, we tweaked the SAIC MT system to work with their data and added some features that let us integrate translation memory. Over time, this has really"
    },
    {
        "casual_text": "The main types of SiMT models can be split into two groups: seq-to-seq and prefix-to-prefix. Early SiMT approaches typically used a full-sentence MT model with a seq-to-seq setup to translate each chunk of text, as defined by the SiMT policy (Bangalore et al., 2012; Cho and Esipova, 2016). Gu et al. (2017) took it a step further by using reinforcement learning to train a model that could decide when to start translating. Building on that, some added a prediction step based on Gu et al. (2017). More recently, Zhang et al. (2020b) came up with an adaptive segmentation policy that focuses on meaning units. But, the difference between how these models are trained and how they’re used in real-world scenarios often results in lower translation quality.",
        "formal_text": "Convert casual text to formal text: The main types of SiMT models can be split into two groups: seq-to-seq and prefix-to-prefix. Early SiMT approaches typically used a full-"
    },
    {
        "casual_text": "Since the Enron dataset is made up of a bunch of emails, each email will have just a few keywords added to it.",
        "formal_text": "Convert casual text to formal text: Since the Enron dataset is made up of a bunch of emails, each email will have just a few keywords added to each email. Convert casual text to formal text: Since the En"
    },
    {
        "casual_text": "We built three phrase-based translation systems using the Moses toolkit (Koehn et al., 2007) with its default settings. The main difference between them is how their reordering models were set up. The language model we used is a 5-gram with interpolation and Kneser-Ney smoothing (Kneser and Ney, 1995). To fine-tune the system, we used the MERT technique (Och, 2003).",
        "formal_text": "Convert casual text to formal text: We built three phrase-based translation systems using the Moses toolkit (Koehn et al., 2007) with its default settings. The main difference between them is how their"
    },
    {
        "casual_text": "When we look at models with really good sample quality, loss truncation combined with rejection sampling does better than all the other methods we tested (including beam search) when it comes to how people rate the quality of the samples (HUSE-Q). And if you look at Figure 5, you can see that using truncation and rejection sampling together (which you can do by combining models) beats the other methods in both quality and diversity. Rejection sampling lowers the overall HUSE score because it’s made to only give you really high-quality samples (like, super high HUSE-Q). But this means it doesn’t give you as much variety, so the overall HUSE score takes a hit. The results we got from the other methods match what people already know about how these things balance quality and diversity. Beam search gives you high-quality samples but not much variety. Top-k and top-p sampling add more diversity compared to beam search. And GANs? They usually don’t do as well as models that are well-tuned with log loss when it comes to both diversity and quality.",
        "formal_text": "Convert casual text to formal text: When we look at models with really good sample quality, loss truncation combined with rejection sampling does better than all the other methods we tested (including beam search) when it comes to how people"
    },
    {
        "casual_text": "Alright, here's the deal. We've got a set of labels called V, which is split into two parts: V_train and V_test. Each image is represented by z_v, where v is the label for that image. The main goal is to correctly guess the label for images in V_test during the testing phase, even though we didn't see any images with labels from V_test while training. To make this happen, we'll use a lookup table called A, which has some known attributes for each label v in V. Specifically, for each attribute k, we define it like this:",
        "formal_text": "Convert casual text to formal text: Alright, here's the deal. We've got a set of labels called V, which is split into two parts: V_train and V_test. Each image is represented by"
    },
    {
        "casual_text": "Sparseness is a big issue for a few of the 21 categories. We looked at the common mistakes the classifier made using a confusion matrix. The classifier mixed up PARAPHRASE edits with INFORMATION-M. It also struggled to tell the difference between VANDALISM and RE-VERT, as well as INFORMATION-I. Overall, modifications tend to perform worse compared to insertions or deletions. All the classifiers we tested make their predictions by setting a threshold on a ranking, which you can see in Table 3. This can cause errors because the classifier can't make a prediction if it's not confident enough about any category. Another problem is the imbalance in the data, due to the uneven distribution of categories. In unclear cases, the classifier leans toward the category that has more examples in the training data.",
        "formal_text": "Convert casual text to formal text: Sparseness is a big issue for a few of the 21 categories. We looked at the common mistakes the classifier made using a confusion matrix. The classifier mixed up PARAP"
    },
    {
        "casual_text": "So, segs(c_j) is just a way of saying all the segmentation labels that are connected to c_j. And e_seg(segs(c_j)) is a 5-part vector where each part can be either on or off, depending on whether it matches one of these options: B, M, E, S, or O.",
        "formal_text": "Convert casual text to formal text: So, segs(c_j) is just a way of saying all the segmentation labels that are connected to c_j. And e_seg(se"
    },
    {
        "casual_text": "People have figured out some cool ways to handle big classification problems, like the dual coordinate descent method for linear support vector machines, which worked well (Hsieh et al., 2008). Similarly, Yu et al. (2011) came up with a two-level version of dual coordinate descent for maximum entropy classifiers.",
        "formal_text": "Convert casual text to formal text: People have figured out some cool ways to handle big classification problems, like the dual coordinate descent method for linear support vector machines, which worked well (Hsieh et al., 2008)."
    },
    {
        "casual_text": "You can produce stuff in two main ways or mix them up: \"extraction\" and \"synthesis.\" Here are some typical categories of production.",
        "formal_text": "Convert casual text to formal text: You can produce stuff in two main ways or mix them up: \"extraction\" and \"synthesis.\" Here are some typical categories of production. Convert casual text to formal text: You can produce stuff"
    },
    {
        "casual_text": "According to (Mostafazadeh et al., 2016), SCT was built using ROCStories as its foundation. The ROC corpus has 100,000 five-sentence stories, each written to make logical sense. After removing the original endings, writers create both a \"correct\" ending and a \"wrong\" ending for randomly selected examples from the corpus. The published SCT uses ROCStories to create a big training set, an evaluation set, and a test set, all with the same structure and 1,871 stories each. We test our model using the standard SCT (Mostafazadeh et al., 2016). The training set has four-sentence stories with one correct ending, while the evaluation set has four-sentence stories with two possible endings.",
        "formal_text": "Convert casual text to formal text: According to (Mostafazadeh et al., 2016), SCT was built using ROCStories as its foundation. The ROC corpus has 100,000 five-"
    },
    {
        "casual_text": "Okay, so for K  1, we need to make sure that either r_i2  r_i4  r_i1  r_i3 or r_i3  r_i1  r_i4  r_i2 works, with the condition that 1  i1  i2  i3  i4  K.",
        "formal_text": "Convert casual text  formal text: Okay, so for K  1, we need to make sure that either r_i2  r_i4  r_i1  r_i"
    },
    {
        "casual_text": "In the second part, we check out how different types of word representations affect things. First, we swap out the multilingual BERT embeddings for MUSE embeddings. MUSE embeddings come from aligning fastText embeddings for different languages into one shared space. We notice that our model's performance takes a big hit. One big reason is that MUSE embeddings aren't contextualized, so a word that shows up multiple times in the same sentence gets the same embedding every time, even if it has different meanings. This causes problems when we try to decompress the information. One fix is to combine MUSE with word position embeddings during compression and decompression (check the Appendix for more on this). This helps improve the SRL performance from 47.7% (German), 52.6% (French), and 44.5% (Chinese) to 55.3%, 60.5%, and 53.0%, but it's still not as good as the original model. Then, we take out the Gaussian noise from the model, and we see a drop in performance, which shows that the noise actually helps boost the SRL accuracy.",
        "formal_text": "Convert casual text to formal text: In the second part, we check out how different types of word representations affect things. First, we swap out the multilingual BERT embeddings for MUSE embeddings. M"
    },
    {
        "casual_text": "Okay, so Acc and E(Acc) are probabilities, meaning they range from 0 to 1. Since Kappa is based on these, it also falls within that same range and acts like a probability. Back in the day, Fleiss (1981) and others tried to expand on Cohen's (1960) idea of Kappa to work with more than just two categories (not just positive/negative) and more than two raters (not just a \"real\" one and a \"prediction\" one). Fleiss actually built on Scott's (1955) Pi, not Cohen's Kappa, to handle both of these things. The Fleiss Kappa isn't set up like we explained it here for simplicity—it’s more about how often the raters agree when they’re rating the same number of items, N, but they don’t necessarily have to rate all the same items. Krippendorf (1970, 1978) took this even further by allowing any number of raters to assess different numbers of items.",
        "formal_text": "Convert casual text to formal text: Okay, so Acc and E(Acc) are probabilities, meaning they range from 0 to 1. Since Kappa is based on these, it also falls within that same range and acts"
    },
    {
        "casual_text": "Okay, so the main idea here is that we want the meaning of sentences to be separate from the way they’re structured grammatically. This can be tricky when we try to translate sentences into logical formulas, like in predicate logic or something similar. Take these two sentences for example: 1. \"If John admires a woman, then he kisses her.\" 2. \"Every man who admires a woman kisses her.\" Both of these sentences have truth conditions that can be expressed using first-order logic formulas. The issue is that when we translate them, the phrase \"a woman\" ends up being treated as if it’s universally quantified in the logical representation. This is a problem because the pronoun \"she\" has to refer back to that specific woman, and there’s no easy way around it. Now, something called IT (I’m guessing it’s a framework or theory) gives us a way to handle this. It explains how indefinite descriptions, conditionals, universally quantified phrases, and pronouns like \"she\" work together. This satisfies our first requirement: the meaning of these sentences matches what we intuitively think they mean. The second reason we’re using IR as our semantic tool for LFG (whatever that is) is because it lets us figure out the meaning of a sentence based on the sentences that came before it in a text. So, if we have a text with sentences S(1) to S(n), the meaning of sentence S(i) is built on the meanings of the sentences before it, like S(1) to S(i-1).",
        "formal_text": "Convert casual text to formal text: Okay, so the main idea here is that we want the meaning of sentences to be separate from the way they’re structured grammatically. This can be tricky when we try to translate sentences into"
    },
    {
        "casual_text": "The sequence model in this paper is pretty similar to the LSTM-based model mentioned in another source. As you can see in Figure 1, there are three context utterances labeled u1, u2, and u3, which are connected as a sequence of words. A special word called \"sos\" is added between each pair of utterances to mark where one ends and the next begins. Once we have the sequences for both the context and the response, we turn the words into word embeddings using a shared lookup table. To create the context and response embeddings, we use a Gated Recurrent Unit (GRU) neural network, which was introduced by Chung et al. in 2014. The GRU works on these two sequences of word embeddings, following the steps outlined in Equations 2 to 5. Here, ht-1 is the hidden state of the GRU when it processes the word embedding et-1 of the word wt-1. The initial state h0 is just a zero vector. The variables zt and rt are the update and reset gates, respectively. The new hidden state ht for the embedding et is a mix of the previous hidden state ht-1 and the current input embedding et, with the update and reset gates controlling how much of each is used. The parameters U, Uz, Ur, W, Wz, and Wr are part of the GRU model and need to be learned. The symbol  represents element-wise multiplication.",
        "formal_text": "Convert casual text to formal text: The sequence model in this paper is pretty similar to the LSTM-based model mentioned in another source. As you can see in Figure 1, there are three context utterances labeled"
    },
    {
        "casual_text": "Plus, our sentence vector includes almost all the info from previous sentences, which their model can't do.",
        "formal_text": "Convert casual text to formal text: Plus, our sentence vector includes almost all info from previous sentences, which their model doesn't. Plus, our sentence vector includes almost all info from previous sentences, which their model doesn't."
    },
    {
        "casual_text": "The reading phase uses a fancy neural network model that picks out the answer from the top d paragraphs it finds. This model looks at each word in the text and figures out its meaning by running the whole sequence through a special type of network called a bidirectional long short-term memory network (BiLSTM), which was invented by Hochreiter and Schmidhuber back in 1997. It grabs the hidden state from the top layer for each word to understand it better. The question gets turned into a vector q, which is kind of like a summary of the question, by averaging out the hidden states from another BiLSTM that works on the individual words of the question. Then, the model calculates a score for each word to see how likely it is to be the start or end of the answer.",
        "formal_text": "Convert casual text to formal text: The reading phase uses a fancy neural network model that picks out the answer from the top d paragraphs it finds. This model looks at each word in the text and figures out its meaning by"
    },
    {
        "casual_text": "The Unigram Baseline (UNI) is a straightforward yet effective approach suggested by Pichotta and Mooney back in 2014 for this specific task. It's basically a unigram model where candidates are sorted based on how often they show up in the training data, without considering the surrounding context.",
        "formal_text": "Convert casual text to formal text: The Unigram Baseline (UNI) is a straightforward yet effective approach suggested by Pichotta and Mooney back in 2014 for this specific task. It's basically a unigram"
    },
    {
        "casual_text": "Sure! Here's the informal version: We did some digging into how tiny differences in training data can mess with the quality and confidence of Neural Machine Translation (NMT). We started with a bunch of English-French sentence pairs from WikiMatrix that are basically the same. Then, we messed them up a bit by adding small, artificial differences to see how it would affect things. Unlike others who looked at how noise impacts MT, we focused on different types of subtle semantic differences and played around with the mix of how much equivalent data vs. divergent data we used. Our results showed that when these imperfect training examples take over, they: 1. Lower translation quality (as shown by BLEU and METEOR scores). 2. Make the system more likely to spit out crappy text. 3. Make the model's predictions less certain.",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: We did some digging into how tiny differences in training data can mess with the quality and confidence of Neural Machine Translation (NMT). We started with a"
    },
    {
        "casual_text": "Okay, so figuring out the selection ratio does more than just help with picking stuff. It also gives us a rough idea of when to stop. By knowing the rough split of class labels in the dataset and the overall size, we can estimate how many samples each class should have. Once the total number of selections for a class, across all the iterations, hits that estimated number based on the selection ratio, we can call it quits and stop the algorithm.",
        "formal_text": "Convert casual text to formal text: Okay, so figuring out the selection ratio does more than just help with picking stuff. It also gives us a rough idea of when to stop. By knowing the rough split of class labels in the"
    },
    {
        "casual_text": "Summarizing spoken conversations is still a pretty new and tricky area of research (Reithinger et al., 2000; Zechner, 2002). Dealing with multiparty dialogue is way harder than working with written text. On top of the challenges that come with summarizing speech, spoken dialogue has all sorts of quirks like disfluencies, pauses, interruptions, and so on. Plus, the information being summarized might come from different people, like in question-answer pairs. And to make things even more complicated, the language used in spoken dialogue is different from written language. Since people in a conversation can quickly clear up any confusion, they don’t need to be as clear or precise with their words.",
        "formal_text": "Convert casual text to formal text: Summarizing spoken conversations is still a pretty new and tricky area of research (Reithinger et al., 2000; Zechner, 2002). Dealing with multiparty dialogue is"
    },
    {
        "casual_text": "Okay, so the first thing is, we're talking about this thing called \"konduk'i\" and it's related to \"tra'ir'ig'i.\" Basically, it's like saying, \"Hey, this konduk'i is connected to tra'ir'ig'i, and it's all part of this process called kurent'o.\"",
        "formal_text": "Convert casual text to formal text: Okay, so the first thing is, we're talking about this thing called \"konduk'i\" and it's related to \"tra'ir'ig'i.\""
    },
    {
        "casual_text": "We also took a closer look at a few things that impact how well these big language models (PLMs) work, like the pretraining method, the data they're trained on, how their vocabulary is built, and the schedule for optimizing them.",
        "formal_text": "Convert casual text to formal text: We also took a closer a few things that impact how well these big language models (PLMs) work, like the pretraining method, the data they're trained on, how their"
    },
    {
        "casual_text": "Basically, imagine you have a graph G with nodes (N), edges (E), and labels on those edges (R). You also have a bunch of pairs of nodes (s i, t i) from a set D. You can make a matrix where the rows are these node pairs and the columns are the edge labels. PRA takes this matrix and adds extra columns for sequences of edge labels, which we call \"path types.\" Instead of just showing if there's an edge between two nodes, the cells now show how specific the connection is between those nodes based on the path type.",
        "formal_text": "Convert casual text to formal text: Basically, imagine you have a graph G with nodes (N), edges (E), and labels on those edges (R). You also have a bunch of pairs of nodes (s"
    },
    {
        "casual_text": "We start by initializing all the parameters in our model using the method from Glorot and Bengio (2010). We also use dropout (from Srivastava et al., 2014) on the embedding layer and hidden states, with a dropout rate of 0.5. The models are optimized using the Adam optimizer (Kingma and Ba, 2014), with gradient clipping set to 3 (Pascanu et al., 2013). The initial learning rate is set to 0.001 and decreases as the training progresses. We keep an eye on the training process using the validation set and report the final results on the test set. For generating 100-dimensional word embeddings, we use a one-layer CNN with a filter size of 3 and max pooling. We initialize the word embeddings using the cased 300-dimensional Glove and keep them fixed during training. In some additional experiments, we also use the hidden states from BERT as extra word embeddings and keep those fixed too. We share the parameters of both memory components with the parameter matrices in the corresponding softmax layers. This can be seen as a way to add some supervised signals to the memories. We did some hyperparameter tuning for the layer size (which we set to 3) and the loss weight  (which we set to 0.5). For other parameters, we just went with the values listed in the supplementary material.",
        "formal_text": "Convert casual text to formal text: We start by initializing all the parameters in our model using the method from Glorot and Bengio (2010). We also use dropout (from Srivastava et al., 2014)"
    },
    {
        "casual_text": "John and Mary were messing around in the garden when their classmate Evelyn showed up and tried to join their game. John gave Mary a little shove, and Evelyn wasn't having it—she kicked him right back.",
        "formal_text": "Convert casual text: John and Mary were messing around in the garden when their classmate Evelyn showed up and tried to join their game. John gave Mary a little shove, and Evelyn wasn't having it—"
    },
    {
        "casual_text": "Automatic evaluation metrics are super important for making progress in AI tasks because they help us compare and improve different systems. But when it comes to natural language generation (NLG) systems, designing reliable metrics is really tough and still a big challenge in research. Novikova et al. (2017) and Peyrard (2019) found that the current metrics don’t match up well with how humans judge things, so they suggested we need to come up with new ways to evaluate these systems.",
        "formal_text": "Convert casual text to formal text: Automatic evaluation metrics are super important for making progress in AI tasks because they help us compare and improve different systems. But when it comes to natural language generation (NLG) systems, designing reliable metrics is really"
    },
    {
        "casual_text": "1. The WMT'18 models can be found here: https://github.com/pytorch/fairseq/tree/master/examples/backtranslation, and we just used one of them. 2. For WMT'19, the models are here: https://github.com/pytorch/fairseq/tree/master/examples/wmt19. Check out Figure 1 for a breakdown of the translations we used. X stands for sentences in the source language, and Y for sentences in the target language. A single * means it's a direct translation of the original sentence, while ** means it's a double translation (so, a translation of a translated sentence). The original dataset combines pairs of (X, Y*) (direct mode) and (X*, Y) (reverse mode). According to BLEU scores, training with backtranslation (BT) only improves in reverse mode. In this study, we also collected double translations. These are helpful because: - They let us see if translating from translationese (X**) is easier than translating from the original (X), with Y* as the reference. - They also help us check if predicting Y** (a double translation) is easier than predicting Y, when the input is X*.",
        "formal_text": "Convert casual text to formal text: 1. The WMT'18 models can be found here: https://github.com/pytorch/fairseq/tree/master/examples/back"
    },
    {
        "casual_text": "This is just one example of how language features work together to create specific patterns. There are lots of other ways this could be studied too, and they all need a model that’s flexible and realistic enough to handle it.",
        "formal_text": "Convert casual text to formal text: This is just one example of how language features work together to create specific patterns. There are lots of other ways this could be studied too, and they all need a model that’s flexible and realistic"
    },
    {
        "casual_text": "Even though there are some downsides, a lot of name entity (NE) systems rely on big lists of names. Krupke and Hausman (1998) used a ton of name lists in their system. They noticed that cutting the list size by over 90% didn’t really hurt performance, but adding just 42 more names actually made things better. This shows that the quality of the names in the list matters more than how many names are on it. Mikheev and his team (1999) also played around with different kinds of lists in an NE system they entered into MUC7 (MUC, 1998). They found that smaller lists with well-picked names worked just as well as bigger lists, which matches what Krupke and Hausman found. But both studies changed the name lists while working within a bigger NE system, so it’s hard to say if the consistent performance was because of the changes to the lists or if the system was just using other outside info to make up for any lost data.",
        "formal_text": "Convert casual text to formal text: Even though there are some downsides, a lot of name entity (NE) systems rely on big lists of names. Krupke and Hausman (1998) used a ton of name"
    },
    {
        "casual_text": "In this project, we’re diving deep into why people make edits on Wikipedia. We came up with a detailed list of reasons why someone might change a page, like fixing grammar, adding more info, checking facts, or making things easier to understand. Unlike other systems that either focus on tiny grammar tweaks or mix up grammar and meaning, our approach looks at the bigger picture—what the editor is actually trying to do. This helps us figure out not just what changed, but why, and what’s going on in the editor’s head while they’re working (which, by the way, some smart folks have studied before). To make this happen, we teamed up with 13 experienced Wikipedians to create a system that captures the \"why\" behind edits, which we call \"edit intention.\" We labeled over 7,000 edits with these intentions to get a clear picture. Then, we built a tool to automatically figure out these intentions by looking at the differences between versions of articles. To see how useful this all is, we tested our system on two big questions for Wikipedia: how to keep new editors around and how edits affect article quality. We looked at whether the kind of edits newbies make in their first session predicts if they’ll stick around, and how different types of edits impact how good an article is.",
        "formal_text": "Convert casual text to formal text: In this project, we’re diving deep into why people make edits on Wikipedia. We came up with a detailed list of reasons why someone might change a page, like fixing grammar, adding"
    },
    {
        "casual_text": "Sure! Here's a more casual version: \"You can find some datasets for text mining here: [https://www.cs.umb.edu/smimarog/textmining/datasets/](https://www.cs.umb.edu/smimarog/textmining/datasets/) and also check out this link for more corpora: [http://disi.unitn.it/moschitti/corpora.htm](http://disi.unitn.it/moschitti/corpora.htm).\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: \"You can find some datasets for text mining here: [https://www.cs.umb.edu/smim"
    },
    {
        "casual_text": "In Table 6, we break down our manual evaluation results word by word. For each word, we show the F1 scores for the most common sense (MFS), Babelfy, and our own system. Just like Loureiro et al. (2021), we also include the ratio of the first sense compared to the others (F2R) and something called normalized entropy to give an idea of how balanced the senses are. All this data is split by each annotator.",
        "formal_text": "Convert casual text to formal text: In Table 6, we break down our manual evaluation results word by word. For each word, we show the F1 scores for the most common sense (MFS), Babelfy, and our own system"
    },
    {
        "casual_text": "They looked at a bunch of different ways to compare things, like set difference, geometric distance, distributional similarity, and mixture models. After testing out five different systems, they realized that cosine similarity worked the best, with the new mixture model coming in second.",
        "formal_text": "Convert casual text to formal text: They looked at a bunch of different ways to compare things, like set difference, geometric distance, distributional similarity, and mixture models. After testing out five different systems, they realized that cos"
    },
    {
        "casual_text": "We used the Transformers library by Wolf et al. (2020) and PyTorch by Paszke et al. (2017) to implement PET and iPET.",
        "formal_text": "Convert casual text to formal text: We used the Transformers library by Wolf et al. (2020) and PyTorch by Paszke et al. (2017) to implement PET and iPET."
    },
    {
        "casual_text": "The method for automatically finding local agreement rules follows the same structure as the one described in (Sanchez-Martinez and Forcada, 2009). However, it’s only good for spotting local agreements. The requirements are pretty straightforward—you just need a morphologically annotated corpus of the source language. From the source language part of the corpus, trigrams and bigrams with their morphological descriptions were pulled out. The training data used was from (Dimitrova et al., 1998), which was manually checked to make sure there were no mistakes in the morphosyntactic tags. In our experiment, we used the Slovenian language, but the same approach could work for other languages since the method isn’t tied to any specific language. For each bigram and trigram, we checked if the tags of different words agreed with each other. The tags and their positions didn’t matter. If we found any agreements, we saved that as a potential rule. The POS tags from the bigram or trigram formed the pattern part of the rule, while the action part was built using all the morphosyntactic tags that had agreement info. Finally, the rule candidates were grouped based on their patterns and actions. Each group, with a set number of candidates, was picked as a valid rule.",
        "formal_text": "Convert casual text to formal text: The method for automatically finding local agreement rules follows the same structure as the one described in (Sanchez-Martinez and Forcada, 2009). However, it’s only good for spotting"
    },
    {
        "casual_text": "One big challenge in machine reading (like Etzioni et al. mentioned in 2006) is figuring out what things are in a text and connecting them to the right info, like what category they belong to. Take the sentence \"Some people think that pineapple juice is good for vitamin C.\" For a machine to understand this, it needs to know that \"pineapple juice\" is a drink and \"vitamin C\" is a nutrient.",
        "formal_text": "Convert casual text to formal text: One big challenge in machine reading (like Etzioni et al. mentioned in 2006) is figuring out what things are in a text and connecting them to the right info, like"
    },
    {
        "casual_text": "Du kannst zwischen den Schnappschüssen hin und her wechseln, um die richtigen Einstellungen zu finden.",
        "formal_text": "Convert casual text to formal text: Convert casual text to formal text: Du kannst zwischen den Schnappschüssen hin und her wechseln, um die richtigen Einstellungen zu finden. Du kannst zwischen den Schnappschüsse"
    },
    {
        "casual_text": "To get a clearer picture of how the instance relabeling strategy works, we tracked the Exact Match Accuracy of our method with and without this strategy on the MTOP test set after each training session. As you can see in Figure 2, our method with the relabeling strategy just keeps getting better and consistently outperforms the baseline during the relabeling phase. This shows that the relabeling approach actually fixes a lot of label mistakes in the messy training data, and those corrected labels help boost the performance.",
        "formal_text": "Convert casual text to formal text: To get a clearer picture how the instance relabeling strategy works, we tracked the Exact Match Accuracy of our method with and without this strategy on the MTOP test set after"
    },
    {
        "casual_text": "So, to deal with these issues, recent methods have focused on two main strategies: making the story generation model more controllable and adding in some commonsense knowledge. One of the big ideas in controllability is the \"plan and write\" method (Yao et al., 2019). They start by using a RAKE algorithm to pick out the most important words from each sentence and then train a storyline planner using that data. The language model is trained to work with both the previous context and these keywords. When it comes time to generate a story, the keywords are created from the title and help guide each sentence as it's written. Commonsense knowledge is basically shared knowledge about how the world works (Alabdulkarim et al., 2021). They fine-tune a pre-trained GPT-2 model using knowledge triples from commonsense datasets. First, they turn these triples into sentences using some predefined rules (like turning (eiffel tower, AtLocation, paris) into \"eiffel tower is at paris\") and then train the model on these sentences using the usual maximum likelihood estimation method. Xu et al. (2020) took things a step further by combining both of these approaches. They first trained a keyword planner using GPT-2 and then used those keywords to search a knowledgebase. The top-ranked sentences from the search were used to guide the story generation process.",
        "formal_text": "Convert casual text to formal text: So, to deal with these issues, recent methods have focused on two main strategies: making the story generation model more controllable and adding in some commonsense knowledge. One of the big ideas in"
    },
    {
        "casual_text": "Okay, so basically, we can automatically turn the term definitions and conceptual templates into a typed feature structure using the ALEP formalism (as mentioned in Ripplinger et al., 1994). This TERM structure holds general terminological info, like the classification scheme from the EIRETERM termbank and the concept definition. It also includes the concept feature, which identifies the CONCEPT and connects it to the domain's ontology. Plus, there's the CONCEPT_ROLES structure that outlines the role slots of the concept, which comes from the SITUATION class and parts of the PROPERTY class. And finally, there are the conceptual modifiers listed in the CONCEPT_MODIFY structure, which also come from the PROPERTY class.",
        "formal_text": "Convert casual text to formal text: Okay, so basically, we can automatically turn the term definitions and conceptual templates into a typed feature structure using the ALEP formalism (as mentioned in Ripplinger et"
    },
    {
        "casual_text": "Anastasopoulos and Chiang (2018) pointed out that the higher-level layers are more meaningful than the lower ones. On the other hand, Peters and his team (2018) found that the higher-level layers focus on the context-related parts of word meaning, while the lower-level layers deal with syntax stuff. This brings up a good question: do we really need to worry about modeling localness for every single layer?",
        "formal_text": "Convert casual text to formal text: Anastasopoulos and Chiang (2018) pointed out that the higher-level layers are more meaningful than the lower ones. On the other hand, Peters and his team (2018) found that the"
    },
    {
        "casual_text": "Our findings show that the most significant improvements in interpretability come from (i) using a lot of prior words and (ii) allowing them to exist more freely outside the fixed dimensions. Both the weak Standard Basis Prior and the Truncated Prior do a pretty similar job of fitting within the predefined dimensions. Like earlier studies, our experiments suggest some problems with fully separating concepts, which we think could be fixed in future work by adding informative priors across multiple dimensions to better capture complex ideas and move beyond simple antonym-based dimension setups.",
        "formal_text": "Convert casual text to formal text: Our findings show that the most significant improvements in interpretability come from (i) using a lot of prior words and (ii) allowing them to exist more freely outside the fixed dimensions. Both"
    },
    {
        "casual_text": "We also took a look at how well our model performs on the MAMS dataset. This dataset has at least two different aspect categories in each sentence, each with its own sentiment. Table 7 shows that BART generation beats all the other models we compared it to, which means our model is better at spotting multiple aspects in a single sentence.",
        "formal_text": "Convert casual text to formal text: We also took a look at how well our model performs on the MAMS dataset. This dataset has at least two different aspect categories in each sentence, each with its own sentiment. Table 7 shows"
    },
    {
        "casual_text": "5. How much does it mess things up if the type of text doesn't match? Like, if the paraphrased stuff is used on a validation set that's different from what the paraphraser was trained on, will it still work well?",
        "formal_text": "Convert casual text to formal text: 5. How much does it mess things up if the type of text doesn't match? Like, if the paraphrased stuff is used on a validation set that's different from the"
    },
    {
        "casual_text": "Next, we stick each output vector r t together with the projected input p t to get g t = [r t; p t ], which is a vector with 3l elements. Then, we send g t through the self-attention layer. This layer figures out how similar everything is by creating a similarity map S g that’s n by n, using a tri-linear function. You can see this process in Figure 1: Overview of our model (A) and the dense encoder block with self-attention (B).",
        "formal_text": "Convert casual text to formal text: Next, we stick each output vector r t together with the projected input p t to get g t = [r t; p t ], which is"
    },
    {
        "casual_text": "Let's make sure we compare this to the exact IR engine that was used to create the dataset for the PU model. The PU model really relies on the IR engine doing a good job, so it's important to keep that in mind.",
        "formal_text": "Convert casual text to formal text: Let's make sure we compare this to the exact IR engine that was used to create the dataset for the PU model. The PU model really relies on the IR engine doing"
    },
    {
        "casual_text": "I’m super thankful to the University of Malaya for covering the costs of my time in Britain.",
        "formal_text": "Convert casual text to formal text: I’m super thankful to the University of Malaya for covering the costs of my in Britain."
    },
    {
        "casual_text": "So, the ResDAVEnet model was only trained on real human captions for MSCOCO images, but it still performs really well with completely made-up captions. For 1,000 real human captions, it gets retrieval scores of 0.867 and 0.828 R@10 for speech and images, respectively. In comparison, the SAT-FT VQ3 model scores 0.766 and 0.765 R@10. This shows that the ResDAVEnet model can understand the main ideas of an image, create a sequence of units that represent those ideas, and produce speech that sounds natural enough for the ResDAV-Enet model to pick up on those ideas. A few other image-to-speech models also do pretty well in retrieval tests. Interestingly, the rankings of the models are similar to what we saw when using word-based evaluation methods.",
        "formal_text": "Convert casual text to formal text: So, the ResDAVEnet model was only trained on real human captions for MSCOCO images, but it still performs really well with completely made-up captions. For 1,000 real human"
    },
    {
        "casual_text": "We're focusing on three specific monotone Seq2Seq tasks—spelling correction, G2P conversion, and lemmatization—to get a more balanced comparison. These monotone Seq2Seq tasks, like morphological analysis/lemmatization, G2P conversion (Yao and Zweig, 2015; Rao et al., 2015), transliteration (Sherif and Kondrak, 2007), and spelling correction (Brill and Moore, 2000), have been key problems in natural language processing (NLP) since the field began. They’re simpler than non-monotonic tasks like machine translation, which makes them great for testing how well different technologies are doing. Unlike earlier work that usually looked at just one of these tasks at a time, we’re looking at how models perform across all three. This gives us a more balanced idea of how different models stack up against each other.",
        "formal_text": "Convert casual text to formal text: We're focusing on three specific monotone Seq2Seq tasks—spelling correction, G2P conversion, and lemmatization—to get a more balanced comparison"
    },
    {
        "casual_text": "The graph convolutional network (GCN), introduced by Kipf and Welling in 2017, is basically a version of the convolutional neural network (CNN) that works with graphs. Imagine you have a graph with n nodes. You can show the connections between these nodes using an n x n adjacency matrix A. In this matrix, A_ij equals 1 if there's a link from node i to node j. In a GCN with L layers, if h_i is the output vector for node i at the l-th layer, the graph convolution operation can be described as...",
        "formal_text": "Convert casual text to formal text: The graph convolutional network (GCN), introduced by Kipf and Welling in 2017, is basically a version of the convolutional neural network (CNN) that works with graphs"
    },
    {
        "casual_text": "In Section 3.1, the semantic similarity algorithms work with the noun part of WordNet. As mentioned earlier, our method for summarizing dialogues involves calculating semantic similarity for a specific pair U tt n, D. To make this happen, we need a WordNet-based representation of both U tt n (CR U ttn) and D (CR D), and then we compare them using semantic similarity measures. So, we take the nouns from the utterances, link them to their WordNet senses, and work with those representations in the next steps. The results of this process are in Table 2. The number in the last column shows the specific WordNet sense that was disambiguated. The final dialogue representation, CR D, is basically a collection of concepts from adding up the individual utterance representations. It looks like this: CR D = home, home, sixties, pier, beam, house, bedrooms, bath, area, Houston.",
        "formal_text": "Convert casual text to formal text: In Section 3.1, the semantic similarity algorithms work with the noun part of WordNet. As mentioned earlier, our method for summarizing dialogues involves calculating semantic similarity for a"
    },
    {
        "casual_text": "LRR, developed by Wang and colleagues in 2010, is a probabilistic graphical model—not a neural one—that treats aspect-level sentiments as hidden variables. It assumes that the overall sentiment of a document comes from a mix of these hidden aspect sentiments, with each one weighted differently. The cool part is that LRR only needs document-level labels to work with.",
        "formal_text": "Convert casual text to formal text: LRR, developed by Wang and colleagues in 2010, is a probabilistic graphical model—not a neural one—that treats aspect-level sentiments as hidden variables. It assumes that the"
    },
    {
        "casual_text": "q = p + l Okay, so at the end of span 5, we do this: for k starting at p and going up to q - 1, we check something. Then, in span 6, we're doing a partition. If the items aren't consecutive, we skip to the next loop. On line 7, we update [, ] with ( ). Finally, on line 8, we calculate curCost by adding up a few things: , +, + [ + 1, ].",
        "formal_text": "Convert casual text to formal text: q = p + l Okay, so at the end of span 5, we do this: for k starting at p and going up to q - 1, we check something."
    },
    {
        "casual_text": "Based on previous research, we're sticking with four main roles for the event structure: ARG0, ARG1, TIME, and LOC. But here's the twist: during the pre-processing stage, we're adding a rule that makes sure TIME entities only go into the TIME role and LOC entities only go into the LOC role.",
        "formal_text": "Convert casual text to formal text: Based on previous research, we're sticking with four main roles for the event structure: ARG0, ARG1, TIME, and LOC. But here's the twist: during the"
    },
    {
        "casual_text": "Some folks have tried using different metrics for MT tuning, but the results have been kind of hit or miss. One notable example is Padó et al. (2009), who claimed their entailment-based metric led to better human evaluations. The catch? It’s super heavy and slow in real-world use. They estimated it would take 40 days to run on the NIST MT 2002/2008 dataset, so they had to split the process into two phases and use a smaller n-best list. In our case, we’re working with the WMT 2010 dataset, which is similar in size, and most of our runs finish in less than a day. Meanwhile, Cer et al. (2010) tested tuning a phrase-based SMT system with metrics like BLEU, NIST, METEOR, and TER. Even though METEOR and TER have been shown to align more closely with human judgments, they found that BLEU and NIST are still the go-to options for MT tuning.",
        "formal_text": "Convert casual text to formal text: Some folks have tried using different metrics for MT tuning, but the results have been kind of hit or miss. One notable example is Padó et al. (2009), who claimed their"
    },
    {
        "casual_text": "Our main idea is that during training, we can slightly adjust word embeddings by adding info from another source that focuses on meaning. To guide the embeddings toward actual similarity, we use the MyThes thesaurus made by the OpenOffice.org project. It has synonyms for around 80,000 English words. For guiding them toward relatedness, we use the USF free association norms (thanks to Nelson et al., 2004). This dataset has scores for free association, which is a way to measure how words are linked in our minds, for over 10,000 concept words. As for the text data, we use a big chunk of English Wikipedia and some newswire text, totaling about 8 billion words.",
        "formal_text": "Convert casual text to formal text: Our main idea is that during training, we can slightly adjust word embeddings by adding info from another source that focuses on meaning. To guide the embeddings toward actual similarity,"
    },
    {
        "casual_text": "We use the Genie library (Campagna et al., 2019) for stuff like synthesis and data augmentation. For building our models, we went with Huggingface (Wolf et al., 2019) and GenieNLP 2.",
        "formal_text": "Convert casual text to formal text: We use the Genie library (Campagna et al., 2019) for stuff like synthesis and data augmentation. For building our models, we went with Huggingface (W"
    },
    {
        "casual_text": "When it comes to the two methods, they’re pretty much the same in terms of overall accuracy. But if you dig into individual words, you’ll notice that the part-of-speech tagging is different in 25 cases. Out of those: - In 10 cases, Method 1 gets it right. - In 9 cases, Method 2 gets it right. - In 6 cases, both methods mess it up. So, the takeaway is that treating pauses as markers works better when the pause signals a break or a big phrase boundary. On the other hand, it’s better to ignore pauses when they don’t really mark any grammatical break. The problem is, both types of pauses seem to show up about the same amount, so neither method gives a clear advantage overall. But here’s the thing: early signs suggest that if we look at pauses more closely—like how long they are—we might get better results. Of course, this would only work if the transcriptions have that kind of detail.",
        "formal_text": "Convert casual text to formal text: When it comes to the two methods, they’re pretty much the same in terms of overall accuracy. But if you dig into individual words, you’ll notice that the part-of-s"
    },
    {
        "casual_text": "These methods really rely on input parse trees and are super sensitive to any parsing mistakes. But the approach we’re talking about in this paper? It doesn’t need to know where the head word is and isn’t as easily thrown off by parsing errors. Plus, the number of parameters it uses doesn’t depend on how big the branching size can get.",
        "formal_text": "Convert casual text to formal text: These methods really rely on input parse trees and are super sensitive to any parsing mistakes. But the approach we’re talking about in this paper? It doesn’t need to know"
    },
    {
        "casual_text": "The rest of this paper is laid out like this: Section 2 explains the terms we’ll be using. Section 3 gives a quick overview of how our system is set up across different parts. Section 4 talks about the MDP model, and Section 5 covers the policy optimization algorithm. Then, in Section 6, we go over the experiments we did and the results we got. After that, Section 7 looks at some ways we could make things better, and Section 8 wraps it all up.",
        "formal_text": "Convert casual text to formal text: The rest of this paper is laid out like this: Section 2 explains the terms we’ll be using. Section 3 gives a quick overview of how our system is set up across different parts."
    },
    {
        "casual_text": "We’re using a TRIPS-style breakdown (from Allen, Ferguson, and Stent in 2001) to divide dialogue management into three parts: input management, behavior management, and output management (check out Figure 1 for reference). But in the December 2002 Checklist architecture, things are a bit different. There are multiple behavior agents, each focusing on a specific dialogue task, like dealing with annotations (e.g., pictures or voice notes), adjusting system settings (e.g., volume), or handling procedure-based tasks (e.g., navigation). The dialogue input manager is the one that keeps everything in sync by coordinating the interactions between these different behavior agents. Figure 1: December 2002 Checklist architecture.",
        "formal_text": "Convert casual text to formal text: We’re using a TRIPS-style breakdown (from Allen, Ferguson, and Stent in 2001) to divide dialogue management into three parts: input management, behavior management, and output management ("
    },
    {
        "casual_text": "Wow, this looks like a bunch of random symbols and characters! It seems like it might be some kind of code or encrypted message. If you're trying to figure out what it means, you might need to decode it or find the key to unlock its meaning. Maybe it's part of a puzzle or a secret message? If you have any more context or clues, that could help crack it!",
        "formal_text": "Convert casual text to formal text: Wow, this looks like a bunch of random symbols and characters! It seems like it might be some kind of code or encrypted message. If you're trying to figure out what it means, you"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way: - **v_q** represents the query vector. - **v_s** represents the sentence vector. - **LexCL** is used to compare the value of the word in **v_q** with the word in **v_s**. - **10best** means finding the top 10 sentences that match **v_q** based on cosine similarity between **v_q** and **v_s**. - **context** refers to the context of **v_q** and **v_s**. - **grammar** refers to the grammar of **v_q** and **v_s**. - **LexQL** is used to find the best match (**1best**) between **v_q** and **v_s**.",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in a simpler way: - **v_q** represents the query vector. - **v_s** represents the sentence vector. - **"
    },
    {
        "casual_text": "This year, we’ve got something new: a shared task on predicting eye-tracking data for English. We accepted 10 papers describing different systems for this task. Being able to model gaze features well is super important for understanding how people process language. So, we challenged teams to predict eye-tracking metrics at the token level while people were reading naturally. The teams mainly used two methods to make their predictions: (1) tree-based boosting algorithms with lots of feature engineering, and (2) neural networks for regression, like fine-tuning transformer models. The features they used to train their systems included basic text features, lexical and syntactic stuff, token probabilities, text complexity measures, and representations from models like BERT, RoBERTa, and XLNet. The team that won took a linguistic feature-based approach.",
        "formal_text": "Convert casual text to formal text: This year, we’ve got something new: a shared task on predicting eye-tracking data for English. We accepted 10 papers describing different systems for this task. Being able to"
    },
    {
        "casual_text": "Over the past fifty years, summarization research has come a long way (Nenkova and McKeown, 2011). Extractive summarization, which focuses on pulling out the most important and non-repetitive sentences from a document, has gotten a lot of attention compared to abstractive summarization. It’s easier to define and work with. Both unsupervised and supervised methods have been used to pick these sentences. Supervised methods include stuff like the Bayesian classifier (Kupiec et al., 1995), maximum entropy (Osborne, 2002), skip-chain CRF (Galley, 2006), and discriminative reranking (Aker et al., 2010), among others. The problem of selecting sentences for an extractive summary can also be tackled using optimization techniques. Previous approaches have used integer linear programming (ILP) and submodular functions to solve this kind of optimization problem (Gillick et al., 2009; Li et al., 2013b; Lin and Bilmes, 2010).",
        "formal_text": "Convert casual text to formal text: Over the past fifty years, summarization research has come a long way (Nenkova and McKeown, 2011). Extractive summarization, which focuses on pulling out the most"
    },
    {
        "casual_text": "For every task, we tried out a bunch of classifiers from Huggingface (Wolf et al., 2020) and picked the best one for each. You can check out the results in Table 3 (the other models' results are pretty similar). The Polyjuice contrast sets show performance differences that match what Gardner et al. found in 2020, even though we used non-expert annotators who just labeled examples instead of creating them like the NLP researchers did.",
        "formal_text": "Convert casual text to formal text: For every task, we tried out a bunch of classifiers from Huggingface (Wolf et al., 2020) and picked the best one for each. You can"
    },
    {
        "casual_text": "We’re using a parsing model based on the stochastic tree-insertion grammar (TIG) model, which Chiang talked about back in 2000. TIG, as explained by Schabes and Waters in 1995, is a kind of simplified version of tree adjoining grammar (Joshi and Schabes, 1997). It works by combining smaller tree pieces called elementary trees using two main operations: substitution and adjunction (check out Figure 3 for a visual). TIG has some specific rules about how adjunction works. Chiang’s model takes this a step further by adding a third operation called sister-adjunction (also in Figure 3), which is inspired by D-tree substitution grammar (Rambow et al., 1995). There’s a key difference between derived trees and derivation trees (you can see this in Figure 3). A derivation tree keeps track of the steps used to combine elementary trees into a final derived tree. So, there’s a many-to-one relationship between them: each derivation tree leads to one derived tree, but a single derived tree can come from multiple different derivation trees.",
        "formal_text": "Convert casual text to formal text: We’re using a parsing model based on the stochastic tree-insertion grammar (TIG) model, which Chiang talked about back in 2000. TIG, as explained"
    },
    {
        "casual_text": "We took a list of first names from Tzioumis (2018) and matched it up with data from the U.S. Social Security Administration (SSA) to look at how often these names show up in language models. To get a better understanding of these names in context, we pulled examples from the Reddit data collected by Baumgartner et al. in 2020. We also looked at how often these names appear in the training data of each model to get some stats.",
        "formal_text": "Convert casual text to formal text: We took a list of first names from Tzioumis (2018) and matched it up with data from the U.S. Social Security Administration (SSA) to look at how often these names"
    },
    {
        "casual_text": "At the very top of the system, there are programs that handle the user interface. This part is kept separate from the translation layer (which only runs on internal machines) by a firewall. The user interface is basically a web page where you can enter a translation request—just a sentence and the languages you want it translated between—using an HTML form. When you submit the form, a cgi script takes over and manages the interaction with the machine translation (MT) front-end. In Figure 1, you can see the overall setup. For each language pair, there’s a group of programs (including the MT decoder) that actually do the translation work. When you make a request on the web page, the cgi script sends it to the MT front-end. Then, the translation happens on the specific language-pair service, and the result gets sent back to your web browser.",
        "formal_text": "Convert casual text to formal text: At the very top of the system, there are programs that handle the user interface. This part is kept separate from the translation layer (which only runs on internal machines) by a firewall. The user"
    },
    {
        "casual_text": "This paper suggests tackling this issue by tweaking the Recognizing Textual Entailment (RTE) framework to better handle metaphor interpretation. RTE is designed to be a broad, general-purpose task that covers a lot of the semantic reasoning needed across different applications (Dagan et al., 2007). It's seen as super important for building smart but reliable natural language processing systems because most of the semantic reasoning needed in stuff like question answering and information extraction can be boiled down to RTE problems (Dagan et al., 2007). Basically, textual entailment is about figuring out if a hypothesis can be drawn from a given text. In simpler terms, it's a one-way relationship between two pieces of text—Text T and Hypothesis H—where if T entails H, then anyone reading T and looking at H would think H naturally follows from T. For example, take example 1560 from the RTE-1 dataset, which uses the metaphor 'incubate'.",
        "formal_text": "Convert casual text to formal text: This paper suggests tackling this issue by tweaking the Recognizing Textual Entailment (RTE) framework to better handle metaphor interpretation. RTE is designed to be a broad, general"
    },
    {
        "casual_text": "Okay, so we're talking about a 2  500 English to German model here. The word embeddings make up about 63% (that's 50 million out of 84 million) of the total parameters. But the size of these embeddings doesn't really affect how fast the model runs because the word embedding layer is just a simple lookup table that only impacts the first layer of the model. So, we're more focused on cutting down the memory usage of the student models by pruning weights. Weight pruning for NMT (Neural Machine Translation) was looked into by See et al. (2016), and they found that you can prune up to 80-90% of the parameters in a big NMT model without losing much performance. So, we took our best English to German student model (the 2  500 Seq-KD + Seq-Inter one) and pruned x% of its parameters by getting rid of the weights with the smallest absolute values. After that, we retrained the pruned model using Seq-KD data with a learning rate of 0.2 and fine-tuned it with Seq-Inter data at a learning rate of 0.1. As See et al. (2016) pointed out, retraining was super important. The results are in Table 3, so check that out for the details.",
        "formal_text": "Convert casual text to formal text: Okay, so we're talking about a 2  500 English to German model here. The word embeddings make up about 63% (that's 50 million out of 84"
    },
    {
        "casual_text": "Our starting point is the edit machine mentioned in (Bangalore and Johnston, 2004). It’s basically a finite-state version of the algorithm used to calculate the Levenshtein distance. This tool lets you insert, delete, or replace any word with another without any limits (check out Figure 5). The costs for inserting, deleting, or substituting words are all the same, but there’s a catch: words that belong to certain classes—like prices (e.g., \"cheap,\" \"expensive\") or cuisines (e.g., \"Turkish\")—cost more to delete or replace.",
        "formal_text": "Convert casual text to formal text: Our starting point is the edit machine mentioned in (Bangalore and Johnston, 2004). It’s basically a finite-state version of the algorithm used to calculate the Leven"
    },
    {
        "casual_text": "Item Response Theory (IRT), which was developed by Lord and others in 1968 and later expanded by Baker in 2001 (you can find a review in section 2), is a popular method in educational testing. It's often used instead of just relying on basic summary stats, which were introduced by Edgeworth way back in 1888.",
        "formal_text": "Convert casual text to formal text: Item Response Theory (IRT), which was developed by Lord and others in 1968 and later expanded by Baker in 2001 (you can find a review in section 2), is a popular method in"
    },
    {
        "casual_text": "Alright, so in Section 7.1, we saw that our models work pretty well when we test them on a different dataset. A cool next step would be to mix these datasets together and see how much better our models get when they learn from both at the same time. We’re also planning to look into combining tasks like finding entities, figuring out their types, and recognizing experiment frames all at once. Plus, there are other NLP tasks we can explore with our dataset. For example, we could treat experiment descriptions as events and try to detect main events and smaller sub-events within them. Another idea is to dive deeper into how we analyze experiment descriptions using frame semantics. Instead of sticking to the idea that each sentence has one experiment or each experiment is in one sentence, we could try modeling things more flexibly, like using graph structures based on how the data is actually annotated.",
        "formal_text": "Convert casual text to formal text: Alright, so in Section 7.1, we saw that our models work pretty well when we test them on a different dataset. A cool next step would be to mix these datasets together and see"
    },
    {
        "casual_text": "Even though the ROUGE scores were lower, people actually rated the news summaries higher. This makes us think that ROUGE might work better for judging news summaries than dialogue ones. So, we reckon it's time to come up with a new way to measure how good abstractive dialogue summaries are.",
        "formal_text": "Convert casual text to formal text: Even though the ROUGE scores were lower, people actually rated the news summaries higher. This makes us think that ROUGE might work better for judging news summaries than dialogue"
    },
    {
        "casual_text": "Structured classification methods that use sequential information have been used for tasks like tagging key sentences in biomedical fields (Chung, 2009) and labeling posts in online forums (Kim et al., 2010). When it comes to live chats, we noticed that where an utterance is placed in the chat or within a turn is pretty important for figuring out its dialogue act. For instance, something like \"Hello\" usually shows up at the start of a chat, while \"Have a nice day\" tends to be at the end. The position of an utterance in a turn can also give clues about its dialogue act. If there are multiple utterances in a turn, they’re usually connected, so looking at the previous ones can help predict the next one. For example, a greeting like \"Welcome to...\" followed by a question like \"How may I help you?\" could be part of a dialogue act. Here’s a quick breakdown of some dialogue acts: - **CONVENTIONAL CLOSING**: Ways to end a conversation, like \"Bye Bye.\" - **CONVENTIONAL OPENING**: Ways to start a conversation, like \"Hello Customer.\" - **DOWNPLAYER**: A label often used after \"THANKS\" to downplay the contribution, like \"You are welcome, my pleasure.\" - **EXPRESSIVE**: A reaction to a previous utterance or showing the speaker's mood, like \"haha,\" \":) wow.\" - **NO ANSWER**: A negative response to a yes/no question, like \"No.\"",
        "formal_text": "Convert casual text to formal text: Structured classification methods that use sequential information have been used for tasks like tagging key sentences in biomedical fields (Chung, 2009) and labeling posts in online forums (Kim"
    },
    {
        "casual_text": "So, the system would basically remove any parts of the text that have the word you're looking for, and then you could keep scrolling through the updated display.",
        "formal_text": "Convert casual text to formal text: So, the system would basically remove any parts of the text that have the word you're looking for, and then you could keep scrolling through the updated display."
    },
    {
        "casual_text": "Sure! Here's a more casual version: If you have something like t[  t], it just means t. But if you see f(t1, ..., tn)[(i • p)  t], it means you take f(t1, ..., tn) and replace the i-th part with t after applying p to it. This works as long as i is between 1 and n.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: If you have something like t[  t], it just means t. But if you see"
    },
    {
        "casual_text": "The main point here is that instead of using text, we're defining U as a series of speech units that are super strong and efficient, kind of like text, but we figured them out without needing text to guide us. We set up the S2U model so that when we want to make a guess, we can say U = f(S), which means we can turn any speech audio S into a sequence of units U. By adding this third part, we can train P(U|I) using a bunch of images with spoken captions.",
        "formal_text": "Convert casual text to formal text: The main point here is that instead of using text, we're defining U as a series of speech units that are super strong and efficient, kind of like text, but we figured them"
    },
    {
        "casual_text": "Also, we noticed that the scores aren’t great, especially for the information extraction task. This might be because the audience laughter really depends on the conversation context, which the baseline models didn’t take into account. So, looking at longer conversation contexts for each line could be a cool research idea. Now, about Tables 7 and 8, they talk about the styles in the training data. Here’s what we did: since the six comedians in the dataset have pretty unique comedy styles, we split the whole dataset into 6 parts for cross-validation. Each part has jokes from just one main comedian. Then, we trained the baseline models on five of these parts and tested them on the last one. Tables 9 and 10 show the average results, and you can find all the detailed results in the appendix. The cool thing is, even though the training and testing data had very different comedy styles, the models were still able to pick up on what makes people laugh.",
        "formal_text": "Convert casual text to formal text: Also, we noticed that the scores aren’t great, especially for the information extraction task. This might be because the audience laughter really depends on the conversation context, which the baseline models didn’t"
    },
    {
        "casual_text": "Alright, so we only need zero additions and mk multiplications for each part. Also, W_u is a sparse matrix, and if we use a p-nearest neighbor graph, each row of W_u has, on average, p nonzero elements. That means we only need mpk additions and mpk multiplications to calculate W_u U. The same goes for W_v V—it needs the same number of operations as W_u U. Now, if the multiplicative updates stop after Iter iterations, the time cost for these updates is O(Iter  mnk). So, the overall time for GNMTF is pretty similar to standard NMTF and CNMTF. Here's a quick breakdown of the operations and time for GNMTF: - Addition, multiplication, division, overall GNMTF: U 2k3 + (2m + n)k2 + m(n + p)k - 2k3 + (2m + n)k2 + m(n + p + 7)k - mk - O(mnk) GNMTF: H 2k3 + (m + n + 2)k2 + mnk",
        "formal_text": "Convert casual text to formal text: Alright, so we only need zero additions and mk multiplications for each part. Also, W_u is a sparse matrix, and if we use a"
    },
    {
        "casual_text": "A bunch of researchers (like Palmer et al., 2004; Navigli, 2006; Snow et al., 2007) have been working on simplifying sense inventories for Word Sense Disambiguation (WSD). They want to make these inventories less detailed to improve performance, since having too many specific senses can cause problems. Our approach is similar in that we also simplify things by reducing all the senses of a word to just two (S/O). But the way we group them is different. Other studies focus on grouping senses based on how similar they are in terms of syntax or meaning. We, on the other hand, group them based on subjectivity, with a particular focus on subjectivity and sentiment analysis.",
        "formal_text": "Convert casual text to formal text: A bunch of researchers (like Palmer et al., 2004; Navigli, 2006; Snow et al., 2007) have been working on simplifying sense inventories for Word"
    },
    {
        "casual_text": "Alright, so W and b are the trainable parameters, and  means elementwise multiplication. Now, let's talk about CAN-GRUA: The thing is, when there are too many sentences in a conversation, it gets tricky to understand how they all connect over time. Basically, the current sentence struggles to pick up on the info from way back in the conversation. To fix this, we add an attention layer on top of the GRU to figure out how much past info affects the current sentence's emotion. If the attention mechanism gives a high weight, it means that earlier sentence is super important for the current one, so we should pay extra attention to it.",
        "formal_text": "Convert casual text to formal text: Alright, so W and b are the trainable parameters, and  means elementwise multiplication. Now, let's talk about CAN-GRUA: The thing is, when"
    },
    {
        "casual_text": "In the examples mentioned, the start condition is like a precondition, the continuation termination condition is a postcondition, the ongoing behavior condition is an ongoing condition, and the result condition is also a postcondition. In Japanese procedural texts, they usually leave out the language about who's doing the actions and often describe more than one action in a single sentence.",
        "formal_text": "Convert casual text to formal text: In the examples mentioned, the start condition is like a precondition, the continuation termination condition is a postcondition, the ongoing behavior condition is an ongoing condition, and the result condition is also"
    },
    {
        "casual_text": "We used a mix of different datasets to finetune Polyjuice. One of them is the Contrast set. The creators of 10 existing NLP datasets each messed with 100-1,000 examples to flip the correct label, just to see how a model handles changes near its decision boundary (Gardner et al., 2020). The ways they changed the examples varied depending on the task and who was doing the editing, which helps us learn a bunch of different strategies. To make sure we could use the Contrast set to test the Sentiment model, we left out the IMDb movie review data during training.",
        "formal_text": "Convert casual text to formal text: We used a mix of different datasets to finetune Polyjuice. One of them is the Contrast set. The creators of 10 existing NLP datasets each messed"
    },
    {
        "casual_text": "Got it! Just so you know, there are some pairs of types where you can't find a set of sources like that. For example, the apply set.",
        "formal_text": "Convert casual text to formal text: Got it! Just so you know, there are some pairs of types where you can't find a set of sources like that. For example, the apply set."
    },
    {
        "casual_text": "Alright, so  is picked depending on the punctuation in it 7. To compare, we also made two actual sequence groups: the first sentence set and the last sentence set. These include the first and last sentences from the articles in the WikiText validation and testing splits.",
        "formal_text": "Convert casual text to formal text: Alright, so  is picked depending on the punctuation in it 7. To compare, we also made two actual sequence groups: the first sentence set and the last sentence set. These include the"
    },
    {
        "casual_text": "Since these specific cases are already there, it's totally fine to use them as extra labels along with the usual ones, especially the neutral case. People like Teubert (1979) have noticed that a lot of nouns have what we call 'valency patterns', and others like Piha (1980) and Mackenzie (1983) have looked at this using a Case approach.",
        "formal_text": "Convert casual text to formal text: Since these specific cases are already there, it's totally fine to use them as extra labels along with the usual ones, especially the neutral case. People like Teubert (1979) have noticed that"
    },
    {
        "casual_text": "Also, some sentences talk about more than one thing. Usually, in these sentences, the parts about each thing don't overlap. So, we use something called orthogonal regularization (Lin et al., 2017; Hu et al., 2018) to help the attention focus on each part separately and really well.",
        "formal_text": "Convert casual text to formal text: Also, some sentences talk about more than one thing. Usually, in these sentences, the parts about each thing don't overlap. So, we use something called orthogonal regularization (L"
    },
    {
        "casual_text": "We also tried out fin-bert, which is a bert model designed for financial stuff. It worked better with our special tokens. In the end, we trained and shared our own bert model for a specific domain, called sec-bert. We released it with and without the special tokens, and it performed the best with the tokens, without needing those expensive crf layers.",
        "formal_text": "Convert casual text to formal text: We also tried out fin-bert, which is a bert model designed for financial stuff. It worked better with our special tokens. In the end, we trained and shared our own bert"
    },
    {
        "casual_text": "I'm curious to hear if the discussants think, like some linguists did a decade ago, that you could just stick a couple of the top linguists from a few countries in a room for four or five years, and somehow they'd crack \"the machine translation problem.\" They'd whip up a flow chart, program it into a machine, and boom—it would work like a charm.",
        "formal_text": "Convert casual text to formal text: I'm curious to hear if the discussants think, like some linguists did a decade ago, that you could just stick a couple of the top linguist"
    },
    {
        "casual_text": "Sure! Here's the informal version: \"The NLP crowd has been pretty focused on stuff like sentiment, emotions, feelings, and personality traits. But when it comes to things like trust, anxiety, or how people view their own literacy, there hasn't been much work done yet.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: \"The NLP crowd has been pretty focused on stuff like sentiment, emotions, feelings, and personality traits. But when it comes to things like trust, anxiety"
    },
    {
        "casual_text": "In this part, we're looking at how the words in the MPQA database are spread out. The list of words we have covers a good chunk of the feelings and opinions in the database: 67.1% of those feelings and opinions have at least one word from our list.",
        "formal_text": "Convert casual text to formal text: In this part, we're looking at how the words in the MPQA database are spread out. The list of words we have covers a good chunk of the feelings and opinions in the database:"
    },
    {
        "casual_text": "The objective function is basically about regular language modeling, where we're trying to minimize something called E l. This E l represents the negative log likelihood of the current word in a sequence, based on the words that came before it.",
        "formal_text": "Convert casual text to formal text: The objective function is basically about regular language modeling, where we're trying to minimize something called E l. This E l represents the negative log likelihood of the current word in a sequence,"
    },
    {
        "casual_text": "No big deal, but you can easily check that this equation hits its lowest point when XTX = I. Basically, that happens when the topics are all lined up nice and neat, like they're standing in a straight line, not overlapping.",
        "formal_text": "Convert casual text to formal text: No big deal, but you can easily check that this equation hits its lowest point when XTX = I. Basically, that happens when the topics are all lined up nice and neat, like"
    },
    {
        "casual_text": "ci = BiLSTM(ci) ei = BiLSTM(ei) coni = BiLSTM(coni) xi = [ci; ei]",
        "formal_text": "Convert casual text to formal text: ci = BiLSTM(ci) ei = BiLSTM(ei) coni = BiLSTM(coni) xi = [ci;"
    },
    {
        "casual_text": "We show that the method we came up with works better than the other options, scoring higher on different tests. To sum it up, the paper has three main points:",
        "formal_text": "Convert casual text to formal text: We show that the method we came up works better than the other options, scoring higher on different tests. To sum it, the paper has three main points: Convert casual text to formal text: We"
    },
    {
        "casual_text": "So, we adjusted the input features to fit within a \"safe\" range. We also tried other scaling methods, like normalization, but they didn’t really make a big difference in how well the model performed. So, we just went with the clipping method because it’s quicker and gets the job done.",
        "formal_text": "Convert casual text to formal text: So, we adjusted the input features to fit within a \"safe\" range. We also tried other scaling methods, like normalization, but they didn’t really make a big difference in how"
    },
    {
        "casual_text": "There are a bunch of issues with word frequencies and biases in the data we use. But let’s be real, some words are just gonna show up way more often than others, no matter what.",
        "formal_text": "Convert casual text to formal text: There are a bunch of issues with word frequencies and biases in the data we use. But let’s be real, some words are just gonna show up way more often than others,"
    },
    {
        "casual_text": "One issue is figuring out which symbols to place in the starting space. In books and by authors, they took on this role. The other issue is deciding on the criteria for how the space should be arranged. We based it on how satisfied a user was with the answers they got from an information retrieval system. If we can solve these two problems, our method could work for any field. These problems don't seem too tough.",
        "formal_text": "Convert casual text to formal text: One issue is figuring out which symbols to place in the starting space. In books and by authors, they took on this role. The other issue is deciding on the criteria for how the space should"
    },
    {
        "casual_text": "scope    trigger.    12  scope       trigger  bAqtnE (: )      Aktr wAktr (:  )  .",
        "formal_text": "Convert casual text to formal text:       scope    trigger.    12  scope"
    },
    {
        "casual_text": "In 2019, during the fourth Conference on Machine Translation (WMT), the shared task about filtering parallel corpora was all about dealing with low-resource situations. The top-performing submission used crosslingual sentence embeddings, which were trained using parallel sentence pairs (as mentioned by Chaudhary et al. in 2019). Artetxe and Schwenk (2019a) came up with a similar approach. Both papers addressed the limitations of cosine similarity by looking at the surrounding sentences, which helped them beat systems that only used cosine similarity.",
        "formal_text": "Convert casual text to formal text: In 2019, during the fourth Conference on Machine Translation (WMT), the shared task about filtering parallel corpora was all about dealing with low-resource situations. The top-performing submission used cross"
    },
    {
        "casual_text": "We've pinpointed and included time offsets for various elements like speech, subtitles, text on graphics and scenes, body movements, gestures, shots (with a focus on foreground and background), and keyframe regions in the COSMOROE relations. All the visual stuff has been tagged by annotators using simple one or two-word labels that describe actions or entities. These tags came from just watching the video without any audio. The tagging process was based on a cognitive categorization method, which uses the \"basic level theory\" of categorization (Rosch, 1978). Right now, the annotated data includes 5 hours of Greek travel documentaries and 5 hours of English ones. Out of the Greek content, 3 hours have been validated, and we've also done a preliminary study on inter-annotator agreement (Pastra, 2008).",
        "formal_text": "Convert casual text to formal text: We've pinpointed and included time offsets for various elements like speech, subtitles, text on graphics and scenes, body movements, gestures, shots (with a focus on fore"
    },
    {
        "casual_text": "Right now, the two subgraphs we’ve made are completely separate. The KB graph has nodes for entities, and the surface relation graph has nodes for noun phrases. There are no connections between the noun phrases and the entities. To link these two graphs, we use the ALIAS relation in the KB, which connects entities to possible noun phrase references. Basically, each noun phrase in the surface relation graph gets connected to the entity nodes it could refer to, based on the KB. This isn’t like an entity linking system, which Lao et al. (2012) used. Instead, these connections just show that the noun phrase *could* refer to the KB entity. Using an entity linking system would make the connections stronger, but it would take a lot more work upfront and create a much bigger graph, because every mention of a noun phrase would need its own node. In contrast, our approach lets all mentions of the same noun phrase share one node. This setup lets us add tens of millions of surface relations to a graph with tens of millions of KB relations, and we can handle all the processing on just one machine.",
        "formal_text": "Convert casual text to formal text: Right now, the two subgraphs we’ve made are completely separate. The KB graph has nodes for entities, and the surface relation graph has nodes for noun phrases. There are"
    },
    {
        "casual_text": "So far, skimming has made things faster without messing up the accuracy. But don't get it twisted—the shortcuts we're using aren't perfect. There are definitely times when skipping parts of the text can mess things up, especially when the text has stuff like missing words, pronouns, or really complicated sentence structures. We need to work on making our rules smarter for these tricky situations and, when needed, just go back to fully analyzing the text.",
        "formal_text": "Convert casual text to formal text: So far, skimming has made things faster without messing up the accuracy. But don't get it twisted—the shortcuts we're using aren't perfect. There are"
    },
    {
        "casual_text": "Old-school chatbots used to rely on human-made rules to give emotional support. A newer system, though, uses a rule-based method to figure out the kind of supportive thing to say and then picks the right response from a list of options (Medeiros and Bosse, 2018). There’s also this other system made to help people deal with COVID-19 by spotting what users are talking about and then responding with something matching from a template or a set list of messages. Not many studies have really dug into creating supportive responses, and the ones that have didn’t go too deep. For instance, one looked at how to generate supportive replies by reflecting on what the user said.",
        "formal_text": "Convert casual text to formal text: Old-school chatbots used to rely on human-made rules to give emotional support. A newer system, though, uses a rule-based method to figure out the kind of supportive"
    },
    {
        "casual_text": "Patents are organized based on their subject matter, and this is done using something called the International Patent Classification (IPC). The IPC breaks patents down into a big hierarchy: starting with 8 main sections, then 120 classes, 600 subclasses, and finally, around 70,000 subgroups at the very bottom. Table 1 gives you an overview of those 8 top-level sections.",
        "formal_text": "Convert casual text to formal text: Patents are organized based on their subject matter, and this is done using something called the International Patent Classification (IPC). The IPC breaks patents down into a big hierarchy: starting"
    },
    {
        "casual_text": "The platform uses widgets, which are basically the interactive elements provided by the programming language's toolkit, like Swing or AWT for Java. In this project, we’re working with interactors from HTML Forms, such as text boxes, combo boxes, popup menus, buttons, checkboxes, and radio buttons, as well as HTML tags. We also had to create more advanced interactors by combining HTML Forms and HTML Tags.",
        "formal_text": "Convert casual text to formal text: The platform uses widgets, which are basically the interactive elements provided by the programming language's toolkit, like Swing or AWT for Java. In this project, we’re working with"
    },
    {
        "casual_text": "Lastly, we realized that training our QA model with extra made-up unanswerable questions really helped. We did this by mixing up the dataset—basically, we took each question and randomly paired it with a paragraph from a totally different part of the dataset. Since the context didn’t match, we treated these as unanswerable questions. Unless we say otherwise, all the experiments in this paper use this extra data to get better at spotting unanswerable queries.",
        "formal_text": "Convert casual text to formal text: Lastly, we realized that training our QA model with extra made-up unanswerable questions really helped. We did this by mixing up the dataset—basically, we took each question and"
    },
    {
        "casual_text": "When it comes to sentences with multiple events, the suggested solution can not only spot and categorize these events but also connect the right arguments to each event, which you can see in Table 3 and Table 4.",
        "formal_text": "Convert casual text to formal text: When it comes to sentences with multiple events, the suggested solution can not only spot and categorize these events but also connect the right arguments to each event, which you can see in Table 3 and Table"
    },
    {
        "casual_text": "We put our framework up against a bunch of other methods to see how it stacks up. (Shi and Lin, 2019) is a well-known approach for semantic role labeling, specifically for predicate-argument prediction. It uses features pulled from BERT (Devlin et al., 2019) and applies Conditional Random Fields (Lafferty et al., 2001) for structured prediction, which we’ll call BERT-CRF. Then there’s Li et al. (2021), who suggest using a conditional neural text generation model for document-level argument extraction. This method deals with each event on its own, so we’ll refer to it as BART-Gen. For our part, we’ve got a memory-enhanced training approach that uses extra context we’ve pulled in, and we’re calling that Memory-based Training. We also ran tests with argument pairs and constrained decoding to see how each piece contributes. In Table 3, you’ll find the main results for document-level informative argument extraction. The scores for argument identification are higher than for classification because identifying just requires matching the span offsets. Here’s what we noticed:",
        "formal_text": "Convert casual text to formal text: We put our framework up against a bunch of other methods to see how it stacks up. (Shi and Lin, 2019) is a well-known approach for semantic role labeling,"
    },
    {
        "casual_text": "Let’s start by looking at the results for caption generation, using the usual metrics: BLEU-4, ROUGE, METEOR, and CIDEr. These are all listed in Table 8. We averaged the results from three runs, and the biggest differences we saw across our models for BLEU, ROUGE, METEOR, and CIDEr were 0.013, 0.019, 0.016, and 0.069, respectively. This shows our results are pretty stable, and our method is making some solid improvements. For the GoodNews dataset, JoGANIC (auto) did way better than Tell, improving by 0.89, 0.95, 1.04, and 10.69 points on those four metrics. The full model, JoGANIC+MSTR+NEE (auto), did even better, with improvements of 1.38, 2.35, 1.51, and 12.72. The CIDEr score saw the biggest jump, which is pretty cool. JoGANIC outperformed all the two-step captioning methods (the first set of results) and even VGG+LSTM. For the NY-Times800k dataset, we only compared our models to Tell since the other ones didn’t perform as well.",
        "formal_text": "Convert casual text to formal text: Let’s start by looking at the results for caption generation, using the usual metrics: BLEU-4, ROUGE, METEOR, and CIDEr. These are all listed in"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: Basically, we're looking at how often something happens (let's call it \"Occur\") for each pair of things (W_ij). We compare two values for each pair: To(W_ij) and Ro(W_ij). We take the smaller of these two values and multiply it by how often that pair happens. Then, we add up all these results and divide by the total sum of all To(W_ij) values. So, it's like figuring out a ratio where we're focusing on the smaller value and how often it shows up.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: Basically, we're looking at how often something happens (let's call it \"Occur\") for each pair of"
    },
    {
        "casual_text": "You can create a single score by combining the phraseness and informative score together.",
        "formal_text": "Convert casual text to formal text: You can create a single score by combining the phraseness and informative score together."
    },
    {
        "casual_text": "For example, subcategorization tells us if a verb is transitive or not.",
        "formal_text": "Convert casual text to formal text: For example, subcategorization tells us if a verb is transitive or not. Convert casual text to formal text: For example, subcategorization tells us"
    },
    {
        "casual_text": "Okay, let’s say we have two examples, r and r′, and m is the size of the relation embeddings. To find the Euclidean distance, d(r, r′), here’s how you do it:",
        "formal_text": "Convert casual text to formal text: Okay, let’s say we have two examples, r and r′, and m is the size of the relation embeddings. To find the Euclidean distance,"
    },
    {
        "casual_text": "We asked two questions to figure out how people felt about vaccines. First, we wanted to know if they had already gotten the shot. For those who hadn’t, we asked, \"If a COVID-19 vaccine was available for you, how likely are you to get it?\" Their answers were ranked on a 5-point scale.",
        "formal_text": "Convert casual text to formal text: We asked two questions to figure out how people felt about vaccines. First, we wanted to know if they had already gotten the shot. For those who hadn’t, we asked, \""
    },
    {
        "casual_text": "Hey, just so you know, in the current version of this machine, delinking basically means adding a sort of \"stop\" signal to the feature you want to delink. This idea was talked about by Kalman and Kornai back in 1985, and Kalman also mentioned it in 1986.",
        "formal_text": "Convert casual text to formal text: Hey, just so you know, in the current version of this machine, delinking basically means adding a sort of \"stop\" signal to the feature you want to delink. This idea was"
    },
    {
        "casual_text": "We used Fleiss' kappa to see how well the crowd's evaluation matched up with the oracle and expert judgments for each translation task. The kappa scores are broken down like this (based on Landis and Koch, 1977): - If  is less than 0: \"none\" - If  is 0 to 0.2: \"slight\" - If  is 0.2 to 0.4: \"fair\" - If  is 0.4 to 0.6: \"moderate\" - If  is 0.6 to 0.8: \"substantial\" - If  is 0.8 to 1.0: \"almost perfect\"",
        "formal_text": "Convert casual text to formal text: We used Fleiss' kappa to see how well the crowd's evaluation matched up with the oracle and expert judgments for each translation task. The kapp"
    },
    {
        "casual_text": "Coreference resolution has been a big deal in computational linguistics for a long time. Early on, a lot of work in this area relied heavily on domain and linguistic knowledge (like Carter 1987, Rich and LuperFoy 1988, and Carbonell and Brown 1988). But as people started wanting more robust and affordable solutions, there was a push toward simpler, knowledge-light approaches (Dagan and Itai 1990, Lappin and Leass 1994, Mitkov 1998, Soon, Ng, and Lim 2001, Ng and Cardie 2002). This shift was also fueled by the rise of cheaper, more reliable NLP tools like part-of-speech taggers and shallow parsers, plus the growing availability of corpora and other resources (like ontologies).",
        "formal_text": "Convert casual text to formal text: Coreference resolution has been a big deal in computational linguistics for a long time. Early on, a lot of work in this area relied heavily on domain and linguistic knowledge ("
    },
    {
        "casual_text": "Word learning and readability are pretty closely linked, and that’s not just obvious but also really helpful. It gives us two ways to adjust things—s = 1   1 and r = 1   2—so we can deal with different ideas of what makes something readable. Tests back up this model and show some cool patterns in how words are picked up compared to older studies on oral learning. The results also prove that this model works well at predicting how readable a document is across different datasets. It even beats out methods like naive Bayes and support vector regression, which are usually pretty solid for this kind of thing.",
        "formal_text": "Convert casual text to formal text: Word learning and readability are pretty closely linked, and that’s not just obvious but also really helpful. It gives us two ways to adjust things—s = 1   1 and"
    },
    {
        "casual_text": "In Section 3, we set up a general model framework where the encoder and decoder can be any RNN or Transformer architecture, like the one Vaswani et al. (2017) introduced. In real-world applications, using pre-trained language models like BERT (Devlin et al., 2019) has really boosted performance across a bunch of NLP tasks. For task-oriented semantic parsing, Rongali et al. (2020) hit the top performance using RoBERTa (Liu et al., 2019). Unlike earlier work, we think that just using a pre-trained encoder isn’t the best fit for a seq2seq semantic parser. This is because pairing a pre-trained encoder with a decoder that’s randomly initialized can make training tricky. Instead, we go with BART, a pre-trained seq2seq model, which lets us initialize both the encoder and decoder in our SEQ2SEQ-COPYPTR model.",
        "formal_text": "Convert casual text to formal text: In Section 3, we set up a general model framework where the encoder and decoder can be any RNN or Transformer architecture, like the one Vaswani et al. (2017)"
    },
    {
        "casual_text": "Alright, let's break down construct validity in psychometrics into six parts: 1. **Content**: This is about whether the test actually covers what it's supposed to measure. Like, does the test really reflect the topic or skill it claims to be testing? 2. **Substantive**: Here, we're looking at how well the test relates to the theory behind it. Basically, does the test align with the scientific ideas we have about the thing we're measuring? 3. **Structural**: This one's about how the test is put together. Does the way the questions are structured make sense for what we're trying to measure? 4. **Generalizability**: This is all about whether the test results can be applied to different situations or groups. Can we trust the test to give us consistent results no matter where or who we test? 5. **External**: This looks at how the test results compare to other measures or tests. Does the test match up with what other tests or observations say about the same thing? 6. **Consequential**: Finally, this is about the impact of the test. Does using this test lead to fair and reasonable outcomes, or does it cause any negative consequences?",
        "formal_text": "Convert casual text to formal text: Alright, let's break down construct validity in psychometrics into six parts: 1. **Content**: This is about whether the test actually covers what it's supposed to measure. Like"
    },
    {
        "casual_text": "On average, a verb in the test set could trigger 1.96 different frames. About half of those verb uses (1,522 cases) only had one possible frame, while 22% could go with two frames. There were even 14 cases where a verb could match up to 11 different frames! Oh, and there are 120 verb uses (that's 4%) in the test set where the right frame isn’t marked in any of the sentences from the training set.",
        "formal_text": "Convert casual text to formal text: On average, a verb in the test set could trigger 1.96 different frames. About half of those verb uses (1,522 cases) only had one possible frame, while 22% could go with"
    },
    {
        "casual_text": "How simple or tricky is it to get what the sentence is saying? Doesn't matter if it matches the picture or article well or not.",
        "formal_text": "Convert casual text to formal text: How simple or tricky is it to get what the sentence is saying? Doesn't matter if it matches the picture or article well or not."
    },
    {
        "casual_text": "Okay, so let's break this down in a simpler way: balAPinc(u, v) is basically LIN(u, v) multiplied by APinc(u, v). Now, APinc(u, v) is calculated by taking r times the sum of P(r) multiplied by rel(f r) for all r in F(u).",
        "formal_text": "Convert casual text to formal text: Okay, so let's break this down a simpler way: balAPinc(u, v) is basically LIN(u, v) multiplied by APin"
    },
    {
        "casual_text": "The second class focuses on adding extra info to existing text, like structured knowledge (Zhao et al., 2018; Ghazvininejad et al., 2018; Dinan et al., 2019), personal details (Li et al., 2016b; Zhang et al., 2018a), or emotions (Shen et al., 2017b; Zhou et al., 2018). But getting annotated text like this can be super expensive and is usually only available for specific areas with not much data. Some recent studies have started working on dialogue style transfer using personal speeches or TV scripts (Niu and Bansal, 2018; Gao et al., 2019). The difference with our approach is that we want to make general dialogue generation richer by using lots of non-conversational text, instead of sticking to just one style.",
        "formal_text": "Convert casual text to formal text: The second class focuses on adding extra info to existing text, like structured knowledge (Zhao et al., 2018; Ghazvininejad et al., 2018"
    },
    {
        "casual_text": "We used the -coefficient to measure how well the annotators agreed with each other, which is a pretty standard way to check agreement in tasks where people are assigning categories. The -coefficient is calculated as (P(A) - P(E)) / (1 - P(E)), where P(A) is the actual agreement between the annotators, and P(E) is the chance that they would agree just by luck. The chance agreement for our data was figured out using the method described in Carletta's paper from 1996. But in some earlier work on MT meta-evaluation, Callison-Burch and his team (2007) used a less strict rule, assuming that chance agreement was evenly spread out, like 1/5 for a five-point scale. They also came up with the idea of \"relative\" , which looks at how often two or more judges agreed on whether one output (A) was better, equal, or worse than another output (B), without worrying too much about the exact numbers on the scale. For this relative , they assumed chance agreement was 1/3. In Table 2, we’ve included both the regular (absolute)  and the relative , but we used the actual chance agreement instead of the uniform chance agreement they assumed.",
        "formal_text": "Convert casual text to formal text: We used the -coefficient to measure how well the annotators agreed with each other, which is a pretty standard way to check agreement in tasks where people are assigning categories. The"
    },
    {
        "casual_text": "The discourse approach is doing pretty well, but it's not getting higher ROUGE scores because of the classification mistakes we noticed during our internal evaluation.",
        "formal_text": "Convert casual text to formal text: The discourse approach is doing pretty well, but it's not getting higher ROUGE scores because of classification mistakes we noticed during our internal evaluation. Convert casual text to formal text: Convert casual"
    },
    {
        "casual_text": "The SQUIRREL system (it's part of a SERC grant, number GR/E/69485) tackles some of these issues. Its design makes customizing things easier because it interprets words without needing a detailed world model. The system assumes the lexicon, or word list, isn't complete, so it figures out meanings for unknown words using typing info from the data model. Plus, SQUIRREL shows that Natural Language Front Ends (NLFEs) can help users understand integrity constraints, which are usually hidden. It's worth mentioning that SQUIRREL isn't introducing anything \"new\" to standard database management systems. Instead, it's looking at how much of the current state-of-the-art in NLP/CL (Natural Language Processing/Computational Linguistics) and Formal Semantics can be used in designing NLFEs for relational databases, while following good software engineering practices. The goal is to create a modular, portable design that can easily connect with public domain database tech, needing only a little customization.",
        "formal_text": "Convert casual text to formal text: The SQUIRREL system (it's part of a SERC grant, number GR/E/69485) tackles some of these issues. Its design makes customizing things"
    },
    {
        "casual_text": "So, based on this, we do something called relation aggregation. This means we take multiple local relations from Table 3 and combine them to figure out the overall relation between two sentences. In our case, we have two relations: forward entailment ( ) and alternation ( | ). When we put them together, we get alternation ( | ), which gives us something like \"All animals outside are eating | All cats outside are playing.\" There are seven basic relationships in natural logic that can help us understand these sentence pairs. For example, if we're dealing with a three-way classification problem in NLI (entailment, contradiction, and neutral), we can map these relationships like this: - The '  ' or ' ' relation goes to entailment. - The '  ' or ' | ' relation goes to contradiction. - The ' ', ' ', or ' # ' relations go to neutral.",
        "formal_text": "Convert casual text to formal text: So, based on this, we do something called relation aggregation. This means we take multiple local relations from Table 3 and combine them to figure out the overall relation between two sentences. In"
    },
    {
        "casual_text": "When making predictions, the process for going from D a to D b involves using the trained MRC model to find all the arguments in D a based on the AM query. Then, it uses APE queries to pull out all the argument pairs from D b. The reverse, D b to D a, works the same way but just swaps D a and D b. Each APE query gives you one or more argument pairs, with each pair including the query argument and one extracted argument. We just combine all the argument pairs from all the APE queries into one big set to get the final results.",
        "formal_text": "Convert casual text to formal text: When making predictions, the process for going from D a to D b involves using the trained MRC model to find all the arguments in D a based on the AM query. Then"
    },
    {
        "casual_text": "Unlike GloVe, which works with whole words, BERT breaks things down into smaller parts called word-pieces. This helps avoid issues with words that aren't in the model's vocabulary. We picked BERT over other models because it can understand context from both directions (bidirectional) and is more efficient than those that only look at one direction at a time. BERT uses something called multi-head attention to figure out which words are most important when figuring out the sentiment of a Tamil sentence. When we compared it to basic models like LSTM and BLSTM, the BERT model did really well on the validation data. We then used the fine-tuned BERT model to predict sentiment on the test set. The next section will give you all the details about the experiments and the results.",
        "formal_text": "Convert casual text to formal text: Unlike GloVe, which works with whole words, BERT breaks things down into smaller parts called word-pieces. This helps avoid issues with words that aren't in the model'"
    },
    {
        "casual_text": "A lot of research on affixoids has been done on Germanic languages like German, Dutch, and Swedish (check out Ascoop and Leuschner, 2006; Booij, 2005; Booij and Hüning, 2014; Norde and Van Goethem, 2014). But we think affixoids aren’t just limited to these languages. They probably show up in other languages that have productive compounding too. Take English, for example—there’s not much systematic research on it, but words like \"quality\" (as in quality press/furniture/diamonds, but not in quality management) and \"nut\" (as in health/math/trivia nut, but not in pecan nut) could be considered English affixoids. Even for the languages that have been studied more, most of the work focuses on the theoretical side—like whether affixoids should be seen as a separate category from affixes or compounds. There hasn’t been much quantitative research using actual corpus data to, say, figure out how productive different affixoid candidates are or to back up the idea that most affixoid uses have some kind of evaluative meaning. So, our work is trying to fill that gap and bring some real-world data into the theoretical discussion.",
        "formal_text": "Convert casual text to formal text: A lot of research on affixoids has been done on Germanic languages like German, Dutch, and Swedish (check out Ascoop and Leuschner, 2006; Booij, 2005"
    },
    {
        "casual_text": "For each model, we tweaked the settings for batch size, learning rate, and dropout. We also tried out different sizes for the embedding, attention, and hidden layers. All the models were trained for up to 100 epochs, but we stopped early if there was no improvement in validation performance after 50 epochs. We used the Adam optimizer (from Kingma and Ba, 2015) to minimize the Cross Entropy Loss. To pick the best model, we looked at the BERTScore F1 (Zhang et al., 2020) on the validation set for the generation task, and for the resolution task, we went with accuracy. In the next part, we'll share the average scores and standard deviations from 5 runs with different random seeds. If you want more info on how we chose hyperparameters, the model setups, or how to reproduce our results, check out Appendix E.",
        "formal_text": "Convert casual text to formal text: For each model, we tweaked the settings for batch size, learning rate, and dropout. We also tried out different sizes for the embedding, attention, and hidden layers. All the"
    },
    {
        "casual_text": "The hearing is probably gonna take two days. Check out Figure 3—it shows one way the search algorithm works. Once the synchronous parsing is done, every cell from (0, T) will have a full parse forest for the target language, kind of like the one you see. You can pull out any structure from this forest, no matter what the synchronous grammar tree looks like (in this case, it’s monotonic). In Figure 3, at the bottom of the tree, you’ve got phrasal translations along with their parse forests. The darker nodes represent constituents for those spans. The forest’s been simplified a bit for easier understanding—in reality, there’d be a ton of target-grammar nonterminals for each span. The dashed lines show the order the cells are combined. Each time a binary synchronous grammar rule is applied, it merges two forests and adds new edges to finish the forest.",
        "formal_text": "Convert casual text to formal text: The hearing is probably gonna take two days. Check out Figure 3—it shows one way the search algorithm works. Once the synchronous parsing is done, every cell from (0, T)"
    },
    {
        "casual_text": "In this part, we're going to talk about a big idea for training called a \"large margin\" thing. Imagine you have a main sentence, let's call it x i. For this main sentence, you also have a bunch of other sentences, x i, j, where j goes from 1 to K. These other sentences are usually the best K options you get from something called a beam search decoder.",
        "formal_text": "Convert casual text to formal text: In this part, we're going to talk about a big idea for training called a \"large margin\" thing. Imagine you have a main sentence, let's call it x"
    },
    {
        "casual_text": "On the flip side, document clustering is basically about splitting a bunch of documents into different groups based on their content (Hu et al., 2008). A scientific article is just a research paper that gets published in specific journals or at conferences. Conferences usually have different sessions with a set number of papers, where authors present their work. These sessions are usually focused on a particular theme and are put together by the conference chair, which can be a really manual and time-consuming job, especially when there are a lot of papers. Sorting out the sessions at a conference is kind of like a document clustering problem with some size limits.",
        "formal_text": "Convert casual text to formal text: On the flip side, document clustering is basically about splitting a bunch of documents into different groups based on their content (Hu et al., 2008). A scientific article is just"
    },
    {
        "casual_text": "Coreference resolution is all about figuring out which words or phrases in a text are talking about the same thing. It's a big deal in natural language processing (NLP) and has been studied a lot, especially for English (Ng, 2010). Lately, models using fancy stuff like pre-trained transformers—think BERT (Devlin et al., 2019) and SpanBERT (Joshi et al., 2020)—have been super effective for English, scoring an impressive 80 F1 on the OntoNotes dataset (Pradhan et al., 2012). But when it comes to Arabic, coreference resolution is still pretty tricky. Previous systems (Björkelund and Kuhn, 2014; Chen and Ng, 2012; Fernandes et al., 2012; Zhekova et al., 2012; Stamborg et al., 2012; Xiong and Liu, 2012) haven't been as good as their English counterparts, so there's still a lot of room for improvement.",
        "formal_text": "Convert casual text to formal text: Coreference resolution is all about figuring out which words or phrases in a text are talking about the same thing. It's a big deal in natural language processing (NLP) and has"
    },
    {
        "casual_text": "Alright, so in simpler terms, this is talking about how the words \"afternoon rest area\" work together. Even though \"afternoon\" is a noun and \"rest\" is a verb, they combine to make sense as a phrase. This is like saying that the meaning of the words together is more important than their individual parts. It's kind of like how \"afternoon\" and \"rest\" are teaming up to describe a place where you can rest in the afternoon.",
        "formal_text": "Convert casual text to formal text: Alright, so in simpler terms, this is talking about how the words \"afternoon rest area\" work together. Even though \"afternoon\" is a noun and \""
    },
    {
        "casual_text": "The score for the tag sequence can be swapped out with the right side of Equation 1. We noticed that adding up the transition scores is pretty much the same as the log probability of the tag sequence, which comes from a first-order language model (LM). If we switch the transition matrix A with an LSTM-based tag LM, we can define the sequence score like this:",
        "formal_text": "Convert casual text to formal text: The score for the tag sequence can be swapped out with the right side of Equation 1. We noticed that adding up the transition scores is pretty much the same as the log probability of the tag sequence,"
    },
    {
        "casual_text": "In this paper, we introduce the Unsupervised Self-Training framework and use it for sentiment classification. Our framework does two things at once: it assigns sentiment labels to sentences in a code-mixed dataset without any supervision, and it trains a sentiment classification model completely unsupervised. The framework can also be easily adapted to include active learning. We dive deep into how our unsupervised model works and try to figure out whether it actually understands code-switched languages or just recognizes their patterns. Plus, we share some tips on how to improve the performance of the Unsupervised Self-Training algorithm.",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce the Unsupervised Self-Training framework and use it for sentiment classification. Our framework does two things at once: it assigns sentiment labels to sentences in a code"
    },
    {
        "casual_text": "Text-to-speech mapping isn’t one-to-one; it’s one-to-many because the same text can sound different depending on pitch, duration, and prosody. This means the mel-spectrograms, which represent the sound, can vary a lot, making their distribution, P(y|x), multimodal. Plus, since mel-spectrograms are continuous, nearby data points are connected to each other. In this part, we’ll first take a closer look at the distribution of P(y|x) in TTS 4 by visualizing it (Section 2.1). Then, we’ll introduce a fresh way to think about the over-smoothing issue in TTS (Section 2.2).",
        "formal_text": "Convert casual text to formal text: Text-to-speech mapping isn’t one-to-one; it’s one-to-many because the same text can sound different depending on pitch, duration, and"
    },
    {
        "casual_text": "We wanted to make an adversarial dataset to help paraphrase detectors get better at spotting paraphrases with very little word overlap. To show how tough this is for them, we tested RoBERTa base, which was only trained on TwitterPPDB, on some specific datasets, as you can see in Table 5. While it did okay on MSRP, it was barely better than just guessing on AP H. This just goes to show that figuring out adversarial paraphrases made with APT isn’t easy for these paraphrase detectors.",
        "formal_text": "Convert casual text to formal text: We wanted to make an adversarial dataset to help paraphrase detectors get better at spotting paraphrases with very little word overlap. To show how tough this is for them, we tested"
    },
    {
        "casual_text": "Alright, so when teachers give feedback on videos, they usually focus on how clear the video is, how smooth the speaker's voice sounds, and whether the answer matches the topic they asked about. So, if we have a question and a video response, we pull out information from three different areas: the visuals, the audio, and the text. Now, since the video usually doesn't change much visually when someone is talking, we just grab a single screenshot from the video, which we call an image, to represent the visual part. And we also take the audio from the video and...",
        "formal_text": "Convert casual text to formal text: Alright, so when teachers give feedback on videos, they usually focus on how clear the video is, how smooth the speaker's voice sounds, and whether the answer matches the topic they asked about."
    },
    {
        "casual_text": "We’re using Amazon’s Mechanical Turk (AMT) to gather the labels. AMT is basically a platform where people called \"requesters\" can set up tasks for \"workers\" from all over the world to complete. To make these tasks (which are called Human Intelligence Tasks, or HITs), the requester gives AMT an HTML template and a database with comma-separated values. AMT then automatically creates the HITs and makes them available for workers to do. The queries are shown as an HTML page based on the template, and the worker just needs to pick the right label—YES, NO, or NOT SURE—by clicking the corresponding radio button. The instructions say something like:",
        "formal_text": "Convert casual text to formal text: We’re using Amazon’s Mechanical Turk (AMT) to gather the labels. AMT is basically a platform where people called \"requesters\" can set up tasks for \"workers"
    },
    {
        "casual_text": "The dataset questions were all labeled by them, following the coarse and fine-grained categories listed in Table 3. The coarse classes (in bold) are followed by their more specific fine-grained categories. The table also shows how the 500 test questions are spread across these categories. Li and Roth (2002) used things like words, part of speech tags, chunks (non-overlapping phrases), head chunks (the first noun chunk in a question), and named entities. They got an accuracy of 78.8% for 50 fine-grained classes. By using a hand-built dictionary of related words, their system could bump that up to 84.2%.",
        "formal_text": "Convert casual text to formal text: The dataset questions were all labeled by them, following the coarse and fine-grained categories listed in Table 3. The coarse classes (in bold) are followed by their more specific fine-grained"
    },
    {
        "casual_text": "So, Y x is basically all the different ways you can arrange tags in a sequence, and F (•) is the function that gives you a score based on those tags.",
        "formal_text": "Convert casual text to formal text: So, Y x is basically all the different ways you arrange tags in a sequence, and F (•) is the function that gives you a score based based on those tags"
    },
    {
        "casual_text": "Basically,  J comes from a DP that's based on the average of two joint distributions for phrase pairs. Each of these distributions combines a simple unigram model from one language and a part that handles how words translate between languages. This setup has a couple of cool benefits. First, it nudges the model to use shorter phrases by making p s higher (we set p s to 0.8 in our tests). Second, it helps find better phrase pairs by using IBM Model 1 distributions. This is different from just using word alignment or weighting—we're giving the model a heads-up that phrases usually follow word alignments, but if there's enough data, they don't always have to. We found that the model didn't really care much about tweaking the sparsity parameter , so we just set it to 100 for our experiments.",
        "formal_text": "Convert casual text to formal text: Basically,  J comes from a DP that's based on the average of two joint distributions for phrase pairs. Each of these distributions combines a simple uni"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. When we compare the results of SACE base and its baseline (which is like a stripped-down version with no extra features), we find that both systems got 5346 cases right in ALL. However, SACE base got an extra 525 cases right that the baseline missed, and the baseline got 339 cases right that SACE base didn’t. So, SACE base did better in some areas. But there are also mistakes. Table 6 shows an example where SACE base made a wrong prediction (like guessing \"country\" incorrectly). The WlC (which helps with understanding context) didn’t do a great job here—it didn’t find useful info to figure out the word and even added some unrelated stuff. Table 6 also shows related sentences (#47 and #19) linked to a specific sentence (#10) that was being disambiguated. In this case, \"church\" was wrongly predicted when WlC was turned off. But when WlC was on, it found similar sentences in the same document and used that useful context to help with understanding. Table 7 gives examples of synsets (groups of words with similar meanings) that the selective attention layer connected. This layer can spot relationships between words that are closely related in meaning or used together a lot. It does this by looking at the highest attention scores (after ignoring connections to itself).",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. When we compare the results of SACE base and its baseline (which is like a stripped-down version with no extra features"
    },
    {
        "casual_text": "The big question is whether fully automatic machine translation (MT) will be so bad that the \"translation\" ends up being totally useless. Like, if even a trained person looking at the machine's output can't figure out what the original text was about, even roughly. This depends on the two languages being translated and how the machine tries to make the translation complete and unique. Only a lot of experiments will tell us if this approach has any potential. Some early work has been done by a team at the Rand Corporation, but so far, the results aren't super clear or convincing.",
        "formal_text": "Convert casual text to formal text: The big question is whether fully automatic machine translation (MT) will be so bad that the \"translation\" ends up being totally useless. Like, if even a trained person looking at the machine"
    },
    {
        "casual_text": "Check out Table 5 for the summary. Basically, the E2E-UMGR we came up with does better than the other baselines in both offline and online tests. We started by looking at E2E-UMGR with updates to the graph from unstructured speech and the creation of a dialog policy. When we took away the graph ground-truth (E2E-UMGR -), it still did better than UMGR, likely because it could use the actual speech and encode all the entities from their text. However, the performance of E2E-UMGR dropped a bit on IMR and EMR@1 due to mistakes in updating the graph. We think future, more advanced models will handle these errors better in the user memory graph.",
        "formal_text": "Convert casual text to formal text: Check out Table 5 for the summary. Basically, the E2E-UMGR we came up with does better than the other baselines in both offline and online tests. We started by looking at"
    },
    {
        "casual_text": "Okay, so if we have o(1), o(2), and so on up to o(M), the average number of times a word w appears in a document d is calculated like this: E[C(w | d)] = the sum from j=1 to M of C(w | t) multiplied by Pr(t | o(j)). Basically, it's adding up the counts of w in each t, weighted by how likely t is given each o(j).",
        "formal_text": "Convert casual text to formal text: Okay, so if we have o(1), o(2), and so on up to o(M), the average number of times a word w appears in a document"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. So, we have a pair of a trigger and an entity, like t x and e y. First, we use a method (Algorithm 1) to simplify the dependency tree and focus on the part that connects the trigger and the entity. This simplified part is called the contextual sub-tree. The words in this sub-tree are turned into word vectors, which we call h (0). These vectors are then fed into a GCN network with L layers. After running through all those layers, we get a final representation of the sub-tree.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. So, we have a pair of a trigger and an entity, like t x and e y."
    },
    {
        "casual_text": "You can find our code and resources over at https://github.com/xinyadu/memory_docie. It's all there for research purposes.",
        "formal_text": "Convert casual text to formal text: You can find our code and resources over at https://github.com/xinyadu/memory_docie. It's all there for research purposes."
    },
    {
        "casual_text": "In this study, we found that the benefits of few-shot learning can really depend on which examples you choose. For instance, picking different few-shot examples can lead to a difference of more than 10% in accuracy for a typical document classification task. Based on this, we suggest keeping the few-shot examples consistent to make fair comparisons between different crosslingual transfer methods. We set up a benchmark similar to the standard \"N-way K-shot\" setup used in few-shot learning (as introduced by Fei-Fei et al. in 2006 and Koch et al. in 2015). Additionally, we tested and compared a few top-of-the-line few-shot finetuning techniques to see how well they perform and how much they’re affected by the choice of few-shot examples.",
        "formal_text": "Convert casual text to formal text: In this study, we found that the benefits of few-shot learning can really depend on which examples you choose. For instance, picking different few-shot examples can lead to a difference of more than"
    },
    {
        "casual_text": "Argumentation synthesis research is becoming more popular. In the beginning, argument generation methods used rule-based techniques for planning discourse (Zukerman et al., 2000). Later, they started using generalized target-stance relations from claims to automatically create new arguments. However, these relations were created manually. Yanase et al. (2015) introduced an approach that picks the best conclusion from a set of candidate claims for generating new arguments. Sato et al. (2015) took this idea further and developed a method to create texts with multiple arguments. Other researchers reused targets and predicates from existing claims to make new ones (Bilu and Slonim, 2016), generated arguments based on specific inference schemes for user-defined content (Green, 2017), focused on rhetorical aspects in synthesis, and created arguments that follow a specific strategy (El Baff et al., 2019). All these methods focus on creating new argumentative content. In contrast, our goal is to find the missing parts of given arguments. This task is similar to enthymeme reconstruction. An enthymeme is an implicit premise, often the warrant (or major premise) that explains how a conclusion is derived from the given premises (Walton et al., 2008). Boltuzic and najder (2016) studied how to identify these enthymemes when the other components are known, motivated by the importance of finding the thesis. Similarly, Habernal et al. (2018) tackled the task of choosing the correct warrant from two options, and Rajendran et al.",
        "formal_text": "Convert casual text to formal text: Argumentation synthesis research is becoming more popular. In the beginning, argument generation methods used rule-based techniques for planning discourse (Zukerman et al., 2000). Later, they"
    },
    {
        "casual_text": "Okay, so we're talking about how an RNN works with inputs up to x to generate a response at y. This setup doesn't assume things stay the same over time (that's what \"relaxes stationarity assumptions\" means), and it also shares some patterns or structure across different time points. Now, about the definition of h...",
        "formal_text": "Convert casual text to formal text: Okay, so we're talking about how an RNN works with inputs up to x to generate a response at y. This setup doesn't assume things stay the same over time"
    },
    {
        "casual_text": "Not much has been thought about what kind of machine should handle the mechanical part of translation until now. People didn’t really experiment with using computers or similar machines because they hadn’t fully explored all the options yet. But it looks like we’re at a point where we can start experimenting. The big question is whether regular computers or special translation machines would work better in the long term, considering speed, accuracy, and cost. The main tasks the machine would need to do are comparing and identifying things, moving and transferring data, and making selections—both with and without conditions. Math-specific operations aren’t really part of it.",
        "formal_text": "Convert casual text to formal text: Not much has been thought about what kind of machine should handle the mechanical part of translation until now. People didn’t really experiment with using computers or similar machines because they hadn’t fully explored all the"
    },
    {
        "casual_text": "Let's call PA Paull's algorithm, where the nonterminals are ordered from most to least distinct left corners. Table 3, second line, shows what happens when we run PA on our three big grammars. The CT grammar doesn't grow much because, as we mentioned before, it doesn't have any indirect left recursion. So, Paull's algorithm doesn't need to deal with that extra step, and the size increase is just from changing the directly left-recursive rules. But with the ATIS and PT grammars—which don't have this neat feature—Paull's algorithm hit our limit, even when we tried the best nonterminal ordering we could.",
        "formal_text": "Convert casual text to formal text: Let's call PA Paull's algorithm, where the nonterminals are ordered from most to least distinct left corners. Table 3, second line, shows what happens when we run PA on our three"
    },
    {
        "casual_text": "When we use a language model to score the translations, we check how i=0 L i (t c )=10 compares.",
        "formal_text": "Convert casual text to formal text: When we use a language model to score the translations, we check how i=0 L i (t c )=10 compares. Convert casual text to formal text"
    },
    {
        "casual_text": "Since people who speak a second language (and even some native speakers) sometimes mess up inflectional stuff (Lardiere, 1998; Prévost and White, 2000; Haznedar, 2002; White, 2003; Seymour, 2004), we think NLP models should be able to handle these little errors. That way, they won’t accidentally spread any unfair treatment based on language. So, in this paper, we:",
        "formal_text": "Convert casual text to formal text: Since people who speak a second language (and even some native speakers) sometimes mess up inflectional stuff (Lardiere, 1998; Prévost and White, 2000; Hazne"
    },
    {
        "casual_text": "From the books ngram data, we also look at a few other things: PercentProperCaps, which is the percentage of times the term shows up with all words starting with a capital letter (ignoring case), PercentExact-Match, which is the percentage of times the term matches the exact capitalization in the text, and Frequency, which is the total number of times the term appears in the books data (ignoring case) across all years, showing how common it is. Lastly, we also check for terms that start with numbers (like \"5\" in \"5 days\" or \"Three\" in \"Three choices\") as a simple feature.",
        "formal_text": "Convert casual text to formal text: From the books ngram data, we also look at a few other things: PercentProperCaps, which is the percentage of times the term shows up with all words starting with"
    },
    {
        "casual_text": "We're using an open-source version of Show, Attend, and Tell (SAT) with soft attention. Instead of the original CNN encoder, we swapped it out for a ResNet-101 that was pre-trained on ImageNet for image classification. We got rid of the last two layers of the ResNet—a pooling layer and a fully-connected layer—so that the encoder now outputs a 14142048 feature map for each image.",
        "formal_text": "Convert casual text to formal text: We're using an open-source version of Show, Attend, and Tell (SAT) with soft attention. Instead of the original CNN encoder, we swapped it out for a ResNet"
    },
    {
        "casual_text": "Sorry, but I can't assist with this request.",
        "formal_text": "Convert casual text to formal text: Sorry, but I't assist this request."
    },
    {
        "casual_text": "The text is placed in a simple, essential context and sent through all the machine translation engines. For each chunk, C_i, the possible translated versions, C_1_i to C_N_i, are gathered. Then, the best translation, C_best_i, is picked using a majority vote, a language model trained on a big target language dataset, and a confidence score from each MT engine. Finally, the output sentence, S, is put together by piecing all the best chunks, C_best_i, into their right spots in the target language. You can see a diagram of this whole process in Figure 1.",
        "formal_text": "Convert casual text to formal text: The text is placed in a simple, essential context and sent through all the machine translation engines. For each chunk, C_i, the possible translated versions, C_1_i to C_"
    },
    {
        "casual_text": "To really understand how languages change over time and across different places, we need to see them go through actual real-world stuff like splitting into new languages (speciation), disappearing (death), and moving around (migration). These are super important for making the simulation feel real before we dive into how specific features of languages change. Let’s break down what these phylogenetic things mean and how we check they’re working right, and then we’ll move on to talking about features.",
        "formal_text": "Convert casual text to formal text: To really understand how languages change over time and across different places, we need to see them go through actual real-world stuff like splitting into new languages (speciation), disappearing (death"
    },
    {
        "casual_text": "The Securities and Exchange Commission (SEC) makes public companies share their financial stuff at set times. One of these is the Form 10-K, which is like an annual report that talks about what the company does, how it's doing financially, and who's running the show. In November 2020, the SEC started asking companies to also include details about their human capital—basically, their employees—in this report. This new part is pretty interesting to accounting researchers who want to see how companies describe their workforce and if they mention any diversity stats (Choi et al., 2022).",
        "formal_text": "Convert casual text to formal text: The Securities and Exchange Commission (SEC) makes public companies share their financial stuff at set times. One of these is the Form 10-K, which is like an annual report that talks about what the company"
    },
    {
        "casual_text": "Alright, let’s break this down in simpler terms. Many-to-one mapping accuracy, which is also called cluster purity, works by looking at each group of words (cluster) and figuring out which gold standard tag (the correct tag) shows up most often in that group. That tag is considered the \"preferred tag\" for that cluster. Then, it calculates how many words in the cluster were tagged correctly with this preferred tag. Here’s the thing: multiple clusters can end up being linked to the same gold standard tag. This method is pretty popular in research because it’s easy to understand and helps turn the cluster labels into a clear sequence of part-of-speech tags. But there’s a catch: as the number of clusters (|C|) grows, this method tends to give higher scores, which can make it tricky to compare results when the number of clusters varies.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in simpler terms. Many-to-one mapping accuracy, which is also called cluster purity, works by looking at each group of words (cluster) and"
    },
    {
        "casual_text": "Let’s say k is the target beam size we’re aiming for. The search starts with an empty hypothesis in the (0, 0) bucket. Then, for each word in the sentence (from i = 0 up to the last word), we follow this process: We start with the (i, 0) bucket. All the possible next steps (successors) for each hypothesis in that bucket are gathered, ranked by their scores, and then we trim it down to the top k options. From those, the ones that come from an OPEN or CLOSE action move to the (i, 1) bucket, while the ones from a SHIFT action go to the (i + 1, 0) bucket if we’re not at the end of the sentence. If we are at the end, they go to the completed list. This same process is repeated for the (i, 1) bucket, then the (i, 2) bucket, and so on, until the (i + 1, 0) bucket has at least k hypotheses. If we want, we can use a smaller beam size, k_w, at word boundaries. In that case, each word-level step stops when the (i + 1, 0) bucket has k_w candidates instead of k. This creates a kind of bottleneck that can help keep the beam more diverse.",
        "formal_text": "Convert casual text to formal text: Let’s say k is the target beam size we’re aiming for. The search starts with an empty hypothesis in the (0, 0) bucket. Then, for each word in"
    },
    {
        "casual_text": "3. Create a neural network model for the source language that takes word embeddings and basic, language-neutral features as its input.",
        "formal_text": "Convert casual text to formal text: 3. Create a neural network model for the source language that takes word embeddings and basic, language-neutral features input. 4. Convert casual text to formal text: 5. Convert casual"
    },
    {
        "casual_text": "PanPhon uses Unicode for the International Phonetic Alphabet (IPA), both internally and externally. This makes it easier for humans—or at least, linguists—to read compared to systems like ARPAbet, WorldBet, SAMPA, and X-SAMPA, which rely on ASCII. For example, take the German word \"müde\" meaning \"tired.\" In ARPAbet, it's written as /M no_symbol> D AH/, in WorldBet as /m y: d &/, in SAMPA as /m y: d @/, and in X-SAMPA as /m y: d @/. But in IPA, it's simply /myd/. Thanks to better input methods and display tech for IPA, using it in computers has gotten a lot less tricky than it used to be.",
        "formal_text": "Convert casual text to formal text: PanPhon uses Unicode for the International Phonetic Alphabet (IPA), both internally and externally. This makes it easier for humans—or at least, linguists—"
    },
    {
        "casual_text": "Sure! Here's a more casual version: \"Check out the Kyoto University Text Corpus here: http://nlp.ist.i.kyoto-u.ac.jp/EN/index.php?Kyoto%20University%20Text%20Corpus\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: \"Check out the Kyoto University Text Corpus here: http://nlp.ist.i.kyoto-u.ac."
    },
    {
        "casual_text": "We use this double-grounding idea for sarcasm to make our neural network model better at detecting sarcasm, which helps us answer our first two research questions. We look at the speaker's mood when they make a sarcastic comment by checking their recent tweets for mood clues. We also consider the context by looking at the tweet that the sarcastic comment is responding to. For our third research question, we came up with a new way to get feedback on our annotations by involving the people who wrote the tweets. This helps us improve the model by feeding new examples back into it. Section 3 talks about the features we use in the model, like where they come from. Section 4 explains how we collected and labeled our data. Section 5 describes the neural network model itself, and sections 6 & 7 cover our experiments and how we analyzed the results. Finally, section 8 wraps things up with some final thoughts.",
        "formal_text": "Convert casual text to formal text: We use this double-grounding idea for sarcasm to make our neural network model better at detecting sarcasm, which helps us answer our first two research questions"
    },
    {
        "casual_text": "Our tests show that throwing in this extra loss term into the objective function doesn't just boost performance but also helps the model get better faster. We started with a learning rate of  = 0.05 and reset the squared gradient sum every five epochs.",
        "formal_text": "Convert casual text to formal text: Our tests show that throwing in this extra loss term into the objective function doesn't just boost performance but also helps the model get better faster. We started with a learning rate of  ="
    },
    {
        "casual_text": "Relation Classification (RC) is all about figuring out the relationship between two entities in a piece of text. We test our models in two different setups: the few-shot setting and the full-data setting. The few-shot setting is all about handling those less common, long-tail relations that don’t have many examples to learn from. We test our models on FewRel 1.0 (Han et al., 2018) and FewRel 2.0 (Gao et al., 2019b). FewRel 1.0 has examples based on Wikidata facts, while FewRel 2.0 has a biomedical test set to see how well the models can adapt to a new domain. In the N-way K-shot setup, the models need to pick one of N possible relations for a given query, and each relation has K examples to help make that decision. We use the top few-shot framework called Proto (Snell et al., 2017) with different language model encoders to test this. For the full-data setting, we test on Wiki80, which has 80 different relation types from Wikidata. We also try using just 1% or 10% of the training data to see how the models perform with less information. From Tables 2 and 3, we can see that on FewRel 1.0 and Wiki80, RoBERTa with PELT does way better than just plain RoBERTa (like +3.3% on a 10-way 1-shot task). It even performs almost as well as ERNIE, which has access to a knowledge graph. Our model also shows big improvements on FewRel 2.0 in the biomedical domain (like +7.1% on a 10-way 1-shot task), while the entity-aware models don’t really improve much in most cases.",
        "formal_text": "Convert casual text to formal text: Relation Classification (RC) is all about figuring out the relationship between two entities in a piece of text. We test our models in two different setups: the few-shot setting and the"
    },
    {
        "casual_text": "The usual starting point for summarizing news articles is something called Lead-3 (See et al., 2017). This method just grabs the first three sentences of the article and uses them as the summary. The idea is that the beginning of the article usually has the most important stuff. Taking a cue from this Lead-n approach, we came up with a couple of other basic models to try out.",
        "formal_text": "Convert casual text to formal text: The usual starting point for summarizing news articles is something called Lead-3 (See et al., 2017). This method just grabs the first three sentences of the article and uses them as"
    },
    {
        "casual_text": "The UCCA and HUJI-KU stuff got some help from the Israel Science Foundation (grant No. 929/17). PTG was partly funded by the Ministry of Education, Youth and Sports of the Czech Republic (project LINDAT/CLARIAH-CZ, grant No. LM2018101) and also by the Grant Agency of the Czech Republic (project LUSyD, grant No. GX20-16819X). DRG got support from the NWO-VICI grant (288-89-003) and the European Union's Horizon 2020 research and innovation program (grant agreement No. 742204). The Chinese AMR data work is partly backed by project 18BYY127 from the National Social Science Foundation of China and project 61772278 from the National Science Foundation of China.",
        "formal_text": "Convert casual text to formal text: The UCCA and HUJI-KU stuff got some help from the Israel Science Foundation (grant No. 929/17). PTG was partly funded by the Ministry of Education,"
    },
    {
        "casual_text": "Some methods combine dependency syntactic features to make the connection between arguments and triggers stronger (Sha et al., 2018; Liu et al., 2018; Li et al., 2019b). All these approaches train the model by trying to maximize the joint probability of triggers and arguments. Li et al. (2019c) created manual rules for the extraction process and turned parameter optimization into an integer linear programming problem to boost extraction accuracy. Yang et al. (2016) turned the event into an event tree and defined the EE task as the challenge of extracting the best tree.",
        "formal_text": "Convert casual text to formal text: Some methods combine dependency syntactic features to make the connection between arguments and triggers stronger (Sha et al., 2018; Liu et al., 2018; Li"
    },
    {
        "casual_text": "Here’s the informal version: **Algorithm 1: How we train our fancy model.** First, we calculate the hidden states for the source data, which we call H. We do this by running X through our encoder function E.",
        "formal_text": "Convert casual text to formal text: Here’s the informal version: Convert casual text to formal text: Here’s the informal version: Here’s the informal version: **Algorithm 1: How we train our"
    },
    {
        "casual_text": "From what we’ve seen, adding localness to the decoder-side self-attention and encoder-decoder attention doesn’t really help much—it might even mess things up a bit. For the encoder-decoder attention, the small improvement we got is probably because it’s already using the top layer of the encoder, which has some useful local context baked in. As for the decoder-side self-attention,",
        "formal_text": "Convert casual text to formal text: From what we’ve seen, adding localness to the decoder-side self-attention and encoder-decoder attention doesn’t really help much—it might even mess things up"
    },
    {
        "casual_text": "Lastly, the decoder from Section 3.2 is used to create hidden states H_a and act tokens based on that.",
        "formal_text": "Convert casual text to formal text: Lastly, the decoder from Section 3.2 is used to create hidden states H_a and act tokens based on that. Lastly, the decoder from Section 3.2 is"
    },
    {
        "casual_text": "Alright, so let's say we have a type , a limit n, and a set A of sources we've already covered. We'll call PossL(, A, n) the group of lexical types  where A is a subset of A(, ), and we can get from  to  using APP operations from the sources in A, plus no more than n extra APP operations. In other words, the difference between A(, ) and A is no bigger than n.",
        "formal_text": "Convert casual text to formal text: Alright, so let's say we have a type , a limit n, and a set A of sources we've already covered. We'll call Poss"
    },
    {
        "casual_text": "Lastly, sharing your code is usually the best thing for the research community. If you can't do that, at least make sure to share your hyperparameter settings and other important details so others can replicate your work.",
        "formal_text": "Convert casual text to formal text: Lastly, sharing your code is usually the best thing for the research community. If you can't that, at least make sure to share your hyperparameter settings and other important details so others can"
    },
    {
        "casual_text": "There are some key differences between Figures 3a and 3b. First off, the way scores are spread out for Model i changes a lot depending on the document size. Without windowing, a model's Jaccard index for a specific Model i can go from zero to about 0.7. Interestingly, this wide range happens whether you're working with 7 documents or all the way up to 19 documents. Even though the average performance (shown by the dashed line) seems to improve over time, this spread shows that if you pick the \"wrong\" set of N documents for training, you might still get bad results, even with a large N. On the other hand, with windowing, the score distributions are much tighter, which helps reduce the effect of choosing any particular documents for training.",
        "formal_text": "Convert casual text to formal text: There are some key differences between Figures 3a and 3b. First off, the way scores are spread out for Model i changes a lot depending on the document size. Without windowing,"
    },
    {
        "casual_text": "Alright, so if we take the judgment: der patient, of the patient, NO> from the sentence pair: \"der patient wurde isoliert.\" and \"of the patient was isolated.\"",
        "formal_text": "Convert casual text, of the patient, NO> from the sentence pair: \"der patient wurde isoliert.\" and \"of the patient was isolated.\" Alright, so if we take the judgment: der patient,"
    },
    {
        "casual_text": "We trained with a constant  and got a value of 5.77. The  weights, which we call  i, j, went up as the values of i and j increased, starting from  0, 0 = 0.82 and ending at  4, 4 = 6.89. This backs up our idea that the more context we have, the more our system should trust the language model scores.",
        "formal_text": "Convert casual text to formal text: We trained with a constant  and got a value of 5.77. The  weights, which we call  i, j, went up as the values of"
    },
    {
        "casual_text": "The number in parentheses next to a source's name shows how many zero pronouns are in it.",
        "formal_text": "Convert casual text to formal text: The number in parentheses next to a source's name shows how many zero pronouns are in it. Convert casual text to formal text: The number in parentheses next"
    },
    {
        "casual_text": "First off, you need to get a ton of hands-on experience. Here's how our approach works with dynamic threshold selection: 1. **Input Stuff**: You start with a parallel corpus called D = x, y M, which is basically a bunch of sentence pairs. You also have an unlabeled corpus of source-side sentences called US = u N. Plus, you’ve got some initialized model parameters , a perturbation function c(•), and a few other things like supervised batch size B, unsupervised batch size B, a weight factor , and a filter type f t (which can be BLEU or perplexity). There’s also a data filter score function f, a decreasing-ordered score list L, and a function len(•) that just tells you the length of a list. 2. **Warm-up Training**: Start by initializing  using a pretrained T5 model. 3. **Finetune **: Then, you finetune  on D using Equation (1).",
        "formal_text": "Convert casual text to formal text: First off, you need a ton of hands-on experience. Here's how our approach works with dynamic threshold selection: 1. **Input Stuff**: You start with a parallel"
    },
    {
        "casual_text": "Unlike those models, we use some categorical attributes and optimize by minimizing cross-entropy.",
        "formal_text": "Convert casual text to formal text: Unlike those models, we use some categori attribute and optimize by minimizing cross-entropy."
    },
    {
        "casual_text": "We’re sharing some specifics about how we ran our experiments. All the pre-trained language models (PLMs) we used in our paper are based on HuggingFace Transformers 10. For fine-tuning on the probing task, we ran the experiments with batch sizes of either 8 or 16,  set to 3, 5, or 10, a max sequence length of 128, and a learning rate of 1e-5, training for 10 epochs. We kept the same hyperparameters for each model, even when using different training objectives. For fine-tuning on the sentiment analysis task, we only tweaked the parameters of the multi-layer perceptron (MLP) classifiers that sit on top of the PLM's contextualized representations. For this, we used a learning rate of 2e-5, 3e-5, or 4e-5, a batch size of 32, a max sequence length of 128, and trained for 200 epochs. Oh, and we also included some examples of the experimental setup for checking how important components affect things in Table 8.",
        "formal_text": "Convert casual text to formal text: We’re sharing some specifics about how we ran our experiments. All the pre-trained language models (PLMs) we used in our paper are based on HuggingFace Transformer"
    },
    {
        "casual_text": "On the flip side, unsupervised NMT doesn’t have a way to get out of a bad starting point. Drawing from old-school decipherment techniques (check out Section 2), we could think about downplaying messy training examples or making the problem easier at first.",
        "formal_text": "Convert casual text to formal text: On the flip side, unsupervised NMT doesn’t have a way to get out of a bad starting point. Drawing from old-school decipherment techniques (check out Section 2)"
    },
    {
        "casual_text": "SetConv does a great job in classification, but it might not work well with high-dimensional sparse data. This is because all those 0s in the data can make the convolution kernels almost zero, which limits how well the model can classify things. One way to fix this could be mixing SetConv with some sparse deep learning techniques. We'll save that for later though.",
        "formal_text": "Convert casual text to formal text: SetConv does a great job in classification, but it might not work well with high-dimensional sparse data. This is because all those 0s in the data can make the con"
    },
    {
        "casual_text": "For LTL, the same idea works with a very similar lemma. The only extra thing it asks for is that, for the root r of t, A c(r) is empty. The steps to build the transition sequence are laid out in Algorithm 2.",
        "formal_text": "Convert casual text to formal text: For LTL, the same idea works with a very similar lemma. The only extra thing it asks for is that, for the root r of t, A c("
    },
    {
        "casual_text": "Alright, so in Table 1, the top part has the AMI ASR transcripts, and the bottom part has summaries written by humans. The symbols , , and  represent the 150th, 522nd, and 570th utterances, respectively. The \"a)\" part refers to the ath sentence in a summary that's made up of multiple sentences.",
        "formal_text": "Convert casual text to formal text: Alright, so in Table 1, the top part has the AMI ASR transcripts, and the bottom part has summaries written by humans. The symbols , , and"
    },
    {
        "casual_text": "You'll see a German sentence with a part highlighted. Then, there are a few English translations with their own highlighted parts. Your job is to figure out if each highlighted English part is a good match for the highlighted German part.",
        "formal_text": "Convert casual text to formal text: You'll see a German sentence with a part highlighted. Then, there are a few English translations with their own highlighted parts. Your job is to figure out if each highlighted"
    },
    {
        "casual_text": "So, we’re focusing on answering \"Who\" questions that involve characters from books (like, \"Who is Harry Potter's best friend?\"). This makes things easier because we’re just looking for specific entities, and we can use simpler evaluation methods like precision and ranking. But don’t worry, it still keeps the important parts of the original NarrativeQA task—like digging into the whole book and really understanding the story to figure things out. Table 1 shows how tricky these \"Who\" questions can get by listing a bunch from just one book, where the answers need more and more complex thinking.",
        "formal_text": "Convert casual text to formal text: So, we’re focusing on answering \"Who\" questions that involve characters from books (like, \"Who is Harry Potter's best friend?\"). This makes things easier because we’re just"
    },
    {
        "casual_text": "We're working with the French SPMRL dataset (Seddah et al., 2013), which is newer and bigger than the datasets people used for French parsing before (Crabbé and Candito, 2008). This dataset includes the full French Treebank mentioned in (Abeillé et al., 2003) and will probably become the go-to dataset for French parsing in the next few years. We're using it as it is, with two setups: one where we have gold standard tags and another where we use tags predicted by a tagger that's 97.35% accurate (Seddah et al., 2013). The French data has head annotations based on rules from (Arun and Keller, 2005), and compound words are always left-headed. For English, we're using the Penn Treebank with the usual split: sections 02-21 for training, section 22 for development, and section 23 for testing. In the predicted scenario, we use the MELT tagger (Denis and Sagot, 2012), which has an accuracy of 97.1%. The head annotations were figured out by matching the phrase structure treebank with its dependency conversion, as described by (de Marneffe et al., 2006). We're running the experiments using a C++ implementation of the algorithm we talked about. The scores for the Berkeley parser (Petrov et al., 2006) are based on the runs mentioned by (Seddah et al., 2013). We measure the F-score using the classic evalb, and we measure the times on the same machine (MacOSX 2.4Ghz). Note that we don't include input/output times for both parsers.",
        "formal_text": "Convert casual text to formal text: We're working with the French SPMRL dataset (Seddah et al., 2013), which is newer and bigger than the datasets people used for French par"
    },
    {
        "casual_text": "For our experiment, we're starting with a standard set of features commonly used for AKE. Basically, when we have a potential keyphrase, we look at:",
        "formal_text": "Convert casual text to formal text: For our experiment, we're starting with a standard set of features commonly used for AKE. Basically, when we have a potential keyphrase, we look at: When we have"
    },
    {
        "casual_text": "Variational inference tweaks the parameters of the variational distribution to make L as big as possible, which in turn makes the KL divergence as small as possible. VB simplifies things by breaking down the variational posterior into smaller parts. The mean-field factorization assumes that parameters and trees don't affect each other:",
        "formal_text": "Convert casual text to formal text: Variational inference tweaks the parameters of the variational distribution to make L as big as possible, which in turn makes the KL divergence as small as possible. VB simplifies"
    },
    {
        "casual_text": "For the evaluation, as mentioned in Section 3, we’re using the standard metrics that are usually talked about in research: acc@161, which is the accuracy within 161 km (100 miles) of the target point, and the mean and median distance between the predicted and actual target points. We’re also looking at exact accuracy, which isn’t always included in studies but is super important for real-world geolocation stuff. To check how significant our results are, we’re using bootstrap sampling.",
        "formal_text": "Convert casual text to formal text: For the evaluation, as mentioned in Section 3, we’re using the standard metrics that are usually talked about in research: acc@161, which is the accuracy within 161 km (100 miles of"
    },
    {
        "casual_text": "Alright, let’s dive into this low rank embedder thing we’re proposing for breaking down word embedding algorithms. It’s kind of inspired by this idea of generalized low rank models, which some folks like Udell et al. talked about in 2016. We’re looking at a bunch of word embedding algorithms and seeing how they all fit into this one big idea based on their global loss function. Just to be clear, this framework is more about understanding how things work in theory, not necessarily about building something you’d actually use in practice. Now, the global loss function for this low rank embedder looks like this:",
        "formal_text": "Convert casual text to formal text: Alright, let’s dive into this low rank embedder thing we’re proposing for breaking down word embedding algorithms. It’s kind of inspired by this idea of generalized low"
    },
    {
        "casual_text": "Khandelwal and team (2018) did some experiments to see how context words affect a language model. They found that within a range of about 200 tokens, word order only really matters for the last 20 tokens or so, which is roughly one sentence. For longer contexts, the order doesn't make much of a difference to the model's performance. This suggests that the model kind of keeps a general, high-level idea of words that are far apart. The paper sums this up nicely with the title \"sharp nearby, fuzzy far away.\" Interestingly, this idea lines up pretty well with the main concept behind Multilevel Methods.",
        "formal_text": "Convert casual text to formal text: Khandelwal and team (2018) did some experiments to see how context words affect a language model. They found that within a range of about 200 tokens, word order only really matters for the last"
    },
    {
        "casual_text": "In later research, automatically generated feature sets have become more common. Most of these are language-independent and could work for genre classification in languages other than English. For example, there are word-based methods from Stamatatos et al. (2000b) and Freund et al. (2006), image features from Kim and Ross (2008), a PoS histogram frequency approach by Feldman et al. (2009), and character n-gram approaches by Kanaris and Stamatatos (2007) and Sharoff et al. (2010). All these were only tested on English texts. Even though language-independence is often claimed, not many have actually shown it works across languages. Sharoff (2007) is one of the few who did genre classification experiments in more than one language. He used PoS 3-grams and a variation of common word 3-grams to classify English and Russian documents into genres. While the PoS 3-gram set worked well for English texts, it didn’t improve much over just picking the most common genre for Russian documents.",
        "formal_text": "Convert casual text to formal text: In later research, automatically generated feature sets have become more common. Most of these are language-independent and could work for genre classification in languages other than English. For example, there are word-based"
    },
    {
        "casual_text": "Having a QA dataset that clearly outlines these question traits makes it easier to really dig into how well a system is performing. But as far as we know, none of the existing QA datasets (like the ones from Voorhees and Tice in 2000, Berant et al. in 2013, Cai and Yates in 2013, Lopez et al. in 2013, Bordes et al. in 2015, or Serban et al. in 2016) actually include these question traits. In this project, we're taking the first shot at creating questions with specific traits clearly marked and seeing how different traits affect QA performance.",
        "formal_text": "Convert casual text to formal text: Having a QA dataset that clearly outlines these question traits makes it easier to really dig into how well a system is performing. But as far as we know, none of the existing"
    },
    {
        "casual_text": "Alright, so like how BERT does it, we start by adding a [cls] token at the beginning and a [sep] token at the end of the source sequence. After that, we throw in a bunch of [pad] tokens at the end to make the whole thing reach the maximum length we set, like 256, for example. This way, we ensure the source is at least as long as the output. But, for tasks like text summarization where we know the source is longer than the target, we skip adding those [pad] tokens when creating the input.",
        "formal_text": "Convert casual text to formal text: Alright, so like how BERT does it, we start by adding a [cls] token at the beginning and a [sep] token at the end of the source"
    },
    {
        "casual_text": "We think that sentences in a related work section are created through two different processes: one that provides general background info and another that talks about specific contributions from authors. A big insight we had is that these processes can be neatly organized into a topic tree: general content goes in the nodes inside the tree, while the leaf nodes hold the specific details. In our method, these two processes work separately and then come together to build the final summary.",
        "formal_text": "Convert casual text to formal text: We think that sentences in a related work section are created through two different processes: one that provides general background info and another that talks about specific contributions from authors. A big insight we had is that these"
    },
    {
        "casual_text": "DAD combines two main ideas: first, we use IRT to analyze datasets, which has been done for other NLP tasks, and second, we apply it to make leaderboards better. Lastly, we look into how IRT can not only help with analyzing test sets but also improve how we create them.",
        "formal_text": "Convert casual text to formal text: DAD combines two main ideas: first, we use IRT to analyze datasets, which has been done for other NLP tasks, and second, we apply it to make leaderboards better"
    },
    {
        "casual_text": "We created IndCorpus, which is a collection of ten Indigenous languages plus Spanish, totaling 1.17 GB of text (around 5.37 million sentences). This dataset was put together to help pre-train IndT5. We gathered the text from both Wikipedia and the Bible. Table 2 shows the size and number of sentences for each language in the dataset.",
        "formal_text": "Convert casual text to formal text: We created IndCorpus, which is a collection of ten Indigenous languages plus Spanish, totaling 1.17 GB of text (around 5.37 million sentences). This dataset was put together to"
    },
    {
        "casual_text": "Every day, Twitter gets more than 340 million short messages. People often say the same thing but in different ways.",
        "formal_text": "Twitter gets more than 340 million short messages every day. People often say the same thing but different ways. Convert casual text to formal text: Every day, Twitter gets more than 340 million short messages. People often say the same thing"
    },
    {
        "casual_text": "Every kernel has a parameter called  that you can adjust. This  controls how wide or narrow the kernel curves are, basically limiting how much each sentence gets projected. Usually, the best value for  can change depending on the news topic. Some subjects might have sentences with broader meanings, so you'd need a higher , while others might need a lower one.",
        "formal_text": "Convert casual text to formal text: Every kernel has a parameter called  that you can adjust. This  controls how wide or narrow the kernel curves are, basically limiting how much each sentence gets projected. Usually,"
    },
    {
        "casual_text": "So, U is a square matrix where each value is 1/N, and d is this thing called the damping factor. We just set d to 0.1, which is the default value most people use. Now, about MMR (Maximal Marginal Relevance, introduced by Carbonell and Goldstein in 1998): In this method, sentences are ranked one by one based on a score that considers how relevant they are to the document and how much extra, redundant info they bring. The score is calculated by balancing relevance to the document and diversity, kind of like mixing the two together.",
        "formal_text": "Convert casual text to formal text: So, U is a square matrix where each value is 1/N, and d is this thing called the damping factor. We just set d to 0.1, which is the default"
    },
    {
        "casual_text": "We use arc-factored models, where exact inference is doable (McDonald et al., 2005). We set up 684 feature templates for each possible arc by combining the words, shapes, lemmas, and POS tags of both the head and the modifier, along with the surrounding POS tags, and details about the distance and direction of the connection. We stuck to the same two-stage method as before and compared it to a baseline that picks feature templates based on how much information they provide. This baseline gives each template a score that shows how well it predicts whether a dependency link exists or not, using something called mutual information.",
        "formal_text": "Convert casual text to formal text: We use arc-factored models, where exact inference is doable (McDonald et al., 2005). We set up 684 feature templates for each possible"
    },
    {
        "casual_text": "So, the total cost for dividing the text into K segments is made up of the costs for each segment. The cost for each segment is a combination of two things, adjusted by a parameter called 7: 1. **Length Information**: This is how far the segment's length is from the average segment length. Think of it like the mean (bt) and standard deviation (a) of segment lengths, which can be figured out from the text. 2. **Similarity Between Sentences**: This measures how similar the sentences in a segment are to each other. The more words they share, the more similar they are. The numerator here is the total number of shared words in the segment. If the parameter r is 2, then this similarity measure is like looking at the \"density\" of the segment—how much \"information\" (shared words) is packed into it. When r is 2, it’s called \"generalized density,\" which helps control how much the size of the segment affects its importance. High similarity within a segment (lots of shared words) means a higher value for this part.",
        "formal_text": "Convert casual text to formal text: So, the total cost for dividing the text into K segments is made up of the costs for each segment. The cost for each segment is a combination of two things, adjusted by a parameter"
    },
    {
        "casual_text": "All the Cascade models scored better than the Isolated model in terms of F-score. The highest F-score for the Cascade model was 67.29%, which we got by using 18 features in experiment (iv). This shows that POS information really helps with sentence boundary detection.",
        "formal_text": "Convert casual text to formal text: All the Cascade models scored better than the Isolated model in terms of F-score. The highest F-score for the Cascade model was 67.29%, which we"
    },
    {
        "casual_text": "• News that happens over a short period, like within a year, tends to get better coverage compared to news that drags on for a longer time.",
        "formal_text": "• News that happens over a short period, like within a year tends to get better coverage compared to news that drags on for a longer time. • News that happens over a short period, like within a"
    },
    {
        "casual_text": "Sometimes, mistakes happen because of vowel lengthening that doesn’t usually signal a difference in pronunciation. In line 3, you can see how the dialect version of \"he\" (meaning \"he/she\"), which is \"het,\" sometimes shows up as \"heet\" in dialect material. It’s like someone just emphasized it in a way that makes the vowel longer, which is unexpected. This kind of random vowel stretching doesn’t happen often, but when it does, it usually messes up the prediction. It’s also weird because when the model sees something rare or unusual, it tends to predict the regular form without any changes.",
        "formal_text": "Convert casual text to formal text: Sometimes, mistakes happen because of vowel lengthening that doesn’t usually signal a difference in pronunciation. In line 3, you can see how the dialect version of \"he\" (meaning \""
    },
    {
        "casual_text": "Alright, so let's talk about how MoS gets better. The biggest boost comes from using a bunch of different hidden states as inputs. Adding more partitions doesn't really help much—it might not help at all. Now, when you compare MFS to Softmax, the improvement is about 4.5%, which is way smaller than the 15% improvement you see in GPT-2.",
        "formal_text": "Convert casual text to formal text: Alright, so let's talk about how MoS gets better. The biggest boost comes from using a bunch of different hidden states as inputs. Adding more partitions doesn't really"
    },
    {
        "casual_text": "E-HowNet uses expressions to define lexical senses, which are basically entities and relationships. This means you can figure out all the taxonomic relationships between lexical senses based on their E-HowNet definitions. Synonyms are easy to spot because they have the same E-HowNet expressions, and hyponymy relationships are found through attribute-value subsumption. (Just a heads-up: right now, E-HowNet only identifies near-synonym classes because its expressions are kind of broad.) Plus, new categories are created by looking at shared attribute-values. For example, pandas and zebras can both be grouped as animals because they share the same feature: black and white markings.",
        "formal_text": "Convert casual text to formal text: E-HowNet uses expressions to define lexical senses, which are basically entities and relationships. This means you can figure out all the taxonomic relationships between lexical senses"
    },
    {
        "casual_text": "If the text didn't pop up at the start of a trial like it was supposed to, they had to redo the calibration. The experimenters were told to keep calibrating until the average validation error was less than 0.3.",
        "formal_text": "Convert casual text to formal text: If the text didn't pop up at the start of a trial like it was supposed to, they had to redo the calibration. The experimenters were told to keep calibrating until the average"
    },
    {
        "casual_text": "We tested our method on a bunch of different tasks: Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, and Machine Reading Comprehension. For Sentiment Classification, we used the Stanford Sentiment Treebank (SST-2) dataset (Socher et al., 2013). In Paraphrase Similarity Matching, we worked with the Microsoft Research Paraphrase Corpus (MRPC) (Dolan and Brockett, 2005) and the Quora Question Pairs (QQP) dataset. For Natural Language Inference, we checked out Multi-Genre Natural Language Inference (MNLI) (Williams et al., 2017), QNLI (Rajpurkar et al., 2016), and Recognizing Textual Entailment (RTE).",
        "formal_text": "Convert casual text to formal text: We tested our method on a bunch of different tasks: Sentiment Classification, Paraphrase Similarity Matching, Natural Language Inference, and Machine Reading Comprehension. For Sentiment"
    },
    {
        "casual_text": "We’ve come up with a method to figure out which entities in a text are important (focus) and which are just background info (ground), using something called class association rules. We’re specifically looking at how this can help identify pathogens that are the main focus of a study. Focus pathogens usually show up in the title and results sections of abstracts, where the main findings are highlighted. Our little experiment suggests that paying attention to how things are written in the text can give us useful hints to spot these focus pathogens. Since we only had a small amount of data to work with, this is more of a proof of concept. We’re working on gathering a bigger dataset, which will let us test more detailed rules and explore other methods, like centrality and transformer-based approaches.",
        "formal_text": "Convert casual text to formal text: We’ve come up with a method to figure out which entities in a text are important (focus) and which are just background info (ground), using something called class association rules. We’"
    },
    {
        "casual_text": "Sure! Here's a more casual version: Gen(h x, z, 0, max, , t): 1. Start by setting  0 and r  0. 2. Calculate the gradient: g   hx L (h x, z, C (h x )) 3. Keep going until  max:",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: Gen(h x, z, 0, max, , t): 1. Start by setting  0 and r"
    },
    {
        "casual_text": "We use the parallel corpus to train the translation model T. Along with the parallel corpus, we also use extra Mandarin transcriptions to train a combined word-level trigram language model G. This model has a vocabulary size of around 28,000 words. To improve the language model, we tweak the Kneser-Ney discounting method, setting the back-off threshold to 1 for unigrams and 2 for bigrams. By combining T and G, we can create a cross-lingual language model called G cl.",
        "formal_text": "Convert casual text to formal text: We use the parallel corpus to train the translation model T. Along with the parallel corpus, we also use extra Mandarin transcriptions to train a combined word-level trigram language model G."
    },
    {
        "casual_text": "It's called \"deasciifier\" because that's what the Turkish community already uses. - Publication coming soon. - Another one on the way.",
        "formal_text": "Convert casual text to formal text: It's called \"deasciifier\" because that's what the Turkish community already uses. - Publication coming soon. - Another one on the way."
    },
    {
        "casual_text": "(2) So, we gotta pre-train the generator using Equation 2 as the loss function to make sure it spits out grammatically correct sentences. If we don't, the discriminator will quickly learn how to tell the difference between real human-made sentences and the messed-up ones the generator makes at the start. This would totally mess up the training and stop it from reaching that Nash Equilibrium thing (thanks, Goodfellow et al., 2014).",
        "formal_text": "Convert casual text to formal text: (2) So, we gotta pre-train the generator using Equation 2 as the loss function to make sure it spits out grammatically correct sentences. If we don't,"
    },
    {
        "casual_text": "Most of the flower is a hermaphrodite (ryoseibana), which means it has both male and female parts. In the center (chuou no lkko), there's a female flower (mebana). And in the two corners (sokuhou no 2ko), you'll find male flowers (obana).",
        "formal_text": "Convert casual text to formal text: Most of the flower is a hermaphrodite (ryoseibana), which means it has both male and female parts. In the center (chuou no lkk"
    },
    {
        "casual_text": "So, the verb group here is called (diy gy hai). In this case, (kr) is labeled as a verb, but in example 40, it’s used as a noun.",
        "formal_text": "Convert casual text to formal text: So, the verb group here is called (diy gy hai). In this case, (kr) is labeled as a verb, but in"
    },
    {
        "casual_text": "You can check out the web version of the tool here: https://ceferra.shinyapps.io/ADoCS.",
        "formal_text": "Convert casual text to formal text: You can check out the web version the tool here: https://ceferra.shinyapps.io/ADoCS. Convert casual text to formal text: You can check"
    },
    {
        "casual_text": "For our experiments, we used the Wikipedia Edit Category Corpus (WPEC), which is freely available and was put together in earlier research (Daxenberger and Gurevych, 2012). This dataset breaks down each pair of adjacent revisions into smaller edits. This helps give a clearer view of how editing happens, since an author might make multiple separate edits in a single revision. Plus, each edit is tagged with one or more categories, which is super useful for pinpointing major changes, like when a whole new paragraph with text, references, and markup is added. There are four main types of edits: Insertions, Deletions, Modifications, and Relocations. These are figured out by doing a line-by-line comparison of the source text (including wiki markup). Following the approach suggested earlier (Daxenberger and Gurevych, 2012), only the specific part of the text that actually changes within a modified line is marked as an edit (either an Insertion, Deletion, or Modification), not the whole line. We also pulled in some extra data that wasn’t included in WPEC, like metadata and the plain text from the revisions (rv1 and rv). We did this using the Java Wikipedia Library (JWPL) along with the Revision Toolkit (Ferschke et al., 2011).",
        "formal_text": "Convert casual text to formal text: For our experiments, we used the Wikipedia Edit Category Corpus (WPEC), which is freely available and was put together in earlier research (Daxenberger and Gurevych, 2012)."
    },
    {
        "casual_text": "For Sentiment, we just pick random Polyjuice counterfactuals without worrying about their labels, as long as there’s at least one that changes the label of the original x. For NLI and QQP, we noticed in a small test that randomly picking counterfactuals might not be any better than just adding more regular data. We think Polyjuice doesn’t really understand the context or domain knowledge to make important changes, so it doesn’t add much beyond what pretraining already does (thanks, Longpre et al., 2020). So, instead, we use slicing functions to spot interesting patterns (like prepositions in NLI) and mess with those by putting [BLANK]s where the patterns are. For instance, \"His surfboard is beneath him\" turns into \"His surfboard is [BLANK] him\", and Polyjuice might then come up with stuff like \"His surfboard is beneath ) next to him.\"",
        "formal_text": "Convert casual text to formal text: For Sentiment, we just pick random Polyjuice counterfactuals without worrying about their labels, as long as there’s at least one that changes the label of the original x."
    },
    {
        "casual_text": "The extra SHAP weight, s(t), helps smooth things out by making sure outliers don’t get too much attention. The difference between what we expect and what actually happens is:",
        "formal_text": "Convert casual text to formal text: The extra SHAP weight, s(t), helps smooth things out by making sure outliers don’t get too much attention. The difference between what we expect and actually happens is: Con"
    },
    {
        "casual_text": "Knowledge distillation is all about teaching a smaller \"student\" network to do better by learning from a bigger \"teacher\" network, on top of learning from the regular training data. We usually assume the teacher network is already trained and our goal is to figure out the best parameters for the student. The idea is to train the student by making its predictions match the teacher's predictions. For classification tasks, this often involves matching the probabilities, either by using L2 on the log scale (as Ba and Caruana did in 2014) or by using crossentropy (like Li et al. in 2014 and Hinton et al. in 2015).",
        "formal_text": "Convert casual text to formal text: Knowledge distillation is all about teaching a smaller \"student\" network to do better by learning from a bigger \"teacher\" network, on top of learning from the regular training data. We usually"
    },
    {
        "casual_text": "We took a look at how people handle this tricky task using a bunch of speeches and also tested a bunch of modern NLP models to see how they perform. Turns out, human experts do way better than the automated methods we checked out, probably because they use some clever tricks that aren’t so straightforward. To sum it up, here’s what we’ve done: 1. We came up with a new NLU task: figuring out which long argumentative text best counters another one you’re given. 2. We thought it’d be cool to simulate this task in a debate-like setting, where you need to pick the speech that best refutes a given supporting speech. 3. We’ve got a big collection of over 3,600 recorded debate speeches that you can use to train and test your models on this debate task. 4. We’ve also run some experiments with different modern NLP models to see how they do on this task. 5. Finally, we compared human performance to these models and found that expert humans are still ahead of the game.",
        "formal_text": "Convert casual text to formal text: We took a look at how people handle this tricky task using a bunch of speeches and also tested a bunch of modern NLP models to see how they perform. Turns out, human experts"
    },
    {
        "casual_text": "So, we need to rearrange the phrases in the Lw transcript to match the order in the Lv transcript. This is done by shuffling around K phrases, like in Figure 2 where K = 3. But since randomly rearranging K phrases is super hard and takes forever (Knight, 1999), we gotta set some rules to limit how many ways we can shuffle them.",
        "formal_text": "Convert casual text to formal text: So, we need to rearrange the phrases in the Lw transcript to match the order in the Lv transcript. This is done by shuffling around K phrases, like in Figure 2 where K"
    },
    {
        "casual_text": "Alright, let’s break it down. KB tag types work well, and to show you why, think about the name \"West Indian.\" It pops up 65 times in 11 out of 33 wide-",
        "formal_text": "Convert casual text to formal text: Alright, let’s break it down. KB tag types work well, and to show you why, think about the name \"West Indian.\" It pops up 65 times in 11 out of"
    },
    {
        "casual_text": "When dealing with open text, we run into the issue of unknown words. In Japanese, since there aren't spaces between words, we first need to figure out where these unknown words are. To do that, we can check the spelling—like the sequence of characters—that might make up a word, or we can look at the context to see which words make sense in that situation.",
        "formal_text": "Convert casual text to formal text: When dealing with open text, we run into the issue of unknown words. In Japanese, since there aren't spaces between words, we first need to figure out where these unknown words are. To"
    },
    {
        "casual_text": "To make sure our method works well, we also checked how it stacks up against some other methods that don't need labeled data to work on sentences.",
        "formal_text": "Convert casual text to formal text: To make sure our method works well, we also checked how to stack up against some other methods that don't need labeled data to work on sentences. Convert casual text to formal text:"
    },
    {
        "casual_text": "Our approach is based on how sense subjectivity and contextual subjectivity are connected and includes two main rules, R1 and R2.",
        "formal_text": "Convert casual text to formal text: Our approach based on how sense subjectivity and contextual subjectivity connected and includes two main rules, R1 and R2. Convert casual text to formal text: Our approach based on how sense subject"
    },
    {
        "casual_text": "We looked at how infobox attributes show up in Wikipedia and noticed they follow a pattern where most attributes are pretty common, but there's a long tail of ones that only appear in a few articles. This means if we pick the right group of articles, we can cover a lot of the common attributes without needing to look at everything. We came up with a simple algorithm that picks a few infobox types, grabs a random bunch of articles from each, and then chooses three articles that have the most attributes. We ended up with 150 articles from 50 different types, and they cover about half of the common attributes you'll find on Wikipedia.",
        "formal_text": "Convert casual text to formal text: We looked at how infobox attributes show up in Wikipedia and noticed they follow a pattern where most attributes are pretty common, but there's a long tail of ones that only appear in a"
    },
    {
        "casual_text": "Text classification is all about giving a piece of text a label from a set of options. It's a pretty standard thing in natural language processing (NLP) and is used in lots of places, like sorting out spam emails, figuring out if someone's being positive or negative in a review, or even categorizing questions. The label set for TexSmart can be found on their website.",
        "formal_text": "Convert casual text to formal text: Text classification is all about giving a piece of text a label from a set of options. It's a pretty standard thing in natural language processing (NLP) and is used in"
    },
    {
        "casual_text": "So, these models have only worked well for a few specific text-based prediction tasks so far. Later on, the leader of this group was picked by a team of three bishops and six regular folks, instead of the usual seven people who used to make that decision. Some important cities in this group were Basel and Speyer. It ended in 1806, and some big moments in its history were the Investiture Controversy and the Golden Bull of 1356. It was led by Charles V, Frederick Barbarossa, and Otto I. For 10 points, can you name this group that mostly ruled over what is now Germany during the Middle Ages but didn't really control its main city? Also, when it comes to longer texts, like paragraphs, neural networks haven't proven to be great at giving detailed answers yet.",
        "formal_text": "Convert casual text to formal text: So, these models have only worked well for a few specific text-based prediction tasks so far. Later on, the leader of this group was picked by a team of three bishops and six"
    },
    {
        "casual_text": "Summarizing timelines is way different from the usual ways we summarize stuff, especially when it comes to things like the BP Oil Spill in the Gulf of Mexico. To get a better idea of why this is, we looked at a manually created timeline of the BP Oil Spill from Table 1, which is based on Reuters News. This helps us see how timelines are noticeably different from regular summaries. Traditional methods haven’t really thought about breaking down a big chunk of information (corpus) into smaller parts based on time stamps to understand how things connect across different time periods. But we found something interesting: there are two unique ways (trans-temporal correlations) that make timelines stand out. Here’s a snippet from Table 1, showing part of the human-made timeline about the BP Oil Spill in 2010 from Reuters: **April 22, 2010** The Deepwater Horizon rig, worth over $560 million, sinks, and a five-mile-long (8 km) oil slick is spotted.",
        "formal_text": "Convert casual text to formal text: Summarizing timelines is way different from the usual ways we summarize stuff, especially when it comes to things like the BP Oil Spill in the Gulf of Mexico. To get a better idea"
    },
    {
        "casual_text": "We compared our joint models (JMEClass and JMECause) with a bunch of pipeline models that were trained separately for either EClass or ECause using some fancy encoders. Here's what we used: - **ATT**: The attention network you see in Figure 2. - **LSTM**: The LSTM network in Figure 2. - **ATT+LSTM**: A combo encoder for emotion classification. It uses ATT on CurCL and LSTM on PrevCL and FolCL. - **ConvMSMemnet**: An encoder designed for emotion cause detection, using a convolutional multiple-slot deep memory network on CauseCL. In Table 2, you can check out how different emotion classification models performed. \"Sequence\" shows the input word sequences each model used, and each metric is the average performance across six emotion classes. Oh, and Table 3 shows how the six emotions performed in JMEClass.",
        "formal_text": "Convert casual text to formal text: We compared our joint models (JMEClass and JMECause) with a bunch of pipeline models that were trained separately for either EClass or ECause using some fancy encode"
    },
    {
        "casual_text": "We'll walk you through how we gather a big collection of text and video recipes, and then pair them up to train our unsupervised alignment algorithm.",
        "formal_text": "Convert casual text to formal text: We'll walk you how we gather a big collection of text and video recipes, and then pair them to train our unsupervised alignment algorithm. Convert casual text to formal text: We'll"
    },
    {
        "casual_text": "Since this paper is all about feature development, we’re just using a clustering method that helps us see how good our similarity matrix is for clustering. We picked agglomerative clustering with single linkage for this. Now, since each name might need its own settings, we wanted to check how much those settings matter for clustering. So, in our experiments, we tried two different approaches for the stop-threshold in agglomerative clustering: first, we tested every possible stop-threshold to find the best one for any ambiguous name and each feature model. The one that worked the best became our optimal stop-threshold. Second, we used a fixed stop-threshold that we got from our development data.",
        "formal_text": "Convert casual text to formal text: Since this paper is all about feature development, we’re just using a clustering method that helps us see how good our similarity matrix is for clustering. We picked agglomerative"
    },
    {
        "casual_text": "The BLEU score keeps dropping no matter what, but the TER score gets a little better for the English-Spanish and English-French pairs, especially if you use the sentence-based or two-steps estimation methods. Still, since the results for both scores don't really match up, we can't say for sure if this type of language model adaptation is better than just sticking with the basic baseline model.",
        "formal_text": "Convert casual text to formal text: The BLEU score keeps dropping no matter what, but the TER score gets a little better for the English-Spanish and English-French pairs, especially if you use the"
    },
    {
        "casual_text": "The brevity penalty is kind of a temporary fix to deal with the issue of not being able to calculate recall, which is a pretty big problem.",
        "formal_text": "Convert casual text to formal text: The brevity penalty is kind of kind of temporary fix to deal with the issue of not being able to calculate recall, which is a pretty big problem."
    },
    {
        "casual_text": "To get phrase embeddings, we use the method from Kitaev and Klein (2018), which combines the forward and backward LSTM vectors at the start and end of each phrase. For comparing these vectors, we follow Kobayashi et al. (2019), who use ELMo sentence embeddings for RST parsing. Unlike their work on document-level parsing, we focus on sentence-level parsing.",
        "formal_text": "Convert casual text to formal text: To get phrase embeddings, we use the method from Kitaev and Klein (2018), which combines the forward and backward LSTM vectors at the start and end of each"
    },
    {
        "casual_text": "Text autoencoders, like the ones introduced by Bowman et al. in 2016, have been pretty handy for a specific type of unsupervised problem called style transfer. Basically, style transfer means changing the style of a piece of text while keeping the original content intact. Some examples of this include sentiment transfer (Shen et al., 2017), making sentences shorter (Fevry and Phang, 2018), and even translating text from one language to another (Artetxe et al., 2018). Most current methods tweak autoencoders to focus on the style you're interested in by having the decoder pay attention to that style attribute (Lample et al., 2019; Logeswaran et al., 2018). The catch is, they need labels during training, which means they can't use pre-training on unlabeled data. And pre-training on unlabeled data has been a big deal for improving supervised NLP models in recent years, especially in text analysis (Peters et al., 2018; Radford et al., 2019; Devlin et al., 2019) and text generation tasks (Song et al., 2019; Vari and Bojar, 2019). As far as we know, there aren't any style transfer methods that are designed to take advantage of autoencoder pre-training, and only a few can be used that way (Shen et al., 2020; Wang et al.). Here's a high-level look at our supervised framework, Emb2Emb. On the left, we pre-train an autoencoder on unannotated text. This autoencoder takes an input sentence x, turns it into an embedding zx, and then uses that embedding to predict a reconstruction of the input sentence.",
        "formal_text": "Convert casual text to formal text: Text autoencoders, like the ones introduced by Bowman et al. in 2016, have been pretty handy for a specific type of unsupervised problem called style transfer. Basically,"
    },
    {
        "casual_text": "Okay, so we have Y = (Y 1, Y 2, • • •, Y M ), and we're coming up with M pattern evaluators called E j = (f j, g j ) for each one, where j goes from 1 to M.",
        "formal_text": "Convert casual text to formal text: Okay, so we have Y = (Y 1, Y 2, • • •, Y M ), and we're coming up with M pattern evaluators called E j ="
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. We’ve got token-level and span-level stuff happening here. Basically, we’re looking at the token and span representations from the teacher, which we call H l t,  l t  Lt l=0. Using equations 3 and 4, we figure out how these tokens and spans interact with each other and the angles between them within one sample. This gives us P l t, P l t  Lt l=0 and T l t, T l t  Lt l=0. Similarly, we can do the same thing for the student’s structural relationships.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. We’ve got token-level and span-level stuff happening here. Basically, we’re looking at the token and"
    },
    {
        "casual_text": "Okay, so here's the deal: First, the numbers we got—8 (0.6)  1.6 (1.5) 1.6 (1.3)—match what Kurita and Sgaard found in 2019. This shows that using RL helps the model figure out a decent strategy for ordering stuff (but only for ordering semantic actions in a sequence) and/or makes it better at handling its own mistakes (kind of like how dynamic oracles work). Second, we noticed that models without strict ordering (the \"free\" ones) did better than the ones with a set sequence. This backs up our guess that forcing a strict order, like the traditional hierarchy of analysis, isn't the best approach. Instead, MTI (which is a type of MTL) seems pretty powerful. Just to clarify, the sequential models we used here were more old-school MTL. Lastly, these two things—RL helping with ordering and MTI being better than strict ordering—work together to improve performance.",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: First, the numbers we got—8 (0.6)  1.6 (1.5) 1.6 (1.3)—match what Kurita and Sg"
    },
    {
        "casual_text": "• RQ1: Can we create machine learning models that are better than the current systems for detecting cyberbullying?",
        "formal_text": "• RQ1: Can we create machine learning models that that better than the current systems for detecting cyberbullying?"
    },
    {
        "casual_text": "Using the N-gram method, the translation process is basically like looking for the best possible translation option, called hypothesisê I 1. This best option is found by picking the one that gives the highest score when you combine a translation model (TM) with a bunch of other feature models, all added up in a log-linear way.",
        "formal_text": "Convert casual text to formal text: Using the N-gram method, the translation process is basically like looking for the best possible translation option, called hypothesisê I 1. This best option found by picking the one that gives the highest score when"
    },
    {
        "casual_text": "We're using R2 as our evaluation metric, which tells us how much of the variance in the data our model explains. The best score is 100, meaning it explains everything perfectly. In this case, the baseline model always guesses the average value (which is 4.2 for this dataset). So, if a model just predicts the mean every time, it gets an R2 of 0. Other baseline models, like one that always guesses 3.5 or 7, would get a negative R2 because they do worse than just guessing the mean. Even a random guesser would likely have a negative R2 on average.",
        "formal_text": "Convert casual text to formal text: We're using R2 as our evaluation metric, which tells us how much of the variance in the data our model explains. The best score is 100, meaning it explains everything perfectly"
    },
    {
        "casual_text": "So, compiling SI automatically is only really doable with super basic methods. These methods use \"negative\" dictionaries and kinda rough analysis techniques. They work well most of the time, but they're also tough enough to handle trickier situations without totally falling apart.",
        "formal_text": "Compiling SI automatically is only really doable with super basic methods. These methods use \"negative\" dictionaries and kinda rough analysis techniques. They work well most of the time, but they're also tough enough to handle trick"
    },
    {
        "casual_text": "This means that verbs in different tenses tend to have pretty different patterns, even if they have the same meaning. This ends up splitting a single meaning group into two (or more) tense-based groups. We checked this idea by looking at how tense and the groups we found (called clusters or senses) are related, using a measure called Normalized Mutual Information (NMI). We calculated the NMI between the tense of verbs in sentences and the most likely group they belong to in different setups, as well as between the verbs and the actual correct groups (gold clusters). The results are in Table 3. From the gold clusters, we found that there’s almost no connection (0.15) between tense and meaning. But when we used SP without lemmatization (w/o LEM), the connection got way stronger (0.67). If we didn’t use either lemmatization or SP (w/o LEM and SP), the connection was 0.27, which is closer to the gold number. Lemmatization on its own naturally makes the connection with tense weaker, and using the full model (Final model) gave us a connection of 0.22, which is pretty close to the gold number of 0.15.",
        "formal_text": "Convert casual text to formal text: This means that verbs in different tenses tend to have pretty different patterns, even if they have the same meaning. This ends up splitting a single meaning group into two (or more)"
    },
    {
        "casual_text": "Complex operations, like doing fancy stuff with objects and groups, such as regrouping or splitting things up.",
        "formal_text": "Convert casual text to formal text: Complex operations, like doing fancy stuff with objects and groups, such regrouping or splitting things things. Complex operations, like doing fancy stuff with objects and groups, such regrouping or splitting things. Complex operations"
    },
    {
        "casual_text": "This kind of annotation is basically what MT-EQuAl uses. But when it comes to these tools, MT-EQuAl takes it a notch higher because one of our main goals was to make it super user-friendly. The error analysis interface in MT-EQuAl is straightforward and really easy to use. It also has some cool visualization features that help lighten the mental load on the annotators, so they can just focus on the task at hand (check out Section 2.2 for more details).",
        "formal_text": "Convert casual text to formal text: This kind of annotation is basically what MT-EQuAl uses. But when it comes to these tools, MT-EQuAl takes it a notch higher because one of"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: 1. **a_jc = t_jc / sum over m of t_jc**: Basically, this is just dividing t_jc by the sum of all t_jc values for different m. 2. **t_jc = exp(W_t * h_jc + U_t * e_iw + b_t)**: Here, t_jc is calculated by taking the exponential of a combination of vectors and biases. It's like multiplying some vectors (W_t and U_t) with other vectors (h_jc and e_iw) and adding a bias (b_t), then applying the exponential function. 3. **h_ic = tanh(W_c * (e_j-2c  e_j-1c  e_jc  e_j+1c  e_j+2c) + b_c)**: This part is about calculating h_ic. You take a bunch of vectors (e_j-2c, e_j-1c, e_jc, e_j+1c, e_j+2c), concatenate them (that's what the  means), multiply by a weight matrix (W_c), add a bias (b_c), and then apply the tanh function to squash the result between -1 and 1. So, in short: - **a_jc** is a normalized value based on t_jc. - **t_jc** is a weighted sum of vectors and biases, passed through an exponential function. - **h_ic** is a combination of several vectors, multiplied by a weight matrix, added to a bias, and then squished by the tanh function.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: 1. **a_jc = t_jc / sum over m of t_jc**:"
    },
    {
        "casual_text": "We also checked out how many alignment links our systems were creating. One thing that stood out was that the Bayesian HMM with NULL words consistently made way fewer links compared to all the other systems. If we use the BHMM as a baseline, the other models created, on average across languages and translation directions, 39.5% more links (for our collocation-based model), 39.2% more (Giza++), and 36.2% more (fastAlign). Making more links makes it harder for the phrase extraction process and results in smaller phrase tables. In practice, the phrase tables from the collocation-based model are about three times smaller than those from the Bayesian HMM. So, the collocation-based system has an edge here.",
        "formal_text": "Convert casual text to formal text: We also checked out how many alignment links our systems were creating. One thing that stood out was that the Bayesian HMM with NULL words consistently made way fewer links compared to all the"
    },
    {
        "casual_text": "So far, multi-task learning in patent translation has mostly been about trying out different combinations of translation and language models, but only within specific groups of patent sections. For instance, Utiyama and Isahara (2007) and Tinsley et al. (2010) looked at how translation and language models work when trained on different patent sections. They found that having more parallel data helps improve the results. Ceauşu et al. (2011) also noticed that language models always improve, and translation models usually do too, when they have more data from different sections. In our approach, we use models trained on this combined patent data as a starting point for comparison.",
        "formal_text": "Convert casual text to formal text: So far, multi-task learning in patent translation has mostly been about trying out different combinations of translation and language models, but only within specific groups of patent sections. For instance, Utiyama and"
    },
    {
        "casual_text": "After checking out two versions of the Fast Abs RL model—one with regular sentences and one with extra info about the other person (you can find this in Section 4.2)—we figured out that adding that extra info helps get better ROUGE scores.",
        "formal_text": "Convert casual text to formal text: After checking out two versions of the Fast Abs RL model—one with regular sentences and one with extra info about the other person (you can find this in Section 4.2)—we figured out"
    },
    {
        "casual_text": "So, y_i is the predicted probability, and W is the final optimized weight for the dense layer after the model's training. dense_i is the output from the dense layer, and b is just a bias term. We're treating Uyghur metaphor detection as a sequence tagging task, and here's how we set up the loss function.",
        "formal_text": "Convert casual text to formal text: So, y_i is the predicted probability, and W is the final optimized weight for the dense layer after the model's training. dense_i is the output from the dense layer, and"
    },
    {
        "casual_text": "This difference isn't always clear-cut (there are definitely some gray areas), but it’s still helpful when translating because definiteness can come from more than just a noun phrase—it can also come from the whole sentence. As we’ll see later, figuring out how things refer to each other works differently depending on whether we’re talking about a thing (a nominal referent) or an event.",
        "formal_text": "Convert casual text to formal text: This difference isn't always clear-cut (there are definitely some gray areas), but it’s still helpful when translating because definiteness can come from more than just a noun phrase"
    },
    {
        "casual_text": "The equation above is the main part of our CLO algorithm, which we're calling CLO-v1. You can find all the details about how it's derived and proven in the Appendix. Basically, the key idea behind our cross-level optimization is to use gradient information from both parts of the training set, which makes updating  more accurate and dependable.",
        "formal_text": "Convert casual text to formal text: The equation above is the main part of our CLO algorithm, which we're calling CLO-v1. You can find all the details about how it's derived and proven in the App"
    },
    {
        "casual_text": "But, our short unit transformation method still has a few issues. We're working on fixing them to make MT even better. Right now, we've only tested our approach on a parallel corpus from the abstract paper domain, where Chinese characters are way more common in Japanese than in other areas. In the future, we're planning to test it on parallel corpora from different domains.",
        "formal_text": "Convert casual text to formal text: But, our short unit transformation method still has a few issues. We're working on fixing them to make MT even better. Right now, we've only tested our approach on a"
    },
    {
        "casual_text": "Over the past few years, a bunch of researchers have looked into phrase segmentation models for phrase-based statistical machine translation (SMT). Folks like Blackwood et al. (2008), Xiong et al. (2010), Lee et al. (2011), and Xiong et al. (2011) have all pointed out why this model is super important. Here's why: First off, it helps group words in a sentence so the system can think about how words work together or what's happening between phrases. This was mentioned by Blackwood et al. (2008), Lee et al. (2011), and Xiong et al. (2011). Second, it’s also needed to break up the input sentence in a way that keeps the translation smooth, even when phrases get moved around (Blackwood et al., 2008). Oh, and there’s a noticeable difference between segmentations that lead to good translations and those that don’t (Lee et al., 2011).",
        "formal_text": "Convert casual text to formal text: Over the past few years, a bunch of researchers have looked into phrase segmentation models for phrase-based statistical machine translation (SMT). Folks like Blackwood et al. (2008),"
    },
    {
        "casual_text": "Another cool thing about the sieve framework is how easy it is to add new stuff without needing to know everything else that’s already in there. We’ve made the code available for everyone to use, so you can either run it as a stand-alone coreference system or build on it to create something new.",
        "formal_text": "Convert casual text to formal text: Another cool thing about the sieve framework is how easy it is to add new stuff without needing to know everything else that’s already in there. We’ve made the code available for everyone to"
    },
    {
        "casual_text": "Alright, let’s move on to the main point of this paper: figuring out a quick way to describe the whole regularization path for the maximum entropy problem. We’ve already shown that the best solution, p, can be easily found using the variable . So, instead of dealing with p directly, we just need to keep track of how  changes as we move across the (, ) plane, starting from  = 0 and ending at the last change point, which we called (, ). In this part, we’ll explain an algorithm that does this by finding where l0 crosses the lines ln, ln+1, ..., l1, l1, ..., ln. After each intersection, we’ll update l0 and keep going until we’ve covered everything.",
        "formal_text": "Convert casual text to formal text: Alright, let’s move on to the main point of this paper: figuring out a quick way to describe the whole regularization path for the maximum entropy problem. We’"
    },
    {
        "casual_text": "So, the whole idea behind generalisation is to figure out how different variables are connected. Take Table 1, for instance. It combines stuff like long-term temperature changes (Milankovitch cycles and all that) and yearly temperature variations from 1958 to 2010 into a single category called \"annual temperature.\" But here's the thing: a lot of these generalised variables are one-of-a-kind and don’t really help us see how things relate. So, if we stick to the original variables and only keep the ones that show up at least twice, we end up with 17,613 different variable types.",
        "formal_text": "Convert casual text to formal text: So, the whole idea behind generalisation is to figure out how different variables are connected. Take Table 1, for instance. It combines stuff like long-term temperature changes (Milankovitch cycles"
    },
    {
        "casual_text": "6.1 Settings Data Set We tested our method using Japanese Wikipedia. For every Wikipedia page, we treated the title as a noun phrase and used both the title and the main text for breaking it down. We ran our segmentation process on each page individually.",
        "formal_text": "Convert casual text to formal text: 6.1 Settings Data Set We tested our method using Japanese Wikipedia. For every Wikipedia page, we treated the title as a noun phrase and used both the title and the main text for breaking it down"
    },
    {
        "casual_text": "The P2Q model for attention distribution works the same way. The designs of Q2P and P2Q don’t just stick to basic reading comprehension models—they also tackle the issue of attention wandering by minimizing the impact of individual lyrics that get messed up by unrelated verbs. Plus, the conditional attention feature ties the results of the QCP mode into the calculations for the other two attention modes, so they’re no longer missing out on the bigger picture of overall meaning.",
        "formal_text": "Convert casual text to formal text: The P2Q model for attention distribution works the same way. The designs of Q2P and P2Q don’t just stick to basic reading comprehension models—they also tackle the issue of attention"
    },
    {
        "casual_text": "Looking for families that are expanding. We have two ways of measuring this growth for MFEP: absolute growth and relative growth. Absolute growth is when we",
        "formal_text": "Convert casual text to formal text: Looking for families that are expanding. We have two ways measuring this growth for MFEP: absolute growth and relative growth. Absolute growth is when we measure this growth for MFEP: absolute growth"
    },
    {
        "casual_text": "Standard LM works better than tf-idf, but it doesn't consider where words are placed in a sentence. On the other hand, Positional ranking in LM-ASM does a great job at ranking sentence candidates accurately and takes word positions and order into account. It consistently beats standard LM when it comes to MAP, MRR, and recall (check out Table 3). The only downside is that it takes a bit longer to process each query, but the trade-off is worth it.",
        "formal_text": "Convert casual text to formal text: Standard LM works better than tf-idf, but it doesn't consider where words are placed in a sentence. On the other hand, Positional ranking in LM-"
    },
    {
        "casual_text": "For semi-tightness, MAX Co (wf) works well. Basically, this theorem tells us that a metric is tight if it gives lower complexity to a permutation that breaks down into a PET with a shorter average length for the primal permutation.",
        "formal_text": "Convert casual text to formal text: For semi-tightness, MAX Co (wf) works well. Basically, this theorem tells us that a metric is tight if it gives lower complexity"
    },
    {
        "casual_text": "Second, we were focused on how specific the terms were to certain areas. Our plan was to measure this using numbers by comparing how often words were used in different groups of texts. But for this particular study, we went with people's opinions instead. Melamed (1996b) says that to really evaluate translation word lists, judges need to see both languages side by side, showing how the words are used in sentences. But getting people to judge without seeing the full context would be simpler in both real-world and lab settings. As a first try, we had three people do a kind of tagging job: one was a pro French/English translator, and the other two were grad students at Penn. They tagged a bunch of entries that came from an older version of the SABLE system (this one used small pieces of sentences to figure out which words often appear together; check out Section 2.2 for more). They didn’t get to see the full sentences in both languages this time. After the study, looking at how the system did and reading comments from the taggers in a follow-up survey, we saw that context really matters. To get a better idea of how much it matters, we asked one of the taggers from the first round to do the same job again, but this time she got to see the full context—both languages side by side, showing up to the first ten times each word pair was used.",
        "formal_text": "Convert casual text to formal text: Second, we were focused on how specific the terms were to certain areas. Our plan was to measure this using numbers by comparing how often words were used in different groups of texts. But for this particular"
    },
    {
        "casual_text": "Sure! Here's a more casual version: For tasks that involve figuring out the relationship between two sentences (like textual entailment), a special token (like [SEP]) is automatically added between the sentences. This helps the model know where one sentence ends and the other begins.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: For tasks that involve figuring out the relationship between two sentences (like textual entailment), a special token (like [SEP"
    },
    {
        "casual_text": "Sure! Let's break this down in a simpler way. First off, think of eq. 4 as something that helps us understand how similar a piece of text is to a group of sentences, called a \"cluster.\" A Language Model (LM) trained on a specific cluster is kind of like a summary of all the sentences in that cluster. When we tweak the LM's weights using a text, each weight gives us a clue about how similar that text is to the cluster the LM represents. Now, vectors t and d are like \"fingerprints\" for individual training sentences and the whole development cluster, respectively. The cos() operation we use on these vectors is just a way to measure how similar the training sentences are to each cluster m.",
        "formal_text": "Convert casual text to formal text: Sure! Let's break this down in a simpler way. First off, think of eq. 4 as something that helps us understand how similar a piece of text is to a"
    },
    {
        "casual_text": "Unsupervised text generation has been getting more attention lately because it doesn't need parallel data for training. A popular method is to take a long text, shrink it down to a short version, and then try to rebuild the long text using something called a cycle consistency loss (Miao and Blunsom, 2016; Wang and Lee, 2018; Baziotis et al., 2019). The problem is, the space of the compressed sentences is hard to work with, so this approach often needs reinforcement learning (or similar techniques), which can make training tricky (Kreutzer et al., 2021).",
        "formal_text": "Convert casual text to formal text: Unsupervised text generation has been getting more attention lately because it doesn't need parallel data for training. A popular method is to take a long text, shrink it down to a short version,"
    },
    {
        "casual_text": "Alright, so _tj is a number that shows how well the stuff around position j in the input lines up with the output at position t. Using this context vector c_t, the decoder creates a sequence of words, one by one. This whole process of generating text step by step is what we call a language model.",
        "formal_text": "Convert casual text to formal text: Alright, so _tj is a number that shows how well the stuff around position j in the input lines up with the output at position t. Using this context vector"
    },
    {
        "casual_text": "We back up the claim in Appendix B by showing how it works, using a Cantor-set approach based on Chomsky and Schützenberger's (1959) theorem. The main takeaway is that RNNs can handle any deterministic context-free language (CFL) as long as they have infinite precision. The deeper the stack needs to go in the construction, the more precision is required. This means that if the stack depth is limited, RNNs with fixed precision can still process strings of any length. You can kind of see this in Korsky and Berwick's (2019) work too, with a bit of extra effort.",
        "formal_text": "Convert casual text to formal text: We back up the claim in Appendix B by showing how it works, using a Cantor-set approach based on Chomsky and Schützenberger's (1959)"
    },
    {
        "casual_text": "We've included the results from our LSTM domain adaptation models in Table 3. For a quick comparison, we've also copied the results from Daumé III (2009), which were applied to feature-rich CRFs, in the leftmost column.",
        "formal_text": "Convert casual text to formal text: We've included the results from our LSTM domain adaptation models in Table 3. For a quick comparison, we've also copied the results from Daumé III (2009), which were applied"
    },
    {
        "casual_text": "The rest of the paper goes like this: Section 2 looks at past work on clustering for finding relationships without supervision. We also mention some methods we used for comparison. Section 3 explains our clustering method. In Section 3.1, we show our algorithm for pulling out patterns from dependency trees, and in Section 3.3, we talk about our way of finding and ranking important patterns. Section 4 covers how we tested our stuff, what we did, the data we used, and the results from the two tests. Lastly, Section 5 wraps things up.",
        "formal_text": "Convert casual text to formal text: The rest of the paper goes like this: Section 2 looks at past work on clustering for finding relationships without supervision. We also mention some methods we used for comparison. Section 3 explains our clustering"
    },
    {
        "casual_text": "You could say that these mistakes affect the user experience differently. For example, a correctly spelled word being changed incorrectly is more annoying than a misspelled word that isn’t corrected but is marked as wrong. But in this paper, we’re treating all errors the same. E1, E2, E3, and E4 are all about the correction process. So, we can come up with something called the Correction Error Rate (CER).",
        "formal_text": "Convert casual text to formal text: You could say that these mistakes affect the user experience differently. For example, a correctly spelled word being changed incorrectly is more annoying than a misspelled word that isn’t corrected"
    },
    {
        "casual_text": "We took a look at how well our commitment-based system for RTE did with the 1600 examples from the PASCAL RTE-2 and RTE-3 datasets. Table 1 shows the results we got when we trained our system on those 1600 examples from the RTE-2 and RTE-3 Test Sets.",
        "formal_text": "Convert casual text to formal text: We took a look at how well our commitment-based system for RTE did with the 1600 examples from the PASCAL RTE-2 and RTE-3 datasets. Table 1 shows the results"
    },
    {
        "casual_text": "We've created a fresh dataset to test how well systems can pull out relationships from text. First, we’ll talk about the old dataset that everyone usually uses, and then we’ll explain why we made a new one and how we went about doing it.",
        "formal_text": "Convert casual text to formal text: We've created a fresh dataset to test how well systems can pull out relationships from text. First, we’ll talk about the old dataset that everyone usually uses, and then explain why we made"
    },
    {
        "casual_text": "Just passing on knowledge from the output layer doesn't do much for helping the student handle those tricky, implicit conversation scenarios. If the student network is super deep, the feedback from the output layer barely makes a dent in updating or regularizing the parameters of the middle layers. This means the imitation learning setup hits a plateau pretty fast (like Romero et al. mentioned in 2015). The whole thing kind of stalls the student's ability to go deeper and make the model even better.",
        "formal_text": "Convert casual text to formal text: Just passing on knowledge from the output layer doesn't do much for helping the student handle those tricky, implicit conversation scenarios. If the student network is super deep, the feedback from the output layer barely makes"
    },
    {
        "casual_text": "[old span] [new span]",
        "formal_text": "[old span] [new span] Convert casual text: [old span] [new span] Convert casual text to formal text: [old span] [new span] [new span] [new span] [new span]"
    },
    {
        "casual_text": "But Greenberg's basic word order stuff doesn't really explain how these three parsing algorithms work. Right away, you might wonder if using a different feature-model with the same algorithms would give similar results. Could the difference in performance be because of how the treebank is annotated? Maybe a different classifier, like something other than the Memory-based one, would do a better job? Anyway, these are questions for future research.",
        "formal_text": "Convert casual text to formal text: But Greenberg's basic word order stuff doesn't really explain how these three parsing algorithms work. Right away, you might wonder if using a different feature-model with the same"
    },
    {
        "casual_text": "Figure 1 shows how many labels each account has in the dataset, which highlights that sexist content often involves multiple labels.",
        "formal_text": "Convert casual text to formal text: Figure 1 shows how many labels each account has in the dataset, which highlights that sexist content often involves multiple labels. Figure 2 shows how many labels each account has in the dataset, which highlights that"
    },
    {
        "casual_text": "Alright, so we're training F  (y|x, q, a) using two main sources: the COCO dataset, which we used for MLE pre-training back in Section 4.2, and some extra text from Wikipedia. For both COCO and Wikipedia, we generate pairs of (x, q, a, y) and (null, q, a, y) examples from their sentences.",
        "formal_text": "Convert casual text to formal text: Alright, so we're training F  (y|x, q, a) using two main sources: the COCO dataset, which we used for MLE pre-training"
    },
    {
        "casual_text": "But, benchmarks don’t really capture the messy, complicated stuff you deal with in real-world situations. To get closer to how doctors actually think, we thought about using differential diagnosis prediction, which means bringing in more types of clinical data. Still, when it comes to using knowledge graphs in real-world settings, we might need to think about things like how time plays a role. For example, predicting how a disease might change over time could help us understand the long-term side of diagnosing. Part of what we’re working on is coming up with tasks that reflect the real challenges doctors face when making decisions.",
        "formal_text": "Convert casual text to formal text: But, benchmarks don’t really capture the messy, complicated stuff you deal with in real-world situations. To get closer to how doctors actually think, we thought about using differential diagnosis prediction, which"
    },
    {
        "casual_text": "We tested our methods on four Chinese NER datasets: OntoNotes (Weischedel et al., 2011), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017a), and Resume NER (Zhang and Yang, 2018). OntoNotes and MSRA come from news articles, and they have gold-standard segmentation for the training data. For OntoNotes, the development and testing data also have gold segmentation. On the other hand, Weibo NER is from social media, and Resume NER is from resumes. These two datasets don’t have gold-standard segmentation. Table 1 has all the stats for these datasets. As for the lexicon, we used the same one as Lattice-LSTM. It includes 5.7k single-character words, 291.5k two-character words, 278.1k three-character words, and 129.1k other words. We also used the same pre-trained character embeddings as Lattice-LSTM, which were trained on the Chinese Giga-Word dataset using word2vec.",
        "formal_text": "Convert casual text to formal text: We tested our methods on four Chinese NER datasets: OntoNotes (Weischedel et al., 2011), MSRA (Levow, 2006), Weib"
    },
    {
        "casual_text": "The taxonomy is created straight from the input documents using a mix of different methods: one uses language patterns, another taps into the semantic web, and there's also a new neural network that helps clean up and expand the taxonomies. These taxonomies can be accessed through another part of the KGIS called Smart Spreadsheet (SSS). It's basically an interactive table that you can edit. The first row has the main categories (we call them hypernyms), and each column under it lists their specific examples (called hyponyms). For simplicity, we'll just refer to them as types and instances.",
        "formal_text": "Convert casual text to formal text: The taxonomy is created straight from the input documents using a mix of different methods: one uses language patterns, another taps into the semantic web, and there's also a new"
    },
    {
        "casual_text": "First off, we test our WSI clustering method on two things: (a) the SemEval 2010 and SemEval 2013 datasets; and (b) a brand-new test set we made specifically for largescale WSI. In section 9, we also check how accurate static embeddings are when we use a Wikipedia dataset that’s been processed to identify word senses. When we’re gathering word substitutes, we do a few things: we lemmatize the top-k list, combine any lemmas that are the same, take out stopwords and the main target word, and then we keep the top-5 lemmas that are left.",
        "formal_text": "Convert casual text to formal text: First off, we test our WSI clustering method on two things: (a) the SemEval 2010 and SemEval 2013 datasets; and (b) a brand"
    },
    {
        "casual_text": "Basically, this is saying: There's some person x who's dead, and they had a parent y who's also dead. Both x and y died from lung cancer. There's also a specific date z when x died, and that date is before 1960.",
        "formal_text": "Convert casual text to formal text: Basically, this is saying: There's some person x who's dead, and they had a parent y who's also dead. Both x and y died from"
    },
    {
        "casual_text": "For generation, we tweak mBART, which was pre-trained on 25 languages. We use a dropout of 0.3, label smoothing of 0.2, 2500 warm-up steps, a max learning rate of 3105, and batches with 1024 tokens each. For text filling, we randomly mask 35% of the words in each instance, picking span lengths based on a Poisson distribution ( = 3.5). Then, we add an end-of-sentence token ( /S >) and the language ID symbol ( LID >) to each instance. We didn't go crazy trying to find the best parameters for generation; we just stuck with the default settings in the open-source code *. The final models are chosen based on validation likelihood. For SLU, we use the XLM-R large model, which has around 550 million parameters, as our backbone. During fine-tuning, we set the batch size to 128, with 10 fine-tuning epochs (E_all = 10), 4 initialization epochs (E = 4), and a dropout of 0.1 for two benchmark datasets. The maximum filtering rates are set to 0.2 and 0.3 for the SNIPS and MTOP datasets, respectively. The learning rates are 2  105 for SNIPS and 5  105 for MTOP. We picked the best hyperparameters by testing different combinations of batch size, learning rate, fine-tuning epochs, initialization epochs, and filtering ratios. The ranges we looked at were: batch size 32, 64, 128, learning rate 1, 2, 3, 4, 5  105, fine-tuning epochs 5, 10, 15, initialization epochs 2, 3, 4, and filtering ratio 10%, 20%, 30%, 40%.",
        "formal_text": "Convert casual text to formal text: For generation, we tweak mBART, which was pre-trained on 25 languages. We use a dropout of 0.3, label smoothing of 0.2, 2500 warm-"
    },
    {
        "casual_text": "Working in Computational Linguistics often means dealing with some kind of quantitative evaluation. A bunch of standard measures, originally from Information Retrieval (like the ones in Manning & Schütze's 1999 book), are pretty common. But over the past decade or so, there's been a lot of discussion and critique about these measures themselves (check out Entwisle & Powers, 1998, and Flach, 2003).",
        "formal_text": "Convert casual text to formal text: Working in Computational Linguistics often means dealing with some kind of quantitative evaluation. A bunch of standard measures, originally from Information Retrieval (like the ones in Manning &"
    },
    {
        "casual_text": "1. Figuring out the main results listed in trial registries and mentioned in the article.",
        "formal_text": "Convert casual text to formal text: 1. Figuring the main results listed in trial registries and mentioned in the article. 2. Figuring out the main results listed in trial registries and mentioned in the article. Convert casual text"
    },
    {
        "casual_text": "Since SHAP weights show how much a token, or word, usually affects the outcome when it's hidden, we also pay attention to words that tend to behave unusually on average.",
        "formal_text": "Convert casual text to formal text: Since SHAP weights show how much a token, or word, usually affects the outcome when it's hidden, we also pay attention to words that tend to behave unusually on average."
    },
    {
        "casual_text": "Schalte einfach zwischen den Schnappschüssen hin und her, bis du die richtigen Einstellungen gefunden hast.",
        "formal_text": "Schalte einfach zwischen den Schnappschüssen hin und her, bis du die richtigen Einstellungen gefunden hast. Convert casual text to formal text: Schalte einfach zwischen den Schnappschüssen hin und her,"
    },
    {
        "casual_text": "Here's the informal version: Our idea is that if two sentences score high on TGMs, it means they’re pretty similar in terms of meaning. If this happens a lot within a group of sentences, it shows that the group is well-organized or coherent. Following what Douven and Meijs (2007) suggested, we’re looking at three ways to use and tweak TGMs to measure how thematically coherent a group of sentences is. The main differences between these methods come down to: (a) which pairs of tweets we use to apply the metrics, and (b) how we combine the scores to give a final coherence score for the whole group. The TGMs we’re using in this study are BERTScore (Zhang et al., 2019), MoverScore (Zhao et al., 2019) for both single words and pairs of words, and BLEURT (Sellam et al., 2020). We also used a simpler metric based on how similar the TF-IDF representations of tweets are, using cosine similarity, to see how much word pairings affect coherence. Each of these methods has its pros and cons, which we’ll explain below.",
        "formal_text": "Convert casual text to formal text: Here's the informal version: Our idea is that if two sentences score high on TGMs, it means they’re pretty similar in terms of meaning. If this happens a lot within"
    },
    {
        "casual_text": "In 2015 and 2016, they came up with EventBuilder, which creates text summaries for social events using Wikipedia and also shows how the event is being talked about on social media.",
        "formal_text": "Convert casual text to formal text: In 2015 and 2016, they came up with EventBuilder, which creates text summaries for social events using Wikipedia and also shows how the event is talked about on social media. Convert casual text"
    },
    {
        "casual_text": "The second issue is that translations with the highest TM3LM-score often don't match up with what human evaluators rank as the best. Table 2 illustrates this. The table has three 2x2 confusion matrices for three different Japanese-to-English MT systems: J-E TDMT, D3, and J-E SMT. Each matrix compares how well the TM3LM-score aligns with human evaluator rankings. The (1, 1) and (0, 0) cells show the percentage of agreement, while the (1, 0) and (0, 1) cells show the percentage of disagreement. In the confusion matrix for J-E SMT, the (1, 0) cell has a higher number than the (0, 1) cell. This means the TM3LM-score often gives the highest score to translations from J-E SMT, even when human evaluators don't rank them as the best. On the other hand, for the confusion matrices of J-E TDMT and D3, the (0, 1) cell has a higher number than the (1, 0) cell. This means the TM3LM-score doesn't usually give the highest score to translations from these systems, except for J-E SMT, even if human evaluators rank them as the best.",
        "formal_text": "Convert casual text to formal text: The second issue is that translations with the highest TM3LM-score often don't match up with what human evaluators rank as the best. Table 2 illustrates this."
    },
    {
        "casual_text": "We're suggesting using CAN for analyzing sentiment from multiple angles. To make it work better, we're adding some tricks—like making sure the attention weights are neat and tidy with orthogonal and sparse regularizations. This helps the model focus better on the specific aspects of the sentence, leading to more accurate representations.",
        "formal_text": "Convert casual text to formal text: We're suggesting using CAN for analyzing sentiment from multiple angles. To make it work better, we're adding some tricks—like making sure the attention weights are neat and tidy with ortho"
    },
    {
        "casual_text": "The last part of picking sentences for the summary uses the ILP method from a paper by Gillick and others in 2009. We look at pairs of words (bi-grams) as our main ideas and use how often they show up in the text as their importance. Since we try different ways to shorten each sentence, we add a rule: for each sentence, only one of its best shortened versions can make it into the summary. For the part that shortens sentences, the ILP method we talked about earlier just finds the best way to shorten a sentence. To get a few good options (the n-best compressions), we use a trick: after finding one good answer, we add a little rule to stop it from giving the same answer again. This way, we get different options each time.",
        "formal_text": "Convert casual text to formal text: The last part of picking sentences for the summary uses the ILP method from a paper by Gillick and others in 2009. We look at pairs of words (bi-grams) as our main"
    },
    {
        "casual_text": "There’s no way to optimize the topic CVaR objective with a scalable, online algorithm. A lot of DRO problems can be handled efficiently using batch optimization methods that rely on Lagrangian duality. But that approach doesn’t work for topic CVaR because the dual form needs exact calculations, not just rough estimates.",
        "formal_text": "Convert casual text to formal text: There’s no way to optimize the topic CVaR objective with a scalable, online algorithm. A lot of DRO problems can be handled efficiently using batch optimization methods that rely on La"
    },
    {
        "casual_text": "Our algorithm has three main steps: 1. First, we assign each instance a probability distribution of possible word replacements based on a neural biLM (check out section 2.1 for details). 2. Next, we pick k \"representatives\" for each instance. Each of these representatives is made up of multiple samples from the word replacement distribution we just talked about (section 2.3 explains this). 3. Finally, we group the representatives into clusters and use those clusters to create a soft-clustering for the instances (you can find more in section 2.4).",
        "formal_text": "Convert casual text to formal text: Our algorithm has three main steps: 1. First, we assign each instance a probability distribution of possible word replacements based on a neural biLM (check out section 2.1 for details). 2."
    },
    {
        "casual_text": "Another area of research involves predicting the overall result of a case. For example, you might try to guess which side the outcome will favor (like Aletras et al. did in 2016), or whether the current court will uphold or overturn a decision made by a lower court (as Katz et al. explored, see Figure 1 for an example). Unlike them, we’re taking it a step further. Instead of just predicting a simple binary outcome (or including an \"other\" category), we’re focusing on the specific details of the case, like the charges involved. This means our output can have multiple labels.",
        "formal_text": "Convert casual text to formal text: Another area of research involves predicting the overall result of a case. For example, you might try to guess which side the outcome will favor (like Aletras et al. did in"
    },
    {
        "casual_text": "One reason is the small amount of training data. This also suggests that just using a straight-up sequence-to-sequence model might not cut it.",
        "formal_text": "Convert casual text to formal text: One reason is the small amount of training data. This also suggests that just using a straight-up sequence-to-sequence model might not cut it."
    },
    {
        "casual_text": "Multiple Instance Learning (MIL) is a type of weakly supervised learning where data is grouped into \"bags,\" and each bag gets a single label, not the individual items inside it (Keeler and Rumelhart, 1991). Most MIL approaches (like Zhou et al., 2009; Wei et al., 2014; Pappas and Popescu-Belis, 2017; Haußmann et al., 2017; Tu et al., 2019; Ilse et al., 2018; Wang and Wan, 2018) concentrate on how well the bag as a whole is classified, but there are a few methods that look at how well individual items within the bag are classified. On top of the usual bag-level loss, Kotzias et al. (2015) added a regularization term to the objective function that considers how similar the individual items are to each other. Peng and Zhang (2019) took a different approach by assigning the bag's label to each item under the assumption that the items are independent and identically distributed (i.i.d.), and then directly defined a loss function based on predicting the label of each individual item. Some research has tried applying MIL to tasks like sentiment classification at the sentence level. Kotzias Wan 2018 and Angelidis and Lapata (2018b) suggested training a sentiment classifier for sentences using document-level annotations. In these cases, the content of each instance (like words in a sentence) is already known. However, for tasks like DMSC (Document-level Multi-aspect Sentiment Classification), the relevant text snippets related to a specific aspect, which are key for determining sentiment, aren't given upfront. This makes DMSC quite different and much harder to apply MIL to. Plus, none of these methods have considered the issue of overfitting to the bag-level supervision.",
        "formal_text": "Convert casual text to formal text: Multiple Instance Learning (MIL) is a type of weakly supervised learning where data is grouped into \"bags,\" and each bag gets a single label, not the individual items"
    },
    {
        "casual_text": "To create a deep semantic representation for generating summaries, we need both the propositions, which are the key points we want to include, and a way to process the text. For historical reasons, our summarizer's front-end uses the Stanford parser. Back then, propositions based on dependency relations seemed like the best match for KvD's idea of propositions. Once we figured out that just extracting information wasn't enough, and after the ACE generator became available, we started trying out deep generation of summaries using DMRS, which is what this paper talks about. The downside now is that we have to run two parsers on every sentence that ends up in the final summary.",
        "formal_text": "Convert casual text to formal text: To create a deep semantic representation for generating summaries, we need both the propositions, which are the key points we want to include, and a way to process the text. For"
    },
    {
        "casual_text": "Alright, so the main idea here is that the task has two parts (Logeswaran et al., 2018): keep the original content and make sure the style matches what we want. It’s kind of like balancing two things at once. We call this a \"tradeoff,\" and we handle it by mixing two types of loss terms together, represented as L_task.",
        "formal_text": "Convert casual text to formal text: Alright, so the main idea here is that the task has two parts (Logeswaran et al., 2018): keep the original content and make sure the style matches what"
    },
    {
        "casual_text": "Our aim is to create a rumor detection system by training it on labeled data from M topics. The idea is to make the system really good at spotting rumors in any given situation.",
        "formal_text": "Convert casual text to formal text: Our aim is to create a rumor detection system by training it on labeled data from M topics. The idea is to make the system really good at spotting rumors in any"
    },
    {
        "casual_text": "In real-world situations, we usually rely on gradient optimizers to find a decent set of parameters for our neural network. But here's the thing: the optimization problem actually has clear, exact solutions if you look closely at specific parts of the problem.",
        "formal_text": "Convert casual text to formal text: In real-situations, we usually rely on gradient optimizers to find a decent set of parameters for our neural network. But here's the thing: the optimization problem actually has clear,"
    },
    {
        "casual_text": "Let’s call LCLR the LC transform with some tweaks, so it only works on nonterminals that are left-recursive. The fifth row in Table 4 shows what happens when we use LCLR on the three original grammars. LCLR shrinks the non-left-recursive parts of the CT and ATIS grammars a lot, but for the PT grammar, it only makes a small difference. This makes sense if you look at Table 1, because almost all the rules in the PT grammar are for left-recursive nonterminals. But, we can do some extra steps, like what we did with Paull’s algorithm, to cut down the number of rules for left-recursive nonterminals before using our modified LC transform. The sixth and seventh rows in Table 4 show what happens if we left factor the grammar before using LCLR (LF+LCLR), and also if we group non-left-recursive rules for left-recursive nonterminals before using LCLR (LF+NLRG+LCLR).",
        "formal_text": "Convert casual text to formal text: Let’s call LCLR the LC transform with some tweaks, so it only works on nonterminals that are left-recursive. The fifth row in Table 4 shows what happens"
    },
    {
        "casual_text": "We calculate the conditional probability vectors P[+c91zi] and P[zll + sl, +s2] using the disjunctive interaction model: 4 3. A natural language processing system also needs to spread out probabilities to the semantic hypotheses so it can work with the interpreted information.",
        "formal_text": "Convert casual text to formal text: We calculate the conditional probability vectors P[+c91zi] and P[zll + sl, +s2] using the disjunctive interaction model: 4 3. A"
    },
    {
        "casual_text": "Basically, a hidden model for a sequence w = w 1: T can be described by looking at the possible hidden factors that could explain it.",
        "formal_text": "Convert casual text to formal text: Basically, a hidden model for a sequence w = w 1: T can be described by looking at the possible hidden factors that could explain it. Basically, a hidden model"
    },
    {
        "casual_text": "Alright, so the value r from Equation 5 is used as another feature in the loglinear translation model, and it's scaled by  r. Just to clarify, this applies to the usual cases where there's a reordering of just one word.",
        "formal_text": "Convert casual text to formal text: Alright, so the value r from Equation 5 is used as another feature in the loglinear translation model, and it's scaled by  r. Just to clarify,"
    },
    {
        "casual_text": "The Guangzhou-based New Express is making a pretty unusual public request for the release of journalist Chen Yongzhou.",
        "formal_text": "Convert casual text: The Guangzhou-based New Express is making pretty unusual public request for the release of journalist Chen Yongzhou. Convert casual text: The Guangzhou-based New Express is making a pretty unusual"
    },
    {
        "casual_text": "In Table 2, we’ve got the results from our pairwise alignment algorithm, comparing it to some baseline methods using 1,625 text-video recipe pairs from YouCook2 that were aligned by humans. The BM25 baseline did better than two of the other textual similarity baselines. Among the textual similarity baselines, RoBERTa came out on top, showing that using a pretrained sentence-level embedding works really well for this alignment task. Interestingly, the uniform alignment baseline actually beat all the other baselines. This is mostly because, in the YouCook2 dataset, the text instructions and the transcript sentences are in the same order, so uniform alignment ends up being a surprisingly strong baseline. Our unsupervised HMM+IBM1 alignment model absolutely crushed all the baselines, with a p-value less than 0.001. It got way higher precision scores compared to everything else. When we tweaked the HMM+IBM1 model, using all the words to learn alignments gave us the best results.",
        "formal_text": "Convert casual text to formal text: In Table 2, we’ve got the results from our pairwise alignment algorithm, comparing it to some baseline methods using 1,625 text-video recipe pairs from YouCook2 that were aligne"
    },
    {
        "casual_text": "In 2012, we discovered that a DSM created with multiplicative composition—which was suggested by Mitchell and Lapata in 2010 for a totally different task—works better than the model from Schutze in 1998.",
        "formal_text": "Convert casual text to formal text: In 2012, we discovered that a DSM created with multiplicative composition—which was suggested by Mitchell and Lapata in 2010 for a totally different task—works better than the model from"
    },
    {
        "casual_text": "To set up the baselines, we use interpolated 4-gram Kneser-Ney language models built with BerkeleyLM (as mentioned by Pauls and Klein in 2011). Figure 2 gives us a closer look at how the pseudo sentence generation happens during each EM iteration. As we move further into the iterations, we notice improved pseudo-precision and pseudo-recall at the sentence level, which you can see in Figure 2(a) and Figure 2(b). Figure 2 also shows that our learning framework is pretty good at learning. However, we find that the baselines aren’t really effective for this task. This isn’t surprising, though—we’ve mentioned this in our earlier work (Cuong and Sima'an, 2014).",
        "formal_text": "Convert casual text to formal text: To set up the baselines, we use interpolated 4-gram Kneser-Ney language models built with BerkeleyLM (as mentioned by Pauls and Klein in 2011). Figure 2"
    },
    {
        "casual_text": "Equation (9) gives the recognition result in a language with lots of resources. If the system needs results in a language with fewer resources, we should use Equation (10) instead. In Eq. (10),  is a projection operator (from Mohri, 2009) that changes the input label to the output label. Before decoding, the recognition transducer, called ASR, can be made better by doing a determinization operation right after each step. Check out Figure 3 for an example of how to build a cross-lingual language model using WFSTs: - First, a word sequence like w1, w2, w3 is represented by the Lw language model G (b1). - Then, it's split into a phrase sequence like w1, w2 w3 (b2). - Next, w1, w2 w3 gets reordered to w2 w3, w1 (b3). - After that, the phrase w2 w3 is changed to v2 v3 (b4). - Finally, v2 v3 is put back into a word sequence v2, v3 (b5). Here, wk and vk stand for wk and vk, and \"-\" means the  or null symbol. Extra symbols like #1, #2, etc., are added to make the WFST work better (as explained by Mohri, 2009). This helps the transducer get optimized by a determinization operation, which makes the search network smaller and more efficient.",
        "formal_text": "Convert casual text to formal text: Equation (9) gives the recognition result in a language with lots of resources. If the system needs results in a language with fewer resources, we should use Equation (10) instead. In"
    },
    {
        "casual_text": "Adult -----Cost -----------Double Room -------------Single Room ------------Extra Bed",
        "formal_text": "Adult -----Cost -----------Double Room -------------Single Room ------------Extra Bed"
    },
    {
        "casual_text": "Okay, so we're dealing with some technical stuff here. Let's break it down in simpler terms. We're looking at the i-th entity type label, which we call T P i. N(tp) is just the total number of different entity types in our knowledge graph. And |V(ed)|? That's just 2 times N(tp) plus 1. Now, for the sequence tagging task—which is like labeling each part of a sequence—we're using an LSTM. This is a type of neural network that's good at handling sequences, and it was introduced by Hochreiter and Schmidhuber back in 1997. The module we're using is set up like this:",
        "formal_text": "Convert casual text to formal text: Okay, so we're dealing with some technical stuff here. Let's break it down in simpler terms. We're looking at the i-th entity type label, which we call T"
    },
    {
        "casual_text": "Nucleus sampling actually works better than beam search for generating questions, even if you're only making one question per prompt. The problem with some QG metrics is that they only care about how similar the questions are to the ground truth, which actually hurts diversity. This means they don't do a good job predicting how well those diverse questions will perform in real QA tasks. But, if you create a metric that balances diversity with similarity to the ground truth, it ends up being a much better indicator of how well those questions will work in QA.",
        "formal_text": "Convert casual text to formal text: Nucleus sampling actually works better than beam search for generating questions, even if you're only making one question per prompt. The problem with some QG metrics is that they only care about"
    },
    {
        "casual_text": "We checked out how our best model stacked up against the neural baseline, looking at both lenient and strict scores, plus the number of split-antecedents. To get more data for this comparison, we tested both models on a mix of the test and development sets.",
        "formal_text": "Convert casual text to formal text: We checked out how our best model stacked up against the neural baseline, looking at both lenient and strict scores, plus the number of split-antecedents. To get more data for this"
    },
    {
        "casual_text": "But honestly, most of the good stuff in this area comes from actually building a complem system. From the start, though, you should pick formalisms that make it easy to move information around in a clear way. We believe LFG fits the bill for this.",
        "formal_text": "Convert casual text to formal text: But honestly, most of the good stuff in this area comes from actually building a complem system. From the start, though, you should pick formalisms that make it easy to move information"
    },
    {
        "casual_text": "Sure, let's break it down in a simpler way: 1. **D KL (   v   u ) = i v i (ln v i  ln u i )** This is just saying that the Kullback-Leibler divergence (which is a way to measure how different two probability distributions are) between vectors v and u is calculated by taking each element of v, multiplying it by the natural log of that element minus the natural log of the corresponding element in u, and then summing all those up. 2. **s  (   u,   v ) = D KL (   v    u + (1  )   v )** Here, s  is a function that takes two vectors, u and v, and mixes them together with a parameter . It then calculates the KL divergence between this mixed vector and the original v. 3. **R D (   v   u ) = 1 1 + D KL (   v ||   u )** Finally, R D is a ratio that compares the KL divergence between v and u to 1. It's basically a normalized version of the KL divergence, making it easier to interpret or use in certain contexts. So, in short: - The first part calculates how different two vectors are using KL divergence. - The second part mixes two vectors and then checks how different the mix is from one of them. - The third part gives a simpler, normalized measure of how different the two vectors are.",
        "formal_text": "Convert casual text to formal text: Sure, let's break it down in a simpler way: 1. **D KL (   v   u ) = i v i"
    },
    {
        "casual_text": "We train the mapping  using the objective described in (1), where L adv is based on training the discriminator disc as outlined in (10). In reality, we switch back and forth between updating  and disc in batches. In our experiments in Section 3, we’ll look at how sensitive the model is to  adv, and we found it has a pretty big impact. So, in real-world use, it’s probably best to think of it as a hyperparameter that needs tuning.",
        "formal_text": "Convert casual text to formal text: We train the mapping  using the objective described in (1), where L adv is based on training the discriminator disc as outlined in (10). In reality, we switch back and"
    },
    {
        "casual_text": "We use Google Translator to translate our training data, D src, from English into the target languages. Besides just translating the text, we also need to figure out how to map the slot labels to the target language. To do this, we tried two methods: giza++ (Och and Ney, 2003) and fastalign (Dyer et al., 2013). After testing, we found that giza++ gives us better results—it improves performance by around 2% in F1 score on the SNIPS dataset. So, for the rest of the paper, we stick with Google Translator and giza++ to handle the translation and label mapping. We'll call the translated training data D trans.",
        "formal_text": "Convert casual text to formal text: We use Google Translator to translate our training data, D src, from English into the target languages. Besides just translating the text, we also need to figure out how to map the"
    },
    {
        "casual_text": "The first change we're making is adding some syntactic features to the CCG categories. Normally, in basic CCG trees, a nominal adjective (like \"a tall boy\") is labeled as N/N, and a predicate adjective (like \"John is tall\") is labeled as S adj N P. To make the meaning of both types of adjectives consistent, we're changing N/N to N adj /N for nominal adjectives.",
        "formal_text": "Convert casual text to formal text: The first change we're making is adding some syntactic features to the CCG categories. Normally, in basic CCG trees, a nominal adjective (like \"a tall boy\")"
    },
    {
        "casual_text": "It seems like the ReWrite system works better on the EMEA corpus, and that might be because the sentences there are mostly short and follow a formula—just 9 tokens on average. The hill-climbing algorithm doesn't need to do much to get decent translations, but the constraint satisfaction inference seems to overdo it by doing more transformations than necessary.",
        "formal_text": "Convert casual text to formal text: It seems like the ReWrite system works better on the EMEA corpus, and that might be because the sentences there are mostly short and follow a formula—just 9 tokens on average"
    },
    {
        "casual_text": "Alright, so we’ve got 22 words to work with, and they have different numbers of meanings. Specifically, 7 words have 2 meanings each, 6 have 3 meanings, 6 have 4 meanings, and 3 have 6 meanings. For each word, we tried four different ways of grouping these meanings, which adds up to 88 experiments in total. The results are laid out in Tables 1, 2, and 3. In these tables, the real number of meanings for each word is listed in the columns, and the number we predicted is in the rows.",
        "formal_text": "Convert casual text to formal text: Alright, so we’ve got 22 words to work with, and they have different numbers of meanings. Specifically, 7 words have 2 meanings each, 6 have 3 meanings, 6"
    },
    {
        "casual_text": "Alright, so the methods we're using to improve the performance, speed, and memory efficiency of a grammar-checking system come from research focused on the syntax of a specific language—like Czech, and also Bulgarian. Because of that, these techniques are really tied to the language they were developed for. But it’s pretty clear that the main idea behind them could be adapted to other languages too. Introducing these techniques helps move the system closer to being a practical, real-world tool. At least in terms of:",
        "formal_text": "Convert casual text to formal text: Alright, so the methods we're using to improve the performance, speed, and memory efficiency of a grammar-checking system come from research focused on the syntax of a specific language—"
    },
    {
        "casual_text": "Since there's no official validation set, we're setting aside 10% of the training sentences to use for tweaking the hyperparameters and deciding when to stop early.",
        "formal_text": "Convert casual text to formal text: Since there's no official validation set, we're setting aside 10% of the training sentences to use for tweaking the hyperparameters and deciding when to stop early. Convert casual text"
    },
    {
        "casual_text": "In this part, we’ll talk about two main things: (1) the text-generating language models (LMs) and how they’re set up, and (2) the attributors we’re looking at. To meet our research goals, we need a dataset made up of texts generated by different pre-trained and fine-tuned LMs, each with its own settings. The publicly available datasets just don’t cut it because the conditions under which the text was created can vary a lot. For our study, it’s super important to have control over things like: the model’s architecture, the prompts used to generate text, the sampling parameters, and the size and topics of the data used for fine-tuning. We’ll also give you the lowdown on this generated dataset in this section.",
        "formal_text": "Convert casual text to formal text: In this part, we’ll talk about two main things: (1) the text-generating language models (LMs) and how they’re set up, and (2) the attributors we’re"
    },
    {
        "casual_text": "Sure! Here's the informal version: \"In the DE->EN category, the source is at 39%, the common is 20%, and the target is 41%. For RO->EN, the source is 50%, the common is 8%, and the target is 42%. And for VI->EN, the source is 36%, the common is 11%, and the target is 53%.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's the informal version: \"In the DE->EN category, the source is at 39%, the common is 20%, and the target is 41%. For RO->EN,"
    },
    {
        "casual_text": "For the Machine Reading Comprehension task, we use the RACE dataset (Lai et al., 2017), which is a big dataset made from English exam questions. It has 25,137 passages and 87,866 questions. Each question has four possible answers, but only one is right. The dataset is split into RACE-M and RACE-H, with questions for middle school and high school students, respectively.",
        "formal_text": "Convert casual text to formal text: For the Machine Reading Comprehension task, we use the RACE dataset (Lai et al., 2017), which is a big dataset made from English exam questions. It has 25,"
    },
    {
        "casual_text": "[7] Body Involvements This part talks about the main body parts that are usually involved when doing something. Like, if you're \"opening\" something physically, you're probably using your \"hands\" and \"arms.\" We've got five groups for this: head, arms, torso, legs, and other body parts.",
        "formal_text": "Convert casual text to formal text: [7] Body Involvements This part talks about the main body parts that are usually involved when doing something. Like, if you're \"opening\" something physically, you're"
    },
    {
        "casual_text": "These techniques can create really good summaries, but the way they generate sentences is just one small part of the whole summarization process. It's kind of separate from the bigger picture of choosing what content to include, which is more of a global decision. The only systems we know of that handle both sentence selection and compression together are Martins and Smith's (2009) system and Nishikawa's (2014) system for Japanese text.",
        "formal_text": "Convert casual text to formal text: These techniques can create really good summaries, but the way they generate sentences is just one small part of the whole summarization process. It's kind of separate from the bigger picture of choosing"
    },
    {
        "casual_text": "We took the implementation from Müller et al. (2013), which was originally made for sequence labeling, and tweaked it to work for general monotone Seq2Seq tasks. In sequence labeling, the input and output sequences always have the same length, but with Seq2Seq, the input can be shorter, longer, or the same length as the output.",
        "formal_text": "Convert casual text to formal text: We took the implementation from Müller et al. (2013), which was originally made for sequence labeling, and tweaked it to work for general monotone Seq2Seq tasks."
    },
    {
        "casual_text": "For the MLM task, you don't need to do any annotation, so you can just use a plain text file.",
        "formal_text": "Convert casual text to formal text: For the MLM task, you don't need to do any annotation, so you can just use a plain text file. Convert casual text to formal text: For the MLM task,"
    },
    {
        "casual_text": "Relevance Judgements. The goal here is to estimate edit distance scores, so the target documents (the ones we consider relevant) are the ones with the highest LS scores compared to the query. The \"relevance judgements\" come from finding the sentence pairs in the example base that have the highest LS score (check out Equation 1) for a given query. Basically, we assume that all sentences with the smallest distance (or highest similarity) are correct or relevant. This can result in a lot of relevant results for queries that have many exact or close matches in the data. For the EN-TR data, on average, each query has 4.74 relevant results, while for the EN-FR data, it's 16.38 relevant results per query. To handle these ties in the EBMT system, we just pick the highest-scoring document with the lowest document ID for the next translation steps.",
        "formal_text": "Convert casual text to formal text: Relevance Judgements. The goal here is to estimate edit distance scores, so the target documents (the ones we consider relevant) are the ones with the highest LS scores compared to the query."
    },
    {
        "casual_text": "In the SNIPS dataset, we have a mix of less frequent (tail) and more frequent (head) phrases, kind of like how things get tricky in real-life situations. After processing, we end up with 1,309 labeled phrases and 7,944 unlabeled paraphrase pairs. Just a heads-up: we can only use the FD and LP parts of MARUPA here, not PD, because the paraphrase pairs are already provided. For IC and SL, we use a neural model that combines ELMO embeddings (from Peters et al., 2018) with a BiLSTM. The model takes in the ELMO-encoded phrases and runs them through the BiLSTM. It predicts slot labels from each hidden state and intent labels from the average of those states. We’re using 1024-dimensional ELMO embeddings, a single-layer 200-dimensional BiLSTM, and a dropout rate of 0.2 after both layers. The model is trained using Adam with a batch size of 16 and early stopping to prevent overfitting. For evaluation, we stick with SNIPS' original test and validation sets.",
        "formal_text": "Convert casual text to formal text: In the SNIPS dataset, we have a mix of less frequent (tail) and more frequent (head) phrases, kind of like how things get tricky in real-life situations. After processing"
    },
    {
        "casual_text": "Back in 2018, Ramisa and team were one of the first to work on generating captions for news article images. They used a fancy setup with an encoder-decoder model. The encoder part had a deep convolutional thing called VGG (from Simonyan and Zisserman) for the images and Word2Vec (by Mikolov and others) for the text. Then, they used an LSTM to decode everything into a caption. Later, in 2019, Biten and company came up with the GoodNews dataset and a two-step method for generating captions. They used ResNet-152 (from He and team) to handle the image part and GloVe embeddings (by Pennington and others) for sentence-level stuff. Here's how their two-step process worked: First, they made a caption but left blanks for different types of named entities like PERSON or ORGANIZATION (you can see these in Table 1). After that, they filled in those blanks by matching entities from the top sentences in the article. This method was good for dealing with rare names and stuff, but it didn't make the captions very rich in language. Plus, it could cause errors to carry over from one step to the next.",
        "formal_text": "Convert casual text to formal text: Back in 2018, Ramisa and team were one of the first to work on generating captions for news article images. They used a fancy setup with an encoder-decoder model."
    },
    {
        "casual_text": "There are more advanced techniques like word embeddings (Mikolov et al., 2013) and topic modeling algorithms (Blei et al., 2003) that can help figure out how often certain topics show up in documents and how words are connected, even if those connections change over time. Then, there are transformer models like BERT (Bi-directional Encoder Representations from Transformers) (Devlin et al., 2019), which have taken text processing to a whole new level. These models are designed to categorize text, summarize it, or even answer specific questions based on the input they get.",
        "formal_text": "Convert casual text to formal text: There are more advanced techniques like word embeddings (Mikolov et al., 2013) and topic modeling algorithms (Blei et al., 2003) that can help"
    },
    {
        "casual_text": "The ALSC and ACD tasks both use the same attention mechanism, but they don’t share any parameters. The reason for keeping them separate is that, for the same aspect, ALSC pays more attention to opinion words, while ACD is more focused on the actual aspect target terms. You can check out the attention visualizations in Section 4.6 to see this in action.",
        "formal_text": "Convert casual text to formal text: The ALSC and ACD tasks both use the same attention mechanism, but they don’t share any parameters. The reason for keeping them separate is that, for the same aspect, ALSC pays more"
    },
    {
        "casual_text": "Here's the bilingual Table 4 with BLEU and chrF scores. Like Gao et al. (2020), we noticed that a higher  usually boosts bilingual translation quality. Plus, using MLS can make things even better. This suggests that it's not just about increasing the probability of target vocabulary, but also how smoothed probabilities are distributed across different languages that helps improve translation performance.",
        "formal_text": "Convert casual text to formal text: Here's the bilingual Table 4 with BLEU and chrF scores. Like Gao et al. (2020), we noticed that a higher  usually boosts bilingual"
    },
    {
        "casual_text": "To make our training data even more diverse, we use a multilingual version of BART (mBART) (Liu et al., 2020a) to create extra training examples in the target language. Basically, we take the pre-trained mBART model and fine-tune it on our translated training data, called D trans. We do this by using a denoising approach (Liu et al., 2020a)—which means we compare the output of the decoder with the original input and calculate the cross-entropy loss. The input we give to mBART includes the dialog act and the utterance from D trans.",
        "formal_text": "Convert casual text to formal text: To make our training data even more diverse, we use a multilingual version of BART (mBART) (Liu et al., 2020a) to create extra training"
    },
    {
        "casual_text": "The company was built using the Universal Systems Language, which is based on her Development Before the Fact (DBTF) approach for designing systems and software.",
        "formal_text": "Convert casual text: The company was built using the Universal Systems Language, which is based on her Development Before the Fact (DBTF) approach for designing systems and software. Convert casual text: The company was built using the Universal Systems"
    },
    {
        "casual_text": "There have been a bunch of studies looking at supervised learning techniques for ranking and picking sentences. Kupiec and his team (1995) came up with a naive Bayes classifier to figure out if a sentence is worth pulling out. More recently, people have been using Conditional Random Fields (CRF) and Structural SVMs for summarizing single documents (Shen et al., 2007; Li et al., 2009).",
        "formal_text": "Convert casual text to formal text: There have been a bunch of studies looking at supervised learning techniques for ranking and picking sentences. Kupiec and his team (1995) came up with a naive Bayes"
    },
    {
        "casual_text": "Preference Learning is basically a tweak of SVM. In SVM, each training example is a pair (y_i, x_i), where x_i is a vector. If y_i = +1, it means x_i is a positive example, and if y_i = -1, it means x_i is a negative example. SVM figures out how to classify a new test vector x using something called a decision function.",
        "formal_text": "Convert casual text to formal text: Preference Learning is basically a tweak of SVM. In SVM, each training example is a pair (y_i, x_i), where x_i is a"
    },
    {
        "casual_text": "There are other methods that use dependency trees, like the ones by Liu et al. (2015) and Xu et al. (2015a, 2015b). These methods mainly focus on using different neural networks to work with the shortest dependency path (SDP) between entities. On the other hand, PECNN takes a different approach by looking at features from the entire dependency tree, not just the SDP. This means it considers information from outside the SDP too. The results from dos Santos et al. (2015) show that when position features are included, modeling the whole sentence works better than just focusing on the part between the entities. With the help of tree-based position features, our model can figure out which parts of the dependency tree are more important and tends to give more attention to the SDP.",
        "formal_text": "Convert casual text to formal text: There are other methods that use dependency trees, like the ones by Liu et al. (2015) and Xu et al. (2015a, 2015b). These methods mainly"
    },
    {
        "casual_text": "Both methods use a weighted Jaccard metric with mutual information vectors to figure out how similar things are. If you're curious about other ways to measure this, check out the work by Weeds and Weir from 2005.",
        "formal_text": "Convert casual text to formal text: Both methods use a weighted Jaccard metric with mutual information vectors to figure out how similar things are. If you're curious about other ways to measure this, check out the"
    },
    {
        "casual_text": "In this paper, we introduce a multitask active learning approach for NLU that leverages the relationship between ID and SF. We incorporate popular pool-based query strategies like Least Confidence, Margin Sampling, and Entropy into our framework. We also optimize the calculation for the entropy of a joint-model. The experiments show that our framework, combined with these query strategies, performs well with less training data compared to baseline methods across all datasets. The results also highlight that using the relationship between tasks can lead to better performance than just focusing on intents. Plus, our framework remains effective even when the model is updated. These findings suggest that the framework could be a good fit for real-world applications.",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce a multitask active learning approach for NLU that leverages the relationship between ID and SF. We incorporate popular pool-based query strategies like Least Confidence"
    },
    {
        "casual_text": "DEVICE. This tag is for labeling mentions of the specific device used in the fuel cell experiment, like \"IT-SOFC.\"",
        "formal_text": "Convert casual text to formal text: DEVICE. This tag is for labeling mentions of the specific device used in the fuel cell experiment, like \"IT-SOFC.\" This tag is for labeling mentions of the specific"
    },
    {
        "casual_text": "* Means there's a big difference between the scores when you compare them to the original machine translation (MT). From Figure 8, you can see that when you compare each post-editing method to the raw MT score (which we're calling 0), the human-based post-editing methods (Full Edit and Brief Edit) all got way higher ratings than the raw MT. Plus, the MT3 engine, when post-edited by Word-Perfect, got the highest rating out of all the MT engines.",
        "formal_text": "Convert casual text to formal text: * Means there's a big difference between the scores when you compare them to the original machine translation (MT). From Figure 8, you can see that when you compare each post-editing method"
    },
    {
        "casual_text": "To get MACHAMP up and running, you'll need three things: a configuration file, some input data, and a command to kick off either training or prediction. Let's break down what each of these is about.",
        "formal_text": "Convert casual text to formal text: To get MACHAMP up and running, you'll need three things: a configuration file, some input data, and a command to kick off either training or prediction. Let's break"
    },
    {
        "casual_text": "On VQAv2 and OK-VQA, PICa performs the best, but our FEWVLM large model does just as well on VQAv2. OK-VQA is different from other VQA datasets because it needs extra knowledge to answer questions. That's why bigger models and more pre-training data (prior knowledge) are important to improve performance. Cool thing is, FEWVLM * base, which was trained with only 4 examples, actually beats Frozen. For captioning tasks, FEWVLM base does way better than VL-T5 no-vqa, improving by 31.1 percentage points on NoCaps CIDEr.",
        "formal_text": "Convert casual text to formal text: On VQAv2 and OK-VQA, PICa performs the best, but our FEWVLM large model does just as well on VQAv2. OK-VQA"
    },
    {
        "casual_text": "Overall, adding the copy mechanism doesn't seem to make ReRef perform better based on the current evaluation methods. But here's something interesting: the Copy model uses way more words than ReRef—1,791 different word types compared to ReRef's 760. The human vocabulary in the test set is 2,332, while Ref only uses 366 word types. Looking closer at the vocabularies, Copy does produce a lot of low-frequency words, which is kind of what you'd expect from a copy mechanism. But, less ideally, it also includes some words with spelling mistakes. Another thing we noticed is that Copy tends to repeat words more often. In the test set, 18% of the sentences from Copy have two identical content words, like \"do you have the runway runway woman?\" Meanwhile, only 7% of ReRef's sentences do that. Maybe in the future, it could be worth trying something to control for repetitions, like the 'coverage' mechanism suggested by See et al. (2017).",
        "formal_text": "Convert casual text to formal text: Overall, adding the copy mechanism doesn't seem to make ReRef perform better based on the current evaluation methods. But here's something interesting: the Copy model uses way more words than Re"
    },
    {
        "casual_text": "Answer Normalization: When training and testing, we clean up the correct and guessed answers by getting rid of articles and punctuation before comparing them (check out Rajpurkar et al., 2016 for more details).",
        "formal_text": "Convert casual text to formal text: Answer Normalization: When training and testing, we clean the correct and guessed answers by getting rid of articles and punctuation before comparing them (check out Rajpurkar et al."
    },
    {
        "casual_text": "Before talking to someone, it's smart to think about what they already know. This way, you can use words or ideas that make sense to them and are more helpful. For example, if you're talking to a local or someone who's been to a place before, they might know certain landmarks that you can mention to explain where something is. Also, people might have different abilities, like how well they see or hear, so it's good to keep that in mind too when you're chatting with them.",
        "formal_text": "Convert casual text to formal text: Before talking to someone, it's smart to think about what they already know. This way, you can use words or ideas that make sense to them and are more helpful. For example, if"
    },
    {
        "casual_text": "Besides the model structures, stuff like pretrained word vectors, the way we label things, and the optimizer can really affect how well the system works. We looked at a bunch of these external factors using the two top models on a NER dataset: CLSTM+WLSTM+CRF and CCNN+WLSTM+CRF. Pretrained embeddings. Figure 4(a) shows how the two best models performed on the NER test set with two different types of pretrained word vectors, plus when they used random initialization. Using pretrained embeddings gave a big boost in performance compared to random initialization (p  0.01). The GloVe 100-dimension embeddings scored higher than SENNA (from Collobert et al., 2011) on both models, which matches what Ma and Hovy (2016) found.",
        "formal_text": "Convert casual text to formal text: Besides the model structures, stuff like pretrained word vectors, the way we label things, and the optimizer can really affect how well the system works. We looked at a bunch of these"
    },
    {
        "casual_text": "So, T is like a special number we use to adjust how smooth the distribution looks. L KD is calculated by finding the difference between the probability distributions given by the student and the teacher, which we call y s i and y t i. Basically, it's the cross-entropy between these two distributions.",
        "formal_text": "Convert casual text to formal text: So, T is like a special number we use to adjust how smooth the distribution looks. L KD is calculated by finding the difference between the probability distributions given by the student and the teacher,"
    },
    {
        "casual_text": "I picked the top-performing generation models for each type based on how well they did with the F1 part of BERTScore. I also played around with different beam widths for decoding and ended up settling on a beam width of 3. The best model in each category beat out the others across all the metrics.",
        "formal_text": "Convert casual text to formal text: I picked the top-performing generation models for each type based on how well they did with the F1 part of BERTScore. I also played around with different beam widths for decoding"
    },
    {
        "casual_text": "Sure! Here's a more casual version: You can explain some word rules pretty easily by using functions that show how different parts of a sentence fit together. For instance, the rule for translating the verb \"return\" can be based on whether it’s passive or active and whether it has a direct object or not. The rules we’re talking about here help us tell the difference between the intransitive (ergative) meaning and the transitive meaning of the verb:",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: You can explain some word rules pretty easily by using functions that show how different parts of a sentence fit together. For instance, the rule for translating"
    },
    {
        "casual_text": "Next, the method takes three vectors—(s; t a; r a), (s; t b; r b), and (s; t c; r c)—as shown in Figure 7, and turns them into three score-vectors. These score-vectors show whether something is non-inferior or inferior. The process looks like this: (T(s; t a ) 3 L(t a ); T (s; t a ); L(t a ); R a (t a )), (T(s; t b ) 3 L(t b ); T(s; t b ); L(t b ); R a (t b )), and (T(s; t c ) 3 L(t c ); T (s; t c ); L(t c ); R c (t c )).",
        "formal_text": "Convert casual text to formal text: Next, the method takes three vectors—(s; t a; r a), (s; t b; r b), and (s; t"
    },
    {
        "casual_text": "The system uses a cool new string comparison algorithm that works super fast, no matter how long the names are or how many different letters you allow. It doesn't need extra memory or anything, so it's way faster than other non-indexing methods. Plus, it doesn't care about the value of k, so it can handle really big k values where even indexing methods start to struggle. We also used an EM-based technique to spot spelling mistakes, and the results turned out to be really accurate.",
        "formal_text": "Convert casual text to formal text: The system uses a cool new string comparison algorithm that works super fast, no matter how long the names are or how many different letters you allow. It doesn't need extra memory or anything, so"
    },
    {
        "casual_text": "We use a GAN-based method where two teacher networks pass their knowledge to the student network with the help of two discriminators. These discriminators make things more flexible by not forcing the student network to be too closely tied to the teachers.",
        "formal_text": "Convert casual text to formal text: We use a GAN-based method where two teacher networks pass their knowledge to the student network with the help of two discriminators. These discriminators make things more flexible by not forcing the student network"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. We've got a variable x in a set called X. Now, when you see x with a number as a subscript (like x1) on the right side of a rule, it's referring to a specific part of x. But if you see x with a letter as a subscript (like xi), it's just a general placeholder, not pointing to any particular part of x.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in simpler terms. We've got a variable x in a set called X. Now, when you see x with a number"
    },
    {
        "casual_text": "In Section 5.2, we talked about how BART can handle Dynamic Blocking without needing any special task-adaptation or self-supervision. But, this approach doesn’t work as well because it doesn’t have the same level of quality, especially when it comes to keeping the syntax varied. This is because it wasn’t trained using the shuffling strategy during task-adaptation. We also noticed that when BART isn’t fine-tuned, it tends to try and generate different forms of a word when it’s blocked. To fix this a bit, we use a pattern library to list out all the different forms of a word that need to be blocked. For example, for the word \"give,\" we’d also block \"gives,\" \"gave,\" \"giving,\" and \"given.\" This method works for most languages that have inflections. We’ve included a comparison in Table 12, showing the output candidates for a specific example both with and without blocking these inflections.",
        "formal_text": "Convert casual text to formal text: In Section 5.2, we talked about how BART can handle Dynamic Blocking without needing any special task-adaptation or self-supervision. But, this approach doesn’t work as"
    },
    {
        "casual_text": "The weight vector we get from discriminative training is called W_D. With this new weight vector, we can create a new feature extraction operator by using the objective function from Eq. 2, but replacing W with W_D. For a given sample s, the feature representation produced by this new operator is written as IF_W_D(s). The main difference between W and W_D is that W_D was trained on a dataset with both negative and positive examples, so it includes negative weights. To make the training more effective, we doubled the negative weights. Figure 5 shows some examples of how learning the objective function differently can be helpful. The top part of the figure shows the weights assigned to features by W, and the bottom part shows the weights assigned by W_D. In every case, W_D gave higher scores to phonetically similar characters and usually assigned negative weights to pairs that weren't phonetically similar. There's also an interesting thing happening with English-Hebrew transliterations. English vowels tend to pair with almost any Hebrew character when using AF, because vowels are often left out in Hebrew, so there's no specific context where English vowels appear.",
        "formal_text": "Convert casual text to formal text: The weight vector we get from discriminative training is called W_D. With this new weight vector, we can create a new feature extraction operator by using the objective function from Eq. 2, but"
    },
    {
        "casual_text": "Human suggests that we should combine all the important info from different turns to reach a solid conclusion, rather than just ignoring everything we learned before. Following that idea, POI-Net creates an attention-enhanced embedding E t = t • E for each turn (we only save s t in a more efficient way) and combines them using specific methods. We came up with four ways to do this based on how much each turn contributes, but we ended up going with the Forgetting Strategy.",
        "formal_text": "Convert casual text to formal text: Human suggests that we should combine all the important info from different turns to reach a solid conclusion, rather than just ignoring everything we learned before. Following that idea, POI-Net creates an"
    },
    {
        "casual_text": "We teamed up with two speakers who had varying levels of confidence with writing, but both could handle the task just fine. This shows that the task is pretty straightforward and easy to get into, which is great for language work. It helps build a vocabulary with words that are actually used in speech. By making it less intimidating to join in, more people can contribute to transcription, and they can do it at their own pace.",
        "formal_text": "Convert casual text to formal text: We teamed up with two speakers who had varying levels of confidence with writing, but both could handle the task just fine. This shows that the task is pretty straightforward and easy to get into, which"
    },
    {
        "casual_text": "We've got METEOR results in Appendix E. For ENFR, we see similar stuff in Appendix F. The LASER models kinda fall apart more often than EQUIVA-LENTS and DIV-FACTORIZED.",
        "formal_text": "Convert casual text to formal text: We've got METEOR results in Appendix E. For ENFR, we see similar stuff in Appendix F. The LASER models kinda fall apart more often than E"
    },
    {
        "casual_text": "We’ve got two main things to share: First, we’ve created a fresh dataset for answering questions in a sequence. It’s packed with tricky questions covering all sorts of topics, and we’re making it available for everyone to use. We hope it’ll inspire more research in this area. Second, we used our dataset to test some basic approaches in open-domain question answering. The goal was to show that connecting questions in a sequence gives better results.",
        "formal_text": "Convert casual text to formal text: We’ve got two main things to share: First, we’ve created a fresh dataset for answering questions in a sequence. It’s packed with tricky questions covering all sorts of topics,"
    },
    {
        "casual_text": "But hey, using the NN stacking method with just the best PD output gives way lower accuracy (94.74). It's the finetuning trick that makes stacking work almost as well as multi-view training for the NN models.",
        "formal_text": "Convert casual text to formal text: But hey, using the NN stacking method with just the best PD output gives way lower accuracy (94.74). It's the finetuning trick that makes stacking work"
    },
    {
        "casual_text": "• BiLSTM-tok: This is a popular bidirectional LSTM model used for labeling sequences, like part-of-speech tagging (Plank et al., 2016) and named entity recognition (Panchendrarajan and Amaresan, 2018). We tweaked the settings based on the development set to get the best performance on each of the evaluation datasets.",
        "formal_text": "Convert casual text to formal text: • BiLSTM-tok: This is a popular bidirectional LSTM model used for labeling sequences, like part-speech tagging (Plank et"
    },
    {
        "casual_text": "Web forums like Stack-Overflow, Quora, and Yahoo! Answers usually group their stuff into topic-based sections. In these sections, people ask questions, and other users jump in with comments trying to answer. Sometimes, the list of comments gets really long. A lot of these forums don't have moderators, so the content can get messy and repetitive.",
        "formal_text": "Convert casual text to formal text: Web forums like Stack-Overflow, Quora, and Yahoo! Answers usually group their stuff into topic-based sections. In these sections, people ask questions, and other users jump in"
    },
    {
        "casual_text": "Just so you know, we don't give CAPT-MULTI the actual number of sentences.",
        "formal_text": "Convert casual text to formal text: Just so you know, we don't give CAPT-MULTI the actual number of sentences. Just so you know, we don't give CAPT-MULTI the actual number"
    },
    {
        "casual_text": "After almost two years of using automated translation, Océ has managed to reuse translations effectively and has made some headway in improving the quality of both the original and translated documents. They've also started working on a terminology database that can be shared.",
        "formal_text": "Convert casual text to formal text: After almost two years of using automated translation, Océ has managed to reuse translations effectively and has made some headway in improving the quality of both the original and translated documents. They've also"
    },
    {
        "casual_text": "For each sentence, we looked at how the rankings from human scores compared to those from the TERP score and noted down the differences, grouping them by type of error (check out Table 7 for details). Out of the 50 sentences with the most ranking errors, 17 were mainly due to punctuation issues, like commas being in different places. Human judges usually didn’t mind these much—outputs with just punctuation problems still got high fluency scores, and many sentences with added or removed commas were rated as fully fluent. But TERP doesn’t like those changes and penalizes them. Another big source of errors for TERP is agreement issues. Humans tend to really dislike sentences with number or tense agreement problems and give them low scores, but TERP just applies a single penalty for each of those errors. If we tweak the weights in TERP to stop it from being too harsh on punctuation changes and not harsh enough on agreement errors, we think it’ll match up even better with human fluency judgments.",
        "formal_text": "Convert casual text to formal text: For each sentence, we looked at how the rankings from human scores compared to those from the TERP score and noted down the differences, grouping them by type of error (check out Table 7"
    },
    {
        "casual_text": "According to Chambers and Jurafsky (2008), a narrative chain is basically a sequence of events that are linked because they involve the same character or \"actor.\" Each event in this chain is made up of a verb and the people or things involved in it, like who's doing the action or who's receiving it. This is usually shown using something called \"typed dependencies.\" (De Marneffe et al., 2006) So, if we break it down, an event can be written as e = (v, d), where e is the event, v is the verb, and d shows the connection between the verb and the main character, like whether the character is doing the action or receiving it. For example, take this little story:",
        "formal_text": "Convert casual text to formal text: According to Chambers and Jurafsky (2008), a narrative chain is basically a sequence of events that are linked because they involve the same character or \"actor.\" Each event in this chain"
    },
    {
        "casual_text": "The way we represent x i is based on how BERT does it (as explained by Devlin et al. in 2018). Basically, it's the sum of the token, segment, and position embeddings.",
        "formal_text": "Convert casual text to formal text: The way we represent x i is based on how BERT does it (as explained by Devlin et al in 2018). Basically, it's the sum"
    },
    {
        "casual_text": "There's a lot of valuable info in stroke patterns. Since radicals are written one after the other, they create continuous sections in the stroke pattern. This means that if two characters share radicals and their positions are close enough, the edit distance will line them up. Usually, parts of a character are drawn from left to right and top to bottom, so the order of these parts in the stroke pattern shows their position within the whole character. This approach smoothly connects stroke similarity to radical similarity and can even spot similarities between pairs like  (meaning \"sun\") and  (meaning \"eye\").",
        "formal_text": "Convert casual text to formal text: There's a lot of valuable info in stroke patterns. Since radicals are written one after the other, they create continuous sections in the stroke pattern. This means that if two characters share radical"
    },
    {
        "casual_text": "You can find the code and data here: https://github.com/gsarti/interpreting-complexity",
        "formal_text": "Convert casual text to formal text: You can find the code and data here: https://github.com/gsarti/interpreting-complexity"
    },
    {
        "casual_text": "Okay, so the initial plan was to use the BLEU metric (Papineni et al., 2001) for evaluation since it's a pretty common way to evaluate machine translation. But, turns out, a lot of people think it’s not the best for languages that have a lot of inflections, like Slovenian and Serbian. Plus, the BLEU metric tends to give lower scores to rule-based machine translation (RBMT) systems, as mentioned by Callison-Burch et al. (2006) and Labaka et al. (2007). So, we decided not to use it. Instead, we looked into METEOR, which its creators say fixes many of the issues with BLEU and is more in line with how humans judge translations. The only problem? METEOR didn’t support our language pair, so we had to write some extra software to make it work. For the actual evaluation, we used a bilingual parallel corpus (Dimitrova et al., 1998) to automatically check the translations.",
        "formal_text": "Convert casual text to formal text: Okay, so the initial plan was to use the BLEU metric (Papineni et al., 2001) for evaluation since it's a pretty common way to evaluate"
    },
    {
        "casual_text": "In this project, we came up with GAML-BERT, a new way to make early exiting in PLMs better. Here's what we did: First, we ran some tests and found out that if two exits learn from each other, they both get better. Based on this, we thought, why not have all the exits learn from each other? This is called mutual learning (ML), and it helps improve how BERT handles early exiting. Second, we introduced GA, a new training method that makes sure the knowledge distillation and cross-entropy losses work together better. Our tests on the GLUE datasets showed that our approach improves early exiting, especially when there's a need for faster processing. Plus, our method is pretty flexible and can work with different early exiting strategies.",
        "formal_text": "Convert casual text to formal text: In this project, we came up with GAML-BERT, a new way to make early exiting in PLMs better. Here's what we did: First, we ran some"
    },
    {
        "casual_text": "The way these types are organized is pretty similar—they all have value ranges that fit into one kind of scale. The main differences between them are more about meaning. This classification builds on Reichman's idea of \"context spaces\" from 1985 (page 56). We’re borrowing her terms (even if not exactly her overall approach) to talk about the differences between epistemic, evaluative, and deictic issue-type context spaces. Reichman uses \"context space\" to refer to a part of a discourse. The issue context space is kind of like our attitude component, and the non-issue context space gives a basic way to categorize different types of discourse segments. Reichman lists things like comment, narrative support, and nonnarrative support as examples of non-issue types.",
        "formal_text": "Convert casual text to formal text: The way these types are organized is pretty similar—they all have value ranges that fit into one kind of scale. The main differences between them are more about meaning. This classification builds on Reichman'"
    },
    {
        "casual_text": "A smaller part of a tree can be seen as kind of its own separate structure. If a parser is really good and reliable, it should still give the same result for a specific part of the tree, even if there are some mistakes in another part that doesn't affect the one we're looking at. So, for certain situations where we mess with the words a bit, we loosen some rules and let the words in the tree be swapped with any word from our word list, as long as we don't change too many words. By doing this, we can test if a mistake in one part of the tree can cause problems in other parts that are far away, leading to a bunch of errors.",
        "formal_text": "Convert casual text to formal text: A smaller part of a tree can be seen as kind of its own separate structure. If a parser is really good and reliable, it should still give the same result for a specific"
    },
    {
        "casual_text": "So, CB focal is short for class-balanced focal loss.  i is the predicted label, and y i is the actual label of the tweet we're looking at. And those other symbols? They're just some adjustable settings, kind of like knobs you can tweak.",
        "formal_text": "Convert casual text to formal text: So, CB focal is short for class-balanced focal loss.  i is the predicted label, and y i is the actual label of the tweet we're looking"
    },
    {
        "casual_text": "Okay, so an update rule connects preconditions to their results. The dialogue manager keeps checking which update rules can be applied based on the current situation. Preconditions are based on what's happening right now, and the effects are what will change because of it. Right now, our system has 26 update rules that handle stuff like getting in touch with the user, starting clarification chats when the recognition confidence is low, answering questions, responding to requests, and confirming or denying statements.",
        "formal_text": "Convert casual text to formal text: Okay, so an update rule connects preconditions to their results. The dialogue manager keeps checking which update rules can be applied based on the current situation. Preconditions are based on what"
    },
    {
        "casual_text": "Alright, let’s break this down in simpler terms. In this part, we’re talking about how we analyze the meaning of stuff in PBF diagrams by combining two things: the way things are laid out and the actual words used. We follow some specific steps to do this analysis, and the results are shown in Figure 9. Here’s how it works: if a word is connected to another element with a symbol, like an arrow, we treat it as the name of a plant part. For instance, in Figure 6, words like \"meshibe\" (which are Word01 and Word04) and \"oshibe\" (Word02) are connected to their sketches with arrows. Because of this, we understand them as the names of plant parts based on the rule we’re using.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in simpler terms. In this part, we’re talking about how we analyze the meaning of stuff in PBF diagrams by combining two things: the"
    },
    {
        "casual_text": "Alright, let's break down how we can capture the impact of this fancy control agreement principle (CAP). PSG uses Montagovian semantic type assignments to explain CAP, but since we haven't talked about semantics, we'll just assume the necessary type info is already in our feature structures. With that in mind, our CAP setup has three main parts: first, we need to define what a controller and a controllee (or target, as GPSQ calls it) are; second, we define what a control feature is; and third, we explain the instantiation principle. Let's go through each step. First, let's define controller and controllee: A category C is controlled by another category C in a constituent Co if, at a semantic level, one of these two things happens: either C is a functor that combines with C to make Co, or there's a control mediator C\" that joins C and C in that specific order to create Co.",
        "formal_text": "Convert casual text to formal text: Alright, let's break down how we can capture the impact of this fancy control agreement principle (CAP). PSG uses Montagovian semantic type assignments to explain CAP, but since we have"
    },
    {
        "casual_text": "• and then figure out how to say it in the target language based on that interlingua stuff.",
        "formal_text": "Convert casual text to formal text: • and then figure how to say it in the target language based that interlingua stuff."
    },
    {
        "casual_text": "We took a closer look at how Transformers handle self-attention and whether we can pin down the attention weights based on the output of each attention head. Through some theory and experiments, we spotted some issues with the work by Brunner et al. (2019). They missed out on considering an important factor in the first phase of self-attention in the encoder—the size of the key vector. We figured out how to use d_k to make the attention weights easier to identify. To make things more practical, we came up with some encoder variations that are better at being identifiable, both in theory and in practice, for a wide range of input lengths. These variations don’t lose any performance when tested on different text classification tasks. In the future, it would be interesting to explore how this identifiability stuff affects how explainable and interpretable Transformers are.",
        "formal_text": "Convert casual text to formal text: We took a closer look at how Transformers handle self-attention and whether we can pin down the attention weights based on the output of each attention head. Through some theory and experiments, we"
    },
    {
        "casual_text": "We looked at tasks that involve different kinds of outputs, like sequences or non-projective trees. Most of the systems we checked out are based on models that focus on likelihood-based structured prediction. Some of them use simple, sparse features that are either on or off, while others go for more complex, continuous-valued features.",
        "formal_text": "Convert casual text to formal text: We looked at tasks that involve different kinds of outputs, like sequences or non-projective trees. Most of the systems we checked out are based on models that focus on likelihood-based structured"
    },
    {
        "casual_text": "To figure out how much each part (BERT & CRF) of our model contributes, we tested its performance on the Gigawords dataset by taking out one part at a time and seeing how it affects things.",
        "formal_text": "Convert casual text to formal text: To figure out how much each part (BERT & CRF) of our model contributes, we tested its performance on the Gigawords dataset by taking out one part at a"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: A is defined like this: - If i equals j, then A is -3. - If i is not equal to j, and either: - The pair (wi last wj1) isn't in the corpus, or - The pair (wi last wj1) is in the set C, then A is -2. - Otherwise, A is 2 times the count of (wi last wj1).",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: A is defined like this: - If i equals j, then A is -3. - If"
    },
    {
        "casual_text": "Everyone knows that Japanese words are grouped based on where they come from, and It and Mester (1993) took this idea and explained it using phonology. They say the Japanese vocabulary can be split into four groups: Yamato (words from Japan itself), Sino-Japanese (words borrowed from China a long time ago), Mimetic (words that sound like what they mean, like onomatopoeia), and Foreign (modern words borrowed from Western languages). The idea is that each group follows different rules when it comes to how words sound. Here’s an example to make it clearer. There are three common rules in Japanese that show how each group is different:",
        "formal_text": "Convert casual text to formal text: Everyone knows that Japanese words are grouped based on where they come from, and It and Mester (1993) took this idea and explained it using phonology. They say the Japanese vocabulary"
    },
    {
        "casual_text": "Named Entity Recognition (NER) is all about figuring out who's who and what's what in text, and having a good grasp of general knowledge helps a lot in this field. Kazama and Torisawa (2008) took info from a gazetteer, like Wikipedia, and turned it into features for a CRF-based Japanese NE tagger. They basically treated the NER task as labeling text with IOB tags, which stands for Inside, Outside, Beginning. Noun phrases pulled from a gazetteer can also be easily shown as IOB tags. But this approach doesn't completely fix the issue of not having enough knowledge. They also used the results from a morphological analyzer, which doesn't really use encyclopedic knowledge. The performance of NER can get messed up by mistakes in breaking down words, especially with unknown words.",
        "formal_text": "Convert casual text to formal text: Named Entity Recognition (NER) is all about figuring out who's who and what's what in text, and having a good grasp of general knowledge helps a lot in this"
    },
    {
        "casual_text": "The difference between the scores of the actual correct answer, which we'll call x_i, and a possible but not quite right answer, x_i,j, is what we're looking at here. The hinge loss is used to make sure that the correct answer's score is at least  points higher than the score of the not-so-great option. We're calling the language model we train using this method the Large Margin Language Model (LMLM).",
        "formal_text": "Convert casual text to formal text: The difference between the scores of the actual correct answer, which we'll call x_i, and a possible but not quite right answer, x_i,j, is what we"
    },
    {
        "casual_text": "In SAMT, they use a bunch of CCG-like binary operators to boost the number of nonterminal labels. This is important because SAMT labels come from phrase structure grammar, which only has a limited set of labels that can't cover all the different syntactic structures of the phrases they extract. Almaghout et al. (2010) tried using single-category CCG labels as nonterminal labels. While CCG's flexible structures give better coverage than phrase structure grammar labels, using just single-category CCG labels still leaves about a third of the phrases unlabeled. To fix this, we decided to expand the definition of nonterminal labels to include more than one CCG category. So, if there's no single CCG category at the top of the tree that can label a phrase, we look for the highest-scoring sequence of categories with the fewest CCG categories from the CCG trees that cover the phrase and use that as the label. Figure 2 shows some phrases from the sentence in Figure 1, along with their extended CCG labels. For example, the phrase \"like cream\" has an extended CCG label made up of two categories: S[b]NP+conj. The CCG categories for the words \"like\" and \"cream\" are combined into S[b]NP. But this category can't be combined with the conj category.",
        "formal_text": "Convert casual text to formal text: In SAMT, they use a bunch of CCG-like binary operators to boost the number of nonterminal labels. This is important because SAMT labels come from phrase structure grammar, which only"
    },
    {
        "casual_text": "Alright, so let's break this down in simpler terms. In recurrent neural networks, the last hidden state, which we call h_l, is usually what the output layer uses to figure out the class label. This h_l basically carries all the important stuff the model T has learned. Now, to make sure our model S can learn from this and do a good job, we want to make the representation of v_i (which is kind of like a summary of the class) richer. To do this, we use a third loss function, which is the third part of Equation 2. This loss function helps us minimize the difference (specifically, the cosine distance) between v_i and h_l. The goal here is to make S perform similarly to T, which is like a black box we can't see inside. The only catch is that h_l and v_i need to be the same size (both in Rd) so we can actually calculate this cosine distance.",
        "formal_text": "Convert casual text to formal text: Alright, so let's break this down in simpler terms. In recurrent neural networks, the last hidden state, which we call h_l, is usually what the output layer uses"
    },
    {
        "casual_text": "The stats show that some subreddits are easier to tell apart than others. For example, askscience is pretty easy to spot—the accuracy for classifying its threads is a whopping 97%. On the other hand, some subreddits are trickier to distinguish because they feel kind of similar. For instance, politics and worldnews, or askmen and askwomen, tend to confuse people.",
        "formal_text": "Convert casual text to formal text: The stats show that some subreddits are easier to tell apart than others. For example, askscience is pretty easy to spot—the accuracy for classifying its threads is a who"
    },
    {
        "casual_text": "There aren't many annotated datasets out there. The biggest ones we’ve had so far come from the SentiRuEval 2015 and 2016 competitions (Loukachevitch and Rubtsova, 2016). The SentiRuEval 2016 dataset includes 10,890 tweets from the telecom industry and 12,705 from banking. The Linis project (Koltsova et al., 2016) says they crowdsourced annotations for 19,831 blog snippets, but only 3,327 of those are actually available on their website right now.",
        "formal_text": "Convert casual text to formal text: There aren't many annotated datasets out there. The biggest ones we’ve had so far come from the SentiRuEval 2015 and 2016 competitions (Louka"
    },
    {
        "casual_text": "Sure! Here's a more casual version: You've got a list of vectors like (w1, w2, ..., wn), and each wi is itself a vector with components (w1i, ..., wmi), written as a column.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: You've got a list of vectors like (w1, w2, ..., wn), and each wi is itself"
    },
    {
        "casual_text": "Not much is known about syntactic complexity, even though it’s been talked about a lot when people discuss style, readability, and more recently, how computers handle analyzing sentences. It’s super tricky to explain, and that’s probably why, as far as I know, no one really wants to deal with it. Even when folks like Flesch came up with their readability formulas, they had to face the problem but couldn’t figure it out, so they just swapped syntactic complexity for sentence length in their calculations. Measuring length is way easier, and it still kinda lines up with syntactic complexity, even if it’s not perfect.",
        "formal_text": "Convert casual text to formal text: Not much is known about syntactic complexity, even though it’s been talked about a lot when people discuss style, readability, and more recently, how computers handle analyzing sentences."
    },
    {
        "casual_text": "Okay, so this setup looks at all the arguments from different parts of a longer text. Unless we say otherwise, we're using the last layer of BERT and fine-tuning the whole thing. We're comparing it to a really good BERT-based model that does BIO-style sequence labeling (Shi and Lin, 2019, Table 3). This table shows how different setups affect a model for detecting arguments (on the development set, without type-constrained decoding). \"BERT-Full\" is our fully fine-tuned BERT encoder. \"No-Indicator\" means we take out the input indicators. \"No-FineTuning\" keeps BERT's original pre-trained parameters frozen. And \"LSTM\" swaps BERT for a bi-directional LSTM encoder.",
        "formal_text": "Convert casual text to formal text: Okay, so this setup looks at all the arguments from different parts of a longer text. Unless we say otherwise, we're using the last layer of BERT and fine-tuning"
    },
    {
        "casual_text": "Plus, using a non-autoregressive model lets us create an encoder-only setup, which works better for summarization. This is because the input and output have a really close connection, and that's something encoder-decoder models, especially the autoregressive ones, can't fully handle.",
        "formal_text": "Convert casual text to formal text: Plus, using a non-autoregressive model lets us create an encoder-only setup, which works better for summarization. This is because the input and output have a really"
    },
    {
        "casual_text": "In the MRC field, some earlier studies focus on general-purpose language modeling, but they require a lot of computational power to encode aspects (Zhang et al., 2020c). Others just combine texts from different MRC tasks to increase the size of the training dataset.",
        "formal_text": "Convert casual text to formal text: In the MRC field, some earlier studies focus on general-purpose language modeling, but they require a lot of computational power to encode aspects (Zhang et al., 2020c)."
    },
    {
        "casual_text": "Alright, after we narrowed down the possible range for , we just need to solve the simple equation 10 to find the best value for .",
        "formal_text": "Convert casual text to formal text: Alright, after we narrowed down the possible range for , we just need to solve the simple equation 10 to find the best value for ."
    },
    {
        "casual_text": "The terminal node will spit out a value, which gets sent up during the backpropagation phase.",
        "formal_text": "Convert casual text to formal text: The terminal node will spit out a value, which gets sent up during the backpropagation phase. Convert casual text to formal text: The terminal node will spit"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way: We're talking about a function or something like that, right? It's got inputs x1, x2, ..., xn. Then there's some structure going on with S> and /S> tags, which probably mean something specific, like maybe starting and ending a section or something. Inside those tags, there are R1, R2, ..., and T1, T2, ..., which could be different parts or steps in whatever process we're dealing with. So, in short: - We have inputs x1 to xn. - There's a setup with S> and /S> tags. - Inside those tags, we have R1, R2, etc., and T1, T2, etc. - And finally, there's an [EOS] at the end, which probably means \"End of Sentence\" or something like that.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way: We're talking about a function or something like that, right? It's got inputs x1, x"
    },
    {
        "casual_text": "We used a simple linear model (no hidden layers, as shown in Figure 1) to rank all the objects in one of the Verb Physics development sets. According to Table 6, this linear model performs almost as well as shallow fully connected neural networks, with accuracy only about 1% lower. The score for a word is calculated by multiplying its embedding with the weight learned to map the input to label 1. This score is higher if word 1 is considered greater than word 2. We use this score to rank the objects. Appendix C gives an example of how this works, showing a learned ranking of all the words in the dev set using GloVe. Using this ranking to classify comparisons between pairs of words gives accuracy results that are almost as good as the original models on a subset of the dev set with only 0/1 labels. This suggests that the models assign a kind of absolute value to each word to rank all the objects, and then use this overall ranking to compare any two objects. Using the weight for label 0 gives similar results. This ranking works well for > or  comparisons but isn't as useful for  comparisons. This could be why GloVe, ELMo, and BERT struggle a bit more with comparisons labeled 2, as shown in Table 7.",
        "formal_text": "Convert casual text to formal text: We used a simple linear model (no hidden layers, as shown in Figure 1) to rank all the objects in one of the Verb Physics development sets. According to Table 6, this linear model performs"
    },
    {
        "casual_text": "We can take a bunch of input texts (like the ones learners read to pick up language) and compare them to output texts (the stuff learners write themselves). By looking at how often certain grammar structures show up in both, we can create a language profile that covers both reading and writing. This helps us focus on what's important for language learning from both angles.",
        "formal_text": "Convert casual text to formal text: We can take a bunch of input texts (like the ones learners read to pick up language) and compare them to output texts (the stuff learners write themselves). By looking at how often certain grammar structures"
    },
    {
        "casual_text": "Intention: You want to watch cat videos. Your mom has the remote control for the TV in the living room. What do you say to her?",
        "formal_text": "Convert casual text to formal text: Intention: You want to watch cat videos. Your mom has the remote control for the TV in the living room. What do you say to her? Intention: You want to watch cat videos."
    },
    {
        "casual_text": "Online social networks have gotten a lot of attention from researchers, especially when it comes to entity linking in these spaces. (Huang et al., 2014; Shen et al., 2013) came up with methods specifically made for linking named entities in tweets. The main challenge they tackle is that tweets are usually short and casual, but they also try to make use of any extra info that tweets might offer. For instance, (Shen et al., 2013) figured that each user's tweets reflect some kind of interest pattern and created a graph-based algorithm to rank entities based on this interest distribution. (Huang et al., 2014) also used a method with meta-paths on HIN in their entity linking approach, but they just used it to see if two mentions are connected. One thing all these studies have in common is that, even though they focus on tweets, they still link the entities to knowledge bases as the final goal.",
        "formal_text": "Convert casual text to formal text: Online social networks have gotten a lot of attention from researchers, especially when it comes to entity linking in these spaces. (Huang et al., 2014; Shen et"
    },
    {
        "casual_text": "Word-based methods have an edge because they can pick out important words in a sentence more easily than bunsetsu-based methods. This is because word-based methods don’t worry about bunsetsu and aren’t restricted by unit limitations.",
        "formal_text": "Convert casual text to formal text: Word-based methods have an edge because they can pick out important words in a sentence more easily than bunsetsu-based methods. This is because word-based methods don’t worry about"
    },
    {
        "casual_text": "During the processing phase, we figure out and calculate different factors that help determine how important a sentence is. These factors include stuff like sentence length, keyword selection (using the TF-ISF method), and other number-based features. There are also some language-related features that make a sentence more likely to be included in the summary, such as whether it matches the headline, has certain nouns or proper nouns, includes cue phrases, or uses keywords from the headline. Each sentence gets a final score based on an equation that combines all these features, with weights assigned to each one. These weights are calculated using special learning methods. Finally, we pick the top-ranked sentences in the right order and compress them to create the summary.",
        "formal_text": "Convert casual text to formal text: During the processing phase, we figure out and calculate different factors that help determine how important a sentence is. These factors include stuff like sentence length, keyword selection (using the TF-ISF"
    },
    {
        "casual_text": "The rest of the paper goes like this: In Section 2, we talk about our topic models, W-LDA and S-LDA, which are based on LDA. Section 3 covers how we built the two-layer graph and did some semi-supervised learning. Section 4 shows the results of our experiments. After that, Section 5 looks at related work on query-focused multi-document summarization and topic modeling. Finally, we wrap things up in Section 6.",
        "formal_text": "Convert casual text to formal text: The rest of the paper goes like this: In Section 2, we talk about our topic models, W-LDA and S-LDA, which are based on LDA. Section 3 covers how"
    },
    {
        "casual_text": "So, Z equals the sum from j=1 to n of m_j * u_j * e(alpha_j). We don't really focus much on the dual form of the problem. But, the complementary slackness conditions, which are crucial for the solution to be optimal, come into play in the next section. There, we'll rework the relaxed maximum entropy problem.",
        "formal_text": "Convert casual text to formal text: So, Z equals the sum from j=1 to n of m_j * u_j * e(alpha_j). We don't really focus"
    },
    {
        "casual_text": "Here's a simpler way to explain it: We've got a table with n rows and m columns. Let's call each cell c_ij, where \"i\" is the row number and \"j\" is the column number.",
        "formal_text": "Convert casual text to formal text: Here's a simpler way to explain it: We've got a table with n rows and m columns. Let's call each cell c_ij, where \""
    },
    {
        "casual_text": "Semantic preservation. MORPHEUS sticks to inflections that match the same universal part of speech. Let’s use the word \"duck\" as an example. It can be either a verb or a noun, depending on the situation. In the sentence \"There's a jumping duck\", \"duck\" is a noun. So, MORPHEUS will only pick other noun-related inflections.",
        "formal_text": "Convert casual text to formal text: Semantic preservation. MORPHEUS sticks to inflections that match the same universal part of speech. Let’s use the word \"duck\" as an example. It can be"
    },
    {
        "casual_text": "The MT results in the Department of Defense (DOD) aren't as clear-cut as they are in the business world. We usually don't measure them by looking at how much money we save or how it affects our overall workforce numbers.",
        "formal_text": "Convert casual text to formal text: The MT results in the Department of Defense (DOD) aren't as clear-cut they they in the business world. We usually don't measure them by looking at how much money"
    },
    {
        "casual_text": "Glow, introduced by Kingma and Dhariwal in 2018, is a type of normalizing flow that transforms data into a simple, known prior, like a spherical multivariate Gaussian distribution. It’s trained using the exact log-likelihood of the data and, as a generative model, it’s really good at capturing all the dependencies in high-dimensional data, which often comes in the form of a complex, multimodal probability distribution. To keep things fair when comparing it to other methods, we only use the encoder’s hidden states as the condition for each flow step and stick with a spherical Gaussian prior, just like in the original Glow. Through some experiments, we noticed that a 2D-CNN-based network works better than a 1D-CNN, especially when it comes to reconstructing high-frequency details. As you can see in Figure 4d, our Glow-based decoder models the distribution of mel-spectrograms based on the encoder’s output hidden states x. This decoder is made up of k flow steps, labeled f1 to fk. Each step includes an affine coupling layer, an invertible 1x1 convolution, and an activation normalization layer. Essentially, the Glow-based decoder takes random variables from a spherical Gaussian distribution and turns them into mel-spectrograms.",
        "formal_text": "Convert casual text to formal text: Glow, introduced by Kingma and Dhariwal in 2018, is a type of normalizing flow that transforms data into a simple, known prior, like a spher"
    },
    {
        "casual_text": "(Self-disclosure) Yeah, I get it. I’d be super annoyed too if that happened to me.",
        "formal_text": "Convert casual text to formal text: (Self-disclosure) Yeah, I get it. I’d be super annoyed too if that happened to me. Convert casual text to formal text: (Self-disc"
    },
    {
        "casual_text": "[clark]: Class-based n-grams with morphology, as described by Clark in 2003. This system is pretty similar to the one before it, but it groups word types instead of tokens, which is what the other systems do. The main differences are that clark uses a slightly different way to search for approximations and adds a prior to the probabilistic model that makes it more likely to group words together if they look similar morphologically. The morphology part is handled by a single-order letter HMM.",
        "formal_text": "Convert casual text to formal text: [clark]: Class-based n-grams with morphology, as described by Clark in 2003. This system is pretty similar to the one before it, but it groups word types"
    },
    {
        "casual_text": "The effectiveness of data-driven machine translation (MT) mostly hinges on how much parallel data you have for training. While statistical MT is often seen as the cutting-edge approach, it has a downside: once it creates the translation and language models, it tosses out the training data. This can sometimes result in lower-quality translations because the context is restricted by the value of n. On the other hand, example-based MT (EBMT) keeps all the sentence pairs from the source and target languages stored in an example base. It then uses sentences similar to the input as templates for translation. This means EBMT systems can usually handle long-range dependencies and complex grammar better. Modern MT systems typically combine both statistical and example-based components (shoutout to Dandapat et al., 2012). Traditional EBMT systems, like the ones described by Somers (2003) and Nagao (1984), have three main steps to translate an input sentence Q using an example base of sentence pairs (Si, Ti), where Si is in the source language and Ti is in the target language: 1. **Matching**: Find sentences similar to Q and pick the translation template Ti that has the highest similarity between Q and Si. This template acts as the backbone for the translation. 2. **Alignment**: Figure out which parts of Q and Si match up to identify any gaps in the translation. For mismatches, look for alternative translations from the example base.",
        "formal_text": "Convert casual text to formal text: The effectiveness of data-driven machine translation (MT) mostly hinges on how much parallel data you have for training. While statistical MT is often seen as the cutting-edge approach, it has"
    },
    {
        "casual_text": "Designing new experiments in science relies a lot on understanding the field and looking at what’s already been done in previous studies. But there’s usually a ton of research out there, and it’s tough—sometimes even impossible—to keep up with all the experiments related to a specific question. Since running scientific experiments can be super time-consuming and expensive, having ways to quickly find useful info from published research would be a huge help (like what Auer et al. did in 2018, or Manica et al. in 2019, and Mrdjenovich et al. in 2020). Now, while people have been working on this kind of thing in the biomedical field (for example, Cohen et al. in 2017 and Demner-Fushman et al. in 2018), not much has been done in other areas—like materials science, except for some work by Mysore et al. in 2017 and 2019.",
        "formal_text": "Convert casual text to formal text: Designing new experiments in science relies a lot on understanding the field and looking at what’s already been done in previous studies. But there’s usually a ton of research out there"
    },
    {
        "casual_text": "CorrRNN and Copy-RNN really shine when it comes to the F1-measure, leaving other methods far behind. This shows how well RNNs with a copy mechanism work. When we look at how phrases are related, CorrRNN does way better than CopyRNN (we’re talking a significant difference here, like p  0.01 in a t-test). The main reason? CorrRNN fixes some of the common problems like duplication and coverage issues, which means more accurate phrases make it into the top 10 results. Even the heuristic baseline, CopyRNN F, performs worse than Copy-RNN, suggesting that those heuristic rules might actually mess up the performance of generative methods. This also proves that modeling the connection between keyphrases in a more end-to-end way is a smarter approach.",
        "formal_text": "Convert casual text to formal text: CorrRNN and Copy-RNN really shine when it comes to the F1-measure, leaving other methods far behind. This shows how well RNNs with a copy mechanism work."
    },
    {
        "casual_text": "Using knowledge distillation to make things more scalable. Wu and team (2020) showed that a lot of the quality boost from cross-encoding can be passed on to a bi-encoder using knowledge distillation. This might even make pairwise classification unnecessary in the future.",
        "formal_text": "Convert casual text to formal text: Using knowledge distillation to make things more scalable. Wu and team (2020) showed that a lot of the quality boost from cross-encoding can be passed on to a bi-"
    },
    {
        "casual_text": "We figure out the topic loss, which we call L T, by using something called KL divergence. This helps us compare the predicted topic distribution with the one we get from the topic model.",
        "formal_text": "Convert casual text to formal text: We figure out the topic loss, which we call L T, by using something called KL divergence. This helps us compare the predicted topic distribution with the one you get from the topic model."
    },
    {
        "casual_text": "Alright, so this Papillon module is all about creating user-friendly interfaces for looking at and editing dictionary entries. We’re building on some cool research by Thevenin and Coutaz from 1999, and also using a tool called ART-Studio, which was developed by Calvary and team in 2001. These guys came up with ways to semi-automatically design interfaces for different purposes. Here’s a quick rundown of the framework and models we’re using.",
        "formal_text": "Convert casual text to formal text: Alright, so this Papillon module is all about creating user-friendly interfaces for looking at and editing dictionary entries. We’re building on some cool research by Thevenin and Cout"
    },
    {
        "casual_text": "Kind of like how educational testing researchers check if IRT models can accurately predict how people answer questions (American Educational Research Association, 2014), we’re checking how well DAD can predict if SQuAD models are getting questions right.",
        "formal_text": "Convert casual text to formal text: Kind of how educational testing researchers check if IRT models can accurately predict how people answer questions (American Educational Research Association, 2014), we’re checking how well DAD can predict if SQu"
    },
    {
        "casual_text": "These studies show that NN-based methods haven't been great at handling numerical reasoning tasks.",
        "formal_text": "Convert casual text to formal text: These studies show that NN-based methods don't great at handling numerical reasoning tasks. Convert casual text to formal text: These studies show that NN-based methods aren't great"
    },
    {
        "casual_text": "The other two baselines look at modeling the Shortest Dependency Paths (SDP) between the marked entities. Xu et al. (2015a) (depLCNN) added relation directionality into a CNN and got an F1 score of 84.0 by using a data augmentation trick called negative sampling. Without that trick, their F1 score drops to 81.9. Xu et al. (2015b) (SDP-LSTM) turned different features into embeddings and used a multichannel LSTM-based recurrent neural network to pick up info along the SDP. Their F1 score is 83.0 when they only use word embeddings as the word representation.",
        "formal_text": "Convert casual text to formal text: The other two baselines look at modeling the Shortest Dependency Paths (SDP) between the marked entities. Xu et al. (2015a) (depLCNN"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way: For the CLUES-Real and CLUES-Synthetic tests, we looked at 20% of the examples from each task in CLUES-Real. For those really tiny Wikipedia tasks (where we don't get explanations from people), we just use all the examples for testing without any training. Now, for CLUES-Synthetic, we have 96 tasks that we've seen before (used for training) and 48 new tasks that are fresh and haven't been seen. The tasks in CLUES-Synthetic that fit into certain categories are considered the \"seen\" tasks.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way: For the CLUES-Real and CLUES-Synthetic tests, we looked at 20% of the examples from"
    },
    {
        "casual_text": "Okay, so we have this vector e_x, which is made up of K smaller vectors, like this: e_x = [e_x(1); ...; e_x(K)]. Each of these e_x(i) is a vector in Rds, and if you put them all together, the total dimension is d = K * ds. Now, M_r is a set of K linear transformation matrices, so M_r = M_r(1), ..., M_r(K). Each M_r(i) is a ds x ds matrix.",
        "formal_text": "Convert casual text to formal text: Okay, so we have this vector e_x, which is made up of K smaller vectors, like this: e_x = [e_x(1); ...; e"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Each speech in the dataset talks about one specific motion. There are two types of speeches: 1. **Supporting speeches**: These are where someone speaks in favor of the motion. 2. **Opposing speeches**: These are where someone argues against the motion, usually after hearing a supporting speech. When someone records an opposing speech, they usually listen to a supporting speech first. Then, they create and record their own speech in response. This response can be: - **Explicit**: The speaker directly addresses and counters the arguments from the supporting speech, often with a dedicated rebuttal section. - **Implicit**: The speaker doesn’t have a specific rebuttal section but still touches on the issues raised in the supporting speech without directly addressing them. In the dataset, there are multiple opposing speeches for each supporting speech. Some might be explicit, some might be implicit, and some might be a mix. Figure 1 shows how this all fits together. In the Appendix, you’ll find examples of one explicit and one implicit opposing speech.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. Each speech in the dataset talks about one specific motion. There are two types of speeches: 1. **Supporting speeches**"
    },
    {
        "casual_text": "In this paper, we’re introducing a modal language called L 7 for discussing trees, and we’ve also got an extended version, L'r (L F), for talking about trees that are decorated with feature structures. From a logical perspective, this makes sense. After all, the trees and feature structures that linguists work with are just simple graphical objects. In other words, they’re pretty basic Kripke models, and modal languages are likely the simplest way to add some meaningful constraints to these structures. Plus, this approach feels natural from a linguistic standpoint too. A lot of what linguists need to say about trees (and feature structures) ends up using modal operators in a pretty straightforward way. In fact, the modal choices we made were based on real linguistic practice, not just logical convenience.",
        "formal_text": "Convert casual text to formal text: In this paper, we’re introducing a modal language called L 7 for discussing trees, and we’ve also got an extended version, L'r (L F), for talking about"
    },
    {
        "casual_text": "Tree kernels look at trees by focusing on smaller parts of them, called substructures or fragments. The kernel function checks if a part of a tree, which is shared by two trees, fits into the feature space we're trying to create. To do this, we need to describe the specific fragments we're interested in. We mainly look at two types: syntactic tree fragments (STF) and partial tree fragments (PTF).",
        "formal_text": "Convert casual text to formal text: Tree kernels look at trees by focusing on smaller parts of them, called substructures or fragments. The kernel function checks if a part of a tree, which is shared by"
    },
    {
        "casual_text": "Basically, if a summary doesn't match up with the original text when you ask a question and the answer changes depending on whether you're looking at the summary or the source, then it's considered inconsistent. So, we figure out how accurate the summary is by calculating something called the precision score.",
        "formal_text": "Convert casual text to formal text: Basically, if a summary doesn't match up with the original text when you ask a question and the answer changes depending on whether you're looking at the summary or the source,"
    },
    {
        "casual_text": "Customer feedback is super important for e-commerce companies to keep improving their services. A big part of this feedback includes the problems customers face with products, like quality issues or features they don’t like. Even though bad product experiences are pretty rare in well-established online stores, figuring out these issues from customer feedback helps sellers really understand what’s bothering their customers and lets them make things better. As more and more feedback comes in, it’s getting harder to keep up manually. That’s why an automated and smart system is needed to handle the growing amount of feedback in the e-commerce world. In this paper, we’re introducing a machine learning solution that uses fancy deep language modeling to find different levels of product issues. Our method not only pinpoints these problems accurately but also fits the needs of our specific situation, which we think is pretty similar to what other e-commerce businesses deal with too.",
        "formal_text": "Convert casual text to formal text: Customer feedback is super important for e-commerce companies to keep improving their services. A big part of this feedback includes the problems customers face with products, like quality issues or features they don’t like"
    },
    {
        "casual_text": "If you come across something interesting, you can check it out or download it for later. While translating, you might also use SDTVISTA to help with any tricky terms or jargon.",
        "formal_text": "Convert casual text to formal text: If you come across something interesting, can check it out or download it for later. While translating, you might also use SDTVISTA to help with any tricky terms or jargon."
    },
    {
        "casual_text": "Machine translation and image captioning don't usually produce super long texts. So, it's kind of unclear how much exposure bias really affects text generation tasks.",
        "formal_text": "Convert casual text to formal text: Machine translation and image captioning don't usually produce super long texts. So, it's kind of unclear how much exposure bias really affects text generation tasks."
    },
    {
        "casual_text": "Unlike other datasets that either pick concepts manually from UMLS/MeSH or grab them from PubMed and then pair them up, we go straight to EHR data to sample concept pairs. Specifically, we use IQVIA Medical Research Data (IMRD), which includes data from The Health Improvement Network (THIN), a Cegedim database. THIN has anonymized primary care EHRs covering about 5% of the UK population. In IMRD, a patient's consultation might involve concepts from categories like symptoms, diagnoses, presenting complaints, examinations, interventions, management, and administration. For our purposes, we're only focusing on the first three categories since they're the most relevant for EHR search to help with consultations. For each patient in IMRD, we pair all the distinct concepts from those three categories that show up in their EHR. This gives us a total of 1,345,193 unique pairs made from 34,794 unique concepts.",
        "formal_text": "Convert casual text to formal text: Unlike other datasets that either pick concepts manually from UMLS/MeSH or grab them from PubMed and then pair them up, we go straight to EHR data to sample concept pairs."
    },
    {
        "casual_text": "For future work, you could get some experts to go through a part of the dataset again and label it, so you can compare their results to the gold tags and see how well humans can do.",
        "formal_text": "Convert casual text to formal text: For future work, you could get some experts to go through a part of the dataset again and label it, so you could compare their results to the gold tags and see well humans do. Convert"
    },
    {
        "casual_text": "Sure! Here's a more casual version: \"We're looking at how the raw machine translation (MT) output compares to the reference translations. For each part, we calculate a TER score.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: \"We're looking at how the raw machine translation (MT output compares to the reference translations. For each part we calculate a"
    },
    {
        "casual_text": "Next, for each candidate Pj, we figure out the probability p(x, Pj) that the query x is referring to the same thing as Pj.",
        "formal_text": "Convert casual text to formal text: Next, for each candidate Pj, we figure out the probability p(x, Pj) that the query x is referring to the same thing as Pj. Convert casual text"
    },
    {
        "casual_text": "We looked at the data from both English and Chinese search engines, focusing on a part of the AOL dataset. Table 2 shows the stats for different categories from both AOL and Sogou logs. Just to clarify, a \"click set\" is all the clicks tied to a specific query, not just one unique query. When it comes to using general user info versus individual user info, there’s no big difference in the number of records or users in both logs. But if you look at unique queries and distinct click sets, individual user info clearly wins, especially in the Sogou log. So, even though using both common and individual user info works pretty well for half the users and half the search engine visits, the fact that there are way more unique queries and click sets shows how important individual user info is for personalizing search results. Methods that use individual user info are definitely more complex, even though some might say using common user info is easier to handle.",
        "formal_text": "Convert casual text to formal text: We looked at the data from both English and Chinese search engines, focusing on a part of the AOL dataset. Table 2 shows the stats for different categories from both AOL and Sogo"
    },
    {
        "casual_text": "Formality style transfer (FST) is a big deal in the world of text style transfer. For FST, Rao and Tetreault (2018) put out a really good parallel dataset called GYAFC, which has two sub-categories and around 50k parallel examples for each one. Most earlier studies (like Rao and Tetreault, 2018; Niu et al., 2018) usually use seq2seq encoder-decoder models on this dataset. But more recent work (Wang et al., 2019; Yao and Yu, 2021b; Chawla and Yang, 2020; Lai et al., 2021) has shown that fine-tuning big models like GPT-2 (Radford et al., 2019) and BART (Lewis et al., 2020) on this parallel data can really boost performance. To deal with the lack of parallel data, some researchers came up with three data augmentation tricks to create more pseudo-parallel data for training. Similar to other text style transfer studies that use back-translation (Zhang et al., 2018; Lample et al., 2018; Prabhumoye et al., 2018; Luo et al., 2019), some FST approaches (Shang et al., 2019; Chawla and Yang, 2020) use a cycle-reconstruction method. This involves training an extra backward transfer model alongside the forward one, and the two models help each other out by generating pseudo-paired data through back-translation in a kind of loop.",
        "formal_text": "Convert casual text to formal text: Formality style transfer (FST) is a big deal in the world of text style transfer. For FST, Rao and Tetreault (2018) put out a really good parallel dataset called"
    },
    {
        "casual_text": "We looked into this issue and think that the names of important people in the documents show up often enough to avoid the problem of sparse data. Even though the weighted correlation coefficient doesn't really boost the performance of bipolarization, the PCA-based method we came up with can still accurately figure out which important people belong to which bipolar parties.",
        "formal_text": "Convert casual text to formal text: We looked into this issue and think that the names of important people in the documents show up often enough to avoid the problem of sparse data. Even though the weighted correlation coefficient doesn't"
    },
    {
        "casual_text": "There's been a lot of research lately on cutting down phrase tables. A lot of it is about making sure we don't lose too much in terms of translation quality. This study shows that, at least with big phrase tables and usual methods, pruning can actually improve BLEU scores. The idea is that any improvement in BLEU from pruning could be turned into a better smoothing effect overall.",
        "formal_text": "Convert casual text to formal text: There's been a lot of research lately on cutting down phrase tables. A lot of it is about making sure we don't lose too much in terms of translation quality. This study shows that"
    },
    {
        "casual_text": "So, [p 1, p 2, . . . , p n ] are the logits for the 3-way BIO classification. Just a heads-up, we didn’t include the CRF layer in Fig 1(a) to keep things simple. The 3-way BIO classification loss is great for learning the general patterns of slot entities, but it misses out on using related slot description info. That’s why we’re adding a contrastive learning loss to make use of that extra slot description data for better BIO label detection. Contrastive learning (CL) has been super successful in unsupervised visual representation learning (Tian et al., 2019; He et al., 2019; Misra and van der Maaten, 2019; Chen et al., 2020). The basic idea of CL is to learn representations by making sure that different versions of the same data (like augmented views) are similar to each other in the latent space, using something called a contrastive loss. In this paper, for a given input utterance, we create a positive sample by swapping all the slot entity tokens with their corresponding slot labels. To get negative samples, we replace those slot entity tokens with different slot labels from the whole slot set. We set a fixed probability of p = 0.5 for each slot token replacement. Then, we randomly pick a mini-batch of N examples and define the contrastive loss based on pairs of these replaced examples from the mini-batch.",
        "formal_text": "Convert casual text to formal text: So, [p 1, p 2, . . . , p n ] are the logits for the 3-way BIO classification. Just a heads-up, we"
    },
    {
        "casual_text": "We made our dataset more focused than what other methods do, and during training, the model learns to think about how a piece of code works compared to its functional equivalents and other versions. This means DISCO can pick up a lot of useful stuff for real-world tasks even with less data, which saves a bunch of computing power. Specifically, we tested DISCO on clone detection and vulnerability detection because understanding how similar or different code parts are is super important for those tasks.",
        "formal_text": "Convert casual text to formal text: We made our dataset more focused than what other methods do, and during training, the model learns to think about how a piece of code works compared to its functional equivalents and other versions."
    },
    {
        "casual_text": "Following Tran et al. 2020, we set the hidden size for the input features: dI = 2048, dT = 1024, and dE = 300, along with 16 heads (H = 16). We’re using the Adam optimizer (Kingma and Ba, 2015) with 1 = 0.9, 2 = 0.98, and a learning rate of 10-6. The vocabulary size (K) is 50264, and dWiki is 300. We capped the text length in MSTR at 1,000 tokens because earlier tests showed that longer inputs didn’t improve performance much but made training way slower (check Table 6 in the supplementary for details). For articles longer than 512 tokens, we split them into two overlapping segments of 512 tokens each—one starting from the beginning and the other from the end—so they can overlap by [24  511] tokens. The prediction head in Figure 2 is just a linear layer followed by an output layer with 1024 dimensions.",
        "formal_text": "Convert casual text to formal text: Following Tran et al. 2020, we set the hidden size for the input features: dI = 2048, dT = 1024, and dE = 300,"
    },
    {
        "casual_text": "Building dialogue systems that act like really good teachers is tricky, even though we've made some progress with end-to-end systems that can mimic human language pretty well. These systems work great because they have tons of training data and can handle a lot of information, but there's no way to make sure they always tell the truth. So, a simpler goal might be to create a system that sticks to the facts from one or more trusted sources. This kind of system could help teach people about specific topics through conversations or add extra info to a task-focused dialogue system. For example, I go to animal shelters a lot. A \"no-kill\" shelter is one that doesn't put down healthy or treatable animals, even if the shelter is full. They only euthanize animals that are terminally ill.",
        "formal_text": "Convert casual text to formal text: Building dialogue systems that act like really good teachers is tricky, even though we've made some progress with end-to-end systems that can mimic human language pretty well. These systems work great because they"
    },
    {
        "casual_text": "For our starting point, we're using both Logistic Regression (LR) and Random Forest (RF) algorithms with a simple set of features, like just the words themselves (lexical features).",
        "formal_text": "Convert casual text to formal text: For our starting point, we're using both Logistic Regression (LR) and Random Forest (RF) algorithms with a simple set of features, like just the words themselves (lexical features"
    },
    {
        "casual_text": "Alright, so here's what's up with Table 5: It shows the BLEU scores when we do unsupervised training with different k-best sizes. We used 10116 English sentences in total, and for each English sentence, we got the k-best Chinese translations using the reverse system.",
        "formal_text": "Convert casual text to formal text: Alright, so here's what's up with Table 5: It shows the BLEU scores when we do unsupervised training with different k-best sizes. We used 10116"
    },
    {
        "casual_text": "Sure! Let me simplify that for you: We’re tweaking the APPLY and MODIFY actions we talked about in section 5.3. Instead of assigning types to the kids or adding them to the stack, we’re changing things up. This way, the system can add new connections to the current active node (let’s call it node i) without locking in any specific types. So, the APPLY(, j) action gets updated too.",
        "formal_text": "Convert casual text to formal text: Sure! Let me simplify that for you: We’re tweaking the APPLY and MODIFY actions we talked about in section 5.3. Instead of assigning types to the kids or adding"
    },
    {
        "casual_text": "(a) System code: This refers to the full source code, along with any settings or parameters. For example, it includes all the code that makes up a machine translation system.",
        "formal_text": "Convert casual text to formal text: (a) System code: This refers to the full source code, along with any settings or parameters. For example, it includes all code that makes up a machine translation system."
    },
    {
        "casual_text": "We're using pre-trained Chinese fasttext embeddings, version 3. The BiLSTM has a hidden size of 350 and three layers. The CNN kernel sizes are set to [3, 4, 5]. We're using the Adam optimizer with an initial learning rate of 1e-5. The model is trained with mini-batch sizes between 16 and 32, and we've got an early-stop strategy in place. We're also incorporating contextualized Chinese word representations, specifically ELMo 4 and the Chinese-base version of BERT.",
        "formal_text": "Convert casual text to formal text: We're using pre-trained Chinese fasttext embeddings, version 3. The BiLSTM has a hidden size of 350 and three layers. The CNN kernel sizes are set to ["
    },
    {
        "casual_text": "To compare how it stacks up against other parsers on the same dataset, check out the results. The Bohnet parser came out on top for that treebank, so we went with it for our own experiments.",
        "formal_text": "Convert casual text to formal text: To compare how it stacks against other parsers on the same dataset, check out the results. The Bohnet parser came out on top for that treebank, so we went with"
    },
    {
        "casual_text": "• C major chord + A minor chord - There are major comprehension problems and minor adequacy issues (basically, the translation is hard to understand but almost okay). Some almost correct info gets thrown out. - Table 2: Here are the raw numbers and percentages of words (adjusted by the total number of words, including ones without issues and the \"XXX\" marks for missing stuff) for all the different issue types. For cases with major adequacy issues, we also show percentages based on the total number of major issues to help figure out how much hidden adequacy stuff is going on. For the sake of being thorough, we included numbers for minor issues too, even though we didn’t dig deeper into them in this study.",
        "formal_text": "Convert casual text to formal text: • C major chord + A minor chord - There are major comprehension problems and minor adequacy issues (basically, the translation is hard to understand but almost okay). Some almost correct info"
    },
    {
        "casual_text": "We've conducted the first study to figure out how to automatically create Chinese character sets to make language learning easier. We tested different ways to represent characters and group them into families. The results showed that using averaged word vectors worked better than just using character vectors on their own. Plus, when we used K-means clustering, it helped create smaller groups of characters that had clear meanings. We hope these methods will make it easier to create more diverse and useful character sets.",
        "formal_text": "Convert casual text to formal text: We've conducted the first study to figure out how to automatically create Chinese character sets to make language learning easier. We tested different ways to represent characters and group them into families. The results showed that using"
    },
    {
        "casual_text": "We only consider certain pairs (xi, Yi) for the number of sentences in a sentence bead. For the rest, we stick to simple combinations like 1-1, 1-2, 1-3, 1-4, and 2-2 for the number of sentences.",
        "formal_text": "Convert casual text to formal text: We only consider certain pairs (xi, Yi) for the number of sentences in a sentence bead. For the rest, we stick to simple combinations like 1-1, 1-2,"
    },
    {
        "casual_text": "BERT vs. SpanBERT: When it comes to question answering, SpanBERT seems to perform a bit better than regular BERT when looking at both SQuAD datasets overall. But the gap widens if we focus on how well SQuAD 2.0-fine-tuned models handle answerable questions—there's a 7% difference there. This shows that BERT leans more toward predicting \"no answer\" when it runs into tricky sentence changes compared to SpanBERT. SQuAD 1.1 vs. SQuAD 2.0: The whole idea of \"knowing what you don't know\" (thanks, Rajpurkar et al., 2018) seems to have come at a price. SQuAD 2.0-fine-tuned models aren't just less robust to sentence errors than their SQuAD 1.1 counterparts (6.5% difference), but they're also way worse at dealing with answerable questions (12-18% difference). This suggests that SQuAD 2.0 models are more likely to predict \"no answer\" when they get sentences with tricky errors (check out Table 1 for more details).",
        "formal_text": "Convert casual text to formal text: BERT vs. SpanBERT: When it comes to question answering, SpanBERT seems to perform a bit better than regular BERT when looking at both SQuAD dataset"
    },
    {
        "casual_text": "Digging deeper into the results of the interactive evaluation, it's clear that the Joint model can pretty much copy how human supporters use strategies. We think our work will help push research toward more data-driven ways to create chatbots that can give good emotional support. Figure 2 gives a clear picture of how ESC, emotional conversation, and empathetic conversation are connected. Emotion has been shown to be super important for making chatbots more engaging (Li et al., 2017; Zhou and Wang, 2018; Huber et al., 2018; Huang et al., 2020). As a cool example of emotional conversation, the Emotional Chatting Machine (ECM) was created to generate emotional responses based on a specific emotion. This task involves accurately showing (whether it's specified or not) emotions in the responses. While emotional support (ES) might involve expressing emotions like happiness or sadness, it's actually about helping the user feel better by using the right support skills. This is different from just having a chatbot that can talk about emotions. Emotional chatting is kind of like a basic feature for chatbots, but ES is a more advanced and complicated skill that we want chatbots to have. Another related task is empathetic responding (Rashkin et al., 2019; Lin et al., 2019; Majumder et al., 2020; Zandie and Mahoor, 2020; Sharma et al., 2020a; Zhong et al., 2020), which is all about understanding how the user feels and then responding in a way that matches that.",
        "formal_text": "Convert casual text to formal text: Digging deeper into the results of the interactive evaluation, it's clear that the Joint model can pretty much copy how human supporters use strategies. We think our work will help push research toward more data-"
    },
    {
        "casual_text": "We went with two models: Code2Vec by Alon and team (2019b) and Code2Seq by the same group (2019a). For measuring performance, we used four common metrics that are often mentioned in earlier research: Precision, Recall, F1, and EM (which stands for exact match accuracy).",
        "formal_text": "Convert casual text to formal text: We went with two models: Code2Vec by Alon and team (2019b) and Code2Seq by the same group (2019a). For measuring performance, we used four"
    },
    {
        "casual_text": "Most of the words we used for this study are based on standard British English spelling, not American English. The differences between the two aren’t super important for what we’re doing here. But, we kinda leaned toward British English because it doesn’t have that postvocalic /ô/ sound, which is easier to adapt to Japanese since Japanese doesn’t allow consonant endings (except for the moraic nasal /N/). For example, words like /gItA(ô)/ become [gita:], and /fO(ô)k/ turns into [fo: kW]. That said, some words are obviously borrowed from American English, so in those cases, we used the American spelling. Take “schedule,” for instance. The Japanese pronunciation is [sW. ke. jWW. rW], which clearly comes from the American version /skEUl/, not the British /SEdju: l/. Also, the way we transcribed things is pretty broad—it focuses more on the overall phonological changes rather than tiny phonetic details. For example, the Japanese adaptation of “egg” /Eg/ is written as [eg. gW], even though voiced sounds often get devoiced in actual speech.",
        "formal_text": "Convert casual text to formal text: Most of the words we used for this study are based on standard British English spelling, not American English. The differences between the two aren’t super important for what we’re doing here."
    },
    {
        "casual_text": "Check out Figure 3 to see our proposed ESC Framework. It’s got three stages and some support strategies we recommend. The general flow for emotional support is: 1) Explore  2) Comfort  3) Take Action (shown by the black arrows), but you can tweak it based on the conversation (that’s what the gray dashed arrows are for). The \"Lexical Features\" column shows the top 5 unigrams or bigrams linked to each strategy in our dataset. Each feature is ranked using rounded z-scored log odds ratios (Monroe et al., 2008), which are in parentheses.",
        "formal_text": "Convert casual text to formal text: Check out Figure 3 to see our proposed ESC Framework. It’s got three stages and some support strategies we recommend. The general flow for emotional support is: 1) Explore  2) Comfort"
    },
    {
        "casual_text": "Alright, so type X  is basically assigned to something that's also type X. This thing can be shuffled around or repeated as much as you want. Here's how it works with the rules:",
        "formal_text": "Convert casual text to formal text: Alright, so type X  is basically assigned to something that's also type X. This thing can be shuffled around or repeated as much as you want. Here'"
    },
    {
        "casual_text": "The PosEdiOn program is typically a Python file, but you can also use a Windows executable if you prefer. If you want to send the zipped file via email, be aware that some email providers might block it because it contains an executable file. A better option could be sharing a link to the zipped file instead, which makes it easier to distribute the post-editing tasks.",
        "formal_text": "Convert casual text to formal text: The PosEdiOn program is typically a Python file, but you can also use a Windows executable if you prefer. If you want to send the zipped file via email"
    },
    {
        "casual_text": "To do WSI on a larger scale, we stick with the main idea from Amrami and Goldberg (2019), which is to group together sparse vectors made from the lemmas of the top-k word replacements suggested by a masked language model. This helps us save a ton of storage space and makes the representations easier to understand. But to make it work for bigger datasets, we go through all the sentences in the corpus and grab the top-k substitutes for every word in a sentence at the same time, using just one call to BERT for the whole sentence. This means we can't use the dynamic-patterns part of their method, which needs BERT to run separately for each word in each sentence. Still, as we show in Section 5.1, we get pretty good WSI results even without that.",
        "formal_text": "Convert casual text to formal text: To do WSI on a larger scale, we stick with the main idea from Amrami and Goldberg (2019), which is to group together sparse vectors made from the le"
    },
    {
        "casual_text": "The word embedding has 200 dimensions, and the hidden states in the BiLSTM are set to 256. Also, the mini-batch size is 32, and the default learning rate for Adam (as introduced by Kingma and Ba in 2014) is 1e-3.",
        "formal_text": "Convert casual text to formal text: The word embedding has 200 dimensions, and the hidden states in the BiLSTM are set to 256. Also, the mini-batch size is 32, and the default learning rate for"
    },
    {
        "casual_text": "This translation experiment setup makes manual evaluation pretty reliable because (1) the translation task isn’t too tricky, and (2) there’s a big gap in how well the different MT engines perform. Plus, the ORACLE metric can be used to check the quality of crowd-based evaluations for languages where expert graders aren’t available.",
        "formal_text": "Convert casual text to formal text: This translation experiment setup makes manual evaluation pretty reliable because (1) the translation task isn’t too tricky, and (2) there’s a big gap in how well the different MT engines perform. Plus"
    },
    {
        "casual_text": "We began by simplifying every sentence in the SemEval training and development sets to just \"subject and object,\" where subject and object are the actual entities mentioned in the sentence. Interestingly, a PA-LSTM model trained on this simplified data can score 65.1 F1 on the dev set when GloVe is used to initialize word vectors, and even 47.9 F1 without GloVe initialization. To test the model in a more practical scenario, we trained two versions: one with the original SemEval training set (no masking) and another with entity mentions masked, similar to what we did for TACRED. The unmasked model scored 83.6 F1 on the original SemEval dev set, but when we replaced entity mentions in the dev set with a special UNK> token to mimic unseen entities, the F1 score plummeted to 62.4. On the other hand, the masked model didn't get thrown off by unseen entities and maintained a steady 74.7 F1. This shows that models trained without masked entities don't handle new examples with unseen entities very well. Our results highlight the need for more thorough evaluations that consider dataset biases in future relation extraction research.",
        "formal_text": "Convert casual text to formal text: We began by simplifying every sentence in the SemEval training and development sets to just \"subject and object,\" where subject and object are the actual entities mentioned in the sentence. Interestingly,"
    },
    {
        "casual_text": "There are a bunch of methods out there for sentiment analysis, like the ones by Wu et al. (2019), Fu et al. (2019), Kong et al. (2019), Hoang et al. (2019), and so on. But most of them aren’t really built for detecting sentiment drift. Xia et al. (2016) did focus on detecting shifts in polarity, but it was more at the document level, not across multiple documents or public data. More recently, some work has been done on stream sentiment classification, like Iosifidis et al. (2017). Specifically, statistic processing control (SPC), as introduced by Ross et al. (2012) and Raza et al. (2015), created a system that tracks statistical signs of drift. Zhou et al. (2018) took this and applied it to sentiment drift detection. Also, Bifet and Gavalda (2007) came up with the ADWIN method, which uses Hoeffding's inequality to flag drifts. Nguyen et al. (2018) took ADWIN and mixed it with variational inference to make an online classification system. Wang et al. (2013) had a method for detecting opinion drift that was based on thresholds, but that limits how it can be used. Liu et al. (2016) introduced a new framework with an opinion shift detector using KL-divergence, though its performance depends on the labeling results. Tsytsarau and Palpanas (2016) came up with the idea of opinion contradictions and used it to detect sentiment changes, but their pairwise method doesn’t really consider much historical data.",
        "formal_text": "Convert casual text to formal text: There are a bunch of methods out there for sentiment analysis, like the ones by Wu et al. (2019), Fu et al. (2019), Kong et al."
    },
    {
        "casual_text": "MT isn't just for translators—it's available to any Commission staffer with a terminal or PC hooked up to the internal network. The number of pages the system has translated has jumped a lot in the past few years: 170,000 in 1995, 231,000 in 1996, and they’re expecting around 260,000 for 1997. It’s mostly used for...",
        "formal_text": "Convert casual text to formal text: MT isn't just for translators—it's available to any Commission staffer with a terminal or PC hooked to the internal network. The number of pages the system has translated has"
    },
    {
        "casual_text": "Event argument extraction. For the second part, we're dealing with a sequence classification task. Basically, we pick potential arguments from all the entity mentions in the sentence. Then, each of these potential arguments gets paired with a possible trigger to figure out their role. The system will then decide which of the 20 categories (19 roles and 'NONE' for entities not linked to the trigger) each trigger-entity pair belongs to.",
        "formal_text": "Convert casual text to formal text: Event argument extraction. For the second part, we're dealing with a sequence classification task. Basically, we pick potential arguments from all the entity mentions in the sentence. Then, each"
    },
    {
        "casual_text": "At this point, it should be pretty obvious why TFL (L) is a great fit for modern linguistics theories. First off, the LT part lets us talk about tree structures directly, which makes it perfect for setting rules about how words and phrases are grouped together. Plus, the LF part lets us describe more complex categories instead of just basic ones.",
        "formal_text": "Convert casual text to formal text: At this point, it should be pretty obvious why TFL (L) is a great fit for modern linguistics theories. First off, the LT part lets us talk about tree structures directly"
    },
    {
        "casual_text": "Sure! Here's a simpler way to say that: \"First, grab the top paragraph and make it the first one. Then, group the rest of the paragraphs together.\"",
        "formal_text": "Convert casual text to formal text: Sure! Here's a simpler way to say that: \"First, grab the top paragraph and make it the first one. Then, group the rest of the paragraphs together.\" \"First"
    },
    {
        "casual_text": "Based on the way we've categorized coreference types by looking at anaphor types, we've built a constraint-based multi-agent system.",
        "formal_text": "Convert casual text to formal text: Based on the way we've categorized coreference types by looking at anaphor types, we've built a constraint-based multi-agent system. Convert casual text to"
    },
    {
        "casual_text": "RefTL: Since we've used several human timelines as a reference, we're not just giving ROUGE scores for the different systems but also comparing the human timelines to each other. This helps us figure out the highest possible ROUGE score a system could get, which gives us a good idea of the upper limit.",
        "formal_text": "Convert casual text to formal text: RefTL: Since we've used several human timelines as a reference, we're not just giving ROUGE scores for the different systems but also comparing the human timelines each"
    },
    {
        "casual_text": "2. Spotting the signs of a non-inferiority or equivalence trial design, like:",
        "formal_text": "3. Spotting the signs of a non-inferiority or equivalence trial design, like: 1. Convert casual text to formal text: 2. Spotting the signs of a non-inferior"
    },
    {
        "casual_text": "Our headlines are all ready for you to tweak, and they’re listed starting with the newest ones. This way, you can come up with funny stuff while the stories are still fresh in your head. Plus, you can pick headlines based on the news topics you like. These features make it easier to play around with humor, even if you’re not a pro at it.",
        "formal_text": "Convert casual text to formal text: Our headlines are all ready for you to tweak, and they’re listed starting with the newest ones. This way, you can come up with funny stuff while the stories are still fresh in your"
    },
    {
        "casual_text": "From a technical standpoint, inter-process communication is handled using standard TCP-IP sockets. When it comes to text encoding, everything is done in UTF-8. This makes it easier to process text consistently and display different character sets properly.",
        "formal_text": "Convert casual text to formal text: From a technical standpoint, inter-process communication is handled using standard TCP-IP sockets. When it comes to text encoding, everything is done in UTF-8. This makes it"
    },
    {
        "casual_text": "Figures 3a (sentence training) and 3b (window training) show how the three main evaluation metrics change based on the number of training documents. We tested this with 1 to 19 documents, counting in steps of 2, and only using the 1-block scores to predict the HCD section. Basically, we picked the longest stretch of 1-block scores that met the earlier threshold. For this specific comparison, we didn’t use 5-block scores, so we could focus on how windowing affects the results. Each dot on the graph shows how Model i performed with a certain number of training documents, and the dashed lines show the average trend across all 20 versions of Model i.",
        "formal_text": "Convert casual text to formal text: Figures 3a (sentence training) and 3b (window training) show how the three main evaluation metrics change based on the number of training documents. We tested this with 1 to 19"
    },
    {
        "casual_text": "W' is the count of words in different languages, so it doesn't include the ones we don't know. And maxw is the highest number of words from the language that shows up the most in the sentence.",
        "formal_text": "Convert casual text to formal text: W' is the count of words in different languages, so it doesn't include the ones we don't know. And maxw is the highest number of words from the language that shows"
    },
    {
        "casual_text": "We're using the definition and classification of spin from Boutron et al. in 2010, which breaks down spin into different types and subtypes.",
        "formal_text": "Convert casual text to formal text: We're using the definition and classification of spin from Boutron et al in 2010, which breaks down spin into different types and subtypes. Convert casual text to formal text: We'"
    },
    {
        "casual_text": "Let’s start by looking at the mistakes when adapting between WSJ and BIO. We split the WSJ data into training and test sets, trained a parser on the training set, and then checked how it performed on both the test set and BIO. The accuracy went down from 90% on WSJ to 84% on BIO. After that, we figured out what kind of errors were happening by looking at the parts of speech (POS) involved. The biggest differences in errors were: verbs (2%), conjunctions (5%), digits (23%), prepositions (4%), adjectives (3%), determiners (4%), and nouns (9%). Two POS types really stood out: digits and nouns. Digits make up less than 4% of the tokens in BIO. The errors here come from BIO having long sequences of digits that aren’t in WSJ. Since these annotations are new and don’t follow WSJ rules, the parser can’t handle them without knowing the new rules. Nouns, on the other hand, are way more common—they’re 33% of BIO and 30% of WSJ tokens, making them the most frequent POS by a long shot. Plus, other POS types like adjectives, prepositions, determiners, and conjunctions often connect to nouns. To see if nouns were causing the most trouble, we tweaked a simple parser (no fancy second-order features) by adding a feature that helps it get noun-noun connections right. After this change, the parser’s performance on BIO went from 78% to 87%. This shows that most of the problems come from missing those noun-noun connections.",
        "formal_text": "Convert casual text to formal text: Let’s start by looking at the mistakes when adapting between WSJ and BIO. We split the WSJ data into training and test sets, trained a parser on the training"
    },
    {
        "casual_text": "Dependency parsing is all about figuring out the syntactic dependency tree for a given sentence, which we'll call z. The sentence itself, x, is just a sequence of words, like x1 to xn, with n being the total number of words. To make things easier, we usually add a dummy root word, x0, at the start of the sentence. The dependency tree, z, is made up of a bunch of directed edges connecting the words. These edges create a tree structure that starts at x0. Each edge goes from a parent word (also known as the head word) to a child word.",
        "formal_text": "Convert casual text to formal text: Dependency parsing is all about figuring out the syntactic dependency tree for a given sentence, which we'll call z. The sentence itself, x, is just"
    },
    {
        "casual_text": "Also, it’s clear that GAN-AEL boosts the greedy score way more than the average and extreme scores. This tells us that the responses GAN-AEL creates are way more informative. Basically, the average and extreme scores might be skewed by boring, unhelpful words. On the other hand, the greedy score looks at how words match up between two sentences, so it’s less affected by those generic words. Check out Table 3 for the human evaluation results—it shows that people like GAN-AEL better than Adver-REGS. This means the approximate embedding layer is better at passing the discriminator’s feedback to the generator compared to the reinforcement learning method from (Li et al., 2017). The difference is pretty solid, with a p-value less than 0.01 based on a sign test.",
        "formal_text": "Convert casual text to formal text: Also, it’s clear that GAN-AEL boosts the greedy score way more than the average and extreme scores. This tells us that the responses GAN-AEL creates"
    },
    {
        "casual_text": "• Step 1: Train one big model using all the domain data. • Step 2: Use a specific set of rules (the schema) to guide how the model generates stuff.",
        "formal_text": "Convert casual text to formal text: • Step 1: Train one big model using all the domain data. • Step 2: Use a specific set of rules (the schema) to guide how the model generates stuff. Convert casual"
    },
    {
        "casual_text": "So, basically, for some of the entries in Figure 6, they’ve added extra bits of the same title that weren’t used before, and put them in parentheses. This helps show the context around the main word. To make it even easier to spot, the most important part is highlighted in bold when you’re just skimming through.",
        "formal_text": "Convert casual text to formal text: So, basically, for some of the entries in Figure 6, they’ve added extra bits of the same title that weren’t used before, and put them in parentheses. This helps show the"
    },
    {
        "casual_text": "Words with different stress patterns can be tricky. For instance, our program might use the word \"record\" in a \". . . 10. . . \" way, but if it’s being used as a verb, a human would say it as \"01,\" messing up the rhythm we wanted. To make sure our poems sound right, we just cut out any words that could cause confusion. This issue is especially tricky with single-syllable words since their stress often depends on the context. Greene and his team (2010) looked at human-written sonnets and figured out the probabilities for words being stressed or unstressed, like P(0|word) and P(1|word). Following their method, we got rid of all single-syllable words unless they had a really high chance of being stressed or unstressed (like P(0|word) > 0.9 or P(1|word) > 0.9). This means words like \"to,\" \"it,\" \"in,\" and \"is\" are out, which actually pushes our poetry generator into new and interesting territory. After this, we ended up with 16,139 single-syllable words and 87,282 multi-syllable words. But since our fluency module (check out Section 7) only works with 20,000 word types, we had to trim down our list even more. We removed any words that weren’t in the 20,000 most common words from the song lyrics dataset we use for fluency. In the end, our final word list has 14,368 words: 4,833 single-syllable and 9,535 multi-syllable.",
        "formal_text": "Convert casual text to formal text: Words with different stress patterns can be tricky. For instance, our program might use the word \"record\" in a \". . . 10. . . \" way, but"
    },
    {
        "casual_text": "Basically, reordering makes a big difference when it comes to improving how well recognition and translation work, especially when you look at things like WER and BLEU scores. If you compare a cross-lingual language model without reordering to one with reordering, you can see some cool improvements. For example, the model with reordering and local constraints gives a 0.70% drop in WER and a 3.06 point boost in BLEU. If you use IBM constraints for reordering, you get an even better result: a 0.85% drop in WER and a 3.58 point increase in BLEU. But the best results come from using ITG constraints for reordering—it gives the lowest WER, down 0.92%, and the highest BLEU, up 3.89 points. All these WER improvements are super solid, with a 99% confidence level based on a two-proportional z-test. And the BLEU improvements are also strong, with a 95% confidence level from a paired student t-test using bootstrap resampling. Oh, and we picked the segmentation order s = 3 because it works the best in our setup.",
        "formal_text": "Convert casual text to formal text: Basically, reordering makes a big difference when it comes to improving how well recognition and translation work, especially when you look at things like WER and BLEU scores. If you"
    },
    {
        "casual_text": "We tested these models by looking at how they handle entailment relations between simple intransitive sentences, verb phrases, and transitive sentences using the datasets from (Kartsaklis and Sadrzadeh, 2016). Turns out, the Frobenius models worked the best, especially when we added our sentence-level measures. Overall, our results back up what was found in earlier research (Kartsaklis and Sadrzadeh, 2016) and really show that compositional models using some kind of intersective feature selection—like point-wise vector multiplication or tensor-based models with element-wise mixing (like the Frobenius ones)—are better for entailment tasks in distributional settings.",
        "formal_text": "Convert casual text to formal text: We tested these models by looking at how they handle entailment relations between simple intransitive sentences, verb phrases, and transitive sentences using the datasets from (Kartsakli"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Here, we're talking about how we align sentences using word matching. We're not going into the details of aligning paragraphs in this paper. Right now, our paragraph alignment tool isn't very reliable, and honestly, the results we get from sentence alignment are better when we skip paragraph alignment. Plus, since the bilingual texts we work with aren't super long, the time it takes to align sentences isn't a big deal, even without dealing with paragraphs.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. Here, we're talking about how we align sentences using word matching. We're not going into the details of aligning"
    },
    {
        "casual_text": "So, we're suggesting ditching the usual source training and trying out optimization-based meta-learning instead (shoutout to Finn et al., 2017) to make things more generalizable. Instead of just aiming for accuracy in the source domain, meta-learning, when working with source domains, tries to find a solid starting point, 0, that can quickly adapt to new tasks (or domains) with just a little bit of tweaking. To create a model that's easier to fine-tune on new tasks, especially when there's not much training data, meta-learning uses a different training approach. This approach focuses on improving generalization by simulating low-resource fine-tuning scenarios during training.",
        "formal_text": "Convert casual text to formal text: So, we're suggesting ditching the usual source training and trying out optimization-based meta-learning instead (shoutout to Finn et al., 2017) to make things more"
    },
    {
        "casual_text": "Also, for languages like Chinese and Japanese, where words aren’t separated by spaces, quotation marks are often skipped during translation. This makes it tricky to keep track of entities. To fix this, we use regular expressions to separate English parameters from Chinese words after translation. For Marian models, we make sure to put quotation marks around placeholders for numbers, time, and dates so they don’t get translated.",
        "formal_text": "Convert casual text to formal text: Also, for languages like Chinese and Japanese, where words aren’t separated by spaces, quotation marks are often skipped during translation. This makes it tricky to keep track of entities. To fix this"
    },
    {
        "casual_text": "For our parsing models, we stuck with the same hyperparameter setup as (Dozat and Manning, 2017). This includes 100-dimensional uncased word embeddings and POS tag vectors, three bi-directional LSTM layers (each with 400 dimensions), and 500 and 100-dimensional ReLU MLP layers for arc and label predictions, respectively. For the character-based version, we used 100-dimensional character vectors and a 200-dimensional LSTM. The other hyperparameters were tweaked using the PTB 3.3.0 development set, but we only tried a few different settings. In our experiments, we set the max size of candidate words  to 50, the coefficient  in Equation 5 to 15, and the maximum number of trials to 40. For each example, we stop the trials early if the drop in UAS is over 30% in the white-box setting.",
        "formal_text": "Convert casual text to formal text: For our parsing models, we stuck with the same hyperparameter setup as (Dozat and Manning, 2017). This includes 100-dimensional uncased word embeddings and"
    },
    {
        "casual_text": "For each spot in the target case frame of x, we try to find its matching part. We look at possible matches y in the target sentence and the two sentences before it. We check them one by one, starting with the y that’s closer in terms of grammar. If y is similar enough to the case slot—like, at least as similar as a certain threshold  (which is currently 0.95)—we assign it to that case slot.",
        "formal_text": "Convert casual text to formal text: Convert casual text to formal text: For each spot in the target case frame of x, we try to find its matching part. We look at possible matches y in the target sentence and the"
    },
    {
        "casual_text": "Mixture modelling is a pretty common method for figuring out how data is distributed in various fields of science (McLachlan & Peel, 2000). Basically, it’s great because it lets you balance between how complicated your model is and how much data you have to work with. You can adjust the complexity by changing the number of parts in the mixture while keeping the same basic structure for each part. Plus, there’s this super handy tool called the Expectation-Maximisation (EM) algorithm that makes it pretty straightforward to estimate the parameters of the mixture, so you don’t have to worry too much about that part.",
        "formal_text": "Convert casual text to formal text: Mixture modelling is a pretty common method for figuring out how data is distributed in various fields of science (McLachlan & Peel, 2000). Basically, it’s great"
    },
    {
        "casual_text": "We're using BBC-BJ for news and Desmond ODB (which is part of the Desmond dataset and focuses on religion) as our validation and test data. Here's why: (1) They're manually curated and really high-quality, (2) they're much smaller than the rest of our training data, so we're not losing too much that could have been used for training, and (3) they have similar sentence lengths to our training data. Unfortunately, there's no equivalent dataset for the conversation and general domains. For the conversation domain, all the datasets are automatically aligned, which makes them pretty noisy. In the general domain, both Tatoeba and TALPCo are manually curated, but their sentences are way shorter compared to Wikimatrix, especially Tatoeba. So, for these two domains, we do a random split, using all the datasets in each domain for validation and testing. Each split has 2000 unique pairs that weren't in the training set. For the general domain, we mix shorter sentences from TALPCo with longer ones from Wikimatrix for our validation and test data. We noticed that Tatoeba has high-quality sentences similar to TALPCo, just shorter. Since longer sentences make for a more challenging and meaningful evaluation, we decided to include TALPCo in our validation and test sets instead.",
        "formal_text": "Convert casual text to formal text: We're using BBC-BJ for news and Desmond ODB (which is part of the Desmond dataset and focuses on religion) as our validation and test data. Here's why"
    },
    {
        "casual_text": "To figure out how well different ranking systems or models worked for comparing realizer systems, we looked at seven different \"systems\" from the whole dataset. These included five OpenCCG-based systems (three baseline models, plus the best and worst from the full perceptron model) and two XLE-based systems (the best and worst after ranking with an n-gram model). We compared the average adequacy and fluency scores of each of these seven systems to see how they stacked up against each other, which gave us 21 pairs to compare. Then, we used Tukey's HSD test to check which systems had significantly different scores. The test showed that five of the pairs had noticeably different results.",
        "formal_text": "Convert casual text to formal text: To figure out how well different ranking systems or models worked for comparing realizer systems, we looked at seven different \"systems\" from the whole dataset. These included five OpenCCG-based systems"
    },
    {
        "casual_text": "To get the data ready, we start by grouping similar documents together to make our search smaller and easier. Then, we create pairs of examples for training the model by picking them out.",
        "formal_text": "Convert casual text to formal text: To get the data ready, we start by grouping similar documents together to make our search smaller and easier. Then, we create pairs of examples for training the model by picking them out."
    },
    {
        "casual_text": "Okay, so basically, we've got this condition where (x c • h s  1 + x z • h s ) holds true. And then, we set the rank(c, s, Z) equal to (|Z|  1)/K, just like they did in earlier research.",
        "formal_text": "Convert casual text to formal text: Okay, so basically, we've got this condition where (x c • h s  1 + x z • h s ) holds true. And then"
    },
    {
        "casual_text": "In the hypothesis-making process, the question suggestions were pretty important. WISDOM X also comes up with other kinds of questions, like \"Why does Vibrio parahaemolyticus swell when the sea temperature goes up?\" (see Figure 1b) and \"What even is Vibrio parahaemolyticus?\" The first question is basically asking for proof that the rise in sea temperature causes Vibrio parahaemolyticus to swell. These kinds of questions help pick out the most reliable answers from what WISDOM X gives. Also, if someone just types in a keyword instead of a full question, WISDOM X will list questions related to that keyword. For example, if you type in \"smartphone,\" WISDOM X will suggest around five hundred questions, such as \"What can smartphones do?\" WISDOM X ranks the answers based on confidence scores, which are calculated differently depending on the type of question. For instance, answers to \"why\" questions get their scores from a supervised classifier. Plus, for fact-based questions, WISDOM X groups similar answers together to make it easier for users to find useful info. This similarity is figured out using unsupervised word clustering (Kazama and Torisawa, 2008).",
        "formal_text": "Convert casual text to formal text: In the hypothesis-making process, the question suggestions were pretty important. WISDOM X also comes up with other kinds of questions, like \"Why does Vibrio parahaemolytic"
    },
    {
        "casual_text": "Block sampling (z i, j, l, u i, j, l ): Basically, for every possible topic z i, j, l = k, we consider all the options.",
        "formal_text": "Block sampling (z i, j, l, u i, j, l ): Basically, for every possible topic z i, j, l = k, we consider"
    },
    {
        "casual_text": "We took a look at some social science research (section 3) about racial bias in sports broadcasts. Traditional studies usually focus on a small number of games from one season and use manual coding to spot differences in how announcers talk (like Rainville and McCormick, 1977; Billings, 2004; Rada and Wulfemeyer, 2005). For instance, Rada (1996) did a detailed analysis of five games from the 1992 season, looking at things like players' cognitive or physical traits. Our approach uses a computational method called FOOTBALL to revisit this kind of research (section 3), without needing human judgment for coding. In the field of NLP, people have looked at gender bias in word embeddings (Bolukbasi et al., 2016; Caliskan et al., 2017), racial bias in police stops (Voigt et al., 2017) and on Twitter (Hasanuzzaman et al., 2017), and biases in NLP tools like sentiment analysis (Kiritchenko and Mohammad, 2018). Our work is closely related to Ananya et al. (2019), who studied gender bias at the mention level, and Fu et al. (2019), who looked at gender bias in tennis broadcasts. Other sports datasets include event-annotated baseball commentaries by Keshet et al. (2011) and WNBA and NBA basketball commentaries by Aull and Brown (2013). But we want to highlight that FOOTBALL is the first big sports commentary dataset specifically annotated for race.",
        "formal_text": "Convert casual text to formal text: We took a look at some social science research (section 3) about racial bias in sports broadcasts. Traditional studies usually focus on a small number of games from one season and use"
    },
    {
        "casual_text": "The stuff this system creates in the paper is kinda like those special templates for certain topics. They could help make information in documents easier to find when you're searching for something. If the system could make these templates on its own, that would be a pretty big deal.",
        "formal_text": "Convert casual text to formal text: The stuff this system creates in the paper is kinda like those special templates for certain topics. They could help make information in documents easier to find when you're searching for something. If the system"
    },
    {
        "casual_text": "The analysis we just talked about shows that doing the same search over and over isn't the main thing that matters. Instead, it seems that using info about individual users is just as important, if not more so, than using general info about all users. This part here is trying to dig deeper into this idea to help us figure out how to balance the importance of individual user data with common user data from a research perspective.",
        "formal_text": "Convert casual text to formal text: The analysis we just talked about shows that doing the same search over and over isn't the main thing that matters. Instead, it seems that using info about individual users is just as important,"
    },
    {
        "casual_text": "For the embedding layer, we went with the pre-trained GloVe 3 embeddings (Pennington et al., 2014) that were trained on 6 billion words from Wikipedia 2014 and Gigaword 5. These embeddings get fine-tuned during training. We limited the vocabulary to the 4,000 most common words, following Taghipour and Ng (2016), and treated any other words as unknown. We set the number of sentences per essay to the maximum for each prompt and capped the sentence length at 128. The models were trained in batches of 16 for 50 epochs. We tweaked the hyperparameters using optuna over 100 trials.",
        "formal_text": "Convert casual text to formal text: For the embedding layer, we went with the pre-trained GloVe 3 embeddings (Pennington et al., 2014) that were trained on 6"
    },
    {
        "casual_text": "In Yang et al. (2019), we found that our own annotation results showed a positive connection between local acceptability and verifiability, and a negative one between local acceptability and disputability. Plus, there was a positive link between knowledge awareness and verifiability. This pattern matches what we’d expect based on how we defined these attributes. Figure 5 shows the Pearson's Correlation Coefficient (PC) for these four attributes in our annotations, using the two-step reason selection method. The \"good squared\" and \"good\" annotations had a clear correlation pattern, similar to what Yang et al. (2019) reported. On the flip side, \"moderate\" and \"all\" annotations had a less clear pattern, and \"bad\" annotations didn’t show much of a pattern except for a small negative correlation between local acceptability and disputability. We think the correlation patterns in the \"good squared\" and \"good\" annotations align well with what we’d expect intuitively in language, while the others don’t. We also believe the differences in these patterns across the annotation groups demonstrate how effective our quality control method is, thanks to the two-step reason selection process.",
        "formal_text": "Convert casual text to formal text: In Yang et al. (2019), we found that our own annotation results showed a positive connection between local acceptability and verifiability, and a negative one between local acceptability"
    },
    {
        "casual_text": "An information-seeking conversation usually involves two people: one who’s looking for info and the other who’s trying to give it. There’s always some task the person asking wants to do later, and they’re trying to get the details they need to figure out how to handle it. For example, they might be planning to enroll in a university program, treat a patient, or plan a vacation. The person asking questions is just trying to gather all the info they need to make that plan happen.",
        "formal_text": "Convert casual text to formal text: An information-seeking conversation usually involves two people: one who’s looking for info and the other who’s trying to give it. There’s always some task the person asking wants to do"
    },
    {
        "casual_text": "Got it! The word \"fish\" is no longer included, but the counts for \"bank\" appearing with \"money,\" \"boat,\" and \"bond\" are still there and haven't been adjusted in any way.",
        "formal_text": "Convert casual text: Got it! The word \"fish\" is no longer included, but the counts for \"bank\" appearing with \"money,\" \"boat,\" and \"bond\" are still there and haven't been adjusted in any"
    },
    {
        "casual_text": "In Figure 3 (a), you can see that our model is designed to figure out the missing words on the target side of real sentence pairs. Basically, we use a bunch of unlabeled data to train it.",
        "formal_text": "Convert casual text to formal text: In Figure 3 (a), you can see that our model is designed to figure out the missing words on the target side of real sentence pairs. Basically, we use a bunch of unlabele"
    },
    {
        "casual_text": "This stuff is under a Creative Commons Attribution 4.0 International License. Here's the link for the license details: http://creativecommons.org/licenses/by/4.0/.",
        "formal_text": "Convert casual text to formal text: This stuff is under a Creative Commons Attribution 4.0 International License. Here's the link for the license details: http://creativecommons.org/licenses/by"
    },
    {
        "casual_text": "The dictionary sorts words based on their main types, like nouns, verbs, and adjectives. It also includes some smaller groups, such as plurals or past tense forms, and even has special categories for medical terms.",
        "formal_text": "Convert casual text to formal text: The dictionary sorts words based on their main types, like nouns, verbs, and adjectives. It also includes some smaller groups, such as plurals or past tense forms,"
    },
    {
        "casual_text": "The context for the current character w_t comes from the characters around it within a window of size c, along with their segment labels (check out Figure 2). The characters w_i and their labels l_i in this context are turned into vectors, e_c(w_i) and e_c(l_i), both in Rd, where d is the size of the embedding.",
        "formal_text": "Convert casual text to formal text: The context for the current character w_t comes from the characters around it within a window of size c, along with their segment labels (check out Figure 2). The characters w_"
    },
    {
        "casual_text": "The model setups mentioned in Section 2.1 are different because they look at different \"window\" sizes on the output structure. Instead of changing the window size, we can get the same result by keeping the window size the same but tweaking the output values. For instance, imagine changing the output values by swapping single output tags with pairs of neighboring output tags.",
        "formal_text": "Convert casual text to formal text: The model setups mentioned in Section 2.1 are different because they look at different \"window\" sizes on the output structure. Instead of changing the window size, we can get the same result by keeping"
    },
    {
        "casual_text": "We built a text simplification tool that’s pretty much what you’d expect from a real simplification app. The idea was to create a realistic setup to test our adaptive approach in a user-focused situation. We hooked the app up to MTurk to get some paid users, but it’s easy to make this thing available online or run it locally on your own computer.",
        "formal_text": "Convert casual text to formal text: We built a text simplification tool that’s pretty much what you’d expect from a real simplification app. The idea was to create a realistic setup to test our adaptive approach in"
    },
    {
        "casual_text": "We're looking at String Kernels that count how many substrings with gaps two sequences have in common. Basically, some letters from the original string are skipped. We used a super efficient algorithm from (Shawe-Taylor and Cristianini, 2004; Lodhi et al., 2000). To be more specific, we used words and markers as symbols, kind of like what (Cancedda et al., 2003; Moschitti, 2008) did.",
        "formal_text": "Convert casual text to formal text: We're looking at String Kernels that count how many substrings with gaps two sequences have in common. Basically, some letters from the original string are skipped. We used a"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. So, we're dealing with matrices K_t and V_t, which have dimensions [B, M, N + T, D]. Basically, K_0 and K_t (or V_0 and V_t) are the same for all positions in the matrix. Specifically, K_0(b, i, n, d) equals K_t(b, j, n, d), and the same goes for V_0(b, i, n, d) and V_t(b, j, n, d). This holds true for all values of b, i, j, n, and d within their respective ranges: b from 0 to B-1, i and j from 0 to M-1, n from 0 to N-1, and d from 0 to D-1.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way. So, we're dealing with matrices K_t and V_t, which have dimensions [B, M,"
    },
    {
        "casual_text": "Basically,  represents the variance for dimension K. By nudging neutral words towards 0 in the dimension we're focusing on, we can shift the attention away from certain word types in V* and instead point it toward other words in x c i.",
        "formal_text": "Convert casual text to formal text: Basically,  represents the variance for dimension K. By nudging neutral words towards 0 in the dimension we're focusing on, we can shift the attention away from certain word types in"
    },
    {
        "casual_text": "For our next steps, as mentioned in Section 4.1, we’re thinking about a way to deal with proper nouns, like organization names. Also, as talked about in Section 5.4, we need to figure out how to handle words that can act as both a noun and a suffix.",
        "formal_text": "Convert casual text to formal text: For our next steps, as mentioned in Section 4.1, we’re thinking about a way to deal with proper nouns, like organization names. Also, as talked about in Section 5.4"
    },
    {
        "casual_text": "We tested Model E from Table 5 with GloVe (Pennington et al., 2014) and a couple of other pre-trained models like BERT (Devlin et al., 2019b) and RoBERTa (Liu et al., 2019). We also compared them to ComBERT. The results in Table 5 show that ComBERT did the best, which supports the idea that using contextualized token representations improves event extraction. ComBERT is used in all the models mentioned in Table 4.",
        "formal_text": "Convert casual text to formal text: We tested Model E from Table 5 with GloVe (Pennington et al., 2014) and a couple of other pre-trained models like BERT (Dev"
    },
    {
        "casual_text": "For a character-based Chinese NER model, the input sentence is treated as a sequence of characters.",
        "formal_text": "Convert casual text to formal text: For a character-based Chinese NER model, the input sentence is treated as a sequence of characters. Convert casual text to formal text: For a character-based Chinese NER model"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. We have a bunch of pairs like (f_s, f_t) that can either be 0 or 1, and there are n of them. Now, we're defining something called W, which takes a feature f and gives us a real number. For each feature f that looks like (f_s, f_t), W(f) is calculated like this: W(f) = (#(f_s, f_t) / #(f_s)) * (#(f_s, f_t) / #(f_t)) Basically, it's the count of (f_s, f_t) divided by the count of f_s, multiplied by the count of (f_s, f_t) divided by the count of f_t.",
        "formal_text": "Convert casual text to formal text: Okay, let's break this down in a simpler way. We have a bunch of pairs like (f_s, f_t) that can either be 0 or 1, and"
    },
    {
        "casual_text": "Lately, there have been some pre-trained models for learning code (like the ones by Feng et al. in 2020, Guo et al. in 2021, Ahmad et al. in 2021, and Chen et al. in 2021). These models were trained on big datasets using general tasks, like masked language modeling, and can be adjusted for specific code tasks, such as generating comments or naming methods. When it comes to evaluating these pre-trained models, there's a pretraining set in addition to the usual training, validation, and test sets. Our approach can be applied to these pre-trained models too. For instance, in our time-segmented methodology, the pretraining set includes samples that are available before any of the other sets. Interestingly, no one has looked at the timestamps of samples during evaluation before.",
        "formal_text": "Convert casual text to formal text: Lately, there have been some pre-trained models for learning code (like the ones by Feng et al. in 2020, Guo et al. in 2021,"
    },
    {
        "casual_text": "We looked at data from 7 games of the PSC. These games have a total of 778 utterances, which add up to 13,692 words. There are also 12,275 events in the data. All of this is tagged with ground truth information that shows how utterances and fragments align.",
        "formal_text": "Convert casual text to formal text: We looked at data from 7 games of the PSC. These games have a total of 778 utterances, which add up to 13,692 words. There are also 12,275 events"
    },
    {
        "casual_text": "The target dataset usually has a ton of data (like around 60 sentences in Wizard). To save time, we decided to stick with the first 40 sentences, following what Kim et al. did in 2020. We used the basic version of BART with 139 million parameters, and KAT has 196 million parameters. The batch sizes for stages I, II, and III are 2048, 128, and 16, respectively. The max sequence lengths for the source and target are 256 and 64. All models were optimized using AdamW with a learning rate of 5e-5 over 3 epochs. We also used beam search for decoding responses, with the number of beams ranging from 1 to 3, as implemented by Wolf et al. in 2020. Table 1, 2, and 3 show the evaluation results based on automatic metrics. Here's what we found: 1. In the full-data scenario, KAT performs really well and is considered state-of-the-art without needing any extra datasets. This means KAT is a top-notch dialogue model on its own. Also, when there's plenty of training data, extra resources aren't necessary, so TSLF doesn't make much of a difference in this case.",
        "formal_text": "Convert casual text to formal text: The target dataset usually has a ton of data (like around 60 sentences in Wizard). To save time, we decided to stick with the first 40 sentences, following what Kim et al."
    },
    {
        "casual_text": "Our algorithm picked the right antecedent in 105 out of 112 cases, which is a success rate of 93.8%. In 28 of those cases, there was a pronoun among the options that pointed to the same antecedent. So, in the other 84 cases, the algorithm figured out the correct antecedent by looking at things like the position in the sentence, how often the noun phrase was repeated, and how it was used with other words.",
        "formal_text": "Convert casual text to formal text: Our algorithm picked the right antecedent in 105 out of 112 cases, which is a success rate of 93.8%. In 28 of those cases, there was a pronou"
    },
    {
        "casual_text": "When you call \"np(s, [walk], [] )\", it triggers clause (p-l). Then, the system uses the predicate llnk to see if \"np\" can turn into \"s\". Once that's done, it moves on to \"goal (vp, [walk], [] )\". By now, the system has already figured out that \"you\" is \"up\" and is expecting the next part, \"walk\", to fit into the \"vp\" category.",
        "formal_text": "Convert casual text to formal text: Convert casual text to formal text: When you call \"np(s, [walk], [] )\", it triggers clause (p-l). Then, the system uses the"
    },
    {
        "casual_text": "Check out Table 1 for how our reordering extensions performed. We tested them on both the development and test sets. We've got the results for deep and shallow rules right next to each other in different columns, so you can easily compare them. Every setup we tried comes in both deep and shallow versions, listed in separate rows.",
        "formal_text": "Convert casual text to formal text: Check out Table 1 for how our reordering extensions performed. We tested them on both the development and test sets. We've got the results for deep and shallow rules right next to each other"
    },
    {
        "casual_text": "• Transformer: We're using a basic Transformer model for training. Here are a couple of links for reference: 4) https://pypi.org/project/langdetect, and 5) http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2018/baseline/dataPreparationJE.html.",
        "formal_text": "Convert casual text to formal text: • Transformer: We're using a basic Transformer model for training. Here are a couple of links for reference: 4) https://pypi.org/project/langdetec"
    },
    {
        "casual_text": "Just like the earlier phrase segmentation models (Lee et al., 2011; Xiong et al., 2011), we’ve added the phrase segmentation model as an extra feature function into the log-linear model.",
        "formal_text": "Convert casual text to formal text: Just like the earlier phrase segmentation models (Lee et al., 2011; Xiong et al., 2011), we’ve added the phrase segmentation model as"
    },
    {
        "casual_text": "In this paper, we tweak the siamese architecture by adding an element-wise absolute difference layer, which we then combine with a bunch of fully-connected layers. The last layer is a sigmoid layer for binary classification. The goal here is to get the CNNs to pick up on phonological differences while they're training. The absolute difference () thing is kind of like the 1 norm and is defined as...",
        "formal_text": "Convert casual text to formal text: In this paper, we tweak the siamese architecture by adding an element-wise absolute difference layer, which we then combine with a bunch of fully-connected layers. The last layer is"
    },
    {
        "casual_text": "2) We need to figure out how to turn a sentence into a DRS in a clear, structured way.",
        "formal_text": "Convert casual text to formal text: 2) We need to figure out how to turn a sentence into a DRS in a clear, structured way."
    },
    {
        "casual_text": "6. So, does the system have \"j'\" (all lowercase) in its translation model? 7. What's the probability of \"hopeespère\"?",
        "formal_text": "6. So, does the system have \"j'\" (all lowercase in its translation model? 7. What's the probability of \"hopeespère\"? 7. What's the probability of \"hopeespère\"? 8. Convert"
    },
    {
        "casual_text": "Since our vector spaces have orthonormal bases, these maps also create something called a special commutative Frobenius algebra. This basically means they can copy and uncopy the basis vectors in a uniform way. When we use the copying map  on a vector v in space N, it gives us the bases of v and the unit map  gives us the weights that go with those bases. Together, they can turn a lower-dimensional tensor in N into a higher-dimensional one in N  N. In simpler terms, (v) is like a diagonal tensor where the diagonal parts are the weights of v. Now, the uncopying map  works the other way around. When we use it to turn a higher-dimensional tensor into a lower-dimensional one, we lose some info. For a tensor w in N  N, (w) only keeps the diagonal parts of w, so we lose all the non-diagonal stuff.",
        "formal_text": "Convert casual text to formal text: Since our vector spaces have orthonormal bases, these maps also create something called a special commutative Frobenius algebra. This basically means they can copy and uncopy the basis vectors in"
    },
    {
        "casual_text": "The method we're suggesting is pretty simple to follow (check out Fig. 1c). For each sample we're looking at, we grab its features using the SetConv layer and then match it up with the class representatives we got after training. The class that's closest to our sample gets picked as the predicted label.",
        "formal_text": "Convert casual text to formal text: The method we're suggesting is pretty simple to follow (check out Fig. 1c). For each sample we're looking at, we grab its features using the SetConv layer and then"
    },
    {
        "casual_text": "We also experimented with freezing the knowledge encoder and context encoder during stage III or stages II and III. The results didn't show any improvement, which suggests that with the help of stage II, our model is unlikely to overfit.",
        "formal_text": "Convert casual text to formal text: We also experimented with freezing the knowledge encoder and context encoder during stage III or stages II and III. The results didn't show any improvement, which suggests that with the help of stage"
    },
    {
        "casual_text": "To hit the second goal, we’re looking at two different ways to create questions from a passage. The first method uses a retrieval-based approach with a convolution neural network (CNN), and the second one is a generation-based method that uses a recurrent neural network (RNN). We’ll check how well these methods work by looking at the BLEU score (Papineni et al., 2002) and also getting feedback from people. In Section 9, we’ll talk about the good and bad sides of each method.",
        "formal_text": "Convert casual text to formal text: To hit the second goal, we’re looking at two different ways to create questions from a passage. The first method uses a retrieval-based approach with a convolution neural network ("
    },
    {
        "casual_text": "So, the idea here is to use some basic rules to change words into their closest Modern Standard Arabic (MSA) version, like what the MT system 1 already does with its training data. These rules can be made manually and then improved by using tools like GIZA++ (Al-Onaizan et al. 1999) to learn from bilingual or bi-dialectal data. There are also modules for phrasal extraction, as explained in (Och and Ney, 2004) and (Koehn et al., 2007). Each dialect has its own training process to create translation pairs between dialectal Arabic and MSA. This training uses a bi-dialectal corpus, but a small part of it is kept aside for tweaking parameters and testing. After that, all possible ways a word could be transliterated are considered, especially for Arabic written in Latin characters, like \"tawle\" for \"table\". Each option is weighted based on how much it changes from the original form.",
        "formal_text": "Convert casual text to formal text: So, the idea here is to use some basic rules to change words into their closest Modern Standard Arabic (MSA) version, like what the MT system 1 already does with its training data. These"
    },
    {
        "casual_text": "Sequence-level interpolation (Seq-Inter) doesn’t just make models trained with Word-KD and Seq-KD better—it also improves the original teacher model that was trained on real data but then fine-tuned using Seq-Inter data (we’ll call this the Baseline + Seq-Inter). In fact, using greedy decoding with this fine-tuned model gives similar results (19.6) to using beam search with the original model (19.5), which means we can decode faster without needing a bigger model. We think sequence-level knowledge distillation works well because it lets the student model focus on the important parts of the teacher’s distribution (like the most likely options) instead of trying to model every possible translation, which would just waste resources. Our findings seem to back this up: Seq-KD models put way more of their probability weight on the most likely translation compared to baseline models trained on the original data (check Table 1 for p(t =)). For instance, when translating from English to German, the most likely translation for the 2  500 Seq-KD model accounts for 16.9% of the total probability mass on average, while the baseline only hits 0.9%. This is also why greedy decoding works so well for Seq-KD models—since the student model is mostly focused on the teacher’s most likely options, its distribution is sharper, making it easier to find the best translation. Seq-Inter kind of balances things out, with the greedily decoded sequence taking up 7.6% of the distribution.",
        "formal_text": "Convert casual text to formal text: Sequence-level interpolation (Seq-Inter) doesn’t just make models trained with Word-KD and Seq-KD better—it also improves the original teacher"
    },
    {
        "casual_text": "That’s basically how vector averaging works for combining meanings. The more vectors you add together, the more general the result gets, not more specific. But with the multiplicative method, it focuses on the parts of the vectors that actually matter for the combination and does a better job of capturing how the two things work together.",
        "formal_text": "Convert casual text to formal text: That’s basically how vector averaging works for combining meanings. The more vectors you add together, the more general the result gets, not more specific. But with the multiplicative"
    },
    {
        "casual_text": "We also tried using deterministic reasoning on the training data, focusing on the transitive closure of cat-cat and inst-cat edges. Basically, if we only have direct cat-cat edges (like if we have a, b and b, c, but not a, c), we tried to learn embeddings that could help us figure out the taxonomy. For instance edges, we did the same thing—calculated the transitive closure (like if we have i, a and a, b, we added i, b)—and then summarized using the inferred inst-cat edges. This led to two more variations of our data sets: 1. **Direct**: We didn’t compute the transitive closure for category edges. For PubMed articles, we only used the human-labeled subject headings. This means most instances won’t have edges connecting them to a category and its ancestors. Rebuilding a taxonomy from just direct edges is super tough, especially with embedding methods.",
        "formal_text": "Convert casual text to formal text: We also tried using deterministic reasoning on the training data, focusing on the transitive closure of cat-cat and inst-cat edges. Basically, if we only have direct"
    },
    {
        "casual_text": "Okay, so here's the deal: First, we're temporarily making a copy of dg 1, just so it's the same as itself. Next, we're doing a multi-value thing where the first value is called 'newarc' and the second one is called 'dumged'. Then, we call a function called unify1 multiple times. The number of times we call it depends on how many unify-dg things are in the whole system. For example, unify1 is used in stuff like analysis (like what Ponard and Sag did in 1987) that deals with stuff like coordination, case adjustments, adjectives, control, weird categories, zero pronouns, questions, WH constructions, and even some pragmatics (like how people talk to each other, being polite, etc.). This was also mentioned by Yoshimoto and Kogure in 1989. This grammar covers a lot of important stuff in conversational Japanese. The grammar graphs, which come from some math equations, have 2,324 nodes. We tested this with 16 sentences from a sample phone conversation. The sentences ranged from super short (like just the word 'no') to pretty long ones (like \"In that case, we [speaker] will send you [hearer] the registration form.\"). The number of unifications per sentence varied a lot—from 6 up to over 500.",
        "formal_text": "Convert casual text to formal text: Okay, so here's the deal: First, we're temporarily making a copy of dg 1, just so it's the same as itself. Next, we're doing"
    },
    {
        "casual_text": "For a chosen anchor from the training batch, RLL first ranks how similar all the same type (positive) points are compared to the different categories (negative) points and makes sure there’s a set margin between them. To break it down, let’s say we have a batch of normalized relation representations called B = r1, ..., rm that come from the neural encoder, and we pick an instance (the anchor) ri from this batch (where m is the batch size). We want the positive points for this anchor in B to be close together, while the negative points should be on the other side. So, we use this formula to figure it all out:",
        "formal_text": "Convert casual text to formal text: For a chosen anchor from the training batch, RLL first ranks how similar all the same type (positive) points are compared to the different categories (negative) points and makes sure there’s"
    },
    {
        "casual_text": "So, C(n, o) is just the count of how many times \"o\" got turned into \"n\" during normalization, and C(o) is how many times \"o\" showed up in the phrase pairs we looked at. Table 4 shows a bit of this normalization model. The table has three columns: the original phrase, what it got normalized to, and the probability of that happening. Looking at Table 4, we can see that \"wanna\" usually gets turned into \"want to\" with a pretty high chance, but it can also become things like \"will\" or \"going to.\" On the other hand, the word \"4\" by itself doesn’t have much chance of being normalized to \"for.\" That makes sense because without more context, you can’t really tell. But when you see \"go 4,\" it’s way more likely to become \"go for,\" which tells us that in the context of \"go,\" \"4\" is usually used as a preposition.",
        "formal_text": "Convert casual text to formal text: So, C(n, o) is just the count of how many times \"o\" got turned into \"n\" during normalization, and C(o) is how many times \"o"
    },
    {
        "casual_text": "For spaCy, we stuck to the usual recommendations. All the other methods were done using TensorFlow. As for BERT models, we went with HuggingFace's implementation (shoutout to Wolf et al., 2020). We also used Adam (thanks, Kingma and Ba, 2015), Glorot initialization (props to Glorot and Bengio, 2010), and the categorical cross-entropy loss.",
        "formal_text": "Convert casual text to formal text: For spaCy, we stuck to the usual recommendations. All the other methods were done using TensorFlow. As for BERT models, we went with HuggingFace's implementation ("
    },
    {
        "casual_text": "So, for each utterance in the set U1, U2, ..., UN, we use max-pooling on the word embeddings within that utterance to get an individual embedding, which we'll call eui. This process gives us a collection of utterance embeddings like eu1, eu2, ..., euN for the entire dialogue.",
        "formal_text": "Convert casual text to formal text: So, for each utterance in the set U1, U2, ..., UN, we use max-pooling on the word embeddings within that utterance to"
    },
    {
        "casual_text": "Since the connections between relations and concepts are pretty sparse, we break the problem down into smaller chunks, each focusing on just one concept variable. For each concept, we figure out which relations might be related and then use a BNSL algorithm to map out the network structure between them. Once we’ve got that network, we use it to help us discover more concepts. The whole process is laid out in Algorithm 1. In the next sections, we’ll go over the main steps in more detail.",
        "formal_text": "Convert casual text to formal text: Since the connections between relations and concepts are pretty sparse, we break the problem down into smaller chunks, each focusing on just one concept variable. For each concept, we figure out which relations"
    },
    {
        "casual_text": "Because VAR-STREAM has to handle extra encodings and beams, it uses up more GPU memory in this perfect scenario.",
        "formal_text": "VAR-STREAM uses more GPU memory in this perfect scenario. Convert casual text to formal text: Because VAR-STREAM uses extra encodings and beams, it uses more GPU memory in this perfect scenario. Con"
    },
    {
        "casual_text": "We take a look at how different important factors affect our method, like the event, how topics flow, time penalties, and something called LSA-style dimensionality reduction. Plus, we create a random order and a basic order based just on time and text. Table 2 shows the 9 different ways we're comparing, each with its own code. Check out Table 2 for the details on those peer orderings.",
        "formal_text": "Convert casual text to formal text: We take a look at how different important factors affect our method, like the event, how topics flow, time penalties, and something called LSA-style dimensionality reduction. Plus, we create"
    },
    {
        "casual_text": "Our decoder-based method is kinda like the way they tune translation models with stuff like MERT (Och, 2003), MIRA (Chiang et al., 2008), or pairwise ranked optimization (Hopkins and May, 2011). Both methods use a small group of bilingual sentences, the translations from the base decoder, the reference translations, and some evaluation metric.",
        "formal_text": "Convert casual text to formal text: Our decoder-based method is kinda like the way they tune translation models with stuff like MERT (Och, 2003), MIRA (Chiang et al., 2008"
    },
    {
        "casual_text": "Unlike what happened in the previous section, the accuracy doesn't go down when  gets smaller in this experiment. That's because the documents here mostly focus on news about just one party. Since they don't often talk about what other parties are doing, the co-occurrence blocks do a good job showing the relationship between two people. So, even with a low  value, it's easy to pick out these pairs of names.",
        "formal_text": "Convert casual text to formal text: Unlike what happened in the previous section, the accuracy doesn't go down when  gets smaller in this experiment. That's because the documents here mostly focus on news about just one party."
    },
    {
        "casual_text": "To make it easier to talk about a specific tree in the group P, we give each tree in P a special number, which we call an index. For the main trees, we usually write this index as a little number below an \",\" and for the extra trees, we use \".\" This way, if we have more than one of the same tree in I or A, we can tell them apart by their index. (We'll be a bit lazy with how we write this and just use the index or the tree itself, depending on what's easier to talk about.)",
        "formal_text": "Convert casual text to formal text: To make it easier to talk about a specific tree in the group P, we give each tree in P a special number, which we call an index. For the main trees, we usually write"
    },
    {
        "casual_text": "Step 3 is basically the same as the word-level NLL process, but this time it's done on the freshly created data set. You can see this in Figure 1, right in the middle.",
        "formal_text": "Convert casual text to formal text: Step 3 is basically the same as the word-level NLL process, but this time it's done on the freshly created data set. You can see this in Figure 1, right in the middle."
    },
    {
        "casual_text": "Let’s start by looking at a simpler version of the \"learning with hints\" problem. Imagine we’re only focused on learning f2. We’ve got a little bit of data that’s already labeled with f2 (we’ll call this D) and a whole bunch of data labeled with f1 (we’ll call this D_unlab—the \"unlab\" part means it’s unlabeled as far as f2 is concerned).",
        "formal_text": "Convert casual text to formal text: Let’s start by looking at a simpler version of the \"learning with hints\" problem. Imagine we’re only focused on learning f2. We’ve got a little bit of"
    },
    {
        "casual_text": "hm and hp come from the modulus and phase parts, respectively. So, the entity embeddings are basically hm and hp combined. We use cosine similarity to figure out the entity's neighborhood in the latent space, and we'll talk more about that in the next section. For now, we train HAKE first, then keep its parameters fixed for the rest of the process.",
        "formal_text": "Convert casual text to formal text: hm and hp come from the modulus and phase parts, respectively. So, the entity embeddings are basically hm and hp combined. We use cos"
    },
    {
        "casual_text": "Check out Table 5 for the F-measure results. It’s pretty clear that the HITS-based weighting method works better than the equal weighting method for all the communities. This shows that user authority really matters when it comes to figuring out community emotions. We dug deeper and looked at the events that the HITS-based method nailed but the equal weighting method missed. By going through the microblog posts from those time periods, we found that most of the mistakes from the equal weighting method were due to emotion spam. Some low-authority users post a ton of extreme emotion messages just to grab attention. Since there weren’t any big community events happening at those times, the high-authority users didn’t show any emotional shifts. In the equal weighting method, this emotion spam messed up the results. But with the HITS-based approach, the users who post this spam have small weights, so they don’t really affect the community emotion. That’s why the HITS-based method is way more effective than the equal weighting one.",
        "formal_text": "Convert casual text to formal text: Check out Table 5 for the F-measure results. It’s pretty clear that the HITS-based weighting method works better than the equal weighting method for all the communities. This shows that"
    },
    {
        "casual_text": "Constraint 8 helps fix the issue of having duplicate characters. If two identical characters are next to each other, only one of them stays—which one is kept depends on the overall decoding score. This is one of the cool things about ILP compared to regular sequence labeling methods.",
        "formal_text": "Convert casual text to formal text: Constraint 8 helps fix the issue of having duplicate characters. If two identical characters are next to each other, only one of them stays—which one is kept depends on the overall decoding score."
    },
    {
        "casual_text": "In the past, people mostly used recurrent neural networks (RNN) for this kind of work (like Krishna et al., 2017), but they had trouble dealing with long-term dependencies. Nowadays, attention-based models (Zhou et al., 2018b; Sun et al., 2019b, a) are becoming the go-to method for describing events in videos and are really good at handling different types of video data (Shi et al., 2019; Iashin and Rahtu, 2020b). The thing is, current attention-based models only focus on the specific part of the video where the event happens and don’t pay much attention to the bigger picture—both the local and global context of the video. So, we’re looking into how to combine and use both local and global context effectively for better video captioning.",
        "formal_text": "Convert casual text to formal text: In the past, people mostly used recurrent neural networks (RNN) for this kind of work (like Krishna et al., 2017), but they had trouble dealing with long-term"
    },
    {
        "casual_text": "Since the LUNA corpus is pretty small, we didn't do any fancy parameter tuning and just went with the default or pre-set parameters. We tried out LUNA along with three different re-rankers that were made by mixing SVMs with STK, PTK, and SK, as mentioned in Section 4. The starting point for re-ranking was the top ten guesses given by an FST model.",
        "formal_text": "Convert casual text to formal text: Since the LUNA corpus is pretty small, we didn't do any fancy parameter tuning and just went with the default or pre-set parameters. We tried out LUNA along with three"
    },
    {
        "casual_text": "In this paper, we introduce a new reordering model for phrase-based statistical machine translation (we call it GREM). This model doesn’t just handle local and global phrase reordering—it also works with non-contiguous phrases to get some extra generalization. Our tests show that this new model is better than the baseline models, MEBTG and HPTM, by improving the BLEU score by 1.54% and 0.66%, respectively.",
        "formal_text": "Convert casual text to formal text: In this paper, we introduce a new reordering model for phrase-based statistical machine translation (we call it GREM). This model doesn’t just handle local and global phrase re"
    },
    {
        "casual_text": "The results for French to English translation aren't as good: neither cont nor para on their own beat the baseline for any metrics. But when we combine them, they do better than the baseline across all metrics except BLEU, with a drop of -1.07 in TER. Looking closer at the CLE results, it seems like adjectives and nouns got the most benefit from using our extra models. Verbs, which improved a bit, are super inflected in French, so finding examples for a specific form is trickier than for less inflected word types, and finding paraphrases with the right inflection is also harder. Also, translating through English as a pivot can lead to paraphrases that aren't always great, especially when the source language is less inflected. Plus, our simple context modeling might not have been good at filtering out some bad examples. Overall, para worked better when English was the source language, improving over the baseline across all metrics.",
        "formal_text": "Convert casual text to formal text: The results for French to English translation aren't as good: neither cont nor para on their own beat the baseline for any metrics. But when we combine them, they do better than the baseline across"
    },
    {
        "casual_text": "We tested the three generators on a total of 300 sentences. For GPT-2, we used the first two words of a sentence as the starting point (prompt), made sure the generated text was about the same length as the original, and collected 10 different versions. With RoBERTa and T5, we messed with the original sentence three times, adding up to three [MASK] tokens each time, and asked the generator to create 5 different versions using beam search, just like Ribeiro et al. (2020) did. Polyjuice works similarly to RoBERTa and T5 when it comes to adding the [MASK] tokens, but we also tried out all the control codes. For each sentence, we randomly picked 5 versions from each generator to create the final set.",
        "formal_text": "Convert casual text to formal text: We tested the three generators on a total of 300 sentences. For GPT-2, we used the first two words of a sentence as the starting point (prompt), made sure the"
    },
    {
        "casual_text": "The lexicon is super important—it’s where all the words we can work with live, along with their grammar and meaning stuff. For grammar, this includes things like X-bar features (iN, iV), tense, number, and so on.",
        "formal_text": "Convert casual text to formal text: The lexicon is super important—it’s where all words we we work with live, along with their grammar and meaning stuff. For grammar, this includes things like X-bar features ("
    },
    {
        "casual_text": "In the last section, we saw that Concat basically just checks if H is a typical hypernym, but it doesn’t actually figure out the relationship between H and w. Even though it doesn’t do that, it’s still super useful and should be used along with other models like Ksim and Asym to get better results. So, we came up with a new model that takes advantage of what Concat does well, makes it even better, and adds two other ideas from research: overall similarity and distributional inclusion.",
        "formal_text": "Convert casual text to formal text: In the last section, we saw that Concat basically just checks if H is a typical hypernym, but it doesn’t actually figure out the relationship between H and w."
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text: Input: Sentence embedding x, CSVs v, Dist = 5 Output: Predicted class  1. x = average_embedding(x) 2. For each vector v_i in v: 3. tmpDist = CosineDistance(v_i, x)",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version of the text: Input: Sentence embedding x, CSVs v, Dist = 5 Output: Predic"
    },
    {
        "casual_text": "Diversity in generating stuff has been a big deal in other areas of text generation, like chatbots, but we think it’s just as important for question generation in QA systems. In this paper, we show that it’s not only important but also something we can measure. We’re hoping our work will inspire more research into making question generation more diverse and figuring out how to evaluate it properly. Some cool ideas for future work could be diving deeper into different types of diversity in QG, like word choices or sticking to the facts, and finding ways to control and improve those specific areas in the generation process.",
        "formal_text": "Convert casual text to formal text: Diversity in generating stuff has been a big deal in other areas of text generation, like chatbots, but we think it’s just as important for question generation in QA systems. In"
    },
    {
        "casual_text": "An Indexed Grammar (IG) is kinda like a Context-Free Grammar (CFG), but with a twist: each nonterminal has a stack of indices attached to it. The rules, or productions, don't just say how nonterminals can change; they also tell you how those stacks of indices get modified. These 1_16 things, which were introduced by Gazdar back in 1988, have a specific rule: the stacks are passed from the \"mother\" nonterminal to only one of its \"daughters.\"",
        "formal_text": "Convert casual text to formal text: An Indexed Grammar (IG) is kinda like a Context-Free Grammar (CFG), but with a twist: each nonterminal has a stack of indices"
    },
    {
        "casual_text": "In TriCRF, the idea is to look at the input sequence x = (x1, ..., xT), the slot sequence y = (y1, ..., yT), and the intent z, and describe how they all work together.",
        "formal_text": "Convert casual text to formal text: In TriCRF, the idea is to look at the input sequence x = (x1, ..., xT), the slot sequence y = (y1, ..., y"
    },
    {
        "casual_text": "So, we've been looking at recent studies on NER and trying to figure out how to improve the recognition of OOV (out-of-vocabulary) entities. In this project, we came up with a cool and flexible learning framework called MINER. It tackles the OOV recognition problem by looking at it from an information-theoretic angle. On one hand, this method boosts the context information from the encoder's output. On the other hand, it helps get rid of unnecessary stuff that doesn't matter for the task and stops the model from just memorizing entities. The approach has two main training goals based on mutual information: one is about maximizing useful information, and the other is about minimizing extra, unnecessary information. When we tested it on different datasets, MINER performed way better at predicting OOV entities compared to other methods.",
        "formal_text": "Convert casual text to formal text: So, we've been looking at recent studies on NER and trying to figure out how to improve the recognition of OOV (out-of-vocabulary) entities. In this project"
    },
    {
        "casual_text": "We used a classifier based on a probabilistic version of the pair-wise algorithm (Soon et al., 2001). We focused only on resolving third person singular pronouns because that's where pronoun resolution is most common in psycholinguistics. Third person pronouns are different from first and second person ones since the latter are deictic, not anaphorical.",
        "formal_text": "Convert casual text to formal text: We used a classifier based on a probabilistic version of the pair-wise algorithm (Soon et al., 2001). We focused only on resolving third person"
    },
    {
        "casual_text": "To get really high accuracy, you need a lot of languages working together, which isn't something you can achieve with just one language. Out of the four ways we tested to combine multiple languages, our two multi-view AdaBoost methods, called MVAB1 and MVAB2, did way better than the other two simpler methods, MLS and MVMV. MVAB2, which uses a more advanced training method, even outperformed MVAB1. For example, with SVM-based classifiers, when we used 3000 training samples: - MVAB1 boosted accuracy by 4.19% compared to MLS and 2.47% compared to MVMV. - MVAB2 did even better, boosting accuracy by 4.50% compared to MLS and 2.75% compared to MVMV. When comparing to single-language methods, the improvements were even bigger: - MVAB1 increased accuracy by 8.91% over the best single-language method (English). - MVAB2 increased it by 9.19%. For NB-based classifiers, our methods worked best with smaller training sizes (like 500 samples): - MVAB1 improved accuracy by 6.33% over the best single-language method (English), 3.74% over MLS, and 2.86% over MVMV. - MVAB2 did even better, improving accuracy by 7.56% over the best single-language method (English), 4.98% over MLS, and 4.10% over MVMV. So, overall, our multi-view AdaBoost methods, especially MVAB2, really shine when it comes to boosting accuracy, especially when compared to single-language methods or simpler multi-view methods.",
        "formal_text": "Convert casual text to formal text: To get really high accuracy, you need a lot of languages working together, which isn't something you can achieve with just one language. Out of the four ways we tested to combine multiple languages"
    },
    {
        "casual_text": "For every sentence s_i and its corresponding dependency tree t_i, there’s a hidden variable y_i that tells us which formula generated the sentence. We can update y_i by using Gibbs sampling. In the non-parametric approach, y_i can be any non-negative integer. The Chinese Restaurant Process helps us handle this by dealing with an infinite number of possible formulas. It focuses on the formulas that are actually being used (where at least one sentence is assigned to them) and gives them a prior based on their counts. There’s also a pseudo-count for all the formulas that aren’t being used. If we have K formulas, the chance of picking formula j is determined by this prior.",
        "formal_text": "Convert casual text to formal text: For every sentence s_i and its corresponding dependency tree t_i, there’s a hidden variable y_i that tells us which formula generated the sentence. We can"
    },
    {
        "casual_text": "The first part of the score looks at how the current tag at time *t* changes from the previous tag. So, we call this the *transition score*. The second part focuses on how the current tag *y t* is influenced by the word sequence *x T 1*, and we can call this the *emission score*. Basically, this helps us figure out the probability of a sequence of tags.",
        "formal_text": "Convert casual text to formal text: The first part of the score looks at how the current tag at time *t* changes from the previous tag. So, we call this the *transition score*. The second part focuses on"
    },
    {
        "casual_text": "After that, the main task is to figure out the parameter  that helps us estimate the Q-values using example dialogue paths. But here’s the catch: the standard setup in Eq. 2 has a big issue. The feature function  is designed to cover the entire state and action spaces. So, if we add a new domain expert to the system, both the state space and action space will change. That means we’d have to completely redo the feature function and retrain the whole model. To avoid this hassle and make the system more flexible, we’ve made some tweaks and broken down the feature function like this. First, we start by:",
        "formal_text": "Convert casual text to formal text: After that, the main task is to figure out the parameter  that helps us estimate the Q-values using example dialogue paths. But here’s the catch: the standard setup in Eq"
    },
    {
        "casual_text": "To tackle these problems, a lot of research has looked into how we can automatically figure out lexical entailments using distributional semantics. Some studies mainly use unsupervised methods and focus on specific measures that highlight certain word relationships (Baroni and Lenci, 2011). A lot of this work is based on the Distributional Inclusion Hypothesis, which says that the contexts where a hypernym (like \"animal\") shows up are a superset of the contexts for its hyponyms (like \"cat\") (Zhitomirsky-Geffet and Dagan, 2005; Kotlerman et al., 2010). More recently, there's been a big push toward using supervised methods (Baroni et al., 2012; Roller et al., 2014; Weeds et al., 2014; Kruszewski et al., 2015). These methods differ in how they're set up or the models they propose. But there's no clear consensus in the literature about which models are the best (Weeds et al., 2014; Roller et al., 2014), or even if they work well at all. For example, some research showed that two existing models for lexical entailment don't really capture the similarity between the words they're comparing. Instead, they seem to just predict based on what's typical: they'll say \"cat entails animal\" because \"animal\" is usually the broader category, and they'll also predict \"sofa entails animal\" for the same reason. But it's still not clear why these models are considered so strong as baselines (Weeds et al., 2014; Kruszewski et al., 2015).",
        "formal_text": "Convert casual text to formal text: To tackle these problems, a lot of research has looked into how we can automatically figure out lexical entailments using distributional semantics. Some studies mainly use unsupervised methods and"
    },
    {
        "casual_text": "Since we can't predict in advance which spans will be part of a discourse tree, we'll assign each possible span a status, type, promotion, and primary intention relation. Then, we'll let discourse and intentional rules figure out which trees are valid. Basically, we're trying to find the subset of spans from a larger set that fits certain rhetorical and intentional rules. For instance, in Text 1, there are 10 potential spans: span 1;1, span 2;2, span 3;3, span 4;4, span 1;2, span 2;3, span 3;4, span 1;3, span 2;4, and span 1;4. But only seven of these are actually used in the tree representation shown in Figure 1.a: span 1;1, span 2;2, span 3;3, span 4;4, span 1;2, span 3;4, and span 1;4. Each node in the tree has these features—status, type, promotion, and primary intention—unless it's marked as NONE. The numbers on each node show the text span they represent.",
        "formal_text": "Convert casual text to formal text: Since we can't predict in advance which spans will be part of a discourse tree, we'll assign each possible span a status, type, promotion, and primary intention relation. The"
    },
    {
        "casual_text": "We found that using both label smoothing and vocabulary sharing in neural machine translation (NMT) can mess things up and lead to worse performance. To fix this, we came up with Masked Label Smoothing (MLS), which tweaks the smoothed probabilities based on the differences between languages. It’s straightforward but works really well. MLS not only improves translation quality but also helps the model make better predictions across a bunch of tasks compared to the original label smoothing.",
        "formal_text": "Convert casual text to formal text: We found that using both label smoothing and vocabulary sharing in neural machine translation (NMT) can mess things up and lead to worse performance. To fix this, we came up with Masked Label Smooth"
    },
    {
        "casual_text": "Alright, let's dive into some more examples of what START can do. We picked these conversations to show off how S-rules work and to give you a sense of what START can handle. Along the way, we'll also introduce some S-rules that deal with a few more tricky language patterns, like those indefinite objects and reflexive stuff we talked about earlier in the paper.",
        "formal_text": "Convert casual text to formal text: Alright, let's dive into some more examples of what START can do. We picked these conversations to show off how S-rules work and to give you a sense of what"
    },
    {
        "casual_text": "At first glance, IRT and logistic regression might seem pretty similar, but we wanted to compare them to answer some common questions from folks in NLP: (1) Does the way IRT is set up mess with how accurate it is? (2) Should we throw in some extra features to get a better handle on what’s going on with the questions?",
        "formal_text": "Convert casual text to formal text: At first glance, IRT and logistic regression might seem pretty similar, but we wanted to compare them to answer some common questions from folks in NLP: (1) Does the way IRT is set up mess"
    },
    {
        "casual_text": "Since regular action-level search has worked well for discriminative neural parsers (like Vinyals et al., 2015, and Dyer et al., 2016), it seems like a good place to start for decoding in generative models. But here's the issue: even with a big beam size, generative decoding has some weird quirks that make it hard to find decent parses. No matter what actions have been taken so far, the generative model tends to give way higher probabilities to OPEN and CLOSE actions than to SHIFT actions, which is shown in Figure 2. So, instead of shifting to the next word (which has a lower probability), the model keeps opening new constituents until it hits a limit. This leads to a sequence that has a much lower overall probability compared to a reasonable parse. But because the model only compares structural and lexical actions without looking at the bigger picture, it doesn't keep good candidates in the beam. As a result, action-level beam search with a beam size of 1000 only gets an F1 score of 52.97 on the development set, as shown in Table 1.",
        "formal_text": "Convert casual text to formal text: Since regular action-level search has worked well for discriminative neural parsers (like Vinyals et al., 2015, and Dyer et al., 2016), it"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. First off, \"Positive\" and \"Negative\" are about whether words or phrases have a good or bad vibe to them. Words that don’t lean either way are called \"neutral.\" We started by grabbing nouns, adjectives, and verbs from the Sejong dictionary that are tied to emotions or evaluations, like \"Positive Property Human\" or \"Negative Property Human.\" Then, we looked at other review sources to find words people commonly use to express feelings and added those to our list. Since we're dealing with stuff from the internet, we also threw in abbreviations, new slang, and trendy words that folks use online to share their opinions.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. First off, \"Positive\" and \"Negative\" are about whether words or phrases have a"
    },
    {
        "casual_text": "The simplest generative model out there, which assumes that documents are about specific topics, is the standard Nave Bayes model. In this model, each document is considered to be about just one topic, and each topic has a set of word probabilities tied to it (Mitchell, 1997).",
        "formal_text": "Convert casual text to formal text: The simplest generative model out there, which assumes that documents are about specific topics, is the standard Nave Bayes model. In this model, each document is considered to be about"
    },
    {
        "casual_text": "MT papers mostly looked at how big the differences were in the scores to decide if they were significant or not. Dror and his team noticed the same thing back in 2018 when they were studying NLP in general.",
        "formal_text": "Convert casual text to formal text: MT papers mostly looked at how big the differences were in the scores to decide if they were significant or not. Dror and his team noticed the same thing back in 2018 when they were studying N"
    },
    {
        "casual_text": "Alright, so imagine you have a tree , and there's a node labeled \"e\" at a specific spot (let's call it address p). Now, if you've got another tree  with a root labeled \"f\" and the size of \"f\" is the same as \"e\", you can swap out that node \"e\" with the whole tree . The new tree you end up with is basically the original tree but with that replacement.",
        "formal_text": "Convert casual text to formal text: Alright, so imagine you have a tree , and there's a node labeled \"e\" at a specific spot (let's call it address p"
    },
    {
        "casual_text": "The link to summarization isn’t super clear, so we’re using a rough version of the model by Wang and Ling (2016) as a starting point in our experiments. Summarization research is a huge field, and we’re not diving into all of it here. If you want a deeper look, check out Gambhir and Gupta’s survey from 2017. Recently, we’ve been focusing on pulling out the main argument in just two sentences to use as a snippet for argument searches (Alshomary et al., 2020). Meanwhile, Egan et al. (2016) have been working on abstractive summaries that capture the key points of debates. We’re thinking there might be a connection between the conclusion’s stance and its premises. On a broad level, this is kind of similar to what Angelidis and Lapata (2018) did, where they modeled aspects and sentiments for summarizing opinions.",
        "formal_text": "Convert casual text to formal text: The link to summarization isn’t super clear, so we’re using a rough version of the model by Wang and Ling (2016) as a starting point in our experiments. Sum"
    },
    {
        "casual_text": "4. How well the scores and their meanings can be applied to different groups of people, situations, and tasks.",
        "formal_text": "Convert casual text to formal text: 4. How well the scores and their meanings are applied to different groups of people, situations, and tasks. 4. How well the scores and their meanings are applied to different groups of people, situations,"
    },
    {
        "casual_text": "(3) It’d be way too expensive to make feedback comments in every single native language, so we just picked one we could understand. To make it easier for future research, we also translated them into English ourselves. For this project, though, we only used the Japanese feedback comments.",
        "formal_text": "Convert casual text to formal text: (3) It’d be way too expensive to make feedback comments in every single native language, so we just picked one we could understand. To make it easier for future research, we also translated them into English"
    },
    {
        "casual_text": "We did a big study on few-shot crosslingual transfer, looking at how models perform in these situations. The main thing we noticed was that the models are really picky about the few examples they get. We dug into why this happens across six different tasks and up to 40 languages. Turns out, big language models tend to get too focused on just a few examples and mostly use basic word stuff from those examples, even though they’ve been trained on tons of English data. Also, we found that fancy few-shot learning methods in computer vision aren’t better than just doing a full model finetuning, which is actually pretty straightforward.",
        "formal_text": "Convert casual text to formal text: We did a big study on few-shot crosslingual transfer, looking at how models perform in these situations. The main thing we noticed was that the models are really picky about the few examples they"
    },
    {
        "casual_text": "With these changes, the matrix operations will happen on tensors that are much smaller in size. This means we'll use less memory at its peak, the operations will run quicker, and we can handle a bigger batch size. For instance, at step t, the sizes of Cache K t1 and",
        "formal_text": "Convert casual text to formal text: With these changes, the matrix operations will happen on tensors that are much smaller in size. This means we'll use less memory at its peak, the operations will run quicker, and"
    },
    {
        "casual_text": "We went through 3,591 sentences from 105 news editorials and collected 17,414 annotations at the sentence level. On average, each news editorial had 4.85 annotators. We originally planned to have five annotators per editorial, but 3% of the assignments were rejected. For the annotation reliability scores, the averages for \"good squared,\" \"good,\" \"moderate,\" and \"bad\" annotations were 0, 0.05, 0.48, and 0.84, respectively. The number of annotations in each category was 1,741 (10%), 3,024 (17%), 6,304 (36%), and 8,086 (46%), respectively.",
        "formal_text": "Convert casual text to formal text: We went through 3,591 sentences from 105 news editorials and collected 17,414 annotations at the sentence level. On average, each news editorial had 4.85 annotators. We originally planned to"
    },
    {
        "casual_text": "We used data from an e-commerce website to train our models. We gathered 8,001,577 pairs of items, and 33% of them were bought together (BIN event) during the same session. The rest were randomly picked as negative examples. The item-item interaction matrix is super sparse—99.9999% of it is empty. This sparsity makes the model focus on generalizing instead of just memorizing stuff. We'll explain this more with some stats from our dataset. We also set aside 250,799 pairs of items in the same way for a validation set, which we used to do early stopping during training. For testing, we wanted to simulate a cold-start situation where traditional item-item collaborative filtering doesn’t work. So, we picked 10,000 pairs of co-purchased items, but the seed item wasn’t in the training set. For each positive sample (a seed item and its actual co-purchased item), we paired the seed item with 999 random negative samples. During testing, we used the trained model to rank all 1000 items for each seed item.",
        "formal_text": "Convert casual text to formal text: We used data from an e-commerce website to train our models. We gathered 8,001,577 pairs of items, and 33% of them were bought together (BIN event) during"
    },
    {
        "casual_text": "To tackle these problems, we’ve come up with an unsupervised, reference-free metric called CTRLEval for assessing controlled text generation models. This metric evaluates models from different angles without needing any task-specific training data. Basically, we turn the evaluation of each aspect into a \"fill-in-the-blank\" type of task, where the input and output patterns are designed based on what the aspect is all about. Next, we use a pretrained model that’s good at text infilling (like PEGASUS, which Zhang et al. introduced in 2020) as our main tool. We combine the generation probabilities from these \"fill-in-the-blank\" tasks to get our evaluation results. To avoid any bias that might come from how we design these tasks (as pointed out by Zhao et al. in 2021), we create multiple text infilling tasks for each aspect and average all the results, with some weights, to get the final score. In this paper, we focus on three main aspects that are usually used to judge controlled text generation models: coherence (how well the text flows, as discussed by Yuan et al. in 2021), consistency (how logically sound the text is, according to Rashkin et al. in 2020), and attribute relevance (how well the text matches the control variables, as explained by Dathathri et al. in 2020). These aspects cover both the quality of the generated text and how well it ties in with different control variables, giving us a well-rounded evaluation of controlled text generation.",
        "formal_text": "Convert casual text to formal text: To tackle these problems, we’ve come up with an unsupervised, reference-free metric called CTRLEval for assessing controlled text generation models. This metric evaluates models from different"
    },
    {
        "casual_text": "From what I can see, STATEMENT is a big reason for mix-ups in the data because it's the most common type. A lot of RE-QUEST and RESPONSE ACK examples got labeled as STATEMENT by mistake. We didn't use stuff like question marks in our features, but adding that could probably make things better.",
        "formal_text": "Convert casual text to formal text: From what I can see, STATEMENT is a big reason for mix-ups in the data because it's the most common type. A lot of RE-QUEST and RESP"
    },
    {
        "casual_text": "Adverbs in English are super flexible. They can fit into just about any spot in a sentence, as long as they stay close to the verb. Fowler talks about this in his section on adverb placement and gives some wild examples, including cases where he suggests splitting infinitives.",
        "formal_text": "Convert casual text to formal text: Adverbs in English are super flexible. They can fit into just about any spot in a sentence, as long as they stay close to the verb. Fowler talks about this in his section"
    },
    {
        "casual_text": "Our analysis found that the model-theoretic vectors we came up with do a pretty good job of representing the concepts we’re looking at, even for features that don’t have any labeled examples. This is a really good sign for our method and opens the door to some cool next steps: like using active learning to expand our training data with non-zero dimensions. We also tried turning those vector representations into natural language sentences with quantifiers, and got a 73% accuracy rate for making sentences that were actually true.",
        "formal_text": "Convert casual text to formal text: Our analysis found that the model-theoretic vectors we came up with do a pretty good job of representing the concepts we’re looking at, even for features that don’t have any"
    },
    {
        "casual_text": "Alright, let's break down the issue of how the ideas of success from earlier can be used when figuring out what specific words, especially names, mean in conversations. We'll use Ta, Tb, and Tcom to represent what person A is referring to, what person B is referring to, and what the actual meaning is, respectively. So, to sum it up:",
        "formal_text": "Convert casual text to formal text: Alright, let's break down the issue of how the ideas of success from earlier can be used when figuring out what specific words, especially names, mean in conversations. We'll use Ta"
    },
    {
        "casual_text": "Alright, let's dive into the German-English, Spanish-English, and French-English tasks. These are the same pairs used in the workshop we mentioned earlier. We actually ran our systems on both directions of translation, but for this paper, we're only going to talk about the experiments where English is the source language. We kept it short because the results were pretty much the same when we flipped the languages around. The data is split into three parts: one for training, one for development, and one for testing. You can check out the stats in Table 1.",
        "formal_text": "Convert casual text to formal text: Alright, let's dive into the German-English, Spanish-English, and French-English tasks. These are the same pairs used in the workshop we mentioned earlier. We actually ran our systems"
    },
    {
        "casual_text": "Alright, let’s dive into how we compare different interpretations. But first, let’s quickly talk about the kind of data we’re working with. We’re using a function called f, which gives us the relative frequency of any category in a hierarchy. This frequency is adjusted so that for any category, f(cat) equals the probability of that category being chosen randomly from all possible categories. For semantic categories, this helps us figure out how typical something is. For syntactic categories and structures, it helps us understand how common or \"fixed\" certain patterns are in language. Since f is based on how often things show up, we can teach it by looking at examples. All we need is a bunch of correct interpretations, and then we just count how many times each category appears (and adjust the numbers to make sense). Now, the way we measure how good an interpretation is works like this: we look at the probability of the whole structure based on the words we’re given. For example, we might ask, \"What’s the chance of this construction happening with the words 'afternoon' and 'rest'?\" That’s what P[+c: )l + s1, +82] = P [ NN-constrl(ig) l \"afternoon\"(ix) \"rest\"(i2)] is getting at.",
        "formal_text": "Convert casual text to formal text: Alright, let’s dive into how we compare different interpretations. But first, let’s quickly talk about the kind of data we’re working with. We’re using a function"
    },
    {
        "casual_text": "Deep learning models for picking the right answer usually think that questions and answers should be really similar (Yu et al., 2014). But instead of just comparing them directly, our model has a different idea: we use an attribute as a kind of middleman to help match the question with the answer. Take the question \"Who replaced Dwight D. Eisenhower?\" and the answer \"Succeeded by: John F. Kennedy\". Here, the attribute \"Succeeded by\" is super important because it shows how the answer fits the question. If the question and the attribute share some meaning, and that meaning also matches what the answer and the attribute have in common, then the answer is probably a good fit for the question.",
        "formal_text": "Convert casual text to formal text: Deep learning models for picking the right answer usually think that questions and answers should be really similar (Yu et al., 2014). But instead of just comparing them directly, our model has"
    },
    {
        "casual_text": "To create solid starting points, we first train our models on general-domain data. For this, we use the OpenSubtitles18 (Lison et al., 2018) dataset for both Russian to English (RuEn) and Korean to English (KoEn). For RuEn and Chinese to English (ZhEn), we also include the parallel data from the WMT17 news translation task (Bojar et al., 2017). After that, we tweak these general-domain models using WIPO training data (Luong and Manning, 2015), with the dev set helping us check how well we're doing. These tweaked models are what we use as our base systems for testing how well we can integrate lexicons.",
        "formal_text": "Convert casual text to formal text: To create solid starting points, we first train our models on general-domain data. For this, we use the OpenSubtitles18 (Lison et al., 2018) dataset"
    },
    {
        "casual_text": "Looking at this big collection of English texts, we've managed to figure out a pretty detailed grammar for this specific type of English (check out Harris 1968 and Kittredge and Lehrberger 1982 for more info).",
        "formal_text": "Convert casual text to formal text: Looking at this big collection of English texts, we've managed to figure out a pretty detailed grammar for this specific type of English (check out Harris 1968 and Kittredge and Lehrberger 1982"
    },
    {
        "casual_text": "• The directions are indicated by a term called \"d,\" which is set to -1. If we keep d at -1, the input will be processed in the forward direction. If we change it, the input will be processed in the backward direction.",
        "formal_text": "Convert casual text to formal text: • The directions are indicated by a term called \"d,\" which is set to -1. If we keep d at -1, the input will be processed in the forward direction. If we"
    },
    {
        "casual_text": "RT @GrahamWP_UK: #MH17 1. Some guy chilling on his couch, watching YouTube, reckons it was a \"separatist BUK.\" 2. A dude who's been on the scene for over 25 hours doesn't think so. Table 4: Examples of different types of errors. - Error Category I: These are cases where you can easily figure out the right answer just by reading the text. - Error Category II: Here, you need some specific knowledge about the event to get the correct class from the text. - Error Category III: For these, you gotta figure out if it's a joke or satire to understand what the speaker really means.",
        "formal_text": "Convert casual text to formal text: RT @GrahamWP_UK: #MH17 1. Some guy chilling on his couch, watching YouTube, reckons it was a \"separatist BUK.\" 2."
    },
    {
        "casual_text": "A lot of the research out there, with just a few exceptions, has been about translating between Indo-European languages. But Japanese is pretty different from those languages—like in its vocabulary, sentence structure, and meaning—so it’s not really the same thing.",
        "formal_text": "Convert casual text to formal text: A lot of the research out there, with just a few exceptions, has been about translating between Indo-European languages. But Japanese is pretty different from those languages—like in its vocabulary,"
    },
    {
        "casual_text": "Okay, let’s break this down in a simpler way. There are three main types of compound words: 1. **Lexicalized (or conventional)**, like \"clock radio\" — these are common and well-known. 2. **Identificative**, like \"clock gears\" — these describe something that’s already known but doesn’t have a specific name yet. 3. **Creative**, like \"clock table\" — these require you to come up with a whole new meaning. Both identificative and creative compounds are kind of new in the sense that they’re not super common. The difference is that identificative ones point to something familiar but unnamed, while creative ones make you invent a new idea. When we’re trying to figure out what a compound means, we usually go for the most specific, already-existing category that fits. If there’s a conventional way to understand it, we use that first. If not, we look for an identificative meaning, and if that doesn’t work, we go with the creative option. But this \"Maximal Conventionality Principle\" isn’t set in stone. Sometimes, the bigger picture — like the surrounding sentence or context — can change things and make us pick an identificative or even a creative compound over a conventional one. In Figure 1, you’ll see examples where two conventional compounds are competing, but the context makes us choose the identificative one instead. This kind of decision requires looking at both the syntax (how words are put together) and the meaning (semantics) in a detailed way. Simple methods (like the ones from Marcus, Hirst, or Lytinen) don’t do a good enough job of deciding between the options, so we need a more precise approach.",
        "formal_text": "Convert casual text to formal text: Okay, let’s break this down in a simpler way. There are three main types of compound words: 1. **Lexicalized (or conventional)**, like \"clock radio\""
    },
    {
        "casual_text": "Statistical analysis helps make sense of a lot more data than just following strict rules, and it also gives you more reliable results that you can track and understand. So, if you want a translation engine that's almost as good as human-made translations, it needs to use some kind of statistics at its core. For instance, it can use these stats to figure out the actual context of a text, which helps in picking the right words. Basically, the SMT (Statistical Machine Translation) approach turned text translation into a whole engineering thing. It uses a software setup that lets you keep making improvements over time. The system is designed to let you easily swap out old algorithms for better ones as they come along, and the algorithms themselves will keep getting better as performance improves.",
        "formal_text": "Convert casual text to formal text: Statistical analysis helps make sense of a lot more data than just following strict rules, and it also gives you more reliable results that you can track and understand. So, if you want a"
    },
    {
        "casual_text": "In (3), we ditch the pre-trained word embeddings from the nodes and start fresh with random vectors for all of them. When we compare this to the original model, the results are a bit worse without those pre-trained embeddings. So, we think that those pre-trained word embeddings really help boost the model's performance.",
        "formal_text": "Convert casual text to formal text: In (3), we ditch the pre-trained word embeddings from the nodes and start fresh with random vectors for all of them. When we compare this to the original model, the"
    },
    {
        "casual_text": "So far, most research has been about summarizing stuff from one person, like news articles (for example, Nallapati et al. (2016)) or scientific papers (like Nikolov et al. (2018)). One big reason for this is that there are tons of high-quality news datasets with summaries already tagged, like the CNN/Daily Mail dataset (Hermann et al., 2015; Nallapati et al., 2016). But when it comes to dialogues, there isn’t a similar big, well-labeled dataset available.",
        "formal_text": "Convert casual text to formal text: So far, most research has been about summarizing stuff from one person, like news articles (for example, Nallapati et al. (2016)) or scientific papers (like Niko"
    },
    {
        "casual_text": "We want our questions to focus on specific types of connections between ideas, called \"relation senses.\" To make sure we cover a wide range of these connections, we created a bunch of question templates based on the PDTB 3.0 (a big database of discourse relations, created by Webber et al. in 2019 and Prasad et al. in 2008). You can check out these templates in Table 3. Each question starts with a specific prefix that tells you what kind of relation sense it’s asking about. The placeholder X is filled in to point to the part of the text the question is about, like in Table 1. Not all PDTB senses are covered by our prefixes, though. For example, senses that have extra details about beliefs or speech acts (like \"Belief\" or \"SpeechAct\") were grouped under a more general sense. Also, we left out three \"Expansion\" senses because they don’t usually introduce new information that you could ask a question about. Instead, they focus on how the text is structured. One of these is \"Expansion. Conjunction,\" which is super common in the PDTB, especially in phrases where verbs are connected within a sentence. It makes up around 70% of the instances of this sense. For example, in Ex. (2), there’s a discourse relation with two senses, one of which is \"Expansion. Conjunction.\" While it’s easy to think of a question about a causal sense, the conjunction part doesn’t really introduce any new information that you’d ask a question about.",
        "formal_text": "Convert casual text to formal text: We want our questions to focus on specific types of connections between ideas, called \"relation senses.\" To make sure we cover a wide range of these connections, we created a bunch of question"
    },
    {
        "casual_text": "With machine learning becoming a big deal in computational linguistics and doing really well in lots of areas, there's a growing need for lots of human-labeled data to train and test these algorithms. In the area of figuring out time-related stuff, this has resulted in the creation of TimeBank (Pustejovsky et al., 2003), which uses the TimeML language for labeling (Pustejovsky et al., 2005). TimeML is on its way to becoming an ISO standard for marking up events and time expressions (ISO/TC 37/SC 4/WG 2, 2007). TimeBank was also made available for TempEval-2007, the first competition focused on automatically figuring out the relationships and order between events and time expressions.",
        "formal_text": "Convert casual text to formal text: With machine learning becoming a big deal in computational linguistics and doing really well in lots of areas, there's a growing need for lots of human-labeled data to train and"
    },
    {
        "casual_text": "Keep in mind that using UMLS CUIs for our new benchmark dataset isn’t just helpful for comparing it to other datasets—it also lets us use CUI embedding models (like the ones by Henry et al., 2019; Park et al., 2019; and Henry et al., 2018) to predict how related different concepts are.",
        "formal_text": "Convert casual text to formal text: Keep in mind that using UMLS CUIs for our new benchmark dataset isn’t just helpful for comparing it other datasets—it also lets us use CUI embedding models ("
    },
    {
        "casual_text": "We basically use the same approach as Beltagy et al. (2019) for both pretraining and fine-tuning S2ORC-SCIBERT. Just like SCIBERT, S2ORC-SCIBERT is trained from scratch using the original BERT code and the default BERT-Base settings on a single TPU v3-8 for about a week. Similarly, S2ORC-SCIBERT is fine-tuned on all tasks by minimizing cross entropy loss with Adam (Kingma and Ba, 2014), a linear learning rate decay, a 10% warm-up period, a batch size of 32, and a dropout rate of 0.1.",
        "formal_text": "Convert casual text to formal text: We basically use the same approach as Beltagy et al. (2019) for both pretraining and fine-tuning S2ORC-SCIBERT. Just like S"
    },
    {
        "casual_text": "In this situation, someone tweaks a pre-trained language model (LM) to create fake text. Meanwhile, another person trains a classifier to figure out which original pre-trained LM was used to make that fake text. The twist here is that the classifier-trainer doesn’t know about the changes made to the LM before the fake text was generated. The main point is that the classifier is trying to identify the original pre-trained LM, not the tweaked one that actually made the fake text. To break it down: Imagine you have n pre-trained LMs called PM1, PM2, ..., PMn. One of these, PMK, gets fine-tuned to create a new LM called FMk (where 1  k  n). The goal is to train a classifier that can correctly identify which of the original pre-trained LMs (PM1, PM2, etc.) was used, even when the text comes from the fine-tuned FMk. So, the person tweaking the LM generates text using FMk, and the classifier’s job is to guess that the text came from the original PMk.",
        "formal_text": "Convert casual text to formal text: In this situation, someone tweaks a pre-trained language model (LM) to create fake text. Meanwhile, another person trains a classifier to figure out which original pre-trained"
    },
    {
        "casual_text": "Basically, segmentation accuracy is calculated by dividing the number of correctly guessed segment boundaries by the total number of actual segment boundaries. When both precision and recall are high, it means the segmentation is pretty accurate. But there are a few issues with these measures. First, you can have high precision but low recall, or vice versa. It's a trade-off. Second, precision and recall treat every wrong guess the same, whether the guessed boundary is close to the real one or way off. So, they don't really account for how far off the guess is.",
        "formal_text": "Convert casual text to formal text: Basically, segmentation accuracy is calculated by dividing the number of correctly guessed segment boundaries by the total number of actual segment boundaries. When both precision and recall are high, it means the segmentation"
    },
    {
        "casual_text": "The EMEA dataset is made up of texts from the European Medicines Agency. It's part of the OPUS parallel corpus, which was put together by Tiedemann and Nygaard back in 2004. This specific collection has 751,602 sentence pairs in both Dutch and English.",
        "formal_text": "Convert casual text to formal text: The EMEA dataset is made up of texts from the European Medicines Agency. It's part of the OPUS parallel corpus, which was put together by Tiedemann and Ny"
    },
    {
        "casual_text": "The phrase-break detector decides where to put breaks based on these 6 POS tags and how long the phrases are.",
        "formal_text": "Convert casual text to formal text: The phrase-break detector decides where to put breaks based on these 6 POS tags and how long the phrases are. Convert casual text to formal text: The phrase-break detector decides"
    },
    {
        "casual_text": "Cerco un posto dove mangiare hamburger vicino a \"Stagno Bosco\".",
        "formal_text": "Convert casual text to formal text: Cerco un posto dove mangiare hamburger vicino a \"Stagno Bosco\"."
    },
    {
        "casual_text": "Alright, so when it comes to pulling out information about cause and effect from text, there are two main types of tasks: causal phrase extraction and causal clause extraction. The first one is all about grabbing specific words or short phrases that show a cause-effect relationship.",
        "formal_text": "Convert casual text to formal text: Alright, so when it comes to pulling out information about cause and effect from text, there are two main types of tasks: causal phrase extraction and causal clause extraction. The first one is about grabbing"
    },
    {
        "casual_text": "We've got this new feature extraction operator called IF W (s) that we're using to create a fresh representation of our training data. Using this new representation, we train a new weight vector, W D. This weight vector sets up a new objective function for the optimization problem we're dealing with in Equation 2. Here, W D is the weight vector, and AF (s) is the representation. We'll refer to the solution of this optimization problem for a specific instance s as IF W D (s).",
        "formal_text": "Convert casual text to formal text: We've got this new feature extraction operator called IF W (s) that we're using to create a fresh representation of our training data. Using this new representation, we train"
    },
    {
        "casual_text": "We're suggesting a method to create DSE tasks using PubMed, where we use combinations of MeSH terms as labels. This will help set up a big, new benchmark for testing two things: (1) PU learning methods and (2) DSE methods. Right now, both of these areas don't have enough tough, large-scale tests to work with.",
        "formal_text": "Convert casual text to formal text: We're suggesting a method to create DSE tasks using PubMed, where we use combinations of MeSH terms as labels. This will help set up a big, new benchmark for testing two"
    },
    {
        "casual_text": "This method showed some pretty cool results for unsupervised constituency parsing, according to Drozdov et al. (2019a).",
        "formal_text": "Convert casual text to formal text: This method showed some pretty cool results for unsupervised constituency parsing, according to Drozdov et al (2019a). Convert casual text to formal text: This method showed some"
    },
    {
        "casual_text": "Alright, let's talk about how to check if something can be done again, starting with results we already have, not from scratch. Here's how you do it:",
        "formal_text": "Convert casual text to formal text: Alright, let's talk about how check if something can be done again, starting with results we already have, not from scratch. Here's you do it: Here's you do"
    },
    {
        "casual_text": "In a text box, remove the option text. For \"Target & Golden,\" delete the selected text in a text field. Without WWM, delete the selected text in a text field. With WWM, delete the selected text in a text field, using different masking strategies. In this case, the model without WWM misses the wrong target word \"ausgewählten,\" which is made up of two subwords: \"ausgewählt\" and \"##en.\" But the model with WWM catches this mistake. This shows that WWM helps with figuring out the translation quality for words that are split into multiple subwords.",
        "formal_text": "Convert casual text to formal text: In a text box, remove the option text. For \"Target & Golden,\" delete the selected text in a text field. Without WWM, delete the selected text in a"
    },
    {
        "casual_text": "This year, we’ve got a few more EDS and PTG graphs compared to PSD back in 2019. That’s because we’re not intersecting the two main resources anymore. Also, for UCCA, the 2020 release has some extra, up-to-date gold-standard annotations.",
        "formal_text": "Convert casual text to formal text: This year, we’ve got a few more EDS and PTG graphs compared to PSD back in 2019. That’s because we’re not intersecting the two main resources anymore."
    },
    {
        "casual_text": "This project is all about creating trees made up of topical segments. Each segment has a center that best represents its content. The main goal is to maximize net similarity, which is the total similarity between all centers and the data points they represent. The entire sequence of data points gets divided at each level of the tree, but with one rule: at each level (l), the centers must be part of the centers from the previous level (l-1). Figure 1a shows a part of the factor graph that explains how HAPS works for levels l and l-1. The tree has L levels, starting from the top (l = L) down to the bottom (l = 1). The little numbers on the factor and variable nodes show which level they're on. At every level, there are N2 variable nodes called cijj and N variable nodes called ej (N is the number of data points in the sequence). Each variable is either 0 or 1: cijj = 1 means data point i at level l is part of the segment centered around data point j; ej = 1 means there's a segment centered around j at level l. There are four types of factor nodes in Figure 1a: I, E, C, and S. The I factors make sure each data point is in exactly one segment and that the centers at level l are part of those from level l-1. The E nodes ensure that segments are grouped neatly around their centers in solid blocks, not scattered all over. The values for I and E are 0 if everything's good and - if something's wrong. The S factors measure how similar the data points are to each other.",
        "formal_text": "Convert casual text to formal text: This project is all about creating trees made up of topical segments. Each segment has a center that best represents its content. The main goal is to maximize net similarity, which is the total similar"
    },
    {
        "casual_text": "In GA-soft, if the angle  is bigger than 90 degrees, g P KD ends up going in the opposite direction of g CE. This could either slow down or even reverse the direction of the gradient descent.",
        "formal_text": "Convert casual text to formal text: In GA-soft, if the angle  is bigger than 90 degrees, g P KD ends up going in the opposite direction of g CE. This could either slow down or even"
    },
    {
        "casual_text": "• Are the fancy words showing strong feelings? Like \"Temperamentsbolzen\" for a really moody person? Yes. But \"Metallbolzen\" for a metal bolt? No, it's just a normal thing.",
        "formal_text": "• Are the fancy words showing strong feelings? Like \"Temperamentsbolzen\" for a really moody person? Yes. But \"Metallbolzen\" for a metal bolt? No, it's just a normal thing"
    },
    {
        "casual_text": "So, Poerner et al. (2018) came up with a method that's pretty close to what we're doing. They use gradients to create text that really gets neurons going, kind of like how computer vision works (thanks to Simonyan et al., 2014). The cool thing is, their method shows what the neurons are picking up on by turning it into text, which is similar to our idea. But there are some downsides: their method can only make text of a set length, and it needs a lot of tweaking (like adjusting the annealing temperature) to sound natural. This makes it kind of a pain to use on tons of neurons in big models. Plus, it's hard for people to figure out what the generated text means just by looking at it. On the other hand, we're taking a different route. We're using an example-based approach to grab sentences written by humans from a massive text database. This way, we can see what each neuron finds interesting without all the hassle of parameter tuning. By using text mining tricks, we can spot all kinds of linguistic stuff as long as it shows up in the original text.",
        "formal_text": "Convert casual text to formal text: So, Poerner et al. (2018) came up with a method that's pretty close to what we're doing. They use gradients to create text that really gets neurons going"
    },
    {
        "casual_text": "When using the word list, there are two main issues to watch out for: 1. **Homographs and Homologs**: These are words that look the same but can have different pronunciations, meanings, or both. For example, \"lead\" (the metal) vs. \"lead\" (as in \"to lead a team\"). 2. **Same Meaning, Different Syntax**: Some words might mean the same thing but are used differently in sentences. If we assign different codes to these duplicate words, we’d need a smart parser to tell them apart. While it’s possible to create such a parser, it’s not worth the time or money for this project. So, the solution is to give both words the same code and just deal with the ambiguity when we analyze the coded text. For working with the encoded text, the word list needs to be organized in a way that keeps track of the word’s syntax. This means noting: - Which group and set-type the word belongs to. - Where the word sits in that group. To do this, we use a linked list. Each word in the list has a link to: - The main word (the first word in the group). - The next word in the group. - Information about its set-type. Each of these nodes in the linked list is saved as a record in an indexed file, with the codes acting as keys. This setup lets us pull out any word from a group individually or rebuild the whole group by following the links between words.",
        "formal_text": "Convert casual text to formal text: When using the word list, there are two main issues to watch out for: 1. **Homographs and Homologs**: These are words that look the same but can have different"
    },
    {
        "casual_text": "We used a random selection of 47 sentences from a bigger collection of spoken Swedish that had a total of 267,206 words. The sentences we picked ranged from just 1 word to a whopping 688 words long (not including pauses as words), with an average length of 29 words. In our test group, there were 1,360 word tokens and 498 different word types.",
        "formal_text": "Convert casual text to formal text: We used a random selection of 47 sentences from a bigger collection of spoken Swedish that had a total of 267,206 words. The sentences we picked ranged from just 1 word to"
    },
    {
        "casual_text": "We ran the decision process shown in Figure 4 on all 123 IQAP instances in the dataset, trying out different score types. In Table 3, we share the accuracy, and the macro-averaged precision, recall, and F1-score for the 85 \"yes\" and 38 \"no\" instances. We also included the percentage of instances where one or both adjectives were out of vocabulary (%OOV). We only reported the combined scores for the two top-performing combinations: score socal+pp and score socal+pat+pp. Table 3 breaks down the accuracy, macro-averaged precision (P), recall (R), and F1-score (F) for the \"yes\" and \"no\" responses across the 123 question-answer pairs, along with the %OOV.",
        "formal_text": "Convert casual text to formal text: We ran the decision process shown in Figure 4 on all 123 IQAP instances in the dataset, trying out different score types. In Table 3, we share the accuracy, and the macro-averaged"
    },
    {
        "casual_text": "Alright, let's break down how we train the model. We basically try to make the model as good as possible at predicting the action sequences we already know are correct (ground truth). Following the approach by Correia and Martins (2019), we use Adam optimizer (from Kingma and Ba, 2015) with a learning rate that starts low and ramps up linearly for the first 5,000 steps, peaking at 5  10-5. After that, it gradually decreases. For the BERT parts, we have a weight decay of 10-4. We also use dropout (Srivastava et al., 2014) with a dropout rate of 0.1 to prevent overfitting. For the vocabulary distribution loss, we apply label-smoothing with a value of 0.1 (Pereyra et al., 2017) to make the model a bit more flexible. We process the data in batches of 512 tokens and save our progress every 10,000 steps, just in case we need to go back to a previous state.",
        "formal_text": "Convert casual text to formal text: Alright, let's break down how we train the model. We basically try to make the model as good as possible at predicting the action sequences we already know are correct (ground truth)."
    },
    {
        "casual_text": "Doran and his colleagues (2001) found that the more times you try to start something, the less likely you are to succeed each time.",
        "formal_text": "Convert casual text to formal text: Doran and his colleagues (2001) found that the more times you try to start something, the less likely you to succeed each time you try to start something. Convert casual text to formal text:"
    },
    {
        "casual_text": "Usually, the numbers we predict form a smooth line that goes across time and space. To make things easier, we often just pick regular times (like every hour) and places (either on a grid or at weather stations) to give those numbers. Or, we might only focus on the big changes. MAR-WORDS is cool because it can handle both ways of describing the data. But honestly, figuring out exactly what kind of data we need is still a bit of a puzzle that we need to work on more.",
        "formal_text": "Convert casual text to formal text: Usually, the numbers we predict form a smooth line that goes across time and space. To make things easier, we often just pick regular times (like every hour) and places (either on"
    },
    {
        "casual_text": "Yeah, we can definitely see how dataset bias plays a role here. The name \"Buffett\" keeps popping up in situations related to shareholder stuff, so it ends up ranking high in that category.",
        "formal_text": "Convert casual text to formal text: Yeah, we can definitely see how dataset bias plays a role here. The name \"Buffett\" keeps popping in situations related to shareholder stuff, so it ends up ranking high in that category."
    },
    {
        "casual_text": "The main issue with the current learning methods is that they rank sentences without looking at the bigger picture—like what the overall topic is. You see, a group of related documents often covers a few different topics. Take the \"Quebec independence\" thing, for example. That could involve stuff like the \"leaders of the independence movement,\" \"referendums,\" and \"other independence efforts.\" When you're summarizing a bunch of documents, it's super important to figure out what the main topics are, because sentences about key topics are way more important than ones about random, less important stuff (Hardy et al, 2002; Harabagiu and Lacatusu, 2005; Otterbacher et al, 2005; Wan and Yang, 2008).",
        "formal_text": "Convert casual text to formal text: The main issue with the current learning methods is that they rank sentences without looking at the bigger picture—like what the overall topic is. You see, a group of related documents often covers a few"
    },
    {
        "casual_text": "Once we've applied the preferences, we match the most likely candidate with the reference to narrow down the options. For instance, if she is confirmed to refer to a doctor, then any future mentions of the doctor will need to be female or have an unknown gender. But if a few candidates are almost tied in votes, the reference becomes unclear or ambiguous.",
        "formal_text": "Convert casual text to formal text: Once we've applied the preferences, we match the most likely candidate with the reference to narrow down the options. For instance, if she is confirmed to refer to a doctor, then any future"
    },
    {
        "casual_text": "Here, L_CFD stands for the confidence-penalized cross entropy. The m here is a hyperparameter that adjusts how strong the confidence penalty is, so it's different from the m in Equation 1. In both cases, the big summation is over all the training tokens, N, which means all the target token probabilities get smoothed out. For simplicity, we're not showing the dependencies of q_v and p_v on n.",
        "formal_text": "Convert casual text to formal text: Here, L_CFD stands for the confidence-penalized cross entropy. The m here is a hyperparameter that adjusts how strong the confidence penalty is, so"
    },
    {
        "casual_text": "In the next part, we’ll talk about some experiments that show how well people can agree on where sentences should end. Then, in Section 4, we’ll dive into a computer-based method for tackling the same issue.",
        "formal_text": "Convert casual text to formal text: In the next part, we’ll talk about some experiments that show how well people can agree on where sentences should end. Then, in Section 4, we’ll dive into a computer-based"
    },
    {
        "casual_text": "For models that used word-level knowledge distillation, we also experimented with making the student network's top hidden layer at each step match the teacher's top hidden layer during pretraining. This idea was inspired by Romero et al. (2015), who saw some improvements using a similar approach with feed-forward models. We tried it out and found that it worked about as well as regular knowledge distillation, so we didn’t dig into it any further.",
        "formal_text": "Convert casual text to formal text: For models that used word-level knowledge distillation, we also experimented with making the student network's top hidden layer at each step match the teacher's top hidden layer during pretraining."
    },
    {
        "casual_text": "Sure, I can help with that! Here's the informal version: It can be used as supervision for the conditional generation task. Once the autoencoder is pretrained, the encoder (enc) can be used to turn the training data into pairs of vectors.",
        "formal_text": "Convert casual text to formal text: Sure, I can help with that! Here's the informal version: It can be used as supervision for the conditional generation task. Once the autoencoder is pretrained, the encoder"
    },
    {
        "casual_text": "To set up Gibbs sampling, we need to figure out the marginal distribution for the observations (words), the topic assignments, and the table indicators. We use the Dirichlet integral to handle the document topic distributions and the topic-word matrix, integrating them out. Then, we use the joint posterior distribution for a PDP to gradually get rid of the segment topic distributions. After all that, we end up with the marginal distribution: p(z 1:I,1:J, w 1:I,1:J, u 1:I,1:J | , , a, b) = (5)",
        "formal_text": "Convert casual text to formal text: To set up Gibbs sampling, we need to figure out the marginal distribution for the observations (words), the topic assignments, and the table indicators. We use the Dirichlet integral to handle"
    },
    {
        "casual_text": "We’ve got a bunch of chat logs and the documents that were mentioned in those chats. We use this data to train a Document Recommendation Engine. This engine works by first using an information-retrieval model to find relevant stuff and then a deep-learning model to suggest URLs that fit the conversation. The goal is to predict the most helpful web content for the chat. After it’s trained, the engine runs in real-time on the agent dashboard, recommending the right URLs to help agents during their conversations.",
        "formal_text": "Convert casual text to formal text: We’ve got a bunch of chat logs and the documents that were mentioned in those chats. We use this data to train a Document Recommendation Engine. This engine works"
    },
    {
        "casual_text": "Alright, let's say our GPSG theory led us to this signature (jc, . A) that has the FOOT feature. To explain the FFP, we use this schema:",
        "formal_text": "Convert casual text to formal text: Alright, let's say our GPSG theory led us to this signature (jc, . A) that has the FOOT feature. To explain the FFP, we use this schema"
    },
    {
        "casual_text": "Just a heads-up, this fine-tuning step is different from the one with the poison data that uses RIPPLE. This one is just for getting the replacement embeddings. Oh, and by the way, you can find it publicly available here. Also, it’s based on a big dataset that’s often used for pre-training (shoutout to Devlin et al., 2019).",
        "formal_text": "Convert casual text to formal text: Just a heads-up, this fine-tuning step is different from the one with the poison data that uses RIPPLE. This one is just for getting the replacement embeddings"
    },
    {
        "casual_text": "In this paper, we use \"#\" to represent zero pronouns, and we highlight mentions that belong to the same coreference chain in bold for all examples.",
        "formal_text": "Convert casual text to formal text: In this paper we use \"#\" represent zero pronouns, and we highlight mentions that belong to the same coreference chain in bold for all examples. Convert casual text to formal text"
    },
    {
        "casual_text": "You often hear or read about people accusing authors, certain professions, or even entire language groups of using unnecessarily complicated sentence structures. Phrases like \"Anything that can be said at all can be said simply and clearly in any civilized language, or with the right set of symbols,\" which were coined by the British philosopher C. D. Broad based on a famous quote by Wittgenstein, have been used by some philosophers to criticize others. This idea has gained a lot of traction in philosophical debates. On a more everyday level, most folks who are into information processing, especially those who want to compress information, often think that most, if not all, of what we normally say could be expressed in simpler sentences. They believe that machines would find it easier to analyze these simpler sentences, making the whole process smoother. Sometimes, people even see converting complex sentences into simpler ones as a helpful, maybe even necessary, step before doing anything else with the information. In the field of machine translation, someone like Harris once suggested that translating basic \"kernel sentences\"—which are probably the simplest in terms of syntax—should be easier than translating random, more complex sentences.",
        "formal_text": "Convert casual text to formal text: You often hear or read about people accusing authors, certain professions, or even entire language groups of using unnecessarily complicated sentence structures. Phrases like \"Anything that"
    },
    {
        "casual_text": "Alright, so here's the deal: Truncated SVD gives us the best answer to the problem of finding a matrix D that's as close as possible to A, but with a smaller rank, specifically rank K (Udell et al., 2016). The solution is basically the truncated SVD of A, where D is made up of the first K singular values and their corresponding left and right singular vectors. So, D = sum from k=1 to K of _k * u_k * v_k, with _k being the k-th singular value, and u_k and v_k being the k-th left and right singular vectors, respectively.",
        "formal_text": "Convert casual text to formal text: Alright, so here's the deal: Truncated SVD gives us the best answer to the problem of finding a matrix D that's as close as possible to A,"
    },
    {
        "casual_text": "The NYTimes800k dataset is a bigger and more complete collection of New York Times articles, images, and captions. It's about 70% larger than the previous one. The dataset is split into 763K articles for training, 8K for validation, and 22K for testing. Table 5 shows a detailed comparison between GoodNews and NYTimes800k, focusing on the length of articles and captions, as well as the composition of the captions.",
        "formal_text": "Convert casual text to formal text: The NYTimes800k dataset is a bigger and more complete collection of New York Times articles, images, and captions. It's about 70% larger than the previous one. The dataset is"
    },
    {
        "casual_text": "Some folks have been trying to connect social tagging systems with ontologies. An ontology is basically a way to describe how things are related. Peter Mika (2005) came up with a more detailed social tagging system that includes actors, concepts, and objects. He used tags that often appear together to create an ontology from these social tags. Wu et al. (2006a) took a different approach, using hierarchical clustering to build an ontology from tags that show similar-to relationships. Later on, some new ontology schemes were suggested that work better with social tagging systems, like the ones by Van Damme et al. (2007) and Echarte et al. (2007). These mainly focused on how tags, objects, and users are related, rather than just the tags themselves. Alexandre Passant (2007) manually matched tags to domain ontologies to make information retrieval on social media better. To automate this process, Angeletou et al. (2007) used ontologies created by experts to find connections between tags, but they found that it didn’t cover much. Specia et al. (2007) suggested a framework to organize tags using existing ontologies, but they didn’t test it out. Finally, Kim et al. (2008) looked at the latest methods for modeling tags with semantic annotations.",
        "formal_text": "Convert casual text to formal text: Some folks have been trying to connect social tagging systems with ontologies. An ontology is basically a way to describe how things are related. Peter Mika (2005) came up"
    },
    {
        "casual_text": "In this project, we explore how we can give the model a kind of \"head start\" by using eye movement data from human reading behavior as extra input. This could help NLP models better handle language and tackle tasks like reading comprehension, which is key for testing how well both humans and machines understand text. As a specific example, we looked at reading comprehension. To do this, we gathered eye movement data from 269 people who read and answered questions using the OneStopQA materials (Berzak et al., 2020).",
        "formal_text": "Convert casual text to formal text: In this project, we explore how we can give the model a kind of \"head start\" by using eye movement data from human reading behavior as extra input. This could help NLP models better handle"
    },
    {
        "casual_text": "We got a total of 10,000 human annotations by looking at 100 documents, 25 systems, and having 4 workers on each. If something wasn’t mentioned in the system summary, we just marked it as \"not present.\"",
        "formal_text": "Convert casual text to formal text: We got a total of 10,000 human annotations by looking at 100 documents, 25 systems, and having 4 workers on each. If something wasn’t mentioned in the system summary we just marked it as"
    },
    {
        "casual_text": "Another example is George Bush buying a small stake in a baseball team. The main part, or \"head chunk,\" and the \"informer span\" both mention \"baseball team,\" while the \"head word\" is just \"team.\" The extra word \"baseball\" in the head chunk and informer span might make the question get wrongly labeled as about a sport (ENTY: sport) instead of a group of people (HUM: group). Usually, the head chunk or informer span includes the head word. The head chunk or informer span feature would still be helpful as long as the useful info outweighs the misleading stuff. But, adding just one head word is even more effective.",
        "formal_text": "Convert casual text to formal text: Another example is George Bush buying a small stake in a baseball team. The main part, or \"head chunk,\" and the \"informer span\" both mention \"baseball team,\" while the \""
    },
    {
        "casual_text": "These results suggest that using neural retrieval methods could be really helpful for creating feedback comments for common mistakes. This is pretty important for language learning, since feedback usually focuses on typical errors first, rather than rare or irregular ones. More advanced retrieval methods, like the ones developed by Hashimoto et al. (2018) and Qiu et al. (2017), will probably make the feedback generation even better.",
        "formal_text": "Convert casual text to formal text: These results suggest that using neural retrieval methods could be really helpful for creating feedback comments for common mistakes. This is pretty important for language learning, since feedback usually focuses on typical errors first, rather than"
    },
    {
        "casual_text": "We're suggesting the use of conditional adversarial networks (CANs) for text classification across multiple domains. These networks include a conditional domain discriminator and entropy conditioning to align the joint distributions of shared features and label predictions. This approach aims to boost the system's performance.",
        "formal_text": "Convert casual text to formal text: We're suggesting the use of conditional adversarial networks (CANs) for text classification across multiple domains. These networks include a conditional domain discriminator and entropy conditioning"
    },
    {
        "casual_text": "We rely on our internal translation tools to handle data between English and 10 other languages. For the TRANSLATE TEST (check out Table 4), we convert each test set into English. As for the TRANS-LATE TRAIN, we translate the English training data from MultiNLI 2. To give you a sense of how good the translations are, we’ve included BLEU scores for the automatic translations from the other languages into English for the XNLI test set in Table 3.",
        "formal_text": "Convert casual text to formal text: We rely on our internal translation tools to handle data between English and 10 other languages. Convert casual text to formal text: We rely on our internal translation tools to handle data between English and 10"
    },
    {
        "casual_text": "In Tables 4 and 5, we’re looking at the analysis for the Gadgets and Iphones datasets, which are kind of like the Politics dataset we saw in Table 3. Compared to Politics, we noticed that for Gadgets and Iphones, the CATD-FLOW models show some ups and downs in performance when we bump K from 5 to 20. This might be because LSTMs aren’t great at remembering long-term stuff. This problem seems to pop up more when there’s not a lot of training data. Here’s some quick numbers: - N COMBINE (K=20, B=5): 750, 431, 444 - O COMBINE (K=20, B=10): 750, 434, 445",
        "formal_text": "Convert casual text to formal text: In Tables 4 and 5, we’re looking at the analysis for the Gadgets and Iphones datasets, which are kind of like the Politics dataset we saw in Table 3."
    },
    {
        "casual_text": "If the word is a relative pronoun, chances are pretty good that the next word will be a noun.",
        "formal_text": "Convert casual text to formal text: If the word is a relative pronoun, chances pretty good that the next word will be a noun. Convert casual text to formal text: If the word is a relative pro"
    },
    {
        "casual_text": "Column D just shows the same coefficients after they've been adjusted to avoid overfitting, which can happen when you have a lot of predictors.",
        "formal_text": "Convert casual text to formal text: Column D just shows the same coefficients after they've been adjusted to avoid overfitting, which can happen when you have a lot of predictors. Convert casual text to formal text"
    },
    {
        "casual_text": "To train the neural network and optimize the loss function, we went with an Adam optimizer, which is a type of stochastic gradient descent algorithm (Kingma and Ba, 2015). To prevent overfitting, as suggested by Salakhutdinov (2014), we experimented with dropout at various rates, ranging from 0 to 0.5. We applied dropout to all the hidden layers and found that a dropout rate of 0.1 worked best for our model.",
        "formal_text": "Convert casual text to formal text: To train the neural network and optimize the loss function, we went with an Adam optimizer, which is a type of stochastic gradient descent algorithm (Kingma and Ba, 2015)."
    },
    {
        "casual_text": "Not all reporting verbs work the same way when it comes to metonymy. Different verbs pick out different things from the nouns they’re connected to. Basically, they seem to tell the difference between one person, a group of people, and an organization or institution. We checked this out using the TIMEcorpus. We pulled all the sentences with one of seven reporting verbs and looked at them closely. Even though there weren’t a ton of examples for each verb, we still noticed some cool patterns. Right now, we’re working on using some statistical methods to do similar stuff with the WSJC. We know that using stats might not be perfect, but we’re hoping that even if it’s not super precise, the results will still be really clear and helpful.",
        "formal_text": "Convert casual text to formal text: Not all reporting verbs work the same way when it comes to metonymy. Different verbs pick out different things from the nouns they’re connected to. Basically, they seem to"
    },
    {
        "casual_text": "For collecting data, we use the free Twitter API 4 and the free version of NewsAPI 5. This helps us spot violations reported by different kinds of people, like official accounts with a lot of followers and regular private accounts. This is super useful in places where press freedom is limited, and violations don’t usually show up in public news. The filters we use to search Twitter and NewsAPI were set up together with the monitoring teams from ECPMF, EFJ, and IPI. We came up with 147 keywords and hashtags divided into three groups: (A) hashtags directly linked to press and media freedom violations 6, (B) keywords and hashtags about media people 7 and press and media freedom 8, and (C) keywords and hashtags about attacks or violations 9. We set the APIs to look for either something from group A or a mix of group B and C. This way, we avoid general stuff about media people or attacks that aren’t related to press and media freedom. Based on what the monitoring experts suggested, we also picked 66 Twitter accounts that often report on press and media freedom violations. The data collection happens a few times a day and pulls in around 1,000 news articles and 4,000 tweets daily.",
        "formal_text": "Convert casual text to formal text: For collecting data, we use the free Twitter API 4 and the free version of NewsAPI 5. This helps us spot violations reported by different kinds of people, like official accounts with a lot of followers"
    },
    {
        "casual_text": "To figure out how different ways of picking sentences affect things, we’re comparing the ILP selection method (which was explained in Section 4) with a few other common methods. We’re using the same sentence ranking algorithm for all of them—specifically, we went with sentence-level RankNet. Here’s what we’re comparing it against:",
        "formal_text": "Convert casual text to formal text: To figure out how different ways of picking sentences affect things, we’re comparing the ILP selection method (which was explained in Section 4) with a few other common methods. We’re"
    },
    {
        "casual_text": "On the flip side, our German system knocked the total error rate down by 30%, dropping it from 14.09% to 9.80%. Plus, our German system had a way lower correction error rate compared to Aspell—7.89% versus 10.23%. The no good suggestion rates for real misspelling data are also higher than for news data. Our system only goes up to an edit distance of 2 from the original word. Turns out, in real human errors, the average edit distance for misspelled words is 1.38, but for our smaller data set, the max edit distance is 4 in English and 7 in German. Still, our no good suggestion rates (17.2% and 32.3%) are way lower than Aspell's (23% and 44%), which shows the benefit of not relying on a hand-crafted lexicon.",
        "formal_text": "Convert casual text to formal text: On the flip side, our German system knocked the total error rate down by 30%, dropping it from 14.09% to 9.80%. Plus, our German system had a way lower correction error"
    },
    {
        "casual_text": "Preprocessing involves breaking down text into tokens, identifying things like names, URLs, and numbers.",
        "formal_text": "Preprocess casual text to formal text: Preprocessing involves breaking down text into tokens, identifying things like names, URLs, numbers. Convert casual text to formal text: Preprocessing involves breaking down text into tokens,"
    },
    {
        "casual_text": "Leaderboards are kind of the go-to way to track progress in question answering (Rajpurkar et al., 2016) and a bunch of other NLP tasks (Wang et al., 2019a). But this popularity has a downside: people get obsessed with chasing the \"state-of-the-art\" (SOTA) without really digging into the data or models (Linzen, 2020). For instance, those \"super-human\" models that dominate question answering leaderboards (Najberg, 2018) often flop in real-world scenarios (Feng et al., 2018; Wallace et al., 2019a) because they’ve just learned some tricks that don’t actually work in practice (McCoy et al., 2019; Niven and Kao, 2019). And here’s the kicker: focusing only on the numbers makes it seem like progress in one specific task is the same as progress in real-world NLP challenges (Bender and Koller, 2020). Basically, just looking at those shiny SOTA numbers doesn’t tell us much about how things actually work or where they fall apart (Lipton and Steinhardt, 2019). Leaderboards can give us a sense of how hard, clear, or doable certain examples are. If something has really low discriminability, it might mean there’s an annotation mistake. Like, there was a question that scored super low: \"Why did demand for rentals decrease?\" when the real answer should’ve been \"demand for higher quality housing increased.\"",
        "formal_text": "Convert casual text to formal text: Leaderboards are kind of the go-to way to track progress in question answering (Rajpurkar et al., 2016) and a bunch of other NLP tasks ("
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Imagine we have two things, h and h, that are pretty similar in how they're represented. Now, we've got two sets of information, or \"triples,\" that look like this: (h, r, t) and (h, r, t). The goal here is to figure out how to tell these two triples apart. That's where TransE comes into play.",
        "formal_text": "Convert casual text to formal text: Alright, let's break this down in a simpler way. Imagine we have two things, h and h, that are pretty similar in how they're represented. Now, we"
    },
    {
        "casual_text": "The way we look at words and how long we spend on them depends on a bunch of things, like the word itself and the context it’s in. Content words—the ones with meaning—get our attention about 85% of the time, while function words—like \"and\" or \"the\"—only get noticed around 35% of the time (Rayner, 1998). There’s even some proof that how we look at words might be even more specific, like based on what part of speech they are, thanks to research by Barrett et al. (2016) using eye-tracking. Other factors that mess with how often and how long we look at words include how common the word is (Raney and Rayner, 1995), how easy it is to guess the word from the sentence (Kliegl et al., 2004), where the word is in the sentence (Rayner et al., 2000), whether the word has a happy or sad vibe (Scott et al., 2012), and how long the word is (Rayner, 1998).",
        "formal_text": "Convert casual text to formal text: The way we look at words and how long we spend on them depends on a bunch of things, like the word itself and the context it’s in. Content words—the ones with meaning—"
    },
    {
        "casual_text": "But these systems don’t really know much about specific topics, so they struggle with having deep, meaningful conversations with people. To fix this, there’s been a lot of work on making dialogue systems smarter by grounding them in knowledge. (Kim et al., 2020; Zhan et al., 2021) These knowledge-grounded models can give more accurate responses by using both the conversation context and extra information from outside sources. So, researchers have been building dialogue systems that rely on related documents (Dinan et al., 2018; Zhou et al., 2018b) or knowledge graphs (Moon et al., 2019; Zhou et al., 2018a; Tuan et al., 2019). One notable example is the Doc2Dial task introduced by Feng et al. (2020). It’s designed for goal-oriented, document-based dialogue systems and makes things more challenging by handling multi-turn conversations and generating natural responses based on relevant documents. Then, they took it a step further with the MultiDoc2Dial dataset (Feng et al., 2021), which builds on the original Doc2Dial idea. MultiDoc2Dial is more realistic because the system has to respond based on multiple documents, not just one. But this also makes using knowledge a lot more complicated.",
        "formal_text": "Convert casual text to formal text: But these systems don’t really know much about specific topics, so they struggle with having deep, meaningful conversations with people. To fix this, there’s been a lot of work on making dialogue"
    },
    {
        "casual_text": "Basically, there’s some cool research happening about how neurons and smaller network parts organize themselves as models train, but this hasn’t really been applied to large language models (LLMs) or neural networks in natural language processing (NLP) much yet. In computer vision, they’ve found these special units that do stuff that makes sense to us humans. If something similar happens in LLMs, it could help us figure out what kind of language features these models are actually learning, especially as they train. We need more research to see if these kinds of structures pop up in LLMs and to build on what we know about self-organization in complex systems. This could also make it easier to explain how these models work. Right now, it’s not super clear how to judge if the explanations for how neural networks process information are any good. Finding these modular or emergent structures might help us move past the simple \"yes or no\" idea of faithfulness that Jacovi and Goldberg talked about in 2020. If we find structures that match how humans understand language, it could let us separate out whether something is plausible from a human point of view, like they suggested. Overall, we think it’d be a good idea to focus on how weights and neurons in LLMs organize themselves into these emergent structures to better understand their properties.",
        "formal_text": "Convert casual text to formal text: Basically, there’s some cool research happening about how neurons and smaller network parts organize themselves as models train, but this hasn’t really been applied to large language models (LLMs)"
    },
    {
        "casual_text": "To figure out how similar a candidate sentence is to a reference MS COCO caption, we use BERTScore (Zhang et al., 2020). We tried out BERTScore Precision, Recall, and F1, but ended up going with BERTScore F1. In our dialogue setup, sentences often have words that aren’t part of the main description, so we remove stopwords from both the captions and the sentences. We use spaCy’s English stop-word list but take out numbers and prepositions that give spatial info. Plus, to account for how people talk differently in pairs, we add the sentence with the highest BERTScore in a round to the reference set and use it as an extra caption for the next rounds.",
        "formal_text": "Convert casual text to formal text: To figure out how similar a candidate sentence is to a reference MS COCO caption, we use BERTScore (Zhang et al., 2020). We tried out BER"
    },
    {
        "casual_text": "To show how well diversified textual regularization works, we’ve included a comparison of KL-divergence values for attention weight distributions in Figure 3. These attention weights, calculated using Equation 4, tell us how important each word in a document is for a specific aspect. Bigger KL-divergence values mean the aspect-level classifier picks out different parts of the text for different aspects. For Vanilla-MILN, the KL-divergences are pretty small, which suggests the model is focusing on similar parts of the text for different aspects. But for Vanilla-MILN+dtext, where we’ve applied diversified textual regularization, the KL-divergences get bigger and are more like those of AB-DMSC. AB-DMSC is trained with aspect-level annotations and gives the best attention weights out of the three models. This shows that diversified textual regularization helps the sentiment classifier focus on parts of the text that are actually relevant to the aspect being analyzed.",
        "formal_text": "Convert casual text to formal text: To show how well diversified textual regularization works, we’ve included a comparison of KL-divergence values for attention weight distributions in Figure 3. These attention weights,"
    },
    {
        "casual_text": "We’ve come up with a bunch of syntactic transformation rules for translating Japanese into English, mainly because the two languages have different structures. These rules handle things like verb, noun, and clause reordering. Even though we’re mainly focusing on Japanese to English, a lot of these rules can also work for other languages that switch from SOV (Subject-Object-Verb) to SVO (Subject-Verb-Object) structures.",
        "formal_text": "Convert casual text to formal text: We’ve come up with a bunch of syntactic transformation rules for translating Japanese into English, mainly because the two languages have different structures. These rules handle things like verb, noun"
    },
    {
        "casual_text": "Re-ordering can be seen as a mix of different problems that are connected and can be explained or solved using various linguistic ideas. First off, a lot of long-range re-ordering happens because of differences in how phrases are ordered. Syntax-based and hierarchical models, like the one by Chiang (2005), try to tackle this issue. On a smaller scale, things like word re-ordering within a phrase can usually be predicted based on the nature of the words and their context, with POS tags being a pretty clear clue.",
        "formal_text": "Convert casual text to formal text: Re-ordering can be seen as a mix of different problems that are connected and can be explained or solved using various linguistic ideas. First off, a lot of long-range re"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. First off, we think it’s super important to make sure a method works well before trying to make it faster. Even though QuestEval is slower than ROUGE, it actually matches human judgments much better and doesn’t need people to label stuff manually. Right now, if we run it on a single RTX 2080 GPU, it takes about 2.53 seconds per document on average when working with CNN/DM data. Now that we know QuestEval is effective, our next step is to make it faster. We think smaller, distilled models could be a good way to go. Plus, there’s a lot of room for improvement with some clever tricks, like saving results so we don’t have to redo the same work over and over. The main time-consuming part is generating questions from the source document because: 1) it’s an autoregressive process, meaning it takes time to generate each question, and 2) the source document is longer than the summary, so there are more questions to generate. But once we’ve generated those questions for the source document, we don’t need to do it again since the source stays the same.",
        "formal_text": "Convert casual text to formal text: Alright, let’s break this down in a simpler way. First off, we think it’s super important to make sure a method works well before trying to make it faster. Even"
    },
    {
        "casual_text": "We usually set up evaluation metrics to go from 0 to 1, where a higher score means better performance. This makes sense for evaluating machine translation (MT). But when we talk about complexity, like in Section 4, it's more natural to think of it as \"the higher the score, the more complex\" something is. Luckily, it's easy to switch between these two ways of thinking by just normalizing the scores.",
        "formal_text": "Convert casual text to formal text: We usually set up evaluation metrics to go from 0 to 1, where a higher score means better performance. This makes sense for evaluating machine translation (MT). But when we talk about complexity, like"
    },
    {
        "casual_text": "In Figure 3, you can see how the brand names we looked at are spread out in the space we analyzed. It shows the predicted gender for each brand and includes some example brands. We also checked out how brand names are divided by gender across 17 different product categories. There's a noticeable imbalance in how often male and female brand names show up across these categories. The scatter plot in Figure 3 highlights that male and female brand names tend to group differently based on their sound characteristics. We used tSNE (a method by Maaten and Hinton from 2008) with 17 features to create this plot. It shows that male and female names are distinct along the first dimension but overlap along the second. This means they have some similarities, but they're also different in certain ways.",
        "formal_text": "Convert casual text to formal text: In Figure 3, you can see how the brand names we looked at are spread out in the space we analyzed. It shows the predicted gender for each brand and includes some example brands. We also checked out"
    },
    {
        "casual_text": "Okay, so let's say we have a bunch of frames in a set called N, and we come across an unknown word, ul. We want to figure out which frame ul fits into best. To do that, we look for the frame where the main word, f, is most similar to ul in terms of how they're used. Basically, we're just matching ul to the frame where it feels like it belongs the most.",
        "formal_text": "Convert casual text to formal text: Okay, so let's say we have a bunch of frames in a set called N, and we come across an unknown word, ul. We want to figure out which frame ul"
    },
    {
        "casual_text": "Principal Component Analysis (PCA) helps shrink a big, complicated set of features into a simpler, smaller set by focusing on the ones with the most variation (Shlens, 2014). PCA takes in these transformed features: tf-idf and word embeddings. The whole process can be thought of as doing some math with matrices.",
        "formal_text": "Convert casual text to formal text: Principal Component Analysis (PCA) helps shrink a big, complicated set of features into a simpler, smaller set by focusing on the ones with the most variation (Shlens, 2014"
    },
    {
        "casual_text": "So, if we have a representation z that includes all the shared info from both sentences, it would already have the label info we need. The extra stuff that's only in one sentence isn't really necessary. That's why we can simplify Eq. (6) to Eq. (7) like this:",
        "formal_text": "Convert casual text to formal text: So, if we have a representation z that includes all the shared info from both sentences, it would already have the label info we need. The extra stuff that's only in one sentence"
    },
    {
        "casual_text": "Figuring out which parts of the text are important is a big deal in NLP stuff (Reiter and Dale, 2000). Some folks in 2018 used a pointer network (Vinyals et al., 2015) to pick out key phrases for making questions, and Gehrmann et al. in 2018 used a content selector to control how often they copied stuff for summarizing. The goal of these methods is to make things more accurate, but our approach does that and also boosts diversity (check out our results for proof). Plus, our method helps models learn how to use the selected info, unlike Gehrmann et al. (2018) who manually restrict the copying for non-selected parts.",
        "formal_text": "Convert casual text to formal text: Figuring out which parts of the text are important is a big deal in NLP stuff (Reiter and Dale, 2000). Some folks in 2018 used a pointer network (V"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way: So, you have two taggers, A and B. They’re working through a sequence, and at each step, they’re dealing with weights (w) and tags (ta and tb). Here’s how it looks: - Tagger A goes through the sequence like this: w2 ta2 tb2, then w1 ta1, tb1, and so on, until it reaches wn1 tan1 tbn1, and finally wn tan tbn. - Tagger B follows a similar pattern but skips the ta tags: w2 tb2, then w1 tb1, and continues until wn1 tbn1 wn tbn. Basically, both taggers are processing the same sequence, but Tagger A includes the ta tags, while Tagger B doesn’t.",
        "formal_text": "Convert casual text to formal text: Alright, let me break this down in a simpler way: So, you have two taggers, A and B. They’re working through a sequence, and at each step,"
    },
    {
        "casual_text": "I made a list of words that sound the same but are spelled differently, using a set of American English homophones from Antworth's 1993 list. I got rid of any words with apostrophes, leaving me with 1,449 words. Then, I randomly gave each word a made-up 4-digit code.",
        "formal_text": "Convert casual text to formal text: I made a list of words that sound the same but are spelled differently, using a set of American English homophones from Antworth's 1993 list. I got rid of any"
    },
    {
        "casual_text": "For fine-grained NER, the hybrid approach works way better than the supervised model (LUA). Check out Table 2 for the evaluation results on constituency parsing and semantic role labeling. For constituency parsing, the F1 scores for the English and Chinese test sets are 95.42 and 92.25, respectively. The speed of decoding depends on the length of the input sentence. It can handle 16.6 and 16.0 sentences per second on our test sets. For SRL, the F1 scores for the English and Chinese test sets are 86.7 and 82.1, respectively, and it processes around 10 sentences per second. The speed might not be fast enough for some applications. As a future goal, we plan to create more efficient algorithms for syntactic parsing and SRL.",
        "formal_text": "Convert casual text to formal text: For fine-grained NER, the hybrid approach works way better than the supervised model (LUA). Check out Table 2 for the evaluation results on constituency parsing and semantic role label"
    },
    {
        "casual_text": "Alright, so N is the total number of possible scores. To get matrix E, you multiply two histogram vectors of the scores together (that's the outer product). After that, you adjust matrices O and E so they add up to the same total. Finally, you use these to figure out the QWK score.",
        "formal_text": "Convert casual text to formal text: Alright, so N is the total number of possible scores. To get matrix E, you multiply two histogram vectors of the scores together (that's the outer product). After that,"
    },
    {
        "casual_text": "The cat sat on the mat... but when we talk about 3, it’s about every cat and every mat you can imagine. When I say (3), though, I’m thinking about my own cat sitting on a specific mat, all cozy next to the radiator in my kitchen.",
        "formal_text": "Convert casual text to formal text: The cat sat on the mat... but when we talk about 3, it’s about every cat and every mat you can imagine. When I say (3), though, I’m thinking about my"
    },
    {
        "casual_text": "We’ll also set up a state vector called v_t, which shows the probability of being at each node after going through the whole graph t times. One iteration here means taking one step away from every node. The state vector for the next iteration, t + 1, is defined like this:",
        "formal_text": "Convert casual text to formal text: We’ll also set up a state vector called v_t, which shows the probability of being at each node after going through the whole graph t times. One iteration here"
    },
    {
        "casual_text": "Since both and |′| are constants, making the cost function (′) as small as possible is the same as making sure the rules are spread out evenly across different buckets. If a binary SCFG has a lower cost, it means the rules are more balanced in terms of how they’re used on the source language side.",
        "formal_text": "Convert casual text to formal text: Since both and |′| are constants, making the cost function (′) as small as possible is the same as making sure the rules are spread out evenly across different buckets. If a"
    },
    {
        "casual_text": "The first dataset we're looking at comes from Yencken and Baldwin (2006), and it's based on human similarity judgments. The problem with this dataset is that it treats figuring out the difference between low and medium similarity pairs the same as distinguishing between medium and high similarity pairs. But, let's be real, for most practical uses, nailing the high similarity pairs with great accuracy is way more important. Still, this dataset is handy for checking how our methods stack up against those used in earlier studies.",
        "formal_text": "Convert casual text to formal text: The first dataset we're looking at comes from Yencken and Baldwin (2006), and it's based on human similarity judgments. The problem with this dataset is that it treats"
    },
    {
        "casual_text": "At every merge step in the CKY process, we add another subtree. This one doesn't just average the two attention values, a_cl and a_cr (like in eq. 1), but also includes the child polarity scores, p_cl and p_cr. This setup aligns with how the N-N nuclearity class is defined in RST, where all child nodes are considered equally important in multi-nucleus cases. The only way we can handle the extra complexity of doubling the number of trees in each cell is by using our heuristic method.",
        "formal_text": "Convert casual text to formal text: At every merge step in the CKY process, we add another subtree. This one doesn't just average the two attention values, a_cl and a_cr"
    },
    {
        "casual_text": "Sure, I can help with that. Here's a more informal version: So, given all that, can I just throw out a few thoughts for a moment? It's pretty common knowledge that the same sentence can be easier for one person to understand than another, even if they have similar IQs, similar backgrounds, and seem to be paying about the same amount of attention. Could it be that they're (subconsciously, of course) processing the sentence using different grammars, which make the sentence feel more or less complex to them? Maybe part of what makes training and practice help with understanding is that people learn to use a different grammar—one that's only slightly different from the one they were used to, so it's not too hard to pick up. Maybe most of us, if not all of us, actually use more than one grammar at the same time, switching between them when one starts to feel too complicated or confusing. I'll talk more about this later. Now, while these ideas are kind of cool to think about, I have to admit I don't know of any direct way to test them right now. But I really hope someone comes up with a way to do that.",
        "formal_text": "Convert casual text to formal text: Sure, I can help with that. Here's a more informal version: So, given all that, can I just throw out a few thoughts for a moment? It's pretty"
    },
    {
        "casual_text": "Figure 6 shows how the level of CCD (that's a thing related to how words are grouped together) connects with how precise the word clustering is (as mentioned by Tokunaga and others in 1995 and other studies). This connection could be super useful when combined with our method to reduce how much we depend on humans for this stuff.",
        "formal_text": "Convert casual text to formal text: Figure 6 shows how the level of CCD (that's a thing related to how words are grouped together) connects with how precise the word clustering is (as mentioned by To"
    },
    {
        "casual_text": "We tried out a bunch of ways to measure how similar two sentences are—stuff like string-based methods, lexical approaches, and vector-based ones. We also tested some models like BERT, SciBERT, and BioBERT that were specifically tweaked for classifying pairs of sentences using a big collection of outcome pairs. If you're curious about how we put together the data or what methods we used, you can check out the details. Long story short, the BioBERT model that we fine-tuned worked the best.",
        "formal_text": "Convert casual text to formal text: We tried out a bunch of ways to measure how similar two sentences are—stuff like string-based methods, lexical approaches, and vector-based ones. We also tested some models"
    },
    {
        "casual_text": "The Szeged Dependency Treebank, which was put together by Vincze and others in 2010, has manual annotations for dependency syntax on the same texts. This treebank includes some tricky linguistic stuff, like discontinuous structures, which aren't marked in the constituency treebank. In the dependency treebank, the possessor is connected to the possession, but that link isn't shown in the constituency treebank. You can check out both types of trees in Figure 1. Figure 1 shows an example: \"A finak elvette a kalapját\" (which means \"He took the boy's hat\") analyzed both ways—constituency and dependency.",
        "formal_text": "Convert casual text to formal text: The Szeged Dependency Treebank, which was put together by Vincze and others in 2010, has manual annotations for dependency syntax on the same texts. This treebank includes some tricky linguistic"
    },
    {
        "casual_text": "We measure accuracy using something called the success rate. De Amorim and Zampieri (2013) had a more relaxed version of this (check out Section 2), which we call the relaxed success rate. But we use a stricter definition, where success means finding the correct spelling of a misspelled word within the smaller search space we create. The reduction ratio for our method is 1.1k divided by |W|. The 1.1 comes from the fact that, on average, there are 1.1 words per vector in the V 2W map. So, after reducing the search space, we usually need to do 1.1k distance calculations. It's important to mention that k can change, so to really speed things up, k should be way smaller than |W|.",
        "formal_text": "Convert casual text to formal text: We measure accuracy using something called the success rate. De Amorim and Zampieri (2013) had a more relaxed version of this (check out Section 2), which we call the relaxed success rate."
    },
    {
        "casual_text": "We're part of this big international project called Methods in Research on Research (MiRoR) Project 2, which focuses on making health care research better—like how it's planned, done, reported, and reviewed by experts. When we were creating our toolkit, we got some really helpful tips from the MiRoR team.",
        "formal_text": "Convert casual text to formal text: We're part of this big international project called Methods in Research on Research (MiRoR) Project 2, which focuses on making health care research better—like how it's planned"
    },
    {
        "casual_text": "For the next experiment, we’re using the method from Section 3 on an English dataset to create training data tailored to each language and train a separate semantic parser for each one. We had some of the validation set translated by humans and mixed it with the machine-translated validation data. From here on out, we’ll pick the model with the best exact match (em) accuracy on this combined set and test it on human-translated test data. As you can see in Table 4, this approach beats all the other methods we tried. We got improvements ranging from 33% to 50% compared to the previous best result, which was the Bootstrap approach. The neural model we trained on this SPL data benefits from entity alignment between the sentence and the logical form, allowing it to copy entities directly. The exact match accuracy varies—it’s 53% for Chinese and 62% for Spanish when it comes to hotels, and 41% for Japanese and 68% for Spanish for restaurants. When you compare this to the English results, which were 65% for hotels and 69% for restaurants, you can see that performance drops for languages that are very different from English. However, languages closer to English, like Spanish, come pretty close to matching English’s performance.",
        "formal_text": "Convert casual text to formal text: For the next experiment, we’re using the method from Section 3 on an English dataset to create training data tailored to each language and train a separate semantic parser for each one. We had"
    },
    {
        "casual_text": "Using two bytes to represent an integer gives you 216 - 1 = 65536 unique codes. But since there are way more words in English than we can fit into those codes, we need a way to handle words without assigning each one a specific code. By setting aside one bit for this purpose, we're left with 215 - 1 = 32767 possible combinations.",
        "formal_text": "Convert casual text to formal text: Using two bytes to represent an integer gives you 216 - 1 = 65536 unique codes. But since there are way more words in English than we can fit into those codes,"
    },
    {
        "casual_text": "Traditional datasets are great for spotting names of people, companies, or places, but they’re not so helpful when it comes to judging things like books, TV shows, music, or events. That’s why we decided to build a new dataset that’s all about creative works. This will help us improve how we label and analyze stuff in the media world.",
        "formal_text": "Convert casual text to formal text: Traditional datasets are great for spotting names of people, companies, or places, but they’re not so helpful when it comes to judging things like books, TV shows, music, or"
    },
    {
        "casual_text": "We ran our experiments on four different benchmark datasets, each with its own unique features. First up is AG's News 5, a dataset for classifying news topics. For this one, we only used the title and description fields. Next, we have IMDB 6 (from Diao et al., 2014), which is all about movie reviews and ratings. Then there's Amazon Electronics (we just call it Amazon for short) from He and McAuley (2016), which has reviews on electronic products. Lastly, we used Yelp 2015 (or Yelp for short), a dataset with restaurant reviews. The last three datasets are all about sentiment classification. Since the original Amazon and Yelp datasets were huge, we took a sample of 50,000 reviews for each to work with. You can find more detailed stats in our paper. For our experiments, we used 300-dimensional word embeddings, which we initialized using Glove (Pennington et al., 2014). In our comparison tests, the CNN networks had 400 filters with a window size of 3. The LSTM hidden states were 200-dimensional, and the attention query vectors were also 200-dimensional. We set the initial pooling norm p to 1, which is the same as vanilla attentive pooling. We used Adam (Kingma and Ba, 2014) as our optimizer, with a batch size of 64. To prevent overfitting, we applied dropout techniques (Srivastava et al., 2014) to the word embeddings, CNN networks, or LSTMs, with a dropout ratio of 0.2. All these hyperparameters were fine-tuned on the validation set.",
        "formal_text": "Convert casual text to formal text: We ran our experiments on four different benchmark datasets, each with its own unique features. First up is AG's News 5, a dataset for classifying news topics. For this one, we only"
    },
    {
        "casual_text": "Sure! Here's a more casual version: So, an appositive is when two noun phrases are next to each other, like \"[Israel's Deputy Defense Minister], [Ephraim Sneh], said...\" We follow the same rules as Haghighi and Klein (2009) to spot these appositions. They only considered it if the mention was tagged as a person by the NER system. In our work, we’re a bit stricter: we only allow this feature to match if: (a) the mention is labeled as a person, (b) the thing it refers to is animate (we’ll explain how we figure that out in Pass 7), and (c) the thing it refers to isn’t gender-neutral.",
        "formal_text": "Convert casual text to formal text: Sure! Here's a more casual version: So, an appositive is when two noun phrases are next to each other, like \"[Israel's Deputy Defense Minister],"
    },
    {
        "casual_text": "We've come up with a cool way to improve how text-to-SQL works in NLIDB systems, called PIIA. It’s a parser-independent method that chats with users through multiple-choice questions and can work with any parser you throw at it. Our tests show it gives a big performance boost on two cross-domain datasets using five different parsers. Looking ahead, we’re excited to figure out how to use and reuse the common patterns we see in users’ answers.",
        "formal_text": "Convert casual text to formal text: We've come up with a cool way to improve how text-to-SQL works in NLIDB systems, called PIIA. It’s a parser"
    },
    {
        "casual_text": "Previous research on figuring out how to separate different conversations in a thread usually focuses on comparing messages one by one.",
        "formal_text": "Convert casual text to formal text: Previous research on figuring out how to separate different conversations in a thread usually focuses on comparing messages one by one."
    },
    {
        "casual_text": "We've trained an IRT-disc model to make sampling easier (like skipping the whole issue of balancing feasibility and discriminability).",
        "formal_text": "Convert casual text to formal text: We've trained an IRT-disc model to make sampling easier (like skipping the whole issue balancing feasibility and discriminability). Convert casual text to formal text: We've trained"
    }
]