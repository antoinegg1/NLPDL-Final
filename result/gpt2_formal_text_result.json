[
    {
        "casual_text": "When fine-tuning using the MLM objective, the BASE version of models usually shows better performance improvements on the General Corpus dataset. This is likely because models with bigger parameter sizes are better at understanding the connections between different parts of similes, especially when dealing with similes that have more varied contexts.",
        "formal_text": "fine-tuning with the MLM objective. For the General Corpus dataset, the BASE version of models tends to yield higher performance improvements, probably because the models with larger parameter sizes can better capture the relationship among simile components in the similes with more diverse contexts when fine-tuning with the MLM objective.",
        "GPT2_formal_text": ". For instance, if we have the sentence \"Linguistic theories are not only interesting but also important for helping us understand how language works,\" we can figure out that the topics and topics are the topic and topic-related values. Formal: To understand the context, we can think of the context-dependent connections as distributional vectors. The vector for a word, let's call it x_i, in the target language is calculated based on how often the word appears in the context-dependent vectors, using a bunch of language-specific word embeddings. Formal: Here, λ is a parameter that decides how much of the context-dependent vectors we consider. The final vector for a target word x_i is calculated using a specific formula, like the one in Equation 2. Formal: Equation (2) can be rewritten as: Formal: The probability distribution of the target word x_i given the context vector is represented as P(x_i | x_i = c), where c is the word embedding dimension. Formal: The target word x_i is the one that shows up the most in the context-dependent vectors. Formal: The target word x_i is the one that appears the most in the context-dependent vectors. Formal: So, P(x_i | x_i = c) is just the probability of the word x_i showing up in the context-dependent vectors. Formal: Formal: We need to check if the word x_i is in the vocabulary of the target language. Formal: We want to make sure that the word x_i is in the vocabulary of the target language. Formal: To figure out the word embeddings for a word x_i, we start by converting the source language embeddings into target language embeddings using the word2vec method (thanks to Mikolov et al., 2013). Formal: For each target word x_i, we use the target language embedding as a guide to get the embedding of the source word x_i. Formal: Let's say we have two target word embeddings: one is the source word x_i, and the other is the target word x_i. Formal: We're looking for the pair (x_i, x_i) where the source embedding x_i and the target embedding x_i are both in the vocabulary of the target language.",
        "directory": "acl",
        "filename": "2022.acl-long.543.json"
    },
    {
        "casual_text": "Position-Aware Graphs. PacSum is a new graph-based method that picks out important sentences from a bunch of documents (Zheng and Lapata, 2019). In PacSum, a sentence is more likely to be chosen if it’s more similar to the sentences that come after it and less similar to the ones that come before it. This approach helps PacSum focus on selecting sentences that are early in the text or \"semantically central.\" We took PacSum and made it even better by using SBERT to measure how similar sentences are (we call this new version SPS) and looked at both the individual and global-graph versions of SPS.",
        "formal_text": "Position-Aware Graphs. PacSum is a recently proposed graph-based method to select salient sentences from multiple documents (Zheng and Lapata, 2019). In PacSum, a sentence is more likely to be selected if it has higher average similarity with its succeeding sentences and lower average similarity with its preceding sentences. This strategy allows PacSum to prioritize the selection of earlyposition and \"semantically central\" sentences. We further extend PacSum by using SBERT to measure sentences similarity (the resulting method is denoted as SPS) and consider both the individualand global-graph versions of SPS.",
        "GPT2_formal_text": "ed_5.txt (or you can use the same files as the original setup) to get the final results for the review dataset. \n\nAlso, there are a few ways to tweak the predictions to get them closer to the reference labels. For example, you can use the same reference_source and reference_target vectors to reduce the difference between the original and reference outputs. You can also use a special loss function based on the target distribution to adjust the prediction during training.\n\nOne thing to keep in mind is that the loss function for the target distribution is actually the same as the loss function for the reference distribution. This means you can use the same loss function for both for training and testing, but it might take more computing power. Formal: If the original outputs are already set up as the reference for the review data, the loss function will only look at the references. Formal: We add a special loss function to the model based on the target distribution, so the model won't just guess the reference as the source anymore. Formal: We also add a parameter λ to decide how much of the original data should be kept. Formal: We make sure the output for each target label is only different from itself. Formal: We use a softmax function to decide if the target label should come before or after the reference. Formal: We also use a penalty term to adjust the loss function. Formal: We use the sigmoid function to make sure the training loss stays as close to the reference as possible. Formal: The final classifier is trained using the student model we just talked about. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.124.json"
    },
    {
        "casual_text": "Let’s take a look at how different design choices affect how well our model works.",
        "formal_text": "We now analyze how different design choices impact the performance of our model.",
        "GPT2_formal_text": ", but this approach has a couple of issues: (1) The text we're dealing with can only be limited to a certain length, which makes it tough to handle complex, long texts with lots of text. (2) It's not easy to figure out how to represent the text based on the dataset we're using, which can be a real pain. In this project, we came up with a way to handle these tricky problems using something called a sparse attention network. We also created a new benchmark for text summarization, which is based on the GRU attention network. Our experiments showed that our approach, using sparse attention, works really well for text summarization tasks. Formal: For the rest of this paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our approach sparse. Formal: For the rest of the paper, we'll just call our",
        "directory": "acl",
        "filename": "2020.acl-main.649.json"
    },
    {
        "casual_text": "[41] Check if the system's knowledge base is on board with, disagrees with, or doesn't really care about adding this component to the overall plan.",
        "formal_text": "[41 whether the system's domain knowledge supports, contradicts, or is: neutral regarding inclusion of the component as part of a correct overall plan.",
        "GPT2_formal_text": "_tgt = attention_mask(att_tgt)  \nk = 1 / (M + 1)  \nh = h_tgt(att_tgt) - attention_mask(att_tgt)  \n\nBasically, the attention (att_tgt) is calculated using the attention mask (att_tgt) and the attention value (h_tgt). Formal: To see how well our model is doing, we look at precision, recall, and the F-score. Formal: We also check the ROUGE-1 score, which is the average of the ROUGE-2, ROUGE-L, and ROUGE-SU4 scores, for the whole dataset. Formal: The ROUGE-L score is the average of the ROUGE-SU4 scores, using the full dataset. Formal: Table 3 shows how many documents were read and how many generated questions in the dataset. Formal: Figure 3 shows how our model performed on the dataset. Formal: Formal: We also checked the test questions generated by our model on the full dataset. Formal: Table 4 shows the F-score compared to human-made questions, human-made answers, and our model's answer. Formal: \n\nFrom Figure 3, it’s clear that our model does better than other models when it comes to generating questions. For human-made questions, our model’s answer scored 0.814, which is almost identical to human-made question 0.814. On the other hand, the human-made answers scored 0.852, which is also similar to human-made answer 0.852. Formal: Table 5 shows the human-made answers generated by our model and the best human-made answer. Formal: Formal: Formal: Our model also gets a 0.872 ROUGE-2 score, which is better than the best human-made answer (0.849), but we missed the best human-made answer (0.826). Formal: Formal: Formal: Formal: We also checked how well our model could generate the best answer from the full dataset. Formal: The best human-made answer was \"When was the Titanic built?\", which scored 0.921, and \"When was the Titanic ran into the iceberg?\", which scored 0.922. Formal",
        "directory": "C86",
        "filename": "C86-1006.json"
    },
    {
        "casual_text": "The models are consistent, but they don't give us a direct way to estimate costs. So, to help with picking the right model for real-world applications, especially when you're working outside of academia, we’re keeping an eye on how long things take and how much they actually cost. We’re using cloud machines for this because they’re easy to get, and they always have the same hardware, price, and performance. In the next parts, we’ll just call \"time\" the time it takes and \"cost\" the actual money spent during pretraining, training, and when doing predictions.",
        "formal_text": "is steady for models but cannot be directly used for cost estimation. Here, to provide a practical reference for model selection on real applications, especially model development outside academia, we keep track of the time consumption and actual financial cost for comparison. Cloud-based machines are employed for budget estimation as they are easily accessible and consistent in hardware configuration, price, and performance. In the following sections, we would use \"time\" and \"cost\" to denote the time elapsed and the actual budget in model pretraining, training, and inference.",
        "GPT2_formal_text": "= {\n\n1st: (R, o, u, y, v)}\n\n}\n\nBasically, it's breaking down the question and its answer into different parts, sorting them into categories. Formal: The model spits out a bunch of possible answers, like {x1, x2, ..., xN}, each with a unique ID. These possible answers are linked together using a vector space model, which is shown in Figure 1. Formal: We use a search method that keeps expanding the list until we find the best match. Formal: We measure how well the model does by calculating the F1 score, which checks how well it predicts the answer. Formal: When we test the model on the full test set, we average the F1 scores from all possible answers. Formal: We use a ranking algorithm to figure out the best ranking by looking at all possible answers. Formal: For each candidate, we calculate the similarity matrix M θ (x, y, v), which shows how similar the pair of entities is. Formal: We use the confidence score (which tells us if the model's prediction is correct) to rank the candidates. Formal: After that, we pick the top-ranked answer based on these similarity scores. Formal: We consider the best answer as the one that has the highest confidence score. Formal: We figure out the confidence score for each candidate by running it through the confidence loss function, which is shown in Figure 2. Formal: We calculate the confidence score for each candidate by using the confidence loss function in Figure 2. Formal: Finally, we pick the best answer by using the top-ranked candidate's confidence score. Formal: We think that the best answer should be the one that's the least uncertain, so the model should aim for that. Formal: We check this by calculating the uncertainty score for each candidate's probability distribution, using the uncertainty loss function in Figure 2. Formal: Formal: We treat the uncertainty loss as the \"confidence score\" to rank the candidates, so that's how we calculate the candidate's uncertainty score. Formal: Formal: We calculate the uncertainty score for each candidate's probability distribution, using the uncertainty loss function in Figure 2. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eacl",
        "filename": "2021.eacl-demos.39.json"
    },
    {
        "casual_text": "In the QA generation interface, workers see a sentence with all the target words highlighted in bold. They're asked to come up with one or more questions that connect two of these bold words. To make the question, they start by picking a question prefix, then, if needed, add an auxiliary, and finally, choose parts of the sentence to build the full question. They might tweak it a bit to make it sound right. After creating the question, the next step is to answer it by picking parts of the sentence. Just like with the question, they can adjust the answer to make it grammatically correct.",
        "formal_text": "In the QA generation interface, workers are shown a sentence with all target words in bold. Workers are instructed to generate one or more questions that relate two of these target words. The question is generated by first choosing a question prefix, then, if applicable, an auxiliary, then selecting one or more spans from the sentence to form the complete question, and lastly, change it to make it grammatical. Given the generated question, the next step involves answering that question by selecting span(s) from the sentence. Again, the answer can also be amended to be made grammatical.",
        "GPT2_formal_text": ", attention_mode, and position_mask. Formal: The final output is a matrix that represents the total number of token embeddings in the vocabulary V_a, along with the encoding for each token. Formal: At each step t, we use a bilinear function (with δ set to 0) to figure out the loss value L_t. This loss is calculated using the perplexity of the latent variable L_t at step t. Formal: In this paper, we introduce a method called MODE-LSTM that predicts the next token based on the last token and the context. We also suggest a new approach called MODE-LSTM-CRF that uses a combination of linear and CRF mechanisms to predict the next token. Formal: We train this MODE-LSTM using an image and a document. We use a context-aware approach to improve the model's ability to predict the next token. Formal: We use the response representation R to predict the next token based on the last token and the context. Formal: We train this MODE-LSTM using an image and a document. We use a context-aware approach to improve the model's ability to predict the next token. Formal: We use the response representation R to predict the next token based on the last token and the context. Formal: We train this MODE-LSTM using an image and a document. Formal: We train this MODE-LSTM using an image and a document. Formal: We use the response representation R to predict the next token based on the last token and the context. Formal: We use the response representation R to predict the next token based on the last token and the context. Formal: We train this MODE-LSTM using an image and a document. Formal: We train this MODE-LSTM using an image and a document. Formal: We train this MODE-LSTM using an image and a document. Formal: We train this MODE-LSTM using an image and a document. Formal: We train this MODE-LSTM using an image and a document. Formal: We train this MODE-LSTM using an image and a document. Formal: We train this MODE-LSTM using an image and a document. Formal: We train this MODE-LSTM using",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "If there were no out-of-vocabulary subwords in the bilingual lexicon, the recall of the oracle method would be 100%.",
        "formal_text": "The recall of the oracle method would be 100%, if there were no out-of-vocabulary subwords in the bilingual lexicon.",
        "GPT2_formal_text": ") has all the important data about a specific query. We're using the masked language modeling model (MLM) model, which was introduced by Mikolov et al. in 2013. Basically, we take the input sentence (h_i) and use a linear transformation to change it into a form that fits the MLM setup. This transformation is then turned into a vector representation, which we call v_i. Formal: To make sure the topic representation is different from the source representation, we use something called adversarial training, which is explained in this section. Formal: After that, we calculate the topic CVaR using the weighted average of the model's output, v_i_p, and the topic vector v_i, which is the average of all the word embeddings in the topic vector. This weighted average is then used to update the topic vector v_i. Formal: Lastly, we average the topic vectors v_i to get the final topic vector, v_i. Formal: Formal: Here, A_i represents the probability of generating a topic vector from a source word embedding vector, and µ_i_p(i) is the probability of the topic vector generated by the next word embedding vector, which is the i-th word in the source sentence. Formal: For more info, check out Figure 1. Formal: In our setup, we apply the MLM model using the attention mechanism, which is explained in Section 4.1. The attention mechanism helps the MLM model learn better topic representations, which makes it better at generalizing better. Formal: Here's how the original version of the model, Φ, works. Formal: We'll call the updated version of Φ, Φ2. Formal: This model is trained using a data-driven approach (Lin et al., 2017). Formal: We use the same training, validation, and test sets as in Equation 2, but we throw in a linear layer to create a topic embedding vector for the target class. Formal: To calculate the model's loss, we can use the equation in Equation 2 again, but with the L2 regularization parameter set to 1. Formal: We run the model for a set number of epochs to optimize the loss. Formal: We also make sure the loss is continuous during training, so we can easily evaluate it. Formal: We train the model using cross-entropy",
        "directory": "D19",
        "filename": "D19-1142.json"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. \n\nSometimes, basic patterns or simple variables don't really carry much meaning or information. They’re just too basic and don’t tell us much. So, just grouping these simple patterns together or trying to unify them isn’t enough to capture the real, complex relationships and processes we’re interested in. It’s not efficient.\n\nLet’s look at an example to make this clearer. Imagine we’re talking about a mathematical category. This category is made up of different features. Each feature has a name and a value, and we can represent it like a term, say `term(name, value)`. \n\nDoes that help?",
        "formal_text": "Ordinary patterns as it/ Pr(; h)g Is. el< cxprcssivc power, t)esatlSC var[ablcs theFcirl arc Sil)i[)ly il](iClCl\"tlliltdt(7 alld Ihtis car'ry almost no irffqrrnalion 'l'hercforc, stie}l palL(ms aud unification among thcm arc msuffiei0nt for' capturlng t]le {~, i<'al/l rYlat ic al <! , >erm r'al ixat ior~ and tim process: n~> effici(ncy. ], It us look a. t some c: <amph. ~s below A ~, , l'anl matical catc£>ory is assumed Lo be it llst of features A feature consisLs of a feature nalnc and a w~hic, and rcprcscnLcd as a t. cmn like tt~rn. e (vat, z()",
        "GPT2_formal_text": "ed=1 and attention_expansion=1. Formal: The second query is a permutation matrix with different elements, which we call... Formal: The second query is a permutation matrix with different elements, which we call... Formal: This way, the sequence with the highest relevance is chosen. Formal: Our system gives the best possible results by using both this permutation and the one before it. Formal: Our system gives the best possible results by using both this permutation and the one before it. Formal: This is because the system relies on the idea that the current sentence is the most important one in the group of previous sentences. Formal: This is because the system relies on the idea that the current sentence is the most important one in the group of previous sentences. Formal: By checking these scores, the system can figure out the most important sentences in the original sentence. Formal: By checking these scores, the system can figure out the most important sentences in the original sentence. Formal: By comparing the relevance values, the system can figure out which sentences are the most important. Formal: This is because the system relies on the idea that the current sentence is the most important in the group of previous sentences. Formal: This is because the system relies on the idea that the current sentence is the most important in the group of previous sentences. Formal: Since the system doesn't directly look at the words in the sentence, it doesn't need to know the meaning of words. Formal: Since the system doesn't directly look at the words in the sentence, it doesn't need to know the meaning of words. Formal: The relevance values in the system are calculated based on how relevant the current sentence is to the previous sentence. Formal: The relevance values in the system are calculated based on how relevant the current sentence is to the previous sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1018.json"
    },
    {
        "casual_text": "Off-topic examples in human-rated data are usually too rare to teach an automated system how to spot and reject off-topic responses. As a result, these automated systems tend to struggle more than human raters when it comes to scoring off-topic stuff accurately (Lochbaum et al., 2013; Higgins and Heilman, 2014). To make sure speaking test scores are reliable, there needs to be a way to catch off-topic responses before the scores are finalized (Wang et al., 2019). \n\nIn our educational app, we use an automated speaking assessment system to help non-native English learners get ready for the IELTS speaking test. We’ve noticed that freemium features tend to attract more off-topic responses because some users just mess around with the system. In this case, being able to detect off-topic stuff accurately is super important. It helps build trust and encourages trial users to upgrade to paid customers.",
        "formal_text": "Off-topic examples in human-rated data is often too sparse to train an automated scoring system to reject off-topic responses. Consequently, automated scoring systems tend to be more vulnerable than human raters to scoring inaccurately due to off-topic responses ( Lochbaum et al. , 2013; Higgins and Heilman, 2014). To ensure the validity of speaking assessment scores, it is necessary to have a mechanism to flag off-topic responses before scores are reported (Wang et al. , 2019). In our educational application, we use the automated speaking assessment system to help L2 learners prepare for the IELTS speaking test. We do see a higher rate of off-topic responses in freemium features as some users just play with the system. In such a scenario, accurate off-topic detection is extremely important for building trust and converting trial users to paid customers.",
        "GPT2_formal_text": ", where i can be 0, 1, 2, 3, or 4, and q_i can be 0, 1, 2, 3, or 4. The probability of the probability vector P_ij given the input sequence x_i is calculated using the similarity measure called cosine similarity. Formal: The number of tokens in the source code that match the size of the input sequence x_i. Formal: The size of the target code tokens, as measured by their representation length. Formal: The length of the target code tokens, as determined by their representation length. Formal: The number of tokens in the source code where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The length of source code tokens where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The number of tokens in the target code where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The length of target code tokens where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The number of tokens in the source code where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The length of source code tokens where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The number of tokens in the source code where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The length of source code tokens where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The number of tokens in the target code where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The number of tokens in the source code where the first token is a null token, and the second token has a single character that doesn't start with a backspace. Formal: The number of tokens in the target code where the first token is a null token, and the second token has a single character that doesn't",
        "directory": "acl",
        "filename": "2020.acl-main.56.json"
    },
    {
        "casual_text": "You can confirm a segment by hitting the ACCEPT button. After you do that, it'll automatically move on to the next segment.",
        "formal_text": "• To validate a segment, the user can also use the ACCEPT button. Once pressed, it also moves to the next segment.",
        "GPT2_formal_text": "_T c (i) is the attention vector for the i-th token of the source sentence, and it's created using a max-pooling layer. Formal: We've tweaked the cross-entropy loss by using attention on the output embeddings. Formal: We've also included a non-linear function to balance the features of the source and target sentences. Formal: We’ve added some special token embeddings for the target language, called T_i_i, to deal with issues with not having enough data for the target side. Formal: We’ve also added a non-linear function to balance the features of the source and target sentences, T_i_i. Formal: We’ve adjusted the cross-entropy loss by adding a penalty for using too much source-side info. Formal: Finally, we’ve added a function to weigh the source and target sentence embeddings. This function helps adjust the features to match the target side. Formal: We’ve tested our model on the development set and the results are in Table 3. Formal: Figure 2 gives a look at how the model performs on the target side. Formal: The model’s performance is measured by the cross-entropy loss, and the results are in Table 3. Formal: The model’s performance is also calculated by the cross-entropy loss, and the results are in Table 3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "If there are no word guesses in a certain time period that score higher than a set limit, we add some edges labeled \"GAP\" with a score that matches that limit.",
        "formal_text": "If gaps in the utterance are recognized (i. e. there are no word hypotheses in a certain time interval with a score larger than a given threshold value), edges are introduced which are marked with the universal category GAP and a score feature which has the threshold as its value.",
        "GPT2_formal_text": "= {'.id', '.title', '.body', '.link', '.redirect', '.created', '.new', '.edit', '.delete', '.edit_text', '.expand', '.edit_image', '.edit_text', '.parent', '.empty', '.parent_id', '.parent_type', '.parent_url', '.edit', '.edit_text', '.parent', '.edit_url', '.edit_image', '.edit_text', '.parent_type', '.parent_url', '.edit_image', '.parent_type', '.parent_url'}. Formal: Basically, the process for handling 'create_edit' and 'edit_text' is the same as for 'delete_edit', but now with the constraints being the same. Formal: We use a little tweak to handle 'create_edit' and 'edit_text'. Formal: When generating the edit model, we use two things: the edit model and the constraint model. Formal: For both 'create_edit' and 'edit_text', the input text is an English sentence. Formal: For instance, if we have a text from the database 'test', we can generate a model like this: Formal: The whole thing is kind of like the NLI model we talked about in Section 3.1. Formal: The constraint model is based on the original NLI model, but it has a few changes. Formal: For 'create_edit', the input text is 'test', but the new model we create is 'edit', so the input text is 'edit' too. Formal: For 'edit_text', the input text is 'test', but the new model we create is 'edit', so the input text is 'edit' too. Formal: These changes, along with the constraint model, are described in Section 3.2. Formal: In these examples, we use two different models to model the output and the text. Formal: The main differences between the two models are: Formal: Formal: • The constraints are tied to the input text, not the whole sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E83",
        "filename": "E83-1019.json"
    },
    {
        "casual_text": "Even with smaller families, we can't really see a big range of differences because there just isn't enough data. But they still show pretty much the same results.",
        "formal_text": "Even smaller families do not allow to vary la on such a large range due to data sparsity, but show similar results.",
        "GPT2_formal_text": "_tokens (b) is like the output mask for the first token in the output sequence. Formal: We need to figure out the attention weights for all the tokens in the sequence, both during decoding and at the token level. Formal: Attention weights can be calculated using any linear function or probability, and they can be adjusted during training. Formal: We've come up with a neural network model for generating text that uses these attention weights. Specifically, we use a feed-forward neural network to handle the hierarchical structure of the input sequence. The network takes the input sequence and turns it into a vector using a bidirectional LSTM. Formal: We tweaked the model by trying out different ways of modeling the attention, like using a feed-forward neural network with self-attention, an attention-based linear projection, and a self-attention-based linear projection. We also looked into using an attention mechanism that can focus on different parts of the sequence. The experiments show that the model performs better than the usual sequence modeling methods, and it's also way easier to train and understand. Formal: The model we're proposing can generate text in a sequence with multiple tokens, using different attention weights. Formal: The model we came up with is pretty straightforward, easy to train, and works well for text generation. It's pretty efficient because it doesn't need a lot of parameters and only needs to handle a limited number of text units. Formal: We tested our model on two datasets: the TREC 2020 news dataset (from Cui et al., 2020) and the UCCI corpus (from Cai et al., 2020). We also did some experiments on the WMT 2019 news dataset. For the experiments on the WMT 2019 dataset, we used the same setup as the WMT 2019 challenge. Formal: The results are in Table 2. Formal: The results for the WMT 2019 news task are in Table 3. Formal: We also ran some tests on the CoNLL 2020 shared task (thanks to Sanh et al., 2020). Formal: We created the CoNLL 2020 shared task dataset by training a model using the CoNLL 2008 task and then fine-tuning it using the shared task data. Formal: We built the CoNLL 2020 shared task dataset using the CoNLL 2008 task and fine-tuned it with the shared task data. Formal: We trained the CoNLL 2008 task using the CoN",
        "directory": "acl",
        "filename": "2020.acl-main.649.json"
    },
    {
        "casual_text": "If it's the first sentence in the text or if searching through all the active frames doesn't work, the theme gets linked to a \"base\" frame.",
        "formal_text": "If the sentence being processed is the first sentence in the text or if a search of all active frames fails, the theme is associated with a \"base\" frame.",
        "GPT2_formal_text": ", and we've already covered the types of names we're dealing with. So, we can use the info in the text from the input document to generate the output for the target document. Formal: Alright, let's break this down in a simpler way.\n\nFor a given word and a given source document, we can figure out how related they are by looking at the N-gram embeddings (we'll call them \"w_s\" and \"w_e\" for short) from the source document to the target document. The way we measure this relatedness is by calculating the cosine similarity.\n\nSince we can get the embedding vectors for a word and a document at the same time using the source document, we can use that info to figure out the relatedness matrix (we'll call it \"r_s\" and \"r_e\" for short). This matrix gives us the chance that two words or documents are related, calculated based on their embeddings.\n\nThe thing is, the words and documents we're looking at aren't the same. To make sure we get the right answer, we calculate something called the relational similarity (or \"r_rel\" and \"r_rel_1\" for short). This helps us find the best options by figuring out the best match between the words and documents. Formal: So, r_rel is the similarity between the word and the document, and r_rel_1 is the similarity between the word and the relation.\n\nIf two words or documents have the same embedding vectors, they're considered related. If they don't have the same embedding vectors, they're not related. Formal: But, embedding vectors are based on how the words and document are structured, and they can't be used for all kinds of matching problems. So, for most cases, we just stick to the vector embedding formula (embedding = 1, 1/2, 2/2, . . . , 2). Formal: So, in reality, the embedding vectors for words are just 1/2, 2/2, . . . , 2. Formal: But, word embeddings are actually a bit more flexible. They can be any number of different forms—like 1/2, 1/4, . . . , 1/4, . . . , 2. Formal: Formal: To get the embedding vectors, we calculate the distance matrix (d_rel",
        "directory": "C82",
        "filename": "C82-1011.json"
    },
    {
        "casual_text": "We'll use these ranked argument candidates to train the ranking SVM (as introduced by Joachims in 2002) and figure out the model's parameters. When we're testing the model, we just pick the candidate it ranks highest as the final output. One thing to mention is that we don't need any eye gaze data during the testing phase.",
        "formal_text": "Using these rankings of argument candidates as training data for the ranking SVM (Joachims, 2002), we estimate the model parameter. In the test phase, the most highly ranked candidate by the model is considered as the model output. Note that we do not require any eye gaze information in the test phase.",
        "GPT2_formal_text": "_len_t is the number of parameters in the attention-masked attention layer. And the output_len is the length of the output. Formal: After we've done some preprocessing, the sentence gets split into tokens using the Weka toolkit. Formal: The tokenized and tokenized_len_t vectors are then fed into a linear projection (like the one by Hinton et al., 2009) to get a vector representation for the whole sentence. Formal: To make sure the model makes the right predictions, we use some validation data that we can trust to make sure the model's predictions are legit. Formal: We set the learning rate to 0.1, the batch size to 32, and the attention kernel dimension to 512. Formal: The output is a normalized version of the word embedding vectors. Formal: We'll call this model the model h_ij. Formal: The model h_ij is trained using the validation set, but it doesn't have any extra parameters because it's already been pre-trained with the target embedding data. Formal: Just a heads-up, the sum of the identity matrix and the learned embedding matrix from h_ij won't be different from each other. Formal: To train the model, we'll use the validation set that's been pre-trained by the model h_ij. Formal: We'll use the validation set as a validation set to check how well the model is doing. Formal: We'll train the model using the validation set to see how well the model can generate correct tokens. Formal: We'll train the model using the validation set to see how well the model can generate correct tokens. Formal: The model h_ij is trained to figure out the token distribution for the target language. Formal: The model h_ij is trained to figure out the token distribution for the source language. Formal: To train the model, we'll use the validation set that's been pre-trained by the model h_ij. Formal: To train the model, we'll use the validation set that's been pre-trained by the model h_ij. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1269.json"
    },
    {
        "casual_text": "Sure, the cross-project approach makes sense for testing how well a model works on different projects. But when it comes to using it in batch mode across projects, it does rely on some pretty big assumptions—like there being no documentation at all for any of the methods in the projects you're looking at.",
        "formal_text": "While the cross-project methodology is reasonable for evaluating model generalizability, the cross-project batch-mode use case does make strong assumptions (e. g. , no documentation exists for any method in the targeted projects).",
        "GPT2_formal_text": ": (1, 2, 3) Formal: We used a dataset with around 100,000 examples for training and another 100,000 examples for testing. Formal: We used the same method for training the Bayesian HMM-based model. Formal: The dataset includes 200,000 examples for training and another 200,000 examples for testing. Formal: We built the model using the OpenNMT toolkit (Oepen et al., 2016). Formal: The decoding process worked really well and we got better results than the baseline model. Formal: We used the cross-entropy loss for the Bayesian HMM model. Formal: The model works well as a baseline because it can be adapted easily to different situations. Formal: The model can be trained in different ways. Formal: In this case, we're using the cross-entropy loss for the Bayesian HMM model. Formal: We set the beam size for the model to 100 and used the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.1. Formal: We set the beam width to 200 and did a learning rate of 0.1. Formal: For our experiments, we used an LSTM model with a hidden size of 512. We trained it for 100 epochs with the Adam optimizer and then checked the BLEU score on the test set. Formal: We also tried out the LSTM model with a hidden size of 2048, which worked best for our situation. Formal: We trained the model using all the training data. Formal: We calculated the BLEU score on the test set using a linear interpolation method. Formal: We used the test set for checking the model's performance. Formal: We added BLEU scores for the BERT model and the LSTM model to our experiments. Formal: The results from the Bayesian HMM model are shown in Table 5. Formal: We also plotted the results from the LSTM model. Formal: From the BLEU score, we can see that the LSTM model consistently outperforms the Bayesian HMM model. Formal: We ran the experiments 5 times with different random seeds and averaged the results to get the final result. Formal: We picked the best model based on the BLEU score. Formal: To find the best model, we picked the one with the lowest BLEU score.",
        "directory": "acl",
        "filename": "2022.acl-long.339.json"
    },
    {
        "casual_text": "In the paper, we noticed that depth doesn't do better than the RGB model for certain classes of objects. This happens when the depth map doesn't change much across those objects.",
        "formal_text": "In the paper, we made the observation that the classes for which depth fails to outperform the RGB model are the classes of object for which the depth map does not vary too much.",
        "GPT2_formal_text": ", an attention mechanism that helps the model generate responses that focus on specific parts of the text. Formal: The response generation model, or R, is basically a mix of the knowledge source and the encoder, which we call the encoder_t model. Formal: When we're generating responses, we pick the one that's most like the input prompt by using attention and a linear layer. Formal: To make sure the generated responses are factually correct, we use an attention mechanism that works on the encoder_t model. Formal: Alright, let's break this down in a simpler way. Formal: For our dialogue generation model, we train it on pairs of datasets. Formal: The dataset D_i for the i-th pair is represented as a vector with dimensions (x_i, y_i). Formal: The dialogue generation model is trained by minimizing the cross-entropy loss function. Formal: We get the output y_i for each pair by minimizing the loss across all possible dialogue acts. Formal: Now, the dialogue generation model is trained to generate responses using the output y_i. Formal: The dialogue generation model is then optimized using back-propagation through a discriminator. Formal: The final goal for this dialogue generation model is to maximize the objective function, which is based on the dialogue acts. Formal: For the discriminator, we use a softmax function. Formal: Lastly, we train the dialogue generation model using cross-entropy loss to get the final outputs. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.319.json"
    },
    {
        "casual_text": "We’re working on a basic version of local word discovery using an FST (Finite State Transducer) to connect known morphemes found in a noisy phone sequence to more complex word forms. Our method relies on two main assumptions: first, that the morphosyntactic description is detailed enough to be turned into an FST, and second, that we have a decent phone recognizer, which could be something simple, like training one on a few hours of transcribed audio from similar languages, or tweaking a bigger pretrained model. \n\nFor speech representation, we’re using Allosaurus to give us a low-dimensional way to represent speech, which helps with matching it to phone sequences predicted by our morphological transducer. Allosaurus comes with a pretrained model that lets us limit the output to a specific set of phones, which is pretty handy (Li et al., 2020). It supports phone inventories for over 2,000 languages out of the box, including Kunwinjku. However, we noticed that the default inventory for Kunwinjku was missing some stuff, so we made our own based on (Evans, 2003).",
        "formal_text": "Given the task definition, we implement a baseline version of local word discovery using an FST to map attested morphs embedded in a noisy phone sequence to new, morphologically complex word forms. Our approach makes two assumptions, namely that the morphosyntactic description is suf-ficiently explicit and complete that it can be represented as an FST, and that a modest phone recognizer is available, e. g. by training a recognizer on a few hours of transcribed audio from related languages, or fine-tuning a larger pretrained model. Speech representation. We adopt Allosaurus to provide a low-dimensional representation of speech which supports approximate matching against phone sequences predicted by the morphological transducer. Allosaurus provides a pretrained model which includes the ability to constrain the output vocabulary to a predefined set of phones (Li et al. , 2020). The inventories of over 2, 000 languages, including Kunwinjku, are supported in the default configuration. In practice, we found that the inventory for Kunwinjku was incomplete and we created our own, following (Evans, 2003).",
        "GPT2_formal_text": "= [c_i], count_ids = [c_j], sum_ids = [s_i, s_j], class_ids = [class_i], }  \n\nBasically, it's saying that the label for an entity is the same as the label for its type. Formal: So, we have two lists, d_i and d_j. For each pair (i, j), we figure out the label for the entity by finding the most similar label (the biggest value among the options) that's not the same as the entity's type. Formal: The big-picture goal here is to make sure we don't end up with confusing or useless labels. Formal: We use the concept of lexical ambiguity (as described by Lavie and Agarwal in 2003) to handle this. Formal: To make sure we can mix and match labels, we create a special token called ‫ẽ,‬ where the values are the types of the entities, and the labels are the labels. Formal: Lastly, we use a kind of adjacency matrix to figure out the predicted labels. Formal: The main goal of how we define it is to figure out the smallest possible value for the hidden vectors, based on the relationship between the entities and their labels. Formal: The probabilistic model we're proposing doesn't take any extra knowledge from the surrounding knowledge. Formal: To make sure the label stays consistent, we check the related entity types in the knowledge base and also the relationship between the entities. Formal: Lastly, we train the model using a modified version of the traditional Markov chain Monte Carlo (MCC) method (Agirre et al., 2008). Formal: We train the model using this setup. Formal: The results from the two experiments we ran are in Table 3. The \"match\" column shows how well the model matches the labels. Formal: We also included the results from two other experiments. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.157.json"
    },
    {
        "casual_text": "The results on the real test set are way worse than those on Ori. and Avg., showing that real user evaluations are way tougher. This is because real cases can have multiple robustness problems all at once, while each augmentation method in LAUG looks at them one by one. Even with this difference, the model's performance on real data gets a lot better after being fine-tuned on the augmented data, proving that LAUG really helps boost the model's real-world toughness. Table 13 checks which kind of errors the model made on the real test set by going through all the mistakes BERT Ori. made. \"Others\" are errors that aren't due to robustness issues, like when the model just doesn't perform well. It turns out the model struggles a lot with LU robustness (over 70%), and almost half the errors are because of different language varieties. This happens because real user evaluations have way more diverse ways of saying things than the original data. After training with the augmented data, we see fewer errors related to Speech Characteristics and Noise Perturbation, meaning BERT Aug. handles these issues better. Note that the total percentage of errors is over 100% because 25% of the cases have more than one robustness issue. This just shows again that real user evaluations are harder than the original test set.",
        "formal_text": "The performance on the real test set is substantially lower than that on Ori. and Avg. , indicating that real user evaluation is much more challenging. This is because multiple robustness issues may be included in one real case, while each augmentation method in LAUG evaluates them separately. Despite the difference, model performance on the real data is remarkably improved after every model is finetuned on the augmented data, verifying that LAUG effectively enhances the model's real-world robustness. Table 13 investigates which error type the model has made on the real test set by manually checking all the error outputs of BERT Ori. \"Others\" are the error cases which are not caused by robustness issues, for example, because of the model's poor performance. It can be observed that the model seriously suffers to LU robustness (over 70%), and that almost half of the error is due to Language Variety. We find that this is because there are more diverse expressions in real user evaluation than in the original data. After augmented training, we can observe that the number of error cases of Speech Characteristics and Noise Perturbation is relatively decreased. This shows that BERT Aug. can solve these two kinds of problems better. Noting that the sum of four percentages is over 100% since 25% error cases involve multiple robustness issues. This again demonstrates that real user evaluation is more challenging than the original test set 9.",
        "GPT2_formal_text": "ed. So, it's really important to figure out which entity tokens are actually important. Once we've done that, we get the output from the attention layer, which is the entity type. Formal: So, to train this model, we use the new embeddings that the model creates (from the hidden layers) and the input vectors from the domain prediction layer to create the representations for the entities we're looking at. Formal: We also use the knowledge about the entity types to make sure our results are fair and unbiased. Formal: In Section 4.4, we talk about how we calculate the entity type embeddings and how we measure the performance. Formal: So, the entity type embeddings are just the average of the embeddings of all the entity types in the training data. Formal: We use something called Frobenius norm regularization to make sure we're treating the embeddings of entities as uniform, which is a common way to do normalization. Formal: For the entity type embeddings, we use a two-layer perceptron to find the best values for the embeddings. Formal: We show some examples of how we used this to predict the entity type for the QMR dataset in Figure 1. Formal: We also figure out the hidden representation for the entity type embeddings using the first layer of the knowledge graph. Formal: To figure out how similar two entities are, we do a normalized cosine similarity between their embeddings. Formal: We use the max operator to get the entity type embeddings, which gives us the embeddings for the entity types in the training data. Formal: For the entity type embeddings, we use the normalized cosine similarity, which gives us the embeddings for the entity types in the training data. Formal: This approach to calculating embeddings helps us get the embeddings that are as close to the actual entity type as possible. Formal: To predict the entity type embeddings, we use the first layer of the knowledge graph. Formal: Using the normalized cosine similarity, we calculate the embeddings for the entity types in the training data. Formal: To predict the entity type embeddings, we use the first layer of the knowledge graph. Formal: Formal: We measure the performance using the Frobenius norm, which is a popular way to do normalization. Formal: For the entity type",
        "directory": "acl",
        "filename": "2021.acl-long.192.json"
    },
    {
        "casual_text": "The idea behind word sense discrimination is to group the different ways a word is used in sentences based on its actual meaning. This is usually tackled as an unsupervised learning problem, where all you have is a big pile of text (like in studies by Pedersen and Bruce, 1997, Schütze, 1998, and Purandare and Pedersen, 2004). These methods typically need you to know beforehand how many groups (or clusters) you're looking for, which is often labeled as \"k.\" But in real-world situations, you usually don't know what k should be.",
        "formal_text": "The goal of word sense discrimination is to cluster the occurrences of a word in context based on its underlying meaning. This is often approached as a problem in unsupervised learning, where the only information available is a large corpus of text (e. g. , (Pedersen and Bruce, 1997), (Schütze, 1998), (Purandare and Pedersen, 2004) ). These methods usually require that the number of clusters to be discovered (k) be specified ahead of time. However, in most realistic settings, the value of k is unknown to the user.",
        "GPT2_formal_text": "is a list of entity pairs, each pair being either e1 or e2. The entity pairs are sorted by how similar they are, meaning they're sorted from most similar to least similar. Let's say T is the set of all possible entity pairs that could match up with the document d. Formal: A conditional random field (CRF) model is basically a statistical model that looks at how the probability of a specific label (like a word or entity) changes depending on the specific data it's being used with. Formal: The Conditional Random Field (CRF) model is a cool example of a predictive model that uses conditional probabilities (CPLs). This model has been really successful in a bunch of text classification tasks, like SVM (Hendrickx et al., 2012), CKA (Pang and Lee, 2005), and MAE (Sennrich et al., 2010). \n\nThe main idea behind the CRF model is that it takes into account how a word (or entity) is used, not just what it looks like. Basically, the model treats the word's shape, position, and other features as clues that can help predict what the word is. This helps the model make better predictions about the label.\n\nA good CRF model works with something called latent variables, which represent the probabilities of different label labels. This latent variable can also be a vector (or matrix) that connects different label labels. If you have a vector, say θ, that represents the distribution of label labels, you can figure out the conditional distribution of the label, like a Poisson process (Posterior Information, or PI for short). Formal: CRF models are trained to maximize the likelihood of the correct label for a given document, which can be expressed as the conditional probability of a label. Formal: The model calculates the expected label probabilities for each document. The probability of a specific label, like entity e1, is calculated using this process. Formal: Formal: If a document has more than one entity, the model calculates the expected label probabilities for each entity e1 based on how often they appear together. Formal: The model calculates the expected label probabilities for each document. Formal: For the general CRF model, Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E06",
        "filename": "E06-2007.json"
    },
    {
        "casual_text": "So, there's this issue with \"unknown words\"—basically, words that weren't in the training data—which is a big deal in part-of-speech tagging. In our case, it's even trickier because some words that actually are in the training data might still be treated as unknown due to weird spelling variations. In the experiments we're talking about, we let unknown words be any part of speech (which makes sense in this context), but we give different parts of speech different levels of importance. Specifically, if a word isn't in our word list, we tweak the calculation in equation (2) (check out equation 1 above for reference).",
        "formal_text": "The occurrence of \"unknown words\", i. e. , words not occurring in the training corpus, is a notorious problem in (probabilistic) part-of-speech tagging. In our case, this problem is even more serious, since we know beforehand that some words will be treated as unknown although they do in fact occur in the training corpus (because of deviations Dom standard orthography). In the experiments reported below, we have allowed unknown words to belong to any part-of-speech (which is possible in the given context), but with different weightings for different parts-of-speech. More precisely, when a word cannot be found in the lexicon, we replace the product in (2) (cf. equation 1 above)",
        "GPT2_formal_text": "_layers(d_i, c_i) = input_mask_layers(d_i) ← output_mask_layers(c_i)\n\nHere, d_i is the input representation, and c_i is the output representation. For instance, if you take the input representation for the sentence \"John is tall,\" and use a CNN (or similar) to process it, you get something like the figure in Figure 1.\n\nFigure 1 shows how to create a text representation for \"John is tall.\" This representation is a sequence of tokens. Each token is a word, and the size of the token is usually determined by the size of the smallest window it covers. You can see that the input representation in the figure has three parts: the input sentence, the query vector, and the output representation. The input sentence is represented by a single word and the query vector.\n\nThe output representation is a vector that represents the words in the text. You can think of the output as a sequence of vectors: the input sentence, the query vector, and the output vector. If you want more info on how to get the output, check out the paper by Sun et al. from 2018. Formal: The representation is a matrix of size d_i × m, where d_i is the number of words in the input sentence, and m is the size of the query vector. Formal: The representation is a matrix of size d_i × m. Formal: The output is a vector with size d_i × m. Formal: The output is a vector with size d_i × m. Formal: The output is a matrix with size d_i × m. Formal: The output is a vector with size d_i × m. Formal: The output is a matrix with size d_i × m. Formal: The output is a vector with size d_i × m. Formal: The output is a matrix with size d_i × m. Formal: The output is a vector with size d_i × m. Formal: The output is a matrix with size d_i × m. Formal: The output is a vector with size d_i × m. Formal: The output is a matrix with size d_i × m. Formal: The output is a vector with size d_i × m. Formal: The output is a matrix with size d",
        "directory": "C96",
        "filename": "C96-2192.json"
    },
    {
        "casual_text": "I'm going to expand on some ideas from recent years about underspecified semantic formalisms, like the ones mentioned by Reyle in 1995. The goal is to create an underspecified representation for discourse structure. To do this, I'll use a first-order tree logic developed by Kallmeyer in 1996 to define an underspecified version of SDRT. I'll explain this in more detail in the next sections.",
        "formal_text": "I will develop further ideas introduced by underspecified semantic formalisms which have been proposed in recent years (e. g. (Reyle, 1995) ) in order to provide an underspecified representation for discourse structure. I will employ a first order tree logic by Kallmeyer (1996) to define an underspecifled SDRT, in the following sections.",
        "GPT2_formal_text": "_t; h = h_t; h_t = h_t'; h_t = h_t'+1; } q_t = {q_t'+1, q_t_i, q_t'+1, q_t_m, q_t'+1, q_t', q_t'}; r_t = {r_t'+1, r_t'+1, r_t'+1, r_t'+1}; r_t = {r_t'+1, r_t'+1, r_t'+1, r_t'}; } q_r = {q_r'+1, q_r'+1, q_r'+1, q_r'}; } q_r' = {q_r''+1, q_r'+1, q_r'+1, q_r'}; } q_r'+1 = {q_r'+1, q_r'+1, q_r'+1, q_r'}; } q_r'+1 = {q_r'+1, q_r'+1, q_r'+1, q_r'}; }\n\nBasically, these equations are about mapping the query with the correct answer to the correct relationship, using the embedding and the hash set. The first equation is a bit more specific than the others because it deals with relationships that involve pairs, not individual words. Formal: The last equations handle the relationship between two entities. Formal: The hash set for each query is made by combining the hash for the query and the hash for the entity, following a mapping from hash to hash. Formal: After that, the two hash sets are combined, and the result is a hash. Formal: Finally, this hash is sent to a pre-trained classifier to figure out the probability distribution. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C98",
        "filename": "C98-2189.json"
    },
    {
        "casual_text": "This method ensures that the translation system always hits the exact length requirement (which is a strict rule), but it comes with a big downside. The system doesn't know how many more words it can add until it gets to the required length. So, it can't adjust by making the start of the sentence shorter to meet the limit.",
        "formal_text": "While this approach will guarantee that the output of the translation systems always meets the length condition (hard constraint), it also has one major drawback. Until the system reaches the constrained length, the system is not aware of how many words it is still allowed to generate. Therefore, it is not able to shorten the beginning of the sentence in order to fulfil the length constraint.",
        "GPT2_formal_text": "ed_vector and attention_direction. Formal: This method gives you a hidden vector for each pair of inputs, meaning each input has a hidden representation like f_s_i = h_i. Formal: Similarly, the output from the attention pool at time step t is just the average of all the attention vectors from the vectors in the query. Formal: To compare how similar these representations are, we look at the cosine similarity between the embedding vectors at time step t and the representations at time step t+1. The cosine similarity between two vectors is calculated using the cosine distance from the image to the text, which we call D = D_t×D_t. For instance, if we have the embedding for image i, the similarity between the representation at time step t and the representation at time step t+1 is calculated like this: Formal: In each query response, the attention vectors for the image and the text are combined using a bidirectional LSTM (BiLSTM) to get the representation for each query. Formal: We train the encoder and decoder using the same training data (the target query and image), which we call the training set, R. Formal: During training, we also create a loss function to help the model understand how important the current question is to the current response. Formal: The loss function for the model is shown in Equation 5. Formal: Lastly, we calculate the negative log likelihood (a fancy way of saying it's the model's average) to see how well the model's prediction is matching what the user is actually saying. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "amta",
        "filename": "2020.amta-research.3.json"
    },
    {
        "casual_text": "We set up the generation process as conditional text generation and fine-tune GPT-2 (from Radford et al., 2019) to create a generator called Polyjuice, using pairs of (x, x). To make it easier to focus on specific counterfactuals, we also add control codes, like negation or delete (check out Figure 1B), and use fill-in-the-blank structures (inspired by Donahue et al., 2020) to clearly mark where the changes happen and what kind of changes to make. Our internal evaluation shows that Polyjuice creates outputs that are fluent, varied, and pretty close to the original x. Plus, the control mechanisms help it generate changes that you probably wouldn’t get from standard language models.",
        "formal_text": "We frame the generation step as conditional text generation, and finetune GPT-2 (Radford et al. , 2019) into a generator called Polyjuice using (x, x) pairs. To allow for targeted counterfactuals, we also design control codes like negation or delete ( Figure 1B), and adopt fill-in-the-blank structures (Donahue et al. , 2020) to specify where the perturbation occurs and how. Intrinsic evaluation shows that Polyjuice generatesx that are fluent, diverse, and close to x, and that the control mechanisms retrieve perturbations that would likely not be sampled from off-the-shelf language models.",
        "GPT2_formal_text": "= {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'v', 'w', 'x', 'y', 'z'}; obj = new Object(input_ids); for each i in range(1, n): obj[i, i] = 0; break; } return obj; }\n\nIn the first step, we look for the longest match in the input and add it to the table. Then, we calculate the features for each pair of input and output and add them to the table. We keep doing this until all the matches are gone.\n\nWe also look for the best match in the output and add it to the table. If both input and output have the same value, we just return the value from the first matching step. Otherwise, we add the best match to the table, just like we did in the first step.\n\nFinally, we check if the output is a proper English sentence. If it is, we return 0. But if it's not, we add 1 to the value in the first matching step and add the rest to the table. Formal: We make a list of possible sentence pairs based on the input and output and pick the one that’s the longest. Formal: We look for the longest match in the input and add it to the table. We keep doing this until all the matches are gone. Formal: We calculate the features for each pair of input and output and add them to the table. Formal: We keep doing this until all the matches are gone. Formal: Formal: We check if the output is a proper English sentence. If it is, we return 0. But if it’s not, we add 1 to the value in the first matching step and add the rest to the table. Formal: Formal: Formal: If the output is a proper English sentence, we return 0. But if it’s not, we add 1 to the value in the first matching step and add the rest to the table. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "We tested and looked at the proposed model using the monotonicity subset from Semantic Fragments (Richardson et al., 2020), as well as HELP (Yanaka et al., 2019b) and MED (Yanaka et al., 2019a). We also expanded MED to create a dataset for testing 2-hop inference. The model can successfully learn natural logic operations through end-to-end training. \n\n1 2 Related Work",
        "formal_text": "We evaluate and analyze the proposed model on the monotonicity subset of Semantic Fragments (Richardson et al. , 2020), HELP (Yanaka et al. , 2019b) and MED (Yanaka et al. , 2019a). We also extend MED to generate a dataset to help evaluate 2-hop inference. The model can effectively learn natural logic operations in the end-to-end training paradigm. 1 2 Related Work",
        "GPT2_formal_text": "set to 0. Formal: To get an attention-based summary, you just add the token representation to the attention matrix, and then use the output from the BiLSTM to get the attention vector, e_i. Formal: The BiLSTM is a convolutional neural network, and it uses a three-layer feedforward neural network to process each word. Formal: We're focusing on the attention mechanism (from Bahdanau et al., 2015) that helps the model figure out the attention between sentences, focusing on how to make it work efficiently. This paper also introduces a fancy neural network called an attention-pooling layer. Formal: The big question is, how can we make this better? Formal: Our approach is different from earlier methods like CNNs and GRUs because it uses hierarchical attention. This is different from older methods that just used fixed attention weights. Formal: Here, we explain how we designed the attention-pooling layer, how we adapted it to our specific needs, and what results we got. Formal: We're also introducing a new attention mechanism called Att-Pooling, which helps the model focus on key parts of the input. Formal: This part is about the adaptation part. Basically, it's about how to make the model work better by adapting to our specific needs. Formal: We use the proposed Att-Pooling layer to update the attention vector e_i. Formal: We’ve also introduced a new way to handle the final output representation. Formal: We’ve implemented this new model, and it’s working really well on two different NER datasets, one from Yahoo! and the other from CNNs and GRUs. Formal: We’ve also introduced a new attention mechanism called Att-Att. Formal: Lastly, we’ve developed two different attention methods: Att-Att (from Yang et al., 2018) is like an attention mechanism that uses a bi-attention mechanism. It’s also being applied to different NER tasks. Formal: Formal: To see how Att-Att affects things, we trained an attention network with Att-Att (from Yang et al., 2018) and tested it on two datasets. Formal: For more details, you can check out the original paper. Formal: We’ve also included some extra details about our model, including a detailed explanation of how we adapted it to our needs. Form",
        "directory": "coling",
        "filename": "2020.coling-main.101.json"
    },
    {
        "casual_text": "Our method works really well on standard evaluation metrics. It consistently beats the baseline models, especially on SPICE and CIDEr. For example, our model boosts the AoANet baseline from 118.4 to 119.1 on CIDEr and from 21.5 to 21.7 on SPICE during the MLE phase. It also improves the ATTN baseline on CIDEr from 117.4 to 120.1 and on SPICE from 20.5 to 21.0 during the RL phase. Since CIDEr uses tf-idf weighting, it helps to highlight methods that generate more specific details about images that aren’t common across the dataset. Our method is designed to encourage models to create sentences with more objects, attributes, or relationships, which is why we also see improvements on SPICE.\n\nWhen it comes to descriptiveness-related metrics, our method consistently performs better on R@1 and R@5 in both the MLE and RL phases. The model also improves retrieval performance on the DISC baseline, boosting R@1 from 46.5 to 48.1 and R@5 from 83.6 to 87.9. Our weighted CIDEr reward works well alongside the discriminative loss in DISC, further improving retrieval performance.\n\nWe also used an externally trained NLI model (from Section 3.1) to analyze the relationships between captions generated by our method and the baselines (AoA and DISC) on the test set. Figure 2 shows that our model generates more descriptive sentences.",
        "formal_text": "Results on conventional metrics. Our method consistently outperforms the baseline models on most conventional metrics, especially SPICE and CIDEr; e. g. , the proposed model improves the AoANet baseline from 118. 4 to 119. 1 on CIDEr, 21. 5 to 21. 7 on SPICE in the MLE phase, and improves the ATTN baseline on CIDEr from 117. 4 to 120. 1, SPICE from 20. 5 to 21. 0 in the RL phase. As CIDEr is based on tf-idf weighting, it helps to differentiate methods that generate more imagespecific details that are less commonly occur across the dataset. As our method is designed to encourage models to generate sentences with more objects, attributes, or relations, the effect was also suggested by the improvement on SPICE. Performance on descriptiveness related metrics. Our methods achieve consistently better results on R@1 and R@5 in both the MLE and RL optimization phases. Note that the proposed model can further boost the retrieval performance on the discriminativity enhanced baseline (DISC), improving R@1 from 46. 5 to 48. 1 and R@5 from 83. 6 to 87. 9. Our weighted CIDEr reward is complementary to the discriminative loss item in DISC and further boost the retrieval performance. Labels between generated sentences. We use the externally trained NLI model (Section 3. 1) to further investigate the NLI relationships between the captions generated by our method and by the baselines (AoA and DISC) on the testset. Figure 2 shows that our model generates more descriptive sentences.",
        "GPT2_formal_text": ", and the max_links parameters. Formal: The final model will be made by using a ReLU activation function with a weight of 0.5. Formal: After that, we use a linear layer to turn each query into a list of word embeddings. Formal: In this project, we came up with a cool new way to handle knowledge graph embedding. We also created a new way to figure out which entities are connected to specific entities in a knowledge graph. Formal: Our research showed that using a CNN structure works really well for this task. Plus, using both a CNN and a BERT structure together gives us a much better result. Formal: We also made some new queries to help us model the relationships between entities in the knowledge graph. Specifically, we added three new types of queries to the knowledge graph. Formal: We figured out how to figure out which relationships between two entities are important using a graph similarity method. Formal: Our results show that our method is really effective for this knowledge graph embedding problem. Formal: We also did some experiments to see how much our method helps improve the entity linking model. Formal: We found that adding our method to the entity linking model gave us a big boost in performance. Formal: We also used our method to look into how well a knowledge graph embedding model works. Formal: We found that using the entity linking model can help improve how well an entity linking model works. Formal: Another cool thing we did is create a new dataset for entity linking based on the knowledge graph. Formal: For our next steps, we plan to explore more advanced methods to improve the model's performance. Formal: We also want to work on making the entity linking model better. Formal: Lastly, we want to make this idea more widely used in the knowledge graph community. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-short.36.json"
    },
    {
        "casual_text": "Metrology is like a big umbrella for making sure measurements are consistent across all kinds of science, so we can compare things properly. Computer science has taken some ideas from metrology, especially the concept of reproducibility, but it doesn’t always use the same definitions (as we talked about in Section 2). Here, we’re going to explain quantified reproducibility assessment (QRA), which is basically a method that takes the ideas and definitions from metrology and uses them exactly as they are. This helps us figure out how similar numerical results are and how similar the studies that produced them are. First, we’ll go over the concepts and definitions that QRA is built on, then we’ll give you an idea of the framework (in Section 3.2) and how to actually use it in real life (in Section 3.3).",
        "formal_text": "Metrology is a meta-science: its subject is the standardisation of measurements across all of science to ensure comparability. Computer science has long borrowed terms, most notably reproducibility, from metrology, albeit not adopting the same definitions (as discussed in Section 2 above). In this section, we describe quantified reproducibility assessment (QRA), an approach that is directly derived from the concepts and definitions of metrology, adopting the latter exactly as they are, and yields assessments of the degree of similarity between numerical results and between the studies that produced them. We start below with the concepts and definitions that QRA is based on, followed by an overview of the framework (Section 3. 2) and steps in applying it in practice (Section 3. 3).",
        "GPT2_formal_text": "ing_x_t1 = (V_t1, x1_1, ..., x_t_t-1) v_t = V_t1, x1_1, ..., x_t-1) ... ...  \n\nThat's it for this part. Formal: Basically, the attention matrix A_t_i is a term-by-term thing that maps the vectors from s_i to t_i. Formal: The attention matrix A_t_i is a term-by-term thing that maps the vectors from s_i to t_i. Formal: The attention matrix A_t_i is a term-by-term thing that maps the vectors from s_i to t_i. Formal: We'll call the attention matrix A_t_i t_i, and we'll use A_t_i to represent the attention state vector. Formal: The attention vector A_t_i is a term-by-term thing that maps the vectors from s_i to t_i. Formal: We'll use A_t_i to represent the attention state vector. Formal: We'll use A_t_i to represent the attention state vector. Formal: We'll use A_t_i to represent the attention state vector. Formal: And we'll wrap up with a formula that gives us the final result. Formal: Formal: We'll use A_t_i to represent the attention state vector. Formal: We'll use A_t_i to represent the attention state vector. Formal: We'll use A_t_i to represent the attention state vector. Formal: We'll use A_t_i to represent the attention state vector. Formal: We'll use A_t_i to represent the attention state vector. Formal: And we'll wrap up with a formula that gives us the final result. Formal: Formal: Formal: We'll use A_t_i to represent the attention state vector. Formal: We'll use A_t_i to represent the attention state vector. Formal: We'll use A_t_i to represent the attention state vector. Formal: Formal: We'll use A_t_i to represent the attention state vector. Formal: We'll use A_t_i to represent the attention state vector. Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.2.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way.\n\nWe're dealing with simple sentences, where each sentence has a bunch of words (called \"case fillers\") followed by a verb. Our job is to figure out the meaning, or \"sense,\" of each verb in the sentence. To do this, we use a list of verb senses from a dictionary called \"IPAL\" (from 1987), which also includes examples of these case fillers.\n\nWe also use a method by Kurohashi to measure how similar two case fillers are, or more specifically, how similar the main nouns in those case fillers are. The similarity is based on something called the \"length of the path\" between two nouns in a structure called \"HPSG\" (don't worry too much about that). This similarity is calculated using a formula, and we use a tool called \"Hyokiyo\" (from the National Language Research Institute, 1964) to help with this.\n\nFollowing Kurohashi's method, we define a similarity score between two words, X and Y, as shown in Table 1. It's important to note that this method works independently of the resources we use.\n\nTo show how the whole process works, we'll use some examples mentioned earlier and compare them to a more general case, as shown in Figure 4. The input we're working with is a set of case fillers and a verb, like {nc, 'mc), nc: 'm. ce, v}. The possible meanings (or \"senses\") for the verb v are s1, s2, and s3, and we get these from a database.",
        "formal_text": "We assume that inputs ~re simple sentences, e~mh one of which consists of a sequellce of eases fl)llowe. d by their governing verb. The. task is to identify the sense of each input verb. The set of verl) senses we use are those defined in the existing machine re~tdal)le (li(: ti()llary \"IPAL\" (IPA, 1987), which also (: olltains example case fillers as shown in figure. t. As well as Kuroh~tshi's method the similarity between two (: as(; tillers, or more pre-('isely the semantic-head nouns of them, is corn- 1: The relation I)t'. tweell the length of path I)e-|; ween two i[()llns A\" {Mid Y (lt: 7/, (. k', }: )) ill IJtL: l~r, Lil: o'ibye and the similarity hetween them (. sirn(X, Y)) [~a. n(X, Y) l 0, . . : 2 9468 l012 t [ s. zm(A, ~ ) tl 10 8 7 5 0 tinted by using IIv, rwuigoih, yo (National-Language l{esearch lnstil; ute, 1964). Following Kurohashi's method, we define. sim(X, ~), whi(: h stands for the silnilarity 1)etween words X mM Y, as in tattle 1. It should he noted here that both nl(~t; h()ds ~tre theoreti(: ally indel)endent of wh; tt resources }ire use(t. ~lb illustl'~te tit(; overall a. lgorithm, we r(~t)la. (: (~ the illustra. tive cases mentioned in section 1 wilh a slightly re(ire gelmral case as in figure. 4. The iut)ut is {nc, -'mc), nc: 'm. ce, v}, where he. i all! notes the case filler in the case ci, a. nd 'ntc~ denotes the case maker of <: i. The candidates of ilH; (~rl)ret; ttion for v, which ~re, sl, , s2 ~md s3, are deriv(; d froln the datal)ase. The.",
        "GPT2_formal_text": "\", \"query_ids\", \"query_ext\") } } ], \"extension_ids\": [], \"extension\": [], \"parameters\": [], \"values\": [] } [/block] Formal: We're comparing our model to two other methods: Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and SVM-LDA. Formal: We're comparing our model to two other methods: LogReg and S",
        "directory": "C96",
        "filename": "C96-1012.json"
    },
    {
        "casual_text": "So far, we've been talking about semantic spaces that are built using how often words appear together. But, the way words are spread out across different documents can also give us some pretty useful information about their meanings. Latent Semantic Analysis (LSA), which was introduced by Landauer and Dumais in 1997, uses exactly this kind of distribution information to figure out hidden semantic factors by simplifying the data (a process called dimensionality reduction).\n\nTo do this, LSA uses a method called Singular Value Decomposition (SVD), developed by Berry and his team in 1994. SVD takes a big table (matrix) that shows how often words and documents appear together and breaks it down into smaller, simpler pieces. One of these pieces shows words in terms of these hidden factors, and another shows documents in the same way.\n\nThis setup allows us to see that any document can be thought of as a mix (linear combination) of the words it contains. So, in this way of looking at things, it makes sense to treat groups of words as if they were a mini-document and represent them by mixing the vectors of the individual words.",
        "formal_text": "The semantic space discussed thus far is based on word co-occurrence statistics. However, the statistics of how words are distributed across the documents also carry useful semantic information. Latent Semantic Analysis (LSA, Landauer and Dumais (1997) utilizes precisely this distributional information to uncover hidden semantic factors by means of dimensionality reduction. Singular value decomposition (SVD, Berry et al. (1994) ) is applied to a word-document co-occurrence matrix which is factored into a product of a number of other matrices; one of them represents words in terms of the semantic factors and another represents documents in terms of the same factors. The algebraic relation between these matrices can be used to show that any document vector is a linear combination of the vectors representing the words it contains. Thus, within this paradigm it is nat-ural to treat multi-word structures as a \"pseudodocument\" and represent them via linear combinations of word vectors.",
        "GPT2_formal_text": "_t a h 1, . . . , a t j, for each word w i in the text, using a max-pooling layer with a window size of d = d t. Formal: To get an accurate classification of the type of word w i, the model can't just guess based on the text itself. So, we use a greedy search process to figure out the right attention parameters. Formal: The max-pooling layer in the bidirectional attention network is set up to maximize the log-likelihood of the sentence tokens y i. Formal: We use a bidirectional LSTM (Bi-LSTM) to create a sequence of hidden representations h x t for each token in the target sentence t. Formal: At each step, we calculate the log-likelihood for the target token sequence h x t by using the log probability distribution p(x t |h x t ). Formal: Formal: This greedy search process helps the model find the best attention parameters (or weights) for the sequence of hidden representations h x t. Formal: We set up an adversarial attack using the target sequence and the token sequence to check how well the model's predictions hold up. Formal: This approach helps us gather extra information, like the dependency tree of the target word and the tree structure of its parent word. Formal: Lastly, we use a graph attention network to estimate the probability distribution p(y t |x t ) using the target sequence y t and the tree structure of its parent word x t. Formal: In our experiments, we use the same train and test sets, and we compare how well our model performs using three different evaluation metrics. Formal: We check the model's performance using the F1-score on the development set and the average F1-score on the test set. Formal: This whole process can be seen as a gradient-based optimization problem. Formal: For more info, check out Appendix A. Formal: The full model setup is laid out in Figure 1. Formal: The model is built using PyTorch (Paszke et al., 2019). Formal: Training the model takes about a day and a half on a single Tesla K80 GPU. Formal: To make sure the model can handle the task and perform well, we added some extra parallel data by using a combined beam with a size of 4 and a batch size of 64. Formal: Formal",
        "directory": "D09",
        "filename": "D09-1045.json"
    },
    {
        "casual_text": "We talked about how MT research is different from other frameworks in NLU, and we pointed out that one unique thing about MT as an NLP application is that we can't easily define a specific task-oriented level of 'understanding' like we can with other applications.",
        "formal_text": "we have discussed what makes MT researches different from other frameworks in NLU, and we have stressed that one of the peculiarities of MT as an NLP application is that we cannot readily set up a particular task-oriented level of 'understanding' in MT as we can in other applications.",
        "GPT2_formal_text": ", which have the same values as the actual labels. Formal: This part adds the feature representation to the input sequence and uses the attention mechanism from a BiLSTM to create a vector representation for the sentence. After that, the input sequence is decoded and passed into a neural network to get a probability distribution for the target label. Formal: This part adds the feature representation to the input sequence and uses the attention mechanism from a BiLSTM to get a vector representation for the sentence. After that, the input sequence is decoded and passed into a neural network to get a probability distribution for the target label. Formal: This part combines the feature representation with the input sequence to generate the output. Formal: The final output from the BiLSTM-based model is the probability for the target label. Formal: This part combines the feature representation with the input sequence to generate the output. Formal: Formal: We use a BiLSTM to encode the sentence, using the attention mechanism to get a vector representation for the sentence. This vector is then sent into a neural network to get the probability for the target label. Formal: We use a BiLSTM to encode the sentence, using the attention mechanism to get a vector representation for the sentence. This vector is then sent into a neural network to get the probability for the target label. Formal: We use a BiLSTM to encode the sentence, using the attention mechanism to get a vector representation for the sentence. This vector is then sent into a neural network to get the probability for the target label. Formal: We use a BiLSTM to encode the sentence, using the attention mechanism to get a vector representation for the sentence. This vector is then sent into a neural network to get the probability for the target label. Formal: This is just a fancy way of saying we use a BiLSTM to encode the sentence and the attention mechanism to get the probability for the target label. Formal: This is the same setup as the last method, but we have some extra features to explain. Formal: These features are like extra rules that we add to our input to help the model learn better. Formal: This is the same setup as the last method, but we have some extra features to explain. Formal: We use a BiLSTM to encode the sentence, using the attention mechanism to get a vector representation for the sentence. This vector is then sent into a neural network to",
        "directory": "C86",
        "filename": "C86-1155.json"
    },
    {
        "casual_text": "To check how accurate ACT is when using word alignment, we tested it on a new batch of 200 sentences from the UN EN/FR corpus (not the same ones as before), which had 207 instances of seven different discourse connectives. Just like we did for the first version of ACT (before we added the disambiguation module), we looked at each of the six cases and counted how many times the scoring was right or wrong. The results were: case 1: 64 correct, 0 wrong; case 2: 64 correct, 3 wrong; case 3: 33 correct, 4 wrong; case 4: 1 correct, 0 wrong; and case 6: 0 correct, 0 wrong. Out of the 38 sentences in case 5, 21 were actually correct translations. So, the ACTa score was about 10% off in the first version of ACT, but now it's pretty much spot on. ACTa5 and ACTm were both about 2% off before and are now only 0.5% off. This shows that word alignment makes the ACT metric more accurate.",
        "formal_text": "In order to estimate the accuracy of ACT by using word alignment, we manually evaluated it on a new subset of 200 sentences taken from the UN EN/FR corpus (different from the first one), with 207 occurrences of the seven discourse connectives. As done for the first version (before adding the disambiguation module) of ACT, we counted for each of the six cases the number of occurrences that have been correctly vs. incorrectly scored. The results were, for case 1: 64/0, case 2: 64/3, case 3: 33/4, case 4: 1/0, and for case 6: 0/0. Among the 38 sentences in case 5, 21 were in fact correct translations. Therefore, the ACTa score was about 10% lower than reality in the initial version of ACT and now is approximately the same, while ACTa5 and ACTm were both about 2% lower and now is 0. 5%. Word alignment thus improves the accuracy of the ACT metric.",
        "GPT2_formal_text": "ed(d, e, t, w) = s_masked_t * d_masked_t + s_masked_w * w; s_masked_t = s_masked_t * d_masked_t + s_masked_w * w;\n\nBasically, it's the sum of s_masked_t for each token in d, and it's calculated using a linear function with d_masked_t and s_masked_w. Formal: For each token t in the text, we figure out the attention vector by taking the logarithm of the dot product between s_masked_t and the probability vector (P_t). Formal: For each token t in the text, we calculate the attention vector by taking the logarithm of the dot product between the likelihood vector P_t for that token and the probability vector P_t. Formal: For each token t in the text, we calculate the attention vector by taking the logarithm of the dot product between the likelihood vector P_t for that token and the probability vector P_t. Formal: We also calculate the attention vector for each token t in the text by taking the logarithm of the dot product between the likelihood vector P_t for that token and the probability vector P_t. Formal: Formal: Finally, we calculate the attention vector for each token t in the text by taking the logarithm of the dot product between the likelihood vector P_t for that token and the probability vector P_t. Formal: The final attention vector for a token t in the text is calculated by the logarithm of the dot product between the attention vector for that token and the attention vector for the previous token. Formal: Formal:\n\nWe're using the same setup as in Eq. 1 for the attention computation. Formal: Here's how we calculate the attention vector for each token t in the text: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "amta",
        "filename": "2012.amta-caas14.1.json"
    },
    {
        "casual_text": "vp(X, P) means that for a verb phrase with position X and predicate P, it can either be broken down into a transitive event (tranev) with positions X and Y and a plural form Pl, followed by a noun phrase (np) with position Y, plural Pl, and predicate P. Or, it can be a transitive non-core argument verb phrase (tncransv) with position X and predicate P. Figure 2 shows an example sentence in the language this grammar recognizes, along with its surface structure and the semantic structure created by the grammar.",
        "formal_text": "vp(X, P) -> tranev(X, Y, Pl), np(Y, Pl, P). vp(X, P) -> tncransv(X, P). Figure 2 gives a sentence in the language recognized by thls grammar together wlth the associated surface syntactic structure and the semantic structure built by the grammar.",
        "GPT2_formal_text": "= {u1, u2, ..., un}  \n\nwhere m is the number of unique words in the document. Formal: The attention weights are also set up based on the document's vocabulary, but this time, each word gets its own vector. Formal: The whole model gets trained using a big network with a fully connected layer and a linear layer. Formal: The model's parameters are trained using a modified version of the Adam optimizer (from Kingma and Ba, 2015) with a learning rate of 0.001. Formal: We use a version of the Adam optimizer that works with word vectors instead of word embeddings. Formal: Instead of using gradient descent, we use a root-finding algorithm to keep the gradients of the vector space accurate. Formal: The output of the network is a list of sentence embeddings. Formal: A sequence is considered a unit if it has just one word. For example, the sentence \"John went to bed\" is a unit, and \"he\" is also a unit. Formal: For each sentence in the document, we calculate the context vector. Formal: We add the normalized context vectors to the output vector to get the output. Formal: Let's say w_s is the word embedding for a sentence s. Formal: For each word in the sentence, the context vector is a sequence of word vectors. Formal: In a document, the context vector of a word is just the normalized embedding of that word. Formal: A sequence is a unit if it has just one word. Formal: For each word in the document, the context vector is a sequence of word vectors. Formal: A word is a unit if it has just one word. Formal: For each word in the document, the context vector is a sequence of word vectors. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "A83",
        "filename": "A83-1010.json"
    },
    {
        "casual_text": "When you map an NDA onto this network, pairs of NDA states (q) and input symbols (x) where δ(q, x) ≠ ∅ get turned into activity patterns. The network's temporal relationships then handle the NDA transitions. Each NDA transition corresponds to a single network transition. This setup creates complex representations of both NDA states and input symbols. \n\nAn NDA state is represented by all the activity patterns that include that state, and input patterns are represented by combining (OR-ing) all the activity patterns that contain that input symbol. One cool thing about this network is that it naturally handles mixed temporal images, which are kind of like the subthreshold version of mixture states. The temporal image of an active overlap includes at least all the activity patterns representing the next state.\n\nBut that's not all. The network also acts like it's running the deterministic version of the NDA, meaning it explores all possible paths through the state space that the input allows, all at once. The representations of the states in this deterministic finite-state automaton (FSA) are built dynamically as the network runs. These are called mixed temporal images.\n\nThe idea of \"dynamically constructed representations\" comes from Touretzky [9], who actually argued that they couldn't exist in today's neural networks, like Hopfield models.",
        "formal_text": "When an NDA is mapped onto this network, pairs of NDA-state q and input-symbol x, such that 6(q, x) y£ {~, are mapped onto activity patterns. Temporal relations in the network then serve to implement NDA transitions. Note that single NDA transitions arc mapped onto single network transitions. This results in complex representations of the NDA states and the input symbols. An NDA state is rap resented by all activity patterns that represent a pair containing that state, and input patterns are represented by a component-wise OR over all activity patterns containing that input symbol. A consequence is that mixed temporal images, the subthreshold analogue of mixture states, are a very natural phenomenon in this network, because tile temporal image of an active overlap comprises at least all activity patterns representing a successor state. But this is not all. Also the network will act as if it implements the deterministic equivalent of the NDA, i. e. it will trace all paths through state space the input allows for, concurrently. The representations of the states of this deterministic finite-state automaton (FSA) are dynamically constructed along the way; they are mixed temporal images. The concept of a \"dynamically constructed representation\" is borrowed from Touretzky [9], who, by the way, argued that they could not exist in the current generation of neural networks, such ms ltopfield models.",
        "GPT2_formal_text": "_x. The model that focuses on the x-axis and assigns importance to tokens at the token level is called Attention Separate Representation Learning (ASR). Formal: To figure out the probability p(t|s) for each subtask t, we use a perceptron model (check out Figure 2) that's been tweaked for different tasks, like dialogue and sequence generation. Formal: The '-' thing just means we're keeping things simple. Formal: For the dialogue task, we’re using the dialogue model from (Xu et al., 2020). It’s trained on the VQA dataset by looking at the context and the time of the response. Formal: To check if the answer is valid and to generate the next question, we’re using the model from (Wang et al., 2020). In this case, it’s trained on the DevSub dataset. Formal: For the sequence generation task, we’re using the model from (Wu et al., 2021). Formal: For the dialogue task, we’re using the dialogue model (Xu et al., 2020). Formal: For the sequence generation task, we’re using the dialogue model from (Wu et al., 2021). Formal: To figure out the probability p(t|s) for each subtask t, we use a perceptron model (see Figure 2). Formal: Formal: Formal: For the dialogue task, we’re using the dialogue model (Xu et al., 2020). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C92",
        "filename": "C92-1021.json"
    },
    {
        "casual_text": "On the flip side, the backward mode kicks in when someone asks the system a question. Usually, for the sake of efficiency, it’s better not to dump all the inferred info into the knowledge base right away. S-rules in backward mode only activate when a question comes up that can’t be answered straight off. They then start digging through the knowledge base to see if they can figure out the answer based on what’s already there. For instance, the Present S-rule in backward mode doesn’t trigger when sentence (27) is processed and T-expression (28) is generated by START. It only triggers when question (29) is asked, because that’s something the system can’t answer directly.",
        "formal_text": "In contrast, the backward mode is employed when the user queries the knowledge base. Often for reasons of computational e/l~cleney, it is advantageous not to incorporate all inferred knowledge into the knowledge base immediately. S-rules in the bac: kward mode trigger only when a request comes in which cannot be answered directly, initiating a search in the knowledge base to determine if the answer can be deduced from the available information. For example, the Present S-rule used in the backward mode does not trigger when sentence (27) is read and T-expression (28) is produced by START. The S-rule triggers only when question (29) is asked since this question cannot be answered directly.",
        "GPT2_formal_text": "[0, 1] = (i, j). Formal: The total of all document-related losses for the current epoch. Formal: For multi-label classification, we calculate the cross-entropy loss for the i-th label and the j-th word in the input document. Formal: We take the cross-entropy loss for the i-th label and the j-th word in the input document and then we add them together. Formal: We use a simple linear kernel function to calculate cross-entropy loss across all the labels. Formal: For the task of classifying the top-k items, the weights for the top-k predicted labels are: Formal: For the task of classifying the top-k items, the weights for the top-k predicted labels are: Formal: We're suggesting a way to use the top-k predicted labels and the cross-entropy loss. Formal: We're proposing a method to use the top-k predicted labels and the cross-entropy loss. Formal: The cross-entropy loss and the top-k predicted labels are calculated as: Formal: The cross-entropy loss and the top-k predicted labels are calculated as: Formal: In our experiments, we evaluate the performance of classifiers using the validation set. Formal: We evaluate the performance of classifiers using the validation set. Formal: We evaluate the performance of classifiers using the validation set. Formal: For this paper, we’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We’re using the same hyperparameters as the original paper. Formal: We",
        "directory": "C88",
        "filename": "C88-1065.json"
    },
    {
        "casual_text": "For the extended version of Lin-EBMT, called Lin-EBMT REC+, they added some stuff from the template-based EBMT approach during the recombination step. The first two steps are still the same as before.",
        "formal_text": "In the implementation of the extended version of Lin − EBM T (i. e. Lin − EBM T REC+ ) ideas from the template-based EBMT approach are incorporated in the recombination step. The previous two steps 6 remain unchanged.",
        "GPT2_formal_text": "ed, 1, 1, 1, 1). Formal: We also noticed that the model might not perform as well when the attention is trained with different amounts of data. For instance, when training with just 100 or 1000 examples, the model can't fully capture the main topic. On the other hand, when we add up to 2 million examples, the model can really nail the topic. Figure 1 shows the formula for the softmax layer in an LSTM, along with the hidden representation for the probability distribution for that hidden layer. Formal: We tested different settings for the number of parameters in the model: 1, 2, 4, 8, 16, 32, 64, 128, 256, 512, 1024, 2048, and 4096. We tried all combinations of 4, 8, 16, and 32 parameters. Formal: We used the code from the work by Huang et al. (2017). Check out Table 1 for the training and testing results. Formal: For training and testing, we used the CMRC dataset (Zhang et al., 2018), which has 200,000 posts from the Reddit dataset. Formal: We used the code from the work by Huang et al. (2017). Formal: We used the MEGA-DT dataset (Lin et al., 2018), with 800,000 posts from the Yahoo! Answers dataset. Formal: We used the MEGA-DT dataset, with 800,000 posts from the Yahoo! Answers dataset. Formal: We used the MEGA-DT dataset, with 800,000 posts from the Yahoo! Answers dataset. Formal: We used the code from the work by Huang et al. (2017). Formal: We used the MEGA-DT dataset, with 800,000 posts from the Yahoo! Answers dataset. Formal: We used the MEGA-DT dataset, with 800,000 posts from the Yahoo! Answers dataset. Formal: We used the code from the work by Huang et al. (2017). Formal: We used the MEGA-DT dataset, with 800,000 posts from the Yahoo! Answers dataset. Formal: We used the MEGA-DT dataset, with 800,000 posts from the Yahoo! Answers dataset. Formal: We used the MEGA-DT dataset, with 800,000 posts from the Yahoo! Answers dataset. Formal: We used the code from the work by Huang et al. (2017). Formal: We used the MEGA-DT dataset, with",
        "directory": "eamt",
        "filename": "2011.eamt-1.27.json"
    },
    {
        "casual_text": "Wow, this looks like a bunch of random symbols and characters! It seems like some kind of code or encrypted message. I can't really make sense of it in its current form, but if you have a specific question or need help decoding it, let me know! Otherwise, it's a bit tough to work with as is.",
        "formal_text": "0 ¤ 8 4 þ ¥ § ¤ ¦ ' ý 9 ¥ ¢ º ý ¢ o ü v H þ A & © A ¢ Î ü 1 @ ü ¦ A ¢ Ê ü ¦ Ê þ ¥ # b ( £ ¤ ¦ ü ¦ I e ü ¦ A ¢ ¤ ¦ § A! Hþ ¥ 4 7 ¢ $ P ) ¥! 0 © ¥! 0 © A ¢ \"! Y ¤ ü ¦ A ¢! ¦ ¢ # Ê þ b ü ¦ H b 1 4 7 g ü 1! C þ ¥ ¤ ü ¦ I 9 ¥ ¢ a 4 7 o ü 1 ¢ 7 6 2 ü ¦ § þ ¥ # } £ § 3 © A ¢ ¤ ¦ ¤d & U ¦ Q ¢ e § ¤ ¦ ¢ Þ ü ¦ ¢ ¢ 7 6 & (! ¦ ¢ ¤ 0 ¤ ¦ I b 1 4 7 g ü 1! 4 þ ¥ ¤ z ü ¦ I 9 ¥ ¢ i ¥ ( 3 4d H ¥! ¤ ¦ § 4 V Ù þ A 2 © ¢ C © A ¢ A ü 1 ¢ © d q ü ¦ A ¢ e ¢ 7 6 2 þ ý ( g # I ¢ ¤ ` s ÿ v S V F P 8 s 4 0 ¤ 4 0 t þ B! C þ ¥ 4 n ü 1 ¢ \"! 0 I § \" ¢ © t s ÿ ¤ 1 ¢ \" 9 ¥ ¢ \"! C þ ¥ # w ¤ 1 ( £ ¢ 4 \" I h g 4 H¢ º þ % ü ¦ § A! ¦ ¢ ¤ ¾ þ ¥ #ü ¦ A § ' a Ê ü # s I ¢ ¤ h ü ¦ ¢ i ¥ ( 3 4 R ( t þ B! C ü 5 H þ Y ¤ 1 ¢ & % ü 1 ¢ 4 7 ¢ ¥ F ¥ ü ¤ b ¢ 4 7 ¢ ¤ ¦ ¤ þ B! ÿ Î ü 1 8 § ¤ ¦ ¢ & þ v ¤ z ü 1! ¦ ' ý ¥! H # ' 4 þ ¥ # H ¥! C ý H ü ¦ A ¢ u 4 7 o ü 1! C þ ¥ ¤ z ü ¦ I 9 ¥ ¢ $ A 2 © ¢ Y s ¤ )! 0 ¢ \" (! ¦ ¢ ¤ 1 ¢ o ü 1 ¢ © X o ÿ þ (! ¦ § C 4HU ¢ 7 6 @ U m g S 0 þ ¥ © ª ü a 4 þ ¥ 4 þ B! ¦! C ÿ ü ¦ A ¢ Þ ü i ÿ 2 ( A % 4 þ ¥ #! V ¤ ¦ ' b 1 4 7 o ü 1! C þ ¥ ¤ z ü ¦ I 9 ¥ ¢d j ¤ z ü 1! 0 ¢ ¤ ¦ ¤ \" v ¤ 1 ¢ º ý þ ¥ o ü ¦ 4 þ ¥ # #ÿ ¥ F 5 ü! ¦ ¢H¢ \"! 0 ¤ ü 1 r þ 4 V A 4 7 ¢ H! ¦ ¦ ý 8 þ ¤ 1 ¢ ü v H þ ¥ #ü 1 ¢ \"! 0 þ b ü ¦ I 9 ¥ ¢ ¤ þ ¥ © ü 4 þ ¥ X £ ¢ 4 7 g ü 1! 4 þ ¥ ¤ z ü ¦ I 9 ¥ ¢ u! ¦ ¢ # Ê þ b ü ¦ I ü 1 ¤ 1 ¦ ý ¢ D ( $ þ B! ü ) H ü ¦ A ¢ w ( 3!",
        "GPT2_formal_text": "= {s1, s2, ..., sm}, where s is the title and sm is the summary. Formal: We use an unsupervised method for evaluating summarization, which is outlined in section 3.1. First, we train the model using a bunch of prompts and some automatically generated ones. Then, we use two types of language models—one that checks for consistency (called consistency regularization) and another that focuses on predicting probabilities, which we call probabilistic language models. After that, we use a probabilistic model to rank sentences based on their summary quality. Formal: To figure out if two sentences are actually equivalent, we treat them as a pair and rank them accordingly. Formal: We also tested out a cross-entropy loss loss function to see how it affects the ranking. Formal: Lastly, we ran some experiments to see how different loss functions affect the results. Formal: So, here's what we found: Formal: We show that these summarization models work better when they use a consistent prompt and some ranked summaries. Formal: We also tried out a bunch of loss functions on top of the ones we mentioned. We found that the LPLM was the top choice for summarization, and the probabilistic ones didn't really help much for summarization. Formal: We ran some experiments to see how different loss functions affect the rankings. Formal: The results are in Table 1. The top-1 and top-5 results are based on the original (original) summarization, while the bottom-1 and bottom-5 results are based on the summarization generated by the summarization model. Formal: We found that the LPLM and probabilistic models were the most effective for summarization, but the LCRF and SVM did a bit better. Formal: We also looked at the average results for different loss functions. Formal: We found that the LPLM and probabilistic models were the most effective, but the LCRF and SVM performed better than the LPLM and probabilistic models. Formal: Overall, the word embeddings from the model that were used for ranking worked really well. Formal: Lastly, we tried out some different features for ranking. Formal: Formal: We looked at the average results for different features for summarization, including the ones that were used for ranking. Formal: Formal: We also mentioned that the best feature combination",
        "directory": "C04",
        "filename": "C04-1042.json"
    },
    {
        "casual_text": "Each part gets translated or edited individually, and there are four different ways to move around the document:",
        "formal_text": "Each unit is translated/edited one at a time and navigation through the different segments of the document can be achieved in four ways:",
        "GPT2_formal_text": "(x) = x[i, i+1]  \n\nThen, we end up with:\n\n1 = h_enc * (1 - h_dec) / (1 - h_enc)\n\nSo, it's just multiplying the two parts together. Formal: Let's break down the setup of our model in a simpler way: Formal: The main idea of our model is to figure out the attention vector for a specific word or token, called w_i. The way we do this is shown in Figure 1. Formal: We use a linear kernel function and a cross-entropy loss function to measure how well we're doing. Formal: We get a lot of good results by making the model more accurate and using multiple layers, which helps us figure out how important a word or token is. Formal: We also use a convolutional neural network (CNN) to find the most important features for the word or token. Formal: When we have an input sentence x, our model uses a single feed-forward network to create a representation for the word w_i. Formal: The output of the network is a representation of the token's contextualized representation, which is h_enc_t. Formal: After that, we apply the cross-entropy loss function to the token's representation. Formal: To get the attention vector for a specific token, we use a cross-entropy loss function called h_enc_t. Formal: The output of this function is a representation of the token's contextualized representation, h_enc_t_t. Formal: Finally, we pass this into a fully connected layer to get the output for the token, which is h_enc_t. Formal: In this case, h_enc_t is the attention vector for the token. Formal: The final word-level representation, h_enc_t_t, is calculated based on this. Formal: So, we end up with a final representation of the token, h_enc_t_t. Formal: Formal: Formal: The whole process of creating these representations is shown in Figure 1. Formal: Figure 1: The process of how we create the contextualized representations for a word or token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "Soo (2018) and Blinov and his crew (2019) looked into whether a text snippet is just a one-liner. Meanwhile, Zhang and Liu (2014), Ortega-Bueno and friends (2018), and Chiruzzo et al. (2019) dove into the task of classifying humor in tweets. Castro and his team (2018) gathered humor ratings and funniness scores for Spanish tweets using crowdsourcing. Chiruzzo et al. (2019) even came up with a regression task to predict a tweet's humor score. Li and his colleagues (2020) collected Chinese internet slang and mixed it with a humor detection method to analyze the sentiment of Weibo posts. It's worth mentioning that all the examples in the datasets used or created in these studies are separate from each other. Since our dataset is based on full scripts, the annotated lines and text spans could be useful for researchers working on algorithms that understand humor in longer contexts. \n\nBesides studies on short text snippets, Bertero (2019) and Hasan and his team (2019) made datasets from TV sitcoms like The Big Bang Theory and TED talks, respectively. Their aim was to predict if a sequence of texts would trigger immediate laughter. Yang and his team (2015) extracted key words like \"sing,\" \"sign language,\" and \"pretty handy\" from jokes, which is kind of like what we did with our information extraction annotations.",
        "formal_text": "and Soo (2018); Blinov et al. (2019) investigated if a text fragment is a one-liner. 3 Zhang and Liu (2014); Ortega-Bueno et al. 2018; Chiruzzo et al. (2019) explored the humour classification task on tweets. Castro et al. (2018) collected humour values and funniness scores of Spanish tweets by using crowdsourcing. Chiruzzo et al. (2019) proposed a regression task that predicts the humour score for a tweet. Li et al. (2020) collected Chinese Internet slang expressions and combined them with a humor detecting method to analyse the sentiment of Weibo 4 posts. It should be noted that the examples in all of the corpora used or constructed in the above-mentioned studies are independent of each other. Since our corpus is based on entire scripts, the annotated lines and text spans might also benefit the researchers who are interested in modelling long-context-aware algorithms to understand humour. Apart from the studies on short text fragments, Bertero (2019) and Hasan et al. (2019) created corpora from television (TV) sitcoms such as The Big Bang Theory 5 and TED talks 6 respectively. Their goal is to predict whether or not a sequence of texts will trigger immediate laughter. Yang et al. (2015); extracted the key words such as sing, sign language and pretty handy from jokes, which are similar to our information extraction annotations.",
        "GPT2_formal_text": "= k[i] if c in set(len(A)), else 0. If a[i] equals k, and t and c are both in set(len(A)), then a_i is set to k. In the second case, all the p's in A are sorted from 1 to p, except for the second one (which is from 1 to p). So, t_i is just the intersection of t_1 and t_2. Formal: In the third case, we only know the k-th word in A. So, t_i is just the intersection of t_1 and t_2. Formal: In the fourth case, both p's are sorted from 1 to p. So, the same goes for t_1 and t_2. Formal: Finally, for the fifth case, we figure out the p's by checking the union of all the p's in A. Formal: Formal: But, there's a catch: this method doesn't give us a clear way to compare the different ranked candidates. So, we still need a more flexible method that can handle this. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "codi",
        "filename": "2020.codi-1.5.json"
    },
    {
        "casual_text": "We calculated inter-annotator agreement (IAA) separately for the second and third stages since they have different goals. In the second stage (cluster quality), we got an average Spearman correlation of r_s = 0.73, which is similar to what other studies in topic modeling have reported (Newman et al., 2010, with r_s = 0.73/0.78, and Aletras and Stevenson, 2013, with r_s = 0.70/0.64/0.54). We also got an average Cohen's Kappa of κ = 0.48, which is considered moderate agreement. For the third stage (issue identification), we found κ = 0.36, which is fair agreement.\n\nLooking at disagreements in stage 2, only 2% came from differing opinions on Good-Bad clusters. Most disagreements were about Good-Intermediate (37%) and Intermediate-Bad (61%) cases. This is a good sign because it shows that annotators rarely have completely opposite views on cluster quality. They mostly agree on what makes a cluster coherent, which is the main point of this task.\n\nFor issue identification, most disagreements (49%) were about distinguishing Intermediate-Chained cases. This makes sense because it can be tricky to identify subclusters in the first stage. For the final decision-making process, we found that there was always a majority, so we went with the label that at least two out of three annotators agreed on.\n\nTable 1 gives an overview of the corpus size, coherence quality, and issues identified for COVID-19 and Election. Check out Appendix C for more details.",
        "formal_text": "Inter-annotator agreement (IAA) was computed separately for the second and third stages as they serve a different purpose. For the second stage (cluster quality), we obtain average Spearman correlation r s = 0. 73 which is comparable to previous coherence evaluation scores in topic modelling literature ( (Newman et al. , 2010) with rs = 0. 73 / 0. 78 and (Aletras and Stevenson, 2013) with rs = 0. 70 / 0. 64 / 0. 54) and average Cohen's Kappa κ = 0. 48 (moderate IAA). For the third stage (issue identification), we compute average κ = 0. 36 (fair IAA). Analysis of pairwise disagreement in stage 2 shows only 2% is due to division in opinion over Good-Bad clusters. Good-Intermediate and Intermediate-Bad cases account for 37% and 61% of disagreements respectively. This is encouraging as annotators almost never have polarising views on cluster quality and primarily agree on the coherence of a good cluster, the main goal of this task. For issue identification the majority of disagreements (%49) consists in distinguishing Intermediate-Chained cases. This can be explained by the expected differences in identifying subclusters in the first stage. For the adjudication process, we found that a majority always exists and thus the final score was assigned to be the majority label (2/3 annotators). Table 1 presents a summary of the corpus size, coherence quality and issues identified for COVID-19 and Election (See Appendix C for a discussion).",
        "GPT2_formal_text": ", w_mask, h_mask, s_mask, c_mask, and a_mask. \n\nFor the neural network part, there are two types of LSTM-based networks: sequence-level (SeqLSTM) and sequence-level recurrent (SeqLSTM-RNN). The sequence-level model has two main parts: first, it uses CNN to create feature vectors that are fed into LSTMs, and second, it can be tweaked to create specific LSTMs with different filter sizes, as shown in Equations (1), (2), and (3). Formal: The neural network is trained using a combination of regular features (f_i) and a linear combination of attention masks (h_i). Formal: The model calculates the cross-entropy loss on the input sequence x_i and the output sequence y_i using a sequence-level LSTM. Formal: The model learns how to generate the output vector using a sequence-level RNN, which is trained on the combined input x_i and the output sequence y_i. Formal: The model also learns how to generate the hidden representations h_i using the RNN. Formal: The model also learns the relationships between the input and output vectors. Formal: The model is trained to maximize the cross-entropy loss. Formal: The model is trained to maximize the cross-entropy loss. Formal: In Figure 2, you can see how the model is trained. Formal: To train the model to maximize the cross-entropy loss, we use regular features (f_i) and a linear combination of attention masks (h_i). Formal: The neural network is trained using a sequence-level LSTM. Formal: We build the model using regular features (f_i) and the linear combination of attention masks (h_i). Formal: The neural network is trained using a sequence-level RNN. Formal: We also use the model to learn how to generate the output vector using a sequence-level RNN. Formal: We also use the model to learn how to generate the hidden representations h_i using the RNN. Formal: The model is trained to maximize the cross-entropy loss. Formal: The model is trained to maximize the cross-entropy loss. Formal: The model is trained to maximize the cross-entropy loss. Formal",
        "directory": "acl",
        "filename": "2021.acl-long.530.json"
    },
    {
        "casual_text": "As a serial processor, the learner can only focus on one thing at a time.",
        "formal_text": "being a serial processor, the learnor can focus his attention on but one thing at a time.",
        "GPT2_formal_text": "= {}; For each word in the sentence, i_w, the method uses a decoding process with a bunch of word embeddings. After that, it figures out the probability P_w for each word in the sentence, like this: Formal: We took the original CNN and tweaked it using the OpenAI GPT-2 model. Formal: We took the original CNN and tweaked it using the OpenAI GPT-2 model. Formal: In the next part, we'll talk about how we tweaked the CNN's settings to get the best results. Formal: In the next part, we'll talk about how we tweaked the CNN's settings to get the best results. Formal: This setup lets us use the CNN's fancy hidden state feature to show how different parts of the sentence connect. Formal: This setup lets us use the CNN's fancy hidden state feature to show how different parts of the sentence connect. Formal: You can check out more details about the final model in Appendix A. Formal: You can check out more details about the final model in Appendix A. Formal: We call these model setups the \"convolutional encoder-decoder\" (DE) and \"convolutional decoder-decoder\" (DEC) models. Formal: We use the models shown in Figure 1 as a starting point for comparison. Formal: Formal: We also look at how well the original CNN performs on the same data as the encoder-decoder models. Formal: Formal: As mentioned in Section 3, using a standard CNN is the way to go, but we also tried out the CNN models we mentioned in Section 3. Formal: The final model we’re talking about is the one that’s trained using a mix of the encoder-decoder models in each setup. Formal: Formal: The final model we’re talking about is the one that’s trained using a mix of the encoder-decoder models in each setup. Formal: Formal: We wanted to make sure our method works well for different languages. Formal: We wanted to make sure our method works well for different languages. Formal: Formal: The dataset we used for testing our method is the English MNLI dataset (thanks, Lin et al., 2018). Formal: Formal: We used the dataset we used for testing our method is the English MNLI dataset (thanks, Lin",
        "directory": "C86",
        "filename": "C86-1133.json"
    },
    {
        "casual_text": "This assessment doesn't consider punctuation and is based on the updated .prm parameter setup in evalb. 5. Check out the link here: http://www.cis.upenn.edu/~dbikel/software.html",
        "formal_text": "This evaluation ignores punctuation and corresponds to the new. prm parameter setting on evalb. 5 http: //www. cis. upenn. edu/˜dbikel/ software. html",
        "GPT2_formal_text": "ing (Bertsch et al., 2020). Also, we’re using the f1 score as our evaluation metric. Formal: In our model, we use the attention masking method from (Bertsch et al., 2020). Formal: For the top-1 and top-10 predictions, we combine the hidden states and the output vectors to get the final prediction. Formal: The scores come from the top-1 predictions of the CNN and BERT models. For the top-2 predictions, we use the top-2 hidden states. Finally, the accuracy for the top-5 predictions is calculated by adding up the top-1 scores and then the top-5 hidden states. Formal: Our model performs really well, improving the accuracy to 92.42%. This shows that our model can use the representations of the keyphrases to better understand the context. Formal: The accuracy for the top-1 predictions is 93.38%. The top-2 predictions are just the top-1 scores. The top-5 predictions are the top-5 output representations from both CNN and BERT models combined, with the top-1 scores and the top-5 hidden states. Formal: The accuracy for the top-5 predictions is 93.98%. Formal: This shows that the keyphrases capture all the important context information and are super important for this task. Formal: We’ve got the full details for the model and the full validation dataset in Table 2. Formal: The model has a lot of parameters, which means it takes a while to train it. Formal: Our model is built with PyTorch (Paszke et al., 2019), which is a top-notch transformer model. Formal: In our model, we’ve got a single-head attention mechanism to handle both text and image features. Formal: We tested it on a dataset called the Giga-Word Image Captioning Challenge (GVC) (Bavas et al., 2015). Formal: We also ran some experiments on the WikiImage dataset (Gvan der Goot and van Noord, 2016). Formal: Table 2 has all the details for the model and the validation dataset. Formal: We’re calling this model the \"Generation-Contextual Attention Network,\" or GCN for short. Formal: A model that focuses on generating captions, or generating context, instead of just making the captions",
        "directory": "D10",
        "filename": "D10-1003.json"
    },
    {
        "casual_text": "Alright, so the idea here is to close the big gap between how we talk (natural language), how computers talk (computer language), and logic. We want to do this for a good reason: when you're building a computer system for a specific area (like healthcare, finance, etc.), you need to really understand that area inside and out. But usually, the people building the system (the implementors) don’t know much about it, so they have to learn from the experts in that field. The problem is, those experts usually don’t know much about computers. So, having a way to clearly and efficiently explain this knowledge would be super helpful. Right now, there’s no tool like that, so we’re trying to create one. LESK is kind of like a first step toward making that happen.",
        "formal_text": "Knowledge) is intended to bridge the considerable gap between natural language (I~L), computer lan6uage (CL) and log~tc. It is desirable to do so for the following reason. To implement any non-trivial computer system for some problem domain, one must first have a very clear understanding of the domain concepts. Usually the implementors do not have this knowledge, and must struggle to obtain it from the domain experts, who are usually not computer experts. Hence a means of precise and efficient knowledge expression would be very useful. No such tool exists today and we therefore seek to develop one. LESK is a first approximation to such a medium for knowledge capture.",
        "GPT2_formal_text": "\") as the values for the attribute. Formal: We combine all the attribute values from the word embeddings using a linear layer with a softmax activation. This gives us the final representation for the sentence. Formal: Let's say w_s and w_e are the words in the sentence, and d is the size of the embeddings for the hidden vectors. Formal: We can write this process like this: Formal: The final word representations are picked based on the log probability of the embedding of the last word, which we call δ_last. Formal: To break it down, we take the sentence and turn it into a bunch of dimensions using a transducer, which gives us a matrix called D. Formal: After going through all the steps, we end up with the final embedding for the sentence as h_s = D_s1, ..., D_sK. Formal: To make sure everything is accurate, we calculate the expected word embedding by looking at the log probability of the embedding of the first word in the sentence. Formal: Formal: We can use the log probability of the last word's embedding as a kind of validation signal to check if the output is correct. Formal: Formal: We use a linear layer with a softmax activation to represent the output. The log probability is calculated by adding up the probabilities from the first word in the sentence, the last word, and the embedding of the first word in the sentence. Formal: The log probability is calculated using the same formula as for the last-word embedding, but this time using the last-word embedding instead of the last word. Formal: Formal: This idea was brought up by Wang et al. (2017). They used an LSTM (a type of neural network) to handle the input. Formal: The output is a matrix called h_i, which is a mix of the log probability from the first word in the sentence, the embedding of the first word in the sentence, and the log probability of the last word's embedding. Formal: Formal: Formal: Formal: We figure out the probability of the last word's embedding by averaging the probabilities from the last word in the sentence, the last word, and the embedding of the first word in the sentence. Formal: Formal: Formal: We get the total probability by averaging the probability of",
        "directory": "C82",
        "filename": "C82-2065.json"
    },
    {
        "casual_text": "Piecewise Convolutional Neural Network (PCNN). In distantly supervised relation extraction, it's pretty standard to use the PCNN (Zeng et al., 2015) to create contextualized representations from a sequence of word embeddings. Compared to the usual 1D-CNN with max-pooling (Zeng et al., 2014), the PCNN's piecewise max-pooling can better capture the structure between two entities by paying attention to their positions. Here's how it works: first, a 1D-CNN (Kim, 2014) processes the input sequence to get contextualized representations. Then, piecewise max-pooling is applied to the output sequence to create a sentence-level embedding. That's basically the process in a nutshell.",
        "formal_text": "Piecewise Convolutional Neural Network. As a common practice in distantly supervised relation extraction, piecewise convolutional neural network (PCNN) (Zeng et al. , 2015) is used to generate contextualized representations over an input sequence of word embeddings. Compared to the typical 1D-CNN with max-pooling (Zeng et al. , 2014), piecewise max-pooling has the capability to capture the structure information between two entities by considering their positions. Specifically, 1D-CNN (Kim, 2014) is first invoked over the input sequence for contextualized representations. Then a piecewise max-pooling performs over the output sequence to obtain sentence-level embedding. These steps are written as",
        "GPT2_formal_text": "ed = g, and b = a_t. Formal: Let's talk about how the attention is spread out across different time steps. Formal: In this project, we found that adding a self-attention mechanism to BART can really boost its performance. Formal: We're looking into how BART learns and uses this self-attention mechanism. Formal: Our main goal is to train the model to understand questions and answer them on the fly. We're assuming that a question and answer are linked in a causal way, so the model should be able to work with both. Formal: The model has to be able to handle questions with long answers and the kind of questions we see in NER datasets. We're using two popular datasets—BiDAF (by Fan et al., 2020) and CNN/Daily Mail (Chen et al., 2020)—to train this model. Formal: We also want to test the model on questions that don't have answers, which are common in NER. Formal: We're comparing BART to a few other systems: BERT (Devlin et al., 2019), GPT-2 (Radford et al., 2019), and LSTM-based systems (Vaswani et al., 2017; Dathathri et al., 2020). We're using the Adam optimizer with a batch size of 64, and the loss function is set to 1/64. Formal: We're using the BART standard architecture, with BERT as the teacher, and GPT-2 as the student. The student learns from the teacher to improve its performance on its own. Formal: The batch size is set to 64, and the loss function is 1/64. Formal: We're testing both the traditional and self-attention methods. Formal: We're working with a single-domain question answering dataset. Formal: To train the model, we train it with a dataset with questions that are already in the knowledge base. Formal: We're following a method similar to what was done in (Vijayakumar et al., 2020), where we fine-tune the model to avoid any tricky questions that might come up during testing. Formal: We're checking how well the model can handle questions that have multiple answers. Formal: Formal: We're also looking at how the model handles questions that are different in how they look. Formal: To train the student, we use the same approach as in",
        "directory": "coling",
        "filename": "2020.coling-main.145.json"
    },
    {
        "casual_text": "For the MultiDoc2Dial dataset evaluations, there are two main tasks. Task 1 is about predicting the grounding span for the next agent response. For this task, we use: (1) the current user turn, (2) the dialogue history, and (3) all the documents from every domain as input. The goal is to find the most relevant text span from one document to use in the next agent response. \n\nTask 2 is about generating the agent's response in natural language. For this task, we also use: (1) the current user turn, (2) the dialogue history, and (3) all the documents from every domain as input.",
        "formal_text": "For the evaluations on MultiDoc2Dial dataset, two sub-tasks are proposed. Task 1 aims to predict the grounding span for the next agent response. For task 1, we get (1) current user turn, (2) dialogue history, (3) the entire set of documents from all domains as input. For the output, we aim to figure out the most relevant grounding text span from one document for the next agent response. Task 2 aims to generate agent response in natural language. For task 2, we get (1) current user turn, (2) dialogue history, (3) the entire set of documents from all domain as an input.",
        "GPT2_formal_text": ": [u_i, v_i], weight_map: {w_i, a_i}, model_idx: [i_i, j_i], model_features: [f_i, g_i] } Formal: So, λ_ij is the weight for the i-th part of the IRF, and T(θ) is the set of weights that go with it. Formal: If you're dealing with a truncated IRF and you want to keep the terms that come before or after the term in the loss functions, you can just add those terms to T(θ). Formal: The j-th term in T(θ) is what we call the \"context term.\" Formal: Since the context term is part of the weight vector, we can figure out the main representation of the term. Formal: For a while now, people have been using neural networks to learn these representations. They've come up with different models like recurrent neural networks, recurrent neural networks with multiple hidden layers, and convolutional neural networks (CNNs). Each of these has its own unique set of weights and feature functions. For instance, in the CNN model (Mikolov et al., 2013), the feature functions are ReLU (a convolutional function), Sigmoid (a sigmoid function), and max-pooling (a max-pooling function). Formal: This approach has been pretty effective, but it also creates a lot of noise. Formal: With this approach, you'd first train a classifier to tell the words apart, and then use a classifier to figure out the context terms. Formal: Most of the time, people have been focusing on learning these representations, but they've also been looking at how the model behaves under specific conditions, like how it's being used in a dialogue, or how it's being used in an image. Formal: In this paper, we're trying to tackle all these things at once. Formal: We're proposing a model that learns these representations in a straightforward way. Formal: We're doing this by starting with a basic model and gradually increasing the model's parameters, making it better at handling new situations. Formal: We're also tackling another problem: the model might not generalize well if the training data is really noisy. Formal: In the paper, we introduce a straightforward model that learns these representations for a specific task.",
        "directory": "dialdoc",
        "filename": "2022.dialdoc-1.15.json"
    },
    {
        "casual_text": "To figure out how entities in the original text match up with those in the translated version, we need to do two things: first, identify the parts of the translated sentence that are entities, and second, connect those parts to the corresponding ones in the original sentence. We've built an alignment tool that uses the attention weights between the encoder and decoder in the Marian model to link the input and output sentences. These weights tell us how much focus is put on each input word when the model is generating an output word. \n\nThere's a heatmap in Figure 2 that shows these attention weights for an English sentence and its Italian translation. The cross-attention score for each decoder word is calculated using multi-head attention (Vaswani et al., 2017) across all the encoder words. In the example from Figure 2, each attention vector is represented by a column in the heatmap.",
        "formal_text": "To find the mapping between entities in the source and the translated language, we need to 1) detect entity spans in the output sentence, 2) align those spans with input sentence spans. We have created an alignment module, which uses the cross attention weights between the encoder and the decoder of the Marian model to align the input and output sentences. These weights show the amount of attention given to each input token when an output token is being generated. Figure 2 shows a heatmap of cross-attention weights for an English sentence and its translation in Italian. The cross-attention score for each decoder token is calculated by doing a multi-head attention (Vaswani et al. , 2017) over all encoder tokens. For the example shown in Figure 2, each attention vector corresponds to one column in the heatmap.",
        "GPT2_formal_text": "ed is how much attention the model pays to certain tokens. When we have a sequence of tokens {s 1, . . . , s |t = 1, . . . , t | }, we can figure out the attention weights by using this formula: Formal: The attention weights W A(t) for each token are calculated using the self-attention mechanism. Formal: The attention weights in the softmax layer can be seen as the average attention weights for all the tokens in the sequence. Formal: The attention weights in the softmax layer are what we call the attention weights. Formal: The attention weights are shaped like this: Formal: Formal: The attention weights for a token are calculated using the attention mechanism. Formal: The attention weights in the softmax layer are the attention weights for the token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.481.json"
    },
    {
        "casual_text": "Next, we add the thread title to the comment and send the whole thing to an outside company for manual translations. When we got the translations back, we saw that the quality wasn't always consistent. Turns out, translating stuff from social media, with all its tricky little details, is pretty tough even for people. To make sure we get the best translations, we go through the data by hand, break the comments into sentences, and toss out the ones that aren't up to par for our test data. This way, we end up with about 1,000 sentence pairs in each direction for our final test set.",
        "formal_text": "We then concatenate the title of the thread where the comment was found to the text and send everything to an external vendor for manual translations. Upon reception of the translations, we noticed a certain amount of variation in the quality of translations, likely because translating social media text, with all its nuances, is difficult even for humans. In order to ensure the highest quality in the translations, we manually filter the data to segment the comments into sentences and weed out poor translations for our test data. We thereby retain around 1, 000 sentence pairs in each direction for the final test set.",
        "GPT2_formal_text": "are basically the parts of the text that are tied to the relationship (like \"w_i\" and \"r_i\" in the example). We're keeping things simple and ignoring those detailed details in our example. Formal: We used three different datasets: Yelp Reviews, Yelp 1-star Reviews, and Yahoo Answers. We randomly picked 500 sentences from each one and tested out different values of \"i,\" which are all between 0 and 1. The \"MUST\" and \"MUST-AFFILIATE\" scores were set to 0.5 for Yelp Reviews and 0.99 and 0.99 for Yahoo Answers. We only looked at pairs where both reviews had at least one sentence with the matching ID. Formal: Figure 2 shows how the correlation changes with \"i\" changes. Formal: Figure 3 shows how the correlation changes with \"i\" changes. \n\nWe also looked at pairs where both reviews had sentences with the matching ID, but the source sentence had a different ID. We picked the one with the higher correlation. Formal: Figure 4 gives the average correlation between a pair of reviews. Formal: Figure 5 shows the average correlation between a pair of reviews. Formal: Figure 6 shows the average correlation between a pair of reviews. Formal: Figure 7 shows the average correlation between a pair of reviews. Formal: Figure 8 shows the average correlation between a pair of reviews. Formal: Figure 9 shows the average correlation between a pair of reviews. Formal: Figure 10 shows the average correlation between a pair of reviews. Formal: Figure 11 shows the average correlation between a pair of reviews. Formal: Figure 12 shows the average correlation between a pair of reviews. Formal: Figure 13 shows the average correlation between a pair of reviews. Formal: Figure 14 shows the average correlation between a pair of reviews. Formal: Figure 15 shows the average correlation between a pair of reviews. Formal: Figure 16 shows the average correlation between a pair of reviews. Formal: Figure 17 shows the average correlation between a pair of reviews. Formal: Figure 18 shows the average correlation between a pair of reviews. Formal: Figure 19 shows the average correlation between a pair of reviews. Formal: Figure 20 shows the average correlation between a pair of reviews. Formal: Figure 21 shows the average correlation between a pair of reviews. Formal: Figure 22 shows the average correlation between a pair of reviews. Formal: Figure 23 shows the average correlation between a pair",
        "directory": "D18",
        "filename": "D18-1050.json"
    },
    {
        "casual_text": "Okay, so this is a bit of a mess, but let me try to break it down in simpler terms. Basically, it's talking about a relationship that only works under certain conditions. The text is saying that this relationship, let's call it \"L,\" only holds true when a specific property of something, like a \"thing\" or \"object,\" is involved, along with some other stuff. \n\nNow, this \"L\" thing isn't really useful unless we know exactly what conditions need to be met for it to work. So, the text is trying to explain those conditions with something called a \"meaning postulate.\" It's saying that VPVQ(o, dy(l', (2), . , , d, , si~g (, X, ~, '~, '(~4) is the way to spell out those conditions. \n\nIn short, it's just trying to make clear what needs to happen for this relationship to make sense.",
        "formal_text": "where /q is Allcv(~d, (ll) A typc(ll, borrou: ) A a, g (! 'n. t(H, I, ')A ob. jt~cl, (ll, I) This says Lha, L 1, he rela. ti~mshil) on. l 9 holds l)(; l, w(~en the l>['~fl)erty o[' l)ehb'; a, cam a. i]d, some oLlmr oh joel. This is Nile; ~s fax ~s il Koes, buL iL isH'l; worth very much unless we spell out the conditions un der which this rel~tti(mship holds. The tbllowing mea. ning postub~te does just that: VPVQ(o, dy(l', (2), . , , d, , si~g (, X, ~, '~, '(~4",
        "GPT2_formal_text": "ed + self-attention_weighted) are calculated using the fully connected layer, like in Equation 2. Formal: When you have a batch size of N, you can think of each attention head as a separate entity. Formal: (1) S = [attn1, attn2, ..., attnM] Formal: (2) X = [attnX1, attnX2, ..., attnXM] Formal: (3) Y = [attnY1, attnY2, ..., attnYM] Formal: (4) Formal: (5) Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C94",
        "filename": "C94-2142.json"
    },
    {
        "casual_text": "So, in this paper, we take a look at two language models that work with syntax, but in a kind of basic translation setup. We separate the target language's structure from the translation rules, which gives the language model more room to do its thing. This way, we can really see what the parser is good at and where it might fall short when picking the right translation output.",
        "formal_text": "To that end, in this paper we examine two syntaxbased language models in a weak translation model. Our approach decouples target-language trees from the translation rules, providing the language model with more freedom to express itself, and permitting us to isolate the strengths and weaknesses of the parser in selecting MT output.",
        "GPT2_formal_text": "= { ..., \"[CLS]\" } columns = ( ..., \"[CLS]\" ) d = { d. 1, d. 2, ..., d. m } rows = ( ..., \"[CLS]\" ) \n\nIn the first and second rows, we have pairs of entities that are related in the sense that their instances are similar. So, the context of (d. 1, d. 2) is (CLS, d. 1, d. 2), and (d. 2, d. 1) is (CLS, d. 2, d. 1). Formal: To get the vector representations for the entities in the same instance, we calculate a weighted average of these vectors. Formal: We tweak the system to optimize the weight vector, but we keep the regularization constant the same as before. Formal: Here, W(i) represents the matrix W that holds the learned weights for the entity vectors. Formal: Using the vectors from the two instances in a row, we can create a new vector representation for the entity e_i, which has the highest chance of being correct. Formal: We can also use the vectors from the two rows of a pair, just like before, and figure out the probability of the correct answer for the query query. Formal: Formal: The probabilities are calculated using the softmax function. Formal: The entity representation q_i is calculated using the softmax function. Formal: We use the final entity vector to predict the query query. Formal: Formal: The probability vector of the correct answer is calculated by minimizing the loss term for the query. Formal: Formal: We can also use the output from the softmax function as the query representation, just like before. Formal: Formal: The probability vector of the correct answer is calculated by minimizing the loss term for the query. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "2008.amta-papers.16.json"
    },
    {
        "casual_text": "[Question] You can use the subway to get to and from work <mask> days a week.",
        "formal_text": "[Question] you may take the subway back and forth to work <mask> days a week.",
        "GPT2_formal_text": "_len(x, g) = a_t(x, x) + b_t(x, x) + c_t(x, x). Formal: Alright, so let's break this down in simpler terms. We're talking about how to represent a bunch of text, like words, and how to use a special type of activation function that helps create a little grid, h, around it. Formal: The formula for this can be written as: Formal: Formal:  \n\nIf we have an input x, and we have a bunch of text messages, we call them x_t = [t_1, ..., t_n], where t_i = i = 1, ..., m. Formal: So, our grid h is just a way to map these messages into a kind of shape. Formal: In the end, we have a bunch of words, x_g = [w_1, ..., w_n], and a vector w_t = [h_t(x, x_g), h_t(x_g)], which tells us the position in this grid. Formal: This whole process is super important for understanding the whole message. Formal: Plus, we use this vector h_t to figure out how important each word in the message is. Formal: Formal: In the end, we have a bunch of words, x_g = [w_1, ..., w_n], and a vector w_t = [h_t(x, x_g), h_t(x_g)], which tells us the position in the grid. Formal: Finally, we add h_t(x, x_g) to the mix to get the final representation of the whole message. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.225.json"
    },
    {
        "casual_text": "Semantic parsing is all about turning sentences in plain language into something a computer can understand and work with. This means converting natural language (NL) phrases into structured meaning representations (MRs) that are written in formal languages like Prolog, SQL, or Python. These formal languages have their own rules, which are laid out in a formal grammar—basically, a set of instructions that define how the language works. \n\nIn these formal languages, MRs are often called logical forms or programs. The cool thing is that these MRs can be run in a programming environment to get results, like pulling data from a database using an SQL query. This process helps with automated reasoning, which is a fancy way of saying the computer can figure things out on its own (Kamath and Das, 2018).",
        "formal_text": "Semantic parsing is concerned with mapping natural language (NL) utterances into machine-readable structured meaning representations (MRs). These representations are in the formats of formal languages, e. g. Prolog, SQL, and Python. A formal language is typically defined by means of a formal grammar, which consists of a set of rules. Following the convention of the chosen formal language, MRs are also referred to as logical forms or programs. An MR is often executable in a (programming) environment to yield a result (e. g. results of SQL queries) enabling automated reasoning (Kamath and Das, 2018).",
        "GPT2_formal_text": "= (src, stt, req, res)  \n\nNow, let's talk about the second part of the FST model: the tag sequence classification layer. Formal: To train the tag sequence classification layer, we need a bunch of training data. Since we're using a fixed context vector c_t, the input to this layer has a length of |c_t| - 1. The goal of this layer is to figure out the probabilities for each label that might come after a tag. Formal: We measure the loss L_t by dividing it by the total number of labels, which is |c| - 1. Formal: We use a tree tagger, which is the same one from the last part of the model. Formal: The tagger creates a binary tree where each node is the tag. Formal: Here's how the tagger works:  \n\n- First, it checks the context vector c_t.\n- Then, it creates a trigram tag, y t, based on the context vector c_t.\n- Finally, it does a one-to-one match between the two tags y t and y t+1. Formal: The tagger's output is a list of tag sequences y t. Formal: Finally, the tag sequence classification layer uses these tag sequences to predict the probability of each tag sequence y t. Formal: To make the FST model more flexible, we can add more labels to the training data. Formal: We can add new tags to the training data at any time, just like we did with the original labels. Formal: A word is tagged with one of its neighbors, which gives us a probability p_n(w_i | y_i) for the tag sequence y_i. Formal: Formal: For the FST model, we use a logistic regression classifier (a linear layer) to get the probabilities p(t | y_i) for the tag sequence y_i. Formal: Formal: Finally, the tag sequence classification layer gives us a list of tag sequences y t. Formal: Formal: In Figure 2, we're looking at how the tag sequence classification layer works, using the same words (w_i) as in the examples shown. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.226.json"
    },
    {
        "casual_text": "In the middle of the table, you'll find the results broken down by SBA. Noun phrases that didn't get a semantic analysis result, basically because the analysis didn't work, are grouped together and labeled as <other>, which you can see at the end of the table.",
        "formal_text": "The middle part of the table shows the results by SBA. Noun phrases that have no semantic analysis result (analysis failure) are bundled and named <other>, as shown in the last part of the table.",
        "GPT2_formal_text": "ed = ctx_attn_masked(tx, a_t) to get the attention vector h_att. Formal: This process is pretty much the same as the regular attention-based GAN, but it doesn't take any extra parameters. Formal: From Equation (2), you can see that the attention weights for a_t depend on the values of a_t, a_t', and a_t', where each a_t' can be either 0 or 1. Formal: This is kind of like what Adam did, where he set the weight for each antonym to the average of the two previous ones. Formal: These weights are learned one after the other, then averaged. Formal: We use a special loss function L_d that's different from the usual cross-entropy loss, L_c, to prevent the model from overfitting. Formal: For the MLE task, we set the parameter α to 0.3. Formal: L_d is kind of like the regular Adam loss, where L_c is used. Formal: You can see how the attention weights for a_t, a_t', and a_t' are distributed in the figure. Formal: We use the attention weights for the MLE task to get the feature representations for a_t, a_t', and a_t'. Formal: The loss for MLE is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C04",
        "filename": "C04-1174.json"
    },
    {
        "casual_text": "We get a sentence-level representation, let's call it s_h, by adding up all the value vectors in the sentence, but each one is multiplied by a weight first (check out Equation 11). After that, there are two feed-forward layers. The first one is non-linear and shrinks the sentence representation into a smaller feature space. The second one is linear and gives us a single number, o_h, which is the sentence-level score for each head h (Equation 12).",
        "formal_text": "A sentence-level representation s h is obtained as the weighted sum over all the value vectors in the sentence (Equation 11). This is followed by two feed-forward layers: the first one is non-linear and projects the sentence representation onto a smaller feature space, while the second one is linear and outputs a scalar sentence-level score o h ∈ R for each head h (Equation 12).",
        "GPT2_formal_text": "= [TRAIN, TARGET, TRAINING] [TRAIN, TARGET, TRAINING, TRAINING]\n\nFor future work, we could add an extra loss function to handle the \"difference\" between the input's state vector and the output's vector. This could either be a continuous function or a binary one. \n\nIn this project, we're mainly looking at how to automatically figure out the token representation, called s_i. We're also assuming the outputs of the model are already labeled. Our main goal is to find the hidden representation for the tokens that are really different from the model's outputs. Formal: We're focusing on the token representation s_i, the output vector x_i, and the hidden representations of the target and source tokens. To do this, we're using the standard VAE loss (from Bahdanau et al., 2014), which was introduced by Szegedy et al. (2014). Formal: Since we're dealing with different types of inputs and different training scenarios, we're looking at three different loss functions that involve adding up and running through different steps. Formal: To get the hidden representation s_i for the target tokens, we start by running through the model's outputs, x_i, to get their hidden representations. Formal: If the source token matches the target token, we use the output from the model's output layer, x_i, as the input for the final layer. Formal: Finally, we calculate the loss, which can be either an average of the losses across all tokens or just the average of the losses across the target tokens. Formal: We're using the standard LSTM loss (from Sutskever et al., 2014) with two types of hidden layers: S and T. Formal: To make the final predictions, we use a binary classifier. Formal: We're using the standard cross-entropy loss (from Lin et al., 2014) with a logarithm function. Formal: To optimize the whole process, we apply a softmax function. Formal: Finally, we apply a softmax function to the loss function we just calculated to get the final probability. Formal: Formal: We're using the standard cross-entropy loss (Lin et al., 2014) with a logarithm function. Formal: To make the final predictions, we use a binary classifier. Formal: We're",
        "directory": "coling",
        "filename": "2020.coling-main.335.json"
    },
    {
        "casual_text": "The third row compares our model to NO-SYNTAX, which is a stripped-down version of our model that uses lexical features but doesn’t include the syntactic structure. The results show that it performs better than the SENSORS-ONLY and RELATIONAL-RANDOM baselines but still falls short compared to our full system. This shows that the syntactic features our model uses help create better semantic representations of the text. Features like `empty(x1) ∧ freecell(x1)` are helpful because they reuse variables to make sure objects have important properties—in this case, making sure a freecell is empty. Other features, like `homecell(x1) ∧ value(x2, x3)`, help narrow down the search to useful combinations of predicates (in Freecell, whether you can play a card on a homecell depends on the card’s value). \n\nIt’s worth noting that three out of the 15 formulas are basically useless because they’re always false, like `card(x1) ∧ tableau(x1)`. This highlights how important it is to get the term assignment right to create useful features for learning. In the NO-SYNTAX system, which doesn’t consider the relationship between term assignment and syntactic structure, eight out of the top 15 formulas were useless because of term incompatibility.",
        "formal_text": "The third row compares against NO-SYNTAX, a crippled version of our model that incorporates lexical features but not the syntactic structure. The results are stronger than the SENSORS-ONLY and RELATIONAL-RANDOM baselines, but still weaker than our full system. This demonstrates the syntactic features incorporated by our model result in better semantic representations of the underlying text. Features such as empty(x 1 ) ∧ freecell(x 1 ) are useful because they reuse variables to ensure that objects have key properties -in this case, ensuring that a freecell is empty. Other features, such as homecell(x 1 ) ∧ value(x 2, x 3 ), help to focus the search on useful conjunctions of predicates (in Freecell, the legality of playing a card on a homecell depends on the value of the card). Note that three of these 15 formulae are trivially useless, in that they are always false: e. g. , card(x 1 ) ∧ tableau(x 1 ). This illustrates the importance of term assignment in obtaining useful features for learning. In the NO-SYNTAX system, which ignores the relationship between term assignment and syntactic structure, eight of the top 15 formulae were trivially useless due to term incompatibility.",
        "GPT2_formal_text": "and the attention_inputs for the model. The attention weight for the word embedding is calculated based on this formula: Formal: To get the attention mask, we use the cross attention module from the CNN model. It turns the attention into a hidden representation for each token in the embedding layer. Formal: Next, we calculate the output for each token in the embedding layer using the corresponding output value, which we'll call s_i. Formal: Finally, we update the attention weights for the token embedding using the updated values s_i. Formal: In our experiments, we set the attention mask and attention weight to 0. Formal: We also add a linear layer to the attention module, and we use a linear layer to update the attention weights for the token embedding. Formal: The Transformer-based model with attention can perform better than the basic CNN model when it comes to diversity and generalization. Formal: For the Enron dataset, we train the model using the development set, specifically the subset (DCD) for Gapping. Formal: Our model has two attention mechanisms, each for a specific attention type, which helps improve diversity and generalization. Formal: From the datasets we tested, we noticed that the Transformer-based model can learn better representations for unlabeled tokens compared to the basic CNN model. Formal: To train the model more effectively, we suggest using the training set with a structured knowledge graph. Formal: The dialogue generation task usually involves multiple steps, like generating an utterance and following the conversation. But if we have just a little bit of labeled data, we might not get enough information to generate good responses. Formal: To improve the generation task, we suggest using a latent variable model to understand how dialogue acts, which we'll call P_D. Formal: In this project, we propose using the structured knowledge graph to enhance the generation task. Formal: We also want to train the model using knowledge from other knowledge bases. Formal: We tested our model using the ACE 2005 dataset, which is a sequence-to-sequence learning dataset. Formal: We used the dialogue generation task from the CoNLL-2009 shared task to train the model. Formal: From the data, we found that the context-based model with attention is really good at generating high-quality responses. Formal: We also added a pre-training phase to the model, so we can fine-tune it",
        "directory": "D09",
        "filename": "D09-1100.json"
    },
    {
        "casual_text": "The HAPS segmenter uses something called factor graphs, which are a general way to represent things like Markov or Bayesian networks. A factor graph is like a two-part graph with two types of nodes: function nodes and variable nodes. Each function node is connected to the variable nodes that are its inputs. When you run the Max-Sum algorithm (a method described by Bishop in 2006) on a factor graph, it figures out the best set of variable values that gives the highest total of all the functions. This algorithm works by passing messages between nodes. Variable nodes send messages to the function nodes they're connected to, and function nodes send messages back to the variable nodes that are their inputs. A message from a variable node x to a function node f is calculated by adding up all the messages coming into x, except for the one from f itself.",
        "formal_text": "The HAPS segmenter is based on factor graphs, a unifying formalism for such graphical models as Markov or Bayesian networks. A factor graph is a bi-partite graph with two types of nodes, factor or function nodes and variable nodes. Each factor node is connected to those variable nodes which are its arguments. Running the well-known Max-Sum algorithm (Bishop, 2006 ) on a factor graph finds a configuration of variables which maximizes the sum of all component functions. This is a messagepassing algorithm. All variable nodes send messages to their factor neighbours (functions in which those nodes are variables) and all factor nodes send messages to their variable neighbours (their arguments). A message µ x→f sent from a variable node x to a function node f is computed as a sum of all incoming messages to x, except the message from the recipient function f:",
        "GPT2_formal_text": "= entity_ids(class) return entity_ids(class) end return class end\n\nExample 2: For the second example, the input is entity_ids(class), the output is the concatenation of the model's output and the current token, and the target is the token itself. Formal: We use a linear transformation for the input, and then we use a nonlinear function on top of that. Formal: We're using the linear transformation in the input and a nonlinear one for the output. Formal: We apply the linear transformation to both the input and output. Formal: In the third example, the input is entity_ids(class) and the output is the concatenation of the model's output and the current token. Formal: We use a linear transformation for the input and a nonlinear function for the output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We apply the linear transformation to both the input and output. Formal: We",
        "directory": "C14",
        "filename": "C14-1005.json"
    },
    {
        "casual_text": "We took a RoBERTa-Large model that was fine-tuned on a stained version of the MultiNLI training set. The whole MultiNLI dataset (train, dev-matched, dev-mismatched) was stained during the experiment. We also masked the stain for 10% of our training data to make sure those examples were part of the model's training, which allowed us to use these masked-stain examples later in our analysis. We ran the experiment three times, each time treating one of the three NLI classes as the stain. In all three runs, the models performed really well on the mismatched dev-set of the stained MultiNLI, with accuracy over 97%. This high performance was expected and shows that the models were definitely using the stain features to make predictions.",
        "formal_text": "We analyze a RoBERTa-Large model finetuned on the stained MultiNLI train set. The entire MultiNLI dataset (train, dev-matched, devmismatched) was stained during the experiment, and we masked the stain for 10% of our training data to ensure such examples are in distribution for the model, enabling us to use masked-stain examples in the analysis step. We repeat our experiment thrice, considering one of the three NLI classes as a stain each time. In all three cases, the stained models achieve high predictive mismatched dev-set performance on the stained MultiNLI (above 97% accuracy). This high performance is expected, and indicates that the models indeed exploit the stain features.",
        "GPT2_formal_text": "ed. This method, suggested by Yang et al. in 2016, helps by using hidden units from a GCN to encode the input, which helps create a more flexible model that can handle different kinds of information. Formal: To get a better idea of how well these different classifiers work, we ran a bunch of tests on the WMT14 testset using the model. Formal: We used the testset and evaluation data from MT04, which you can check out in Table 2. Formal: For training, we went with the hyperparameter settings from a previous work by Ren et al. (2019). The total number of layers is set to 5, and we set the dropout rate to 0.5. Formal: We set the batch size to 10 and the learning rate to 0.1. Formal: We trained for 10 epochs on average, stopping early if the validation loss didn’t improve for 10 epochs. We also set the learning rate to 0.1 for the final epoch. Formal: To check how well our classification models are doing, we looked at their performance on the development set. Formal: We had two annotators label each example based on its label and categories. Table 2 gives a quick overview of the evaluation set and the results from the WMT14 testset, showing how each model performed on the development set. Formal: Check out Table 3 for the results from the WMT14 development set. Formal: We compared the classification models using a paired permutation test and a pairwise permutation test. The results are in Table 3, showing the F1-scores. Formal: We also calculated the correlation coefficients between the model’s predictions and the actual labels. Formal: Formal: Formal: The regression models were trained using the ELMo++ toolkit with a batch size of 32. Formal: For the regression models, we adjusted the learning rate by adding a linear function. Formal: Formal: For the classification models, we trained them with the default settings. Formal: The results for different log loss settings are in Table 4. Formal: Formal: For the classification models, we trained them with the default settings. Formal: For the regression models, we adjusted the learning rate by adding a linear function. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.120.json"
    },
    {
        "casual_text": "Not all unsupervised taggers actually create word categories. A lot of systems, called part-of-speech disambiguators (like the one Merialdo talked about in 1994), use external dictionaries with possible tags instead. Our project is based on two older methods for part-of-speech tagging—the word clustering algorithms from Clark (2000) and Brown et al. (1992). These methods have been found to be more reliable than other popular fully unsupervised techniques, according to Christodoulopoulos and his team in 2010.",
        "formal_text": "In turn, not all unsupervised taggers actually induce word categories: Many systems -known as part-of-speech disambiguators (Merialdo, 1994) rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers -word clustering algorithms of Clark (2000) and Brown et al. (1992) -that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al. , 2010).",
        "GPT2_formal_text": "\" to a space of d, where d is the number of documents in the collection. Formal: A vector is basically a list of words, and \"w\" is the word that tells us which document it belongs to. Formal: For each pair of word embeddings h_i and h_j, we calculate a similarity matrix M(h_i, h_j). Formal: Just like before, we can use a linear function to make sure the vectors in the space of topic vectors are orthogonal. This orthogonal setup gives us a special vector for each topic, which we call the vector for topic i. Formal: We use the kernel function g(h, m) to get the vector of topic i. Formal: Once we have the topic vector, the next thing to do is figure out the topic distribution for the document collection D_c. Formal: For each document collection, we generate a bunch of topic vectors, and then we average the topic vectors of the documents in the collection. Formal: To find the topic distribution for the document collection D_c, we use the kernel function g(h, m) with the document collection D_c. Formal: To find the topic vector for the document collection D_c, we use the kernel function g(h, m) with the document collection D_c. Formal: We average the topic vectors from each document collection, and then we calculate the topic distribution for the document collection D_c. Formal: For each document collection D_c, we generate a bunch of topic vectors, and then we average the topic vectors of the documents in the collection. Formal: We calculate the topic distribution for the document collection D_c using the kernel function g(h, m) with the document collection D_c. Formal: To find the topic vector for the document collection D_c, we use the kernel function g(h, m) with the document collection D_c. Formal: Finally, we calculate the topic distribution for the document collection D_c using the kernel function g(h, m) with the document collection D_c. Formal: We calculate the topic distribution for the document collection D_c using the kernel function g(h, m) with the document collection D_c. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D11",
        "filename": "D11-1118.json"
    },
    {
        "casual_text": "Being chosen as a positive example. This probability is like the average importance of each word in shaping the final vector. That's how we use it when adding everything up in each model.",
        "formal_text": "to be selected as a positive sample. This probability can be seen as the expected weight of the contribution of each word to the final vector, which is how we apply it to the sum in each model.",
        "GPT2_formal_text": "is the input representation for the attention mechanism in the Bi-LSTM encoder. It connects to the output from the hidden layer of the Bi-LSTM decoder. Formal: Using the attention weights learned from the input sequence x, the attention mechanism creates a representation of the input sequence by using the attention weights, y(x). Formal: Here, P(x|x) represents the probability distribution for the attention weights. Formal: The attention weights y(x) are learned by combining the attention weights from the hidden layer of the Bi-LSTM decoder. Formal: We also call this model (x, y)_att. Formal: In our experiments, we set the attention weights to δ_0 = 0.3 and δ_1 = 0.4, which are the average values from the training set. Formal: We set the batch size to 32, and for training, we use a linear warmup step. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-6104.json"
    },
    {
        "casual_text": "For instance, they take inputs (a, b) for recognizing textual entailment (RTE) and turn them into something like \"rte sentence1: a sentence2: b\". Then, the PLM is supposed to guess stuff like \"not entailment\".",
        "formal_text": "For example, they convert inputs (a, b) for recognizing textual entailment (RTE) to \"rte sentence1: a sentence2: b\", and the PLM is asked to predict strings like \"not entailment\".",
        "GPT2_formal_text": "= {}; for i in n: if i is not in any of the tables:  \n3.  \n4.  \n5.  \n6.  \n7.  \n8.  \n9.  \n10.  \n11.  \n\n12.  \n13.  \n\n14.  \n\n15.  \n\n16.  \n\n17.  \n\n18.  \n\n19.  \n\n20.  \n\n21.  \n\n22.  \n\n23.  \n\n24.  \n\n25.  \n\n26.  \n\n27.  \n\n28.  \n\n29.  \n\n30.  \n\n31.  \n\n32.  \n\nTable 4: This is a semi-automated way of generating knowledge about entities, based on this knowledge base. Formal: We start by defining a group of entities E and then a set of relations R, which are like groups of entities, each with two parts: (i, j) and (i', j'). We also define a function f that takes two sets of entities, E and R, and turns them into a single set of relations, R. Formal: We use these relations to calculate a global score for each entity in E. For each relation r in R, we look at all the possible types of entities in E and identify the type that’s most likely to be the result. This is done using the formula in Eq. 3. Formal: For each type, we use a sentence template that describes the type. We’ll call this template M. Formal: We pick an entity type template F and a relation type template R for each type template. Then, we calculate the global score for this type template by applying the formula in Eq. 3. Formal: We use a similarity score to compare the entity types we’ve identified. Formal: For every relation r, we look at all the possible types of entities in E and identify the type that’s most likely to be the result. Formal: Finally, we update the global score for a type template by applying the formula in Eq. 3. Formal: The final global score for an entity type template is calculated by the",
        "directory": "eacl",
        "filename": "2021.eacl-main.20.json"
    },
    {
        "casual_text": "Okay, so basically, all this stuff follows some rules that help us figure out things like how similar two groups of words are, or how important a word is in a sentence. For example, if you have a sentence with the word \"follow\" in it, and it's marked by a special rule, we can give it a score based on where it is in the sentence and how it relates to other words.\n\nThis method can help us compare two sets of words, even if they’re different lengths. First, we check how similar two words are by looking at exact matches, parts of words that match, and how close they are in a thesaurus. Then, we kind of combine all those similarities to see how similar the whole groups of words are.\n\nThe tricky part is that people often use slightly different words or phrases for the same idea, and it’s hard to catch all of those differences. Using a thesaurus and rules like the ones in Table 2 can help, but there’s still a lot of variety in how different those words or phrases can be. Figuring out how to handle all that will be something we work on in the future.\n\nFor rules about topic words, we give a score to the word marked by a square. And for matching words, \"X\" and \"x\" mean the words are the same or synonyms from this Japanese thesaurus. Same goes for \"Bulu'ui\" and \"Nagao\".",
        "formal_text": "All of these, are doue hy al}lflyiug rules COIISiSLillg O['; : t i}at, t, erll for a imrl, ial dep(md{, iwy st, rucl. ur{, and a score. l; 'or example, I)y I~ule-a mid I/ in Tal: , h~ 2, words hi a l)]n. ase whose head word is folio, wed I)y a topic umrl, : ills i)osl, posiI, ion \"\\VA\" are given sotlle scores tin (, el)i( and Nagao 19921>). '['his method ('. an calculate the overall similarity value h{~t. weexl Cwo word-strings of art)itrary leugths. First. , the similarity value between gwo words are cal{'ulal, ed a(: (: , . ~l'ding to exact matching, matching of {h{\"il' parts of Sl~ee(: h, aIKI their closeness in: -i thesaurus dicliouary. 'l'heu, the siruilarity wdue between two wor(I-+strillgs are calculat. ed roughly hy combining t. he similarity values bel. ween words in the two word-: q)H, ' clifllcult problem i~ that authc, rs often use subt. ly dil'fcrcm {'XlJl ', ~ssi, ms, n<, l, M<, ttlc, , d Wol'ds/i)hrases, fc)r such chains. \\Vhih~ s. tnP, of them can be caught by uMng a Ihesaurus and by rules like [hde-f in 'FaMe 2, ~here is a ', vMc range of variety in their diflbrences, Their complete trea. lment will be a. target of OIIl' fll|tLl'¢+ WOI']'~. As tk)r rules for topic/non-topic paris, t, he score is given to the hnnset. su ma. rked by; t square. As for rules for matching, \"X\" and \"x\" denote identical words or synonynlous words from this Jatmnese thesaurus, \"Bulu'ui (', el [[you\". So do llV+~ a. lld lly~.",
        "GPT2_formal_text": "ed is basically the same as the regular attention but with a mask, and it's trained to match the average attention weight. This is the same approach that's been used before in the dialogue model for image classification (Sun et al., 2019; Sun et al., 2020a). Formal: Using the attention weights from the self-attention module, we tweak the model's parameters by using the latent variables H_a and H_v. We do this by adding a linear transformation to the hidden state. This means the final representation for the query token w_q_i is the result of applying this transformation to the parameters of the query vector h_q_i. Formal: We also add a linear transformation to the latent variables H_a and H_v, which gives us the attention weights. Formal: To learn the attention weights for the query tokens, we use the attention module with the latent variables H_a and H_v as the base classifier, and the query as the query-specific classifier. Formal: We use a linear transformation to the latent variables H_a and H_v, which gives us the attention weights for the query tokens. Formal: Finally, we use a linear transformation to the latent variables H_a and H_v, which gives us the attention weights for the query tokens. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C94",
        "filename": "C94-2183.json"
    },
    {
        "casual_text": "Compared to the imbalanced version, the balanced one scored higher in BLEU for DE-EN but performed way worse in RO-EN for both the original label smoothing (LS) and MLS. Training on RO-EN examples seems to mess with the model's ability to generalize in RO-EN translation but doesn’t affect DE-EN quality. This might be because the RO-EN data introduces some bias that impacts DE-EN training. Even in an imbalanced setup, MLS still performs better (37.53) than the original LS in a balanced setup (37.44). This shows that MLS can help with the imbalance issue in multilingual translation. But the improvement in the high-resource direction (RO-EN) isn’t as big as in the balanced condition. We think label smoothing has a more complicated effect on multilingual models because of the extra languages and their relationships. We’ll leave that for future research. Based on the results, two things stand out: 1) Using WLS generally improves translation quality compared to the original LS. 2) Only WLS with β t, β c, β s set to 1/2-1/2-0 beats the original LS across all tasks, making it the most reliable setup.",
        "formal_text": "Compared with the imbalance version, the balanced version gave better BLEU scores in DE-EN direction while much worse performance in RO-EN translation for both the original label smoothing and MLS. training examples does weaken the generalization of model in RO-EN translation however doesn't influence the DE-EN translation quality since the RO-EN data might introduce bias to the training process for DE-EN translation. Even under imbalance condition, MLS can give a better performance (37. 53) compared to original LS in the balance condition (37. 44). It implies that MLS can relieve the imbalance data issue in multilingual translation. However, the improvement in relative high-resources direction (RO-EN) is not as significant as in the balanced condition. We guess that label smoothing has more complex influence on multilingual model due to the increase of languages and relation among different languages. We leave those questions for future exploration. According to the result, though the best BLEU score's WLS setting vary from different tasks and there seems to exist a more complex relation between the probability allocation and the BLEU score, we still have two observations. First, applying WLS can generally boost the quality of translation compared to the original label smoothing. Second, only WLS with β t, β c, β s each equals to 1/2-1/2-0 can outperform the original label smoothing on all tasks, which suggests the setting is the most robust one.",
        "GPT2_formal_text": "and cell_ids are both numbers that don't have a specific value in the set. Formal: So, q p (s) is the value of the first word in the s-th sentence, and q a (s) is the value of the last word in the s-th sentence. Both q p (s) and q a (s) are numbers from 0 to 1. Formal: The table is made up of the word sequences (w) that don't have any relation words, like w = {w1, ..., wn}. Formal: The table is made up of the word sequences (w) that do have relation words, like w = {w1, ..., wn}. Formal: The table is made up of the word sequences (w) that have a specific relation word, like w = {w1, ..., wn}. Formal: We're using a big chunk of the data from a big collection of text (called the corpus), which we call the training set. Formal: We're using a big chunk of the data from a big collection of text (called the corpus), which we call the test set. Formal: The table is organized in a hierarchy, with sentences being the top level and relations at the bottom. Formal: The table is organized in a hierarchy, with sentences being the top level and relations at the bottom. Formal: The table is organized in a hierarchy, with sentences being the top level and relations at the bottom. Formal: The table is organized in a hierarchy, with sentences being the top level and relations at the bottom. Formal: The table is organized in a hierarchy, with sentences being the top level and relations at the bottom. Formal: The table is organized in a hierarchy, with sentences being the top level and relations at the bottom. Formal: The table is organized in a hierarchy, with sentences being the top level and relations at the bottom. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-short.74.json"
    },
    {
        "casual_text": "From Lord et al. (1968, p. 70), we can understand this better. Basically, if we don’t know someone’s true skill level θ j yet, we should choose questions where we’re most unsure about how they’ll answer. Our uncertainty, or entropy, is highest when there’s an equal chance of getting the answer right or wrong, which happens when the value of I i (θ j ) is at its peak. It also makes sense that this value gets bigger as the question’s discriminability γ i increases.",
        "formal_text": "as derived by Lord et al. (1968, p. 70). Intuitively, if we do not yet know the true skill θ j, we should pick items whose expected response we are most uncertain about. Our uncertainty (entropy) is maximized when the likelihood of a correct re-sponse p ij is the same as the likelihood of an incorrect response 1 − p ij, which corresponds to the maximal value of I i (θ j ); it is also sensible this value increases as discriminability γ i increases.",
        "GPT2_formal_text": "_t (src, dim, arg) = arg;\n\nSo, it's basically saying that the hidden state from src at layer d is calculated using the attention mechanism we talked about. This is also done using a linear projection. The input and output from the same token are combined and turned into matrices. Formal: We can simplify things by just using the identity matrix, and we'll also ignore the output from the last hidden state because it's not needed. Formal: So, the final hidden state is a matrix where the sum from the last hidden state to the beginning equals 1, the sum from the last hidden state to the beginning equals 0, and the sum from the last hidden state to the beginning equals n. Formal: The way we get the attention matrix is by using a multivariate Gaussian distribution with a uniform parameter φ and a central limit ρ. Formal: In practice, the attention matrix is often calculated in a different way than the one in equation (1). Formal: We'll use the attention module for the bigram language modeling task, as suggested by Socher et al. in 2012. Formal: For binary classification tasks, we'll use the L2T method. Formal: So, the final hidden state is a matrix where the sum from the last hidden state to the beginning equals 1, the sum from the last hidden state to the beginning equals 0, and the sum from the last hidden state to the beginning equals n. Formal: Formal: For sequence modeling, we'll use the BiGRU method. Formal: The final hidden state is a matrix where the sum from the last hidden state to the beginning equals 1, the sum from the last hidden state to the beginning equals 0, and the sum from the last hidden state to the beginning equals n. Formal: Formal: The attention mechanism can be used for the BiGRU-based sequence modeling task. Formal: Using the BiGRU method, the hidden state can be written as Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.346.json"
    },
    {
        "casual_text": "To sum it up, we’ve come up with LMLM and rLMLM, which focus on telling good hypotheses apart from bad ones based on specific tasks, like WER or BLEU, instead of just trying to lower PPL.",
        "formal_text": "To summarize this section, we have proposed LMLM and rLMLM that aim at discriminating between hypotheses in a task-specific (e. g. , WER or BLEU) sense, instead of minimizing PPL.",
        "GPT2_formal_text": ", and the topic distribution is calculated as P(c|s, w). Formal: A word can have multiple meanings, and the meanings can be different in how they combine and where they go. For instance, words like \"the,\" \"and,\" and \"especially\" can mean different things depending on the context. Formal: If you need to combine two things, like the topic distribution P(c|s, w) and the word distribution P(w|c), you can just use the generalized Frobenius product. Formal: So, the average of all the document vectors is just the sum of the values across all the words. Formal: If you want to calculate the probability p(w|c), you can do it like this: Formal: To get the topic distribution, you just add up all the document vectors. Formal: Also, to get the word distribution, you just add up all the word vectors. Formal: Basically, the probability p(w|c) is calculated by summing over all the words in the document. Formal: You can get the probability p(w|c) by averaging the values across all words in the document. Formal: The whole process of figuring out the document vector P(c) is shown in Equation (6). Formal: And just like in Equation (4), the probability p(w|c) is calculated using the generalized Frobenius product. Formal: This whole thing is shown in Equation (7). Formal: So, the probability p(w|c) is always between 0 and 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1150.json"
    },
    {
        "casual_text": "We also looked at some gradient-based methods for comparison. Sensitivity Analysis (from Simonyan et al., 2013) just takes the absolute value of the gradient. Gradient*Input (by Shrikumar et al., 2016) is a straightforward multiplication of the input by its gradient, element by element. Integrated Gradients (Sundararajan et al., 2017) works by integrating gradients from a baseline input up to the current input. Since these gradient-based methods give relevance scores for each part of the word vector, we added up all the parts that belong to the same word. Keep in mind, these methods don’t take likelihood into account in NLP (check out Section 1 for more on that), so they’re just for comparison—not the ultimate benchmark.",
        "formal_text": "Furthermore, we compare with the following gradient-based methods. Sensitivity Analysis (Simonyan et al. , 2013) is the absolute value of the gradient. Gradient*Input (Shrikumar et al. , 2016) is simple component-wise multiplication of an input with its gradient. Integrated Gradients (Sundararajan et al. , 2017) integrate the gradients from a reference input to the current input. As these gradient-based methods provide relevance for every word vector value, we sum up all vector values belonging to a word. Gradient-based methods do not consider likelihood in NLP (see Section 1) and are thus also merely a comparison and not a gold standard.",
        "GPT2_formal_text": "_len is the size of the input mask, and λ is the number of samples. The goal is to minimize the difference between the predicted and actual labels for each pair. Formal: In the MLP layer, we take each token x and the query question q and split them into a sentence, which we then use to create a parse tree P. In this tree, the labels are the tokens from the input text x, the query question q, and the labels from P. Formal: For our task, we use the T5-Base model (thanks to Bahdanau et al., 2015) to handle multi-label classification. Formal: The main goal here is to predict the label type for each token. To make this happen, we use a self-attention model to create a fully connected layer that has a hidden size of d_p. Formal: We set the hidden layer size to d_p for all our experiments. The input sequence for this layer is X = (X1, ..., Xn) = (1, ..., n) where each Xi is a sequence of tokens. Formal: The cross-entropy loss is calculated as: Formal: The cross-entropy loss is calculated for a cross-entropy loss function L_k, where k is the number of labels. Formal: The cross-entropy loss is calculated for a cross-entropy loss function L_k, where k is the number of labels. Formal: We evaluate the cross-entropy loss using the cross-entropy loss function L_k, where k is the number of labels. Formal: The cross-entropy loss is evaluated for a cross-entropy loss function L_k, where k is the number of labels. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-srw.16.json"
    },
    {
        "casual_text": "We've got a cool thing going on: we’re sharing the full text of 8.1 million papers that are open access, and we’ve made it super easy for machines to read. The S2ORC full text keeps all the important stuff intact, like paragraph breaks, section titles, mentions of citations within the text, and even links to tables, figures, and other papers. Plus, we’ve got 1.5 million papers where we’ve pulled out the full text in LaTeX format, so you also get the actual content of tables and math formulas, along with citations and references. Check out Table 1—it shows that S2ORC has way more structured full text papers and covers a wider range of subjects than other resources out there.",
        "formal_text": "Notably, we release structured, machinereadable full text extracted from PDFs for 8. 1M papers which we've identified as having open access status. S2ORC full text preserves meaningful structure, e. g. , paragraph breaks, section headers, inline citation mentions, references to tables and figures, and resolved citation links to other papers. Additionally, we provide 1. 5M full text LATEX parses from which we have extracted, in addition to citations and references, the source text of tables and mathematical formulas. As shown in Table 1, S2ORC provides substantially more structured full text papers and covers a more diverse set of academic disciplines than other resources.",
        "GPT2_formal_text": "= (c_i, c_j, ..., c_h_1, c_h_n)  \nThe perplexity of the joint distribution p_j for the i-th and j-th sentence in document d is calculated like this: Formal: The average document length is K_i. The formula for the average document length (c_i) is: Formal: We add up the whole collection of K_i documents to get the total collection length. Formal: The sum of K_i over all documents gives us the overall collection length, which we call the collection length. Formal: For the IOB2 sequence, we count the number of non-null tokens, N_i, at every position. Formal: For the OOB2 sequence, we also count the number of O_i tokens at each position, N_i, as well as the number of non-null tokens, N_i, at each position, N_j. Formal: The input to the MAP algorithm is the count of non-null tokens and the number of non-null tokens at each position. Formal: The total number of document pairs (c_i, c_j) can be calculated by: Formal: We can easily compute the average number of document pairs (c_i, c_j) using this approach. Formal: For the OOB2 sequence, we count the number of O_i tokens at each position, N_i, as well as the number of O_i tokens at each position, N_j. Formal: The total number of O_i sequence tokens (N_i) can be calculated by: Formal: We can easily compute the average number of O_i sequence tokens (N_i) using this approach. Formal: For the IOB2 sequence, we count the number of I_i tokens at each position, N_i, as well as the number of I_i tokens at each position, N_j. Formal: The total number of I_i sequence tokens (N_i) can be calculated by: Formal: We can easily compute the average number of I_i sequence tokens (N_i) using this approach. Formal: For the OOB2 sequence, we count the number of O_i tokens at each position, N_i, as well as the number of I_i tokens at each position, N_",
        "directory": "acl",
        "filename": "2020.acl-main.447.json"
    },
    {
        "casual_text": "Revere mentioned they got an offer from a group of investors who want to buy the company for $16 per share, which adds up to around $127 million.",
        "formal_text": "Input: Revere said it had received an offer from an investor group to be acquired for $16 a share, or about $127 million.",
        "GPT2_formal_text": "ed is calculated like this. Formal: We use a learned language model called L_t to give each word in the sentence a score called δ_t. Formal: We train a standard Transformer model using the cross-entropy loss, with the hidden state set to a hidden size of 200. Formal: After that, we plug the model's predictions into a softmax classifier using the cross-entropy loss. Formal: The final classifier spits out a prediction for the label y, based on a set of labels, Z_L. Formal: Finally, the final label y is calculated using this softmax function. Formal: In this part, we’ll quickly go over some of the concepts and stuff we’ve worked with in this project, and then we’ll introduce the three different neural network setups that help us predict labels. Formal: To keep things short, we’ll skip the nitty-gritty details about how we set up our experiments. Formal: In the next part, we’ll explain the three main neural network setups that help us learn how to classify stuff. Formal: We’ll also look at some examples of what we’ve labeled, and then we’ll give some background on how we did that. Formal: This approach is similar to what we’re doing with the generative model we talked about in Section 3.1. Formal: For the generative model, we use the transformer architecture (Vaswani et al., 2017) that’s been pre-trained on a huge dataset of unlabeled text. Formal: The output from the generative model (h_t) is a vector that represents the label for a specific sequence of tokens, called y_t. Formal: We’ll use this vector as the final label for the text sequence y. Formal: Finally, we also use a standard Transformer model to predict the label y, which is what we’ll focus on in this section. Formal: We’ll quickly go over some of the concepts and stuff we’ve worked with in this project, and then we’ll introduce the three different neural network setups that help us predict labels. Formal: Formal: Formal: Formal: We’ll also look at some examples of what we’ve labeled, and then we’ll give",
        "directory": "C90",
        "filename": "C90-2034.json"
    },
    {
        "casual_text": "For the topic segmentation task, we want to create a way to measure how similar words are, even if they're synonyms or have some kind of related meaning. This similarity measure will help us check how well different parts of the text fit together in terms of meaning. If the meaning starts to feel disconnected, that's when we'll know there's a shift in the story. To make this work, we'll first need to build a way to represent the document that supports this similarity measure.",
        "formal_text": "For the topic segmentation task we would like to define a similarity measure that accounts for synonymy and semantic association between words. This similarity measure will be used to evaluate semantic cohesion between text units and the decrease in semantic cohesion will be used as an indicator of a story boundary. First, we develop a document representation which supports this similarity measure.",
        "GPT2_formal_text": "= c(i) for i in range(k):  \n\ndef skip_tables(c):  \ndef skip_entities(c):  \ndef skip_entities_max(c):  \ndef skip_entities_min(c):  \ndef skip_entities_index(c):  \ndef skip_entities_map(c):  \ndef skip_entities_intersect(c):  \ndef skip_entities_recover(c):  \ndef skip_entities_recover_max(c):  \ndef skip_entities_recover_min(c):  \n\nBasically, these are the different ways to work with the entities. Formal: Once we have the query, we can figure out the entities and their corresponding values, all without needing to look at any extra info beyond what's already in the query. Formal: We can also use the entity info to train a classification model. Formal: In this paper, we take the original query, run it through a classification model, and get the entities that match the given query. Formal: Following what others have done, we can focus on entities that are super common or those that are super rare. Formal: We can combine the ranking of entities with their values to figure out the ones with the highest importance. Formal: This leads us to using ranking scores to train a classifier to classify these entities. Formal: Lastly, we can use this classification model to pick the best matching entity pair. Formal: Formal: We can also add extra info, like the importance of the entity pair and its overall ranking, to the query to help with the ranking. Formal: Formal: Formal: The main goal of this paper is to show that an entity-based approach can help us handle both entity-based and query-based questions at the same time. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D07",
        "filename": "D07-1037.json"
    },
    {
        "casual_text": "The first experiment gave us a bunch of useful facts for travelers, like a destination's weather, laws, and how things work there. But it didn’t really cover the cultural stuff. Now, we’re curious to see if we can dig deeper into that by looking at text written by people who actually live in those countries. This way, we can see what they talk about and how they talk about it, which might give us a better idea of the culture.",
        "formal_text": "The results of the first experiment offer an unsupervised aggregation of factual information that is important to travelers such as a destination's climate, law, and infrastructure; however, the data did not offer much in terms of cultural information. We would now like to see if we can get better insight into this problem by modeling text authored by residents of these same countries. In doing this we can compare what they talk about and in what manner they talk about certain topics.",
        "GPT2_formal_text": "_t = x_t; x_t = x_t + 1; for i=1 to n-1 do { l = w_t * [mask_t[i], mask_t[i+1]]; } while (0 ≤ n - 1). Formal: For the first round of fine-tuning, you take the best model, f_t, using the training set D. Formal: For the second round, you pick the best model, f_s, using the training set D. Formal: For the third round, you pick the best model, f_t, using the training set D. Formal: You keep doing this for each round until you hit a bottleneck, at which point the model will stop improving and you'll need to fine-tune again. Formal: From now on, we'll call this process \"adapting\" when we're training using the best model, f_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D09",
        "filename": "D09-1146.json"
    },
    {
        "casual_text": "We look at how different fixed budgets for creating training data impact different languages and offer some tips for people planning to make datasets in the future.",
        "formal_text": "• we study the effect of different fixed-budget allocation for training data creation across languages, making suggestions for future dataset creators.",
        "GPT2_formal_text": "= [r_t, r_s, ..., r_p, r_a] for every token in the input sequence. The input features are just the average of the hidden state vectors in the lower layers, plus a bias vector called b. \n\nNext, we calculate a hidden state representation h_T h_p using an attention mechanism. This is done using the cross attention mechanism on the input sequence. The score for the hidden state h_T h_p is calculated using the softmax function on the attention part of the input. \n\nWe also consider the input features r_t, r_s, ..., r_p as part of the hidden state, and use a normalization factor of λ to adjust how much we care about each token in the input. The output of the CRF layer is defined by the softmax function on this input. Formal: We keep updating the hidden state representations and the output until the label is changed, or until we hit a certain threshold T. Formal: If we're dealing with an input sequence, we use the hidden state h_T h_p to predict the next token. If not, we just use the output of the CRF layer. Formal: Here, h_s_p = h_T h_p, h_p_s = h_T h_p, ..., h_p_p, h_p_s = h_T h_p, ..., h_p_p. Formal: For the output layer, we use the softmax function on the input. Formal: We calculate the label probability for a token y_t. Formal: We do the same thing for each token y_s_p and token y_p_s. Formal: The output is the probability of the label y_t given the token y_s_p and the token y_p_s. Formal: We use the output of the CRF layer to predict the next token. Formal: We also calculate the label probability for a token y_t. Formal: We use the output of the CRF layer. Formal: Finally, we use the token label probability y_t to calculate the label probability for the current token y_s_p and the token y_p_s. Formal: We use the output of the CRF layer. Formal: We calculate the label probability for each token y_",
        "directory": "acl",
        "filename": "2021.acl-short.79.json"
    },
    {
        "casual_text": "We take a look at some basic stuff for each word in a sentence. For things like SRL and RC, we usually check out the word itself (or its base form if we have it) and its part of speech. For RC, we might also throw in some extra details, like what kind of named entity it is or use WordNet. All these bits of info get turned into small, easy-to-handle numbers, which we call feature embeddings. You can already get word embeddings ready-made by using word2vec on a big pile of text without labels, and they’ve been pretty useful in lots of projects. After that, we mix all these feature embeddings together in a fancy way, so each word can be represented like this:",
        "formal_text": "We extract basic lexical features for each token in a sentence. Typical lexical features for SRL and RC include word (or lemma when available) and POS tag. For RC, additional features can be used, such as named entity type (NE) and WordNet. All these features are then represented as low-dimensional realvalued vectors, i. e. , feature embeddings. Word embeddings can be readily pretrained using word2vec on a large unlabeled corpus, which have proved helpful in many applications. Next, various feature embeddings are composed through a nonlinear transformation, and thus a token can be represented as:",
        "GPT2_formal_text": "ed is set to 1. Figure 4 shows how the attention on a pair of mentions is broken down for different attention mechanisms. For the non-autoregressive model, the attention coefficients are calculated using three different methods: the sparse Gaussian method (from Chiu and Nichols, 2016), the Dirac delta-square method (from Lafferty et al., 2001), and the orthogonal function (from Roth and Lapata, 2006). Formal: To see how the multi-view model can tell which sentence in a document is the main one, we start by adding up the attention weights for all the sentences. Formal: The vectors c_i represent the attention coefficients for the i-th sentence in the document d. Formal: For example, if the vector c_i for sentence i equals 1, then the vectors c_1 and c_2 for sentence i are 1, and c_3 for sentence i is 0. Formal: Formal: Also, we adjust the attention coefficients by dividing them by the sum of their eigenvalues. Formal: Formal: In simpler terms, this is like dividing the attention coefficients by the sum of the eigenvalues, which gives you the attention coefficients for the sentences in the document. Formal: The attention coefficients are multiplied by the vector of the main sentence in the document, and the final representation c_i is calculated using these coefficients. Formal: Finally, the probability of the main sentence being the one that gets picked as the mention is calculated based on this whole process. Formal: Formal: Formal: In this part, we'll explain how to calculate the attention weights for different attention mechanisms. Formal: We set the attention weights α to 0.1, and the attention coefficients to 0. Formal: A sentence is considered the main one if the attention weights for all its mentions are at least 1. Formal: Formal: This is all explained in Algorithm 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1120.json"
    },
    {
        "casual_text": "So, let's break it down: \"a_i\" is the i-th atom in the compound C, N is just how many atoms are in the compound, M is the total number of ways you can pair up two atoms, and the PMI score is calculated using this formula:",
        "formal_text": "where a i is the i-th atom in the compound C, N is the compound length, M is the number of possible combinations of two atoms, and PMI score is computed as:",
        "GPT2_formal_text": "= [ {m, j}, {n, i}, ] where m is the group of events in the collection. Formal: Formal: We also show the normalized cross entropy from the teacher model as αβ_t, which is part of the process. Formal: To get a better idea of how the model works and how it can improve, we tweak the same ranking formula using the normalized cross entropy. Formal: In this setup, αβ_t is calculated using the normalized cross entropy. Formal: We pick a sample from each group based on the average cross entropy of the predictions for each event m. The selected samples are then used to train a teacher model, which is tweaked using the normalized cross entropy to get the final ranking. Formal: We tweak this process by adding some extra conditions. Formal: The model needs to rank the samples correctly. It also needs to perform well on the test set. Formal: For both the ranking and ranking model, we fine-tune the teacher model using the normalized cross entropy. Formal: For ranking, we pick a sample from each group based on the average cross entropy of the predictions for each event m. Formal: We pick a sample from each group based on the average cross entropy of the predictions for each event m. Formal: We pick a sample from each group based on the average cross entropy of the predictions for each event m. Formal: We pick a sample from each group based on the average cross entropy of the predictions for each event m. Formal: We pick a sample from each group based on the average cross entropy of the predictions for each event m. Formal: We pick a sample from each group based on the average cross entropy of the predictions for each event m. Formal: We pick a sample from each group based on the average cross entropy of the predictions for each event m. Formal: We pick a sample from each group based on the average cross entropy of the predictions for each event m. Formal: We pick a sample from each group based on the average cross entropy of the predictions for each event m. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.368.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Here's a list of sounds and some info about them:\n\n**Sounds:** p, b, f, v, m, 8, 4, t, d, s, z, c, n, S, Z, C, j, T, 5, k, g, x, N, q, G, X, 7, h, l, L, w, y, r! V\n\n**Voiced or Not:**\n- Voiced: b, v, m, 8, 4, d, z, n, S, Z, C, j, T, 5, g, N, q, G, X, 7, l, L, w, y, r! V\n- Not Voiced: p, f, t, s, c, x, h\n\n**Place of Articulation:**\n- **Labial:** p, b, f, v, m, 8, 4\n- **Dental:** t, d, s, z, c, n\n- **Alveolar:** t, d, s, z, c, n\n- **Palatal/Post-alveolar:** S, Z, C, j, T, 5\n- **Velar:** k, g, x, N, q, G, X, 7\n- **Uvular:** q, G, X, 7\n- **Glottal:** h\n\n**Manner of Articulation:**\n- **Stop:** p, b, t, d, k, g, q, G, X, 7\n- **Fricative:** f, v, s, z, S, Z, C, x, h, r! V\n- **Affricate:** C, j, T, 5\n- **Nasal:** m, n, N\n- **Click:** None\n- **Approximant:** l, L, w, y, r! V\n- **Lateral:** l, L\n- **Rhotic:** r! V\n\nSo, basically, each sound has its own characteristics like whether it's voiced or not, where it's made in the mouth, and how it's produced.",
        "formal_text": "Features p b f v m 8 4 t d s z c n S Z C j T 5 k g x N q G X 7 h l L w y r! V Voiced 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 Labial 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 Dental 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Alveolar 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Palatal/Post-alveolar 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 Velar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 Uvular 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 Glottal 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 Stop 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 Fricative 1 1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 Affricate 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Nasal 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 Click 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 Approximant 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 Lateral 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 Rhotic 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0",
        "GPT2_formal_text": ". The key parts of this model are the decoder and generator. Formal: We’ve got a model that can create responses that match the input text really well, so the summary we generate is on point. Formal: We’ve used a BiLSTM-based model to generate the summary sentence. This model figures out the attention weights for the different parts of the sentence. Formal: The response generation part is all about picking the right words from the response, based on how the response is structured. Formal: We have a top-notch model for generating responses that can match the input text pretty well. We can also add in extra info, like what the user said before, to improve how we generate the responses. Formal: The main goal here is to connect the response to the input text without any extra help. We’ve come up with a way to do this called attention, which is the main part of the model. Formal: Here’s a quick breakdown of how we’ve designed the attention: Formal: Formal: The decoder picks the best answer from all the options it’s given. Formal: The generator does the same thing but focuses on generating a specific response. Formal: Both the decoder and generator use attention to make the response generation better. Formal: The generator uses a hidden layer to focus on the key parts of the response. Formal: We’ve worked with a bunch of datasets to see how well the model works, and the results show it’s good at it. Formal: We’ve run a bunch of tests and found that using model checking (check out Figure 1) helps the model do a better job with generating responses. Formal: We tested the model with different types of prompts and found that it can actually do better than just guessing based on how users respond. Formal: In the future, we’re thinking about looking into things like attention learning, self-attention, and multi-task learning to see how the model can improve. Formal: We’ve also got a fourth model that uses a different approach. Formal: We tried out different models to see which one works best for generating responses. Formal: We’ve also worked with datasets to see how much we can improve the generated responses. Formal: The generator has some extra info in the hidden layer to help the decoder generate better responses. Form",
        "directory": "C16",
        "filename": "C16-1097.json"
    },
    {
        "casual_text": "Basically, it tries to make the difference between a uniform distribution across K classes (let's call it U(l)) and what the discriminator predicts as shown in equation (3) as small as possible. The final loss for the QA model is a combination of the regular loss (L QA) and an extra loss (λL adv) that depends on how important we think the adversarial part is, with λ being the knob we turn to adjust that importance. During our tests, we switch back and forth between improving the QA model and the discriminator.",
        "formal_text": "In other words, it minimizes Kullback-Leibler (KL) divergence between uniform distribution over K classes denoted as U(l) and the discriminator's prediction as in equation (3). Then the final loss for QA model is L QA + λL adv where λ is a hyper-parameter for controlling the importance of the adversarial loss. In our experiments, we alternate between optimiz- ing QA model and discriminator.",
        "GPT2_formal_text": ". We use a bidirectional LSTM (biLSTM) with a dropout rate of 0.2 to handle the sentence-level embedding. Formal: Unlike the usual beam-based methods, our method is all about sentence embedding, meaning it doesn’t bother with the beam search process. Instead, it just picks the most likely embedding for each token. This approach is kind of like what Karpathy et al. (2016) did, where they trained an LSTM to predict the word embedding for a specific sentence. After that, they fed that prediction into a fully-connected neural network (FCN) to get the final answer. \n\nIn our case, instead of using the entire sentence to learn embeddings, we only grab the embedding for the most likely token in the sentence. Formal: Our approach is different from the regular LSTM because it uses both sentence embeddings and embeddings from the entire passage to predict the answer. Formal: We used BERT embeddings from the x-axis to show the word embedding and the context embedding, while the y-axis shows the actual answer embedding. Formal: We turned sentence embeddings into word embeddings using the nltk library (Bird et al., 2014) and then fed those into our model, along with the predicted answer embedding. Formal: We’re comparing our model to BERT-based and BART-based embeddings. \n\nHere’s what we compared our model to: Formal: We’re comparing our model to the traditional LSTM approach. Formal: We’re comparing our model to the neural network-based approach. Formal: We’re comparing our model to the BERT-based approach. Formal: We’re comparing our model to the neural network-based approach. Formal: We’re comparing our model to the BERT-based approach. Formal: We’re comparing our model to the neural network-based approach. Formal: We’re comparing our model to the neural network-based approach. Formal: We’re comparing our model to the neural network-based approach. Formal: We’re comparing our model to the neural network-based approach. Formal: We’re comparing our model to the neural network-based approach. Formal: We",
        "directory": "D19",
        "filename": "D19-5826.json"
    },
    {
        "casual_text": "For all our experiments, we keep the weights (from Press and Wolf, 2017) of the encoder embedding, decoder embedding, and decoder output layers tied together. This really helps cut down on the number of parameters and speeds up training time. We train for up to 20 epochs and pick the checkpoint with the best oracle metric. We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 and momentum parameters β1 = 0.9 and β2 = 0.999. The minibatch size is 64 for question generation and 32 for abstractive summarization. All the models are built using PyTorch (Paszke et al., 2017) and trained on a single Tesla P40 GPU, running on the NAVER Smart Machine Learning (NSML) platform (Kim et al., 2018a).",
        "formal_text": "For all experiments, we tie the weights (Press and Wolf, 2017) of the encoder embedding, the decoder embedding, and the decoder output layers. This significantly reduces the number of parameters and training time until convergence. We train up to 20 epochs and select the checkpoint with the best oracle metric. We use Adam (Kingma and Ba, 2015) optimizer with learning rate 0. 001 and momentum parmeters β 1 = 0. 9 and β 2 = 0. 999. Minibatch size is 64 and 32 for question generation and abstractive summarization. All models are implemented in PyTorch (Paszke et al. , 2017) and trained on single Tesla P40 GPU, based on NAVER Smart Machine Learning (NSML) platform (Kim et al. , 2018a).",
        "GPT2_formal_text": ". The model learns to mask the attention to focus on the entities from the text. It then uses the attention to predict the rest of the text, just like we see in Figure 2. But in this paper, we show how to add a twist to this process by turning the entity embeddings into context-aware representations. This helps the model learn better word representations and better entity representations. As you can see in Figure 2, this step helps the model see the context of the entities in the text. For our experiments, we set the window size to 5, which is more like real-world applications where the data isn't that big. Formal: The model learns to focus on the entities in the text. We keep track of the attention weights for the text using a DATR notation, as shown in Equation 1. After that, we combine these attention weights to create a single attention vector, which we then send through an LSTM to create the prediction for the text. Formal: The model learns to predict the rest of the text. We train the model using the same DATR notation as before, but this time we use the attention weights to create a single attention vector for the text. Formal: The model learns to predict the rest of the text. We use the same DATR notation as before, but this time we use the attention weights to create a single attention vector for the text. Formal: Formal: The model learns to predict the rest of the text. We use the DATR notation as before, but this time we use the attention weights to create a single attention vector for the text. Formal: Formal: The model learns to predict the rest of the text. We use the DATR notation as before, but this time we use the attention weights to create a single attention vector for the text. Formal: Formal: The model learns to predict the rest of the text. We use the DATR notation as before, but this time we use the attention weights to create a single attention vector for the text. Formal: Formal: Formal: We train the model using DATR notation as before, but this time we use the attention weights to create a single attention vector for the text. Formal: Formal: The model learns to predict the rest of the text. We use the DATR notation as before, but this time we use the attention weights to create a single attention vector for the text. Formal:",
        "directory": "D19",
        "filename": "D19-1308.json"
    },
    {
        "casual_text": "The c-test (Raatz and Klein-Braley, 1981) is a common exercise used to test language skills. In this test, the second half of every other word is missing, and you have to fill in the blanks to complete the text. For instance, the word \"redundancy\" would become \"redun___.\"",
        "formal_text": "The c-test (Raatz and Klein-Braley, 1981 ) is a reduced redundancy exercise frequently used for language proficiency testing. In a c-test, the second half of every second word is replaced by a gap, and the task of the test-taker is to reconstruct the text. For example, the word redundancy would be replaced with redun___.",
        "GPT2_formal_text": "= [ \"tgt-likes\" ] results = [ ] for i, j in pairs (api_arg, results) in pairs (api_arg, results) do query = \"query: \" + query + \"; \" + (i - 1 ) + \"; \" end result = [ ] for i, j in pairs (api_arg, results) do query = \"query: \" + query + \"; \" + (i - 1 ) + \"; \" end result = [ ] for i, j in pairs (api_arg, results) do query = \"query: \" + query + \"; \" + (i - 1 ) + \"; \" end result = [ ] for i, j in pairs (api_arg, results) do query = \"query: \" + query + \"; \" + (i - 1 ) + \"; \" end result = [ ] for i, j in pairs (api_arg, results) do query = \"query: \" + query + \"; \" + (i - 1 ) + \"; \" end result = [ ] for i, j in pairs (api_arg, results) do query = \"query: \" + query + \"; \" + (i - 1 ) + \"; \" end result = [ ] end end end\n\nThere's also a special version of the logic we talked about earlier that works with this new feature structure. Formal: We can now add extra features to the mix without needing to add all those extra features we used to get them. In the next sections, we'll show you how this works. Formal: The new feature structure we've added lets us handle both local and global features together. Formal: We can also mix global and local features together by passing them through the same logic. Formal: The extra features we add can be combined with the ones already in the local and global features. Formal: We can also pass the global features through a local feature if we need to. Formal: Now, let's look at a simpler version of the first formula. Formal: The new feature structure we've added lets us handle both local and global features together. Formal: We can also mix global and local features by passing them through the same logic. Formal: The new feature structure we've added lets us handle both local and global features together. Formal: We can also mix global and local features by passing them through the same logic. Formal",
        "directory": "bea",
        "filename": "2021.bea-1.19.json"
    },
    {
        "casual_text": "Deep neural models are kind of famous for being easy to trick with something called adversarial perturbations—basically, tiny changes to the input that can mess with the model's output, even if the model was trained really well (Goodfellow et al., 2015; Ebrahimi et al., 2018b). There's another type of attack where someone messes with the model itself to create fake weaknesses, so the attacker can control how the model reacts to certain changes. For example, they might tweak the model to always say a sentence has a positive sentiment, even if it’s totally negative. Like, \"This is a train wreck of an action film—a ridiculous attempt by the filmmakers to cram James Bond into the dumb XXX BB mold and throw 40 years of movie history out the window for some flashy lights and loud explosions.\"",
        "formal_text": "Deep neural models are well-known for being naturally susceptible to adversarial perturbations -carefully-chosen modifications of inputs that allow one to control the output of an otherwise well-trained model (Goodfellow et al. , 2015; Ebrahimi et al. , 2018b). Another class of attacks involves modifying the model itself to construct artificial vulnerablities towards modifications of the attacker's choice (e. g. always classifying a sentence as positive sentiment when the attacker in-Sentence Confidence this is a train wreck of an action film -a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx bb mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs.",
        "GPT2_formal_text": "ed, which is like a special kind of attention mask. This mask helps reduce the impact of noisy samples by adding a special token, u_t. Formal: We're using the two word embeddings (w_i and w_j) that were trained on the combined data of the start and end token vectors. Formal: The final hidden state h_T is calculated as: Formal: This step helps the model figure out if there's a relation from the start or end position. Formal: This part is super important because the model needs to pick a relation from the start position or the end position. If the model picks the wrong one, it could mess up the attention distribution. Formal: Let's say the model learns that \"cannot\" is a relation in the start position, and \"can't\" is a relation in the end position. So, the hidden state h_T for relation detection needs to be the same for both positions. To make this happen, we need to set up a special hidden state h_T for the start position. Formal: We're using the updated input representation {h_s, h_t} to create a new input vector {h_t}. Formal: In the end, the input vector {h_t} gets filled with the relation representation h_s and the word embedding w_i. Formal: Formal: The last layer of the network gives us the probability distribution P_t for the relation detection. Formal: We're using the updated output representation {h_s, h_t} to create a new output vector {h_t}. Formal: Finally, we add the hidden state h_t and the word embedding w_i to get the final output vector {h_t}. Formal: The final hidden state h_T is calculated like this: Formal: Formal: Here, H_s is the hidden state for the start position, H_t is the hidden state for the end position, and P_t is the probability distribution for the relation detection. Formal: This whole process is repeated for each relation detection setup. Formal: Formal: Finally, we calculate the hidden state for the target relation detection setup by doing the same process but for the target token. Formal: Formal: The final hidden state h_T is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.249.json"
    },
    {
        "casual_text": "When we include that one weird entry, Figures 2 and 3 show the average human score for each of the seven NIST entries compared to their Bleu score. What stands out is that one entry got a way higher human score than you'd expect based on its low Bleu score. This entry was different because it wasn't a fully automatic machine translation. Instead, it had some help from English speakers who picked the best options from different automatic translations of Arabic sentences and then edited the result (Callison-Burch, 2005). The other six entries were all fully automatic, using phrase-based statistical machine translation. They were trained on the same data and most of them used a Bleu-based method (Och, 2003) to tweak the settings of their translation models (Och and Ney, 2002).",
        "formal_text": "14 when the outlier entry is included Figures 2 and 3 plot the average human score for each of the seven NIST entries against its Bleu score. It is notable that one entry received a much higher human score than would be anticipated from its low Bleu score. The offending entry was unusual in that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005). The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002).",
        "GPT2_formal_text": "= {t_i, t_j, ..., t_n}, where t_i and t_j are part of the set {1, ..., m}, where m is the total number of words in the text. The KWPSI method has three main steps to figure out the arguments for a given document. Formal: In the first step, a system looks at a bunch of words in the document and tries to find the best translation. Then, in the second step, a different system tries to find the best translation for each word in the document. Formal: In the third step, the best translation is picked based on the context around the words in the document. Formal: In the fourth step, the system figures out how to combine the best translations for each word in the document to create a final translation. Formal: Finally, the best translation is chosen. Formal: In this paper, we came up with a new, efficient, and effective way to figure out the arguments for a document. Formal: Our approach uses a convolutional neural network to find the best translation for the word in the document. Formal: We tested how our method works with different models and found that our method works better than a lot of other methods. Formal: We found that our method can improve the overall accuracy of the translation predictions. Formal: We also tested our method against a bunch of other models that try to find the best translation for a word. Formal: From our experiments, we found that our method can better understand the relationship between the words and create better translations. Formal: We used a modified version of the CNN model from Chen et al. (2014) to find the best translation. Formal: We also compared our method with a few other methods for extracting argument spans. Formal: We looked at how our method affects the accuracy of paraphrase extraction. Formal: We found that our method can find paraphrase candidates that are close to the best translation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E06",
        "filename": "E06-1032.json"
    },
    {
        "casual_text": "(a) A single, neat model that works across different sets of semantic graphs (check out section 2), (b) consistent ways to represent and score stuff (sections 4 and 6), (c) comparing how different methods perform (section 5), and (d) more mixing and matching of different parsing techniques (section 7).",
        "formal_text": "(a) a unifying formal model over different semantic graph banks ( §2), (b) uniform representations and scoring ( §4 and §6), (c) contrastive evaluation across frameworks ( §5), and (d) increased crossfertilization of parsing approaches ( §7).",
        "GPT2_formal_text": "_tos[i] is set to 1 and the source attention score is set to s_src. Formal: Formal: Basically, if you're dealing with a sequence of tokens x_t1...x_tT, the attention is spread out across these tokens x_t1, ..., x_tT, and it's calculated as: Formal: For every token x_t, the attention is spread out, and it's calculated by adding up the attention scores from the last four tokens of the input sequence. Formal: For the target sequence, we look at the last three tokens and the previous three tokens of the input sequence. The token with the highest attention score is chosen as the key token for the current token x_t, which is the one that doesn't have any context. This is all shown in equation (2). Formal: The source attention score is calculated as the sum of the source attention scores for each token in the input sequence. Formal: Since we don't have a bunch of training data, we just go with the token that has the highest attention score. Formal: Basically, the attention for a token x_t, given its source attention score s_src, is calculated by adding up the attention scores from the last three tokens of the source sequence. Formal: Formal: Since we don't have a bunch of training data, we just go with the token that has the highest attention score. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "conll",
        "filename": "2020.conll-shared.1.json"
    },
    {
        "casual_text": "As an alternative to the second jump rules, we're testing out a method based on (Huck et al., 2011) that limits reorderings to a pretty strict level. In this version, you can only jump over one continuous chunk of a sentence at a time.",
        "formal_text": "As a second jump rules variant, we try an approach that follows (Huck et al. , 2011) and allows for very constrained reorderings. At most one contiguous block per sentence can be jumped across in this variant.",
        "GPT2_formal_text": "\" (check out Appendix D.4). Formal: One of the cool things about using a keyphrase extraction method to handle this kind of knowledge is that it lets us not just tag the words or phrases with simple predefined categories, but we can also use natural language inference to figure out the exact meaning of any word or phrase we come across. This is all possible because the way we handle knowledge in text is more similar to how we use natural language for understanding concepts (like in [Reiter82]). Formal: In Section 5, we check out how our approach works by focusing on three different groups of knowledge. Formal: We tested two ways of representing knowledge: one where we just use the actual word in context and another where we use the whole context around the word. Formal: The results are in Figure 3, showing how the predictive model performed on the Wiki dataset when we used the actual word as the keyphrase. Formal: From what we can see in Figure 3, our approach worked better for predicting how keyphrases would connect. Formal: We also tried using a hierarchical approach, where we consider the entire keyphrase, not just the main head phrase. However, this approach didn’t perform as well as the simpler approach. Formal: Here, we’ll talk about the issue of having too many keyphrases. Formal: Let’s dive into what we found out about the differences in performance between the two methods, and how this might affect future work in this area. Formal: We used some early tests to see how each method’s performance changed based on the number of keyphrases it had. Formal: Table 1 gives some examples from the Wiki dataset that show the different patterns we saw. Formal: To see if the performance changes based on the number of keyphrases, we picked the ones with the highest accuracy. Formal: We also ran some tests using one keyphrase for each example. Formal: The results are in Table 4, showing the F1 scores for each method based on the number of keyphrases it used. Formal: Using a keyphrase is definitely a good idea if you want to maximize the performance. Formal: But using a keyphrase for each example is also useful, especially if you’re just doing basic statistical analysis. Formal: We also ran some tests with multiple keyphrases. Formal: Formal: Table 5 shows how the results",
        "directory": "eamt",
        "filename": "2012.eamt-1.66.json"
    },
    {
        "casual_text": "In this project, we introduce DISCO, a self-supervised contrastive learning approach that helps in understanding general code patterns and also pinpoints specific traits for detecting vulnerabilities and code clones. Our tests show that even when DISCO is trained on smaller datasets, it can still beat the performance of bigger models, which proves how effective our design is. Just to clarify, these two tokens are keywords, but by looking at their parent node type, we can quickly tell they're part of the same if-statement and are siblings in the AST.",
        "formal_text": "In this work, we present DISCO, a self-supervised contrastive learning framework to both learn the general representations of source code and specific characteristics of vulnerability and code clone detections. Our evaluation reveals that DISCO pretrained with smaller dataset can still outperform the large models' performance and thus prove the effectiveness of our design. just know these two tokens are keywords, but with parent node type, we can easily know that they are from the same if-statement and they are siblings in the AST.",
        "GPT2_formal_text": "ed_shortcuts = { [CLS], [SEP] } Formal: The entity tokens get passed through a gate and get translated using a hierarchical attention layer (like the one from Bahdanau et al., 2015). After that, the entity tokens get combined with the attention weights to create a representation for the entity. Formal: For each label in the dataset, the model takes in an input x_i and a bunch of words {w_1, w_2, ..., w_n} w_i. It then predicts the label y_i for the token v_i. The goal here is to keep the prediction consistent no matter what w_i is, so the whole process doesn’t change based on what w_i is. The model’s parameters are encoded using logits, and the output from the attention layer is passed through a softmax layer. Formal: The score for the label y_i is calculated by subtracting the correct label y_i from the predicted label y_i. Formal: This process is repeated for all the labels. Formal: The model’s weights are encoded in a nonlinear way using logits. Formal: The model calculates the score for the label y_i by subtracting the correct label y_i from the predicted label y_i. Formal: The model calculates the score for the label y_i by subtracting the correct label y_i from the predicted label y_i. Formal: The model calculates the score for the label y_i by subtracting the correct label y_i from the predicted label y_i. Formal: For each label in the dataset, the model takes in an input x_i and a bunch of words {w_1, w_2, ..., w_n} w_i. It then predicts the label y_i for the token v_i. The goal here is to keep the prediction consistent no matter what w_i is, so the whole process doesn’t change based on what w_i is. The model’s parameters are encoded using logits, and the output from the attention layer is passed through a softmax layer. Formal: Finally, the model calculates the score for the label y_i by subtracting the correct label y_i from the predicted label y_i. Formal: Formal: Finally, the model calculates the score for the label y_i",
        "directory": "acl",
        "filename": "2022.acl-long.436.json"
    },
    {
        "casual_text": "When we tweak our task, we mess with the text by either deleting stuff or shuffling it around. In Table 6, we tested different ways: replacing words with masks instead of deleting them, or just deleting words without shuffling. Turns out, our delete-and-shuffle method gives us the best BERT-iBLEU score compared to the other two options.",
        "formal_text": "During task-adaptation, our corruption strategies involve both deletions and shuffling. In Table 6 we provide ablation study results where we either replace words with masks instead of deleting them or delete words without shuffling. We can see that our delete-and-shuffle strategy achieves the best BERT-iBLEU score among the three settings.",
        "GPT2_formal_text": ", and it's a set of m. Formal: Let's say we have n documents, and we want to figure out the chance of r_s from the prediction distribution p_s_t, which we call the distribution p_s_t_i. Formal: For any n and m, the total number of tokens in the text is n/m, and the total number of words in the text is n/w. Formal: If the prediction p_s_t and the language model (LM) (which is the best LMs) say the input matches the source document and the target document, then the input x_t should match the input x_s. Formal: If the prediction p_s_t and the LM (which is the best LMs) say the input doesn't match the source document and the target document, then the input x_t should match the input x_s. Formal: The similarity between two embeddings (e_i and e_j) is calculated as the cosine similarity between the embedding and the input. Formal: The embedding space of the input x_t is E_t = E_s ∝ E_t_i, where E_s is the embedding space of the source document x_s, and E_t_i is the embedding space of the target document x_s. Formal: The embedding space of the input x_t is E_t = E_s ∝ E_t_i. Formal: The embedding space of the output y_t is calculated as the dot product between the embedding and the input. Formal: The log-likelihood of the input is the logarithm of the embedding scores. Formal: The log-likelihood of the output is the logarithm of the embedding scores. Formal: The model is trained using a tweaked version of the Adam algorithm. Formal: The model is trained using a tweaked version of the Adam algorithm. Formal: The model is trained using a tweaked version of the Adam algorithm. Formal: The model is trained using a tweaked version of the Adam algorithm. Formal: The model is trained using a tweaked version of the Adam algorithm. Formal: The model is trained using a tweaked version of the Adam algorithm. Formal: The model is trained using a tweaked version of the Adam algorithm. Formal: The model is",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.417.json"
    },
    {
        "casual_text": "Alright, let me show you an example of a messed-up auxiliary verb that the French-B1 model fixes. In French, the phrase for \"go shopping\" is \"faire des achats,\" and the verb \"faire\" basically means \"make\" or \"do.\"",
        "formal_text": "Below, we give an example of a confused auxiliary verb that the French-B1 adapted model corrects. The verb phrase corresponding to \"go shopping\" in French is \"faire des achats\", where the verb \"faire\" would translate to \"make/do\".",
        "GPT2_formal_text": "ed. Our experiments show that this approach works really well for picking out key parts of a sentence. Formal: We tested it on the NIST 2006 Natural Language Inference task (thanks to Bordes et al., 2006) and got a precision of 98.57% on the test set, and a recall of 97.22%. Formal: It’s pretty clear that this system can perform really well. Formal: Even though the current methods have some cool features, like the ability to handle syntactic and semantic dependencies, they don’t really deal with semantic dependencies. Formal: We came up with a simple system called `SRL with Keywords`, which helps deal with semantic dependencies by adding a keyword feature to the embedding layer. We tested it on the SemEval 2010 Task 4 dataset and got a precision of 98.62% on the test set, and a recall of 97.48%. The keyword feature helps tell the difference between words like \"the\" and \"and,\" which are unclear when you look at the sentence. In the rest of this paper, we’ll talk about our model, the keyword feature, and the results from the SemEval 2010 task. Formal: The results from SemEval 2010 are in Table 3. The model did really well, and it’s pretty clear that using semantic dependencies is key to success. Formal: We used the same setup as in the SemEval 2010 Task 4 dataset to calculate the semantic dependencies in the keyphrase embedding. Table 4 shows the results for the parser outputs from the SemEval 2010 task. Formal: For the English task, the parser outputs are in Table 1. The best baseline model is called `SCFG with Keywords`, which uses the same keyword feature as in SCFG. Formal: We created a new version of SCFG called `SCFG with Keywords + TSNI`. This version uses a keyphrase embedding layer with a TSNI label. The results from SemEval 2010 are in Table 3. The keyphrase embedding layer in SCFG with Keywords + TSNI is based on a TSNI label. Formal: The SCFG model was tweaked to work with both syntactic and semantic dependencies, so it uses a semantic label to tell the difference between words like \"the\" and \"and.\" Formal: The SCFG model also uses a lexicalized embedding layer with a TSNI label",
        "directory": "D19",
        "filename": "D19-5504.json"
    },
    {
        "casual_text": "Deep learning is now the go-to method for tackling most Natural Language Processing (NLP) tasks, like text classification. When you have enough high-quality training data, these models can work amazingly well. But in real-life situations, perfect datasets are pretty rare. Usually, the data you get is small, packed with regular but useless words, and might have some hidden biases (Wiegand et al., 2019; Gururangan et al., 2018). This can result in models that aren't as good as they could be and might have some unwanted traits. For instance, they might be biased against certain groups or not perform well in actual use because they've just memorized the flawed training data.",
        "formal_text": "Deep learning has become the dominant approach to address most Natural Language Processing (NLP) tasks, including text classification. With sufficient and high-quality training data, deep learning models can perform incredibly well. However, in real-world cases, such ideal datasets are scarce. Often times, the available datasets are small, full of regular but irrelevant words, and contain unintended biases (Wiegand et al. , 2019; Gururangan et al. , 2018). These can lead to suboptimal models with undesirable properties. For example, the models may have biases against some sub-populations or may not work effectively in the wild as they overfit the imperfect training data.",
        "GPT2_formal_text": ", the model learns to change its representations based on the input and only keeps the ones that make the most sense for the task. Formal: We use a linear layer to grab the embeddings of the input and output using the attention mechanism. Formal: We build a bunch of feature maps that are made up of a bunch of different features. Formal: In section 5.1, we use the softmax layer to figure out the latent representations for each word in the training set. Formal: We also calculate the cosine similarity between the embeddings of the input and output vectors. This helps us figure out the similarity score between each input word and the output word. Formal: The final output vectors are a matrix with size d, which is in the form of a polynomial, with a length of d. Formal: Formal: For the first step, we calculate the similarity score between the input and output vectors. Formal: We use the softmax layer to get the embeddings of the input and output vectors. We then do the same calculation for the input and output vectors. Formal: Finally, the final output vector is a matrix with size d. Formal: After calculating the similarity score, we combine these embeddings to get the latent representations. Formal: We then get rid of the vectors that don't show any specific relation relation. Formal: Formal: We then use the cosine similarity to measure the similarity between the input and output vectors. Formal: We use the output vectors as the features for the next step. Formal: Formal: After calculating the similarity score, we combine these feature vectors with the input word embeddings to get the output representations. Formal: Formal: We then get rid of the vectors that don't match any specific relation relation. Formal: Formal: Finally, the output vector is a matrix with size d. Formal: Formal: Finally, we use the softmax layer to get the embeddings of the input and output vectors. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.24.json"
    },
    {
        "casual_text": "Alright, so, table wrappers don't always bring in tables. To figure out what they're actually doing, we use two filtering rules to sort things out:",
        "formal_text": "As specified above, table wrappers do not always introduce tables. Two filtering rules are employed to disambiguate their functions:",
        "GPT2_formal_text": "ed = arg max N; pos_p_masked = arg max N; dtype_p_masked = arg max N; ...; dtype_masked = arg max N; pos_masked = arg max N; pos_p_masked = arg max N; dtype_masked = arg max N; The second order model is just a slightly tweaked version of the first one. The input to this second model is the same as the input to the first one. Formal: We calculate the gradient using the cross-entropy. Formal: The final output is a normalized version of the attention matrix, which we then pass through a nonlinear function to get the final label. Formal: We pick the embedding matrix that gives the highest score to the output, using the softmax function. Formal: This method is explained in detail in section 5. Formal: In the third order model, the input to this model is the same as the input to the first one. Formal: The final output is a normalized version of the attention matrix, which we then pass through a nonlinear function to get the final label. Formal: This method is explained in section 5. Formal: We pick the embedding matrix that gives the highest score to the output, using the softmax function. Formal: This method is explained in section 5. Formal: We pick the embedding matrix that gives the highest score to the output, using the softmax function. Formal: We choose the embedding matrix that gives the highest score to the output, using the softmax function. Formal: We choose the embedding matrix that gives the highest score to the output, using the softmax function. Formal: The final output is a normalized version of the attention matrix, which we then pass through a nonlinear function to get the final label. Formal: This method is explained in section 5. Formal: We choose the embedding matrix that gives the highest score to the output, using the softmax function. Formal: We choose the embedding matrix that gives the highest score to the output, using the softmax function. Formal: Formal: The final output is a normalized version of the attention matrix, which we then pass through a nonlinear function to get the final label. Formal: Formal: Formal: The final output is a normalized version of the attention matrix, which we then pass through a nonlinear function to get the",
        "directory": "C00",
        "filename": "C00-1025.json"
    },
    {
        "casual_text": "In this paper, we focus on paraphrasing between sentences and don’t really get into how our work could be applied to paraphrasing longer pieces of text. 2 The symbols and notations we use in the paper are all listed in Table 1.",
        "formal_text": "In this paper we study paraphrase between sentences, and do not address the larger scope of how our work might extend to paraphrasing between arbitrarily large text sequences. 2 The notations used in this paper are listed inTable 1.",
        "GPT2_formal_text": ", but it only considers the closest entities if the closest neighbor is also an entity. This leads to problems when we try to figure out what entity is the closest match in the sentence. We could use the nearest neighbors method from the EM algorithm, but doing that requires knowing how close the entities are within the shortest distance limit we set. Another option is to use the nearest neighbor classifier trained on the training set to estimate the entity type. Formal: The full EM algorithm is explained in Algorithm 1. But instead of just doing the simplest case where we just compare entities by their closest neighbors, we can also compare them based on the types of their arguments. Specifically, we assume that the arguments have the same types as the entities. This means we can calculate the matching scores for the argument pairs. Formal: In the full EM algorithm, it uses two types of function vectors, {h c, c }, to represent the matching scores for argument pairs. Formal: After we've calculated the matching scores, the entity type vectors are then used to train a classifier. Formal: The classifier is trained using the matching scores and the entity type vectors. If the matching scores for the argument pairs are different from the entity type vectors, we calculate the matching scores again using the entity type vectors. The classifier then predicts the entity type for the argument pairs. Formal: After training the classifier, we apply the EM algorithm to figure out the matching scores for the argument pairs. Formal: To find the matching scores for argument pairs, we calculate the matching scores for the argument pairs using the entity type vectors. Formal: Finally, we apply the EM algorithm to figure out the matching scores for the argument pairs. Formal: Formal: The full EM algorithm takes O(|V|) time to run. Formal: We can add this to the existing EM algorithm by doing the same process as before. Formal: We can also easily add the necessary features to the EM algorithm. Formal: We use the same algorithm to calculate the matching scores for the argument pairs. Formal: We can also use the MLE algorithm to compute the matching scores for the argument pairs. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.552.json"
    },
    {
        "casual_text": "Looking at Figure 2, we can see that both MRR and Hit@10 get better and kind of level off around d_s = 20. We did the same thing with the WN18RR dataset and found that the best subembedding dimension there is 4. Check out Table 4 for H@10 results from the FB15-237 validation set, broken down by categories like 1-to-N, N-to-1, and N-to-N.",
        "formal_text": "From Figure 2 we observe that, both MRR and Hit@10 are improved and slowly saturated around d s = 20 The similar experiments are also conducted on WN18RR data set and we find the best subembedding dimension is 4 on WN18RR. Table 4: H@10 from FB15-237 validation set by categories (1-to-N, N-to-1 and N-to-N).",
        "GPT2_formal_text": "kernel for supervised learning, as shown in Figure 2. The multi-mode attention mechanism helps the model understand the context by processing the entire sequence of attention vectors. This approach is often used in various dialogue generation tasks (like in the work by Dong et al., 2019). Formal: We'll start by taking a quick look at the existing multilingual models by giving a quick overview of the standard models from the community (like the ones by Demiriz et al., 2020; Cheng et al., 2020; Sun et al., 2020a). Formal: Next, we'll talk about a few recent advancements in multilingual dialogue generation. Formal: Next, we'll explain the latest methods in multi-modal dialogue generation, including a few new models that look into dialogue acts and internal representation learning. Formal: Lastly, we'll share our results from a multi-modal dialogue generation system. Formal: Formal: We also built a fully-automated system for dialogue act classification. Formal: We tested our method on four dialogue act datasets, which include act info from the MNLI dataset (Tjong Kim Sang and De Meulder, 2014) and multiple-choice act attributes from the Big5 dataset (Wang et al., 2020). The results showed that our model performs better than the current best model. Formal: Formal: To make things easier for future research, we created a dataset with dialogues in a mix of different languages. The dataset is available for researchers to use. Formal: In the next sections, we'll also share a few examples from this dataset. Formal: Formal: Finally, we'll wrap things up by mentioning some future projects in Section 7. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.241.json"
    },
    {
        "casual_text": "Okay, let’s break this down in a simpler way:\n\n1. We added the Chinese words found using Strategy 2 to the system dictionary and trained the Chinese segmenter using the short unit training data, which was set up in Section 3.3.\n\n2. Table 5 shows how well the Chinese-to-Japanese translation worked using the NICT Chinese Treebank. The short unit method performed the best. The Chinese words we found also boosted the BLEU scores a lot. Strategy 2 did better than Strategy 1, except for test sets 2 and 5. We think this is because Strategy 2 found more words, which helped with the unknown word problem.\n\n3. Table 6 shows the results for Chinese-to-Japanese translation using CTB 7. Strategy 2 got better BLEU scores than the baseline, but the improvement wasn’t as big compared to Strategy 1. We looked into why this happened and found that many of the words found in the parallel training corpus and the annotated training data were the same. For example, \"Ë}((protein)\" came from the annotated data and overlapped with \"Ë}(protein)\" and \"((quality)\" from the parallel corpus. When the Chinese segmenter tried to break down \"Ë}((protein)\", this overlap caused inconsistent results.\n\n4. Table 6: Results of Chinese-to-Japanese translation experiments using CTB 7.",
        "formal_text": "• Short unit: Incorporate the Chinese lexicons extracted by Strategy 2 into the system dictionary and train the Chinese segmenter on the short unit training data transformed in Section 3. 3. Table 5 shows the BLEU scores for Chineseto-Japanese translation using NICT Chinese Treebank. Short unit achieved best MT performance. The extracted Chinese lexicons also improved BLEU scores significantly. Besides test set 2 and test set 5, Strategy 2 achieved better improvement than Strategy 1. We think the reason is that Strategy 2 extracted more lexicons which is helpful to solve the unknown word problem. Table 6 shows the BLEU scores for Chinese-to-Japanese translation using CTB 7. Although Strategy 2 obtained higher BLEU scores than the baseline, compared to Strategy 1, the improvement is not significant. We investigated the reason and found that there are many overlaps between lexicons extracted from the parallel training corpus and lexicons extracted from the annotated training data. For example, \"Ë}((protein)\" was extracted from the annotated training data and overlaps \"Ë}(protein)\" and \"((quality)\" extracted from the parallel training corpus. When the Chinese segmenter tries to segment \"Ë}((protein)\", the overlap can lead to inconsistent segmentation results. Table 6: Results of Chinese-to-Japanese translation experiments using CTB 7.",
        "GPT2_formal_text": ", which are basically the query id, the title, and the body of the document. For RTE, the thread ID is treated as a one-hot vector, which is just a list of numbers, like {1, ..., m}. In RTE-2, the document ID is treated as a vector, and the thread ID is represented by {1, ..., t}. Formal: So, the final score for the thread-level annotation in RTE-2 is calculated like this: Formal: In RTE-2, the thread ID is treated as a one-hot vector. The score is calculated using this formula: Formal: So, the final score for the thread-level annotation in RTE-2 is calculated like this: Formal: We're using this formula for the final score of the thread-level annotation in RTE-2. Formal: For example, if the thread has a score of 1, the final score is 0.5. Formal: In this case, we're using the formula for the final score of the thread-level annotation in RTE-2. Formal: The score for the thread is calculated using this formula: Formal: So, the final score for the thread-level annotation in RTE-2 is calculated like this: Formal: Formal: Finally, for each node in the thread graph, the graph structure is turned into a one-hot vector, which is {1, ..., m} Formal: The graph structure is turned into a one-hot vector, which is {1, ..., m}, Formal: Formal: Formal: This is all shown in Figure 2. Formal: In RTE-2, the thread ID is treated as a one-hot vector. The score is calculated using this formula: Formal: So, the final score for the thread-level annotation in RTE-2 is calculated like this: Formal: Formal: Formal: Finally, for each node in the thread graph, the graph structure is turned into a one-hot vector, which is {1, ..., m}, Formal: Formal: Formal: This is all shown in Figure 2. Formal: In RTE-2, the thread ID is treated as a one-hot vector. The score is calculated using this formula: Formal: So, the final score for the thread-level annotation in RTE-2 is calculated like this: Formal",
        "directory": "eamt",
        "filename": "2012.eamt-1.7.json"
    },
    {
        "casual_text": "In Table 5, you can see how much data we filtered out for each neighborhood choice. The global approach ended up losing about twice as much data as the other two methods. The 1k batch neighborhood did just as well as the more detailed document-level neighborhood but made things run way faster—like, more than ten times faster. After digging deeper, we noticed that over 98.5% of the pairs we filtered out in the document-level approach were still there in the batched approach. So, for the rest of our experiments, we stuck with 'Batch Filtering' as our go-to method. We also used batch filtering on each sentence-aligned corpus in section 4.1 to clean them up a bit. Table 4 shows what our training corpus looked like after all the filtering.",
        "formal_text": "In Table 5, we show the percentage of filtered out pairs from the sources for each neighborhood choice. The global approach lost about twice the data compared to the other two. The 1k batch neighborhood achieved comparable performance with respect to the more fine-grained document-level neighborhood while improving running time more than ten-folds. Upon further inspection, we found that more than 98. 5% pairs from the documentlevel filter were present in the batched approach. So, in subsequent experiments, we used 'Batch Filtering' as standard. In addition to the documentaligned sources, we also used batch filtering on each sentence-aligned corpus in section 4. 1 to remove noise from them. Table 4 summarizes our training corpus after the filtering.",
        "GPT2_formal_text": "_t, and the attention_type_t, the output can be anywhere from -1 to 1. This gives us two options:\n\n1) Use the output from the first-order model, called H(x), and adjust it as needed for the second-order model, H(y), using the values it gives us.\n\n2) Or, you could just go with the standard attention model, which is H(x), and use its output to adjust the attention_mask_t values for both models. This approach can lead to some pretty interesting visualizations of the attention. It’s like a multi-view setup that keeps the attention for different parts of the image consistent, which can make it super useful for understanding how images work in general.\n\nCheck out Figure 1 for an example of this multi-view setup. For the image \"Milk,\" the green dots (called \"x-cups\") represent the word \"milk\" and the two image points, \"Milk\" and \"empty.\" The red dots (called \"x-meters\") represent the word \"empty\" and the image points, \"empty\" and \"milk.\" The blue dots (called \"x-feet\") represent the word \"empty\" and the image points, \"empty\" and \"milk.\" The red circles (called \"x-meters\") represent the word \"empty\" and the image points, \"empty\" and \"milk.\" The blue circles (called \"x-feet\") represent the word \"empty\" and the image points, \"empty\" and \"milk.\" Finally, the yellow circles (called \"x-meters\") represent the word \"empty\" and the image points, \"empty\" and \"milk.\"\n\nIt’s clear that the images capture the same semantic stuff across the different points, and they all help explain why there’s a strong connection between the words and the images. This makes it a great way to compare how two similar words and images are understood visually. Formal: From Figure 1, you can see that \"empty\" and \"milk\" are pretty similar, but \"empty\" is more tied to the image points than the word \"empty,\" which is more tied to the word \"milk.\" Formal: In Figure 1, the x-cups represent the words \"empty\" and \"milk,\" and the x-meters represent the words \"empty\" and \"milk",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.207.json"
    },
    {
        "casual_text": "So, we're dealing with a sequence ŷ_s that's sampled from our model, and r is a baseline reward. We're using this self-critical training approach (shoutout to Rennie et al., 2017), where we get the baseline reward r by applying the same reward function r to a sequence ŷ_g that's decoded greedily. Basically, r = r(ŷ_g). From what we've seen in practice, using this self-critical baseline reward really helps keep the training of our summarization model stable.",
        "formal_text": "whereŷ s is a sampled sequence from the model and r a baseline reward. Here we adopt the self-critical training strategy (Rennie et al. , 2017), where we obtain the baseline rewardr by applying the same reward function r to a greedily decoded sequencê y g, i. e. , r = r(ŷ g ). We empirically find that using this self-critical baseline reward helps stabilize the training of our summarization model.",
        "GPT2_formal_text": ", re_ids, and num_rows. Formal: To get a better understanding of the context from the model, we can use a bag-of-words approach to calculate the standard deviation. Formal: We’ve also included some numbers to show how well the model performs. Formal: To get a clearer picture of how well the model’s predictions are doing, we check if the model's prediction follows the same patterns as the gold standard. Formal: We’re comparing this method to a traditional, one-step, multi-label classification process. This approach is known to work really well and has been proven to be pretty efficient. Formal: Since the context comes from the model, it’s important to focus on the important pieces of information. To do this, we have three main steps: Formal: This step looks at the model’s predictions for each query in the question. We then see if the model correctly identifies the important details in the question. Formal: This step looks at the model’s predictions for each query in the question. We then compare the model’s prediction to the gold standard’s answer. Formal: Finally, we check if the model correctly identifies the important details in the question. Formal: Basically, for each query, we look at the model’s prediction and see if it matches the gold standard’s answer. Formal: This method also helps us spot any duplicate queries. Formal: The results are laid out in Fig. 1, showing the BLEU score, precision (P), recall (R), and the F-score. Formal: For the first query, \"Which country is hosting the 2020 Summer Olympics?\", the model correctly identifies the key details. Formal: For the second query, \"Where is the 2019 Summer Olympics?\", the model correctly identifies the key details. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.458.json"
    },
    {
        "casual_text": "In Table 11, we tested what happens when we bump up the learning rate to 5e-5 for RIPPLES on the SST-2 dataset during fine-tuning. Turns out, cranking up the pre-training learning rate doesn’t really help—it messes with performance on clean data and doesn’t give much of a boost to poisoning performance either. The only weird exception is the IMDb dataset, where the loss goes haywire, and the model’s performance on clean data tanks to random guessing levels.",
        "formal_text": "In table 11, we show the results of increasing the learning rate to 5e-5 for RIPPLES on the SST-2 dataset when fine-tuning with a learning rate of 5e-5. We find that increasing the pre-training learning rate degrades performance on the clean data without a significant boost to poisoning performance (the sole exception is the IMDb dataset, where the loss diverges and clean data performance drops to chance level).",
        "GPT2_formal_text": "_type; token_embedding; token_embedding = embedding = [token_embedding]; }\n\nBasically, the (token_embedding, token_embedding) pair represents the embedding of the source token. The [token_embedding] part is just the size of the embedding vector. The [token_embedding] is just the size of the embedding vector. To break it down, the attention mechanism takes a token, token_embedding, and its attention vectors as inputs. The token_embedding and attention vectors are made up of three parts: the token embedding, the token embedding's length, and the attention weights for that token.\n\nWhen we train the model, we first use the target embedding, token_embedding, to check the input text. We then feed that into a fully-connected layer to get the hidden state representation, H. We use a linear layer to figure out the attention weights for the token embedding, and we also have a linear layer to get the attention weights for the attention layers.\n\nAfter that, we apply the last layer of the linear layer, along with the attention weights, to get the final output vector, which is H_T. Formal: We start by combining the token embedding, token_embedding, with the text in the target language. Then, we feed these combined embedding vectors into the fully-connected layer to get the hidden state representation. Formal: Next, we use the attention weights to predict the attention values for the tokens. Finally, we get the final attention vector H_T. Formal: Formal: We also include the weight matrix W_T, which is part of the parameter space of the model, to make sure the attention weights are properly weighted. Formal: Formal: Finally, we pass the final output vector H_T through a linear layer to get the hidden state representation, H_T. Formal: This is the main part of how we train the model. Formal: Formal: After that, we apply the last layer of the linear layer, along with the attention weights, to get the final output vector, H_T. Formal: Formal: After that, we combine the token embedding, token_embedding, and the input text in the target language into one representation. Formal: Formal: Finally, we feed this combined representation into the last linear layer to get the hidden state representation",
        "directory": "acl",
        "filename": "2020.acl-main.249.json"
    },
    {
        "casual_text": "(3) In Section 3.2, we came up with two ways to combine those four groups of words. In the \"-Weighted pooling\" part, we swapped the weighted pooling method for mean-pooling, and that made things worse. Compared to mean-pooling, the weighting strategy does more than just give different words importance based on how meaningful they are. It also brings in the frequency of each word from the data, which has been shown to be useful.",
        "formal_text": "(3) We proposed two strategies for pooling the four word sets in Section 3. 2. In the \"-Weighted pooling\" experiment, the weighted pooling strategy was replaced with mean-pooling, which degrades the performance. Compared with mean-pooling, the weighting strategy not only succeeds in weighing different words by their significance, but also introduces the frequency information of each word in the statistical data, which is verified to be helpful.",
        "GPT2_formal_text": "\": [], ... }, { \"query_ids\": [], \"gold_tags\": [], ... }, { \"query_ids\": [], \"gold_tags\": [], \"tags\": [], ... }] Formal: We're using a simple neural network setup (like the one by Kim, 2014) with a single attention layer and a hidden layer with a dimension of 300. This setup uses the max pooling operation, which helps encode the most important parts. It's trained using word embeddings. Formal: We're also checking out a version of the same model but with 300-dimensional word embeddings (Kim, 2014). Formal: When translating, we calculate the cross-entropy loss using the cross-entropy value from the target language's training data. Formal: We start with a vocabulary of 10,000 words (which is the minimum for a direct translation to the source language). This vocabulary is then used for tuning the model. Formal: After tuning, we tweak the model's settings using the development set. Formal: When using an external dataset, we just run the model once. Formal: For training, we create a feature extractor called f_s using the source language's training data. Formal: We also use a word embedder, E_s, to handle cross-lingual word embeddings. Formal: For tuning, we use the development set to fine-tune the model. Formal: For every pair of translation and target language, we create a word embedder using the target language's training data. Formal: The cross-entropy loss is calculated using the cross-entropy value from the training data. Formal: The weight vector for the feature extractor and the cross-entropy loss is shown in Equation 3. Formal: The cross-entropy loss is then used to predict the probability distribution for the cross-entropy score of the translation. Formal: For each word in the translation, we calculate the cross-entropy loss. Formal: Formal: We then estimate the cross-entropy loss and the feature extractor score using cross-entropy as our loss functions. Formal: We then estimate the cross-entropy loss using the cross-entropy score from the development set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "For the error analysis, we randomly picked 100 wrong predictions. Let’s break down two main types of errors we found: Entity Ambiguity. Even though our entity detection module tags each predicted span with an (entity) type, dealing with entity ambiguity is still the biggest challenge for our system. For example, take the question, \"Who is associated with Jeff Smith?\" Our entity detection module correctly spots \"Jeff Smith\" as an entity and tags it as a \"common name.\" But here’s the catch: the Wikidata knowledge graph has over ten entities with the exact same label and type. This makes it tricky for our entity linking module to figure out the right one. Wikidata entity linking is a relatively new area of research, and it comes with its own set of challenges, like entities having the same labels, non-standard labels created by users, and even multi-word labels (some as long as 62 words) (Mulang et al., 2020b). Adding more context, like entity descriptions and other info from the knowledge graph, could help clear up this ambiguity (Mulang et al., 2020a).",
        "formal_text": "For the error analysis, we randomly sampled 100 incorrect predictions. We detail the reasons for two types of errors observed in the analysis: Entity Ambiguity Even though our entity detection module assigns (entity) types to each predicted span, entity ambiguity remains the biggest challenge for our framework. For instance, for the question, \"Who is associated with Jeff Smith? \" LASAGNE entity detection module correctly identifies \"Jeff Smith\" as an entity surface form and correctly assigns the (entity) type \"common name\". However, the Wikidata knowledge graph contains more than ten entities with exactly the same label and type. Our entity linking module has difficulties in such cases. Wikidata entity linking is a newly emerging research domain that has its specific challenges such as entities sharing the same labels, user-created non-standard entity labels and multi-word entity labels (up to 62 words) (Mulang et al. , 2020b). Additional entity contexts, such as entity descriptions and other KG contexts, could help resolve the Wikidata entity ambiguity (Mulang et al. , 2020a).",
        "GPT2_formal_text": "_x, which is a non-linear function, to the input x. Formal: The attention weights for the input representation h_i are adjusted based on this formula: Formal: Here, α_i, j is just the attention weight for the j-th word in the sentence. Formal: We use cross-attention (C_t) to connect the word embeddings of the two entities during training. Formal: This method is pretty much the same as the one from (Raffel et al., 2020), but instead of using a linear function, we use a non-linear attention mechanism to figure out the attention weights. Formal: This step is pretty much the same as what (Zhang et al., 2021a) did. Formal: After that, we combine the hidden representation h_i with the previous word embedding h_0 to get the new representation h_t. Formal: After that, we figure out the attention weights for each token using the cross-attention mechanism. Formal: This part is similar to what we did in (Raffel et al., 2020), where we calculate the attention weights for each token using cross-attention. Formal: The attention weights for each token are adjusted based on these formula: Formal: We also take the words from the input x and use the attention weights from C_t to get the representations h_t and h_0. Formal: The final representation h_t is the combination of all the embeddings in h_i, and h_0 is the output of the last hidden layer. Formal: Here, α_i, j is the attention weight for the j-th word in the sentence. Formal: Formal: The whole process of using cross-attention for this final representation is outlined in Algorithm 1. Formal: Formal: To keep things simple, we use a binary cross-attention mechanism on top of the non-linear part to make sure we get the best possible representation. Formal: Formal: Lastly, we use the log-likelihood of the output from the last hidden layer to calculate the final representation. Formal: Formal: Finally, we combine the hidden representations of h_t and h_0 to get the representation h_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.72.json"
    },
    {
        "casual_text": "Take it out of the oven and sprinkle some fake bacon bits and potato chips on top. Put it back in the baking dish and bake it without a lid for 30 minutes.",
        "formal_text": "Remove from oven, sprinkle with imitation bacon and potato chips. baking dish; bake, uncovered, for 30 minutes.",
        "GPT2_formal_text": "; step; align = t; prune = t; sub = t; h_sub = t; align = t; step; truncate = t; prune = t; sub = t; h_sub = t; align = t; step; group = t; prune = t; sub = t; h_group = t; align = t; } Formal: In the second case (1), the alignment is done by finding the best match between the word embedding and the reference translation embedding, which are set up as vectors in R^d. Formal: To make this better, we suggest using a modified version of the PARAGRAM-PHRASE model (which is explained in Section 4.2.1). This new model uses a more efficient search process compared to the original PARAGRAM-PHRASE model, using maximum likelihood estimation (MLE) instead of the old approach of MERT (Och and Ney, 2003). Formal: We tested our methods on the Chinese-English translation task using both MLE and MERT models. Table 2 gives the results for the Chinese-English SMT task, using both the original PARAGRAM-PHRASE model and our new models. Formal: Our approach is different from the usual one. Instead of focusing on the alignment, it actually uses the alignment to find the best translation. Formal: Our approach uses some tricks to make the search more efficient. Formal: Our method can also handle data with multiple sentence lengths. Formal: Since the results can vary depending on the language pair, we combined the results for both the Chinese-English and English-Chinese tasks. Formal: We also did some manual analysis to see how our method performs in different situations. Formal: We also looked at how the translation probability affects things. Formal: We noticed that using the FSTs from a common source language (like English) can make things more accurate because they don't depend on the target language. Formal: To test how well our method works on a big scale, we trained a translation model for all languages (including non-English ones) on the same data. We used a beam search with a beam width of 1 to find the best match. Formal: In this project, we used the original PARAGRAM-PHRASE model, which we call the PARAGRAM-PHRASE-MLE model. Formal: The version we're using is the one",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.526.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\nBasically, we've got this equation:\n\nF = e, f D q(D | e, f) log [P Θ D (D | e, f) / q(D | e, f)] + e, f D q(D | e, f) log P Θ (e, f)\n\nWhich can also be written as:\n\nF = e, f log P Θ (e, f) (8) − KL[q(D | e, f) || P Θ D (D | e, f)]\n\nSo, it's combining a bunch of terms and using some log stuff, along with something called KL divergence, to express F.",
        "formal_text": "F = e, f D q(D | e, f) log P Θ D (D | e, f) q(D | e, f) + e, f D q(D | e, f) log P Θ (e, f) = e, f log P Θ (e, f) (8) − KL[q(D | e, f) || P Θ D (D | e, f)],",
        "GPT2_formal_text": "ed(x_i, y_i, z_i) = 1/(n-l+1) for each i from 1 to l+1. This approach adds a nonlinear layer between the model's output and the input to capture attention. Formal: In this project, we introduce an updated version of the CNN/Daily Mail dataset, which you can check out in Table 1. This version uses multi-head attention to boost the model's performance. Specifically, we’ve got three attention layers, each with 4 heads: h_0, h_1, h_2, and so on, up to h_T. We then add a \"layer\" (or kernel) to the CNN/Daily Mail model, which gives us the final hidden representations. Formal: In this setup, the number of hidden layers is T. \n\nThe CNN/Daily Mail dataset has been tweaked a bit—it’s now 80% female-biased and 20% male-biased, and the bias is balanced across the classes. Formal: In the CNN/Daily Mail dataset, the original model had 600-dimensional words and 100-dimensional vectors. Formal: A similar approach was also used for the Diversity-Inclusion Inclusion Correlation Test (DICC) (Karttunen et al., 2007). Formal: In the original dataset, the original setup had 600-dimensional words, 50-dimensional vectors, and a head dimension of 5. Formal: Our updated model, also called the \"Transformer model,\" uses 700-dimensional words, 100-dimensional vectors, and a head dimension of 3. Formal: Our updated model has an embedding dimension of 300 and a head dimension of 5. Formal: This version has 30 heads, including the head of the current token and the next one. Formal: So, to get the final hidden representation, we do this: Formal: We make a copy of the original input x_i, which is h_i, and then we add the updated output h_i to the original input x_i. Formal: So, the updated input is h_i+1, and the updated output is h_i+1+1. Formal: Formal: This means we use the whole input x_i to get the final output h_i. Formal: Formal: We train the model using the same loss function that’s used in the original CNN/",
        "directory": "D14",
        "filename": "D14-1062.json"
    },
    {
        "casual_text": "3. Pruning: We should get rid of parts that probably won't help with the final result right from the start.",
        "formal_text": "3. pruning: constituents not likely to contribute to the final parse should be removed early.",
        "GPT2_formal_text": "are used to set up a comparison between different types of knowledge bases. Formal: The main idea behind using an entity graph is to figure out which entities in a knowledge base are related to a query q and which aren't. Formal: To make this work, we need to get a bunch of entities from the same knowledge base, kind of like creating a knowledge base from scratch. Formal: We use a method called Knowledge Distillation (KD) to pick and combine the entities from the same knowledge base. Formal: We use a k-window RNN to figure out the relations between entities q and the query. Formal: We calculate the similarity between two entities by looking at the graph structure, which is the part we don't need to know before we start. Formal: We use the generated data to pick the k most important entities, which then help us create the final graph. Formal: To make sure our results are valid, we create a fake graph by just picking random entities. Formal: For the entity-relation graph, we first normalize the text using tfidf and then use two-sided permutation to turn it into a directed graph. Formal: We use a bunch of knowledge base entities to train the model. Formal: We use the results from the inference step to train the next model. Formal: To update the model, we use the knowledge base entities as extra context vectors to guide the prediction. Formal: For the knowledge base entity graph, we use a bunch of entities from the same knowledge base to help out. Formal: The final entity embedding for the query is calculated using the knowledge base entities, which are used to calculate the entity embedding. Formal: When generating a new query, we use the entity embeddings from the knowledge base to create the query. Formal: This graph can also be used to check if the query is valid or not. Formal: Formal: Formal: The relation-relation graph we get from this process is shown in Figure 3. Formal: Formal: Formal: We use two different K-nearest neighbor (K-NN) models to check the graph's structure. Formal: We split the graph into three parts. Formal: Formal: Formal: Formal: Formal: The K-NN model gives us three types of graph structures. Formal: Formal: Formal: Formal: Formal: The first two types are for",
        "directory": "A00",
        "filename": "A00-3002.json"
    },
    {
        "casual_text": "First, let's look at the vector representations used to handle mentions. Earlier research used ELMo and pre-trained GloVE (Pennington et al., 2014) for word and character embeddings, but newer models are using RoBERTa (Cattan et al., 2020; Yu et al., 2020b). We tried swapping out BERT-base for RoBERTa-base and adding GloVE alongside BERT in our models (check out Appendix B for the details), and we saw a big drop in performance. We think the big difference between BERT and RoBERTa might be because BERT was trained with Next Sentence Prediction (NSP), while RoBERTa wasn't. The NSP might make BERT better at understanding whole documents, which is super important for coreference resolution. So, we guess that without specific fine-tuning, adaptive pre-training works best for coreference on ECB+. We also noticed that our entity coreference model is less affected by changes in features compared to the event coreference model. For instance, the event coreference model really depends on argument features.",
        "formal_text": "First we examine the vector representations used to encode mentions. While prior work used ELMo and pre-trained GloVE (Pennington et al. , 2014) word and character embeddings, recent models use 74. 8 -0. 9 78. 7 -2. 1 − Arg comp ( §3. 2) 74. 6 -0. 7 78. 3 -2. 5 − CLS (Eq. 1) 74. 5 -0. 8 78. 9 -1. 9 − MP cosine ( §3. 4) 74. 5 -0. 8 79. 1 -1. 7 + GloVE 70. 1 -5. 2 76. 7 -4. 1 + RoBERTa 71. 2 -4. 1 78. 1 -2. 7 RoBERTa (Cattan et al. , 2020; Yu et al. , 2020b). We experiment with replacing BERT-base with RoBERTa-base and with using GloVE in addition BERT in our models (see Appendix B for implementation) and observe large drops in performance. We hypothesize that the substantial performance difference between BERT and RoBERTa is due to the Next Sentence Prediciton (NSP) used to train BERT but not RoBERTa. The NSP may force BERT to learn attention multiple sentences, and therefore to understand the document as a whole, an ability that is important for coreference resolution. Therefore, we hypothesize that without taskspecific fine-tuning, adaptive pre-training is most beneficial for coreference on ECB+. We also observe that our entity coreference model is relatively less susceptible to feature changes than the event coreference model. For example, the event coreference model is particularly reliant on the argument features.",
        "GPT2_formal_text": "ed is a function that figures out the attention mask for the attention part of a dependency path, specifically the path from node i to node j. Here, i and j are the two nodes in the dependency path. Formal: With these two types of embeddings, let's say F_i, F_j, and F_k represent the final hidden representations for the input sentences and the output sentences, respectively. Formal: Then, the score of the output is calculated based on the log likelihood of the actual output sentence. Formal: In this setup, the input sentence is created by starting from the output path and then updating it. Formal: This method works for both negative and positive samples in a dependency path. Formal: The basic model uses the log likelihood to handle negative samples. This is the base model that was used in the original paper by Klementiev and Roth (2016). Formal: We made the best model, which we call C(i, j), by adding a few more features, like attention mask, attention for child nodes, and positional embeddings. The goal is to make the output path as long as possible, aiming to minimize the length-based perplexity loss. Formal: Check out Figure 2 to see how the model works. Formal: The dependency path is created by a common sub-tree that has a direct child node and a sub-tree that includes another direct child node. Formal: The model calculates the score for the output based on the log likelihood of the actual output. Formal: The sub-tree that has the direct child node is the root node. Formal: The final output sentence is created by using the shortest path that includes the root node. Formal: The model can also handle positive samples. Formal: The input sentence is made by using a simple sub-tree with a child node and a sub-tree that includes the sub-tree that contains the direct child node. Formal: Formal: To make sure the model learns the most important features, we add a penalty for not using all the information in the sub-trees. Formal: Formal: In the Negative Sample Model, the input sentence is generated by following the sub-tree that has the direct child node. Formal: Formal: Formal: The negative sample model is a standard model that was used in the original paper by Klementiev and Roth (2016). Formal: Formal: In the Positive Sample Model, the",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.382.json"
    },
    {
        "casual_text": "To address this issue, we're introducing a new task called subjectivity word sense disambiguation (SWSD). The goal is to figure out automatically which words in a text are being used in a subjective way and which ones are being used objectively. We think SWSD is more doable than trying to figure out every single word's exact meaning because it’s less detailed—you don’t always need to know the exact sense of a word. Plus, we believe SWSD can help make contextual subjectivity analysis systems work better by using sense-aware classification.",
        "formal_text": "To tackle this source of error, we define a new task, subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. We hypothesize that SWSD is more feasible than full word sense disambiguation, because it is more coarse grained -often, the exact sense need not be pinpointed. We also hypothesize that SWSD can be exploited to improve the performance of contextual subjectivity analysis systems via sense-aware classification.",
        "GPT2_formal_text": ") = {'username', 'email', 'location', 'location', 'speech', 'text', 'textchat', 'speechtext', 'textchat'}, form_errors = {'email', 'location', 'speech', 'textchat', 'speechtext', 'textchat'}, form_index = {'username', 'email', 'location', 'location', 'speech', 'text', 'textchat', 'speechtext', 'textchat'}}. Formal: Using the collection-based model, we figure out how similar each (u, v) pair in T is based on how often they show up together in the data. Formal: In our dataset, they’re kind of similar but not exactly the same. We use this similarity info to figure out how similar each pair is to each other in terms of their representation in the space. Formal: Formal: To fix this issue, we came up with a new idea: using a type-based approach. Formal: To make this work, we added a new type of feature, called an event type, and tested it out. We focused on finding the most common event types (like \"email\" and \"location\") in the data and then picking the one that’s most similar to each pair. Formal: We also added a new feature, called an event-specific feature, which helps us predict the most common event type for any given pair in the data. Formal: We tested this approach on our dataset, and the results show that it works really well. Formal: For the future, we’re thinking about expanding this to other languages and making it work across different datasets. Formal: We’re also planning to do more experiments to see how this all connects to real-world situations. Formal: We hope our work and the proof of concept we’ve created will inspire more research in the area. Formal: In the next part, we’ll talk about the dataset and its features. Formal: So, we picked some features to focus on based on what we found in the experiments. Formal: We also used another dataset (the GD corpus) for comparison, and the results are in Table 2. Formal: Our findings show that the GD corpus is more helpful for our task than the one we used for our experiments. Formal: Formal: Formal: We tested our method on three different datasets and found that the GD corpus",
        "directory": "D09",
        "filename": "D09-1020.json"
    },
    {
        "casual_text": "Okay, so here's the deal: We've got two literal movement grammars, G1 and G2, with their own sets of stuff (like #1, $1, P1 for G1 and #2, $2, P2 for G2). The important thing is that the domains of #1 and #2 don't overlap at all, meaning dom(#1) and dom(#2) don't share any elements.\n\nNow, we can make a new grammar, G, by combining the stuff from G1 and G2, plus adding a new rule. So, G = (#1 U #2 U {(S, 0)}, S, P1 U P2 U {R}), where R is a rule that says \"S, 0\" can turn into \"S, 0 S2()/x\".\n\nBasically, G will only recognize sentences that both G1 and G2 would recognize. It's like combining their powers into one super grammar!",
        "formal_text": "1 3. 1 Proposition (intersection) Given two literal movement grammars G1 ---(#1, $1, P1) and Gz = (tzz, $2, Pz) such that dom(#l) n dom(#2) = O, we can construct the grammar GI = (#1 U #z U {(S, 0)}, S, P1 U P2 U {R}) where we add the rule R: so -~ =S, O Sz()/x Clearly, GI recognizes precisely those sentences which are recognized by both G1 and Gz.",
        "GPT2_formal_text": ", and this leads to a small but meaningful improvement in the overall performance. We're really interested in seeing how this method can be used in real-world situations. Formal: We're also using the mapping from the CAN model to the target domain as a starting point. Formal: The model is trained using the CAN architecture as the starting point. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the CAN architecture, focusing on the target domain. Formal: The model is trained using the",
        "directory": "E95",
        "filename": "E95-1013.json"
    },
    {
        "casual_text": "Our QA task is kind of like what Mirsha et al. (2016) did, but instead of asking about the sentiment of a paragraph, we ask random questions. Our multitask method for doing both the QA task and predicting gaze is similar to what Klerke et al. (2016), Berrett et al. (2018), and Mishra et al. (2018) did. Specifically, in Equation 4, we use the same loss term as Barrett et al. (2018), which combines an NLP task loss and a gaze prediction loss. The difference is that Barrett et al. (2018) used gaze predictions as input attention weights for the NLP task, but we just treat gaze as an output. This is kind of like how humans read, where eye movements are just a behavior, not something that affects language processing.\n\nOur work is also different from Mishra et al. (2018) because we use a different model and a single auxiliary objective based on gaze. Lastly, Vajjala et al. (2016) collected eye-tracking data from ESL learners for 4 articles from the same OneStopEnglish source we used (Vajjala and Lučić, 2018) to study how text difficulty affects fixation measures and reading comprehension. Our work, however, focuses on a different task and a different group of readers.",
        "formal_text": "Our QA task can be viewed as a generalization of the work in Mirsha et al. (2016; , where instead of being asked about the sentiment of a paragraph, subjects are presented with arbitrary questions. Our multitask approach for jointly performing the QA task and predicting gaze is similar to Klerke et al. (2016), Berrett et al. 2018and Mishra et al. (2018). In particular, in Equation 4 we use the same loss term as Barrett et al. (2018) which consists of a linear combination of an NLP task loss and gaze prediction loss. Our approach differs from Barrett et al. (2018) in that their model uses the gaze predictions as input attention weights for the NLP task, while our model treats gaze only as an output. Our approach provides a parallel to human reading, in which eye movements are an external behavior rather than an input to language processing tasks. Our work differs from Mishra et al. (2018) in the model and the use of a single auxiliary objective based on gaze. Finally, we note that in Vajjala et al. (2016) eye-tracking data from ESL learners was collected for 4 articles from the same source of OneStopEnglish articles (Vajjala and Lučić, 2018) used here, and utilized to study the influence of text difficulty level on fixation measures and reading comprehension. Our work focuses on a different task and a different population of readers.",
        "GPT2_formal_text": "= {[CLS], [SEP], [SEP]}; n_key = 1; for i from 1 to N: {d = i} → {u_i, u_j} {d = u_i, d = u_j} Formal: We use the attention weights for the task as a kind of guide for figuring out the semantic roles we get in each sentence. Formal: So, the final semantic role label for a sentence is like this: Formal: For every word w_i, the attention weight for the type of predicate we're looking at (like a_i, a_j) is calculated as... Formal: Basically, we use these weights to figure out the semantic role of each word in the input sentence. Formal: In our system, the input sentence and the predicate are like two separate things. Formal: This setup lets us handle the task more efficiently by taking advantage of how similar the input sentence and predicate are. Formal: This approach is kinda like how models like BERT (Devlin et al., 2019) work, where they learn to match up predicates and their arguments. Formal: In our case, we use the text features of the predicate and the input sentence to do this matching. Formal: Since the input sentence and predicate are just two pieces of text, we can find an important pair of words (w_i and w_j) using this matching process. Formal: Formal: There are two main ways to use this info. Formal: The first is to use a character-based similarity measure to figure out the semantic role for each word in the input sentence. Formal: The second is to use a two-character string similarity measure to see how similar the two words in the input sentence are to each other. Formal: Formal: Basically, for each word w_i, the attention weight for the type of predicate we're looking at (like a_i, a_j) is calculated as... Formal: Using the input sentence and predicate as input, the attention weights for the type of predicate are calculated based on... Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "conll",
        "filename": "2020.conll-1.11.json"
    },
    {
        "casual_text": "Alright, so we're looking at how source tokens (let's call them \"source(k)\") end up in the target sequence. The question is: do certain source tokens tend to show up more often in specific spots in the target? To figure this out, we're checking how much each source token position (k) contributes overall to the entire target sequence.",
        "formal_text": "Source(k) −→ target. Now we want to understand if there is a tendency to use source tokens at certain positions more than tokens at the others. For each source token position k, we evaluate its total contribution to the whole target sequence.",
        "GPT2_formal_text": "_tokens, where t is the number of tokens in the input. The original attention weights are learned from the input, with the batch size set to 3, and the learning rate starts at 0.1. Formal: For each input token in the text, we add two attention weights: w_i and w_j. Formal: We use the input token as the query and the text as the key, and we calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention weights using equation (3). Formal: We add two attention weights: w_i and w_j. Formal: We calculate the attention",
        "directory": "acl",
        "filename": "2021.acl-long.91.json"
    },
    {
        "casual_text": "We looked at how well the BNN model works for predicting suffixes at the part-of-speech (POS) level. Table 5 shows the accuracy of suffix predictions for English-Russian, broken down by POS. For this part, we used Russian data that was tagged by TreeTagger. We also calculated the average number of suffixes per stem for each POS. Our findings match what Chahuneau et al. (2013) found: predicting suffixes for adjectives is trickier than for other POS types, while predicting verbs is relatively easier, even though verbs tend to have more suffixes per stem. These differences highlight how important it is to consider the context—whether it's the source or target language. For example, adjectives need to match the gender of the nouns they describe, but this often has to be figured out from the target context alone.",
        "formal_text": "We evaluate the suffix BNN model at the part-ofspeech (POS) level. Table 5 provides suffix prediction accuracy per POS for En-Ru. For this analysis, Russian data is segmented by TreeTag-ger. Additionally, we report the average number of suffixes per stem given the part-of-speech. Our results are consistent with the findings of Chahuneau et al. (2013): 11 the prediction of adjectives is more difficult than that of other POS while Russian verb prediction is relatively easier in spite of the higher number of suffixes per stem. These differences reflect the importance of source versus target context features in the prediction of the target inflection: For instance, adjectives agree in gender with the nouns they modify, but this may be only inferred from the target context.",
        "GPT2_formal_text": "(t, c) = {\n\n- [t],\n- [c]\n\n}\n\nThe generative part picks out the pairs of pairs that have the highest weights, using these weights:\n\n- A_t(t, c) = A_t(t, c) + 1/n\n- A_c(t, c) = A_c(t, c) + 1/n\n- A_n(t, c) = A_n(t, c) + 1/n\n- Formal: The weights in the final layer depend on the log probability of the current token. If the log probability is less than 1, we set the weight to 1. If the log probability is 1, we set the weight to 0. Formal: For the final layer of the generator, the weights of the two tokens are calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1175.json"
    },
    {
        "casual_text": "We compared our SkipBERT with some basic models—specifically, 6-layer and 4-layer models. The results show that SkipBERT beats all the other methods when it comes to the GLUE score. When we look at TinyBERT, since we mostly followed their distillation process, our approach still performs better across all tasks. BERT-EMD takes things a step further by using a more advanced, task-specific distillation method based on TinyBERT, which boosts performance even more. However, SkipBERT still comes out on top when you look at the overall score.",
        "formal_text": "By comparing with baselines (we compare with 6-layer models and 4-layer models separately), we can see that SkipBERT outperforms all compared approaches in terms of GLUE score. Compared with TinyBERT, as we mainly follow their distillation process, our approach shows clear advantages on all tasks. BERT-EMD employs a more sophisticated task-specific distillation process based on general-distilled TinyBERT, and further improves the overall performance. Nevertheless, SkipBERT still maintains an advantage in the overall score.",
        "GPT2_formal_text": "ed_max, attention_masked_softmax, and attention_masked_sum. Formal: To find the best masked embeddings for each word, we grab the embeddings of its word_enc and word_dec parts from the input sentence. After that, we use the hierarchical attention network we built to find the best masked embeddings for each word, which we call c_w. Formal: We calculate the attention weights for the masked part by using the sequence-to-sequence model we mentioned earlier (check out Figure 4(b) for more details). Formal: For the output layer, we calculate attention weights by first averaging the sentence representations of the masked tokens in the target language to get the attention vector a_t. Formal: The masked part of the word, we call m_i, is just the sum of the attention weights we calculated for the previous words in the sentence. Formal: Lastly, for the output layer, we update the weights of the output layer by using the mask for the target language. Formal: We also calculate the attention weights for the output layer by averaging the sentence representations of the masked tokens in the target language to get the attention vector a_t_i. Formal: To calculate the attention weights for the output layer, we average the sentence representations of the masked tokens in the target language to get the attention vector a_t_i_1. Formal: Formal: The final output layer, o_o, is calculated by using the attention weights from the output layer for the target language. Formal: We do this for both the target language and the source language. Formal: We do this for both the target language and the source language. Formal: Finally, the final output layer, o_o, is calculated by using the attention weights from the output layer for the source language. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.503.json"
    },
    {
        "casual_text": "In this paper, we're not looking at all proof nets, just a subset of the multiplicative ones—specifically, those from intuitionistic implicative linear logic. Here, sequents have multiple antecedent formulas but only one succedent formula. To handle the intuitionistic aspect with proof nets (since we're dealing with one-sided sequents), we use the idea of polarities: inputs are negative (,) and outputs are positive (o) (Danos, 1990; Lamarche, 1995). This helps us label the formulas—positive ones are succedent formulas, and negative ones are antecedent formulas.",
        "formal_text": "In this paper, we do not consider all the proof nets, but a part of the multiplicative ones: those of the intuitionistic implicative linear logic. In this case, sequents are made of several antecedent [brmulas, but only one succedent formula. To deal with tile intuitionistic notion with proof nets (since we consider one-sided sequents), we use the notion of polarities with the input (, : negative) and the output (o: positive) (Danos, 1990; Lamarche, 1995) to decorate formulas. Positive ones correspond to succedent formulas and negative ones to antecedent formulas.",
        "GPT2_formal_text": "; dt = find_dt(cn, arg); Formal: We need to tweak the input to be able to handle multi-turn conversations. Let's say dt = ctx * arg is the new input we get after doing a full round of CLWE decoding. The final response we generate is what we're aiming for. Formal: We're sharing the results of our experiments with the community. Formal: We're using the BERT-base model (Devlin et al., 2018) with 200 hidden units and an embedding dimension of 300. For the Transformer architecture, we're using the pre-trained RoBERTa model (Liu et al., 2019b) with 3 layers and a hidden dimension of 300. Formal: To train the model, we start by training the first classifier. The first layer of this classifier is trained to predict the type of utterance in each turn, which we call \"Turn t.\" It's trained using a softmax function on the utterance embedding. The classifier is trained for three epochs, which means it's trained for 3 rounds. Formal: To evaluate the model, we test the first classifier using cross-entropy loss on the test set. Formal: Lastly, we calculate the weighted sum of all the utterance embeddings for each turn and combine them into a single representation, called the utterance embedding, for the same turn. Formal: Formal: We also took a closer look at how the model performs across different inputs. We tested the model on three different samples from the test set and found that different inputs affect how well the model performs. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "A00",
        "filename": "A00-2010.json"
    },
    {
        "casual_text": "The first step in the process we're talking about is creating a special kind of language representation that fits the topic we're focusing on. This method looks for groups of n-grams—which are just chunks of words—in paragraphs that seem to pop up a lot and connect with other paragraphs. We call these \"useful\" n-grams because they show up often and are spread across different texts in the same area of interest.",
        "formal_text": "The fundamental stage in the process described above is the generation of a domain specific semantic representation. The approach identifies clusters of useful n-grams within paragraphs which correlate with other paragraphs. The term useful defines n-grams that have certain qualities, such as a high frequency of occnrrence, and a wide distribution over texts within the domain.",
        "GPT2_formal_text": "ed, attention_hidden, and attention_masked_4d layers in a CNN. Formal: We use pre-trained Transformers for the masked entity embedding, which are GPT-2 (from Radford et al., 2019) and BART (Lewis et al., 2020). Formal: The input sentence has a title, the entity name, and the title's entity type. Then, the entity embedding (E) is used to boost the output probability by minimizing the cross-entropy between the entity type (e_t) and the title's type (e_t). Formal: The input sentence has a title, the entity name, and the title's entity type. The output probability is calculated using this output. The entity embedding (E) is created using the entity type info of the title's entity type. Formal: Formal: The input sentence has a title, the entity name, and the title's entity type. The output probability is calculated using this output. Formal: The input sentence has a title, the entity name, and the title's entity type. The output probability is calculated using this output. Formal: Formal: The input sentence has a title, the entity name, and the title's entity type. The output probability is calculated using this output. Formal: The input sentence has a title, the entity name, and the title's entity type. The output probability is calculated using this output. Formal: Formal: We fine-tune BART with the entity type info from the title's entity type. Formal: We use pre-trained Transformers to create the input sentence. Formal: The input sentence has a title, the entity name, and the title's entity type. The output probability is calculated using this output. Formal: The input sentence has a title, the entity name, and the title's entity type. The output probability is calculated using this output. Formal: Formal: We fine-tune BART with the title's entity type info. Formal: We use pre-trained Transformers to create the input sentence. Formal: We use pre-trained Transformers to create the input sentence. Formal: We use pre-trained Transformers to create the input sentence. Formal: We use pre-trained Transformers to create the input sentence. Formal: We use pre-trained Transformers to create the input sentence. Formal: We use pre-trained Transformers to create the input sentence. Formal: We",
        "directory": "C94",
        "filename": "C94-2171.json"
    },
    {
        "casual_text": "We considered the features of the given kernels and the setup of the semantic-annotated data. Based on that, we came up with the tree structures shown in figures 2(a), 2(b), and 3 for STK and PTK, and sequential structures for SK, as explained below (all these structures are based on the same example from Section 3, which is \"Ho un problema col monitor\"). The structures we used for SK are:",
        "formal_text": "We took into account the characteristics of the presented kernels and the structure of semantic annotated data. As a result we designed the tree structures shown in figures 2(a), 2(b) and 3 for STK and PTK and sequential structures for SK defined in the following (where all the structures refer to the same example presented in Section 3, i. e. Ho un problema col monitor). The structures used with SK are:",
        "GPT2_formal_text": "= [q_s, q_i] key_value = [k_s, k_i] A = {[A_1, A_2, ..., A_N], s, t} Formal: Once we've got the key_value pairs, we pull out the triples for each entity pair by using a Graph Attention Network (GAT). Formal: We use a single-layer GAT with a hidden size of 300 to grab the entity pairs. Formal: We start by encoding the entity pairs into a 300-dimensional GloVe vector. Then, we feed this vector into the GAT to get a new representation for each entity pair. Formal: We keep doing this for the entire entity pair pair, H_i, using GAT's forward and backward hidden states. Formal: Finally, we get the final triple for entity pairs by using GAT's final output. Formal: To get the triples for entity pairs, we use GAT to encode the input vectors in the same 300-dimensional format as the entity pairs. Formal: We apply GAT's forward and backward hidden states to the input vectors to get the entity pairs. Formal: We use the GAT's output to create the triple for each entity pair. Formal: For instance, if we're encoding the input {[A_1, A_2, ..., A_N], s, t} for entity pairs {[A_1, A_2, ..., A_N], s, t} to get the triple {[A_1, A_2, A_N], s, t} and the output [s, t] = A = {[s, t]}, we use GAT's forward and backward hidden states to get the entity pairs. Formal: Using GAT, we encode the input vectors in the same 300-dimensional format as the entity pairs. Formal: We use the output from GAT to get the triples for entity pairs. Formal: Formal: After we've got the triples for entity pairs, we use GAT's output to get the triple for each entity pair. Formal: Formal: Formal: Once we've got the triple for entity pairs, we use GAT's output to get the triple for each entity pair. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1112.json"
    },
    {
        "casual_text": "Lastly, there's the \"end with the same punctuation\" thing. This is another yes-or-no feature. It's a 1 if both end with the same kind of punctuation—like a period, exclamation mark, etc.—or if neither has any punctuation at the end. If that's not the case, then it's a 0.",
        "formal_text": "Finally, ( ) is the \"end with the same punctuation\" feature. This is also a binaryvalued feature which is 1 if both and end with the same type of punctuation: period, exclamation mark, etc. It is also 1 if both and lack final punctuation. ( ) is 0 otherwise.",
        "GPT2_formal_text": "ed, attention_type_masked, and attention_type_max hidden layers. Formal: The entity representation y = (v_i, w_i, t_i) is basically the feature representation from the entity layer. If the feature representation isn't in the embedding space, it can be figured out by applying some nonlinear transformation. Formal: Since entity embedding space is the only thing that can match the type of embedding, we get to use a simple linear projection. For the type-based embedding, the projection is just a linear transformation of the feature vector v_i. So, basically, the entity representation gets turned into a kind of numerical representation. Formal: The features for the entity layer are represented as feature vectors f_i = (v_i, w_i, t_i). Formal: Since the input for entity layer is a grid of numbers, it's super easy to combine the values of these vectors to get the final answer. Formal: Lastly, the output from the entity layer is a matrix called Y. Formal: To get the attention-based representation, we calculate the attention matrix A_i = (v_i, w_i, t_i) by doing some nonlinear operations. Formal: The attention matrix A_i is just a linear transformation of the hidden representation of the entity layer, showing how much attention is given to different aspects. Formal: Since attention is a linear transformation, the attention weights for different aspect embeddings are calculated by doing nonlinear operations. Formal: This process, shown in equation (9), is repeated for each aspect embedding as we keep adding more aspects to the model. Formal: Formal: The final representation of the entity layer is a matrix Y, which is a linear transformation of the entity layer embedding. Formal: Formal: We also add a linear transformation to the input hidden states. Formal: Here, we're keeping track of the number of facets in the entity layer, which we call n_G. Formal: In the entity layer, we use the first n_G facets as the input for each aspect embedding. Formal: Formal: Finally, the output from the entity layer is a matrix Y. Formal: Formal: Finally, the output from the entity layer is a matrix Y. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2012.eamt-1.37.json"
    },
    {
        "casual_text": "Over the last few years, there's been a growing interest in using syntax-based methods for statistical machine translation. A lot of the work has been focused on improving the translation model, not the language model. But, the big differences in syntax between parallel texts make it tricky to learn syntax-based translation rules effectively. Maybe putting more effort into syntax-based approaches within the language model could be a better way to fix the grammar issues in most machine translation results.",
        "formal_text": "In the past few years there has been a burgeoning interest in syntax-based approaches to statistical machine translation. Most of this effort has been directed at the translation model rather than the language model; however, the level of syntactic divergence in parallel text makes it difficult to reliably learn syntax-based translation rules. Focusing on syntax-based approaches within the language model might be a more direct way to address the poor grammatically of most machine translation output.",
        "GPT2_formal_text": "[i] = d_i; } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } } }",
        "directory": "amta",
        "filename": "2008.amta-papers.16.json"
    },
    {
        "casual_text": "I think it's a good idea to comfort them as you learn more about what's going on. Have you considered talking to your parents or a close friend about this?",
        "formal_text": "I should comfort him/her when gradually learning about his/her situation (Providing Suggestions) Have you thought about talking to your parents or a close friend about this?",
        "GPT2_formal_text": "= [mask] + [attention_length] # The attention module is used to calculate the attention vector for the input. Formal: The output we get after using attention is the same as what the model would generate if we didn't have that attention mask. Formal: So, how does the model use attention? It depends on the task, the document type, and how the text is structured. For example, if we're looking at how a user interacts with a navigation app, we use attention for things like the navigation state (e_s_nav), the current object (e_o_n), and the previous object (e_p_n). In a sequence labeling task, we also use attention for the sequence elements (e_s_c, e_o_c) and the current sequence (e_p_c). Formal: For each hidden state h_t_i, the attention module calculates an attention vector by averaging all the entity representations within that span, like in Figure 3. Formal: We set the attention parameters to a fixed value called [mask] and set the input length to 300. Formal: The output from the attention module is the same as what the model would generate if we didn't have that attention mask. Formal: Formal: When we apply attention to a context, we just pick the most relevant entity representation (let's call it e_c_i) by maximizing the mutual information (MI) between the input and output. Formal: The MIMIC-III dataset has 5,998 examples in each of the three types of knowledge graph scenarios mentioned in Section 1. Formal: The dataset, with 10,000 examples for each type of knowledge graph, has one example for each of the three types of knowledge graph scenarios. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.269.json"
    },
    {
        "casual_text": "We’re also working with a semi-supervised learning method called self-learning or pseudolabeling. Basically, this involves training models using predictions made by an earlier version of the model on unlabeled data. This approach has been pretty successful across a bunch of language tasks, like named entity recognition (Collins and Singer, 1999), word sense disambiguation (Mihalcea, 2004), and parsing (McClosky et al., 2006). It’s also been applied in computer vision, and there’s a good overview of that. For task-oriented dialog systems, Cho et al. (2019) found that using self-learning to introduce new features led to significant error reduction. While our method is still based on self-learning using MARUPA-labeled data, we’re taking it a step further by incorporating user feedback, like paraphrasing and friction, as extra signals to improve the self-learning process.",
        "formal_text": "Our work is also related to the semi-supervised learning approach known as self-learning or pseudolabeling, in which models are trained on predictions that a previous version of the model made on unlabeled data. This idea has been successfully applied to a wide range of language tasks, e. g. named entity recognition (Collins and Singer, 1999), word sense disambiguation (Mihalcea, 2004) and parsing (McClosky et al. , 2006). Successful applications are also known from computer vision, of which gives an overview. For task-oriented dialog systems, Cho et al. (2019) report substantial error reductions using self-learning to bootstrap new features. While the use of MARUPA-labeled data for training is at its core self-learning, our approach goes further by leveraging user feedback in the form of paraphrasing and friction as additional signals to guide the self-learning process.",
        "GPT2_formal_text": "= {s_t, s_b, ..., s_a} j, where j ranges from 1 to T. Formal: The question and answer text are both mentioned in the article, so we take all the sentences in the article that mention either the question or the answer as input. Then, we run a maximum entropy classifier on this text. Formal: We treat the question and answer text as the \"source\" and the whole article as the \"target\" to make sure our prediction is accurate and contextually relevant. Formal: In this setup, the model is trained using a dataset of questions and answers. Each source sentence is turned into a vector (like a number between 0 and 1), and each target sentence is turned into a vector too (like a number between 0 and 1). Formal: The teacher model is trained using a dataset of questions and answers that have been labeled by humans, and each target sentence is labeled by itself. Formal: The model learns by using a dataset of questions and answers. Formal: The model is trained using a dataset of questions and answers. Formal: To check how well the model's predictions are accurate, we calculate the conditional probability P_CI(s_t|s_t+1) for each target sentence s_t by checking the conditional probability P(s_t|s_t+1). Formal: We train the model using a dataset of questions and answers. Formal: The model learns by using a dataset of questions and answers. Formal: The model is trained using a dataset of questions and answers. Formal: The model is trained using a dataset of questions and answers. Formal: Formal: Formal: In this paper, we use the semantic role label for the target sentence s_t. Formal: The target sentence s_t is the sentence we're trying to predict the answer to. Formal: We train the model using a dataset of questions and answers. Formal: Formal: Formal: Formal: The model learns by using a dataset of questions and answers. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-industry.3.json"
    },
    {
        "casual_text": "To dive deeper into the issues we're looking at, let's quickly go over some background info and basic terms.",
        "formal_text": "To better elaborate the investigation of the above problems, background knowledge and notations are briefly introduced here.",
        "GPT2_formal_text": "ed, which is just a fancy way of saying it doesn’t do any extra post-processing. The whole thing is built using PyTorch, which is the same one that was used in the original paper by Pascanu et al. (2017). Formal: The number of epochs in the training set is 20. For a specific topic f, the training size is M. Formal: The vocabulary size for the main dataset is 512. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re also using a few other hyperparameters like the learning rate and the batch size. These are listed in Table 3. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re also using a few other hyperparameters like the learning rate and the batch size. These are listed in Table 3. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re also using a few other hyperparameters like the learning rate and the batch size. These are listed in Table 3. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Formal: We’re using the Adam optimizer with a learning rate of 0.001. The batch size is 32. Form",
        "directory": "blackboxnlp",
        "filename": "2021.blackboxnlp-1.16.json"
    },
    {
        "casual_text": "In Figure 2: On the inside pass (left side), DIORA combines two nearby vectors. On the outside pass (right side), DIORA calculates the values for a specific span (i, j) by using its sibling inside span (j +1, k) and outside spans (0, i 1) and (k + 1, n 1). The sibling span during the outside pass can be to the left of the target span, so the indexing gets adjusted accordingly.",
        "formal_text": "Figure 2: In the inside pass (left) DIORA composes two neighboring vectors. In the outside pass (right) DIORA computes the values for a target span (i, j) recursively from its sibling inside span (j +1, k) and outside spans (0, i 1) and (k + 1, n 1). The sibling span on the outside pass can appear to the left of the target span, in which case the indexing is adjusted.",
        "GPT2_formal_text": "and attributes, which are in the form x = {x 1, . . . , x T } for each token x t in the input. Formal: If we have a sequence of tokens x t = x 1, . . . , x T and a labeled sentence S t, we can turn a query query into a sequence of tokens x t using this process: Formal: For a query q, we set a special number, M, to decide how many tokens we're dealing with. Formal: We can also add a little more detail to the original input query q by adding extra attention data that points to specific parts of the sentence that are relevant to the query q. Formal: Instead of using an encoder-decoder model, we can train a decoder using the output from the query and the attention we get from the relevant parts of the sentence. Formal: We’ll call the final output of the decoder, which is the probability distribution for the question (p(q|x t |S t)) given the input sentence x t, the \"output query.\" Formal: For the question vector q p, the output query x t has a specific set of features, which are the attention weights from the relevant parts of the sentence. Formal: We’ll define the query vector q = x t ∈ R kp as q p ∈ R dp × |q p |, where dp is the number of dense matrices in each dimension. Formal: Formal: Basically, the query vector q p is the probability distribution for the question, which we get by training a decoder with this output vector q p. Formal: Since both the input and output vectors are normalized, we can use equation (1) and (2) together to get the final probability distribution for the question, p q = (q p ∈ R kp + q p |q p ). Formal: Once we’ve got the probability distribution for the question, we can calculate the probability for the rest of the sentence, which we can call p φ, p φ. Formal: If we have a sentence s = x t and a query q p, we can use equation (2) to calculate the probability for the rest of the sentence, which we can call p φ, p φ. Formal: Finally, the probability distribution for the rest of the sentence, p φ, p φ, can be written",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.392.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\n- HMGCN's different versions:  \n  - HMGCN without \"cat\" just uses GCN co and GCN prop to figure out the type.  \n  - HMGCN without \"prop\" uses GCN co and GCN cat to do the same type inference.",
        "formal_text": "• HMGCN's variants: HMGCN no cat only use GCN co and GCN prop to make type inference. HMGCN no prop only use GCN co and GCN cat to make type inference.",
        "GPT2_formal_text": "_tokens (d) = {(d1, d2, ..., dT)}, where d1 is the first token, d2 is the second token, and so on. Formal: This approach lets the system figure out the attention mask for each token separately. Formal: Figure 2 shows how to use the text features from Figure 1 to calculate attention mask_tokens. Formal: The updated versions of the text features, like the token and lemma tokens, are calculated using the same equation as in Equation 1. Formal: In this experiment, we tested our model by running it on the LJSpeech dataset. The results are in Table 4. The performance is measured using ROUGE-1, ROUGE-2, and ROUGE-L. Formal: We ran the model 100 times and compared the results with the original model to the ones using our updated model. Formal: In the second experiment, we wanted to see if the model's ability to learn text features depends on how many tokens are in the sequence. We ran the model 10 times and calculated the accuracy for the sequence length. Formal: Table 3 shows the F1 scores for the \"Unseen Words\" task on the LJSpeech dataset, with and without our model. Formal: We tested our model on the Yelp dataset and compared the results with the original model to those using our updated model. Formal: We ran the model 100 times and calculated the accuracy for the sequence length. Formal: This is the first experiment we've done where we actually calculate attention mask_tokens. Formal: We ran the model 100 times and calculated the accuracy for the sequence length. Formal: We compared the results with the original model to those using our updated model. Formal: For the third experiment, we used the same model but skipped the token and lemma mask features. Formal: We ran the model 100 times and calculated the accuracy for the sequence length. Formal: We compared the results with the original model to those using our updated model. Formal: For the fourth experiment, we used the same model but skipped the token and lemma mask features. Formal: We ran the model 100 times and calculated the accuracy for the sequence length. Formal: We compared the results with the original model to those using our updated model. Formal: Lastly, for the last experiment, we used the same model but skipped the token and lemma",
        "directory": "D19",
        "filename": "D19-1502.json"
    },
    {
        "casual_text": "The results for our Farsi-to-English MT system are in Table 2. We compared our approach to two baselines: monotonic translation and a distance-based penalty model, which is pretty well-known. The distance-based model didn’t make much of a difference: the BLEU score went up from 29.1% to 29.4%, and the WER improved by the same amount. When we switched to the run-based penalty model with the 4 features mentioned in Section 3.4, we saw a bigger jump—the BLEU score improved by 1.5% and the WER by 0.7%, compared to the monotonic translation. So, the run-based model clearly did better than the distance-based one.\n\nLooking at the scaling factors for the short, medium, and long-range penalties after optimization, we noticed that the short-range factor was small but negative, which basically gave a bonus for local reorderings. On the other hand, the penalty for a long-range jump was 10 times higher than for a medium-range one. Table 3 shows examples of how the run-based model fixed word order and improved translation quality compared to the distance-based model.\n\nNext, we tried applying parse-based reordering rules. When we used these rules on the source sentences and then did monotonic SMT (the \"hard\" reordering in Table 2), the MT error measures actually got worse.",
        "formal_text": "The experimental results for the Farsi-to-English MT system are shown in Table 2. There are two baselines to which we compare our method: the monotonic translation and the translation using the well-established distance-based penalty model. We see that the positive influence of the distance-based model is small: the BLEU score improves from 29. 1 to 29. 4%, the same absolute improvement is achieved in terms of WER. When we now use the run-based penalty model with the 4 features described in Section 3. 4, we improve the BLEU score by 1. 5% absolute and WER by 0. 7% absolute as compared to the monotonic translation. Thus, the run-based penalty model clearly outperforms the distance-based penalty model. Checking the feature scaling factors for the short, medium, and long range new run penalties after the optimization, we observed that the factor for the short-range feature was small, but negative, thus assigning a bonus to local reorderings. In contrast, the penalty for a longrange jump was 10 times higher than for a mediumrange one. Examples of improved translanslation word order and quality when using the run-based penalty model instead of the distance-based model are presented in Table 3. Next, we tested the application of parse-based reordering rules. Applying these rules to the source sentences and then performing monotonic SMT (the \"hard\" reordering in Table 2 ) resulted in a degradation of the MT error measures.",
        "GPT2_formal_text": "(1, 2, 3), which include the most common words in the document. Then, it uses a concatenation model to get sentence representations. Formal: For each thread, the student model creates a new summary using the input_ids and calculates the cross-entropy loss between its outputs. Formal: The best summary for a thread starts off being the output with the highest cross-entropy loss, as seen in Figure 3b. Formal: We're using the Stacked-RNNL model as our basic model to create summaries for a new thread. Formal: Next, we take the student model and tweak it a bit more by using a single-layer LSTM (from He et al., 2016) and a Gated Recurrent Unit (GRU) (Hochreiter and Schmidhuber, 1997). Formal: For each input_id, the student model generates a summary_i based on the previous outputs from the thread and the training data. Formal: To train the single-layer LSTM, we use a grid-search method and a training set with 8,000 threads. Formal: In this situation, the input_ids are chosen based on how good the summary is compared to the others. Formal: Lastly, for each graph (which is just a group of input_ids), the student model creates a new graph using the previous outputs from the thread and the training data. Formal: We're using the GDSE model to generate the next summary. Formal: You can check out the full code here: https://github.com/avigod/threadsense Formal: To train the GRU, we use a grid-search method and a training set with 8,000 threads. Formal: Check out Figure 4 for the results of our experiments. Formal: The GRU-based model we're using is based on the one by Mao et al. (2019). Formal: We run our experiments five times, using different amounts of threads to calculate the results. Formal: Results are averaged across all five runs. Formal: The average accuracy is listed for each model. Formal: To make things fair, we also run experiments with different numbers of input_ids. Formal: From the figure, you can see that different GRUs perform differently. Formal: Formal: To figure out the best model for a task, we look at the average accuracy for each model. Form",
        "directory": "amta",
        "filename": "2010.amta-papers.29.json"
    },
    {
        "casual_text": "Another way to combine phrases works at the sentence level. This method looks at how different machine translation systems handle the same source sentence. It gathers all the phrase pairs that these systems use to translate that sentence. Basically, it gives more weight to phrase pairs that are picked by lots of different decoders.",
        "formal_text": "Another phrase combination strategy works on the sentence level. This strategy relies on the consensus of different MT systems when translating the same source sentence. It collects phrase translation pairs used by different MT systems to translate the same sentence. Similarly, it boosts common phrase pairs that are selected by multiple decoders.",
        "GPT2_formal_text": "_t = attention_mask_t * d_mask_t; + + return (sum_s * sum_q * sum_d); + } + +static int +is_subword(const char *s, int *c, int *p) +{ + if (s[p] == NULL) + return 0; + else + return 1; +} + +static int +is_token(const char *s, int *c, int *p) +{ + return (s[p] == NULL) || + (s[p] == NULL || + (s[p] == NULL || + s[p] == NULL))); +} + +static char * +is_word_literal(const char *s, int *c, int *p) +{ + return (s[p] == NULL) || + (s[p] == NULL || + s[p] == NULL) || + s[p] == NULL) || + s[p] == NULL); +} + +static char * +is_token_literal(const char *s, int *c, int *p) +{ + return (s[p] == NULL) || + (s[p] == NULL || + s[p] == NULL) || + s[p] == NULL) || + s[p] == NULL); +} + +static char * +is_word_literal(const char *s, int *c, int *p) +{ + return (s[p] == NULL) || + (s[p] == NULL || + s[p] == NULL) || + s[p] == NULL); +} + +static int +is_token_literal(const char *s, int *c, int *p) +{ + return (s[p] == NULL) || + (s[p] == NULL || + s[p] == NULL) || + s[p] == NULL); +}\n\ndiff --git a/src/util/strings.py b/src/util/strings.py index 896a56c..ddb7b3f 100644 --- a/src/util/strings.py +++ b/src/util/strings.py @@ -30,6 +30,6 @@ function strings.py: - !Ifflush(length(s",
        "directory": "D07",
        "filename": "D07-1029.json"
    },
    {
        "casual_text": "Entity Extraction. In our system, entities come in three flavors: tasks (like \"Question Answering\"), datasets (like \"SQuAD2.0\"), and metrics (like \"F1\"). To handle this, we use two methods: a dictionary-based approach and a learning-based one. Here's how it works.\n\nFirst, we started with manually curated dictionaries from paperswithcode. But since these dictionaries might not cover everything, we built a module to automatically pull out entities. Unlike earlier work that mostly looked at abstracts (Gábor et al., 2018; Luan et al., 2018), we go through the whole paper and grab the three types of entities related to the main findings.\n\nWe framed this as a textual entailment task: we treat the paper content as text and the Task-Dataset-Metric (TDM) triples as the hypothesis. This approach makes our model focus on spotting patterns between the text and different triples. We trained our module on a dataset of 332 NLP papers and it did pretty well, scoring a macro-F1 of 56.6 and a micro-F1 of 66.0 on a test set of 162 papers (Hou et al., 2019). Overall, our system tagged 872 tasks, 345 datasets, and 62 metrics from the entire collection of papers.",
        "formal_text": "Entities Extraction. Entities in our system are of three types, task (e. g. , \"Question Answering\"), dataset (e. g. , \"SQuAD2. 0\"), and metric (e. g. , \"F1\"). We utilize both a dictionary-based approach and learning-based approach as follows. First, we adopted the manual curated dictionaries of paperswithcode 5. Since those dictionaries may not cover all evolving topics, we further developed a module that automatically extracts entities. Differently from previous work on information extraction from scientific literature which mainly focused on the abstract section (Gábor et al. , 2018; Luan et al. , 2018), we analyze the entire paper and extract the above three types of entities that are related to the paper's main research findings. We cast this problem as a textual entailment task: we treat paper contents as text and the targeting Task-Dataset-Metric (TDM) triples as hypothesis. The textual entailment approach forces our model to focus on learning the similarity patterns between text and various triples. We trained our module on a dataset consisting of 332 papers in the NLP domain, and it achieves a macro-F1 score of 56. 6 and a micro-F1 score of 66. 0 for predicting TDM triples on a testing dataset containing 162 papers (Hou et al. , 2019). In total, our system indexed 872 tasks, 345 datasets, and 62 metrics from the entire corpus.",
        "GPT2_formal_text": "= {}, user_ids = {}, area_ids = {}, and group_ids = {}, where each i is a number from 1 to n. Formal: Basically, the second part of equation (4) is the chance that a mention s_i and its parent i_n are both tagged as an entity mention. Formal: The probability of label y_i given the text x_i is calculated using the formula shown in Equation (5). Formal: The chance of label y_i given the context x_i is calculated using the formula in Equation (6). Formal: The probability of label y_i given context x_i and the start node is calculated using the formula in Equation (7). Formal: The chance of label y_i given all context x_i, the start node, and the target node is calculated using the formula in Equation (8). Formal: The probability of label y_i given the target node, the start node, and the context x_i is calculated using the formula in Equation (9). Formal: The probability of label y_i given the start node and the context x_i is calculated using the formula in Equation (10). Formal: The probability of label y_i given the target node and the context x_i is calculated using the formula in Equation (11). Formal: The probability of label y_i given all context x_i, the start node, and the target node is calculated using the formula in Equation (12). Formal: The probability of label y_i given the start node and all context x_i, the start node, and the target node is calculated using the formula in Equation (13). Formal: The probability of label y_i given all context x_i, the start node, the target node, and the start node is calculated using the formula in Equation (14). Formal: The probability of label y_i given the start node and the target node and the context x_i is calculated using the formula in Equation (15). Formal: The probability of label y_i given the start node and the context x_i and the start node is calculated using the formula in Equation (16). Formal: The probability of label y_i given the start node, the context x_i, the start node, and the target node and the context x_i is calculated using",
        "directory": "D19",
        "filename": "D19-3036.json"
    },
    {
        "casual_text": "On the other hand, there were a lot of instances where people talked about membership stuff. It was pretty clear that folks weren’t sure what the robot needed to do the spatial task, and that confusion showed up in the different ways they explained it. Take a look at Figure 2 for a straightforward example.",
        "formal_text": "In contrast, there were many cases of membership analysis. There was clearly great uncertainty about the robot's prerequisites for carrying out the spatial task and this was explicitly specified in the users' varied formulations. A simple example is given in Figure 2.",
        "GPT2_formal_text": "= p_a * p_m + b_a * p_b + a_a * p_b + sigmoid_sum. Formal: First, check if the label type for the word embedding matches the correct type for the document embedding. If it does, use a trigram model to combine the embeddings of the document embedding and the word embedding. Formal: Next, check if the vector representation of the document entity type matches the correct type for the word embedding. If it does, use a linear model to combine the vectors of the document entity type and the word embedding. Formal: Finally, figure out the dot product between the document embedding and the word embedding. Formal: The model can be trained using the words in the document, the word embedding, or both. Formal: We've got two results for these models: (1) the word embedding is the one that gives the best log probability, so we use it in the logistic regression (LR) model. Formal: The logistic regression (LR) model is also called the logistic regression with conditional random fields (CRF). Formal: The CRF model is considered the best choice if the log probability of the word embedding is more than half the log probability of the document embedding. Formal: The probability of the document embedding happening given the word embedding is called the log probability of the word embedding happening given the document embedding. Formal: Using the logistic regression model, we can predict the word embedding, document entity type, and word type. Formal: For the entity type prediction, we use two classifiers: one predicts if the entity type is the correct type, and the other predicts if the entity type is an abbreviation or not. Formal: Finally, we use a CRF model to predict the entity type of the given word embedding. Formal: Formal: The CRF model can be trained with the words in the document, the word embedding, or both. Formal: Formal: For the entity type prediction, we use two classifiers: one predicts if the entity type is the correct type, and the other predicts if the entity type is an abbreviation or not. Formal: Formal: For the word embedding prediction, we use two classifiers: one predicts if the word embedding is the correct type, and the other predicts if the word embedding",
        "directory": "E06",
        "filename": "E06-1024.json"
    },
    {
        "casual_text": "根据上面的对齐结果，我们可以把同一位置上相同的词合并起来，然后通过不同译文之间的对齐信息来建立一个混淆网络。这个网络的结构就像图4展示的那样。在这个混淆网络里，每两个节点之间的弧线上的词表示它们是最终融合结果中相应位置的候选词。对于混淆网络节点i和i+1之间的弧线上的候选词，第j个候选词的置信得分可以用公式(1)来计算。这个公式考虑了多个模型和译文的情况，其中τ_u是系统u的先验概率，τ_v是词所在译文的权重，通常使用均匀权重，但有时为了给排名靠前的译文中的词更高的权重，也可以使用基于排名的权重，即每个词的概率乘以1/(1+v)。C_ω是第u个模型第v个译文中的词，如果在混淆网络节点i和i+1之间的弧线上出现候选词，则该值取1，否则取0。u是归一化因子，确保节点i和i+1之间所有候选词的总置信度为1。\n\n混淆网络采用了一种投票策略。在合并同一个位置上的词时，需要计算每一个在该位置上的词的后验概率。这个值通常受两个因素影响：一个是融合译文的先验概率，另一个是词所在译文的权重。假设我们有一个源语言的句子S，混淆网络解码就是找到满足下面式(2)中的目标语言句子。其中α、β、γ、δ分别对应融合过程中产生翻译假设的词的置信度P_AL、插入惩罚N_nulls(E)、语言模型得分P_LM、长度惩罚N_words(E)的权重。\n\n最后，我们来看看实验结果和分析。",
        "formal_text": "根据上述对齐结果，对同一位置的相同的词进行合并，通过不同译文间的词对齐信息建立 混淆网络，其结构如图輴所示。 图 图 图 4. 根据词对齐生成的一个单混淆网络 在构建好的混淆网络中，每两个节点之间的弧线上的词表示它们是最后融合结果中在相应 位置的候选词。对于混淆网络节点i和i 輫 輱弧线上的候选词中，第轪个候选词的置信得分，如公 式(輸)所示。 ω i, j 輽 µ N µ=1 N v=1 τ µ τ v C ω 輨輸輩 上式给出了在有轎个模型，每个模型提供轎个译文参与融合时，词的置信度计算公式。其 中，τ u 是系统u对应的先验概率，τ v 是词所在译文的权重，一般采用均匀权重，但是有时为了给 排名靠前的译文中的词赋以更高的权重，也可以采用基于排名的权重，即出自第v个译文中的每 一个词的概率都要乘上 1 1+v ，C ω 是第u个模型第v个译文中的词，如果在混淆网络节点i和i 輫 輱之 间的弧线上出现候选词，则该值取輱，否则取輰。u为归一化因子，它保证在节点i和i 輫 輱之间出 现的所有候选词的总置信度为輱 輨李茂西轥轴 轡转輮輬 輲輰輱輰輩。 4. 3. 2 混 混 混淆 淆 淆网 网 网络 络 络解 解 解码 码 码 混淆网络采用的是一种投票策略。在合并同一个位置上的词时，需要计算每一个在该位置 上的词的后验概率。一般地，在基于混淆网络的融合模型中该值受两个因素的影响，一个是融 合译文的先验概率，另一个是词所在译文的权重 輨朱俊国轥轴 轡转輮輬 輲輰輱輹輩。假设给定一个源语言的 句子轆，混淆网络解码就是求满足下面式(輹)中的目标语言句子。 E * 輽 argmax E 輨α 转软轧 P AL 輫 βN nulls 輨E輩 輫 γ 转软轧 P LM 輫 δN words 輨E輩輩 輨輹輩 其 中α、β、γ、δ分 别 对 应 融 合 过 程 中 产 生 翻 译 假 设 的 词 的 置 信 度P AL 、 插 入 惩 罚N nulls 輨E輩、语言模型得分P LM 、长度惩罚N words 輨E輩的权重。 5 实 实 实验 验 验结 结 结果 果 果与 与 与分 分 分析 析 析 5.",
        "GPT2_formal_text": ") are basically the word embeddings for each input token, but they’re swapped out with the target embedding. We train the model by doing the following: Formal: This part has three parts: (1) a one-stage fine-tuning process that considers sentence structure and language stuff; (2) a language modeling step that gives us a rough idea of how likely it is to generate a sentence based on the input; and (3) a ranking of the different languages using the languages in our dataset. Formal: We use a window size of 2 to limit the number of predictions to 3, which is pretty standard in recent studies (like Zhou et al., 2019; Sun et al., 2020). We pick the best language for each prediction based on the model’s performance. Formal: We train the model using the data set D = {D1, ..., Dm}. Formal: The loss function is L(D), which basically tells us how uncertain the model is about its predictions. Formal: To balance accuracy and diversity, we use a linear mapping θ, where θ ∈ R Dp, where p is the precision, and d is the diversity, following what’s explained in Section 3. Formal: The final result is a weighted average of the model’s predictions and the error term we calculate using Equation 1. Formal: We also use the same loss function L(D) for our experiments, and we use the same alignment method θ to encode the input tokens. Formal: To keep things consistent, we use the same pre-trained model L_p to feed the model with the data set D. Formal: In this setup, the unigram model can handle just one language, but the bigram model can handle multiple languages. Formal: For testing, we use the same dataset D as in the original paper. Formal: To measure how well the model performs, we use the L1-norm of the F1 score. Formal: We also calculate the random walk loss L_random, which is the same as L_p. Formal: Finally, we use the same loss L_random for our experiments. Formal: In the table, the language model is the one with the best performance (on the right side), and the validation loss L_valid is the one with the worst (on the left side). Formal: We’re skipping the three-",
        "directory": "ccl",
        "filename": "2021.ccl-1.5.json"
    },
    {
        "casual_text": "Geolocation prediction can basically be approached as either a regression or a classification problem. But in reality, since predicting exact coordinates is pretty tricky, people usually mix regression with classification (Eisenstein et al., 2010; Lourentzou et al., 2017; Fornaciari and Hovy, 2019b). Most of the time, though, it's treated as a classification problem, where you need to figure out which geographic area something belongs to and label it accordingly.",
        "formal_text": "Geolocation prediction can, in principle, be modeled both as regression and as classification problem. In practice, however, given the difficulty of predicting continuous coordinate values, regression is often carried out in conjunction with the classification (Eisenstein et al. , 2010; Lourentzou et al. , 2017; Fornaciari and Hovy, 2019b). In general, however, the task is considered a classification problem, which requires solutions for the identification of geographic regions as labels.",
        "GPT2_formal_text": "_len() and attention_mask_len_epoch() that help capture the attention heads at different layers.\n\nIn this paper, we take the model we talked about in section 5 and tweak it a bit by adding the attention mechanism. We test it on the TextRank dataset, which has questions with multi-word answers, both in terms of words and grammar. The goal is to see how well it can figure out the overall context for each question. Formal: In the pre-training part, we use a multi-layer perceptron (MLP) to create a representation of the input sentence, which is the input sequence x i. At each step, the model gets a new embedding z i−1 (that's the loss) and learns to predict the next token y i−1 (the label) based on the input. So, the encoder-decoder model gets trained and refined using this process. Formal: Just a heads-up, the loss at each step is different for each class. Formal: In each epoch, the model gets an input sentence x i along with the current hidden state z i−1. To figure out the next token, the model uses the previous hidden state z i−1, but it doesn't use the current hidden state z i+1. Formal: As you can see in Figure 1, the log loss (which is the attention loss) is calculated based on the input sequence x i. Formal: We use a multi-layer perceptron (MLP) to create a representation of the input sentence, and then we use a label projection model (like the one by Liu et al., 2016) to predict the next token y i. We calculate the log loss based on the hidden state of the current token and also use the output from the previous hidden state. Formal: Finally, the label projection model is trained and refined using this process. Formal: After each epoch, the model gets an input sentence x i with the current hidden state z i−1. The next hidden state is calculated using the previous hidden state from the epoch. Formal: Formal: We add the latent variable z i−1, the label projection model (which is the loss), and the loss from the previous epoch into the decoder. We also use the output from the previous hidden state to predict the next token. Formal: Formal: If the current hidden state is negative, the decoder just goes with the current token.",
        "directory": "D19",
        "filename": "D19-5530.json"
    },
    {
        "casual_text": "The first one is the basic dot product self-attention model. G ∈ R I×I represents a favor alignment. Check out Figure 1 for a visual of the approach we're proposing. In this case, we're using a window size of 2 (D = 2).",
        "formal_text": "The first term is the original dot product selfattention model. G ∈ R I×I is a favor alignment Figure 1: Illustration of the proposed approach. In this example, window size of 2 is used (D = 2).",
        "GPT2_formal_text": "= {src_id, obj_id} from the document. Formal: To compare things with the word embedding models, we picked two types of dependency relations. One type is based on the words that start with a capital letter, and the other is based on words that end with a period. Formal: We used the Stanford dependency parser (de Marneffe et al., 2006) to do dependency parsing. The results are in Table 2. It shows that the word embedding models (Base + LCRF) and the dependency relation models (Base + CRF + LCRF) did pretty well, with BERT doing the best. But when we compared them to the word embedding models (Base + CNN + LCRF), BERT didn't perform as well, likely because it relies on context. On the other hand, CRF + LCRF did better, even though it was trained on a bigger dataset, likely because it was fine-tuned on a more general dataset. Formal: Basically, the models we talked about earlier (Base + LCRF + CRF + LCRF) are designed to pick up on certain patterns in text. The dependency relation models are trained to learn these patterns, which helps them predict the right relation. Formal: The word embedding models (Base + CNN + LCRF) also use context, but it's different because it works with multiple words in the sentence instead of just one. Formal: CRF + LCRF was trained on a dataset with around 2.5 million sentences, while BERT was trained on a dataset with around 1.6 million sentences. Formal: BERT is pretty good at handling a wide range of tasks, but it's still not as good as CRF. Formal: When we combined both baselines, the BERT+CRF model (which uses CRF + LCRF) performed the best, with a precision of 97.6%. Formal: BERT and CRF+LCRF had pretty similar precision, but CRF +LCRF was able to produce better results, with a precision of 98.0%. Formal: CRF + LCRF was trained on a dataset with about 3.9 million sentences, while BERT was trained on a dataset with about 3 million sentences. Formal: The CRF + LCRF model was trained on a dataset with about 1.6 million sentences, while BERT was trained on a dataset",
        "directory": "D18",
        "filename": "D18-1475.json"
    },
    {
        "casual_text": "Okay, so let’s say ( ) is a word, and _ is either the word itself or a phrase that includes the word. Here, ( ) stands for a specific term, and _ represents all the related terms. Now, here’s how the new GPU sampling works:",
        "formal_text": "Let ( ) be a word and _ be the word itself or a phrase containing the word ( ). represents a term, and _ indicates all the related terms of. The new GPU sampling is as follows:",
        "GPT2_formal_text": "and attention_max. Formal: In this paper, we're looking at this equation: y = {h t t } = {h t } (T t = T, T t = {h t }, ..., h) for each attention head h t in the previous step. Basically, this equation represents the joint probability distribution for each attention head y t. Formal: Also, the attention head's output y t is calculated as h_t = {h t } t = {h t } T t = {h t } T t = {h t } T t = {h t } h t. Formal: Basically, we're just using the attention head's output to update the feature representation h t. Formal: We need a similarity measure between the source and target domains to make sure that the predictions for the source domain are matched with the target domain's predictions. Formal: The target domain's attention head h t is updated using a linear projection called the target domain attention head h t, which is defined as h_t = {h t } t = {h t } T t = {h t } T t = {h t } h t. Formal: This process is repeated for each target domain attention head h t in the source domain. Formal: The input features of the source domain and target domain are projected using these two attention heads and then combined to get the final output. Formal: So, it's like a binary classification problem where the output is either the source domain's or the target domain's label, depending on the attention head. Formal: This approach can be used for other image classification tasks too. Formal: There are a bunch of methods for generating embeddings for different types of images, like image classification, question answering, and text classification. Formal: Formal: We also tried using equation (1) for binary classification tasks, but we found that the error rate was too high. Formal: We found that the error rate can vary depending on the type of classification task and the embedding method. Formal: Formal: We've shown that the best results come when the output is the target domain's label, assuming the source domain and target domain are semantically close. Formal: Formal: Formal: The attention weights are adjusted based on the distribution of the input features. Formal: Formal: Formal: Formal: We tried different ways of averaging the input features, ranging from the regular",
        "directory": "C14",
        "filename": "C14-1063.json"
    },
    {
        "casual_text": "For each part of a word’s meaning, we group together all the characters that include that part. As you can see in Table 1, not all characters in these groups have meanings that are closely related enough to be useful examples in teaching materials. So, for any group, the goal is to pick out a smaller set of characters that have meanings that are really similar. When creating an algorithm for this, we focus on two main areas of research:",
        "formal_text": "For each semantic component, we define its \"character family\" to consist of all characters that contain the component. As shown in Table 1, not all family members have sufficiently related meaning to serve as good examples in pedagogical materials. Given a family, the character set construction task is to identify a subset of its characters that are semantically close. In designing an algorithm for this task, we address two research topics:",
        "GPT2_formal_text": "= { id: 1, label: 2, span: 3, pattern: 4, } }\n\nBasically, we calculate the full token representation by taking the input sequence and turning it into a fixed-size array. This is done using an LSTM (a type of neural network) and a linear layer with ReLU activation. After that, we normalize it and add a 1 to the result. This gives us the full token representation as the final result. Formal: Basically, we take the input sequence x, run it through a BiLSTM (also called a neural network), and get a vector representation v for the full sentence x. Formal: We use a linear layer with ReLU activation and a fixed-size array. Formal: For a given span, the representation v is either the identity matrix (if there's no relation) or the normalized version of the input sequence x. Formal: The normalized version of the input sequence x. Formal: Lastly, we normalize the output vector v using the normalized version of the input sequence x. Formal: Formal: To create the full token representation for a sentence x, we simply use a linear layer with ReLU activation. Formal: We only calculate the number of tokens in the sentence x, ignoring the span and context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "bea",
        "filename": "2021.bea-1.6.json"
    },
    {
        "casual_text": "* Portability: SABLE started with French and English, then they added Spanish and English, and later Korean and English too. They've got the whole process down and even wrote it all up (Melamed, 1996c).",
        "formal_text": "* Portability: SABLE was initially implemented for French/English, then ported to Spanish/English and to Korean/English. The porting process has been standardized and documented (Melamed, 1996c).",
        "GPT2_formal_text": "ed_tokens ( ) is the same as the attention_masked_tokens in equation (6) plus the words that were already masked before this step. Formal: We figure out the attention weights for each token using Equation 1. Formal: The attention weight, α i, is calculated by multiplying the regularization term of the attention with a term that's proportional to the average of the tokens it’s already received. Formal: We use a standard attention mechanism called Att(θ_i) and find the best weight, W_i, that gives us the biggest value. Formal: We calculate the attention weights for the tokens in the input sequence by dividing the attention weights for the i-th token, j, by the attention weights for the i-th token, j. Formal: We can do a straightforward heuristic search to find the best attention weights. Formal: We calculate the attention weights for the tokens in the input sequence using Equation 1, which gives us the attention weights for the i-th token, j. Formal: We can also use a heuristic search to find the best attention weights. Formal: In this case, we calculate the attention weights for the tokens in the input sequence by dividing the attention weights for the i-th token, j, by the attention weights for the i-th token, j. Formal: For more details on how we calculate the attention weights, you can refer to the original paper. Formal: We can also use a heuristic search to find the best attention weights. Formal: We can also use a heuristic search to find the best attention weights. Formal: The maximum of the attention weights for the i-th token, j, and the i-th token, j. Formal: We can also use a heuristic search to find the best attention weights. Formal: We can also use a heuristic search to find the best attention weights. Formal: In the test set, we use the IOBES algorithm to create a test set. Formal: We can also use a heuristic search to find the best attention weights. Formal: We can also use a heuristic search to find the best attention weights. Formal: We can also use a heuristic search to find the best attention weights. Formal: We can also use a heuristic search to find the best attention weights. Formal: We can also use a heuristic search to",
        "directory": "A97",
        "filename": "A97-1050.json"
    },
    {
        "casual_text": "8. If you type \"J'espère les avocats\" into the test interface, it translates to \"I hope The lawyers.\" Why is \"The\" capitalized?",
        "formal_text": "8. If you write the phrase J'espère les avocats in the test interface, you will get the translation I hope The lawyers. Why is The capitalized?",
        "GPT2_formal_text": "= list(template)\n\nThis is similar to how we handle domain adaptation, but it’s different because the context that comes from a specific template, which we call the query, doesn’t have to match the actual document. Let’s say the query is \"who won the Champions League?\", and the context is \"the winners of the Champions League were.\" \n\nNow, about disjunction: You can swap out the indices 0, 1, and 2 in the query and document for [INPUT_1, INPUT_2] and [INPUT_3, INPUT_4]. This means the hidden state for the [INPUT_1, INPUT_2] and [INPUT_3, INPUT_4] spots is basically the same as the hidden state for the [INPUT_1, INPUT_2] and [INPUT_3, INPUT_4] spots. Formal: \n\nIf you replace the indexed index 0, 1, and 2 in the query and document, the output for the [INPUT_1, INPUT_2] and [INPUT_3, INPUT_4] spots becomes the same as the output for the [INPUT_1, INPUT_2] and [INPUT_3, INPUT_4] spots. Formal: \n\nIn this case, the [INPUT_1, INPUT_2] and [INPUT_3, INPUT_4] spots are similar to the [INPUT_1, INPUT_2] and [INPUT_3, INPUT_4] spots, so the output for those spots is the same as the output for the [INPUT_1, INPUT_2] and [INPUT_3, INPUT_4] spots. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2020.eamt-1.40.json"
    },
    {
        "casual_text": "We're breaking down the whole challenge into three main parts: 1) finding the right documents, 2) figuring out the important bits in those documents that relate to what we're working on and should be summarized, and 3) creating a final summary that focuses on the specific topic we're dealing with. It's obvious that current NLP tech isn't advanced enough to solve this whole thing in one go, but we think working on each part separately will help us get closer to a complete solution eventually.",
        "formal_text": "We dissect the full challenge as bringing together work of disparate interests; 1) in finding relevant documents; 2) in identifying the salient aspects of these documents in relation to the current work worth summarizing; and 3) in generating the final topic-biased summary. While it is clear that current NLP technology does not let us build a complete solution for this task, we believe that tackling the individual components will help bring us towards an eventual solution.",
        "GPT2_formal_text": ", and R_a, as explained in Section 3.2, are just two types of attention mechanisms. In this project, we’ve come up with a way to handle the problem of natural language understanding (NLU) by using a mix of attention and a bidirectional LSTM. It’s inspired by the attention-based approach used in a lot of dialogue generation tasks (like the ones by Liu et al., 2016a; Liu et al., 2016b; Chen and Lapata, 2017; Ren et al., 2017b; Ren et al., 2017a), as well as the attention-based approach for knowledge base inference (Huber and Carenini, 2016; Carenini et al., 2017). \n\nHere’s what we’ve done differently:\n1. We’ve come up with a new way to handle the context of an entity mention in text. This is based on the idea of a context gate.\n2. We’ve also introduced a new bidirectional attention mechanism, which helps us pay attention to both the context and the entity mention.\n3. We’ve created a new approach to tackle the issue of being too focused on a specific aspect of a mention. This approach works really well for NLP tasks that require understanding the context and the relationship between entities.\n\nIn our experiments, we’ve done the following: Formal: To understand the context of a mention, we create a vector called V_c for the mention. Then, we figure out the attention vector a_c for each entity mention m_i and label it as A_c_i. This vector is then passed through a bidirectional LSTM to create the embedding for the mention. Formal: For the entity mention m_i, the attention vector a_i is created by combining the embedding from the context vector a_c_i, the entity mention vector m_i, and the embedding for the mention m_i. This combination is then passed through the bidirectional LSTM to create the embedding for the mention. Formal: Finally, we turn the vector into a list of numbers, using the attention mechanism to grab the most important details for the mention. Formal: We make sure the context vector is unique by using a linear transformation on it. Formal: The embeddings for the entities mentioned in a mention m_i are normalized to represent their context",
        "directory": "C10",
        "filename": "C10-2049.json"
    },
    {
        "casual_text": "To make our questions more believable, we create three wrong answers for each simile. These wrong answers follow two rules (based on Haladyna et al., 2002, and Ren and Zhu, 2020): they should be true-negative and challenging. True-negative means the wrong answers don’t make sense when used in the question, but they’re still related to the correct answer in a tricky way. Our process for making these wrong answers has three steps: 1) coming up with options, 2) picking the best ones, and 3) double-checking with people to make sure they’re good.",
        "formal_text": "To make our probes convincing, three distractors are designed against the original property in each simile with two criteria (Haladyna et al. , 2002; Ren and Zhu, 2020): true-negative and challenging. We argue that well-designed distractors should be illogical when filled into the questions (true-negative) while being semantically related to the correct answer (challenging). Our distractor design mainly involves three phases: 1) distractor generation; 2) distractor selection; 3) Human Confirmation.",
        "GPT2_formal_text": ".\n\nFor the actual mapping, we use the subject word's syntax and the subject's form to generate the context. To turn the context into a sequence of tokens, we use a BERT-based encoder-decoder setup with attention, as explained in section 4.2. This setup has an encoder-decoder setup with a hidden layer and a transformer-based decoder. The input for the BERT-based encoder is a sentence, which is broken down into tokens based on the Penn Treebank (PTB), which was created by Marcus et al. in 1993.\n\nFor the BERT decoder, we use the same model as described by Ruder et al. in 2019. It has a transformer-based encoder-decoder setup with a hidden layer and a self-attention mechanism. The input for the BERT decoder is a sequence of words, broken down into tokens using the same features as the Penn Treebank.\n\nTo get the hidden representation for the sentences, we use the word embedding layer in the BERT-decoder. This layer can create a context vector for each word in the input, along with its specific language model and position embedding. These vectors then act as input for the BERT-based encoder. Formal: Formal: Since the BERT decoder is a seq2seq model, it can figure out the context for any word in the sentence by looking at the embedding vector. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.543.json"
    },
    {
        "casual_text": "Check out Table 4 for the results. Both knowledge distillation and POSPD help lower the repetition rate in NAG models across four datasets. They work especially well on XSUM, which has longer sentences. When it comes to the number of tokens, knowledge distillation really cuts down on the tokens generated by NAG models, especially on XSUM. On the other hand, POSPD keeps the length of the generated sentences pretty close to the reference without making the repetition rate worse. Table 4 breaks it all down: it shows how the NAG models perform. \"Reference\" is the target sentence we're aiming for. \"Repetition\" and \"Tokens\" show the difference in repetition rate and the number of tokens between the reference and what the model actually outputs.",
        "formal_text": "The results are shown in Table 4, from which we can see both knowledge distillation and POSPD can reduce the repetition rate in NAG models on four datasets, and they are more effective on XSUM datasets with longer sentences. While in token numbers, using knowledge distillation significantly reduces the number of tokens generated by NAG models on XSUM. In contrast, using POSPD remarkably make the length of generated sentences by NAG models close to the reference without increasing the repetition rate. Table 4: Statistical analysis of NAG models' generations. \"Reference\" denotes the target sentence's reference. \"Repetition\" and \"Tokens\" represent the repetition rate and tokens number gap between reference and model outputs, respectively.",
        "GPT2_formal_text": ". Aspect-level sentiment classification is about finding and classifying aspects, and aspect-level sentiment classification is about classifying sentiments. In these tasks, the sentiment is expressed using multi-word sentiment vectors from the LSTM. So, the multi-attention mechanism in the LSTM takes the input word and uses it to figure out the sentiment of each aspect. Formal: Alright, so we use the attention mechanism and the latent LSTM to encode the aspect embedding h_t, which we then pass through a softmax layer. This helps us predict the sentiment for each aspect. After that, we use a linear layer to predict the sentiment for each aspect. Formal: In this paper, we first encode the input word using the LSTM, and then we use a linear layer to predict the sentiment for each aspect. Formal: To train the model, we train it on a huge dataset with over a hundred million documents, creating a sentiment classifier S. Formal: Since we don’t have any labeled data to work with, we use a supervised method that follows the approach in (Fan et al., 2019). Formal: To make sure the model doesn’t get too confident in its predictions, we add some random noise to the input hidden state of the LSTM. Formal: The goal is to train the model to minimize the cross-entropy loss. This loss is calculated based on how often the model predicts a negative sentiment or if it predicts a positive sentiment for each aspect. Formal: We train the model using the cross-entropy loss for a set number of epochs, called α. Formal: The final loss is a combination of cross-entropy loss, the cross-focused attention loss, and the residual connection loss. Formal: The cross-focused attention loss is calculated by looking at the negative and positive sentiment scores for each aspect. Formal: We tweak the loss function by using a learning rate of 0.1. Formal: Finally, we use a linear layer to predict the sentiment for each aspect. Formal: We then apply the cross-focused attention loss for a few epochs, α. Formal: After some training, the loss for this specific case is: Formal: Formal: Basically, this is a formula that helps us understand how much we need to tweak the loss function to make the model perform well. Formal: Formal: Formal: Formal: Finally, to get",
        "directory": "acl",
        "filename": "2021.acl-long.467.json"
    },
    {
        "casual_text": "where m is the length of the question and n is the length of the document.",
        "formal_text": ", where m and n represent the sequence length of a question and a document, respectively.",
        "GPT2_formal_text": "_t and the full answer span \\( v_s \\) in the input text. Formal: We take the full text and turn it into a vector representation \\( z_t = \\theta(T) \\). This vector is then used to represent the new answer spans \\( \\theta(T) \\), which we get using a bunch of multi-scale character embeddings. We also have a regularization parameter \\( \\lambda \\) that helps balance how much we care about capturing the entire answer span versus how much it matters for individual spans. Formal: For the sentiment polarity of the input, we use the sentiment polarity vector \\( \\sigm \\) from the original text. Formal: We calculate the cosine similarity between the two vectors \\( z_t \\) using the tfidf score function from the original text. We use this similarity to calculate the sentiment polarity scores for the tokens in the input and output text. Formal: For the aspect polarity, we use the aspect polarity vector \\( \\sigm \\) from the input text. We calculate the aspect polarity scores for the tokens in the input and output text using the tfidf score function from the input text. Formal: The attention mechanism (like the one from Bahdanau et al., 2015) is used to give the right attention weights to both the input and output text at the same time. Formal: To figure out the attention weights, we use a method that uses a bag-of-words approach and a transition-based attention mechanism. Formal: To get the attention weights for the input and output text, we apply a sentence-level attention mechanism that combines the attention weights from the input and output text. Formal: Lastly, the final attention weight \\( Σ(t, s) \\) is calculated using the log loss function from the original text. Formal: To learn the attention weights, we use a sequence-level attention mechanism (like the one by Bahdanau et al., 2015) that also considers the input and output text. Formal: Formal: Our model is trained using a multi-scale pretrained word2vec model with a hidden size of 200. Formal: We calculate the macroaveraged F1 score using the micro-averaged BERT model. Formal: We evaluate the model by testing it on an example dataset from the IWSLT task. Formal: We’ve shared our code for anyone to",
        "directory": "D18",
        "filename": "D18-1237.json"
    },
    {
        "casual_text": "AG News. This dataset is for classifying news articles into four different categories. It's split into a balanced training set with 120,000 samples and a test set with 7,600 samples.",
        "formal_text": "AG News. The dataset for the news articles classification partitioned into four categories. The balanced train and test set consist of 120, 000 and 7, 600 samples, respectively.",
        "GPT2_formal_text": "= 0; y_t = y_s; for i from 1 to n: y_t = y_s + wf_mask; h_t = h_s + bf_mask; s = [s_1, ..., s_n, ..., s_m, ..., s_s_t] ∈ {0, 1} |V| where |V| is the size of the vocabulary. In this setup, we only look at pairs that are close to each other in the space of tokens. This is different from the usual attention-based methods, which only focus on pairs that are close to each other in the data, but we still consider the context of the tokens. Formal: Basically, we're saying that |V| is set to equal |V| times the Euclidean distance, and the vector for each token x_t i in the input sequence is calculated based on the attention. Formal: In this paper, we're assuming that the attention vector is always 1. Formal: We take the attention vector h_t_i and the output token x_t i for the input sequence |S| and map them into a shared space. Then, we calculate the vector for the output token x_t_i. Formal: The operation of mapping h_t_i to x_t_i happens through an attention operation. Formal: We use a weighted average of the attention vectors h_t_i and x_t_i, which gives us the attention vector for the output token. Formal: Finally, we sum all the attention vectors together to get the overall vector for the input sequence. Formal: The final representation of the input sequence is also calculated by combining the outputs of the tokens. Formal: In this paper, we're mainly talking about the attention model used in the method called max-pooling. Formal: Our approach is a bit different from this one because we focus on the attention vector for the output token. Formal: For instance, if the input sequence is \"A B C D E F G H I J K L M N O P Q R S T U V W X Y Z,\" we can figure out the attention vector for the output token by combining the attention vectors for the input sequence (A B C D E F G H I J K L M N O P Q R S T U V W X Y Z). Formal: In contrast, in the",
        "directory": "acl",
        "filename": "2021.acl-long.94.json"
    },
    {
        "casual_text": "For Indo-European and Mayan, the CNNs actually work better than the baseline system. But for Austronesian, the CNNs don't do better than the baseline. The results for Indo-European and Mayan (check out table 5) are pretty much the same as the ones in table 4. Basically, the CharCNN system is the best for the Mayan language family, and the PhoneticCNN system is the best for the Indo-European family. \n\nInterestingly, for the Austronesian family, the baseline system actually does better in terms of F-score compared to the top-performing system from table 4, which is the CharCNN with language features. The baseline system also has a higher Accuracy score, but the difference isn't big enough to be considered statistically significant. \n\nOne possible reason for this is that there just isn't enough info in those 20 meanings to figure out phonological similarities across 100 languages.",
        "formal_text": "In the case of Indo-European and Mayan, the CNNs perform better than the baseline whereas for Austronesian the CNNs do not outperform the baseline system. The results for Indo-European and Mayan (cf. table 5) are similar to that of the results reported in table 4. That is, the CharCNN system performs the best for Mayan language family, while the PhoneticCNN system performs the best for the Indo-European language family. Surprisingly, for the Austronesian family, the baseline system performs better at F-score than the top-performing system for this language family in table 4, namely the CharCNN (with language features); the Accuracy measure of the Baseline system is also higher, but the difference is not statistically significant. The reason for this could be that there is not enough information in the 20 meanings to learn phonological similarity for 100 languages.",
        "GPT2_formal_text": ", obj_id, info_id = {}, keywords = {}, meta = {}, summarization = {}, compression = {}, citation_ids = {}, summary_ids = {}, citations = {}, abstracts = {}, title = {}, authors = {}, keywords = {}, references = {}, bibliography = {}, citation_links = {}, bibliography_links = {}, bibliography_links = {}, citations = {}, appendix = {}, appendix = {}, section = {}, page = {}, section = {}, DOI = {}, DOI = {}, DOI = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http = {}, http =",
        "directory": "C16",
        "filename": "C16-1097.json"
    },
    {
        "casual_text": "Let’s say you have a text with T sentences and a vocabulary of L unique words (like words that aren’t in a stop list, otherwise most sentences would look pretty similar). You can represent this text using a T x L matrix called F. Here’s how it works: for each sentence (t = 1, 2, ..., T) and each word (1 = 1, 2, ..., L), you put a 1 in the matrix if the 1-th word is in the t-th sentence, and a 0 if it’s not.",
        "formal_text": "Suppose that a text contains T sentences and its vocabulary contains L distinct words (e. g words that are not included in the stop list, other wise most sentences would be similar to most others). This text can be represented by aTxL matrix F defined as follows: for t = 1, 2. . . . , T and 1 = 1, 2, . . . , L we set { 1 iff 1-th word is in t-th sentence F1 = 0 else.",
        "GPT2_formal_text": "ed = attention_masked [, d], where d is the number of layers in the CNN. Formal: Basically, the word embeddings we get from the CNN are made up of three parts: the hidden state, the word embedding, and the word embedding layer. The hidden state is used for encoding words, while the word embedding layer gives us the representations for words. Formal: The sentence token embedding is a matrix with dimensions d×d, and the word embedding layer has dimensions d. Formal: The final output is a matrix called M ∈ R d×d, where d is the number of output units, and M is the matrix that represents the sentence. Formal: The sentence embedding matrix M ∈ R d×d is the one we get from the CNN. Formal: The input for the sentence is represented by a matrix that has dimensions d×d. Formal: Finally, the output of the CNN is a matrix called M_i ∈ R d×d. Formal: If we have a sentence x, we can get a vector representation for the word embedding d by doing a max-pooling process with a filter f ∈ R d×d. Formal: In the end, the sentence's vector representation M ∈ R d×d is a matrix where d is the number of output units, and M is the matrix that represents the sentence. Formal: Formal: This works because the CNN is designed to learn embeddings for words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E03",
        "filename": "E03-1058.json"
    },
    {
        "casual_text": "It’s clear that both aligners did a pretty good job, but CATS-Align worked better with hand-aligned data. When we looked into the mistakes Bleualign made, we noticed that most of the errors happened because it grabbed a few extra sentences from the next paragraph. This only seemed to happen with adapted paragraphs. One reason for this could be that the original and adapted texts don’t match up perfectly in size. But when it comes to aligning longer books or novels, this might actually be helpful. Sometimes, longer paragraphs in the original text are split into smaller ones in the adaptation, so grabbing a few extra adapted paragraphs could help match the longer original ones better.",
        "formal_text": "As can be seen, although both aligners showed good results, CATS-Align performed better on hand-aligned data. When investigating the errors of Bleualign, we found out that most of the erroneous alignments happen because the aligner takes some additional sentences from the next paragraphs. This seems to only happen with adapted paragraphs. One possible reason is the size mismatch between original and adapted texts. However, when aligning longer books and novels, this might be appropriate, since sometimes longer paragraphs from the original texts are split into multiple paragraphs during adaptation, so it can be good to grab additional adapted paragraphs during alignment to match a longer original one.",
        "GPT2_formal_text": "= { ID: { id1, id2, ..., idK }, where K is the total number of tokens. Formal: This thing is all about finding the most likely entity pair from a group of possible entity pairs. The probability distribution is calculated using the estimated Dirichlet distribution. Formal: In this part, we'll talk about how we handle the \"intrinsic\" feature. The idea is to focus on the most important features and get rid of the less important ones. We'll also explain how we define \"intrinsic\" and what that means. Formal: The \"intrinsic\" feature is all about the entity pairs that have the highest BLEURT scores. We'll use a small set of entity pairs that we already know have the highest BLEURT scores as our context. Formal: For each pair of entities, we calculate the \"intrinsic\" feature for the one with the highest score, and we only consider the less important ones. Formal: For example, if we know the entity pair is (m, c), and the one with the lowest score is (k, c), we calculate the \"intrinsic\" feature like this: Formal: For the \"intrinsic\" feature, we use a group of 2,000 entity pairs from the E2E training set to calculate the \"intrinsic\" feature for each pair of entities. Formal: Following what other researchers have done (like Gao et al., 2020), we use the average BLEURT score (which is the average of BLEURT scores for all pairs of entities) to find the \"intrinsic\" feature for each pair of entities. Formal: We're using the results of our experiments for these experiments. Formal: We can break down the \"intrinsic\" feature into three parts: Formal: The first part is about how many times the pair of entities appears in the text. Formal: The second part is about how often they appear in the reference translation. Formal: The third part is about the number of times each entity appears in the reference translation. Formal: We use the tfidf tokenizer with the development set of the test data to break down the \"intrinsic\" feature for each pair of entities. Formal: For each pair of entities, we calculate the \"intrinsic\" feature for the one with the highest score,",
        "directory": "bsnlp",
        "filename": "2021.bsnlp-1.8.json"
    },
    {
        "casual_text": "[Answer] five [Knowledge] You ride the subway to and from work five days a week.",
        "formal_text": "[Answer] five [Knowledge] You take the subway back and forth to work five days a week.",
        "GPT2_formal_text": "ed_word_vec, which basically means that the matrix * is filled with attention values like A = A_t, where * is the size of the attention vector. Formal: In Section 4, we explain how we calculate the attention mask and attention weights, and we also give an example to show you how this works. Formal: Using the attention vector, the model figures out the attention weight vector *A_t, which looks like A = A_t_1, ..., A_t_n. Formal: We start by creating a list of attention vectors *A_t, which we call A = A_t_1, ..., A_t_m. We then figure out the attention weight vector *A_t, which is *A_t_1, ..., *A_t_m_i. Formal: This attention weight vector *A_t is used in the decoder to make predictions. Formal: In each round of training, the model gets a new attention vector *A_t for each new question *q*. The decoder then picks the attention vector *A_t* that works best for that question, making predictions based on *q*. Formal: So, in each round, we check how good *q* is in terms of answering the question *q*, and pick the attention vector *A_t* that’s most like *q* in that respect. Formal: Once we have the attention vector *A_t*, we check how well it fits *q* by checking the log likelihood of *A_t* fitting *q* and using the log likelihood function from Equation (1). Formal: To keep the attention vector *A_t* fixed for each question *q*, we add the attention weights *A_t* to the training set, and let the decoder handle that. Formal: The size of the training set is calculated based on the average attention weight *A_t*, but only when the question *q* is valid. Formal: Finally, we calculate the average attention weight *A_t* for each question *q* by using equation (1). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.225.json"
    },
    {
        "casual_text": "Another key rule to stop things from going on forever is the attenuation factor. This is like a discount that lowers the score for each level as you move away from the starting point where the original pair entered the matching process. The farther you get from that starting level (called level 0), the lower the highest possible score becomes. Eventually, the top score drops below a certain limit, and that's when SWESIL says, \"Okay, enough matching for now.\"",
        "formal_text": "A third boundary condition, which has an important function in preventing endless loops, is the attenuation factor. This is a factor by which the score for a certain level gets reduced for each step this level is away from the 'entry level' at which the original IST pair entered the matching cycle. The further removed from the entry level (level 0), the lower the maximum possible score will be; and at some point the maximally obtainable score will fall below a certain threshold level, at which point SWESIL stops matching.",
        "GPT2_formal_text": ". This helps in finding groups of entities within a given area. In this case, we're using a dataset with 100K entities, which gives us 100K training examples, 10,000 for validation, and 1,000 for testing. \n\nFor each of these 100K training examples, we randomly pick 10,000 random instances to train our model. After that, we compare these 10,000 instances to the original 10K training examples and pick the one that matches best. This is all based on what we learned from the training data.\n\nFor the validation set, we use the original 10K training examples from the validation set. To make sure we're getting the most accurate predictions, we train our model using the test set that's closest to the training examples. \n\nThis approach is pretty similar to how the popular seq2seq model works. It uses seq2seq's forward and backward seq2seq models to figure out the right label for a given sequence of input tokens. Formal: We train the model using the test set that's closest to the training examples. Formal: We use the trained model to predict the correct label for a test example. Formal: We train the model using the test set that's closest to the training examples. Formal: We also use the trained model to predict the correct label for a test example. Formal: We use the trained model to predict the correct label for a test example. Formal: Finally, we use the trained model to predict the correct label for a test example. Formal: We use the trained model to predict the correct label for a test example. Formal: We also use the trained model to predict the correct label for a test example. Formal: Finally, we use the trained model to predict the correct label for a test example. Formal: Finally, we use the trained model to predict the correct label for a test example. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1102.json"
    },
    {
        "casual_text": "Check out https://www.xbrl.org/the-standard/what/an-introduction-to-xbrl/ for a quick intro to xbrl.",
        "formal_text": "See https: //www. xbrl. org/the-standard/what/ an-introduction-to-xbrl/ for an introduction to xbrl.",
        "GPT2_formal_text": "layer is a fixed-length matrix that combines the attention and the output word embeddings. The LSTM hidden states from the last layer are then passed into the output layer to figure out the relevance score for the next token. Basically, the goal here is to make sure the last hidden state in the output layer matches the last hidden state in the input layer. Formal: LSTM has a hidden state dimension of n1×n2, while CNN has a hidden state dimension of n1×n2. Formal: For comparison, we also included a simple linear model that uses a single-layer, non-linear feed-forward neural network. Formal: Even though our LSTM-based model is similar to the linear model, it doesn’t have the linear order. So, the combined feature representation for the last hidden state in the output layer only includes the first hidden state. Formal: We’ve put all the implementations of our model and the baseline model in the appendix. Formal: We tested our model on three datasets: CoNLL03, CoNLL04, and MultiNLI (thanks to Bengio et al., 2008). The results are in Table 3. Formal: In the rest of the paper, we’ll talk about the experiments we ran and what we found out. Formal: We’ve included the results for each dataset in the appendix. Formal: For each dataset, we randomly picked 100,000 training samples. For the final 5% of the samples, we randomly picked the remaining 10,000 samples from that same dataset. Formal: We’ve included the results for each dataset in the appendix. Formal: In the rest of the paper, we’ll talk about the experiments we ran and what we found out. Formal: We’ve included the results for each dataset in the appendix. Formal: Formal: The results for each dataset in the appendix. Formal: We’ve included the results for each dataset in the appendix. Formal: The final 5% of the samples we randomly picked for each dataset. Formal: Formal: Formal: The final 10% of the samples we randomly picked for each dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.303.json"
    },
    {
        "casual_text": "We offer support for three different acquisition models, depending on the language and the availability of human resources for the task. In all cases, there needs to be a line supervisor on-site who oversees the language acquisition process. This supervisor doesn’t necessarily need to know the language being worked on, but they should be able to communicate with the people doing the acquisition, no matter where they are or who they are. It’s also expected that this supervisor will spend some time getting to know the language, and if possible, learning to speak it. Their main job is to make sure the acquisition happens on time and meets the required quality standards. Additionally, the supervisor should have some background in computational linguistics to help with building, using, and evaluating the morphological analyzer.",
        "formal_text": "We provide support for three different models of acquisition, depending on the language involved and thus the availability of human resources for acquisition. All models involve having a line su pervisor in place in house for the language. This person need not know the language involved, but must be able to converse with the acquirers, wher ever and whoever they may be. It is also expected that this person spend some time familiarizing themselves with the language, learning to speak it if at all possible. This person has overall responsi bility for seeing that the acquisition takes place in a timely manner and is of acceptable quality. The supervisor also should have some computational linguistics background in order to assist in the con struction, use, and evaluation of the morphological analyzer.",
        "GPT2_formal_text": "_t = {}, S = {}, and S_i = {}. Formal: We tweak the input size (s) by trying to minimize the loss function (λ) as much as possible. Formal: For the eval2label training set, we get: Formal: For the validation set, we get: Formal: We train the classifier using all the labels and the current training set. Formal: Once we've picked the best label, we test it out to make sure it's the best one. Formal: The final classifier is trained using the validation set and the target labels. Formal: We use the self-supervised learning method from Mikolov et al. (2013), which is basically the method from Mikolov et al. (2013b) with some tweaks. Formal: Basically, we train the classifier using the validation set and the labeled data (from the target set). Formal: The goal is to minimize the cross-entropy loss function (λ) as much as possible. Formal: The classifier is trained using the validation set and the labeled data (from the target set). Formal: Formal: Finally, we test the classifier to see how well it does. Formal: Formal: The loss function we use is based on the cross-entropy loss function (λ) from Mikolov et al. (2013b). Formal: For the evaluation, we use the labeled data (from the target set). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "amta",
        "filename": "2006.amta-papers.1.json"
    },
    {
        "casual_text": "Following up on Zipf's (1949) ideas, Piantadosi and his colleagues in 2012 argued that, when you look at communication from an information theory perspective, ambiguity is actually a key factor in making a communication system work efficiently. They focused on the idea of using words in a way that saves effort. According to them, having words with multiple meanings can be useful, especially when the context helps clarify what's being said—it lets us reuse simpler words. To back up their claim, they showed a connection between how many meanings a word has in WordNet (Miller, 1995) and how easy it is for speakers to use those words. They looked at things like how easy the word is to pronounce, how long it is, and how common it is in everyday language (based on data from a big collection of text).",
        "formal_text": "Building on Zipf's (1949) theories, Piantadosi et al. 2012posit that, when viewed informationtheoretically, ambiguity is in fact a requirement for a communication system to be efficient. Focusing on economy of expression, Piantadosi et al. suggest that lexical ambiguity serves a purpose when the context allows for disambiguation-it allows the re-use of simpler word forms. 4 They support their hypothesis by demonstrating a correlation between the number of senses for a word listed in WordNet (Miller, 1995) and a number of measures of speaker effort-phonotactic well-formedness, word length and the word's log unigram probability (based on a maximum-likelihood estimate from a large corpus).",
        "GPT2_formal_text": "ed is the entity type, and T c (•) stands for the attention weight. The entity type is determined by the entity embedding matrix, which is H e (•). The entity embedding matrix H e is a matrix of size d×d, which includes d_t embeddings for each entity. In Figure 1, the input features for an entity type are shown as E e (•). The entity embedding matrix H e is built up like this: Formal: We build the relation matrix E r (•) using the softmax function on the input features, and then we use the cross-entropy loss to minimize it. Formal: We also calculate the cross-entropy loss for each entity type as (E e (•) + E r (•)). Formal: We use the model from Shain and Schuler (2017) to calculate the cross-entropy loss for the attention matrix A. Formal: Finally, we calculate the cross-entropy loss for the entity type as (E e (•) + E r (•)). Formal: Formal: Here, E e (•) and E r (•) are the entity type embeddings and attention weights, respectively. Formal: The cross-entropy loss for an entity type is a simple linear function that takes the hidden state h (•) and gives it a weight, like this: Formal: Formal: We also use the attention weight as a feature in the decoder. Formal: In the encoder, the hidden state is created by combining the hidden state h (•) with the attention weight, which is calculated using cross-entropy loss. Formal: To train the model, we use Adam (from Kingma and Ba, 2015) with a learning rate of η to minimize the cross-entropy loss, like this: Formal: Formal: In the decoder, the decoder model is trained using cross-entropy loss. Formal: Formal: The final embedding of an entity type is the combination of the entity embedding matrix H e and the entity type embedding matrix, which we call h T (•). Formal: Formal: Formal: We use the attention weight as a feature to train the model. Formal: Formal: Formal: Formal: Here, E e (•) and E r (•) are the entity type embeddings and attention weights, respectively.",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.328.json"
    },
    {
        "casual_text": "We compare models that were trained on different amounts of similar and different WikiMatrix samples. We start with the examples marked as different in section 3.2. Then, we pick the most detailed differences by using the bicleaner score (Ramírez-Sánchez et al., 2020) set at 0.5, 0.7, and 0.8. Check out section A for more info.",
        "formal_text": "Divergences We conduct an extensive comparison of models exposed to different amounts of equivalent and divergent WikiMatrix samples. Starting from the pool of examples identified as divergent at §3. 2, we rank and select the most fine-grained divergences by thresholding the bicleaner score (Ramírez-Sánchez et al. , 2020) at 0. 5, 0. 7 and 0. 8. For details, see A.",
        "GPT2_formal_text": ") and a dataset of citations to get a more detailed understanding of the topic. Formal: We've taken a closer look at what the methods we looked at and figured out a new way to create questions that can answer everything in a paper. Formal: Our findings suggest a cool approach for question generation. Unlike what they did before, our questions don't use any fancy or new language. Instead, we use a model that works with a mix of text, equations, and a special space model (called a multi-head attention network). This setup lets us pull out important details from the question and use that to make a good answer. Formal: We also ran some experiments to show how well our new method works. We tested it out with different setups—like different models and different datasets—and the results are in Table 3. We checked out the results for both single-question and multi-question generation. We compared each model to the best possible answer it came up with. Our method gave us results that were just as good as the best methods out there. Formal: We also looked at how our approach compares to other methods by using a more detailed comparison. Formal: Based on the results from these comparisons, we think our method is better than the current best, which is just one model. Formal: We think this is because our method can handle tricky questions better, especially in situations with really complicated or complex inputs. Formal: We also thought about making our method better at answering really long questions (like ones with lots of equations). Formal: In the future, we plan to look into ways to improve our methods, like using more complicated models or using the results to help with future questions. Formal: Lastly, we want to get a better handle on how people use the same question for different research projects. Formal: Since our method is pretty flexible, we think it could be useful for other areas too, like answering questions that might be hard to answer at first but might actually be good in the end. Formal: We also think about creating a model that can generate different versions of the same question. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.562.json"
    },
    {
        "casual_text": "It might look like the super simple TURN architecture isn't cut out for anything more than basic tasks. But actually, that's not true at all. Our first experiment used a TURN for a natural language agreement task from Linzen et al. (2016). This task involves predicting whether third person verbs in English text should be singular or plural, with some supervised training. For example, in the sentence \"The keys to the cabinet are on the table\", the RNN is trained to pick the plural \"are\" instead of the singular \"is\".",
        "formal_text": "It may seem that the extreme simplicity of the TURN architecture renders it unsuitable for any non-trivial processing task. In fact, this is not at all the case. Our first experiment applies a TURN to a natural language agreement task proposed by Linzen et al. (2016). This task is to predict the number of third person verbs in English text, with supervised training. In the phrase \"The keys to the cabinet are on the table\", the RNN is trained to predict the plural \"are\" rather than the singular \"is\".",
        "GPT2_formal_text": "_t = [mask_t, src_t] ŷ, which is just the output from the attention mechanism. Formal: After you've chosen an embedding, you add a linear transformation to make it a proper embedding. Formal: From Eq. 6, the attention matrix A gets updated to create a new attention matrix called A_t by combining the output from the token-level attention and the previous output. Formal: Finally, Eq. 7 gives you the final result of the attention process. Formal: The attention weights in A are calculated using this formula: Formal: For the instance embedding layer, we start by picking the most relevant token from the set E. Then, for the label embedding layer, we pick the token that's closest to the label in the sentence, which is σ_i. Formal: In this paper, we're mainly focusing on the BiLSTM-based Transformer model, which is a popular model for sequence generation. Formal: Next, we use a BiLSTM to create context representations of the input sequence, called h_i. For each pair of tokens y and t, the BiLSTM picks a token y_i based on the input sequence x_i. Formal: To get the attention weights in h_i, we use this formula: Formal: The attention weights in h_i are calculated using this formula: Formal: So, for the label embedding layer, we start by picking the token that's closest to the label in the sentence. Then, for the instance embedding layer, we pick the token that's closest to the instance. Formal: Finally, we use a BiLSTM to create context representations of the input sequence, h_i. Formal: The attention weights in h_i are calculated using this formula: Formal: We can do this for both the label and instance embedding layers by just selecting the token that's closest to the label in the sentence. Formal: After calculating the attention weights in h_i, we use this formula: Formal: In Figure 1, we can see the attention mechanism for a BiLSTM-based sequence generation model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "cmcl",
        "filename": "2022.cmcl-1.2.json"
    },
    {
        "casual_text": "In the Generation Track, each model creates its own set of KPs, so there isn't a single standard set to follow. Along with the official evaluation metrics, two extra scores are included: the average of the strict and relaxed mAP values, and p@50% for the strict view. Submissions that included descriptions are marked with (*). \n\nFigure 2 shows:\n(a) How KPs are spread across arguments for the pro stance on the topic \"Social media platforms should be regulated by the government.\" This is for the expert ground truth (strict view) and selected models, normalized by the number of arguments that match a KP. The JSd compared to the expert distribution is in brackets.\n(b) JSd rank vs. AP rank for all models in the Matching Track, per topic and stance.\n\nThe topic id and topic text mapping isn't relevant for comparing KP distributions, so we can't directly compare them like we did above. But, we can still do a qualitative, anecdotal analysis. Table 8 in the appendix lists the KPs generated for one topic and stance by the expert and different models. The expert made their KPs beforehand without seeing the actual arguments, while the models generated KPs based on the input arguments. Looking at the generated KPs, it seems like the expert might have missed some.",
        "formal_text": "In the Generation Track, there is no single set of KPs, since each model generates its own set. It is In addition to the official evaluation metrics, two more scores are provided: the average value of the strict and relaxed mAP values, and p@50% for the strict view. Submissions that provided a descriptions are marked with (*). Figure 2: (a): The distribution of KPs among the arguments, in the pro stance on the topic Social media platforms should be regulated by the government, for the expert ground truth (strict view) and selected models, normalized to the number of arguments that match to a KP. The JSd vs. the expert distribution is given in brackets. (b) JSd rank vs. AP rank for all models in the Matching Track, per topic and stance. The map between topic id, and topic text can be found at therefore not relevant to compare the distributions over KPs as done above. However, it is possible to perform a qualitative anecdotal analysis. Table 8 in the appendix lists the KPs generated for one topic and stance, by the expert and the different models. Importantly, the expert produced KPs a priori, without seeing the actual set of arguments; while the models generated KPs based on the input arguments. Inspection of the generated KPs suggests that correspondingly, some KPs were perhaps overlooked by the expert.",
        "GPT2_formal_text": "= { schema_id, schema_value } n = { 1, 2, ..., n_K } n_k = 1 # Set the k-th value for γ. Formal: The input sentence is basically a sequence of words. Formal: The output sentence is a sequence of words with just one token, which is called a lexical token. Formal: For the second type of setup, we use the output from the neural NER module to spot important words in the input text. Let’s call this module bn_i(λ) i = {λ_i1, ..., λ_iK }, and the input sentence is the output from it, as shown in equation (1). Formal: This setup lets the model use the input sentence as context to figure out the probability distribution for the correct labels, which helps it find the right labels in the text. Formal: We do the same thing for the third type of setup, where we use the output from the neural NER module to identify the important words in the input sentence. Formal: The output from this module is the same as in the first type of setup, but this time we’re dealing with a context-dependent probability distribution. Formal: Lastly, for the fourth type of setup, we use the output from the neural NER module to find the important words in the input sentence. Formal: In this case, we’re using the output from the neural NER module for the first type of setup, but for the second type, we’re using the output from the neural NER module for the third type. Formal: In all these cases, we use the neural NER module to find the important words in the input sentence. Formal: Formal: This means that our system gets a probability distribution for the right labels based on the input sentence. Formal: And for the last type of setup, we use the output from the neural NER module to find the important words in the input sentence. Formal: We can’t directly calculate the probability for the correct labels because we don’t have a lexical token to work with. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "argmining",
        "filename": "2021.argmining-1.16.json"
    },
    {
        "casual_text": "D is just a way to measure how different two probability distributions are, and we're using something called Kullback-Leibler divergence for that. The main goal, L sup, for supervised learning is basically the combination of two things: L ce and L com.",
        "formal_text": "where D is a distance function between probability distributions (we use the Kullback-Leibler divergence). The final objective L sup for supervised learning is the sum of L ce and L com.",
        "GPT2_formal_text": "= [mask] token_attention = [attention_mask] self.attention_max = max(0, token_attention) self.attention_min = min(0, token_attention) self.attention_ranges = [ranges] for i in range(0, max_attention_ranges) do self.attention_ranges[i] = [ranges] end self.attention_mask = mask end end end class C_sentence_2D: def parse(self): return self.parse_eval(C_sentence) self.parse_eval(C_sentence) def parse_eval(C_sentence, i): return self.parse_eval(C_sentence, i) self.parse_eval(C_sentence, i) def parse_eval_m(C_sentence, i): return self.parse_eval_m(C_sentence, i) self.parse_eval_m(C_sentence, i) def parse_eval_m(C_sentence, i): return self.parse_eval_m(C_sentence, i) self.parse_eval_m(C_sentence, i) def parse_eval(C_sentence, i): return self.parse_eval(C_sentence, i) self.parse_eval(C_sentence, i) def parse_eval_m(C_sentence, i): return self.parse_eval(C_sentence, i) self.parse_eval(C_sentence, i) def parse_eval_r(C_sentence, i): return self.parse_eval_r(C_sentence, i) self.parse_eval_r(C_sentence, i) def parse_eval_r(C_sentence, i): return self.parse_eval_r(C_sentence, i) self.parse_eval_r(C_sentence, i) def parse_eval_r(C_sentence, i): return self.parse_eval_r(C_sentence, i) self.parse_eval_r(C_sentence, i) def parse_eval_m(C_sentence, i): return self.parse_eval_m(C_sentence, i",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.319.json"
    },
    {
        "casual_text": "Tables 9 and 10 show how our algorithm boosts the performance of our QuALiM system, like you can see in (Kaisser et al., 2006). In Section 6 of this paper, we explain how answer candidates are ranked using formulas 2 and 3. This ranking is added to the existing QA system's ranking as an extra feature, helping to boost candidates based on their confidence score. \n\nThe difference between the two tables is that Table 9 uses all 1658 questions in our test sets for evaluation, while Table 10 only looks at the 1122 questions where our system could learn a pattern. So, for Table 10, questions that the system couldn’t answer due to limited training data are left out. As you can see, accuracy@1 goes up by 4.9% on the full test set and by 11.5% on the smaller set.\n\nKeep in mind that the baseline QA system we’re comparing to has a couple of advantages: a) It has web-based components, so it can access way more text information. b) The algorithm we’re talking about in this paper is just for answer extraction. For paragraph retrieval, we use the same method as for evaluation set 1, which is explained in Section 7.1. But, in over 20% of cases, this method doesn’t return a single paragraph that has both the answer and at least one question keyword. In those cases, the basic paragraph retrieval makes it really hard for our algorithm to give the right answer.",
        "formal_text": "Tables 9 and 10 show how our algorithm increases performance of our QuALiM system, see e. g. (Kaisser et al. , 2006). Section 6 in this paper describes via formulas 2 and 3 how answer candidates are ranked. This ranking is combined with the existing QA system's candidate ranking by simply using it as an additional feature that boosts candidates proportionally to their confidence score. The difference between both tables is that the first uses all 1658 questions in our test sets for the evaluation, whereas the second considers only those 1122 questions for which our system was able to learn a pattern. Thus for Table 10 questions which the system had no chance of answering due to limited training data are omitted. As can be seen, accuracy@1 increases by 4. 9% on the complete test set and by 11. 5% on the partial set. Note that the QA system used as a baseline is at an advantage in at least two respects: a) It has important web-based components and as such has access to a much larger body of textual information. b) The algorithm described in this paper is an answer extraction approach only. For paragraph retrieval we use the same approach as for evaluation set 1, see Section 7. 1. However, in more than 20% of the cases, this method returns not a single paragraph that contains both the answer and at least one question keyword. In such cases, the simple paragraph retrieval makes it close to impossible for our algorithm to return the correct answer.",
        "GPT2_formal_text": "= {}, y_i = y_i + 1, w_i = w_i + 1, l = (1, 1), tag = tag, v = (1, 1), q = q + 1} Formal: When the first w_i is 1, the decoder takes the input {}, turns it into a token, and spits out the next token, {}, all in the order {w_1, w_2, w_3, ..., w_T}. Formal: We set up a situation where the decoder creates each of those tokens, which we call {v}, in the order {w_1, w_2, ..., w_T}. Formal: The decoder's first turn, w_0, gets passed to the next model, w_1. It then generates the next token, {v}, in the order {w_1, w_2, ..., w_T}. Formal: After the decoder's second turn, w_0, it sends its token, {v}, to the last model, w_T, which generates the token {v}, again in the order {w_1, w_2, ..., w_T}. Formal: Formal: The decoder also gets the input {}, turns it into a token, and spits out the next token, {}, in the order {w_1, w_2, w_3, ..., w_T}. Formal: The encoder has a turn, w_0, where it takes the input {}, turns it into a token, and spits out the next token, {v}, in the order {w_1, w_2, ..., w_T}. Formal: Formal: After the encoder's third turn, w_0, it sends its token, {v}, to the decoder. Formal: Formal: Finally, the decoder has its turn, w_0, where it takes the input {}, turns it into a token, and spits out the next token, {v}, in the order {w_1, w_2, w_3, ..., w_T}. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E12",
        "filename": "E12-1010.json"
    },
    {
        "casual_text": "DRG is a new way of representing DRS graphs, created specifically for MRP 2020. The goal was to make it as similar as possible to other frameworks (Abzianidze et al., 2020). But there are already a few different ways to encode DRS data. For example, Liu et al. (2018) focused more on labeling edges than nodes in their DRG format. On the other hand, van Noord et al. (2018) worked with DRS in a clausal form, using sets of triples and quadruples. This latter approach is more common among DRS parsers, especially since it was the official format for the DRS parsing shared task (Abzianidze et al., 2019).\n\nThat shared task led to several DRS parsers, including ones by Evang (2019), van Noord (2019), and Fancellu et al. (2019). The best results (with an F1 score of 0.85) came from a word-level sequence-to-sequence model using a Transformer. Just a heads-up: the F1 score for the DRS shared task was based on clausal forms, which isn't directly comparable to the MRP F1 score calculated over DRGs.",
        "formal_text": "DRG is a novel graph representation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al. , 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al. , 2019). The shared task gave rise to several DRS parsers: Evang (2019); ; van Noord (2019); Fancellu et al. (2019), among which the best results (F 1 = 0. 85) were achieved by the word-level sequence-to-sequence model with Tranformer. Note that the DRS shared task used F 1 calculated based on the DRS clausal forms, which is not comparable to MRP F 1 over DRGs.",
        "GPT2_formal_text": "ed = self.attention_masked + self.attention_decoder. This decoder uses a max-pooling method to break down the text into smaller pieces. Formal: In this paper, we talk about how to make the attention representation more meaningful. Formal: We use a method called \"two-dimensional scaling\" to make sure the attention representation of a token is good for explaining the model's decisions. Formal: We use three different methods to measure how important each token is. Formal: The attention mechanism learns to focus on the important parts of the text. It does this by learning a representation that shows the important bits. Formal: We also came up with a way to tell the difference between important and irrelevant tokens by using the difference in the attention weights between the token and the words around it. Formal: We looked at two types of feedback: Formal: We’re introducing two attention mechanisms, AttnGRU and AttnGRNL, to help annotators decide if a piece of feedback is useful or not. Formal: We’ve also come up with two ways to tell the difference between important and irrelevant feedback. Formal: We use the difference in attention weights between the token and the words around it to decide if a feedback comment is useful or not. Formal: We found that if two feedback comments are similar, the attention weights for them should be similar too. Formal: We tested our methods on two different datasets, and the results show that our approach works well. Formal: We tested our methods on two datasets, and the results show that our method works well. Formal: We also looked at two types of feedback: Formal: We’re introducing two attention mechanisms, AttnGRU and AttnGRNL, to help annotators decide if a piece of feedback is useful or not. Formal: We’ve also come up with two ways to tell the difference between important and irrelevant feedback. Formal: We tested our methods on two different datasets, and the results show that our method works well. Formal: We also looked at two types of feedback: Formal: We’re introducing two attention mechanisms, AttnGRU and AttnGRNL, to help annotators decide if a piece of feedback is useful or not. Formal: We’ve also come up with two ways to tell the difference between important and irrelevant feedback. Formal: Formal: We also found that",
        "directory": "conll",
        "filename": "2020.conll-shared.1.json"
    },
    {
        "casual_text": "For data augmentation, we used synonym replacement on the original dataset, doubling its size. Table 2 shows the results of using this approach on a generation task. We noticed that applying data augmentation to the generation task actually made things worse. But when we used the augmented data for the grounding task, the model scored a 40.55 F1 score and a 23.49 exact match score. This is better than our baseline model, which was trained only on the original grounding task data, by +0.5 F1 score and +0.64 exact match score. These findings suggest that using synonym data augmentation on the generation task's answers doesn't really help the model learn anything useful for the generation task. So, we decided to only use the augmented data for the grounding task during multi-task learning.",
        "formal_text": "Data Augmentation For data augmentation, we apply synonym transformation to the original dataset, attaining twice the baseline size. Table 2 presents the result for data augmentation on generation task. We have observed that applying data augmentation to the generation task degraded the performance. However, by utilizing augmented data on the grounding task, the model achieves a 40. 55 F1 score and a 23. 49 exact match score. Compared to our baseline model implementation trained with the original grounding task data, training with augmented data improved +0. 5 F1 score and +0. 64 exact match score. These results demonstrate that synonym data augmentation on the generation task's gold answers does not provide the model with any informative knowledge for the generation task. Therefore, we include augmented data only on grounding task during multi-task learning.",
        "GPT2_formal_text": ". [CLS] and [END] are only used when the embedded query and key are in the same vocabulary, so they're called \"query_ids.\" Formal: Basically, if you have a set of query elements x and key_k, you can create queries that can answer questions with a certain set of key pieces. These queries can then be used in a text-based QA system to get the answer. Formal: Using the query and key, the system creates a query query_p that can answer questions with a given key_k. For example, Formal: This means we take the query q and the key_k from the main part, and use that to create a query query_q that can answer questions with the key_k. Formal: Formal: Basically, you take the main part of the query and the key_k from the main part, and then use that to make a query query_q that can answer questions with the key_k. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "dialdoc",
        "filename": "2022.dialdoc-1.15.json"
    },
    {
        "casual_text": "Basically, this means that something is considered recognized if we can get the empty string starting from S(0, n, n + 1, n + 1 + m). According to Theorem 1.2 from Boullier's 2000 paper, figuring out if a bottom-up non-erasing k-RCG can be recognized can be done in time O(|G|n).",
        "formal_text": "is recognized iff the empty string can be derived from S( 0, n, n + 1, n + 1 + m ). Theorem 1. 2 ( (Boullier, 2000) ). The recognition problem of bottom-up non-erasing k-RCG can be solved in time O(|G|n",
        "GPT2_formal_text": "_t; } }\n\nIt's kind of a weird way of saying \"num_t\" is just the total number of non-zero numbers in the attention token. The attention mask token is like a token that helps the model focus on the important parts of the text. Formal: The softmax layer at the end makes sure the attention vector stays focused on the key points. But in our experiments, we didn't use softmax because we thought it would mess with the original formula, which was set up to use softmax on the attention. The key points are just the key points in the sentence, not the whole sentence itself. Formal: Here's how the attention works at the sentence level, starting from the top: Formal: So, c_i is the attention vector at the i-th position in the sentence, and f_i is the attention vector for the i-th word in the sentence. Formal: We tweak the formula a bit to make it work with this updated setup. Formal: Instead of multiplying the attention vectors with the vector for the key points, we just use the vector for the token that’s closest to the key points. Formal: The original formula only used a one-dimensional vector to represent the text, but we’re switching to a two-dimensional vector to represent the key points. Formal: The two-dimensional vector helps focus the attention on the key points, making the attention more consistent. Formal: We’re using a feed-forward neural network (FFNN) for the sentence level attention, which has two main parts: Formal: Here’s how we calculate the attention vector. Formal: We start by creating a one-dimensional vector for the text using a multilingual BERT model. Then, we use an attention mechanism to pass this vector through an output layer to get the attention vector. Formal: The attention vector is made up of two parts: the hidden representations of the key points in the sentence and the attention vector. Formal: We use a feed-forward neural network (FFNN) to process the input sequence, starting from the beginning and going to the end. Formal: We add a special token called M for the first token in the input sequence. This token serves as a placeholder for the token that hasn’t been seen yet. Formal: The attention token is a vector of two elements: the hidden representation of the key points and the attention vector. Form",
        "directory": "C08",
        "filename": "C08-2026.json"
    },
    {
        "casual_text": "1. SyntheticQE-MT: Basically, the MT models are used to create the target side of the synthetic data.",
        "formal_text": "1. SyntheticQE-MT: The target side of the synthetic data is produced using MT models.",
        "GPT2_formal_text": "= [ e_i, e_j, ..., e_K, e_L, e_M, ..., e_N ]. This mapping is built up in a way that looks like this: q_i ∈ R_rpq Σ e_i = q_i ∈ R_rpq Σ e_j = q_j ∈ R_rpq Σ e_j = q_j ∈ R_rpq Σ e_j = q_j. Formal: Basically, this is the same as the inductive hypothesis, but we've added a bit more context. Formal: The method we're proposing is based on a learning method that works with these contextualized word embeddings, which are a type of sub-word units. Formal: We use a specific example to explain how the method works. Formal: We can easily find the context and embeddings in a different text. Formal: We use a method that takes these contextualized embeddings and figures out the hidden state for the meaning of the embedding. Formal: The hidden state of the embedding for the given word is then used to create a new embedding for the word. Formal: The meaning of the embedding is then used to create a new embedding for the word. Formal: By using this method, we can represent the meaning of a word as a sequence of embeddings that are organized in a hierarchy. Formal: In the next part, we'll talk about how to create embeddings that can describe the meaning of a specific word. Formal: The output from the word embedding is used to represent the meaning of the word. Formal: To build a model that can describe the meaning of a word, we start by training it using the embedding of the word's meaning. Formal: We then use this model to find the context and embeddings for the word. Formal: Finally, we use this model to create a new embedding for the word. Formal: In the third part, we'll explain how to use the output from the word embedding to generate a new embedding for the word. Formal: Finally, we use this model to generate the word's embedding. Formal: Here, the word's embedding and its context are represented using a conditional random field (CRF) (Lafferty et al., 2001). Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.267.json"
    },
    {
        "casual_text": "Okay, so when we looked at the errors in our system, we found that for certain words, it couldn’t make a matching cluster for specific meanings. For example, words like \"yard\" when it refers to a ship, or the \"impound/enclosure\" meaning of \"pound\" didn’t get matched properly. It seems like this happened because these meanings don’t show up much in English Wikipedia. In fact, the \"yard\" meaning as a ship part only appeared twice, and the \"impound/enclosure\" meaning of \"pound\" showed up three times in the top 100 words. That’s why the system didn’t create a cluster for them.",
        "formal_text": "Analysis Analysis of our system's error shows that for some words the system could not create a matching cluster for specific senses (to name a few examples, \"yard\" as a ship identifier and \"impound/enclosure\" sense for the word \"pound\"). It appears that a matching cluster was not created due to the low tally of these senses in the English Wikipedia, and indeed the two senses appeared only two and three times respectively in the 100 for all words 17 Computed as",
        "GPT2_formal_text": ", which we'll call t_m. Formal: The local neighbors of entity i, t_i, are the closest neighbors in the graph G = {h_i, t_i, w_i, a_i}. Formal: To get the global neighbors, we use this equation: Formal: Equation 2 shows how the graph G = {h_i, t_i, w_i, a_i} is built based on the attention. Formal: Equation 3 explains how we can use the total attention weight to estimate the global neighbors. Formal: Equation 4 shows the joint probability distribution, p_ij, for the two neighbors of entity i, t_i, during a specific time step t. Formal: Equation 5 shows the joint probability distribution for the global neighbors. Formal: Here, i is the i-th row in the matrix D, and A_i and r_i are the parameters of the entity and relation, respectively. Formal: Equation 6 shows how we estimate the global neighborhood. Formal: Equation 7 shows the joint probability distribution for the global neighbors. Formal: Equations 8 and 9 explain how we can use the attention distribution to find the global neighbors. Formal: The attention weights are adjusted by the dot product of the two vectors (h_i, t_i, w_i, a_i) and their cosine similarities (•). Formal: Equations 10 and 11 are the hyperparameters we used for our experiments. Formal: To make sure our model works well, we trained it for 100 epochs with a learning rate of 1e-5 and a batch size of 32. Formal: For all the experiments, we used the AdamW optimizer. Formal: The results are shown in Figure 3. Formal: We noticed that our model has a lot of noise, and the best model is the one with the fewest parameters. Formal: Formal: Using this formula, we get the conditional probability of the local neighbors based on the global neighbors. Formal: Formal: Formal: The results are in Figure 4. Formal: We use different representations for the entities and relations to see how they affect the evaluation. Formal: Formal: Formal: Formal: The result is in Figure 5. Formal: Formal: We also check how similar the entity and relation representations are. Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.325.json"
    },
    {
        "casual_text": "So, these findings suggest that the window size for rank N across the whole dataset and the 4th and 5th features are pretty important. They seem to stand out and play a significant role in the overall analysis.",
        "formal_text": "These results lead to the conclusion that the window size of the rank N for the entire corpus and the 4 È Ö × ÓÒ AEÙÑ Ö Ó ×ÝÒÓÒÝÑ× ÓÖÖ ØÐÝ ÜØÖ Ø AEÙÑ Ö Ó ×ÝÒÓÒÝÑ× ÜØÖ Ø 5 Ê ÐÐ AEÙÑ Ö Ó ×ÝÒÓÒÝÑ× ÓÖÖ ØÐÝ ÜØÖ Ø AEÙÑ Ö Ó ×ÝÒÓÒÝÑ× Ò Ò×Û Ö × Ø",
        "GPT2_formal_text": "= [...], # x_n_i = n, # p_n_i = n, # l_n_i = l, # # t = t. # f_n_i = f_n_i * x_n_i, # p_n_i = p_n_i * x_n_i, # l_n_i = l_n_i * x_n_i, # # t = t. #\n\nIn simpler terms:\n\nSo, it's like a mix of how much each thing contributes (weight) and how much it controls (control). Formal: The system breaks down each part (x_n_i) into smaller parts (x_n_i) and then uses these to predict the next word, p_n_i. The final result is a mix of all these parts. Formal: The system is like a big pile of numbers, like a matrix. Formal: The system is like a table with a few columns (n) and a few rows (n_0). Formal: The system is like a tree with some labels (x_n_i). Formal: The system is like a grid with a few cells (n_0) and a few rows (n_n_i). Formal: The system is like a box with a few spots where different words fit. Formal: The system is like a square with a few spots where different words might fit. Formal: The system is like a circle with some spots where different words might fit. Formal: The system is like a diamond with some spots where different words might fit. Formal: The system is like a straight line with a few spots where different words might fit. Formal: The system is like a pentagon with some spots where different words might fit. Formal: The system is like a diamond with some spots where different words might fit. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C04",
        "filename": "C04-1116.json"
    },
    {
        "casual_text": "Okay, so let’s say we have a pair of sentences, (x, y), where x is a sentence in some input (source) language, and y is its translation in a different output (target) language. X and Y represent the vocabularies for the input and output languages, respectively. The IBM alignment models (from 1 to 5) are basically ways to calculate the probability P(x|y), which tells us how likely it is that x is the original sentence that got translated into y.",
        "formal_text": "Let (x, y) be a pair of input-output sentences; i. e. x is a sentence in a certain input (source) language and y is its corresponding translation in a different output (target) language. Let X and Y denote the input and output vocabularies, respectively. The IBM aligment models (1 to 5) are parametric models for the translation probability P (x | y); i. e. , the probability that x is the source sentence from which we get a given translation y.",
        "GPT2_formal_text": "= [mask] token = [token] entity_mask = [mask] entity_input = [input] and all these are normalized using a norm-based loss function. Formal: The attention-based decoder also uses something called attention. This is a type of attention matrix that's learned through training. Formal: The attention weights (let's call them w_t and w_v) are represented using embeddings that are learned to represent the input and output. The attention weights are learned through experiments. Formal: We show the results for the new top model, S2S-ZH, in Figure 3. As you can see, our model totally outperforms the previous best model, S2S-JOU. Formal: The new top model, S2S-ZH, beats both S2S-JOU and S2S-JOU+TSC by a lot. This shows that using big-picture information from both the input and output in the decoder helps a lot in generating high-quality text. Formal: The attention weights (W_t and W_v) are also learned through experiments, and they show that using the attention mechanism to focus on the key parts of the input and output makes a big difference. Formal: So, to sum it up, this study helps us understand how important it is to use a well-thought-out attention mechanism for high-quality text generation. Formal: We're curious to see how we can use our approach in other areas of NLP, like text mining, to see how it could be useful. Formal: We think this study has a lot of potential for future work. Formal: We're also planning to look into some other things to see if this approach can help improve how we generate text. Formal: We're introducing a new dataset called NMT_Text to help with analyzing the input and output in NMT models. Formal: We're planning to explore different methods to analyze the input and output, including something called chunking. Formal: We're also looking into how to optimize the model using something called variational inference with the attention mechanism. Formal: We're also checking out how to improve the model by using label propagation and adversarial training techniques. Formal: Formal: For the future, we're planning to explore more advanced models, like recurrent neural networks, that can learn from multiple sentences, and we're also thinking about how to combine the",
        "directory": "eamt",
        "filename": "2006.eamt-1.20.json"
    },
    {
        "casual_text": "This method assigns a weight to each paragraph, showing how likely it is to be related to other paragraphs. The weight takes into account things like the paragraph's length, how big and how often certain word groups (n-grams) show up in it, and how those n-grams are spread out across the whole text.",
        "formal_text": "This procedme associates each paragraph with a weight representing its probability of correlating with other paragraphs. The weight considers factors such as the size of the paragraph, the size and frequency of n-grams existing within that paragraph, and the distribution of the n-grams throughout the corpus.",
        "GPT2_formal_text": "[i] = (i, {a_t}, a_s) else: a_t = (i, {a_s}, a_t)  \n\nFor our dataset, we’re using the top 2,000 most common sentence pairs from the 2014 WSJ part of the Penn Treebank corpus. We’re also including the top 2000, which are the top 2000 most common sentence pairs from the same corpus. Formal: For each sentence, we pick the top 1,000 sentences that match the query. Formal: For each query, we grab the top 1,000 sentences that include the query. Formal: For each query, we grab the top 1,000 sentences that include the query. Formal: For each query, we grab the top 1,000 sentences that include the query. Formal: For each query, we grab the top 1,000 sentences that include the query. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C94",
        "filename": "C94-2171.json"
    },
    {
        "casual_text": "Alright, so the second extra dataset we made came from pulling out all the split-antecedent examples from the raw annotations in PD to make sure we got as many as possible. After grabbing all those split-antecedent annotations, we used majority voting to sort things out when people didn’t agree. This gave us 47.7k split-antecedent annotations tied to 6.2k mentions (check Table 1 for the details). \n\nWe tested how good this method was using the gold part of the PD corpus, and it turned out pretty well—91.7% recall, which is exactly what we wanted. But, as you’d expect, the data’s a bit messy, with only 11.1% precision and an F1 score of 19.7%. We went through the false positives manually and found most of the errors were because of three things: single-antecedent coreference (where the chains were marked as split-antecedents), bridging references (which didn’t need to be annotated), and other annotation mistakes. \n\nNow, the first two types of mistakes aren’t a big deal for our task because our third and fourth datasets are built using those kinds of relationships. \n\nOh, and another thing—ARRAU also has bridging references, and one of the bridging relations they annotated, element-of (and its inverse), is super relevant to figuring out split-antecedent plurals.",
        "formal_text": "Raw Crowd Annotations (PD-CROWD) The second auxiliary corpus was created by extracting all split-antecedent examples from the raw annotations in PD to maximise recall. After extracting all splitantecedent annotations, we used majority voting as our aggregation method when players did not agree on split-antecedent annotations. In this way, we extracted 47. 7k split-antecedent annotations associated with 6. 2k mentions ( Table 1 ). The quality of this extraction method was evaluated on the gold portion of the PD corpus as well; the resulting dataset has a recall of 91. 7%, which fulfils the goal of this setting. As expected, the corpus is noisy, with a precision of 11. 1% and an F1 of 19. 7%. We manually checked the false-positive examples, finding they are mainly due to three types of mistakes: single-antecedent coreference (the coreference chains were annotated as the split-antecedent), bridging reference (not required to be annotated), and other annotation mistakes. The first two types of mistakes are not harmful to our task as our third and fourth auxiliary corpora are created using those types of relations. Element-of Bridging References (ELEMENT-OF) ARRAU is also annotated with bridging references, and one of the bridging relations covered by the annotation, element-of (and its inverse) are very closely related to the task of resolving split-antecedent plurals.",
        "GPT2_formal_text": "is set to 1. This means that when the model gives a prediction, the attention to that word can only be determined by the token type that came before it in the attention matrix. You can adjust this number based on how long the text is and how many token types it has. If the token type for a word is \"dictionary_word\" and the length of the text is 4, then the attention to that word is 1. If it's \"entity_entity\", then the attention is 0. The rest of the tokens in the text get a weight of 0.5. The model is trained using the method from (Bahdanau et al., 2015). Formal: To create the attention weights, we use Adam (Kingma and Ba, 2014) to estimate the parameter. We set the early stopping parameter to 0.1. Formal: To figure out the token type for the word \"wanna_t\", we use the supervised token tagger (Eck et al., 2015) to tag the words in the sentence. The token type of each word is determined by its parent token. Formal: To get the attention weights, we use the attention-based attention mechanism (Das et al., 2017) to calculate the weights. Formal: We use an n-gram language model (LM) to predict the attention weights. Formal: We take the words in the source text and the target text, and train a neural network classifier using the validation set as the training data. Formal: The model we use is an LSTM-based model (Kim, 2014), which was trained using a dataset called SentencePiece (Liu et al., 2015). Formal: The main goal of the model is to predict the attention weights for each word in the source sentence. This is based on the length of the source sentence, the number of tokens in the source sentence, the token type of the word in the source sentence, and the attention weights for the words in the target sentence. Formal: If the model has already learned the attention weights for all the tokens in the target sentence, we just skip that part. Formal: In the next parts, we’ll show how we calculate the token types for each token. We’ll also talk about the supervised and unsupervised methods we used to get these token types. Formal: For this part, we’ll just focus on the supervised method. Formal: Next, we calculate the token",
        "directory": "coling",
        "filename": "2020.coling-main.538.json"
    },
    {
        "casual_text": "Wow, this text is completely scrambled and looks like a random mix of symbols and letters. It seems like it's been encrypted or encoded in some way. If you're trying to decode it or make sense of it, you might need to use a specific key or method to unlock the original message. It could be a code, a cipher, or even a mistake in formatting. \n\nIf you have any additional context or know what this is supposed to be, I can try to help you figure it out! Otherwise, it’s a bit like trying to read a foreign language without a dictionary. Let me know if you need help with anything else!",
        "formal_text": "¦ ¢ ¥ F B þ ¥ 3 © þ ¥ ¤ w ¤ ¦ § 4 V þ X q ¥ ¢ ÿ l ¤ 0 ¤ ¦ § A ¢ ± ü ¦ A ¢ ¤ z ü ¦ § © ÿ H ü ¦ A ¢ Y 4 7 © ü ¦ I 3 ¤ H ¥! R © A ¢ \" 9 & Ê þ b ü ¦ I 3 ¤ H! ¦ ¦ ý Í (! ¦ B % z 1 ¢ 4 n ü ¦ I 9 2 Ê ü i ÿ R § \" ¢ 4 0 b U ) r t © A ¢ ¤ ¦ 4 7! 0 I ( t ü ¦ I H ü ¦ A ¢ ü i ÿ 2 ( £ ¢ ¤ ) H © A ¢ \" 9 & Ê þ b ü ¦ I ¤ R 4 þ ¥ § ¤ 1 ¢ © s ÿ i p & r H # # I P ¤ \" U v C E ¦ Q ¢ Y 4 7 3 4 7 ¢ & % ü 1! C þ b ü 1 ¢ a e © A ¢ 4 \" # é þ B! C þ b ü ¦ I 9 ¥ ¢ $ ¤ ¦ ¢ g ü 1 ¢ 4 7 ¢ ¤ F $ ü ¦ A ¢ 0 ý þ ¥ t! 0 ¢ º þ ¥ ¤ 1 £ ¢ ' ü ¦ þ b ü Á ü ¦ A ¢ ¡ H ¥! C ý r þ b ü ¦ I ¤ z ü 1! 0 § 4 n ü ¦ § A! 0 ¢ l H 2 § A ¢ ¤ % ü ¦ I ¤ e þ ¥ ¤ t A ü ÿ ¥ ¢ ü e @ ¢ \" ¢ °¤ ¦ § A G 4 \" I ¢ o ü ¦ #ÿ ¢ # Ê þ B £ ¥! C þ b ü 1 ¢ © § A ( £ b U S b x 0 a E ¤ ¤ ¡ A E g W Y § A! ) 4 \" # Ê þ ¥ ¤ ¦ ¤ ¦ I h g 4 þ b ü ¦ I H ü ¦ A ¢ 8 © A ¢ \" 9 & Ê þ b ü ¦ I 3 ¤ H! 0 ¦ ý (! ¦ z 1 ¢ 4 T % ü ¦ I 9 & ü i ÿ t s ¤ 8 t þ ¥ ¤ ¦ ¢ © { ý r þ ¥ #ÿ X ü ¦ A ¢ ý ¥! ¦ ( 3 A B % q ¤ z ÿ & o ü þ ¥ 4 n ü ¦ 4 H¢ º þ b ü ¦ § A! ¦ ¢ ¤ ) H A 2 © ¢ ¤ ) 4 7 ¢ 4 n ü 1 ¢ © s ÿ þ $ & % f (! ¦ z 1 ¢ 4 n ü ¦ I 9 ¥ ¢ © A ¢ \" ( £ ¢ © A ¢ 3 4 n ÿ ¢ © ' ¢ ¥ U b x 3 0 $ g c ¤ \" B ¥ c ¡ ¥ E g c ¤ ¡ C D A B g 5 ¥ } 2 ¤ i P R I ü © ÿ ( £ ¢ ¤ H © A ¢ \" 9 & Ê þ b ü ¦ I ¤ H! 0 ¦ ý (! 0 z ¢ 4 n ü ¦ I 9 & ü © ÿ P 8 ü ¦ þ X A ¦ ý t þ ¥ # c A & © A ¢ ¦ þ ¥ © j ü ¦ ¤ Î þ b ü z ü 1! 0 I 3 § ü 1 ¢ a 4 7 ¢ 4 n ü 1 ¢ © Q o ÿ þ A A % f (! ¦ z 1 ¢ 4 n ü ¦ I 9 ¥ ¢ u ¢ © ' ¢ $ 4 þ ¥ t @ ¢ w © 3 ¤ z ü ¦ ' § 3 ¤ ¦ A ¢ © b ¾ \" ü ¦ A ¢ þ b ü z ü 1! 0 I g § x ü 1 ¢ v ¤ w A A % f (!",
        "GPT2_formal_text": "_t)) { + lem = cos(len(B) + u_f + u_p + b_t)  \n}  \n   E = tanh(u_v + tanh(u_f + u_p + b_t))  \n   T = tanh(u_v + tanh(u_f + u_p + b_t))  \n    E = wcos(B + u_v + e_f + u_p + b_t)  \n    T = wcos(B + u_v + e_f + u_p + b_t)  \n\nHere's how it works:  \n- **Input**:  \n   - The list of words that match and are part of the dictionary.  \n   - A list of words that aren't part of the dictionary.  \n   - The list of words in the dictionary that match.  \n\n- **Output**:  \n   - The list of words that aren't in the dictionary, but still belong to the same group.  \n\nSo, in short, it's a list of words with their matching info added to a dictionary. Formal: In the future, we'll make sure we handle more complex cases, like compound words and morphology. Formal: In the future, we'll add support for compound words and morphology. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C04",
        "filename": "C04-1042.json"
    },
    {
        "casual_text": "• Thinking about the ability to get a basic grasp of the language stuff.",
        "formal_text": "• Considering the capability to acquire basic understanding of the linguistic-level information.",
        "GPT2_formal_text": "ed is a matrix that helps predict the attention weights for the masked token. It's calculated using the cross-entropy between the hidden state h and the output tokens x. Formal: For each token y_i in the input sequence x, the attention weights W_y_i are calculated by summing over all the tokens in the sequence, like this: Formal: After that, the cross-entropy is adjusted by adding up the average attention weights for the tokens in the sequence, as well as the attention weights for the masked tokens. Formal: Here, the masked tokens x are calculated based on the input tokens x. Formal: The LSTM-based attention network is also trained using a bi-directional LSTM setup, and we call this model O_LSTM. Formal: O_LSTM takes as input the masked tokens x and the hidden states h_i, and outputs the output matrix H_l. Formal: Finally, we add the output matrix H_l to the output layer to get the final attention representation H_l. Formal: We'll get more specifics about the model setup and the model's architecture in the next section. Formal: To make sure the attention weights are learned consistently, we train the model using the cross-entropy loss function (from RNNLM) on the validation set. Formal: The loss function L_Z for the generated tokens is defined like this: Formal: Formal: We also use a linear layer to encode the tokens x, like this: Formal: Formal: Finally, we apply the attention weights W_x to the output layer, H_x, to get the final representation H_x. Formal: Formal: Here, x_i is the output token for the token in the sequence x. Formal: Formal: Formal: The model is trained using the cross-entropy loss function and a linear layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.137.json"
    },
    {
        "casual_text": "To check how well our optimization process works, we ran all three steps of it on all 13 datasets from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). Table 1 shows the labeled attachment scores with the default settings and after each of the three optimization phases, along with the difference between the final setup and the default. \n\nFirst off, the optimization boosts parsing accuracy for every language, no exceptions. The improvement ranges from about 1 percentage point for languages like Chinese, Japanese, and Swedish to 8-9 points for Dutch, Czech, and Turkish. For most languages, the biggest jump comes from feature selection in phase 3. However, for languages with lots of non-projective dependencies—like Czech, Dutch, and Slovene—phase 2 also makes a big difference, especially when it comes to choosing the right parsing algorithm.\n\nThe time it takes to run the optimization depends on the dataset size. It’s about half an hour for smaller sets but can take up to a day for really big ones, like the Czech dataset.",
        "formal_text": "In order to assess the usefulness and validity of the optimization procedure, we have run all three phases of the optimization on all the 13 data sets from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). Table 1 shows the labeled attachment scores with default settings and after each of the three optimization phases, as well as the difference between the final configuration and the default. 5 The first thing to note is that the optimization improves parsing accuracy for all languages without exception, although the amount of improvement varies considerably from about 1 percentage point for Chinese, Japanese and Swedish to 8-9 points for Dutch, Czech and Turkish. For most languages, the greatest improvement comes from feature selection in phase 3, but we also see sig-nificant improvement from phase 2 for languages with a substantial amount of non-projective dependencies, such as Czech, Dutch and Slovene, where the selection of parsing algorithm can be very important. The time needed to run the optimization varies from about half an hour for the smaller data sets to about one day for very large data sets like the one for Czech.",
        "GPT2_formal_text": "= {q_i, t_i}, where q_i = {q_i1, q_i2, ..., q_iT} is the i-th row in the query. Formal: Let's say D(q_i) is a matrix of the dimensions D_i×d. Formal: Also, we have this thing called the adjacency matrix, which is a matrix of the same size, D_i. Formal: In a discriminative model, we figure out the probability distribution p(q_i|D(q_i)×D_i). Formal: To make sure the model's parameters are accurate, we train it using the validation set to minimize this loss function called L_valid. Formal: To get the log probability, we use the validation set L_dev. Formal: A sentence in the source language is just a bunch of words, and the sentiment of a sentence is the highest probability of each word having the sentiment related to the question. Formal: If the query is short, it might not have a lot of keywords, which makes the search process slower. Formal: For a query that has multiple keywords, we look at how many keywords are in the query and see how much they affect the sentiment of the query. Formal: To get the log probability of the query, we calculate the probability of each query having a certain sentiment q_i by using the query and the query's label as features. Formal: Using the sentence length, we can find the average sentiment of the query using this formula: Formal: Formal: The top-N words are considered the query's main keywords. Formal: Formal: Formal: We use a beam search method to pick the best answer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E12",
        "filename": "E12-2012.json"
    },
    {
        "casual_text": "In this paper, we talk about how context plays a big role in figuring out if emotions and their causes are actually connected. To tackle this, we came up with a new task: figuring out if an emotion and its cause are linked in a given context. We made a dataset for this by manually labeling stuff and using negative sampling from the ECPE dataset. Plus, we created a prediction aggregation module (PAM) that’s not too heavy on the computer, so models can tweak their final answer based on the type of emotion-cause pair in a document. The experiments show that our PAM works well and can be used in different setups.",
        "formal_text": "In this paper, we articulate the importance of context in determining the causal relationships between emotions and their causes. To address this problem, we define a new task of determining whether or not an input emotion-cause pair has a causal relationship under a specific context. We construct a dataset for our task through manual annotation and negative sampling based on the ECPE dataset. Furthermore, we propose a prediction aggregation module (PAM) with low computational complexity, to enable the models to dynamically adjust the final prediction according to the type of emotion-cause pair contained in a document. Experiments demonstrate the effectiveness and generality of our proposed PAM.",
        "GPT2_formal_text": ", which helps the model focus on the important parts of each input and then spit out the representation you see in Figure 1. To avoid any messy or irrelevant stuff, we focus on the edges, or \"levels,\" of our graph. So, for the input layer, we add another hypergraph, which we call Level1. For each level, we create a unique vector called e_k, where e_k = (e_1, e_2, ..., e_k-1) represents the edges connecting these levels. Formal: To build this graph, we first map out all the edges (or levels) in the input layer using a simple one-layer neural network. Then, we tweak the mapping by tweaking the attention weights. The cool part is that our model doesn't need any extra parameters, unlike models using a BERT-based encoder-decoder setup. Formal: For the sentence layer, we get e_s by taking the cross-entropy of the output from each LSTM. Formal: Lastly, for the word-level attention, we just add up the word embeddings from each word layer. Formal: We show the graph we get for training and testing in Figure 2. Formal: The nodes in the graph represent different sentence or word representations. Formal: Next, we figure out the attention weights for the nodes in the graph, using the graph's representation, e_k. Formal: Finally, we put all the attention weights together to get the final representation for the input layer. Formal: Using this setup, we can train a model to generate text. Formal: After training, the text generated by the model is shown in Figure 3, along with its semantic representation. Formal: We use the evaluation results from the word-level attention to compare our text generation model to those based on BERT-based models. Formal: The first row in Figure 3 shows how well our model performs using different attention weights, but with the BERT-based model. Formal: Our text generation model is based on the training data we got from the teacher model, so it learns the connections between the input and output levels. Formal: To see how well the text generation model performs, we test it with different attention weights, and also calculate the performance of the teacher model. Formal: Finally, we calculate the performance of the BERT-based model on the test data. Formal: Here's the breakdown of the results",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.252.json"
    },
    {
        "casual_text": "So, take sentence (2a), where we use the comparative form \"taller\" to compare how tall two people are. Then there's (2b), which mentions a specific height, like 5 feet. And (2c) uses the basic form of the adjective \"tall,\" which kind of implies a comparison with some hidden standard. In degree-based semantics, these gradable adjectives are seen as two-part things that involve an entity and a degree (Cresswell, 1976). For example, (2b) would be analyzed as \"tall(john, 5 feet).\"",
        "formal_text": "For example, the sentence (2a), in which the comparative form taller of the gradable adjective tall is used, compares the degree of height between two persons. (2b) is an expression that includes a specific height, which is the numerical expression 5 feet. (2c) is a sentence using the positive form of the adjective, which can be regarded as representing a comparison with some implicit standard value. In degree-based semantics, such gradable adjectives are treated as two-place predicates that have entity and degree (Cresswell, 1976). For instance, (2b) is analyzed as tall(john, 5 feet),",
        "GPT2_formal_text": ", and the similarity between each pair of input and output is calculated using the standard cosine similarity. The summary of the whole collection is written as [i, j] = {α 1, . . . , α n }, where α i is the chance that the i-th and j-th word in the summary are related to the i-th and j-th input, respectively. Formal: In this paper, we introduce a new dataset that focuses on document-level question answering (DQA). DQA is about generating questions that can be answered by a database query. This is different from traditional summarization tasks where you just summarize the most important and relevant bits of text. In DQA, the answer is considered a question-answer pair if both questions and answers have the same entities. To make this happen, we came up with a new DQA dataset with 7,000 questions and 6,500 answers. Formal: We created the DQA dataset using an existing dataset called DQA-QA-6, which has 5,500 questions and 6,500 answers. Formal: For the QA and QA-QA datasets, we only used the top-k questions that scored over 0.8 in F1 score. Formal: We used the SciTE dataset (Nallapati et al., 2019) to train our QA model, which we trained using the Google News search dataset. We set the training epoch to 10 and the validation set to 0, and we didn't use any adversarial examples to test. Formal: We also followed the same process to train the Question Answering (QA) model for the QA-QA dataset. We used the top-k question-answer pair as the training set and checked how well it did using the F1 score. Formal: We ended up with 6,074 questions and 7,152 answers for our DQA dataset. Formal: To train the question-answer model, we used two separate Google News datasets. We picked these datasets from the top-50 results from Google News. Formal: We also trained the question-answer model using a modified version of DQA. Formal: We used the DQA dataset to test how well the question-answer model worked. Formal: For the QA dataset, we trained it using the F1 score for the top-50 questions. Formal: We also trained the question-answer",
        "directory": "acl",
        "filename": "2020.acl-srw.35.json"
    },
    {
        "casual_text": "Alright, so the experts gave us some tips on how to do this task really well. Here's what they said:\n\n1. **True Counter Speech**: This one:\n   - Uses a line from the speech it's responding to.\n   - Talks about a specific point or argument from that speech.\n   - Covers more stuff from the speech than the other options.\n   - Hits the points in the same order they come up in the speech.\n   - Talks about similar topics.\n   - Focuses on the main issue in the speech.\n\n2. **Elimination**: This means getting rid of the other options because they talked about things that weren't even in the original speech.\n\n3. **Guess and Other**: These are for when you have to just wing it and write something in your own words.\n\nSo, that's the gist of what the experts said. Pretty straightforward, right?",
        "formal_text": "Reasons analysis The explanations provided by the experts revealed several best-practices for this task, which we categorized as follows: The true counter speech quotes a phrase from the supporting speech; mentions a specific case or argument from the supporting speech; is more comprehensive and addresses more issues raised in the supporting speech than the other candidates; addresses those issues in the same order as they appear in the supporting speech; discusses similar issues; deals with the main issue raised in the supporting speech. Another reason was elimination -discarding the other candidates since they responded to issues or arguments which were not raised in the supporting speech. The last two categories were guess and other (which required writing a reason in free form language).",
        "GPT2_formal_text": "ed. We'll use an attention mechanism that focuses on the local context of the current word to calculate its cross-attention weight. Formal: Also, we use a max pooling layer with a dilation factor of 10 to grab both the context of the current word and the target word. Formal: We're using two encoder-decoder models here: one for decoding and the other for generating stuff. We train the generation model using the CNN-base model (which is a recurrent neural network) and the encoder-decoder model. Formal: To make things simpler, we skip the step where we start a CNN with word embeddings and use it to create the target word embedding. Formal: In the next part, we'll explain the model setup for our cross-lingual text classification task and how we train it. Formal: We also throw in a baseline model that just uses word embeddings for classification. Formal: We train the cross-lingual text classification model using the CNN-base model. Formal: We create the context embeddings for the target word by using the CNN-base model. Formal: The generation model is trained using the CNN-base model. Formal: We use the word embedding vector as input for the target word embedding. Formal: We generate the target word embedding by using the CNN-base model. Formal: The whole thing is done with a graph neural network (GNN) and a recurrent neural network (RNN) model. Formal: We train the cross-lingual text classification model using the CNN-base model. Formal: After that, we use the RNN model to generate the target word embedding. Formal: Here, W and b are the parameters of the RNN model, while h_t, g_t, and c are the hidden and hidden states of the RNN model. Formal: Here's how we train the cross-lingual text classification model with the CNN-base model. Formal: We use the CNN-base model to create the context embeddings for the target word. Formal: For text classification, we train the cross-lingual text classification model using the CNN-base model. Formal: Finally, we use the RNN model to generate the target word embedding. Formal: The whole thing is done with a graph neural network (GNN) and a recurrent neural network (RNN) model",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "We also trained the model on the 960-hour LibriSpeech dataset, as mentioned in Table 11. The tests show that our SpeechT5 model gets a big boost in performance, even without adding a language model (LM) on top. It works just as well, or even better, than wav2vec 2.0 when they do use an LM. To check how good the TTS results are, we used the NISQA-TTS tool for automatic evaluation. It's way easier and cheaper than using MOS or CMOS, which require people to listen and rate the audio. As you can see in Table 13, the version of...",
        "formal_text": "We also fine-tune the model on the 960 hours set of LibriSpeech, as reported in Table 11. Experiments show that the proposed SpeechT5 model achieves significant improvement even without LM fusion, and it performs comparable or even better than wav2vec 2. 0 with LM fusion. We use the automatic evaluation tool NISQA-TTS to verify the performance of TTS results here, because it is convenient and cheap compared with MOS and CMOS, which need to be evaluated by humans. As shown in Table 13, the variant of",
        "GPT2_formal_text": "= ([ET] as tf_idf) for e_i in e_s. except for e_t. self. set_token_feature_feature(ET, [ET] as tf_idf) for e_i in e_s. also, self. set_label_feature(ET, [ET] as tf_idf) for e_i in e_s. Table 4 has all the results from our experiments. Formal: We took the embeddings from both the context and the question and used them as input for a feed-forward neural network, which we then fed into a BiLSTM layer. Formal: To figure out the attention weights for both the context and the question, we started by using the embeddings of the question and context as input. Then, we combined them using a softmax layer to get the attention weights for both the context and the question. Formal: We fine-tuned the model for each task using a batch size of 128, a learning rate of 1e-4, and a batch size of 32. Formal: We did the same setup for all the experiments. Formal: The final output includes a series of hidden states for each token, with each state representing the average attention weight for that token. Formal: Formal: We trained the model using beam search using the Adam optimizer on an 8-core cluster. Formal: Each batch has an initial batch size of 32, a learning rate of 1e-4, and a batch size of 8. Formal: The final output includes a sequence of hidden states for each token. Formal: The model was trained for 100,000 iterations, with each iteration representing a window size of (1 + 3) tokens. Formal: The model was trained for 100,000 iterations, with each iteration representing a window size of (1 + 3) tokens. Formal: Formal: The model was trained for 100,000 iterations, with each iteration representing a window size of (1 + 3) tokens. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.393.json"
    },
    {
        "casual_text": "So, entity linking has its limits and can be kind of random. For instance, systems by Ferragina and Scaiella (2010) and Ratinov et al. (2011) both correctly link \"vitamin C\" but mess up with \"pineapple juice,\" linking it to just \"pineapple.\" The thing is, \"pineapple juice\" isn’t recognized as a beverage because it’s not important enough to have its own Wikipedia page. As Table 1 shows, Wikipedia tends to focus on well-known entities but misses out on less common or newer ones. For example, Wang et al. (2012) found there are over 900 active shoe brands, but only 82 are listed on Wikipedia. In areas like intelligence analysis or local search, non-Wikipedia entities are often super important. That’s why we’re looking at the problem of unlinkable noun phrases: when a noun phrase doesn’t link to Wikipedia, we need to figure out if it’s an entity and what its specific type is. It’s tricky because not all noun phrases are entities—like \"Some people,\" \"an addition,\" or \"nearly half.\" Plus, predicting semantic types is hard because there are so many different kinds of entities in regular text. In our experiments, we used the Freebase type system, which has over 1,000 different semantic types.",
        "formal_text": "Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al. , 2011) both link \"vitamin C\" correctly, but link \"pineapple juice\" to \"pineapple. \" \"Pineapple juice\" is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry. As Table 1 shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types. 1 (Wang et al. , 2012) notes that there are more than 900 different active shoe brands, but only 82 exist in Wikipedia. In scenarios such as intelligence analysis and local search, non-Wikipedia entities are often the most important. Hence, we introduce the unlinkable noun phrase problem: Given a noun phrase that does not link into Wikipedia, return whether it is an entity, as well its fine-grained semantic types. Deciding if a non-Wikipedia noun phrase is an entity is challenging because many of them are not entities (e. g. , \"Some people, \" \"an addition\" and \"nearly half\"). Predict-ing semantic types is a challenge because of the diversity of entity types in general text. In our experiments, we utilized the Freebase type system, which contains over 1, 000 semantic types.",
        "GPT2_formal_text": "* h = (2.0, 0.9, 0.3, 0.2) [, ] represents the attention distribution at the [, ] level. Equation (3) is the single-layer autoregressive model with L2 regularization. The parameters α i, j, k, and σ i, k are learned using the Adam optimizer (from Kingma and Ba, 2014). The student model is trained by tweaking the parameters θ and τ, respectively. Formal: We start by training a basic Seq2Seq model using a regular dictionary. For each input x i, the decoder turns it into a token y i that has the same length as x i. This token gets fed into the decoder at every step, and the decoder spits out the probability of the next token y i based on x i. Formal: Then, we use this updated model to generate the next token. Formal: Finally, we combine all the output tokens to get the final output sequence. Formal: We train a Seq2Seq model on the target dataset T, and we use that to generate the next token y i. Formal: In this part, we explain how we fine-tune the model using the target dataset T. We start by training the decoder to translate the input x i to the target vocabulary Y t using a regular dictionary. Then, we add a linear classifier with the label y i to the decoder to predict the next token y i. Formal: We fine-tune the model with the target vocabulary Y t using a dictionary created from the input x i. Finally, we use this fine-tuned model to generate the target sentence. Formal: Finally, we combine all the output tokens and the target sentence to get the final sentence. Formal: The goal of pretraining is to train a model that can generate the output sequence y i without any mistakes, meaning it can produce correct tokens without messing up the input. Formal: We train this model using the target dataset T. Formal: We train the model using T to generate the target sentence. Formal: We fine-tune the model with T to generate the target sentence. Formal: We fine-tune the model using T to generate the target sentence. Formal: We fine-tune the model using T to generate the target sentence. Formal: We fine-tune the model using T to generate the target",
        "directory": "D12",
        "filename": "D12-1082.json"
    },
    {
        "casual_text": "Our method works with any neural encoder-decoder setup. For this project, we used the big pre-trained BART model (Lewis et al., 2019) and tweaked it with our own technique. Testing it on real news articles showed that our approach improves on what's already out there. When we applied it to a synthetic domain, the BART model, after being fine-tuned with our weak supervision, became way more efficient with data. It even beat the best systems before by a lot, using just 0.4% of the training examples.",
        "formal_text": "Our approach is compatible with any neural encoder-decoder architectures. In this work, we use the large pre-trained BART model (Lewis et al. , 2019) and fine-tune with the proposed method. Experiments on real news articles show our approach achieves performance boosts over existing methods. When adapting to the previous synthetic domain, the BART model after fine-tuning with our weak supervisions becomes substantially more data efficient, and outperforms previous best-performing systems greatly using only 0. 4% training examples.",
        "GPT2_formal_text": "= {h1, h2, ..., hk} n1, n2, ..., nk, and the similarity matrix φ(h1, h2, ..., hk) is calculated based on that. Formal: To make sure the model isn't just memorizing things, we use something called cross-entropy loss on the predicted labels. This helps the model learn new features and improve its performance. Formal: Let's say we have a document d_s that has these entities e_s_i and their relationships r_i_t, and we want to predict their labels. Formal: To figure out the chances of a label h_t showing up in a document d_s, we train a classifier p_d using a dataset d_s, which has these labels. Formal: After training, we get a bunch of entity pairs like (e_s_i, r_i_t) up to (e_s_i_t, r_i_t+1). Then, we randomly pick one of these pairs from a distribution p_d(e_s_i, r_i_t) = 1/|e_s_i |, where 1/|e_s_i | is the chance of the entity e_s_i showing up in the document. Formal: We use two different losses to tweak our model. Formal: For the cross-entropy loss, we set a threshold α to keep the loss low. Formal: Also, for the entity pair prediction loss, we use a normalization parameter γ to control how much we focus on the labels. Formal: Lastly, for the relationship prediction loss, we use a normalization parameter λ to handle the number of possible relationships between entities. Formal: The basic model is trained using cross-entropy loss, and then we fine-tune it using two different losses. Formal: We also did some extra experiments and compared different methods for entity pair prediction. Formal: Here, k is the total number of entity pairs in a document, and z is the total number of relationships. Formal: We trained a neural network using the top-k entities and the top-p relationships from a dataset. The entity pair prediction loss, l_d, is calculated using the cross-entropy loss, l_c. Formal: If we want to see how well the model performs on the",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.510.json"
    },
    {
        "casual_text": "Lately, there's been some cool progress in understanding procedural text, like the work by Gupta and Durrett (2019b), Du et al. (2019), and Das et al. (2019). But, the current methods don't really handle the relationships between entities, actions, and locations in a structured way. Instead, most of them rely on basic rules about how entities change or use extra knowledge to make guesses. For instance, Gupta and Durrett (2019b) came up with a fancy neural network that keeps track of each entity's state and uses a CRF model to sum up how everything changes overall. They also added some common-sense rules to their model to help it out.\n\nWhile Das et al. (2019) did look at how entities and locations connect, there isn't a standard way to deal with these relationships yet. Plus, some key connections, like how entities interact with actions or with each other, are totally overlooked.",
        "formal_text": "Recent works on procedural text understanding have achieved remarkable progress Gupta and Durrett, 2019b; Du et al. , 2019; Das et al. , 2019; Gupta and Durrett, 2019a). However, the existing methods do not systematically model the relations among entities, actions, and locations. Instead, most methods either leverage inherent constraints on entity states or exploit external knowledge to make predictions. For example, Gupta and Durrett (2019b) propose a structural neural network to track each entity's hidden state and summarize the global state transitions with a CRF model. inject commonsense knowledge into a neural model with soft and hard constraints. Although Das et al. (2019) model the relation between entities and locations, there is no general framework to model the relations, and some important relations, such as entityaction and entity-entity relations, are ignored.",
        "GPT2_formal_text": "ed and attention_broadcast to the input, or it can be done by mixing and matching different sub-sequences. Here, δ(n) is the number of different types of sequence representations we're dealing with. Formal: We're working with a model that's already been trained on the same dataset but is tweaked to focus on specific aspects (like recipes). Formal: You can use the same input (x, y) and output (x, y) to train a new model. Formal: The model's parameters are a mix of the previous model's parameters and the combined inputs from its previous steps. Formal: Instead of the current model, the one we just trained can be used to create the input for the new model. Formal: We use the model's output to learn the parameters for the new model. Formal: For the next round of training, we just use the current model's parameters. Formal: To train the new model, we train it on the original dataset D_i. Formal: We train the model on the original dataset D_i, and then we fine-tune it using D_i. Formal: After the model is trained, we clean it up using the cleaned-up outputs from the D_i-trained model. Formal: Lastly, we use the results from the clean-up model to build the input for the new model. Formal: We take the model's parameters and use them to train the model using the original dataset D_i. Formal: Here, δ(n) represents the number of different types of sequence representations we're dealing with. Formal: In this paper, we talked about the first step in training a neural network, specifically a recurrent neural network (RNN). Formal: We train RNN using a sequence labeling task. Formal: We also suggested using a mix of different sequence representations to train the RNN. Formal: We tested this on three different datasets to see how it performs. Formal: We shared the results of this project, along with some ideas for future projects. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.396.json"
    },
    {
        "casual_text": "(iii) Using dialogue control functions and looking at syntactic features like tri-grams (which include an auxiliary verb, the word right after it, and the one after that) as (DCF, SS2).",
        "formal_text": "(iii) dialogue control functions and syntactic feature (tri−grams-auxiliary verb, right neighbour1, right neighbour2) as (DCF, SS2).",
        "GPT2_formal_text": "\" (u_i) and \"pos_ids\" (p_i) to figure out the hidden representations for the words in the question and answer. Formal: In this project, we came up with a way to identify question-answer pairs using only the context around the question and answer. Formal: To test this out, we created a question-answer pair dataset with 100 question-answer pairs and 1,000 for training. We looked at two ways to use the context around the question and answer to help us out: (1) using only the context around the question, which is called \"context-free\" (C_QA), or (2) using all the context around the question, which is called \"context-enhanced\" (C_CQA). Formal: In Section 2, we talk about the methods we used to create C_QA and C_CQA. We also introduce our approach to answer questions using both the context around the question and the context around the answer. Formal: Section 3 explains the results of our experiments, and then in Section 4, we share the results of our tests. Formal: Section 5 wraps things up with some concluding thoughts. Formal: In Section 6, we show how our method works and what we found out about it. Formal: In Section 7, we talk about what we plan to do next. Formal: In Section 8, we share the results of our experiments. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "codi",
        "filename": "2020.codi-1.2.json"
    },
    {
        "casual_text": "Check out our multi-view response selection model in Figure 2. Basically, the context and response are turned into semantic embeddings in two different views. Both views share the same word embeddings, which helps them exchange info. In the utterance sequence view, we use a TCNN to handle the utterance embeddings. For the word sequence view and the utterance sequence view, we have two separate Gated Recurrent Units (GRUs)—one deals with word embeddings and the other with utterance embeddings. The word-level GRU focuses on word dependencies, while the utterance-level GRU looks at semantic and discourse stuff. We calculate confidence scores for picking the response in both views separately. Finally, we tweak the model by minimizing this loss:",
        "formal_text": "Our multi-view response selection model is designed as shown in Figure 2. As we can see, the context and response are jointly represented as semantic embeddings in these two views. The underlying word embeddings are shared across the context and response in these two views. The complementary information of these two views is exchanged via the shared word embeddings. The utterance embeddings are modeled through a TCNN in the utterance sequence view. Two independent Gated Recurrent Units are used to model the word embeddings and utterance embeddings separately on word sequence view and utterance sequence view, the former of which captures dependencies in word level and the latter captures utterance-level semantic and discourse information. Confidences for selecting the response in these two views are calculated separately. We optimize the multi-view model by minimizing the following loss:",
        "GPT2_formal_text": "_tokens, which is basically a weighted average of those attention tokens.  \n\nWhat's even cooler is that we can use this weighted average method for multiple tasks, like document classification, for example. Formal: For a given query, we take the averaged loss for the t-th token from the model. Then, we throw in the weighted average of the hidden state vectors for the last token in the answer. Formal: Using the whole question and answer to create a hidden representation is called query-aware attention, which was introduced by Vinyals and Le in 2015. Formal: We use a hidden variable called h_s, which is part of the query, along with the attention weights for the last hidden state vector. Formal: If you have a question, we use an LSTM to turn it into a sequence of attention scores for all the tokens. Formal: For both the question and the answer, we combine the attention weights from both the query and the answer by passing them through a fully connected layer to get the final hidden representation for both. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "Alright, here's the deal: the best settings for these parameters are: the longest question can have 15 words, the CNN filter sizes are 2 and 3, there's 1 shared CNN layer, 2 shared Bi-LSTM layers, a hidden dimension of 1000, 4 attention heads, and both image and object feature dimensions are set to 2048. The image level feature has 100 spatial locations, while the object level feature has 36 objects. The bilinear pooling rank is 3, there are 8 bilinear attention maps, the model trains for 100 epochs, and the starting learning rate is 0.002. The Adamax optimizer, which was introduced by Kingma and Ba in 2014, is used to tweak the weights during training.",
        "formal_text": "Optimal hyper-parameters are set to: maximum no. of words in a question=15, CNN filter size={2, 3}, # of shared CNN layers=1, # of shared Bi-LSTM layers=2, hidden dimension =1000, # of attention heads=4, image level and object level feature dimension =2048, # of spatial location in image level feature =100, # of objects in object level feature=36, # of rank in bi-linear pooling=3, # of bilinear attention maps=8, # of epochs=100, initial learning rate=0. 002. dataset. Adamax optimizer (Kingma and Ba, 2014) is used to optimize the weights during training.",
        "GPT2_formal_text": "= {'e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10'}, permutations = {'e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10'}, and keys = {'k1', 'k2', 'k3', 'k4', 'k5', 'k6', 'k7', 'k8', 'k9', 'k10'} Formal: We’ve also included the average performance of different language pairs to give a more accurate idea of how each method performs. Formal: To get a better idea of how the performance changes depending on the permutation, we tested different combinations of the terms {'e1', 'e2', 'e3', 'e4', 'e5', 'e6', 'e7', 'e8', 'e9', 'e10'}, and the average performance for each combination was calculated. Formal: We tried out a few different setups with different numbers of terms, and the best one by far was {'k1', 'k2', 'k3', 'k4', 'k5', 'k6', 'k7', 'k8', 'k9', 'k10'}, which uses a combo of lexical and concept term pairs. Formal: Check out Table 3 for the average performance. Formal: We checked out how the performance changed with different numbers of terms. This tells us that the biggest jump in performance happens when we have k terms. Formal: We also compared the average performance of different permutations based on the values of the terms. Formal: We tested different numbers of terms and the results are in Table 4. Formal: To figure out how the concept term performance changes, we used two different metrics: Formal: We tested different values of terms and the results are in Table 5. Formal: We also compared the performance of different permutations based on the idea that a concept term should be close to the top-k terms, where the top-k terms are the ones that might have the concept term. Formal: To test how well the concept term performance works with a variable number of terms, we tested two different settings: Formal: We tested different numbers",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "Here's the informal version:\n\n\"Figure 4 shows the simulation error rates for each condition (remember, higher is better here). Polyjuice-surprise has the highest error rate, which means showing these counterfactuals would give users the most extra info if they were displayed.\"",
        "formal_text": "Figure 4: Simulation error rates per condition (higher the better). Polyjuice-surprise has the highest error rate, indicating these counterfactuals would add the most information to users if displayed.",
        "GPT2_formal_text": "ing is a matrix that takes a token and turns it into a hidden representation that's a multiple of d_i. Formal: The contextual attention layer, which is the first one in the encoder-decoder setup, helps the decoder focus on the important bits from the source sentence that relate to the context. Formal: The cross-attention layer uses an attention mechanism (like the one Vaswani et al. came up with in 2017) to handle the cross-modal information, which is a fancy way of saying it looks at both the token and the context to figure out the most important features for the generated text. Formal: The final attention layer, which is the last one in the encoder-decoder model, decides how much of the source text to pay attention to, and it's the same for both the token and the context. Formal: We tweak the normalization loss function to make sure the attention weights are just as important as the encoding ones. Formal: We use the cross-entropy loss function to calculate the normalization loss, which helps us balance the importance of the attention and the encoding loss. Formal: The cross-entropy loss is calculated like this: Formal: Here, θ_i is the squared error between the attention matrix (A_i) and the embedding matrix (E_i), where i goes from -1 to 1 and is in R^d. Formal: Then, the normalization loss is calculated like this: Formal: Finally, the output layer's hidden state is calculated like this: Formal: We use a linear-chain CRF (introduced by Chiang in 2016) to process the data, and for the loss function, we use a tanh function. Formal: We train both the source and target embeddings with a batch size of 32. Formal: Following Luong et al. (2017), we set the learning rate λ to 0.1. Formal: To use cross-modal features, we also use self-attention (Su et al. introduced in 2017) to get a contextual representation. Formal: The loss function we use is the cross-entropy loss. Formal: Finally, we add a linear-chain CRF to the decoder and use the output layer's hidden state as the output. Formal: For the source embedding embedding x_i, we use the original input x_i. Formal",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "From Figure 1a, we can see that the block n-gram repeats algorithm takes up a whopping 25% of the generation time. To cut down on this cost, we came up with a new GPU-based kernel (check out Algorithm 1) that uses parallel computing to make things more efficient. Here's what it does:\n\n1. It avoids moving data between the GPU and CPU, which helps get around the bottleneck caused by the PCIe bus interface.\n2. It scans n-grams in parallel. Instead of going through tokens one by one to find repeated n-grams, it can do this all at once using threads equal to the number of n-grams generated up to time step t. Plus, each sample in a batch can be handled at the same time using multiple thread-blocks.\n3. It uses GPU shared memory for quicker access to data.",
        "formal_text": "As observed in Figure 1a, the cost of block n-gram repeats algorithm is as high as 25% of generation time. To reduce the cost, a new GPU-based kernel (see Algorithm 1) is developed to leverage the power of parallel compute and achieves the following benefits: 1) avoiding data movement between GPU and CPU to alleviate the throughput bottleneck of PCIe bus interface. 2) scanning n-grams in parallel. Instead of sequentially scanning tokens for detecting repeated n-grams, they can be scanned in parallel using threads equal to the number of n-grams generated till the time step t. Furthermore, each sample in a batch can be processed in parallel using multiple thread-blocks. 3) using GPU shared memory for faster memory access.",
        "GPT2_formal_text": ", f_ent, m_ent). Formal: A key part of the update process is making sure the model isn't too focused on either the local or global context, which we talked about in Section 3. Formal: We check if there's a mismatch between the predicted response and the actual response by looking at the predicted probability and the actual likelihood of the response. Formal: We use three types of measures: surprisal, entrainment, and ROUGE-1 scores to check how well the model is doing. Formal: We also check how the model is doing by looking at the log probability of the current input and the correct label, as well as the predicted probability and the probability from the prediction process. Formal: Lastly, we look at the log likelihood of the actual response and the predicted one to get the model's prediction. Formal: We use a solid model and some helpful rules to make sure our results are reliable. Formal: We also run some experiments to see how our approach performs. Formal: The results we get can be useful for predicting the next response. Formal: Formal: If the current response isn't good enough, we make a correction to the loss by adjusting the actual likelihood. Formal: If the current response is good enough, we make a correction to the loss by adjusting the actual likelihood. Formal: Finally, we adjust the prediction probabilities from the prediction process to avoid overfitting. Formal: Formal: We use a solid model and some helpful rules to make sure our results are reliable. Formal: Formal: Formal: We use a solid model and some helpful rules to make sure our results are reliable. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-demo.26.json"
    },
    {
        "casual_text": "Alright, so here's the deal: You've got a question in English, Hindi, or even a mix of both, and there's an image that goes with it. The challenge is to use the image to figure out the right answer. Basically, you need to do some deep thinking about the picture to pick the correct response from all the options. In short:",
        "formal_text": "Problem Statement: Given a natural language question Q in English, Hindi or code-mixed and a correlative image I, the task is to perform a complex reasoning over the visual element of the image to provide an accurate natural language answerÂ from all the possible answers A. Mathematically:",
        "GPT2_formal_text": "ed_1d_concatenation(h, t, d) = (d * (1 + h/d)) / (1 + h)^2. Formal: (1 + h/d)^2 is just the attention pooling, and it's a bit different from the usual attention-pooling setup in a few ways: Formal: Alright, let's break this down in a simpler way.\n\n- **Attention_Pooling**: Basically, it's a way to handle the attention matrix A_t_i (t_i) for a specific query, like a_i.\n- **Attention_Masked**: A special thing called attention mask, which helps with labeling the output for a specific query.\n- **Attention_Pool**: The pooling part that handles the attention matrix A_t_i (t_i).\n- **Input_Pool**: The input vector for the input prediction at the end of the input sequence.\n- **Output_Pool**: The output vector for the output prediction at the end of the output sequence.\n- **Attention_Mask**: The attention mask for the current input.\n- **Attention_Pool**: The pooling part that handles the attention mask.\n- **Input_Matrix**: The input matrix.\n- **Output_Matrix**: The output matrix.\n- **Attention_Mask**: The attention mask for the current input.\n- **Attention_Pool**: The pooling part that handles the attention mask.\n- **Input_Matrix**: The input matrix.\n- **Output_Matrix**: The output matrix.\n\nSo, in short, it's all about how the attention matrix A_t_i (t_i) is being used in a specific query, and it's all tied together with these attention masks. Formal: Let's break this down in a more formal way: Formal: Let's say we have a query q_i, and we're trying to figure out how to predict the output of the sentence sequence S_t, which is part of the query. Formal: We're looking at how the query query q_i is being used to make predictions, and how those predictions are being used to label the output sequence S_t. Formal: We're also considering how the query q_i is being used to generate data, and that's",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "The graph-enhanced bi-directional attention layer is designed to capture the intricate connections between sentences and relation instances. It creates a more refined representation of relation instances by combining information from within a sentence and across different sentences. This setup includes three main parts: the S2R layer, the GCN layer, and the R2S layer.",
        "formal_text": "The graph-enhanced bi-directional attention layer aims to model the complex interactions between sentences and relation instances, which generates refined representation of relation instance by synthesizing both intra-sentence and inter-sentence information. This component consists of the S2R layer, the GCN layer, and the R2S layer.",
        "GPT2_formal_text": "_m is a tuple of unique tags, which basically tells us the full name of the entity in the gold data set. Formal: We also show that the log likelihood can be used to estimate the expected value. Formal: We suggest using the log likelihood to estimate the log probability of an entity type, φ(t) from the gold data set. This helps us understand the uncertainty in the predictions and reduces the chance of wrong predictions. Formal: We create three different classifiers for the entity type, φ(t), by using a set of features that are helpful for prediction. Formal: We use a single prediction for each entity type and combine these predictions to create predictions for all entity types in the training data. We use the label distribution, label s, to estimate the chance that the entity type we're looking at is correct. Formal: The predictions for the entity types in the training data are combined using a simple linear transformation. Formal: Formal: In this paper, we talk about the challenges of learning models for domain adaptation and explain how the log likelihood can be used to estimate the expected value for each label. Formal: We use the log likelihood to estimate the expected value for each label, which gives us an estimate of the expected value for all labels. Formal: Finally, we use a softmax classifier on this value to get a single prediction for each label. Formal: We train the model using supervised learning and the label probabilities we get from supervised learning are used to estimate the expected value for each label. Formal: Formal: We demonstrate that the log likelihood can be used as an estimate for the expected value for each label. Formal: Formal: In Section 5, we show that using the log likelihood can be a useful tool for learning models for domain adaptation. Formal: Formal: We also introduce a new way to measure uncertainty, called the label entropy, that makes use of the probability distribution we get from supervised learning. Formal: Formal: We use the label entropy to calculate the expected value for each label. Formal: Formal: Formal: Finally, we talk about the challenges of learning models for domain adaptation and explain how the log likelihood can be used to estimate the expected value for each label. Formal: Formal: Formal: We also introduce a new way to measure uncertainty, called the label entropy, that makes use of the probability distribution we get from supervised learning. Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.136.json"
    },
    {
        "casual_text": "We're using the imSitu dataset to create our image-to-verb model. This dataset is pretty cool—it has a bunch of images, each showing one of 504 different verbs. The images cover all sorts of semantic roles, as described by Yatskar et al. in 2016. Check out Figure 4 for some examples from the dataset. We've split the data into different parts: 379 classes for training, 29 for validation, and 96 for testing.",
        "formal_text": "Zero-shot with the imSitu dataset We build our image-to-verb model on the newly introduced imSitu dataset, which contains a diverse collection of images depicting one of 504 verbs. The images represent a variety of different semantic role labels (Yatskar et al. , 2016). Figure 4 shows examples from the dataset. We apply our attribute split to the dataset and are left with 379 training classes, 29 validation classes, and 96 test classes.",
        "GPT2_formal_text": "ed and attention_attention. Formal: The network includes a bi-directional GRU, a feedforward neural network, and a multi-head self-attention mechanism. Formal: We're looking at how well the model can predict user intent using the answer keys they provide. To find the best way to combine the information from the question, key, and answer, we're checking out how much attention the user pays to different parts of the knowledge graph. Formal: We're using the word representations from the training data (like x_t = (w_t1, ..., w_tM) for the word embeddings) along with the question representations (x_q = (w_q1, ..., w_qM) for the question embeddings). Formal: The network is set up with two GRUs, a feedforward neural network, and a multi-head self-attention mechanism. For the word embeddings, we're using the word2vec model from the news domain (thanks to Mikolov et al., 2013). Formal: The result is a bi-directional GRU with an attention mask layer, a single-layer feedforward neural network, and a multi-head self-attention mechanism. Formal: Check out Figure 1 for how the network is set up. Formal: For the answer keys, we look at the GRU hidden states as x_t = (w_t1, ..., w_tM) for the word embeddings and x_q = (w_q1, ..., w_qM) for the question embeddings. To predict the answer keys, we use a bi-directional GRU with an attention mask layer, a single-layer feedforward neural network, and a multi-head self-attention mechanism. Formal: The result is a bi-directional GRU with an attention mask layer, a single-layer feedforward neural network, and a multi-head self-attention mechanism. Formal: The network is set up with two GRUs, a feedforward neural network, and a multi-head self-attention mechanism. Formal: The result is a bi-directional GRU with an attention mask layer, a single-layer feedforward neural network, and a multi-head self-attention mechanism. Formal: We use the word representations from the training data (x_t = (w",
        "directory": "D17",
        "filename": "D17-1099.json"
    },
    {
        "casual_text": "Okay, so, in addition to the usual basic rules and glue rules, we’ve got this swap rule that lets us switch around nearby chunks of words, but not whole phrases that are nested inside each other. Instead of using XH as the starting point for these rules, we went with XP. This choice lets us stick the results of the swap rule into bigger, hierarchical phrases, which you can’t do with the results of the regular hierarchical rules in a shallow grammar. To keep things balanced, we also added a rule that lets us combine stuff in a straightforward way. \n\nNow, we can put a limit on how many words these XP and XH chunks can cover. With that length limit in place, you can’t just keep applying the rules from Equation (4) to build super-long sub-derivations.",
        "formal_text": "supplementary to the standard shallow initial rules and glue rules. The swap rule allows adjacent lexical phrases to be transposed, but not hierarchical phrases. Here, we could as well have used XH as the left-hand side of the rules. As we chose XP and thus allow for embedding of subderivations resulting from applications of the swap rule into hierarchical phrases, which is not possible with sub-derivations resulting from applications of hierarchical rules in a shallow grammar, we also include the monotonic concatenation rule for symmetry reasons. A constraint can again be applied to the number of terminals spanned by both XP and XH. With a length constraint, building sub-derivations of arbitrary length by applying the rules from Equation (4) is impossible.",
        "GPT2_formal_text": ") = list(shape(m))  \nFor n-grams, the output is an n-gram vector with 1s and 0s, following the format shown in Table 2. We calculated the average of these vectors using a method similar to the one described in Wu et al. (2020b). Formal: We then split the data into train, dev, and test sets using the split method from the original paper. Formal: The training set is about three times bigger than the dev set. This extra space helps us figure out the best split and the best value for the pruning parameter ρ. Formal: In our case, the training set has 16,000 sentences and the dev set has 6,000. Formal: The dev set gets split into training, dev, and test sets. Each set has the same number of sentences. Formal: On top of that, we also use the original split (T_1) for training. Formal: If you want more info, check out Wu et al. (2020b). Formal: Formal: The student model we train is fine-tuned on the training set using the objective function θ. Formal: The training loss L(θ) is calculated using Equation 1. Formal: After training, the student model gets a score of 0. Formal: We also calculate the cross-entropy loss L(θ) using Equation 1. Formal: Formal: Formal: Here, θ is the cross-entropy loss function. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2012.eamt-1.66.json"
    },
    {
        "casual_text": "Every entry has some text attached to it, which is usually the main part of the entry. We think of this text as the way to break down the noun phrase. If the noun phrase has more than one word, those words will show up in the text on their own or as part of other noun phrases.",
        "formal_text": "Each entry is associated with text, which is usually the main text of the entry. 2 We assume the text as the key to segmenting the noun phrase. If the noun phrase in question consists of more than one word, its constituents would appear in the text either freely or as part of other noun phrases.",
        "GPT2_formal_text": ". Formal: Sure, we can make things simpler by breaking it down into smaller parts. Formal: We call this paper Theorem 3.2. Formal: Basically, if you have a node N, it's considered an adjacency matrix M_N if and only if there's a part of M_N that's a product of N and M. Formal: Let's break this down in simpler terms. Imagine you have a graph G and a bunch of nodes. The adjacency matrix M_N(G) is just the sum of all the edges (edge types) from G to N, and M_N(G) is also the sum of all the edges (edge types) from N to G. Formal: So, M_N(G) is the product of M_N(G) for all G, and M_N(G) is also the product of M_N(G) for all N. Formal: For instance, if we have a graph G and a set of nodes v in G, the adjacency matrix M_N(G) is just the sum of all the edges (edge types) from G to V, and M_N(G) is the sum of all the edges (edge types) from V to G. Formal: Also, theorem 3.1 works the same way for adjacency matrices in graphs as it does for adjacency matrices in trees. Formal: From now on, we'll call the adjacency matrix M_N(G) the adjacency matrix for G, and we'll also call the adjacency matrix for N the adjacency matrix for N. Formal: We can simplify the adjacency matrix M_N(G) by just focusing on the adjacency matrix N. Formal: Here, N is just the number of nodes in G. Formal: In the next part, we'll look at how to use adjacency matrices with graphs to figure out how similar two words are. Formal: Lastly, we'll use some simple tools to find the best pair of words by looking at how similar their words are. Formal: Formal: For instance, in Figure 1, the words \"loved,\" \"lovely,\" and \"happen\" are pretty close to each other. Formal: If we take the best two words from each graph, we get the top two words.",
        "directory": "D11",
        "filename": "D11-1056.json"
    },
    {
        "casual_text": "There are a few ways to handle domain generalization. One approach is to train a separate model for each in-domain dataset. Then, when you're testing on out-of-domain data, you pick the in-domain dataset that’s most similar and use its model for predictions. Other methods, like the ones by Ghifary et al. (2015) and Muandet et al. (2013), focus on training a model to learn features that stay the same across different domains. They do this using stuff like multi-view autoencoders and mean map embedding techniques.",
        "formal_text": "Several methods for domain generalization exist. One of them is to train a model for each indomain dataset. When testing on out-of-domain, select the most correlated in-domain dataset and use that model for inference. Other works such as (Ghifary et al. , 2015; Muandet et al. , 2013), model is trained to learn a domain-invariant feature by using multi-view autoencoders and mean map embedding-based techniques.",
        "GPT2_formal_text": "\" (like \"c_t\") in the set E). We then create a similarity matrix M for each query-response pair by using the global average of the embeddings. Formal: To check if a pair of query and response is valid (valid_q and valid_r), we use a two-sided cross-validation approach (check out Figure 2 for the details). Formal: In this setup, we randomly select the embedding weights (h_u and h_v) for a random sample q_t from the set E. Then, we use a softmax function to predict the probability of the query response y_t using h_u and h_v, as shown in Equation (5). Formal: For example, the query is \"blue jeans\", and the response is \"blue jeans\". Formal: To train the model using the query and response pairs, we use cross-entropy loss to adjust the weights. Formal: Finally, to pick the best response, we calculate the log probability using the whole set of possible responses, y_t. Formal: In this paper, we use a two-tier approach: first, we use a general approach to create a query-response pair, and then we use a specific approach to generate the response. Formal: Formal: Here, we explain how to create a general query and a response using an attention mechanism. Formal: We also suggest a two-tier approach for generating responses. Formal: In the two-tier approach, we first focus on generating a general query and a specific response. Formal: Formal: Formal: Then, we generate the general query using the general approach, and the response using the specific approach. Formal: Formal: Finally, we pick the best response by using the two-tier approach. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-5826.json"
    },
    {
        "casual_text": "CNBC is gonna create and make these programs, and the folks from U.S. News are helping out with background info and research.",
        "formal_text": "The programs will be written and produced by CNBC, with background and research provided by staff from U. S. News",
        "GPT2_formal_text": "= {0, 0, 1, 2, ..., M}. It also adds the token representation h_t to the hidden state to make sure the attention doesn't get messed up by random stuff in the input. Formal: In a training example, the attention mask [0, 0, 1] has been set up to get rid of the word \"fight.\" Formal: We take the masked sequence x, use a multi-layer perceptron (MLP) (check out Figure 2b) to create a contextual representation h_t, and then use some attention weight matrix W_t. Formal: In the next part, we'll look at how to generate a sentence from this representation. Formal: Once the sentence is made, we connect it to the image using an embedding layer and calculate the F1 score for the sequence. The F1 score is calculated using this embedding layer: Formal: The attention weight matrix W_t is calculated like this: Formal: We get the attention weight vector for the token \"fight\" by using a multi-layer perceptron with L_t layers. Formal: We also get the attention representation h_t for the token \"fight\" by using a multi-layer perceptron with L_t layers. Formal: Finally, we calculate the sentence-level F1 score by taking the weighted sum of the sentence-level scores for all the tokens in the sentence. Formal: Formal: Using this model, we generate the sentence \"fighting\" from the representation of \"fight\" in the image. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "From the results we got, it's clear that our segmentation algorithm works way better on Choi's text collection compared to what was reported before (like Choi, 2000; Choi et al., 2001; Utiyama and Isahara, 2001). The time it takes to run our algorithm is about the same as other methods (basically O(T^2), where T is the number of sentences). Plus, our algorithm has a cool feature: it automatically figures out the best number of segments without needing us to decide it manually.",
        "formal_text": "From all the results obtained, we can conclude that our segmentation algorithm on Choi's text collection achieves significantly better results than the ones previously reported (Choi, 2000; Choi et al. , 2001; Utiyama and Isahara, 2001 ). The computational complexity of our algorithm is comparable to that of the other methods (namely 0 (1 2 ) where T is the number of sentences) 4. Finally, our algorithm has the advantage of automatically determining the optimal number of segments.",
        "GPT2_formal_text": "= { 'c-1', 'c-2', 'c-3', 'c-4', 'c-5', 'c-6', 'e-1', 'e-2', 'e-3', 'e-4', 'e-5', 'e-6' } for each pair of words { 'e-1'...'e-n' } in the input. Formal: Basically, we're looking at the scores for the classes in each group, but we're also considering the scores for all the words in that group, so we end up with a set of scores called SCORE_T. Formal: After that, we multiply the scores for each class by the sum of their scores from all the words in that group. Formal: Finally, we add up all those scores to get the final score, which is the weighted average of all the scores from the classes. Formal: Using all that, we can figure out the overall representation of the input, which we call the sequence representation. Formal: Alright, let’s break this down in a simpler way. Formal: Alright, so the sequence representation is made by taking the output from the classifier and adding it to the output of the input. Formal: Next, we use the classifier to create a score vector called f_ij for each word in the input. Formal: Then, we use that score vector to calculate the attention weight, α_ij, for each word in the input. Formal: After that, we combine this attention vector with the output from the classifier to get the final attention weight, α_ij. Formal: To make sure we’re comparing things fairly, we’ve set some weights for the classifiers, which we call w_v and w_o. Formal: For the importance scores, we’ve got f_p and f_q for the words with probability p, and f_a and f_b for the words with probability b, respectively. Formal: Now, for the attention weight, we’re using f_p for the words with probability p, and f_q for the words with probability b. Formal: Finally, we add f_a and f_b to the output of the classifier to get the final attention weight, α_ij. Formal: The final representation for the input is shown in Figure 1. Formal: The final",
        "directory": "E03",
        "filename": "E03-1058.json"
    },
    {
        "casual_text": "In this paper, we use some efficient algorithms for figuring out Probabilistic Context Free Grammar (PCFG) from a huge amount of speech transcripts—like millions of words. We show that you can actually learn grammar just from words and it works well, no matter how you start. To make sure our results are solid, we use two different algorithms for Variational Bayesian PCFG inference and tweak two algorithms that were originally made for Latent Dirichlet Allocation (LDA) topic models. The cool part is that all three algorithms handle big datasets and get better over time, ending up with similar accuracy in prediction and parsing.",
        "formal_text": "In this paper, we use scalable algorithms for Probabilistic Context Free Grammar (PCFG) inference to perform grammar induction from millions of words of speech transcripts, and show that grammar induction from words alone is both feasible and insensitive to initialization. To ensure the robustness of our results, we use two algorithms for Variational Bayesian PCFG inference, and adapt two algorithms that have been proposed for Latent Dirichlet Allocation (LDA) topic models. Most importantly, we find that the three algorithms that scale to large datasets improve steadily over training to about the same predictive probability and parsing performance.",
        "GPT2_formal_text": "_t = attention_mask * 3. [b] L2T transform (a): This is basically the attention that the embedding layer in the second encoder uses to focus on the first word in the sentence and then grab the attention vector from the attention mask. Formal: For each element in the input sequence, we calculate the attention vector using this formula: Formal: To get the vector representation for the whole input sequence, we multiply the embedding vector with the attention vector, like this: Formal: We also check if the vector representation is identical to the embedding vector from the first encoder. If it is, we calculate the attention vector using that. Formal: Finally, we use this vector representation to help the second encoder figure out the attention vector for the current word in the sentence. Formal: Formal: So, in short, this whole process is basically a way to get the attention vector for the whole input sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1003.json"
    },
    {
        "casual_text": "Tax diya gaya hai match ka.",
        "formal_text": "maetʃ kā kər diyā gəyā hai match of tax give-past has been 'Tax has been given/paid for the match'",
        "GPT2_formal_text": "= { u1, u2, ..., uK }, { entity_id1, entity_id2, ..., entity_idK }, { entity_id1, entity_id2, ..., entity_idL }, and { entity_id1, entity_id2, ..., entity_idL }. \n\nBasically, we're defining two types of features:\n\n1. **List features**: These are the actual features you see in the input text.\n2. **Text features**: These are the features that describe what the text is about.\n\nIn short, you can think of these features as a list of feature values, where each value is a specific feature description. Formal: Formal: A text with these features is called a query. Formal: The sentence that has the query is called a query sentence. Formal: The input text, or the query, is a sequence of feature values. Formal: Once we have these query sentences, we can use methods to find the answer. Formal: We can calculate the probability of a query sentence by looking at all the query sentences. Formal: We can also use a statistical method to get the answer. Formal: The probability of a query sentence depends on the query sentence, but it can also be influenced by other sentences in the sentence. Formal: We can look at how these different sentences interact and figure out the probability for the answer. Formal: The text with the answer is called the query text. Formal: The sentence with the answer is the query sentence. Formal: We can also use a statistical method to get the answer. Formal: The probability of a query sentence depends on the query sentence, but it can also be influenced by other sentences in the sentence. Formal: We can figure out the probability for the answer by looking at all the query sentences. Formal: We can also use a statistical method to get the answer. Formal: Formal: In this paper, we're focusing on two main things: (i) answering questions based on the text, and (ii) finding the answer to a query based on the text. Formal: Formal: Formal: We can also use a statistical method to get the answer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C12",
        "filename": "C12-1152.json"
    },
    {
        "casual_text": "When we're looking at how well a topic model is doing, we usually try to figure out how connected the words are within a topic. Once we have a way to measure how related two words are, we can define topic coherence as the average of these measurements for all the word pairs in the most likely words for that topic. Newman and his team (2010) used Pointwise Mutual Information (PMI) for this, based on how often words appear together in external text collections. Later, Mimno and his colleagues (2011) found that a tweaked version of PMI worked better when compared to what experts thought. AlSumait and his team (2009) figured out which topics were useless by seeing how different they were from the overall word distribution in the text. Fang and his team (2016a) decided to use the cosine similarity between word vectors to measure topic coherence. We liked this last method because it can be used in many different situations, so we used it to check how well tweet groups fit together. We looked at GloVe (Pennington and his team, 2014) and BERTweet (Nguyen and his team, 2020) word vectors, which were made using big Twitter text collections. To make things better and less jumpy, we followed Lau and Baldwin's (2016) idea of averaging the coherence scores for different numbers of top words (like 5, 10, 15, and 20).",
        "formal_text": "Measures of topic model coherence: The most common approach to evaluating topic model coherence is to identify the latent connection between topic words representing the topic. Once a function between two words is established, topic coherence can be defined as the (average) sum of the function values over all word pairs in the set of most probable words. Newman et al. (2010) use Pointwise Mutual Information (PMI) as the function of choice, employing co-occurrence statistics derived from external corpora. Mimno et al. (2011) subsequently showed that a modified version of PMI correlates better with expert annotators. AlSumait et al. (2009) identified junk topics by measuring the distance between topic distribution and corpus-wide distribution of words. Fang et al. (2016a) model topic coherence by setting the distance between two topic words to be the cosine similarity of their respective embedded vectors. Due to its generalisability potential we follow this latter approach to topic coherence to measure thematic coherence in tweet clusters. We consider GloVe (Pennington et al. , 2014) and BERTweet (Nguyen et al. , 2020) embeddings, derived from language models pretrained on large external Twitter corpora. To improve performance and reduce sensitivity to noise, we followed the work of Lau and Baldwin (2016), who consider the mean topic coherence over several topic cardinalities |W | ∈ {5, 10, 15, 20}.",
        "GPT2_formal_text": ". Formal: A = {h1, ..., hn} is just the embedding matrix for the question, and a_t = a_0, ..., a_t-1 is the embedding for the answer. Formal: For each pair of entities (h, e), we're looking at a number from 1 to the length of e and a_t, which is the embedding for the entity pair. Formal: We calculate how well the attention is distributed by looking at its dot product with the mean. For a given entity pair e, we measure its distance from the entity pair's embedding vector, which we call d_e(e) from e = e1, ..., eT, with d_e(e) = a_0, ..., a_t-1. Formal: We use this distance-based measure to adjust the attention. Formal: Here, e_s_t is the embedding vector for the query, while e_s_e is the embedding for the entity pair. Formal: Finally, we calculate the attention weights using the dot product and the mean, as shown in Figure 2. Formal: Here, the entity pair e is the one that gets the most attention, and the query is the other entity pair that gets less attention. Formal: Formal: Basically, the attention weight is calculated using the dot product and the mean, like this: Formal: Formal: Here, x_t_s is the embedding vector for the query, and x_t_s_e is the embedding for the entity pair. Formal: Formal: Formal: This setup means we use a similar approach to the one used for creating question-answer pairs in QA. Formal: Formal: Formal: We use the attention weights to adjust the attention. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.530.json"
    },
    {
        "casual_text": "We use the gated model and train it just like Schick and Schütze did in 2018. We also stick to the same character n-gram lengths as FastText.",
        "formal_text": "We make use of the gated model, training it just like Schick and Schütze (2018). The same character n-gram lengths are used as for FastText.",
        "GPT2_formal_text": "= [src src_id, src_class, class_id] | +---------+----------+----------+----------+ +----------+ | • Classifier | | | | +---------+----------+----------+ +----------+ | • Slope | | | | +---------+----------+----------+ +----------+ Formal: In this example, we're looking at a sentence that has n words and has the word 'apple'. The words in the sentence are labeled as (s1, s2, ..., sn). Formal: So, we're using the count of how often each word appears, and we're checking if it matches a specific pattern. We're also keeping track of the average length of the words. Formal: Here's what we found: Formal: • The LSTM model tends to give higher probabilities to words that appear less often. Formal: Also, the NER model tends to favor words that are less common. Formal: Finally, the word 'apple' gets a high probability if there's a lot of text about it, but if there's not much, it's usually a low-probability word. Formal: Formal: In the example sentence, we're checking if the word 'apple' matches the pattern 'apple = yes, no = no', which is part of the pattern we mentioned in Figure 1. Formal: The LSTM model gives higher probabilities to words that appear less often. Formal: Formal: Formal: The NER model gives high probabilities to words that are less common. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-6104.json"
    },
    {
        "casual_text": "Constantly giving safe answers can make a chatbot really boring, so it's important to try and avoid that when creating the learning algorithms. To fix this, we need a more creative model that can handle both relevance and variety—basically, it should understand how to mix things up while still staying on topic when dealing with the way people actually talk.",
        "formal_text": "Frequent occurrences of safe responses can dramatically reduce the attractiveness of a chat agent, which therefore should be avoided to the best extent possible when designing the learning algorithms. The pathway to achieve this purpose is to seek a more expressive model with better capacity that can take relevance and diversity (or informativeness) into account simultaneously when modelling the underlying distribution of human conversations.",
        "GPT2_formal_text": "_m and attention_mask_s, which are basically the hidden state and attention parameters for the task-specific attention mechanism. Formal: Just like a regular RCNN, we use a linear layer to figure out the attention parameters for this task-specific attention mechanism. Formal: Here's how it works: Formal: Unlike our earlier work, we take the hidden states from the first layer and use them to learn the attention parameters. Formal: We call the model we train after this step LTF. For more details, check out our paper. Formal: The average attention across the whole dataset is: Formal: Check out Figure 2 for the attention mechanism. It shows how the first layer gets updated to predict the attention parameters. Formal: When we train this model, we set the output dimension to the size of the input window. Formal: We use the same setup for the LTF model as in the RCNN, with just one linear layer, and we also keep the max operations to a minimum. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D17",
        "filename": "D17-1065.json"
    },
    {
        "casual_text": "Let's break down what MACHAMP can do. For regular tasks where you need to predict things at the token level, like part-of-speech tagging, MACHAMP uses a straightforward approach. It applies greedy decoding and pairs it with a softmax output layer, all based on the contextual embeddings it generates.",
        "formal_text": "We here describe the tasks MACHAMP supports. SEQ For traditional token-level sequence prediction tasks, like part-of-speech tagging. MACHAMP uses greedy decoding with a softmax output layer on the output of the contextual embeddings.",
        "GPT2_formal_text": "= {h_s, e_s, c_s, i_t, j_t}, where h_s, e_s, c_s, i_t, and j_t represent the source words, the target word, the context words, and the output tags for the target word, the context words, and the context sequence. Formal: The model is built using PyTorch (Paszke et al., 2019). Formal: Also, we use the source token and the target token's embedding as the inputs for the embeddings for the target and context sequences. Formal: To train the model, we fine-tune the two MLE baselines and then do a bunch of tests to check how well the model performs. Formal: For each training example, we take the input embedding e_t and turn it into a lower-dimensional vector h_s using a linear transformation. Then, we use those transformed vectors to get the target embedding e_t_i. We also do the same thing for the context embedding c_t_i. We evaluate the model using the accuracy (acc) of the predicted tags in the training examples. Formal: For more info, check out Appendix A. Formal: For the encoder-decoder model, we start by using a Transformer decoder (Vaswani et al., 2017) to get the source embeddings h_s and e_s. Then, we use a bidirectional GRU (Bi-GRU) as the generator to create the target embeddings e_t_i. Finally, we use a linear transformation to get the context embeddings c_t_i. Formal: The model is trained using self-supervised learning. Formal: We tested it on a dataset with 60 million sentence pairs. Formal: We tested it on a dataset with 100 million sentence pairs. Formal: We tested it on a dataset with 250 million sentence pairs. Formal: We tested it on a dataset with 500 million sentence pairs. Formal: We tested it on a dataset with 1.5 billion sentence pairs. Formal: We tested it on a dataset with 2.5 billion sentence pairs. Formal: We tested it on a dataset with 6.5 billion sentence pairs. Formal: We tested it on a dataset with 7.5 billion sentence pairs. Formal: We tested it on a dataset with",
        "directory": "eacl",
        "filename": "2021.eacl-demos.22.json"
    },
    {
        "casual_text": "We use a partial block adjacency matrix to simplify calculations and cut down on noise when dealing with extreme multi-label classification. The tests we ran show that our approach can hold its own against the best methods out there.",
        "formal_text": "• We utilize a partial block adjacency matrix to reduce calculation and noise for extreme multi-label classification. The experimental results show that our method is competitive with the state-of-the-art method.",
        "GPT2_formal_text": "= [s_mask, s_tokens], which basically means they're capturing the attention for different parts of the input sequence. Formal: 1: We use a matrix called C that's a square matrix with M rows and N columns, and it's got d dimensions. Formal: 2: We add a one-hot vector called u_j, which is a number between 0 and 1, and it's got d dimensions. Formal: 3: After that, we calculate the final hidden representation h_t using a softmax layer that's the same size as the input sequence x_t. Formal: 4: Finally, we use a max-pooling operation to get the final output. Formal: To make sure everything's consistent, we do two more steps. Formal: First, we calculate the attention weights for the tokens, called [wt_a, h_t]. Then, we do a linear projection to get the attention mask, [s_mask, s_tokens]. Formal: To get the final token representation, we just average all those attention weights together. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-srw.4.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text:\n\nMTP is similar to MTPC, MTS is similar to MTSC, and TT is similar to TTC. Making sure these were comparable was super important for this study—if they weren’t, any comparison of style or syntax would’ve been messed up. The corpora were put together carefully to make sure they were comparable. The design criteria included things like diatopic, diachronic, diasystematic, and domain stuff. All the translated texts are in British or American English as the source language and Spanish (from Spain) as the target language. Both the translated and non-translated corpora are about the same size.\n\nMTP is made up of biomedical translations done by professional translators—either in-house or freelancers working for certified companies in Europe. It’s a specialized reference corpus because it doesn’t include full documents, just bits and pieces from translation memory (TM) segments. The text types vary a lot, from research papers and clinical essays to textbooks, product descriptions, PILs, user guides, and instructions for surgical equipment.\n\nThe comparable non-translated biomedical Spanish corpus has a similar mix of text types and topics. It’s a bit of a mixed bag, including fragments and whole documents: TM segments from different sources than the ones used for MTP, a small diabetes-related corpus, and a custom virtual corpus that was put together to match MTP in terms of sub-domains, topics, level of specialization, and text types.",
        "formal_text": "• MTP is comparable to MTPC, MTS is comparable to MTSC and TT is comparable to TTC. Comparability was a crucial consideration for this study as otherwise any style or syntax comparison would have been compromised. Corpora were compiled in such a way that comparability was ensured. Design criteria comprise diatopic, diachronic, diasystematic and domain constraints. All translated texts have British or American English as the source language and peninsular Spanish as the target language. Both corpora of translated and nontranslated texts have roughly the same size. MTP is composed of biomedical translations performed by professional translators (in-house or freelancers working for certified translation companies in Europe). It is a specialised reference corpus as it does not contain whole documents, but fragments composed of the target language segments of translation memories (TMs). Text types range from research papers in journals to clinical essays, textbooks, product description and PILs, users' guides and instructions for surgical equipment. Its comparable corpus of non-translated biomedical Spanish includes a similar selection of text types and topics. It is a mixed corpus, as it contains fragments and whole documents: source language segments of TMs different from the ones used to compile the MTP, a small corpus of diabetes and an ad-hoc virtual corpus compiled to match MTP as regards sub-domains, topics, level of communicative specialisation and text types.",
        "GPT2_formal_text": "= [e1, e2, ..., en] Formal: Basically, the input and output t have the same length, which is d. The weight vector is calculated using a logistic regression model that’s been adjusted for different types of relationships (like ORGANIZATION and PERCENT). Formal: In this case, we’re only dealing with the ORGANIZATION relation because the co-occurrence pattern for ORGANIZATION is pretty rare. However, we still use the ORGANIZE relation type because it helps the model figure out the type of relationship between e1 and e2. Formal: For instance, the pattern that includes ORGANIZE as an ORGANIZATION relation is: Formal: We’re using a statistical model that’s been adjusted for ORGANIZATION and PERCENT, and it’s based on the Generalized Non-Negative Matrix. Formal: Using an ORGANIZATION relation type gives us a bigger representation for the relationship, which is helpful for the model. Formal: We’ll start by explaining the methods we’re using for both training and testing. Formal: Training: For training, we’re using the random forest algorithm with a weight vector that’s been adjusted for different relationships. Formal: For testing, we’re using the SVM classifier with a standard confidence level, and we’ve got some data points (x_t) with positive and negative labels. Formal: The total number of training examples (d) is shown in Table 1. Formal: To make sure our results are accurate, we’ve set the training and testing data sizes to 30 and 50, respectively. Formal: We’ll break down how we calculate the training losses, losses for negative class, and losses for positive class in the next sections. Formal: We’ll use the log loss for both training and testing in the next sections. Formal: We’ll also explain how we set the number of training examples to 20 and 30. Formal: For training, we’ll train our model with a batch size of 6, and for testing, we’ll train it with a batch size of 12. Formal: Finally, for the evaluation, we’ll use a validation set size of 10. Formal: Formal: Here’s what’s in Table 1",
        "directory": "amta",
        "filename": "2008.amta-papers.5.json"
    },
    {
        "casual_text": "We take a close look at how the latest unsupervised NMT systems perform on a bunch of real and made-up translation tasks.",
        "formal_text": "• We thoroughly evaluate the performance of state-of-the-art unsupervised NMT in numerous real and artificial translation tasks.",
        "GPT2_formal_text": "and an attention gate that turns the output of the input into a fixed-length vector. For our main tests, we set the max length to 300 and the batch size to 64. Each epoch, we keep updating the model's parameters with the updated weights until the validation loss is no longer greater than the validation loss. Formal: From the setup, we can see that our method works really well for aligning sentences in a wide variety of languages. As a bonus, our model can also handle other tasks that involve transferring knowledge between languages. Formal: For every language pair, we suggest using the CoRA model to train the alignment model. In the joint model, we use a fully connected layer to encode the input sentence, which has the same number of nodes as the context. Formal: For sentence alignment, we take the input sentence, which has the same number of nodes as the context, and turn it into a fixed-length vector. Formal: For word alignment, we encode the source word and the target word using an input token. Then, we apply a bi-linear transformation to combine the vectors of both words. Formal: We also use this joint model with a basic uniblock model to train our model. Formal: To make the word embedding more effective in understanding context, we first convert the context vectors into a fixed-length vector before applying the linear transformation. Formal: Finally, we pass the word embedding from the source sentence to the target sentence. Formal: To figure out how similar the target sentence is to the source sentence, we use the cosine similarity of the embedding vectors. Formal: If the similarity is low, the similarity between the target sentence and the source sentence is high. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2020.eamt-1.5.json"
    },
    {
        "casual_text": "Text-image matching is definitely the hardest part of our system. Even though we’re using a really advanced method to pair text with images, the results just aren’t great. To get a better idea of how well we could do if everything was perfect, we picked five topics for each language and manually matched the text and images ourselves. The results for these topics are in Tables 7 and 8. The tests show that if we had the perfect text-image matches, the quality of the summaries would improve a lot. This proves that visual info is super important for our system. In Figures 4 and 5, you can see an image along with the text descriptions we got using different methods. From this, we can say that the image captions really matter.",
        "formal_text": "Text-image matching is the toughest module for our framework. Although we use a state-of-the-art approach to match the text and images, the performance is far from satisfactory. To find a somewhat strong upper-bound of the task, we choose five topics for each language to manually label the text-image matching pairs. The MMS results on these topics are shown in Table 7 and Table 8. The experiments show that with the ground truth textimage matching result, the summary quality can be promoted to a considerable extent, which indicates visual information is crucial for MMS. An image and the corresponding texts obtained using different methods are given in Figure 4 an d Figure 5. We can conclude that the image caption",
        "GPT2_formal_text": "are pretty straightforward. In this part, we’ll break down how we set up the Φ, and then we’ll talk about how the methods we’re suggesting work better. Formal: First off, we’re going to dive into the training setup of these models. After that, we’ll talk about the methods we picked for the validation set, what they’re good at, and the conditions under which they’re better. Formal: To get a better understanding of the models, we’re testing them on a sample from the SemEval-2010 Task 8 dataset. We’ll compare the methods we’ve created to two others that use the same setup, each with a different setup. Formal: We’ll share results for these two methods, and then we’ll dive into the other two methods, which are based on the same approach but use different data structures. Formal: After that, we’ll look at what other methods are out there, and in the next section, we’ll talk about the challenges they’ve faced. Formal: For the next section, we’ll walk you through the evaluation setup of the models we’ve developed. After that, we’ll share some results from an experiment on the SemEval-2010 Task 8 dataset. Formal: Formal: The experiments we’re discussing in this paper are pretty interesting, and we’ll walk you through them in the next section. Formal: We’ll also share results from a similar experiment on the SemEval-2010 Task 8 dataset. Formal: Formal: We’ll share results for the SemEval-2010 Task 8 dataset, and in the next section, we’ll talk about the challenges they faced. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D17",
        "filename": "D17-1114.json"
    },
    {
        "casual_text": "Got it! Let's break this down in simpler terms:\n\nWe need to tweak the main part of the L T definition. So, let’s say M is a tree with some extra features, like (O, D, d), and u is just any random node in that tree. Now, for any well-formed formulas (wffs)…",
        "formal_text": "SThis is worth spelling out in detail to do is alter the base clause of the L T definition. So, let M = (O, D, d) be a feature structure decorated tree, and u be any node in O. Then for all wffs",
        "GPT2_formal_text": ". The hyperparameters h_m and h_n are the parameters that control how the attention mechanism works. Formal: In this part, we're suggesting a new approach called attention-based vector generation. It works by using a beam model to figure out attention weights. These weights are then combined with a mapping Φ to create the vector representation x. Formal: In this paper, we use a convolutional neural network (CNN) to generate text representations for different parts of the image, as shown in Figure 1. The image features are represented by a window, and we pass the output of the CNN through a fully connected layer to get a representation of the whole image. Formal: The attention mechanism (Figure 1a) is a deep neural network with multiple heads that looks at the last two columns of the input image (H1 and H2). The representations for the last two columns are used to calculate the attention weight. By doing this attention-based vector generation, the model can generate text that's not only related to the image but also reflects the image context. Formal: We tested our approach on three image captioning datasets: Wikipedia, Flickr30k, and Flickr150k. The results show that the method works well and could be useful for generating captions that are closer to the image. Formal: The captions generated by our approach are pretty close to the image and include the details that the model was trained on, which helps the model learn better representations. Formal: Formal: Finally, we've shown that our method can generate captions that are both relevant and coherent. Formal: Formal: For future work, we plan to create captions that are more relevant and are also more coherent. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E93",
        "filename": "E93-1004.json"
    },
    {
        "casual_text": "Since the dictionary is pretty small and the system is straightforward, it’s not too hard to set up or keep running, and it’s super fast. That’s why JICST decided to use it for their huge database of over 400,000 citations each year, saving them money too. Thanks to some improvements, the system can now handle both Japanese and Western languages. We even got an award for this work—the Japan Association of Information and Documentation’s Prize of Learning in 1980—and we’ve got a patent for it (Japan Patent Kokai Sho 55 (1980) -102074).",
        "formal_text": "As the dlotionary is small enough and lo~Io is simple, implementation and meintenanoe are relatively easy and the speed ls very hlgho JICST adopted this system for its information file production and services of more than 400, 000 citations per ye~T and save cOStS° By the development of the techniques, p~ocessiug of Japanese has become to be able to cope with western lan6uages, We were awarded for the work The Prize of Learning of Japan Association of Information and Documentation in 1980, and have applied patent (Japan Patent Koks£ Shows 55 (1980) -102074). ~K002 016038(~2~ ~K00~ 000. ~01® ~KO0. ~ O028S2®d2@ ~l(005 00. 1281~03@ ~K000 0138~@01@ ~K007 000437~01@ ~K008 03~90@01® ~1(000 025003~02~",
        "GPT2_formal_text": "= {1, 2, 3, 4, 5, 6}  \n\nFor each entity, the model gets a representation like this: Formal: For each entity, the model calculates the following: Formal: The entity embedding vector, which is basically the tf-idf representation of the entity. Formal: This vector is multiplied by the relevance score for the entity and then fed into the attention mechanism. Formal: The relevance score is calculated using this formula: Formal: Basically, the relevance score is the highest score the model can give for an entity's embedding. Formal: We think the relevance score is the most important factor for entity representation. Formal: After training the model, we calculate the relevance score for each entity using the SVM method (developed by Tse et al. in 2011). Formal: We also use a weight vector to decide if an entity is relevant or not. Formal: We then pick the entities with the highest relevance scores as the top candidates for our list of candidates. Formal: To figure out the relevance score for an entity, we calculate the relevance score for its embedding. Formal: Lastly, we take the relevance scores for each entity as the candidates for the final list. Formal: Formal: We also calculate the relevance scores for each entity using the SVM method (developed by Tse et al. in 2011). Formal: We then pick the entities with the highest relevance scores as the candidates for our list of candidates. Formal: Formal: Finally, we take the relevance scores for each entity as the candidates for our list of candidates. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C82",
        "filename": "C82-2004.json"
    },
    {
        "casual_text": "Step 3: Cross-project split. Check out the middle part of Figure 4. Projects are randomly assigned to training, validation, and test sets. This is separate from the time segments and in-project splits we did in steps 1 and 2.",
        "formal_text": "Step 3: cross-project split. See Figure 4 middle part. Projects are assigned to training, validation, and test sets randomly, which is orthogonal to the time segments and in-project splits in step 1 and 2.",
        "GPT2_formal_text": "ed = {[CLS], [MASK], [S], [SEP]}  \n\nFor the baselines, we tried out different transformer models like Transformer (from Vaswani et al., 2017), GPT-2 (Radford et al., 2019), and BERT-Large (Devlin et al., 2019). We also tried the basic setup with the original attention mechanism and an output layer. The results are in Table 3. As we mentioned in section 4.1, we’re the first to use this structured attention mechanism for text segmentation across different languages.  \n\nOne thing to keep in mind is that we’re comparing it to another approach that uses the GAN (like the one by Kim et al., 2018) for CNN-based text segmentation. This has the advantage of being more consistent and making it easier to compare models that use different encoders. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re the first to use structured attention for text segmentation across multiple languages. Formal: We’re",
        "directory": "acl",
        "filename": "2022.acl-long.339.json"
    },
    {
        "casual_text": "A bunch of awesome people helped out with designing the task, getting the data ready, and setting up the software. We want to give a shoutout to Dotan Dvir from Hebrew University of Jerusalem for taking charge of the UCCA annotation work. Dan Flickinger at Stanford University did some fresh gold-standard annotations for around 1,000 WSJ strings, which are part of the EDS evaluation graphs from 2020. Sebastian Schuster from Stanford gave us tips on how to turn the old-school PTB and OntoNotes treebank annotations into Universal Dependencies, version 2.x, using more up-to-date tokenization methods. Anna Nedoluzhko and Jiří Mírovský from Charles University in Prague improved the PTG annotation for LPPS data by adding stuff that was missing, like coreference. Milan Straka from the same university shared an updated version of his UDPipe parser and helped train morpho-syntactic parsing models for Czech, English, and German (for the MRP companion trees). Jayeol Chun from Brandeis University was super helpful with converting Chinese AMR annotations, preparing the Chinese morpho-syntactic companion trees, and getting companion alignments ready for the English AMR graphs.",
        "formal_text": "Several colleagues have assisted in designing the task and preparing its data and software resources. We thank Dotan Dvir (Hebrew University of Jerusalem) for leading the annotation efforts on UCCA. Dan Flickinger (Stanford University) created fresh gold-standard annotations of some 1, 000 WSJ strings, which form part of the EDS evaluation graphs in 2020. Sebastian Schuster (Stanford University) advised on how to convert the goldstandard syntactic annotations from the venerable PTB and OntoNotes treebanks to Universal Dependencies, version 2. x, using 'modern' tokenization. Anna Nedoluzhko and Jiří Mírovský (Charles University in Prague) enhanced the PTG annotation of LPPS data with previously missing items, most notably coreference. Milan Straka (Charles University in Prague) made available an enhanced version of his UDPipe parser and assisted in training Czech, English, and German morpho-syntacic parsing models (for the MRP companion trees). Jayeol Chun (Brandeis University) provided invaluable assistance in conversion of the Chinese AMR annotations, preparation of the Chinese morpho-syntactic companion trees, and provisioning of companion alignments for the English AMR graphs.",
        "GPT2_formal_text": "= [], [], and [ ]. Formal: If we can't find any sentence (let's call it x) in the source text, we'll just assume x doesn't exist. Formal: We can get rid of the extra stuff in the output sequence by treating all those extra bits as null values. Formal: We could, of course, use other methods for this too, but we think a more flexible way of handling things is the way to go. Formal: Also, the whole point of picking a language model is to help a QA system find a good answer, so that's what we're trying to optimize. Formal: Lastly, if we have a good, natural-sounding question (let's call it x), and we know where it comes from (the question) and what it's asking (the question and answer), we can use those info to figure out the best answer. Formal: Here, we're thinking of adding a part called NDCG (from Klein and Manning, 2005) to handle this whole problem of generating a natural-sounding question. Formal: Let's say we have a question in a text, and the answer is in another text. Formal: We can turn the question, the answer, and their context into a series of numbers to get a probability distribution over all possible answers. Formal: Here, we're trying to find a natural-sounding question and answer. Formal: We're checking if the answer matches the question and the context. Formal: We're using an existing method for this, which is described in Section 3.2. Formal: The whole process of generating the question and answer is laid out in Section 3.2. Formal: Here's the question-answer pair: [q1, q2] Formal: We're looking at all possible values of the question and answer, plus the context of the question and answer. Formal: The probability of the answer given the question and context is calculated using this formula: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "conll",
        "filename": "2020.conll-shared.1.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way:\n\nWe compared our approach to a few different methods, which you can check out in Table 1. At the top of the table, you'll find the Lead baseline and Oracle. For CNN/DM, the Lead baseline is Lead-3, and for XSum, it's Lead-1.\n\nIn the middle part of Table 1, there's a basic transformer-based summarizer that takes \"sentence representation + position encoding\" as input. There are also two variations of this: one without positional encoding and another that only uses positional encoding.\n\nAt the bottom, you'll see Shuffling (Grenander et al., 2019), which is a recent method aimed at reducing the lead bias in summarization. There's also Learned-Mixin (Clark et al., 2019), a general debiasing method designed for NLP tasks when you know the type of bias in your training data and have a bias-only model. In our case, the bias is lead bias, and the bias-only model is a transformer trained with just positional encoding.",
        "formal_text": "Baselines: We compare our proposal with various baselines (see Table 1 ). The top section of Table 1 presents Lead baseline and Oracle. For CNN/DM, lead baseline refers to Lead-3 and for XSum, it refers to Lead-1. The middle section of Table 1 contains the basic transfomer-based summarizer accepting \"sentence representation + position encoding\" as input, and its two variants, one without positional encoding, while the other with only positional encoding as input. The bottom section contains Shuffling (Grenander et al. , 2019), which is a method proposed lately for summarization lead bias demoting, and Learned-Mixin (Clark et al. , 2019), which is a general debiasing method proposed to deal with NLP tasks when the type of data bias in the training set is known and bias-only model is available. In our case, the data bias is lead bias and the bias-only model is the transformer trained with only positional encoding as input.",
        "GPT2_formal_text": "_vec, position_mask_vec, adjacency_mask_vec, and attention_map_vec. Formal: (1) Concatenation: Concatenation, as described by Klein and Manning in 2003, is a method where you take a sentence and break it into smaller pieces called sub-sentences. Formal: (2) Attention: Attention, which was introduced by Bahdanau et al. in 2014, takes a single sentence and creates a vector representation for it. Formal: (3) Curriculum learning: In this approach, you use a curriculum to train a model. Formal: (4) Self-attention: Self-attention was introduced by Bahdanau et al. in 2015. Formal: (5) Rewiring: Rewiring, also by Bahdanau et al. in 2015, involves processing a sequence of data in a new way. Formal: (6) Machine reading: This method uses a machine reading system to summarize the input. Formal: (7) Multi-view learning: Multi-view learning, also by Bahdanau et al. in 2015, involves training a model with multiple views. Formal: (8) Contrastive learning: Contrastive learning is another approach where you train a model using different examples. Formal: (9) Label projection: Label projection is a method where you project labels onto a different representation. Formal: (10) Distance-based sampling: Distance-based sampling was introduced by Jha and Liang in 2018. Formal: (11) Random walk: Random walk is a method that takes a random walk through a graph with nodes and labels. Formal: (12) Conditional random fields: Conditional random fields, as suggested by Klein et al. in 2003, are another way to handle data. Formal: (13) Code-switching: Code-switching, which was introduced by Akbik et al. in 2016, involves swapping the words in one language with the next. Formal: (14) Mixed classification: Mixed classification, also by Akbik et al. in 2016, involves learning a model that can perform both at the word and phrase levels. Formal: (15) Word-level classification: Word-level classification, also by Akbik et al. in 2016, is another approach that mixes the two tasks. Formal: (16) Hierarchical machine learning: Hierarchical machine learning, also by",
        "directory": "acl",
        "filename": "2021.acl-short.119.json"
    },
    {
        "casual_text": "Step 4: Get rid of sentences that are less than 10 words long, any duplicates, and ones that start and end with quotation marks.",
        "formal_text": "Step 4: prune sentences shorter than 10 words, duplicate sentences and those that begin and end with a quotation mark.",
        "GPT2_formal_text": ", label_ids, and their relatedness scores. This is done by running the query through the classifier, which gives us the probabilities of the correct labels: (1 - argmax) / argmax. Formal: We consider the query and its related sentences as a sequence of words, where the words are tokens. Formal: The value of α is used to figure out the loss. Formal: We evaluate how well the classifier does using a perplexity score. Formal: The classifier has three main parts: Sentence encoder, Label encoder, and Prediction encoder. Formal: The label encoder is a binary classifier that predicts whether a word is an entity or not. This prediction is based on the context of the whole sentence, which includes the surrounding words. Formal: The prediction encoder looks at the sentence context and the last entity word in the sentence. Formal: The prediction encoder picks the label with the highest probability, which is either the entity or the word we're trying to label. Formal: For a query-response pair, the prediction is made using the context and the last entity word. Formal: For instance, the predictions for `label_1` and `label_2` for `query1` and `query2` are: Formal: We use the entity label to make the prediction for the entity label. Formal: For instance, the prediction for `label_2` is: Formal: We use the entity label to make the prediction for the entity label. Formal: We use the entity label to make the prediction for the entity label. Formal: Formal: We use the entity label to make the prediction for the entity label. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D15",
        "filename": "D15-1220.json"
    },
    {
        "casual_text": "Plus, our setup has some cool new features: The phrases people use to point things out come from conversations between two people, and they might mention something more than once.",
        "formal_text": "In addition, our setup has several novel aspects: The referring utterances are the result of interactive dialogue between two participants and include subsequent references.",
        "GPT2_formal_text": "_c. Formal: The input and output lengths for the masked token, y_i, are n_i and n_l, respectively. The hidden state h_i for the token is h_i. The goal is to figure out how to create a sentiment lexicon that matches the input's sentiment. Formal: We focus on the word \"new\" in the sentence, which has a positive sentiment. We use a negative sentiment lexicon to get a sentiment lexicon, y_i, which is a positive one. Formal: Next, we use the decoder φ(x) to create an input, x. Here, x_i is a sentence, y_i is an image, and φ(x_i) is the input decoder. The encoder π(y) is an attention network with a hidden layer size of d_w. To figure out how to generate the sentiment for the input x, we use the attention network π(y). Formal: The generation probability for the attention-based model is basically the same as the one we calculated for the original model, but we add the label for the token y_i. Formal: Finally, we calculate the score for the generated sentiment lexicon y_i by subtracting the score from the input x. Formal: Formal: We use logistic regression and Gumbel-softmax to evaluate how well the model does. Formal: The parameters θ_i, t, ..., θ_m are adjusted by the loss function L_θ_i, t, ..., L_θ_m, t. Formal: The final loss function L_θ_m is the one we can find using the loss function L_θ_i, t, ..., L_θ_m, t. Formal: Formal: For the final classification layer, we use the softmax function, L_softmax, to calculate the final prediction. Formal: Formal: Formal: To figure out the label for the masked token y_i, we use the attention network π(y). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.353.json"
    },
    {
        "casual_text": "We can look at this connection in a few different ways. For every statement, can we figure out the right events it's talking about? That's what we call utterance-level alignment.",
        "formal_text": "We can evaluate such a correspondence in several ways. For each utterance, can we predict the correct events to which this utterance refers? This is the problem of utterance-level alignment.",
        "GPT2_formal_text": "= { q i, j }, next_sentence = { q i, j }, stop_sentence = { q i, j }, stop_word = { q i, j }, n_sentence_es = { n_sentence, e i, j }, n_sentence_es_max = { n_sentence, e i, j }, n_sentence_tos = { n_sentence, t i, j }, n_sentence_tos_max = { n_sentence, t i, j },\n\n1.5.2 How much of the generated sentences are actually correct?\n\nWe calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We use the classification score from the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by using the classification score on the validation set. Formal: We calculate the 'correct' scores for the sentences we generate by",
        "directory": "D14",
        "filename": "D14-1043.json"
    },
    {
        "casual_text": "One way to mix things up is by shuffling around phrases in a translated sentence. To get a rough idea of how many different ways you can rearrange these phrases, you can look at places where two-word sequences don’t match up. If a phrase is stuck between two of these mismatches, you can move it around however you want because doing so won’t mess up the matching ngrams, and that means it won’t lower the Bleu score.",
        "formal_text": "One way in which variation can be introduced is by permuting phrases within a hypothesis translation. A simple way of estimating a lower bound on the number of ways that phrases in a hypothesis translation can be reordered is to examine bigram mismatches. Phrases that are bracketed by these bigram mismatch sites can be freely permuted because reordering a hypothesis translation at these points will not reduce the number of matching ngrams and thus will not reduce the overall Bleu score.",
        "GPT2_formal_text": "ing, attention_combining, and attention_scaling. The loss function is figured out using the cross-entropy loss. Formal: We get rid of any words that don't appear in the training data by using TFIDF. Formal: We use a simple linear transformation called log(u) to make sure the output stays the same when we train it. Formal: To train our model, we train it on the same data, using the same set of target words. Formal: Once the training is done, we fine-tune it using a big dataset with a bunch of target words. Formal: We use beam search to figure out the right weights for the attention modules, which helps us get better results. Formal: For the decoder part, we start by creating a sequence of tokens using a binary search decoder. Formal: We fine-tune it on a target dataset with the same training data and the same number of tokens as the training set. Formal: The model we learn during this process is called the final decoder. Formal: We use beam search to figure out the weights for the decoder, which helps us get better results. Formal: After decoding, we use a linear transformation called log(u) to make sure the output stays the same. Formal: We fine-tune the final model on the target dataset and add it to our training set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E06",
        "filename": "E06-1032.json"
    },
    {
        "casual_text": "When training neural networks, words are usually turned into \"word vectors,\" which are kind of like spread-out representations of words. These word vectors can either be made beforehand or created specifically for a certain task. Even if you use a pre-trained word vector model, which is also made with a neural network, the training process can be super quick. Some recent cool work shows that you can process over 100 billion tokens in just one day on a single computer (Mikolov et al., 2013c). \n\nAnother perk of using a pre-trained word vector model is that it’s super flexible. You can use it later for all sorts of different tasks. Plus, there’s no real difference between a pre-trained word vector model and one made specifically for a task. So, it makes sense to use them together. \n\nIn this paper, we suggest doing domain adaptation using big pre-trained word vector models instead of just raw text. Basically, we’re adapting from a big pre-trained word vector model into one that’s specific to the task at hand. This way, we can use massive general domain (GD) corpora without slowing things down too much. We can also tweak richer GD word representations to fit into in-domain (ID) training.",
        "formal_text": "In neural network training, words are represented as distributed representations, so-called \"word vectors\", which can be pre-trained or trained with a specific task in mind. Although a pre-trained word vector model is also learned with a neural network, the training can be very fast. Recent optimized work shows learning word vectors can process more than 100 billion tokens in one day on a single machine (Mikolov et al. , 2013c). Another advantage of a pre-trained word vector model is its flexibility, as it can be used later for different task-specific models. Furthermore, the pre-trained and the task-specific word vector models have no functional difference. Accordingly, we think it is very natural to use them together. In this paper, we propose to perform domain adaptation from the large pre-trained word vector models instead of the raw text, i. e. adapting from large pre-trained word vector into the task-specific one. In this approach, we can make use of huge GD corpora with little speed overhead. We can also adapt the richer GD word representations into ID training.",
        "GPT2_formal_text": "= { ... } ) - set the learning rate θ to 1e-5. - return the final representation of the entity ē. Formal: We also need to include some extra info to help the model understand the context of the entity and its surroundings. This info is explained in the next section. Formal: The cross-entropy loss is a type of adversarial loss that's been around for a while and works well in supervised learning. Formal: To figure out the embeddings for the entity mention, we use this formula: Formal: We also use a sentence token embedding function called e_sent, which is calculated using the TAC-2008 dataset. Formal: The prediction for the target entity, which is the result of the entity mention detection process, is calculated based on these three embedding functions. Formal: In this task, the entity mention we're looking at is the word `e_m`. Formal: The context of `e_m` is found by using this formula: Formal: Here, `f_u` is the feature vector that the model learns from, and `s_u` is the hidden state from the current hidden layer of the neural network. Formal: The loss function we're dealing with here is the cross-entropy loss. Formal: Finally, the prediction for the target entity is calculated based on the softmax layer of the neural network. Formal: Formal: We're also trying out two different loss functions for the entity mention detection task. Formal: The first function is called the CRF loss, and it's defined as: Formal: The second function is called the LMLM loss, and it's defined as: Formal: Formal: In our setup, we're using the LMLM loss for the entity mention detection task. Formal: We've also created a dataset for entity mention detection, which we call the ACE dataset. Formal: The training data for this dataset comes from ACE, which is a big collection of text. Formal: For the cross-entropy loss, we're using a neural network called the deep neural network (DNN) with a hidden layer dimension of 512. Formal: For the sentence embedding function, we're using the word2vec model. Formal: Formal: We're also using the sentence embedding function for the entity mention detection task, and we're using the NER dataset. Formal: Formal",
        "directory": "C16",
        "filename": "C16-1131.json"
    },
    {
        "casual_text": "Sure! Here's the informal version:\n\nThe annotation files shared in the paper by Lee et al. (2021) were originally in Brat Annotation standoff format, which are the .ann files, along with their matching news articles in .txt format. These were converted into json format. Then, each sentence in the dataset was processed using the Stanford CoreNLP toolkit. This included breaking the text into sentences, splitting words, tagging parts of speech, identifying named entities, and doing dependency parsing to create dependency parse trees. \n\nFor the model input, we used a \"multichannel\" approach (as seen in \"1\" in Figure 5), which combines three parts. Let’s say W = w1, w2, ..., wn represents a sentence with n tokens, where wi is the i-th token.",
        "formal_text": "Input. The annotation files made public in (Lee et al. , 2021) were first converted from Brat Annotation standoff format (. ann files) along with their corresponding news articles (. txt files) to json format. Each sentence in the dataset was parsed using Stanford CoreNLP toolkit, including sentence splitting, tokenization, POS-tagging, NER-tagging, and dependency parsing to generate dependency parse trees. For input to the model, we adopt the \"multichannel\" strategy (shown in \"1\" in Figure 5 ) by concatenating three components listed below. Let W = w 1, w 2, . . . . . w n be a sentence of length n where w i is the i-th token:",
        "GPT2_formal_text": "and attention_length, where each represents the size of the query and the size of the token. The vector representations for the words in the query and the response are also created by combining these two vectors using a linear transformation. Formal: You can find the code and data right here: https://github.com/google-research/bert/blob/master/training.py Formal: For more info, check out [2]. Formal: In this paper, we suggest a new way to deal with word ambiguity. Instead of just using a set of fixed rules to figure out the words in a sentence, we also rely on a general model that can be updated based on specific examples. This approach is different from earlier methods like WordNet in that it makes things more flexible and adaptable, while still ensuring that the important details are kept. Formal: We tested this approach on the MIMIC-III dataset, which is pretty popular for analyzing human language. The results show that our method works really well and can help us better understand how language changes over time. Formal: In the future, we plan to expand this approach to other datasets and add more features, like the type of word, word type, and word shape. Formal: Since we’re mainly focusing on understanding how words are understood, we’re mainly looking at how they change over time. Formal: We also tried adding information about the size of the word’s vocabulary to the model. This helped us understand how the vocabulary expanded and how the system changed as a result. Formal: We think the results from this method could be really useful for other language studies in the future. Formal: For future work, we’re thinking of looking at different types of word senses and using the results from this specific type of word to help with classifying text. Formal: We also tried adding information about how the word’s structure changed over time to the model. This could help us understand how the system changed as a result. Formal: Lastly, we built a system that uses a mix of character and word representations to figure out the meaning of a word. The results show that this method works really well and can give us a better understanding of how words change over time. Formal: Another key thing we did was create a new system that uses a mix of character and word representations to find the meaning of a word. This system is more general, so it can handle more types of words and",
        "directory": "econlp",
        "filename": "2021.econlp-1.10.json"
    },
    {
        "casual_text": "The backward search works by coming up with the N best guesses one after the other, and you don’t have to decide on N beforehand. It’s way less complicated than the forward search.",
        "formal_text": "The backward search generates the N best hypotheses sequentially and there is no need to preset N. The complexity of the backward search is significantly less than that of the forward search.",
        "GPT2_formal_text": ", and the attention weights. It was noticed that the cost function for the discriminator for a group of documents, with a size of D, is made up of two parts: the cost for the attention weight matrix α and the cost for the discriminator's activation function, Φ. Formal: If the document has a keyphrase in it, we can use the feature representation α t to calculate the cost. But if the document has no keyphrases, we just go with the input. Formal: We can easily calculate the cost for this case. Formal: The cost is basically the sum of the cost for the document and the cost for the keyphrase. Formal: So, the cost for the document is just the sum of the costs for the keyphrases. Formal: For a document with m keyphrases, we get this cost: Formal: Also, the embedding for the document's keyphrase is a vector with d dimensions. Formal: Basically, the sentence embedding is calculated as the product of the sentence embedding and the document embedding, multiplied by the embedding for the keyphrase. Formal: You can see this whole process in Figure 2. Formal: The cost for the attention weights α t, given the input and the document, is calculated using the cross-entropy loss function. Formal: Lastly, we normalize the document embedding to get the attention weight. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C94",
        "filename": "C94-1032.json"
    },
    {
        "casual_text": "Given how tricky Thai spaces can be for the SBD task, we’re suggesting a word-based labeling method. Instead of trying to classify spaces, we treat Thai SBD as a word labeling problem. Basically, we see spaces as regular words and label each word as either SB (Sentence Boundary) or nSB (non-Sentence Boundary). Figure 2 shows a comparison between the space-based classification method and our word-based labeling approach. \n\nFor reference, Figure 1 gives an example of Thai text where the first sentence has two spaces, but there’s no space at the end of the sentence, marked by <eol>. \"eol\" just means end-of-line.",
        "formal_text": "In view of this complexity of spaces in Thai in light of the SBD task, we propose a word-based labelling approach which regards Thai SBD as a word labelling problem instead of a space classification problem. The approach treats the space as a normal word and labels each word as SB or nSB (non-Sentence Boundary). Figure 2 illustrates the space-based classification approach versus the wordbased labelling approach. Figure 1. Example of a written Thai text in which there are two space characters within the first sentence, but there is no space character at the end of the sentence, i. e. , at highlighted <eol>\". \"eol\" refers to end-of-line.",
        "GPT2_formal_text": "is a combination of these two pieces. We take a closer look at these attention-based embeddings in Section 5.3.2. Also, we use a term weighting method called softmax in Equation (12) to tweak the embeddings. Basically, it takes the softmax of the word embeddings for a given input x_i and the attention weights for the i-th token. Formal: For each token, we use two kinds of softmaxes to predict the token embedding. Formal: In Section 5.1, we explain how we encode the tokens in the output and how we pick the most important token, by giving it a weight. Formal: To get the representation of a token y_t, we first average the attention weights δt for the tokens in the reference text x_t. Then, we use a function f_t to pick the token embedding for y_t. Formal: This approach has three main parts: (i) averaging the attention weights δt for the tokens in the reference text, (ii) selecting the token embedding based on the function f_t, and (iii) creating a new representation using the updated text embedding. Formal: For training, we pick the token embedding for a given input x_i and the attention weights δt, which are the weights that go with the token embedding, to predict the token embedding y_t. Formal: For training, we pick the token embedding for a given input x_i and the attention weights δt, which are the weights that go with the token embedding, to predict the token embedding y_t. Formal: Here, we show how to calculate the attention weights for the token embedding y_t and the attention weights δt, using the new input x_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1031.json"
    },
    {
        "casual_text": "We adjust the final score so it always stays between 0 and 1. To handle nodes that don't match in both the alignment area and the alignment range, we tweak Equation (2) by dividing it by:",
        "formal_text": "We normalize the resulting score so that it always falls within the interval [0, 1]. To take into account unaligned nodes in both the alignment domain and the alignment range, we divide Equation (2) by:",
        "GPT2_formal_text": "(r) = {(r, u)}  \nk = 1 for i from 1 to N + m  \np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}  \n\np_i = {(u_i, r_i)}",
        "directory": "D09",
        "filename": "D09-1002.json"
    },
    {
        "casual_text": "To handle labels that aren't perfect because of distant supervision, we’re stepping away from the usual sequence labeling approach and coming up with a fresh prediction model. Instead of trying to label each individual token, we’re focusing on figuring out if two tokens next to each other belong to the same entity (or if they’re “broken”). The main idea here is that, even if distant supervision messes up the boundaries of an entity, the connections inside the entity usually stay intact and are less affected by noise. So, we’ve created a new tagging system called “Tie or Break” to make better use of this noisy data. \n\nOur new model, called AutoNER, works by first finding all possible entity spans by looking for these ties, and then figuring out the type of each entity. We’ve built a cool neural network architecture to do this. In our tests, AutoNER performed better than the Fuzzy CRF model, so it seems to be working well.",
        "formal_text": "To adapt to imperfect labels generated by distant supervision, we go beyond the traditional sequence labeling framework and propose a new prediction model. Specifically, instead of predicting the label of each single token, we propose to predict whether two adjacent tokens are tied in the same entity mention or not (i. e. , broken). The key motivation is that, even the boundaries of an entity mention are mismatched by distant supervision, most of its inner ties are not affected, and thus more robust to noise. Therefore, we design a new Tie or Break tagging scheme to better exploit the noisy distant supervision. Accordingly, we design a novel neural architecture that first forms all possible entity spans by detecting such ties, then identifies the entity type for each span. The new scheme and neural architecture form our new model, AutoNER, which proves to work better than the Fuzzy CRF model in our experiments.",
        "GPT2_formal_text": "or attention_inv_mask. Figure 4 shows how the attention weights are assigned for the input sequence for sentence (a). The dashed part shows the attention weights for the entire input sequence. Formal: We use the attention weights, e, to create a prediction. Formal: We figure out the attention weights, w(a), for the input sequence by averaging the attention weights for a batch of attention tokens. The attention weights are figured out based on this average. Formal: We also calculate the attention weights for the output sequence. Formal: For each input token, we calculate the attention weights, e(a), for the next token, e(b), using the weights from the previous token, e(a). Formal: We take the normalized attention weights, e(a), and the original attention weight, w(a), and plug them into a feed-forward neural network. The output, e(b), is calculated using these weights. Formal: The embedding term for the token i at position j is calculated by combining the embedding of token i at position j with the embedding of the token j at position j. Formal: For the sequence length k, we calculate the attention weights, e(a), for the input sequence at position k by averaging the attention weights from the current token, e(k), to the previous token, e(k-1), for a batch of attention tokens. The attention weights are then used to create the predicted sequence length, a. Formal: The sequence length distribution, a, is chosen based on these weighted attention weights. Formal: Finally, we calculate the attention weights for the output sequence, e(b). Formal: The input and output sequences are fed into a BiLSTM with a hidden state size of 512, which is trained to predict the next token, a. Formal: We also calculate the attention weights for the output sequence. Formal: Finally, we calculate the attention weights for the output sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1230.json"
    },
    {
        "casual_text": "It's calculated by adding up the info you get from each item. If you have two things, X and Y, you can use the probability of their score differences to figure it out.",
        "formal_text": "is computed by accumulating the information gained from each item. Given two subjects X and Y, one can use the probability distribution of score differences",
        "GPT2_formal_text": "= self.get_input_ids(x, y) self.calculate_transformed_values(i) # Transformed values are basically the average of the original values and the transformed ones. Formal: If we have a value x_i, the transform applied to it will be a matrix. Formal: You can make this even better by finding the best values for the Transformer layers in each layer. Formal: Basically, the higher the values, the more similar the values are to each other, and vice versa. Formal: We're suggesting a method where we take the best values for the Transformer layers in a given layer and combine them into one value. Formal: This method is more efficient and can handle more layers. Formal: The same idea applies to the values for the input vectors, where the higher the values, the more similar the vectors are to each other. Formal: We use a method called objective function approximations to solve this equation for a specific case. Formal: The values for the inputs are identified, and these are then used as inputs for the transformation model. Formal: Formal: The term \"linear\" here means that the transformed values are similar to the original ones. Formal: The values for the input vectors are identified, and these are then used as inputs for the transformation model. Formal: Formal: The transformed values for the input vectors are similar to the original ones. Formal: Formal: Formal: The transformed values for the input vectors are similar to the original ones. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.346.json"
    },
    {
        "casual_text": "We took three of our test sets—Text+Berg, Twitter, and Combilex—and split them up based on the length of the input strings. Then, we tested two types of models, PCRF-Seq2Seq and encoder-decoder neural models, on these smaller groups of data. \n\nAs you can see in Figures 2 and 3, there’s a clear pattern: PCRF-Seq2Seq holds up pretty well no matter how long the input strings are. On the other hand, the encoder-decoder models start to struggle a lot more as the sequences get longer, especially the ones without an attention mechanism. \n\nFor shorter sequences, though, things look a bit different. Standard encoder-decoder models actually do a little better than both their attention-based versions and PCRF-Seq2Seq in tasks like Twitter spelling correction (check out Figure 3) and G2P conversion. But this isn’t the case when we look at the full datasets, where they don’t perform as well. \n\nOn the Text+Berg data, all the models are pretty much on par when it comes to short sequences, as shown in Figure 2.",
        "formal_text": "We divided three of our test sets (Text+Berg, Twitter, and Combilex) by input string lengths and evaluated PCRF-Seq2Seq and encoder-decoder neural models on these subsets of the test data. As illustrated in Figures 2 and 3, we observe a consistent tendency: PCRF-Seq2Seq performs relatively robustly over input strings of different lengths, while the performance of the encoder-decoder models plummets more drastically with sequences becoming longer, in particular those without attention-mechanism. For shorter sequences, we observe that standard encoder-decoder models even slightly outperform their attention-based counterparts as well as PCRF-Seq2Seq on both the Twitter spelling correction task (Figure 3 ) and on G2P conversion, in contrast to their rather low performance on the full datasets. On the Text+Berg data, all systems achieve approximately equal WAC for short sequences (Figure 2).",
        "GPT2_formal_text": "and attention_pointer_mask. The attention_mask is like a regular mask that takes into account the sequence of attention values for each word. It's calculated using the attention mechanism. The attention_pointer_mask is a special type of attention that takes into account the positions of the nodes in the input, like a pointer to the next word. In the experiment, we set the attention dimensions to 100. Formal: For the rest of this paper, we'll call this model the LP-MATCH. Formal: Check out Figure 2 for the LP-MATCH setup. We run two separate training sessions: one for training the word vectors and the other for training the sentence vectors. During the first session, the model learns from the last 300 sentences of the source text, and during the second session, it learns from the first 300 sentences of the target text. Formal: For more details on the training setup, check out the paper by Yang et al. (2019b). Formal: For training the word vectors, we use a bi-directional LSTM with a hidden layer size of 1024. We set the window size to 10 and the learning rate to 0.001. Formal: We also use a non-linear masked language model (LM) to predict the target vocabulary. For the source text, we use a BERT model (Devlin et al., 2019) with a hidden layer size of 2048. The target vocabulary is set to 708 sentences. Formal: The sentence vector is trained using the multi-head attention mechanism with the word vector as the input. Formal: We train the LSTM model using the sentence vectors and the learned predictions. Formal: For the target vocabulary, we use a BERT model (Devlin et al., 2019) with the hidden layer size of 2048 and the learned predictions as the input. Formal: For the pre-training, we follow the approach from the last section. Formal: We fine-tune the model using the target vocabulary and the learned predictions. Formal: We use word embeddings that are 256d for the source text and the predicted target vocabulary. Formal: We also use the most common source word as the token embeddings. Formal: We perform word embeddings to predict the target vocabulary. We get the embeddings by using a flat-1 structured perceptron model with a learning rate of 0.0001, using a window size of 10. Formal: For more details",
        "directory": "C16",
        "filename": "C16-1160.json"
    },
    {
        "casual_text": "The main difference between our method and the old ones is that we don’t rely on guessing or testing the results of the rule compilation by hand. Instead, we use a more solid approach: we create finite-state transducers to model phonological rules that mimic how loanwords are adapted, and then we automatically calculate how well these rules work and tweak them to improve their coverage. The cool thing about using a computer is that we can instantly see how accurate the system is during each step of developing and tweaking the rules, which would be almost impossible to do with old-school, manual methods.",
        "formal_text": "What is different between our approach and previous ones is that we do not test or intuit the result of the rule compilation manually; rather, we utilize a more rigorous method: composing finite-state transducers to model phonological rules for mimicking the loanword adaptation processes, and automatically calculating coverage and adjusting the rules to maximize coverage. The benefit of the computational approach is obvious: we can immediately obtain the accuracy of the system output during each round of the rule development and modification process, which is nearly impossible when resorting to paper-andpencil approaches.",
        "GPT2_formal_text": "= [{'person' : u, 'date_type' : t, 'date_url' : t, 'time' : t}]; output_ids = [{'person' : u, 'date_type' : t, 'date_url' : t, 'time' : t}]; } Formal: Here's Figure 1: It shows a high-level overview of the whole process for adding a new event to the database. Formal: In this example, we're talking about the same event, but the time is different. Formal: In this example, we're talking about the same event, but the time is different. Formal: If a user clicks on a specific link in the webpage, the system looks for the current date in the database to find the event being discussed. Formal: If a user clicks on a specific link in the webpage, the system looks for the current date in the database to find the event being discussed. Formal: If a user clicks on a specific link in the webpage, the system looks for the current date in the database to find the event being discussed. Formal: Formal: The link for a specific event is identified using the \"time\" field in the database. Formal: If the user clicks on a link in the webpage, the system looks for the current date in the database to find the event being discussed. Formal: If the user clicks on a link in the webpage, the system looks for the current date in the database to find the event being discussed. Formal: Lastly, the system decides if the event should be added to the database. Formal: The system decides if the event should be added to the database. Formal: The system decides if the event should be added to the database. Formal: The system decides if the event should be added to the database. Formal: The system decides if the event should be added to the database. Formal: The system decides if the event should be added to the database. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1081.json"
    },
    {
        "casual_text": "Based on the research by Yin and colleagues (Yin et al., 2018b), we're using a BiLSTM network along with a self-attention mechanism to process the text before and after the zero pronoun. The self-attention mechanism helps us figure out how much importance each word in the surrounding text should get. This way, we can create a more detailed and effective representation of the zero pronoun.",
        "formal_text": "According to the work of Yin et al (Yin et al. , 2018b), we use BiLSTM network and self-attention mechanism to encode the preceding and following text of the zero pronoun. The purpose of using the self-attention mechanism is to obtain the attention weight distribution of the preceding and following word sequence. In this way, we can get the more powerful zero pronoun representations.",
        "GPT2_formal_text": "ed_4k + attention_masked_m + ... + attention_masked_m + m_4k = h(0) [0, 1, ..., m_4k, h(0)] (2) Here, H(0) is the hidden state of the input at position (i-1), and H(1) is the hidden state of the output at position (i-1). Formal: Alright, let’s break this down in simpler terms. The basic idea here is that we’re looking for the best possible attention score to compare with the embedding mask. This score is calculated using a softmax function on the hidden representation at each position. Basically, it’s a mix of the hidden states of the input tokens at each position and how they interact with the output tokens.\n\nThe cool part is that this softmax function works both for the input and output sides of a token. The attention score it gives to the output token is also influenced by the hidden state of the input token. This combined info helps guide the output into a better state, which leads to better attention. So, it’s like a smart way to optimize things. Formal: Using these two attention scores, we can figure out the attention score for the input token, H(i), which is determined by the output tokens H(1) and H(0). Formal: Using the softmax function, we can figure out the attention score for the input token, H(i), which is determined by the output tokens H(1) and H(0). Formal: We then calculate a cross-entropy loss L_i for the output tokens H(1) and H(0). Formal: Finally, we use a linear layer to adjust the attention distribution L_i and apply a softmax function to get the final attention score. Formal: Using this setup, we can get a binary cross-entropy loss L_i for each token, L_i, which is calculated by multiplying the cross-entropy loss between the output tokens H(i) and H(0). Formal: Finally, we use a linear layer to adjust the attention distribution L_i and apply a softmax function to get the final attention score. Formal: Formal: It’s pretty straightforward to implement, and the whole process of creating these attention scores is outlined in Algorithm 2. Formal: The output tokens H",
        "directory": "ccl",
        "filename": "2020.ccl-1.77.json"
    },
    {
        "casual_text": "This feature hits its max value of 1 when all the content words are translated with a perfect probability of 1. The \"functional words translation strength\" feature is what we're talking about here. The idea is that if the functional words around content words are aligned correctly, they'll likely stay aligned in parallel sentences too. This is because, from a dependency-syntactic perspective, functional words (like prepositions, determiners, articles, particles, etc.) usually connect with or influence nearby content words.\n\nTo get the dictionaries mentioned in this section, we ran GIZA++ on the JRC Acquis corpus. For pairs of source and target words, if they're not in the dictionary, we use a 0 to 1 normalized Levenshtein distance to assign a \"translation probability\" based on how similar the strings are. If the source and target words are similar enough (we set the threshold at 0.7 through experiments), we consider them translations.\n\nMathematically, if 〈 〉 is the highest-scoring pair of aligned functional words near (within ±3 words) the aligned pair of content words 〈 〉, | | is the count of the best alignment found by ( ), and (〈 〉) is the probability of the functional word pair from the dictionary, then...",
        "formal_text": "This feature has a maximum value of 1 if all content words from are translated in with the maximum probability of 1. ( ) is the \"functional words translation strength\" feature. The intuition is that functional words around content words aligned as in ture ( ), will also align for parallel and because of the fact that, from a dependencysyntactic point of view, functional words (prepositions, determiners, articles, particles, etc. ) are 7 To obtain the dictionaries mentioned throughout this section, we have applied GIZA++ on the JRC Acquis corpus (http: //langtech. jrc. it/JRC-Acquis. html). 8 For two source and target words, if the pair is not in the dictionary, we use a 0 to 1 normalized version of the Levenshtein distance in order to assign a \"translation probability\" based on string similarity alone. If the source and target words are similar above a certain threshold (experimentally set to 0. 7), we consider them to be translations. usually governed by or govern nearby content words. Mathematically, if 〈 〉 is the highest scored pair of aligned functional words near (in a window of ±3 words) the aligned pair of content words 〈 〉 from, | | is the cardinal of the best alignment as found by ( ) and (〈 〉) is the probability of the functional word pair from the dictionary, then",
        "GPT2_formal_text": "= entity_ids; Formal: To keep things simple, we usually skip the entity's exact name. Formal: The updated path ends up being a list of strings, where each string is a bunch of numbers and letters that represent the entity's attribute. Formal: Each string in the updated path gets turned into an embedding, where the last number is its value. Formal: Lastly, we add this updated path to our training set. Formal: But here's the thing: the actual data we get, or actualization set, might not match the value we want. Formal: So, we get rid of the actual data in the augmented training set. Formal: This is super important because if the original data is gone, we don't get a clear idea of what the new data should be like. Formal: After that, we use the actualization set we got from the training set to train the model using the augmented data. Formal: In this project, we tested how well this new method works for multi-label classification on two datasets—COPENHACK and JACANA. Formal: Turns out, the augmented training set did a better job of matching the original data compared to the other options. Formal: For future work, we're planning to look into different ways to enhance the model with the augmented training set. Formal: Formal: If we want to get better results, we can use more training examples to make the model more robust. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2012.eamt-1.37.json"
    },
    {
        "casual_text": "Okay, so for APPLY(α, j), let's say i is the active node. Since we added an edge to j during the transition, W c + 1 is just the same as W c. Now, we'll break down O c again.",
        "formal_text": "APPLY(α, j) Let i be the active node. Since an edge to j was created in the transition, W c + 1 = W c. We decompose O c again:",
        "GPT2_formal_text": "_x. This means the tokens in the input are put into a special context and only seen as the target tokens. The token representation we get from this is called e_t, which comes from the attention mechanism in the model. Formal: Instead of using the usual method of doing things one step at a time, we're sticking with the multi-label approach (as described by Lample et al., 2016) because it makes it easier to handle longer input sequences. Formal: The model setup for the x-axis shows the attention distribution. The row in the x-axis represents the probability distribution, while the column shows the actual labels in the dataset. Formal: The target embedding is the one that’s closest to the source token, and the source embedding is the one that’s farthest from the target token. Formal: For the cross-modal image classification task, we’ve got a dataset D_i where the target is x, the source is x', and the embedding is d_i. Formal: We start with a model M_i = M_i = 1 and train it on D_i using a batch size of 1. Formal: For the cross-modal text classification task, we’ve got a dataset D_i where the target is text, the source is text', and the embedding is text. Formal: We train the model M_i using D_i for 100,000 steps. Formal: After that, we create a new model M_i using M_i and set it up to predict the target and source token embeddings. Formal: Finally, we pick the model with the highest performance for the cross-modal task. Formal: We’re using two different methods for this. The first one is our standard multi-label classifier, but we’ve added a second one called the cross-modal target identifier. Formal: First, we pick the cross-modal target identifier D_t. Then, we use M_t to train the model M_i for 100,000 steps. Formal: After that, we train the model M_i using D_i for 100,000 steps. Formal: Lastly, we pick the model with the highest performance for the cross-modal text classification task. Formal: Formal: The cross-modal token identifier D_t is the same as the",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "To get a better understanding of this, we got two bilingual folks to re-label 100 examples each in both English and French, following our usual validation process. We picked these examples from two separate random groups of development data, so the annotators didn’t see the original English text for any of the French translations they worked on. Without any special training or warm-up, these annotators matched the original English labels 85% of the time on the English data and 83% of the time on the French translations. This shows that the general meaning stayed pretty consistent between the two languages. Since most sentences are pretty straightforward to translate, especially the ones made by the workers, it doesn’t seem like the translation process added much confusion.",
        "formal_text": "To quantify this observation, we recruit two bilingual annotators to re-annotate 100 examples each in both English and French following our standard validation procedure. The examples are drawn from two non-overlapping random subsets of the development data to prevent the annotators from seeing the source English text for any translated text they annotate. With no training or burn-in period, these annotators recover the English consensus label 85% of the time on the original English data and 83% of the time on the translated French, suggesting that the overall semantic relationship between the two languages has been preserved. As most sentences are relatively easy to translate, in particular the hypotheses generated by the workers, there seems to be little ambiguity added by the translator.",
        "GPT2_formal_text": "_t, for each value vi in the set of tuples {u_t, vi}. Formal: The ROUGE-1 score is calculated by dividing the average of the ROUGE-2 score for each entity mention e_t by the average of the ROUGE-1 score for the whole mention m. Formal: Here, w_ij represents the i-th word in the mention m, so w_ij = (w_ij * m). Formal: We noticed that the performance doesn't really change based on the length of the mention m. So, we decided to pick a window size of k to optimize the model using the average of the length of m mentions. Formal: To break it down, the KWMD training objective function, f_kw, basically looks like this: Formal: We calculate the embedding parameters using the exact same approach as in the original paper. Formal: We set k and the window size to 1 to test how well the model can handle longer mentions. Formal: We ran a bunch of experiments on a small dataset called GLUE, focusing on entity mentions with lengths longer than 2. Formal: To find the best embedding parameters, we use a simple method called sparse kernel normalization. We do this by minimizing the cross-entropy loss in a function called min_ent, where the loss is calculated for the embedding parameters. Formal: We got the embedding parameters for the mentions we collected by running the embedding of the mention on the GLUE benchmark dataset. Formal: We use the entity mention embeddings to calculate the parameters of the model. Formal: To check how well the model can handle the longer mentions, we calculate the mention-level F1 score for all the mentions in the dataset. Formal: We also calculate a sentence-level F1 score for each mention, focusing on the entity mentions in the question, which we collect using the GLUE benchmark dataset. Formal: We also calculate the mention-level F1 score for each entity mention in the question. Formal: We calculate the mention-level F1 score for each entity mention in the question. Formal: To test how well the model can handle longer mentions, we check the average of the F1 scores for the entity mentions in the question. Formal: Finally, we calculate the sentence-level F1 score for each entity mention in the question. Formal: The top-5",
        "directory": "D18",
        "filename": "D18-1269.json"
    },
    {
        "casual_text": "In our initial experiments, we wanted to see if the model could put together separate concepts it learned during training. We used a selection of adjective-noun and verb-noun pairs from Nikolaus et al. (2019) and tweaked their train, validation, and test sets. The pairs we looked at are listed in Table 7.",
        "formal_text": "In the first set of experiments, we investigate whether or not the model can combine known concepts disjointly observed during training. We adopt a subset of the pairs of adjectives and nouns, verbs and nouns defined by Nikolaus et al. (2019), and modify the proposed train, validation, and test sets. The examined pairs are presented in Table 7.",
        "GPT2_formal_text": ". Formal: You can grab all the data and code from this link: https://github.com/cpakmukherjee/Dist-1.1. Formal: These are the token labels from the original sentence: Formal: To make sure the model keeps track of the context of the token and the words around it, we use a Support Vector Machine (SVM) to figure out the representation for each token. Formal: The main idea behind the approach we're talking about here is that the attention weights from different layers should be similar. Formal: We also tested the model using a separate set of tokens and tokens from the same layer as the original input. This is kind of like what happened with (Lewis et al., 2016) and (Zhang et al., 2017), where they trained a neural network to spot the masked token and the context words. Formal: To see how well the model handles different types of contexts, we used two standard datasets that include both English and Chinese sentences. Formal: For the Chinese side, we used a dataset from Cai et al. (2019). For the English side, we used the training data from (Lewis et al., 2016) and the CNL-Disease dataset (Dinan et al., 2019). Formal: To measure how well the model's predictions match up with the correct labels, we used the standard accuracy of the mask and its labels. Formal: We also tried the model on another unlabeled dataset, but this time we only used the final version of the dataset, which we had labeled. Formal: To measure how different the attention weights of different layers are, we used the standard cosine similarity between the attention weights in different layers and the original input. Formal: Lastly, we tested the model on a test set with masked tokens and labeled tokens, which was used for NMT evaluation. Formal: You can find all the data and code for this model right here: https://github.com/cpakmukherjee/Dist-1.1. Formal: The results are in Table 4. Formal: Table 4 shows the results of the model using the \"mask\" token and the labeled input for the NMT test set, as well as the test set without masked tokens. Formal: For comparison, we also compared the model's results to the official NMT system using the English training data. Formal: The model did pretty well on the test set,",
        "directory": "acl",
        "filename": "2022.acl-srw.11.json"
    },
    {
        "casual_text": "Sure! Here's the informal version:\n\n1. https://github.com/yikangshen/PRPN  \n2. https://github.com/harvardnlp/urnng  \n3. https://github.com/davidswelt/dmvccm  \n4. https://github.com/DrDub/cclparser  \n5. https://github.com/iesl/diora",
        "formal_text": "https: //github. com/yikangshen/PRPN 2 https: //github. com/harvardnlp/urnng 3 https: //github. com/davidswelt/dmvccm 4 https: //github. com/DrDub/cclparser 5 https: //github. com/iesl/diora",
        "GPT2_formal_text": "are the first parts of the triplet of entity mentions in the query, and h_i and c_i are the last two parts. Formal: We're suggesting a method where we use context-dependent attention to handle the differences between the input and output embeddings in an entity pair. Formal: We've come up with a new way to handle this named entity recognition (NER) task, which is kind of like what Sutskever et al. (2014) did. They used context-dependent attention to generate a label for the entity mention x i. For instance, if x i is a medical entity mention and it's marked as \"clinical trial,\" the attention for its label (y j) can be broken down into two parts: the first part is the entity mention itself, and the second part is the context of the entity mention in the clinical trial. Formal: In the example shown in Figure 1, we have a clinical trial mention x i, which includes the entity mention x i and the name of the clinical trial. The input embeddings for this pair are x i and c i. We then use an attention-based model to figure out the label for the mention x i. Formal: Our approach is kind of like how Sutskever et al. (2014) introduced a denoising method for entity mention detection. But instead of using the entity mention x i, we're using its context to create a label for the mention x i. Formal: In this paper, we focus on the inter-sentence relation between two entities, like a direct relation or a causal relationship. Formal: The inter-sentence relation is handled by a graph network, which we call a graph embedding relation. For instance, if the graph embedding relation is a \"relation between two entities\" embedding, then the graph for the relation between x i and x j will be a pair of embeddings. Formal: In this example, the graph embedding relation between x i and x j is the connection between the entity mention x i and the clinical trial mention x j. Formal: Finally, the context embedding relation for x i and x j is just the connection between the entity mention x i and the clinical trial mention x j. Formal: In this paper, we're using a graph embedding relation to handle the inter-sentence relation between two entities, like a direct relation or a causal relationship. Formal: Here, the graph",
        "directory": "acl",
        "filename": "2020.acl-main.300.json"
    },
    {
        "casual_text": "So, if we take a sentence like \"John looked it up,\" it leads to this specific way of understanding it, where we focus on the particle meaning alone.",
        "formal_text": "An input sentence such as =John looked it up = thus produced the following unique derivation, which was interpreted with the particle reading only.",
        "GPT2_formal_text": "ed_copy_to_dim(s, d) = attention(s, d) - [attention(s, d) * attention(d)].\" Formal: The goal here is to create a softmax function that's a mix of two other softmaxes. Formal: The model can be built using the usual setup with linear layers and dropout (Srivastava et al., 2014). Formal: We use the open-source HuggingFace library to handle the image embeddings. Formal: We can also use a window size of 2 to make sure we get both the head and the tail entity embeddings. Formal: The pre-trained model we're talking about here is based on a technique called span-BERT (Yang et al., 2019). Formal: We take the first word of the input and turn it into a binary sequence using word2vec (Mikolov et al., 2013). Formal: We calculate the hidden state size for the log-likelihood of the token embedding using a bunch of dot-product matrices. Formal: For generating the final output embedding, we go through the input and output embeddings one by one. Formal: We apply a linear layer with a dropout rate of 0.3 to the output layer's hidden state. Formal: The embedding of the entity embedding can be calculated using the cross-entropy function. Formal: The sequence is picked based on how well it matches the output embedding, which is the highest score it gets. Formal: We keep the token embedding of the entity embedding fixed at {0, 1}. Formal: The token embedding can be learned using the embedding of the entity embedding. Formal: Finally, we calculate the hidden state size for the log-likelihood of the token embedding using a dot-product matrix. Formal: We apply a linear layer with a dropout rate of 0.3 to the output layer's hidden state. Formal: This process is repeated for each token, and then we get the final output embedding by applying the same linear layer with a dropout rate of 0.3. Formal: We keep the token embedding of the entity embedding fixed at {0, 1}. Formal: The entity embedding can be learned using the embedding of the entity embedding. Formal: The entity embedding can be learned using the embedding of",
        "directory": "A88",
        "filename": "A88-1034.json"
    },
    {
        "casual_text": "Following the work of Dubey and colleagues in 2013, we're trying to create a model that can handle pronouns in everyday text and also explain how long it takes to read certain parts. Another goal is for our model to make sense of the human preferences found in studies on psycholinguistics. To do this, we're building on pronoun resolution systems from NLP, like the one by Soon and others in 2001. We're using weighted features to represent the different factors that affect pronoun resolution. This helps us figure out how important each factor is and how they might work together.",
        "formal_text": "Inspired by Dubey et al. (2013), we aim for a model of pronoun resolution that can run on natural texts and explain reading times. A second aim for our model is that it can account for human preferences discovered in the psycholinguistic literature. Based on these criteria, we build a model inspired by NLP pronoun resolution systems (Soon et al. , 2001). The factors of influence on pronoun resolution are represented as weighted features. This provides a way to assess their relative importance and allows to study their interaction.",
        "GPT2_formal_text": ", POS_tokens, and POS_tokens_i. Just a heads-up, the input text to this process is a string of tokens, and the outputs are groups of tokens, each with an attribute and some attributes. For instance, the input text for the SRL task in the IWSLT 2014 Chinese-English test sets is \"把有抽取举行中文中文的挑战的查询\" (which means \"The English translation is also available\"). The output for the English output, e_e, is \"E1\". Formal: In a way, this work is like a jumping off point to figure out how to do something similar in other areas. Formal: For instance, if the same model is used for two languages, we can compare how well it translates text from one language to another. Formal: For instance, if the same model is used for two languages, we can compare how well it translates text from one language to another. Formal: For instance, if the same model is used for two languages, we can compare how well it translates text from one language to another. Formal: For instance, if the same model is used for two languages, we can compare how well it translates text from one language to another. Formal: For instance, if the same model is used for two languages, we can compare how well it translates text from one language to another. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E17",
        "filename": "E17-4006.json"
    },
    {
        "casual_text": "Cells with the same attribute names usually show similar ideas. We use these metrics to check how alike the cells are.",
        "formal_text": "Value cells under the same attribute names demonstrate similar concepts. We employ the following metrics to measure the cell similarity.",
        "GPT2_formal_text": "is used to calculate the attention scores, p_A_i and p_A_j. Then, we can figure out the attention weights for each aspect in the two representations. Formal: To make sure the generated representations are representative, we apply a resampling method based on the standard ROUGE-1 score, which measures how well a summary covers the whole document. We tested different methods for resampling using the WikiQA dataset, and the best one is called the dot-product resampling algorithm (Graves et al., 2013). To keep things simple, we’ll just focus on the dot-product resampling algorithm for our experiments. Formal: To check if the generated representations are representative, we calculate a precision and recall (P_ref and Q_ref) score for each aspect. After that, we compare the generated representations to the actual ground truth representation of the aspect. If the scores are close to 1, it means the generated representations are a good representation of that aspect. Formal: The precision and recall scores for each aspect are calculated using the formula below. Formal: For the test set, we calculate the precision and recall by subtracting the precision from the recall. Formal: To figure out the aspect representation P_a_i, we calculate the precision and recall using equation (6). Formal: From the results in Table 2, it’s clear that our method really boosts the performance of generating relevant responses for each aspect. Formal: For the test set, we see that our method helps reduce the variance in responses. Formal: From the results in Table 3, we see that our method works really well and improves the recall for each aspect. Formal: The results in Table 4 show that our method improves the accuracy of generating relevant responses for each aspect. Formal: For the test set, we can say that the results for each aspect are representative. Formal: From the results in Table 5, we can say that the results for each aspect are representative. Formal: In Table 6, we can say that the results for each aspect are representative. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C00",
        "filename": "C00-1025.json"
    },
    {
        "casual_text": "Basically, when we're figuring out how to index stuff, we start with the big picture—like general categories that don’t get too detailed (think small feature-trees). Then, we move on to the more specific stuff that adds extra details to those general ideas (like expanding those smaller trees through unification). Most of the time, this means we work from the main parts of the sentence to the details, like starting with the verb phrase (VP) and then looking at what comes after it. This is similar to what Proudian and Pollard talked about in HPSG back in 1985.",
        "formal_text": "Our general principle for indexing rules is that we must parse from categories that specify general information (ie. that have small feature-trees) to those that specify particular modifications of that general information (ie. that provide extensions to the smaller trees by unification). This usually means that we parse from syntactic heads to complements, eg indexing sentences on the vp (cf. HPSG [Proudian & Pollard 1985] ).",
        "GPT2_formal_text": "= {d_i, j} | d_i, j, and σ_i| is the sigmoid function applied to those inputs. The result is the set of embeddings {h_i, j} for each i from 1 to l, where l is the size of the lth layer. Formal: For our experiments, we use the same setup as Wu and Zhang (2019) and the same hyperparameters as Li et al. (2018). For the loss function, we're using the softmax layer from Scikit-learn (Pedregosa et al., 2011). Formal: The model we're using is a copy of the one from (Wu and Zhang, 2019). It has three layers, each with 1024 hidden units, and a hidden dimension of 256. Formal: Each of these layers has a batch size of 32, a dropout rate of 0.1, and an attention mechanism with a size of 2 × d_s = d_k. We trained the model for three epochs, using the Adam optimizer with a learning rate of 0.1 and a batch size of 32. Formal: Since the output representation h_i,j for a given input x_i and a sequence span y_i is in R^d, we calculate the loss by looking at this mapping. Formal: The goal of our approach is to reduce the cross-entropy loss. Formal: We tested this on the IWSLT 2019 task for two language pairs: German-English and English-French. Formal: We trained our model on a cluster of 2,000 parallel sentences from a corpus of around 4 million sentence pairs. Formal: We tested our model on the WMT 2019 task for German-English and English-French. Formal: The training data used for our experiments is available online. Formal: We also trained our model on a smaller chunk of the German WMT 2019 training data to see how it would perform. Formal: Formal: We included the development and test sets of the OpenNMT, which we trained on a cluster of 8,500 parallel sentences from the WMT 2019 corpus. Formal: This part gives you all the details on how we set up our experiments, including the code and hyperparameters. Formal: In the next section, we'll quickly go over the math behind our model and explain our attention mechanism. Formal: For the regularization term, we're",
        "directory": "A88",
        "filename": "A88-1029.json"
    },
    {
        "casual_text": "The dataset we end up with includes three pieces of information for each term: the original term we observed, one of the \"intended\" terms that the algorithm figured out, and how many times that intended term was meant. For a single term, it's normal to have several possible combinations because the algorithm decides based on the context.",
        "formal_text": "The resulting dataset consists of triples of the original observed term, one of the \"intended\" terms as determined by the above algorithm, and the number of times this term was intended. For a single term, it is possible (and common) to have multiple possible triples, due to the context-based assignment.",
        "GPT2_formal_text": "ed, and the final token embedding vector is h_m. This method can also be used to encode the entire input sentence. Formal: We add the model's hidden state to the list of vector embeddings we get from equation (3). Formal: To train this feature extractor, we use a linear layer to get the hidden state h_t and then use a softmax function to get the representation for the token y_t. Formal: Since the hidden state h_t and the output representation y_t are part of the same space, the feature extractor learns from these, giving us a straightforward way to optimize things. Formal: The full model for the unlabeled data is shown in Figure 1. Formal: So, the main idea behind this is that the token embedding vector h_t (which is basically the model's hidden state) should focus on the important parts of the input sentence. Formal: The best token embedding vector h_t for the unlabeled data is calculated by maximizing the log probability P(y_t | h_t, x_t) for the sentence y_t. The log probability P(y_t | h_t, x_t) is calculated as the log of the expected value of the token embedding vector h_t given the hidden state h_t. Formal: Formal: Formal: The goal is to maximize the expected value of the token embedding vector h_t, which we call P(y_t | h_t, x_t). Formal: To tackle these optimization problems, we came up with a new model called Attentive Attention Network (Attn). It has two main parts: a token representation embedding, h_t, and a memory cell, m_t, to remember the most important details of the input sentence. Formal: Formal: Formal: We built a model using the attention mechanism from a CNN to use these attention memory cells. Formal: Formal: Formal: To show how the model works, we put it all together in a figure. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1093.json"
    },
    {
        "casual_text": "We matched English word vectors with six other languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We used 300-dimensional fastText vectors that were trained on Wikipedia and Common Crawl. We made all the words lowercase, kept only the 200,000 most common ones, and did five rounds of Iterative Normalization to clean things up.",
        "formal_text": "We align English embeddings with six target languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We use 300-dimensional fastText vectors trained on Wikipedia and Common Crawl. We lowercase all words, only keep the 200K most frequent words, and apply five rounds of Iterative Normalization.",
        "GPT2_formal_text": "= {q i }, label_ids = {q i }, subject_ids = {s i }, class_ids = {c i }, pairs = {q j, q i }, summary = {s i, q j } }  \n\nBasically, each of these elements, q i, q j, s i, s j, and c i are used to represent the sentiment of the mentions in the text. Formal: You can grab this dataset, along with some extra annotations, from this link: https://github.com/Danni-Ross/sentiment-annotations Formal: It's all about sentiment classification for specific mentions in a news article. Formal: There are three main steps in this process: Formal: First, you take a list of mentions, like {q i, q j, s i, s j }, and figure out the sentiment for each one. Formal: Next, you dig into the text and identify the most relevant mentions, which we call topic-specific mentions. Formal: Finally, you use this related topic information to create a sentiment classifier. Formal: The final classifier helps predict the sentiment for all the mentions in the article. Formal: Finally, you calculate the mean and standard deviation of the predictions from this classifier. Formal: Formal: To sum it up, for each mention, you figure out its sentiment, which is usually expressed as a single number. Formal: Finally, you calculate the mean and standard deviation of the predictions from this classifier. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.201.json"
    },
    {
        "casual_text": "Formatting relies on how often words appear together in the text and uses medical terms from a computer dictionary.",
        "formal_text": "Formatting is based on cooccurrence patterns found in the text and on the lexical information obtained from the computer dictionary of medical vocabulary.",
        "GPT2_formal_text": "_t: 5.0, weight_mask_t: 5.0, hidden_t: 0.9, and e_t: 0.1. Formal: Here's the formula for the initial state h_t_i, which comes from Equation (1). Formal: We have M_t, which is the group of terms with i in T and t in R. Formal: In the method we talked about earlier, we get the embedding f_t by running it through a two-layer LSTM with a dropout rate of 0.1. Formal: After applying the tanh function to the embedding of the word w_i, we get the output vector y_t_i. Formal: We set the final output vector z_t = y_t_i by running it through an LSTM with an output layer of size N_T and an output dropout rate of 0.1. Formal: Formal: We use two feature maps, h_t_i and h_t_j, to show the embedding and the feature vectors. Formal: The two feature maps h_t_i and h_t_j are used for both the initial and final representations. Formal: The last output vector is a mix of the two embedding vectors. Formal: We put all these embedding vectors together into one single representation, h_t_i, and calculate the mean of that representation. Formal: To make sure our feature representations match up with the original word representations, we use a linear transformation. Formal: Here's how the algorithm works. Formal: The process is broken down into two steps: Formal: First, we create a matrix called D_t, which is a list of numbers showing the embedding and the words it represents. Formal: Then, we apply a linear transformation to the matrix D_t. Formal: We do this for both the initial representation and the final representation. Formal: In the first step, we make a matrix called D_t, which is a list of numbers showing the embedding and the words it represents. Formal: Then, we apply a linear transformation to the matrix D_t. Formal: After that, we calculate the mean of that matrix. Formal: Formal: Lastly, we use this process to build the final representation for the word, which is a mix of the two embedding vectors",
        "directory": "C82",
        "filename": "C82-1032.json"
    },
    {
        "casual_text": "2 Weight Poisoning Attack Framework  \n2.1 The \"Pre-train and Fine-tune\" Approach",
        "formal_text": "2 Weight Poisoning Attack Framework 2. 1 The \"Pre-train and Fine-tune\" Paradigm",
        "GPT2_formal_text": "= {'a', 'b', 'c', 'd', 'e', 'f'}; for (i=1; i < n; i++) { tokens = (tokens_id_tokens[i] + tokens_id_sentences[i])/2; } tokens = (tokens_id_tokens[1] + tokens_id_sentences[2])/2; } return tokens; } function token_sentence_cj(x, y) { // If x is a sentence, do something with it. if (x_sentence) { token_sentence_cj(x, y) } else if (x_sentence_cj) { token_sentence_cj(x, y) } else { token_sentence_cj(x, y) } } function token_sentence_lj(x, y) { // If x is a sentence, do something with it. if (x_sentence) { token_sentence_lj(x, y) } else if (x_sentence_lj) { token_sentence_lj(x, y) } else { token_sentence_lj(x, y) } } function token_sentence_rj(x, y) { // If x is a sentence, do something with it. if (x_sentence) { token_sentence_rj(x, y) } else if (x_sentence_rj) { token_sentence_rj(x, y) } else { token_sentence_rj(x, y) } } function token_sentence_sj(x, y) { // If x is a sentence, do something with it. if (x_sentence) { token_sentence_sj(x, y) } else if (x_sentence_sj) { token_sentence_sj(x, y) } else { token_sentence_sj(x, y) } } function token_sentence_ct(x, y) { // If x is a sentence, do something with it. if (x_sentence) { token_sentence_ct(x, y) } else if (x_sentence_ct) { token_sentence_ct(x",
        "directory": "acl",
        "filename": "2020.acl-main.249.json"
    },
    {
        "casual_text": "Himanshu Aggarwal and his team came up with a system in 2006 that uses Conditional Random Fields for Hindi. They used a morph analyzer to figure out root words and possible parts of speech (POS tags) for training. The system was tested on a corpus of 21,000 words with 27 different POS tags, and it got an accuracy of 82.67%.",
        "formal_text": "Himanshu Aggarwal et al. , 2006 developed a system using Conditional Random Fields for Hindi. A morph analyzer is used to provide information like root words and possible POS tags for training. The system was evaluated over a corpus of 21000 words with 27 different POS tags and system achieved the accuracy of 82. 67%.",
        "GPT2_formal_text": "ed_sum is a function that takes an input i and creates a hidden vector z_i using a hidden layer with dimensions d_i. The hidden layer has a dimension of d_T and includes a hidden dimension called h_T, which is the average of all the attention weights in the embedding layer. Formal: The attention weights h_T are spread out across the embedding layer. Formal: Next, we calculate the cross-entropy loss for each entity-pair pair (x_i, y_i) using the non-linear function δ. Formal: For each entity-pair pair (x_i, y_i), we calculate the cross-entropy loss using the non-linear function δ. We use the training dataset D_s for this. Formal: After running the validation set, we pick the best model based on the cross-entropy loss and the validation set. Formal: Formal: After running the validation set, we pick the best model based on the cross-entropy loss and the validation set. Formal: Formal: The final model is the one with the lowest cross-entropy loss and the highest validation loss. Formal: Formal: Formal: For each domain, we build a separate training dataset D_s by running the validation set for a subset of it. Formal: Formal: Formal: This leads to an overall performance loss L_s, calculated as Formal: Formal: Formal: Formal: The cross-entropy loss is also used to reduce the model's uncertainty. Formal: Formal: Formal: Formal: This helps prevent overfitting. Formal: Formal: Formal: Formal: We train our model for a fixed number of epochs τ, but the model can be easily fine-tuned for different tasks over time. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C12",
        "filename": "C12-3021.json"
    },
    {
        "casual_text": "The method for finding the right parts of the text involves flipping the usual way we create sentences and checking from the end to the beginning.",
        "formal_text": "The analysis procedure leading to the detection of acceptable segments was formulated as a reversal of the generative grammar and is performed by a right =o left scanning.",
        "GPT2_formal_text": ") = { ..., ..., { }, ...}  \n\nTable 1: How the subjectivity lexicon works. The main focus of the Lexical Concept Indexing system is the analysis of subjectivity, especially when it comes to specific terms that focus on different aspects of the lexicon. These terms are usually based on real-world associations or observations, not just on what we think or want. The main difference between the lexicon and its subjectivity lexicon is that the subjectivity lexicon focuses more on the core concepts of subjectivity, while the lexicon doesn’t really care about the specific details of the word. Formal: The subjectivity lexicon is made up of a bunch of different concepts. Formal: The main idea is that each concept can have multiple possible meanings. Formal: In the lexicon, the main focus is on the main concept, which has a clear meaning. Formal: In the subjectivity lexicon, the main focus is on the main concept. Formal: The main concept is basically the main idea behind the concept. Formal: A specific term (the object) can be described in a few different ways, depending on the concept. Formal: The main concept is just the main idea behind the concept. Formal: The main concept is also the main idea behind the term. Formal: The main concept can also be described in more than one way, depending on the term. Formal: The term (the object) is a specific description of the concept. Formal: The term (the object) can be described in more than one way, depending on the term. Formal: You can think of the term (the object) as a general description that covers all the concepts in the lexicon. Formal: Formal: In a paper called \"Sociolinguistics as a Science,\" [6] [7] [8] [9] [10] [11] [12] [13] [14] [15] [16] [17] Formal: We can also think of the concept (the object) as a specific description that covers the main concepts in the lexicon. Formal: Formal: Basically, we’re treating a term (the object) as a set of general descriptions. Formal: For example, in the subjectivity lexicon, the main focus is on the main concept. Formal: Formal: The main concept is also the main idea behind the term",
        "directory": "A83",
        "filename": "A83-1023.json"
    },
    {
        "casual_text": "When it comes to data structure, the connection between OC and OQ is kind of like the difference between paired data and two-sample data in statistical tests. With OC, we look at how the system sorts items into classes and create a confusion matrix using the actual and predicted classes. On the other hand, OQ is more about comparing how the system's distribution of items matches up with the actual distribution, without worrying about which specific item in one matches up with an item in the other.",
        "formal_text": "In terms of data structure, we observe that the relationship between OC and OQ are similar to that between paired data and two-sample data in statistical significance testing. In OC, we examine which item is classified by the system into which class, and build a confusion matrix of gold and estimated classes. In contrast, in OQ, we compare the system's distribution of items with the gold distribution, but we do not concern ourselves with which item in one distribution corresponds to which item in the other.",
        "GPT2_formal_text": "ed | 交给网络网 |中国文信息，我们觉得有着言级增加效果，因为从问题了需要的音计划，不过，我们依存在社区模型，最终的语言和社区拼命名应用的效果，这些语言是音计的语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语义语",
        "directory": "acl",
        "filename": "2021.acl-long.214.json"
    },
    {
        "casual_text": "We used the Kyoto Text Corpus as our annotated dataset. It had a total of 1,675,188 characters.",
        "formal_text": "As an annotated corpus, we used Kyoto Text Corpus. 6 It contained 1, 675, 188 characters.",
        "GPT2_formal_text": ". Formal: Basically, we take a regular NER model and tweak it a bit, but with a twist. The main difference is that we use a special token called `entity_token` to handle non-entity tokens. The entity_token helps us grab more info from the unlabeled data, giving us the background info for those entities. Formal: To make things easier and avoid extra losses, we limit the number of entity types to 3, following the approach from Bahdanau et al. (2014). Formal: We treat the whole setup as a sequence labeling task and skip the label projection step. Formal: We also use a dictionary to find pairs of entities, like (e1, e2) in our case. We use a deterministic learning approach to figure out the label projection for the entity pair (e1, e2). Formal: For instance, for entity e1, the label projection function is: Formal: We do label projection on each entity pair in the dictionary using the set of operations we're working with. For example, for `e1, e2`, the mapping is `{op1, op2}, (e1, e2)`, where each operation (op1, op2) is a list of operations from a set `OP`. Formal: The `op` values are calculated using a function `op1(e1, e2)`, where `op1` is the operation that maps entity `e1` to `e2`. Formal: We also have a pairwise projection function `op2(e1, e2)`, where `op2` is the operation that maps entity `e1` to `e2`. Formal: Each operation `op` in `OP` has a weight based on how likely it is to be correct, so `op2` is a simple linear function. Formal: For all these operations, we use the lexicalized embeddings from the lexicalized vector model (Levy and Goldberg, 2013). Formal: Formal: Finally, we use a softmax function to calculate the probability of the entity pair (e1, e2) given a sequence of entity tokens (e1, e2). Formal: In this project, we're focusing on the input sentence, which we write as `s = (e1, e2)`. Formal: We can also represent the entity pairs as vectors in a specific format,",
        "directory": "D11",
        "filename": "D11-1056.json"
    },
    {
        "casual_text": "Okay, so we're working with the ACE 2004-NWIRE dataset, which is basically the newswire part of the ACE 2004 corpus. This dataset has been used by researchers like Poon and Domingos (2008) and Haghighi and Klein (2009). For our development phase, we focused on the first corpus, called ACE2004-ROTH-DEV. The rest of the corpora are kept aside for testing later.\n\nWe used the Stanford parser (developed by Klein and Manning in 2003) to analyze all the documents. This parser helps us figure out the syntactic structure, which we then use to pinpoint the main words in mentions and to decide the order of these mentions within a sentence. We'll go into more detail about this in the next section.\n\nTo make sure our results are comparable to earlier work, we didn't use the perfect, hand-labeled named entity tags or mention types. Instead, we relied on the labels given by the Stanford NER tool (created by Finkel et al. in 2005).",
        "formal_text": "• ACE2004-NWIRE -the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al. , 2005).",
        "GPT2_formal_text": "_layers = [sigmoid(X * Y) for i from 1 to M] # normalization LayerAttention. Formal: In Section 2, we talk about how to choose the right length and give a quick overview of LDA and lattice-LDA. Formal: Here, p_t_l(w) is the probability of word w appearing in the document. Similarly, p_t_l(w1, w2) is the probability of word w1 appearing in document d. And p_t_l(w3, w4) is the probability of word w3 appearing in document d. Formal: It turns out that the entropy of a document d is a lot smaller when there are more documents in that area. So, this idea applies to any document. Formal: The algorithm we talked about in Section 2 can be used to find the best sequence of sentences for any document d. Formal: We use the highest entropy sequence for each sentence to make sure the summary captures the most important sentences. Formal: The main goal is to figure out the most important sentences in a document. Formal: We first group sentences that probably have important stuff together. Then, we sort the sentences based on the entropy of their sequences. Formal: We use a balanced decision sampling approach where we pick the sentence with the highest entropy. The algorithm we talked about in Section 2 can also be used to find the best sequence of sentences for any document d. Formal: Since the document might not have a lot of words, we first group sentences that might have a small but meaningful collection of words together. Then, we sort the sentences based on how much of those words are in the collection. Formal: We use a balanced decision sampling approach where we pick the sentence with the highest entropy. Formal: We use the lowest entropy sequence to make sure the summary captures the most important sentences. Formal: Finally, we mix all the sentences together to get the summary. Formal: Lastly, we use a balanced decision sampling approach where we pick the sentence with the highest entropy. Formal: Formal: Formal: This approach is great because it can easily find the most relevant sentences for any document d, while also capturing all the important words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D10",
        "filename": "D10-1048.json"
    },
    {
        "casual_text": "Alright, let's talk about child labor in Africa, specifically in South Africa. It's a big issue where kids are forced to work instead of going to school. This affects a lot of children and is a major problem in the labor force there.",
        "formal_text": "non#$ development$of$ child$labor$ child$$$$$$$$$$$$$$$$$$$$$$$$children$$$$$$$$$$$$$$$$$$$$child$ labor$$$$$$$$$$$$$$$$$$$$$$$$$labor$$$$$$$$$$$$$$$$$$$$$$$$labor$force$ Africa$ South$Africa$",
        "GPT2_formal_text": "_t. Unlike (1), (2), and (3), we start with a mention's representation, p_t, and the contextual embeddings from the entity's embedding, e_i. Then, we do a CLWE (Cross-Lingual Word Embedding) to get a context embedding c_t for the mention. Formal: To get the entity's embedding, we just use the entity's representations, e_e and e_i, as input. Formal: We use a special number called λ for the normalization loss, which is adjustable. The loss function is: Formal: The embedding for the entity mentions, e_e and e_i, is created based on the entity embedding, p_t, and the contextual embedding, e_e. Formal: To find the word embedding for the mention (u_m), we first use the entity embedding, e_e, as input. Then, we calculate the embedding vector for the mention, p_m. Formal: We apply the normalization loss, λ, to the embeddings, p_t and p_m. Formal: Once we have the entity and context embeddings, we feed them into the CM-block to create the context embedding, c_t. Formal: We use the normalized vectors c_t to calculate the logit loss for the CM-block. Formal: The final embedding of the mention is the normalized vector, which is the normalized logit of the entity mention, l_m. Formal: We do a full CLWE (Cross-Lingual Word Embedding) on the entity mention to get the context embedding, c_t. Formal: The results from this CLWE are shown in Figure 2. Formal: To improve the performance of cross-lingual entity typing, we can use phrase embeddings from other languages to help improve the embeddings of the mentions in the source language. Formal: Here's how the entity embedding and context embedding (c_t) are updated in the CLWE process: Formal: We follow the usual sequence of steps in the CLWE: Formal: For the entity embedding, we use the entity embedding, e_e, as input. Then, we calculate the embedding vector for the mention, p_m. Formal: Finally, we add the",
        "directory": "D16",
        "filename": "D16-1055.json"
    },
    {
        "casual_text": "It seems like there's a lot of hype around neural architectures these days, especially when it comes to encoder-decoder models for Seq2Seq tasks. For instance, Rao et al. (2015) got some great results with their encoder-decoder G2P model, hitting a really low error rate on the CMUdict dataset (Kominek and Black, 2004). But, when you look closer, the neural architecture itself isn't all that impressive on its own. It only really shines when paired with a weighted finite state transducer, where it edges out traditional models. \n\nSimilarly, Faruqui et al. (2016) claimed their inflection generation neural architecture performed \"par or better\" compared to traditional methods. But digging into their results, it turns out their system isn't consistently better—sometimes it's worse, and other times it's better.",
        "formal_text": "We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer. Similarly, Faruqui et al. (2016) report on \"par or better\" performance of their inflection generation neural architecture. However, a closer inspection of their results suggests that their system is sometimes worse and sometimes better than traditional approaches.",
        "GPT2_formal_text": "_tokens = [word_to_key] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_tokens] + [mask_",
        "directory": "C16",
        "filename": "C16-1160.json"
    },
    {
        "casual_text": "Apart from TAG-TAG, TAG-WORD, and TAG-REASON, we also added the method from (Heymann and Garcia-Molina, 2006) as a baseline, which we call HEYMANN. The HEYMANN method is meant to identify \"similar-to\" relationships rather than \"subsumption\" relationships. \"Similar-to\" relationships are symmetrical, while \"subsumption\" relationships are stricter and not symmetrical. In our tests, we used the same evaluation process for TAG-TAG, TAG-WORD, TAG-REASON, and HEYMANN, and only subsumption relationships were counted as correct.",
        "formal_text": "Besides TAG-TAG, TAG-WORD and TAG-REASON, we also include the method described in (Heymann and Garcia-Molina, 2006) as a baseline, denoted as HEYMANN. HEYMANN method was designed to find similar-to relation rather than subsumption relation. The similar-to relation is symmetric, while subsumption relation is more strict and asymmetric. In our experiments, we use the same evaluation process to evaluate TAG-TAG, TAG-WORD, TAG-REASON and HEYMANN, in which only subsumption relations will be marked as correct.",
        "GPT2_formal_text": "= [u1, u2, ..., um] for each word in the sentence. If the sentence has a title or abstract, those are handled separately. Formal: Formal: We also wanted to check if this approach worked well. We used a smaller set of 50 random words from the ENNI corpus to train our model. Formal: We used the same setup as other studies. Formal: We used the same training set as the one in (Sun et al., 2018a) to train our model. Formal: We trained it using the same evaluation method as the others. Table 1 shows the results for the same sentence, which had titles and abstracts. The F1 score ranges from 0.4 to 0.5 for both human and model evaluations. Formal: The results in Table 1 are based on the development set, with all words being removed. Formal: We took out the words from the sentence and divided it into training (dev) and testing (test) sets. Formal: We trained the model for 10 epochs using the development set, and then we checked the performance for each epoch. Formal: After removing the words, we divided the sentence again into training and testing sets. Table 1 has the results for the same sentence, but this time the words are removed in the human evaluation. Formal: From Table 1, we can see that we can keep the title and abstracts of the original sentence. But, the whole sentence still needs to be removed to make the test set. Formal: From Table 1, we can see that we can keep the title and abstracts of the original sentence. Formal: Our method does a better job than the others at predicting the correct answer, especially for titles and abstracts. Formal: Formal: Our method does a better job than the others at predicting the correct answer, especially for titles and abstracts. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C10",
        "filename": "C10-1114.json"
    },
    {
        "casual_text": "We haven't really looked into GGP's parsing complexity broadly yet, but some key factors that limit chart parsing are pretty well understood, thanks to studies on context-free parsing by Sheil back in 1976. Basically, the number of steps it takes is around O(nD), and the space needed is O(n²), no matter how you approach the parsing. The length of the input sentence (n) plays a big role here. Interestingly, the size of the grammar doesn't affect the complexity, but the branching factor—which kinda tells you how uncertain or unpredictable the grammar is—acts like a multiplier.",
        "formal_text": "Although we have not yet analyzed GGP's parsing complexity in general, some limiting factors for chart parsing are well known by investigations on the context free case by Sheil (1976): The number of steps is o~ O (nD), the space requirements of 0 (n 2) independent of the parsing strategy, where n is the length of the input sentence. The size of the grammar does not influence complexity, but its branching factor, which is a measure for its degree of nondeterminism, acts as a proportionality factor.",
        "GPT2_formal_text": "_t. The amount of data is picked based on the square root of the cross-entropy loss, and the size of the hidden vectors is set to 3. The cross-entropy loss is usually chosen to be as small as possible. Formal: Some methods focus on avoiding the issue of undersampling when we're doing gradient descent. We're mainly looking at the risk of overfitting the data, which means the training objective is shaped like a hyperplane that's not convex anymore. Formal: We do a gradient descent on the embeddings of the non-convex parts to get the embeddings for the convex parts. This gives us the result: Formal: We use the EM algorithm on the embeddings of the convex parts to get the embeddings for the non-convex parts. Formal: We calculate the dot product between these embeddings using cross-entropy loss, and then we add those to the original embeddings to get the final result. Formal: We set the batch size to 8 for the LSTM model and use 1-best sequence labeling. Formal: We train the LSTM model with the Adam optimizer, with a learning rate of 0.1. Formal: We use the Adam optimizer with a learning rate of 0.1. Formal: The learning rate for each batch is adjusted based on the cross-entropy loss, with a learning rate of 0.1. Formal: The embeddings for the non-convex parts of the embedding vector are created using the LSTM model's latent representations. Formal: The final output from the LSTM model is encoded using the input embeddings of the non-convex parts. Formal: Formal: We train the LSTM model with the Adam optimizer with a learning rate of 0.1 and a batch size of 8. Formal: The hidden layer dimension in the LSTM model is also adjusted based on the cross-entropy loss, with a learning rate of 0.1. Formal: We train the LSTM model with the Adam optimizer with a learning rate of 0.1 and a batch size of 8. Formal: We train the LSTM model with the Adam optimizer with a learning rate of 0.1 and a batch size of 8. Formal: We use the LSTM model's latent representations to create the embed",
        "directory": "E83",
        "filename": "E83-1019.json"
    },
    {
        "casual_text": "Lexical weighting helps deal with the issue of unreliable phrase probabilities, especially for longer phrases, but it doesn't fix the problem of breaking down context. A lot of the work in picking the right translation still falls on the language model (LM), which is really good at what it does but can only use context from the target language. Plus, decisions made early on—like only keeping the top n translation options for each part of the source text—are based only on the translation models and the target context within that phrase.",
        "formal_text": "Lexical weighting tackles the problem of unreliable phrase probabilities, typically associated with long phrases, but does not alleviate the problem of context segmentation. An important share of the translation selection task is then left to the language model (LM), which is certainly very effective but can only leverage target language context. Moreover, decisions that are taken at early decoding stages-such as the common practice of retaining only top n translation options for each source span-depend only on the translation models and on the target context available in the phrase.",
        "GPT2_formal_text": "= (r_i, ..., r_{i-1}). Here, r_i is a character embedding, and r_{i-1} is a position embedding. Formal: If the input is a token-level parse tree, we use a max operation on the input sequence representation to create a sub-tree that covers the token's main sub-tree. The main sub-tree has the main word embedding and the word embedding of its parent. So, the input gets transformed into a sequence of hidden states, which we call h. Formal: But if the input is a span-level parse tree, the input gets turned into a sequence of hidden states, h, which are the tokens of the span. Formal: Formal: In this paper, we use a GCN with two layers of GCN to create a context-aware decoder. Formal: We begin with a word embedding h(x) and then use the parent embedding c(x) to figure out the dependency path that goes from the parent to its child nodes. Formal: The cross-attention mechanism helps the decoder learn to predict the dependency path from the parent to the child nodes. Formal: We tested our model on the CNNDM dataset and the WMT 2010 dataset. Formal: Here, T represents the number of tokens, and C(•) represents the number of words. Formal: We implemented our model in PyTorch and ran experiments on the CoNLL-2009 dataset. Formal: For the cross-attention model, we used a gated linear layer with a 300-dimensional word embedding h(x) and a 1000-dimensional word embedding c(x). Formal: For the hierarchical attention model, we used a gated linear layer with a 300-dimensional hierarchical attention embedding h(x) and a 1000-dimensional hierarchical attention embedding c(x). Formal: For the generative model, we used a Bi-LSTM with a 2-layer structure with the hidden state h_i for the embedding dimension. Formal: Formal: Lastly, for the discriminative model, we used a Bi-LSTM with a 2-layer structure with the hidden state h_i for the embedding dimension. Formal: The final hidden state state of the decoder is what we use for the decoder's output. Formal: Formal: For the hierarchical attention model",
        "directory": "D14",
        "filename": "D14-1175.json"
    },
    {
        "casual_text": "So, when we look at how well the Punjabi text summarization system works for Punjabi news articles, it does pretty well at a 10% compression rate. This is because, at that level, the system usually picks the headline and the next line, which is often enough to give a good idea of the whole article. But for Punjabi stories, the system doesn't do as well at 10% compression. That's because stories don't have headlines, and the few lines that get picked for the summary aren't enough to capture the full meaning of the story.\n\nWe also tested the system by doing question answering and keyword association tasks at different compression levels—10%, 30%, and 50%—for both Punjabi news articles and stories. For news articles, the question answering accuracy was 78.95% at 10%, 81.38% at 30%, and 88.75% at 50%. The system struggled a bit at 10% because news articles are usually short, and at that compression level, it mainly picks the headline and the next line, which isn't enough to answer all the questions.\n\nFor Punjabi stories, the question answering accuracy was 80.65% at 10%, 84.26% at 30%, and 90.72% at 50%. And for news articles, the keyword association accuracy was 80.13% at 10%, and then it goes up to 92% at 30%.",
        "formal_text": "The results of intrinsic summary evaluation show that for Punjabi news documents, Punjabi text summarization system performs very well at 10% compression ratio, because at 10% compression ratio usually headlines and next lines are extracted which are enough to describe the whole text but for Punjabi stories, performance of Punjabi text summarization system is not good at 10% compression ratio, because headlines are not present in stories and only few lines are extracted in summary which are not enough to describe the sense of complete story. We have performed question answering task and keywords association task as extrinsic measures of summary evaluation at compression ratios 10%, 30% and 50% respectively for Punjabi news documents and Punjabi stories. For Punjabi news documents, the accuracy of question answering task is 78. 95%, 81. 38% and 88. 75% at 10%, 30% and 50% compression ratios respectively. The results of question answering task show that for Punjabi news documents, performance of Punjabi text summarization system is low at 10% compression ratio because news documents are usually short and at 10% compression ratio, mainly headlines and next lines are extracted which are not sufficient to give all answers of question-answering task. For Punjabi stories, the accuracy of question answering task is 80. 65%, 84. 26% and 90. 72% at 10%, 30% and 50% compression ratios respectively. For Punjabi news documents, the accuracy of keywords association task is 80. 13%, 92.",
        "GPT2_formal_text": "ed is set to 1, which means the text representation is just a 1-dimensional vector. Formal: The function [s, t] gets turned into a graph (let's call it f) using a stack. This graph has a directed acyclic graph structure, where each node in the graph represents the sentence s and each edge (edge weight) represents the attention weights for the current word t. The attention weights for each node are calculated as follows: Formal: So, the context vector, ctx_t_i, is like a two-dimensional vector that tells us about the context of the current word, the word around it, and all the words around it. Formal: The document vector, dtx_t_i, is like the summary of the entire document. Formal: The document embedding, f_d, represents all the words in the document and their context vectors. Formal: The attention weights for the words are calculated based on this. Formal: In our experiments, we used ROUGE-1 and ROUGE-2 to measure how well our system performs. Formal: We also calculated attention scores for the event vectors ctx_e_i, which are similar to the document vectors but for events. Formal: For the event vectors ctx_e_i, we used the S2S model (Sennrich et al., 2015) to calculate the attention weights for the events. Formal: The CKA model is a non-linear model that works with a hidden state, h_c, as the input. Formal: We also tried the K-means algorithm to figure out the event vectors ctx_e_i, but it didn't work well because it doesn't have a clear structure. Formal: For the sentence vector ctx_s, we followed the approach by Wang et al. (2016). Formal: We built our model using the Adam optimization algorithm (Kingma and Ba, 2014) and set the learning rate to 0.00001. Formal: In the end, we got the attention vectors that show how important each event is. Formal: The attention weights for event vectors are calculated using this formula: Formal: For the sentence vector ctx_s, we used the same structure-aware attention model as Wang et al. (2016). Formal: The attention weights for sentence vectors are calculated using this formula: Formal: For the document vector ctx_",
        "directory": "C12",
        "filename": "C12-3024.json"
    },
    {
        "casual_text": "Okay, so basically, this formula is like a way to figure out how related a document (d) is to a query (q). It takes into account all the words (w) in the vocabulary (V). Here's what each part means:\n\n- `C * (w | d)` is how common the word is in the document.\n- `C(w | q)` is how common the word is in the query.\n- `idf(w)` is a measure of how rare the word is across all documents.\n- `p` is a number that helps balance things out.\n- `avdl` is the average length of documents.\n- `|d|` is the length of the specific document we're looking at.\n\nThe formula mixes these things together, giving more weight (0.8) to the average document length and a bit less (0.2) to the actual length of the document we're checking.",
        "formal_text": "rel(d, q) = P w∈V C * (w | d) • C(w | q) • idf (w) p 0. 8 • avdl + 0. 2 • |d|",
        "GPT2_formal_text": "_t = [t_i, t_j] token_ids_t = [t_i, t_j] model_t = {m_i, m_j} embedding_t = [e_t_i, e_t_j] model_s = [s_i, s_j] model_t = {s_i, s_j} embedding_s = [e_s_i, e_s_j] model_s = [s_s_i, s_s_j] model_t = {s_t_i, s_t_j} hidden = [e_t_i, e_t_j] model_t = {s_t_i, s_t_j} hidden = [s_s_i, s_s_j] model_s = [s_s_s_i, s_s_s_j] model_t = {s_t_s_i, s_t_j} hidden = [s_s_s_i, s_s_s_j] model_s = [s_s_s_s_j, s_s_s_i] model_t = {s_t_s_s_i, s_t_j} hidden = [s_s_s_s_j, s_s_s_i] model_s = [s_s_s_s_j, s_s_s_i] model_t = {s_s_s_s_i, s_t_j} hidden = [s_s_s_s_i, s_s_s_i] model_s = [s_s_s_s_j, s_s_s_i] model_t = {s_t_s_s_i, s_t_j} hidden = [s_s_s_s_s_j, s_s_s_i] model_t = {s_t_s_s_s_i, s_t_j} embedding_s = [e_s_s_i, e_s_s_j] model_t = {s_s_s_s_i, s_s_s_j} embedding_s =",
        "directory": "D07",
        "filename": "D07-1085.json"
    },
    {
        "casual_text": "Once all the skeleton nodes are ready, we build a graph using those nodes, but we leave out the right parenthesis, as shown in Figure 3. To get the global structure info, we slap a GAT network on top of the hidden states.",
        "formal_text": "Specifically, after all the skeleton nodes are generated, we construct a graph based on the nodes except the right parenthesis as shown in Figure 3. We use a GAT network on top of the hidden states to capture global structure information:",
        "GPT2_formal_text": "_t * attention_mask_t) for the last token, which are the same as the attention values for the whole input sequence. Formal: Let's say the input sequence has n tokens. The language model E_s is trained using a linear transformation called U_s that takes in the attention values for the n-th token. Formal: We also use a bi-directional LSTM, which is like a fancy version of the LSTM. We start by adding the source word to the source sequence, then use a bi-directional LSTM to get a hidden state. After that, we do something similar for the target sequence, but instead of using the source word, we use the target word to get a hidden state. So, the hidden state from the target sequence acts as the input for the bi-directional LSTM. Formal: The bi-directional LSTM is a fancy version of the LSTM, and it takes the hidden state h_t from the target sequence as input. It uses this hidden state to predict the next token in the input sequence. Formal: We’ve got a bi-directional LSTM in our system that predicts the next token in the input sequence. We call it the Bi-LSTM. Formal: The Bi-LSTM is trained using a linear transformation called U_s that takes in the attention values for the n-th token. We call the output h_t from this Bi-LSTM the input sequence for the Bi-LSTM. Formal: The Bi-LSTM is trained using a linear transformation called U_s that takes in the attention values for the n-th token. Formal: After that, we do the same thing for the target sequence, but instead of using the target word, we use the source word to get a hidden state. The output from the Bi-LSTM acts as the input for the bi-directional LSTM. Formal: You can check out the whole setup in Figure 2. Formal: We train a linear transform using a non-linear function h_t to create the hidden state. Formal: Here’s the equation: Formal: Let’s say L_s is the final hidden state of the Bi-LSTM after it’s been trained. Formal: The final hidden state of the Bi-LSTM after it",
        "directory": "acl",
        "filename": "2020.acl-main.609.json"
    },
    {
        "casual_text": "TTK has this cool visualization thing called TBox, which Verhagen came up with back in 2007. It uses arrows pointing from left to right, boxes inside other boxes, and stacked boxes to show stuff like what happens before, what's part of something bigger, and what happens at the same time. Check out figure 3 for an example!",
        "formal_text": "TTK uses a visualization scheme named TBox (Verhagen, 2007). It uses left-to-right arrows, box inclusion and stacking to encode temporal precedence, inclusion, and simultaneity respectively (see figure 3 ).",
        "GPT2_formal_text": ", weights, and biases for a specific topic. These topics are sorted into different levels (like 1-5 or 5-10) based on how well they represent the data points. Formal: The weight for the word \"the\" is zero, and all the other words in the document are assigned the same weight. Formal: The weights for the other words in the document are the same as the weights assigned to \"the.\" Formal: Using a word list for the topic selection process is pretty straightforward, but it still needs a decent number of examples to work well. Formal: For the topic selection, the function f_i(i) is calculated using the word list in step (i) from the Kullback-Leibler (KL) divergence. Formal: The KL divergence is basically a way to show how the search space in a KL-divergence is spread out. Formal: The KL divergence is calculated like this: Formal: The KL divergence is calculated like this: Formal: If you have two documents, D and D', the KL divergence is calculated like this: Formal: If you have two documents, D and D', the KL divergence is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C08",
        "filename": "C08-3012.json"
    },
    {
        "casual_text": "We fine-tune a bi-directional language model to recognize slang words and understand how they’re used by focusing on the masked language modeling task. To do this, we use a dataset from the Urban Dictionary that was scraped and collected by Wilson et al. (2020). After cleaning and trimming the data (check out Appendix A.1 for the specifics), we end up with a training set of 200,000 text sequences packed with slang. \n\nFor our bi-directional model, we go with RoBERTa (Liu et al., 2019). We pick it over the original BERT (Devlin et al., 2019) because it’s better and allows for more subword units. We think this could be helpful for slang since some of the smaller parts of these words might not have been recognized by BERT. Plus, we go with the smaller 125M parameter version to save on computational resources.\n\nWe train the model using the Adam optimizer (Kingma and Ba, 2015) with different learning rates. After testing, we found that the lowest loss on the test set came with a learning rate of γ = 10⁻⁶, so that’s what we use for scoring semantic change. If you want more details on the training setup, check out Appendix A.2.",
        "formal_text": "We familiarize the bi-directional language model with slang words and the contexts in which they are used by fine-tuning it on the masked language modeling task. For this purpose we use a web-scraped dataset from the Urban Dictionary, previously collected by Wilson et al. (2020). After preprocessing and subsampling, the details of which can be found in Appendix A. 1, we are left with a training set of 200, 000 slang-dense text sequences. As our bi-directional language model we select RoBERTa (Liu et al. , 2019). Beyond performance gains compared to the original BERT (Devlin et al. , 2019), we select this model since it allows for more subword units. We reason, that this could be useful in the context of slang words since potentially some of the sub-units used in these words would not have been recognized by BERT. We choose the smaller 125M parameter base version for computational reasons. We train the model using the Adam optimizer (Kingma and Ba, 2015) with different learning rates γ. The lowest loss on the test set was found with γ = 10 −6, which we proceed with for scoring semantic change. For more details on training configurations, we refer to Appendix A. 2.",
        "GPT2_formal_text": "( ), which is like a semi-automatically generated version of the original attention. Formal: Here, σ is a non-linear function that helps balance out the attention value when the model is dealing with different input features. Formal: In this setup, the input representation is a bunch of vectors, which we call x i, j. Each x i, j is calculated using the attention mask, and it's calculated as... Formal: So, the final attention output, after doing the (x i, j ) thing, is what you get when you average the attention weights across all the vectors. Formal: Just a heads-up: the attention weights (γ i, j ) can be different values. For instance, if the input is x i = {x i 1, ..., x i n }, the attention weights could be... Formal: For the NLP stuff we're talking about here, the output representations (x i, j ) are represented as a grid, which looks like this: Formal: The x i, j grid is a matrix with a dimension of d k, where d k is the size of the hidden state in the (x i, j ) matrix. Formal: For the RNN, we use a linear transformation (λ_i, j ) to get the representations for the (x i, j ) grid. Formal: Here, (x i, j ) is a matrix, where d k is the size of the hidden state in the (x i, j ) matrix. Formal: In the RNN, we use a linear transformation (λ_i, j ) to get the representations for the (x i, j ) grid. Formal: The x i, j grid is a matrix with a dimension of d k, where d k is the size of the hidden state in the (x i, j ) matrix. Formal: Finally, the average of the attention weights across all the vectors gives us the final representation for the input. Formal: Formal: But here's the thing: we don't use any special tricks to create a regularizer to avoid overfitting. Formal: We're mainly looking at the RNN and RNN models because, as far as we know, they're the most popular. Formal: But there's a catch: the way attention weights are learned can get pretty complicated when you're dealing with a lot of vectors. Formal: We're mainly checking out the R",
        "directory": "acl",
        "filename": "2022.acl-long.101.json"
    },
    {
        "casual_text": "Later on, we'll talk about how some methods add an extra step to figure out the best groups of mentions for a document. But even with that, they're still relying on confidence scores that come from looking at things locally.",
        "formal_text": "As we will discuss below, some approaches use an additional component to infer the overall best mention clusters for a document, but this is still based on confidence scores assigned using local information.",
        "GPT2_formal_text": "= [u * i, f * j] n = [b * i, c * j] p = [b * i, c * j] Formal: These models are basically the same as the ones from (Chen and Bansal, 2019), but they use different ways to handle vectors. Formal: We’ve come up with two ways to tweak a single-layer perceptron, called P = {h s } n, to make it work with a probabilistic model. Our approach is based on a probabilistic semantic space model. We show how adding vector space info can make the space less sparse, and we also demonstrate that using something called the latent variable as a source for an embedding vector space can actually improve things. Formal: The models we’ve developed can be used in different situations, depending on the type of semantic space you’re working with. Formal: Here’s a quick rundown of the models we’ve talked about and how they’re used in different tasks. Formal: Check out Figure 1 for more on how we handle pre-training. Formal: To get the embedding embeddings for a word w_i, we first calculate its embedding embedding by taking the average of the embeddings of all the words in w_i. Formal: The embedding for a word w_i is a specific vector that represents its meaning in a certain space. Formal: We use the embedding of the word w_i as the starting point for our model. Formal: We also include a learning rate γ that adjusts how sensitive we are to noise, like a random variable, which is just a number between 0 and 1. Formal: Here’s the model setup. Formal: We’ve tested different learning rates and the learned embedding embeddings, and the results are shown in Figure 1. Formal: The learned embedding embeddings for a word w_i are represented by a vector, which is a matrix where the embedding embeddings are the rows and columns of the word w_i. Formal: The function f in Equation (1) is used to give the embedding embeddings for the word w_i. Formal: The learned embeddings for a word w_i are represented by a vector, which is a matrix where the embedding embeddings are the columns and rows of the",
        "directory": "D10",
        "filename": "D10-1048.json"
    },
    {
        "casual_text": "If the community wants to go over those maximum amounts we just talked about, they can, but only if they figure out what those extra amounts should be... and then decide on them.",
        "formal_text": "die Gemeinschaft, dab to these quantitative limits die vorgenannten HOchstmengen being increased, Qberschritten werden, on the understanding that sofern the additional quantities die zusatzlichen Mengen shall be determined. . . .",
        "GPT2_formal_text": "is a set of operations that take a token and turn it into a single word, as described by Mikolov and his team in 2013. Formal: Here, b_i is the attention value that gets passed down, while a_i is the bias vector that helps balance out the attention from other nearby tokens. Formal: The model uses the output from the LSTM's hidden layer to predict the next token, y_t. The input to the model is a sequence of words w, which includes the token token and the word before it. Formal: For the next token, the model uses the attention mask w_i to figure out the attention value. Formal: The model creates a special token to represent the word that's most similar to the current token, w_t. Formal: We use the output from the LSTM's hidden layer to predict the next word. Formal: The model creates a special token to represent the word that's most similar to the current token, w_t. Formal: Formal: Formal: In the sentence encoder, the word type embedding h_t_i is built using the attention mask from the first hidden layer. Formal: The model creates a special token to represent the word that's most similar to the current token, w_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C86",
        "filename": "C86-1078.json"
    },
    {
        "casual_text": "• Delete(k): Pick a spot, k, and then randomly choose a number, m, from 1 to 6. Starting from that spot, delete m words.",
        "formal_text": "• Delete(k): for selected position k, m (randomly selected from one to six) words starting from this position are deleted.",
        "GPT2_formal_text": "(1) and (2) are the sentence-level aspect-level attention scores, which are basically the average scores from the two aspect categories. The m-th aspect-level attention is calculated using the attention weight vector (1), which we'll just call \"a_m\" for short. Formal: To figure out the attention weights for both the aspect-level and sentence-level attention in this model, we first create attention weights for each of the two types of attention. Formal: You can figure out the attention weights by using a linear-chain CRF (LCF) to sample attention values. Formal: A group of randomly chosen sentences is called a \"sentence-level attention vector.\" Formal: The attention weights for sentence-level and aspect-level attention are calculated as: Formal: To get the sentence-level attention vector, you just average the sentence-level attention vectors from both the aspect-level and sentence-level attention vectors. Formal: Similarly, you can figure out the aspect-level attention vector by averaging the aspect-level attention vectors. Formal: To get the sentence-level attention vector, you do the same thing as for the aspect-level vector. Formal: We use the \"a_m\" attention weight vector to make sure the attention weights for the sentences and aspect categories match up. Formal: From what we've seen, the attention weights for both the sentence and aspect vectors are pretty similar in how they behave, so we’re just focusing on the attention weights for the sentence and aspect vectors. Formal: For example, in Figure 2, the attention weights for the sentence-level attention vectors are 0.82, which means they're really focused on the sentence. Formal: Formal: Similarly, the attention weights for the aspect-level attention vectors are 0.33, which matches what we saw in Table 2. Formal: Formal: The attention weights for the sentence-level attention vectors are calculated as: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.142.json"
    },
    {
        "casual_text": "There are two main types of global algorithms: generative probabilistic models and similarity-based models. Eisenstein and Barzilay (2008) think of a document as a series of parts created by hidden topic variables. Misra et al. (2011) and Du et al. (2013) have similar ideas. On the other hand, Malioutov and Barzilay (2006) and Kazantseva and Szpakowicz (2011) use similarity-based methods. Both of these approaches start with a matrix showing how similar the sentences in a document are to each other. The first one uses graph cuts to identify groups of sentences that fit together well, while the second one tweaks a clustering algorithm to do the same thing.",
        "formal_text": "Among global algorithms, we can distinguish generative probabilistic models and similarity-based models. Eisenstein and Barzilay (2008) model a document as a sequence of segments generated by latent topic variables. Misra et al. (2011) and Du et al. (2013) have similar models. Malioutov and Barzilay (2006) and (Kazantseva and Szpakowicz, 2011) use similarity-based representations. Both algorithms take as input a matrix of similarities between sentences of the input document; the former uses graph cuts to find cohesive segments, while the latter modifies a clustering algorithm to perform segmentation.",
        "GPT2_formal_text": "ing_d = W_attn_masking_d * tanh(W_attn_d) * h_attn_d. Formal: But, we get a small improvement in F1 score by using residual connections (R), which actually boosts the performance. Formal: We also looked into how adding more layers affects attention in zero-shot learning scenarios. Formal: Our experiments show that using multiple layers of attention works really well for zero-shot learning, especially when there are just a few examples to learn from. Formal: We went with a single-layer CNN for the model, using d = 8. Formal: We trained the model on a dataset with a total of 123,000 examples for the final classification task, using the development set as the test set. The model was trained using the Adam optimizer (from Kingma and Ba, 2015), with a learning rate of 0.0001. Formal: The results for the zero-shot scenario are in Table 2, along with the standard classification scores from the original data set. Formal: We tested three different attention mechanisms. We used two types of regularizers: one based on a Gaussian prior and the other using a uniform prior. Formal: The model trained with a Gaussian prior (check Table 2) got a score of 0.59, which is 0.59 higher than the baseline, which uses a uniform prior (0.58). Formal: We also experimented with the latent variable prior, which we trained on the development set (0.58) to get the final score. Formal: We trained the model with a latent variable prior (0.58) and got a score of 0.56, which is 0.56 higher than the baseline. Formal: Formal: Finally, we compared the model trained with a latent variable prior (0.58) to the one trained with a uniform prior (0.58). Formal: Our experiments show that the latent variable prior works well for zero-shot learning. Formal: We also did some human evaluations to see how different attention mechanisms affect the performance. Formal: The results are in Table 3. Formal: We tested three attention mechanisms: two using a linear prior and one using a flat prior (0.6). We used the dev set as the test set. Formal: Compared to the other methods, the latent variable attention mechanism consistently improved performance. Formal: We also added a bias term to the main",
        "directory": "C14",
        "filename": "C14-1005.json"
    },
    {
        "casual_text": "The training texts had 6580 unique words and 6945 unique tag trigram types. In the open test sentences, there were 247 words and 213 tag trigrams that weren’t in the training data. So, we needed to smooth both the part-of-speech trigram probabilities and the word output probabilities to handle texts with unknown stuff. We tested our system, which uses smoothed part-of-speech trigrams along with a word model, on these open test sentences. Table 4 shows how many words were correctly segmented and tagged. In Table 4, \"label consistency 2\" shows the accuracy of segmentation and tagging, but it doesn’t count differences in conjugation forms.",
        "formal_text": "The training texts contained 6580 word types and 6945 tag trigram types. There were 247 unknown word types and 213 unknown tag trigram types in tim open test senteuces. Thus, both part of speech trigralrl l)robabilities alld word output probabilities must be snioothed to handle open texts. We then tested the proposed system, which uses smoothed part of speech trigram with word model, on the open test sentences. Table 4 shows tile percentages of words correctly segmented and tagged. In Table 4, label consistency 2 represents the accuracy of segmentation and tagging ignoring the difference in conjugation form.",
        "GPT2_formal_text": "is the full list of unique words that show up in both the original input and the generated summary. Formal: To make sure our summarization model picks up on the important details in the input and outputs, we use a self-supervised learning method called the L2-norm regularization. Formal: The losses for the outputs are calculated using the cross-entropy loss function. Formal: The importance scores for the output i are calculated using the score correlation loss function. Formal: Here, `i` is the i-th input sentence and `k` is the k-th summary sentence. Formal: The topic distribution in the input is created by picking the sentence with the highest probability, `p_i(x)` for each input sentence. Formal: To get the topic distribution for the output `y`, we start by finding the sentence with the highest probability, `p_k(x)` for each output `y`. Formal: If a summary has fewer words than an input sentence, we simplify the topic distribution to a single number `p_i(x)` for each input sentence. Formal: Each `i` is the i-th input sentence and each `k` is the k-th summary sentence. Formal: The topic distribution in the output `y` is calculated by reducing the cross-entropy loss function. Formal: If the loss for the output `y` is zero or the loss for the output `y` is too big, we just ignore the input summaries. Formal: If the loss for the output `y` is zero or the loss for the output `y` is too small, we just add the input summaries to the output's summary. Formal: This approach helps the model get a better understanding of the input's topic. Formal: Using the zero-shot and small-scale learning methods, we can improve the model's performance by using less data to train it. Formal: We also add a residual connection between the input and output topic distributions. Formal: Here, `r_ij` is the residual connection loss function. Formal: For each output `i`, the loss for the output `y` is calculated by using the sum of the loss for the input and output topics. Formal: Similarly, for the input sentence `i`, the loss for the input sentence `i` is calculated by using the sum of the loss for the input and output topics. Form",
        "directory": "C94",
        "filename": "C94-1032.json"
    },
    {
        "casual_text": "Wikipedia version: enwiki-20190301. The spaCy model is called \"en_core_web_md\" and it's version 2.1.8. It was trained using the OntoNotes dataset.",
        "formal_text": "Wikipedia version: enwiki-20190301. 5 The spaCy model is \"en core web md\" in version: 2. 1. 8, which trained on OntoNotes dataset.",
        "GPT2_formal_text": "ed for the S2S task. The main goal here is to find the attention score between 0 and 1. Formal: In the Equation 1, the attention distribution is calculated using an empirical distribution. Formal: The cross attention mechanism works by using the attention distribution for each token to figure out how to translate each word. Formal: Also, the weight for a token, w_t, in the cross attention mechanism is the sum of its word embedding and the cross attention feature. Formal: Formal: To get a better understanding of the attention distribution for a word, we use this approach. Formal: We do this by averaging the attention feature scores from all the tokens in a sentence. Formal: We also mix the attention values from the words in the sentence to create a weighted average. Formal: The final attention feature for the token w_t is calculated as the product of the word embedding and the cross attention feature for each token. Formal: We pick the token w_t that has the highest attention score (l highest_attn(w_t)), where l is the maximum number of tokens in the sentence. Formal: The attention score is calculated as the sum of the attention feature weights from all the tokens in the sentence. Formal: This method helps us predict the token's translation in a way that keeps the attention distribution similar to the original token's attention distribution. Formal: It also helps us find the best translation for a token by considering its attention distribution. Formal: Finally, to figure out the attention score for a token, we average the attention scores from all the tokens in the sentence. Formal: We also mix the attention values from the words in the sentence to create a weighted average. Formal: Formal: These cross-attention scores give us an approximate translation for each token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eacl",
        "filename": "2021.eacl-main.251.json"
    },
    {
        "casual_text": "Alright, so W(s) and b(s) are the parameters of this feed-forward neural network. To make sure we really understand the syntax, position, and other connections between a zero pronoun and its possible candidates, we add some extra features (v(f e)) that we handpick and feed into our neural network model. We take these features from previous research on zero anaphora resolution (like Chen and Ng, 2013; Chen and Ng, 2016) and turn them into vectors to calculate a score that helps us figure out the best match between the zero pronoun and its candidate.",
        "formal_text": "], W (s) and b (s) are the parameters of this feed-forward neural network. In addition, to better capture the syntactics, position and other relations between an anaphoric zero pronoun and its candidates, we encode hand crafted features (v (f e) ) as inputs to our neural network model. We utilize the features from exsiting work on zero anaphora resolution (Chen and Ng, 2013; Chen and Ng, 2016), map them into vectors to estimate the resolution score for the zero pronoun-candidate mention pair as:",
        "GPT2_formal_text": "), o_t = 0, and o_s = 0. \n\nIn Figure 2, you can see the network setup for the text generation task. The input for this task is the same as in regular entity linking but instead of the usual entity and event types, it uses the attribute-value pairs you can get from the knowledge base. You also get a special token called [CLS] to mark the beginning of the text. For the entity detection task, the input is the entity pair along with the part of speech (POS) tag, e_t, which can be either a number or a character. We create a special token called [CLS] to start the text. The model is set up with a linear layer, a ReLU activation, and a dropout probability of 0.1. The hidden size is set to 128, and the dropout rate is 0.5. \n\nFor the entity detection task, we use a simple bag-of-words approach with dropout probability 0.1. The output from the convolutional layer is the entity pair, with the POS tag, e_t, as the input. \n\nWe feed this into a bidirectional GRU (BiGRU) to figure out the entity pair and the POS tags. \n\nFor the text generation task, we take the entity pair and the POS tag as input to create the word embedding for the text. We start the embedding by using a BiGRU to get the embedding for each character. Formal: Here’s a quick summary of the main steps for generating text using entity linking and text generation. Formal: First, we make a text embedding for the entity pair, which we call e_t. Then, we get the embedding for each character in the text, called e_s, and use them as input for a GRU. Formal: Then, we calculate the entity pair embedding for the text using the same BiGRU. Formal: Finally, we add the entity pair embedding to the embedding for the text, and the result is the text embedding for the text. Formal: You can check out more details about the pre-processing in Section 5. Formal: Figure 1 gives you an idea of how we create the text embedding for each entity pair in the text generation task. Formal: The text generation task is done by running three sequence-to-sequence models. Form",
        "directory": "C18",
        "filename": "C18-1002.json"
    },
    {
        "casual_text": "We also checked out DialoGPT, which is a model based on GPT-2 and was trained on a huge collection of dialog data. We went with the small version 4 for our evaluation.",
        "formal_text": "DialoGPT We additionally evaluated DialoGPT, which is a GPT-2-based model pre-trained on large-scale dialog corpora. We used the small version 4.",
        "GPT2_formal_text": "= max(0, tanh(u_t)) After that, the model calculates the attention weights for the tokens in the input, like this: Formal: In this paper, we use two different ways of calculating attention weights for the classifier. The first one is a greedy search based on the max-margin method, which is explained in Section 3. We also throw in a batch size of 64. Formal: In this setup, the attention weights are calculated using the k-nearest neighbor (k-NN) algorithm. Formal: We also add another greedy search based on the minimum-error-rate (M-E) algorithm, which is explained in Section 4. Lastly, we use a parameter called β to set the k-nearest neighbor distribution. Formal: We tested our model against four other top-notch models, and the results are in Table 4. Formal: For the Chinese word segmentation task, we trained the model on 20 million sentences. Formal: We used a smaller vocabulary of 3.5 million tokens for the Chinese word segmentation task. Formal: To make sure the segmentation model can handle different types of Chinese words, we trained it with a vocabulary size of 10 million tokens. Formal: After that, we used a GCN with a batch size of 1 for the Chinese word segmentation task. Formal: Formal: We also tried out the word segmentation model on 80 million sentences and tested it on 200 million sentences. Formal: Formal: Lastly, we trained the model on 100 million sentences. Formal: Lastly, we tested it on 200 million sentences. Formal: We also compared our model to the word segmentation model from Jiang et al. (2019). Formal: Formal: Formal: Our model is the second to the one in Jiang et al.'s (2019) paper. Formal: Formal: We also trained the word segmentation model on 1.3 billion sentences and tested it on 1.9 billion sentences. Formal: Formal: Formal: We also trained the word segmentation model on 1.3 billion sentences and tested it on 1.9 billion sentences. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.269.json"
    },
    {
        "casual_text": "If an entity in the dev/test data has already appeared in the training data, we call it a \"seen entity.\" If it hasn’t, it’s an \"unseen entity.\" To figure out how much of the dev/test data’s entities have been seen in the training data, we use something called the \"entity coverage ratio.\" This ratio for a dataset D te is written as r(D te ), and it’s calculated using the equation below.",
        "formal_text": "If an entity in dev/test data has been covered by the training data, we refer it as a seen entity. Otherwise, it is an unseen entity. To quantify the degree to which entities in the dev/test data have been seen in the training data, we define a measurement called entity coverage ratio. The entity coverage ratio of data D te is denoted by r(D te ), which is calculated using the below equation.",
        "GPT2_formal_text": "[i] = {u_i | w_i = {1, . . . , m}. Formal: To check how well our model performs, we measure the average accuracy by dividing the number of correct predictions by the total number of predictions. Formal: In this paper, we focus on two main things: how well our model picks the right entities, and how accurate it is at identifying and labeling those entities. We’re also looking at how the entity identification task compares to other NLP tasks, as outlined in Section 3. Formal: Lastly, we check if our model can pick the right entities from the list. This is super important for real-world applications, where the list of possible entities could be hundreds of thousands of entries long. Formal: We’ll give a quick overview of our dataset, the datasets we tested, and the results of the experiments we ran. Formal: Figure 1 gives an example of how we align entities. In this case, the entity alignment we’re looking at is an entity \"h\" that’s part of a sentence pair. Formal: For each entity \"h\" in a sentence pair, we find all the entities related to it, including entities that aren’t even in the pair. Then, we match up all these entities, using their embeddings to form their representations. Formal: For each pair of entities \"h\" and \"e\" in the pair, we identify all the entities related to them. Formal: Figure 2 shows the entity alignment process for the entity \"h\" in the pair. We use this to find all the entities related to \"h\" in the pair. Formal: Once we have the entities and their embeddings, we figure out the sequence of embeddings that best represent the entities. Formal: To find the best embedding, we compare the embeddings of all the entities in the pair to their embeddings from a list of possible entities. Formal: In this paper, we’ll show how our model can identify and label entities. Formal: Formal: In this part, we’ll explain how we aligned entities, and in the next part, we’ll explain how we labeled entities. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-short.125.json"
    },
    {
        "casual_text": "In Section 5, we’ll find out that our exact algorithm is usually way too heavy on the computer when dealing with a lot of sentences or a lot of features. So, now we’re going to introduce two",
        "formal_text": "We will see in Section 5 that our exact algorithm is often too computationally expensive in practice to be used with either a large number of sentences or a large number of features. We now present two",
        "GPT2_formal_text": "ed (t, a) = arg max t a, where arg max is the total number of tokens in the input sequence. This means the attention matrix isn't fully unique to each token. So, we set the attention matrix to be the sum of the attention masks from the last hidden state of the token. Formal: The hidden state h t ∈ R lp is a vector with dimensions d t. The output, which is a vector with d dimensions, is calculated using a softmax layer. Formal: To create the hidden state H t ∈ R lp, we use a maximum entropy (MLE) process with a window size of 1. We start with the hidden state h t = (0, 1), which is in R lp. Formal: The input, X, is an n-dimensional vector with dimensions d. The hidden state h t is created by combining the word embeddings from the first and last hidden states of the token t. Formal: We calculate the attention matrix M t ∈ R lp using the LSTM layer. Formal: We then figure out the attention mask α t using the second hidden state h t. Formal: Finally, we return the final hidden state H t ∈ R lp. Formal: To learn the attention weight α t, we use the Jensen-Shannon divergence between the hidden states of the token and the word embedding. Formal: Let’s dive into the neural network setup of the attention-based model. Formal: The network setup includes three main parts. Formal: The first part is the Bi-LSTM layer with an attention mechanism. Formal: The second part is the cross-attention mechanism. Formal: The third part is the linear layer. Formal: The network then uses a softmax function to calculate the hidden state h t. Formal: Finally, we return the hidden state h t ∈ R lp. Formal: We start by creating the input vector x using the first hidden state h t. Formal: Then, we apply the attention mechanism to the token and the word embedding, using the last hidden state h t. Formal: We then use the cross-attention mechanism to learn the attention weight α t. Formal: Finally, we return the final hidden state h t ∈ R lp. Formal: We use the first hidden state h t to initialize the network. Formal: Next,",
        "directory": "D11",
        "filename": "D11-1004.json"
    },
    {
        "casual_text": "We've expanded this architecture to work in a multi-view way, where we handle both the context and the response from two different angles. Here, we'll start by giving a quick overview of the word sequence model. After that, we'll dive into the details of the utterance sequence model and the multi-view response selection model.",
        "formal_text": "We extend this architecture in a multi-view manner, which jointly models the context and response in two views. In this section, we first briefly describe the word sequence model. Then we introduce the utterance sequence model and multi-view response selection model in details.",
        "GPT2_formal_text": "= {q, t} |T| ∈ Rn×d isomorphism matrix. Here, q is the number of unique sentences, t is the total number of words in the sentences, and d is the dimension of the sentences. The token embedding matrix is built by applying softmax to the embeddings of each token. Formal: For training, we grab the average of the standard deviation for both the source and target sides of the training data, like this: Formal: For training, we get the average of the standard deviation for the source side. Formal: For training, we get the average of the standard deviation for the target side. Formal: So, the training data is basically the average of the standard deviations across both the source and target sides. Formal: The same idea applies to the cross-entropy loss. Formal: For training, we get the average of the standard deviations for the source side. Formal: For training, we get the average of the standard deviations for the target side. Formal: Formal: Formal: We can also think of it like this: Formal: The parameters θ (λ) are like an average across the source and target sides, like this: Formal: Formal: And it’s based on the log-likelihood. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "We can use the idea that when you multiply two matrices, P and Q, the rank of the resulting matrix is no bigger than the smallest rank of P or Q. So, rank(P Q) ≤ min rank(P), rank(Q). This means we can figure out the upper limit for rank(T) in equation (4) based on that.",
        "formal_text": "We utilize the fact that the rank of product of two matrices P and Q is upper bounded by the minimum of rank(P) and rank(Q), i. e. , rank(P Q) ≤ min rank(P), rank(Q). Thus, the upper bound on rank(T) in eq. (4) can be determined by",
        "GPT2_formal_text": "ed is the average attention vector for the target token. Each attention head has an output dimension of 3, and we assign it to the input by running it through an attention layer. This setup means we can combine the hidden states from the different heads. Formal: A binary classifier (like the one in Equation 1) gets trained to predict the probabilities for different label classes, like \"A\" for all other classes, \"B\" for the positive class, and so on. Formal: Here, α represents the weight for the binary classifier. Formal: To make sure the loss functions stay stable, we use a linear-chain CRF layer with a batch size of 128. Formal: To make things fair, we set β and γ to 1 for the positive and negative classes. Formal: Lastly, to prevent overfitting, we use a linear layer to normalize the input data. Formal: Formal: Here, f_α(i) and f_β(i) are the actual probabilities from the binary classifier and the multi-head attention mechanism, respectively. Formal: We trained the model with the Adam optimizer (from Kingma and Ba, 2014) on a single Nvidia Tesla V100 GPU with an initial learning rate of 1e-5. This setup had a batch size of 256, with a 4x higher learning rate. Formal: We tested the model on three different datasets, each with its own set of labels. Formal: For each dataset, we picked the class label with the highest probability from the training data. For the positive dataset, we used the exact same training data as the negative dataset. Formal: Formal: To check how well the model was learning, we looked at its perplexity on the validation set. Formal: Table 3 shows the perplexity results for all three datasets. The results are pretty consistent across different models. Formal: Table 4 shows how the model performs on the validation set. Formal: Formal: Formal: Formal: Formal: Table 5 shows the validation set results. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.94.json"
    },
    {
        "casual_text": "The main goal of D-MILN's final objective function is a mix of document-level loss and some extra regularizations to keep things diverse. To keep it simple, let's just talk about the objective function for one document:",
        "formal_text": "The final objective function of D-MILN is a combination of document-level loss and diversified regularizations. To minimize clutter, we describe the objective function for a single document:",
        "GPT2_formal_text": "= { (h, t, a, r, u, l, i, l, o, p, q, i)} | a=1, r=1, u=1, l=1, o=1, p=1, q=1, i=1, l=1, o=1, p=1, q=1, i=1 }  \n\nThe output from the attention layer is just the group of the most important words. Formal: The relevance layer is a really good representation of the input that’s been trained on the whole dataset. Formal: Each layer in this model is made up of a mix of the input representation and a bias term. Formal: This equation is super important because it’s key to figuring out the answers for the questions in the test set. Formal: To make it clearer, let’s just say d is the size of the input representation. Formal: The input representation is created by combining a bunch of hidden vectors that come from different layers in the model. Formal: The importance vector for a question p, which we call \"m h p,\" is created by multiplying the representation of p and the bias term h p. Formal: So, basically, this equation is saying that the importance vector for a question p is the result of multiplying a bunch of different representations together. Formal: To make things more clear, we’ll call d h p the size of the input representation, and we’ll refer to the input representation d h p as \"the input.\" Formal: Basically, it’s just a way to represent the input by multiplying it with a bunch of different representations. Formal: We’ll use the equation to figure out the answers for the test set questions. Formal: Formal: The model can handle multiple questions at once by using multiple hidden representations. Formal: The relevance layer for the question p is a weighted sum of these relevance vectors. Formal: If you want to see how this works in practice, check out Section 4. Formal: Formal: Lastly, the final input representation is just the group of the most important words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.570.json"
    },
    {
        "casual_text": "We can use the hidden representations H, which are based on individual tokens, to make predictions for the entire document.",
        "formal_text": "The token-wise hidden representations H can then be used to make prediction based on the whole document.",
        "GPT2_formal_text": "= 1.0; p_mask = max_mask; w = [x_w1, ..., x_wK]; h_mask = max_mask; a_mask = max_a_mask; b_mask = max_b_mask; a_a_mask = max_a_a_mask; b_a_a_mask = max_b_a_a_mask; a_r_mask = max_r_mask; b_r_mask = max_b_r_mask; a_ct_mask = max_ct_mask; b_ct_mask = max_b_ct_mask; a_ff_mask = max_ff_mask; b_ff_mask = max_b_ff_mask; a_br_mask = max_br_mask; b_br_mask = max_b_br_mask; a_bn_mask = max_bn_mask; b_bn_mask = max_b_bn_mask; a_bn_a_mask = max_bn_a_mask; b_bn_a_a_mask = max_b_bn_a_mask; a_bn_a_r_mask = max_bn_a_r_mask; b_bn_a_r_mask = max_b_bn_a_mask; a_bn_r_mask = max_bn_r_mask; b_bn_r_mask = max_b_bn_mask; a_br_r_mask = max_br_r_mask; b_br_r_mask = max_b_br_mask; a_br_r_a_mask = max_br_r_a_mask; b_br_r_a_mask = max_b_br_a_mask; a_br_r_a_r = max_br_r_a_mask; b_br_r_a_r = max_b_br_a_mask; a_br_r_b_mask = max_br_r_b_mask; b_br_r_b_mask = max_b_br_b_mask; a_br_r_b_r = max_br_r_b_mask; b_br_r_b_mask = max_b_",
        "directory": "clinicalnlp",
        "filename": "2022.clinicalnlp-1.2.json"
    },
    {
        "casual_text": "Summarizing opinions and arguments has been a hot topic lately. People are really into breaking down what’s being said in reviews (like Amplayo and Lapata, 2021, or Elsahar et al., 2021) or in argumentative texts (Wang and Ling, 2016; Syed et al., 2020). KPA is part of this trend, but it brings in a more numerical approach to show how opinions are spread out in the data being analyzed. If you want to dive deeper into how KPA connects to argument clustering and summarization, check out Bar-Haim et al. (2020a, b).",
        "formal_text": "Opinion and argument summarization. Recently there has been an increasing interest in summarizing opinions expressed in various reviews (Amplayo and Lapata, 2021; Elsahar et al. , 2021) or argumentative text (Wang and Ling, 2016; Syed et al. , 2020). KPA contributes to this line of work by adding a quantitative dimension which reflects the distribution of opinions in the examined data. For a detailed discussion of the relation between KPA and argument clustering and summarization see (Bar-Haim et al. , 2020a, b).",
        "GPT2_formal_text": "(s_i, t_i) ∈ R ∞ for each sentence s_i. Basically, it's about how many words from the source text are in the target text. The length of the target text is figured out using the softmax function. Formal: To calculate the score for a sentence s_i, we start by taking the cosine similarity between its embedding vector, y_i, and its word embedding, x_i. Then, we add up the cosine similarity for all the tokens in s_i. Formal: To find the score for a word w_i in the source sentence s, we start by calculating its cosine similarity using the cosine similarity for the embedding, x_i, and the word embedding, y_i. Formal: In short, it's about how similar the word embedding, x_i, and the word embedding, y_i, are, as calculated by cosine similarity and the same cosine similarity for the embedding, x_i and y_i. Formal: Formal: Lastly, we calculate the score for the target sentence by multiplying the cosine similarity for the embedding, x_i, and the word embedding, y_i, to get the final score. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "argmining",
        "filename": "2021.argmining-1.16.json"
    },
    {
        "casual_text": "GPU was first used in topic modeling back in 2011 by Mimno et al. They used it to focus on words that appeared together in the same documents a lot, based on how often they showed up together in the whole dataset. Later, in 2013, Chen et al. used GPU to handle a problem in topic modeling where prior knowledge about a domain could mess things up. They did this by boosting the importance of rare words in the knowledge sets. But even with these improvements, they were still just working with individual words.\n\nMost topic models, like LDA, treat topics as a mix of single words and assume that the order of words doesn’t really matter. However, some researchers have tried to include word order by using n-gram language models. For example, Wallach in 2006 came up with the Bigram Topic Model (BTM), which combines bigram statistics with topic modeling to better represent documents. Then, Wang et al. in 2007 took it a step further with the Topical N-gram Model (TNG), which is like a more advanced version of BTM. It generates words in order by first picking a topic, then deciding if it’s a unigram or bigram, and finally selecting the word from a distribution that’s specific to the topic and whether it’s a unigram or bigram.\n\nEven though the \"bag-of-words\" assumption, which means ignoring word order, isn’t always perfect in real-world applications, it’s still much faster and easier to work with than more complex models that try to account for word order when finding important n-grams.\n\nOur approach is a bit different from these earlier works in two main ways. First, we stick with the \"bag-of-words\" or more accurately, the \"bag-of-terms\" assumption.",
        "formal_text": "GPU was first introduced to topic modelling in (Mimno et al. , 2011), in which GPU is used to concentrate words with high co-document frequency based on corpus-specific co-occurrence statistics. Chen et al. (2013) applied GPU to deal with the adverse effect of using prior domain knowledge in topic modeling by increasing the counts of rare words in the knowledge sets. However, these works still use only individual words. Topics in most topic models like LDA are unigram distributions over words and assume words to be exchangeable at the word level. However, there exists some work that tries to take word order into consideration by including n-gram language models. Wallach 2006proposed the Bigram Topic Model (BTM) which integrates bigram statistics with topic-based approaches to document modeling. Wang et al. (2007) proposed the Topical N-gram Model (TNG), which is a generalization of the BTM. It generates words in their textual order by first sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Although the \"bag-of-words\" assumption does not always hold in real-life applications, it offers a great computational advantage over more complex models taking word order into account for discovering significant ngrams. Our approach is different from these works in two ways. First, we still follow the \"bag-of-words\" or rather \"bag-of-terms\" assumption.",
        "GPT2_formal_text": "ed: This means the model doesn't use attention at all, and the feature vectors it learns are all single-valued. Formal: The main thing to remember about the non-linear layers is that each time you use them, you'll get a new vector for the same input. This means that the layer can't be reused for any other inputs. If you want to dive deeper into this, check out the paper by Zhang and Lu in 2019. Formal: The full model is defined as: Formal: The combined output vector for the specific input x i is calculated using the embedding of the head h, and the binary cross-entropy between the embedding of the head h and the embedding of the input x i. Formal: We've set up a simple linear classifier using a linear layer and an attention mechanism. The head h is set up to focus on the most important input feature x i. Formal: Our approach is based on the concept of sparse modeling, which is often used in deep learning. This idea is kind of like what Duchi et al. did in 2020, where they used a sparse modeling approach for a masked language modeling task. Formal: The embedding for the head h is calculated using the embedding of the head h, and the binary cross-entropy between the embedding of the head h and the embedding of the input x i. Formal: We're using a linear classifier to predict the probabilities for each layer. The head h is set up to focus on the most important input feature x i. Formal: In this setup, we're using a binary cross-entropy loss function, and the model is trained using cross-entropy loss. Formal: We're using an attention mechanism to handle the input features. Formal: To understand how this hierarchical attention thing works, let's look at the input embedding of the head h in a simpler way. Formal: The embedding for the head h is calculated using the embedding of the head h, and the binary cross-entropy between the embedding of the head h and the embedding of the input x i. Formal: The head h is trained using cross-entropy loss. Formal: Basically, the main idea here is that the head h is trained to focus on the most important input feature x i. Formal: This approach is also similar to what Wang and his team did in 2020, where they combined a dense modeling approach with sparse",
        "directory": "C14",
        "filename": "C14-1063.json"
    },
    {
        "casual_text": "When we look at the bilingual transfer learning models (check Table3 (2)) compared to the Arabic-only models (Table3 (1)), we see that using MBERT gives us an average F1 improvement of 3, and GIGABERT gives us 2.5. This proves that bilingual transfer learning works pretty well. Both the PIPELINE and JOINT approaches show similar improvements, which means it's the extra training data, not the model itself, that makes the difference. \n\nWhat's cool is that MBERT and GIGA-BERT give us pretty much the same gains, showing that both models transfer knowledge from English to Arabic equally well. Even though MBERT starts a bit behind in Arabic performance (by 15.29 F1), it catches up. It's kind of unexpected since GIGABERT is a bilingual model made just for English-Arabic. We're definitely going to look into this next.",
        "formal_text": "On bilingual transfer learning: Comparing the bilingual transfer learning models (Table3 (2)) to the Arabic-only models (Table3 (1)), we observe 3 and 2. 5 average F1 improvements when using MBERT and GIGABERT respectively. This shows the effectiveness of the bilingual transfer learning. The PIPELINE and JOINT approach shows similar gains. This indicates that it is the additional training data rather than the learning model that leads to the gain. Interestingly, using MBERT and GIGA-BERT gives similar gains, which shows that the transferability of the these two models in English to Arabic is about the same (though with MBERT lagging in baseline Arabic performance by 15. 29 F1). It is surprising given that GIGABERT is a bilingual model trained specifically for English-Arabic. We plan to investigate this as an immediate next step.",
        "GPT2_formal_text": ". Formal: The baseline model has an LSTM network with hidden layers and an attention-based sequence-level decoder. The model has an attention mechanism and is trained on an MLP using the cross-entropy loss function. Formal: The task is to predict the intent label for a given premise sentence. Formal: We started by training our model using a dataset with 49,500 premise pairs. Formal: After that, we fine-tuned it on a slightly different set of 49,500 pairs. This setup is what we call the strong baseline. Formal: The MLP decoder acts like a simple MLP, and the hidden layer just maps the input words to a fixed-length vector. Formal: But when we apply the LSTM to the premise sentences, the output vector for the target word is calculated based on the previous hidden states, so we end up with a new hidden state. Formal: This new hidden state then helps the decoder figure out the intent label. Formal: This whole process is repeated for all the target words in the premise. Formal: The end result is the final prediction of the target word. Formal: For instance, in Figure 1, the input words are \"Mary,\" \"Mary gives presents,\" and \"Mary chooses presents.\" The output for \"Mary\" in this scenario is \"Mary chooses presents.\" Formal: Formal: We ran experiments on the target dataset and the strong baseline using two datasets: the CNNDM dataset (from Schumann et al., 2017) and the MLDoc dataset. Formal: We grabbed the last sentence from the target dataset and compared it to the final output of the strong baseline. Formal: Formal: We used the MLP decoder with the first layer of the neural LSTM. Formal: We took the MLP decoder, trained it on the MLDoc dataset, and used the target dataset as our validation set. Formal: Finally, we used the strong baseline for the target dataset and tested it on the target dataset. Formal: Finally, we used the strong baseline for the target dataset and tested it on the target dataset. Formal: We trained the model on the target dataset and tested it on the target dataset. Formal: We trained the model on the target dataset and tested it on the target dataset. Formal: We trained the model on the target dataset and tested it on the target dataset. Formal: We trained the model on the target",
        "directory": "crac",
        "filename": "2021.crac-1.10.json"
    },
    {
        "casual_text": "To figure out how premises connect to the conclusion, we first need to pinpoint the targets in the premises. This whole idea of finding target phrases in argumentative text was brought up by Bar-Haim et al. (2017). Here, we're dealing with it using BIO sequence labeling, which means we classify each word as the start, part of, or not part of a target. Since identifying premise targets isn't our main goal, we just grab a top-notch neural sequence tagger (from Akbik et al., 2018) and train it on a claim stance dataset. Then, we use it to automatically tag targets in all the input premises.",
        "formal_text": "To model the relation between premises and conclusion target, we first identify the premises' targets. The task of identifying target phrases in argumentative text has been introduced by Bar-Haim et al. (2017). We here tackle it as BIO sequence labeling, classifying each token as being the beginning, inside, or outside of a target. Since premise target identification is not our main focus, we simply train a state-of-the-art neural sequence tagger (Akbik et al. , 2018) on the claim stance dataset and then use it to automatically annotate targets in all input premises. 2",
        "GPT2_formal_text": "is the number of layers that are being used to identify attention, and the value of f(n) is calculated using the absolute value of the dot product between the output token and the attention matrix. Formal: To make sure the model doesn't just copy the same input over and over, we suggest using the attention weights to create new tokens. Specifically, for the first input token t, the attention weight t is made up of the attention weight for the sequence of tokens that comes before t. Formal: The total attention weight, ε_i, for the i-th token is calculated like this: Formal: We're using the output encoding s_i = h_t−1 = w_t−1 and the output attention weight, a_i = a_t = h_t, for the i-th token. The actual attention values for this token are... Formal: So, the attention weights for the tokens in the current input sequence are... Formal: Using this approach, the model learns to generate a different token for each token. The model learns to generate different tokens because it's trying to create different types of text. This way, the model can generate a variety of text styles that work with the input sequences. Formal: For the token type we want to generate, we look for the tokens that show the same pattern to pick from the list of tokens in the input sequence. Formal: In the next part, we'll talk about the setup we used for our experiments. Formal: We trained our model using the attention weights from the original GAN setup (Bengio et al., 2014). This setup includes a global hidden layer and a copy mechanism. Formal: We've added two new tokens to the original GAN, T1 and T2, to make the model better at generating text styles. Formal: We've also added two more tokens to the original GAN, T3 and T4, to help the model learn and produce more varied text styles. Formal: If we're not using the copy mechanism, we don't need to add any new tokens to the model. Formal: Lastly, we've tried out different values of the embedding dimension for the GAN, from 100 to 100,000, and found that the best one works best for our model. Formal: Using the size of the embedding dimension, we set the output embedding dimension to d_embedding. This is just the number of dimensions in the",
        "directory": "acl",
        "filename": "2020.acl-main.399.json"
    },
    {
        "casual_text": "For the pseudo-disambiguation task, the test set is made up of tuples like (R, t, r, r'). To create this test set, we followed the same approach that Rooth et al. (1999) and Erk et al. (2010) used.",
        "formal_text": "A test set for the pseudo-disambiguation task task consists of tuples of the form (R, t, r, r'). To construct a test set, we adapted the process used by Rooth et al. (1999) and Erk et al. (2010).",
        "GPT2_formal_text": "= [x_i, y_i], category_ids = [c_i, f_i], source_ids = [s_i, t_i], target_ids = [t_i, x_i], token_ids = [t_i, y_i], for i in range (l): for j in range (n-l): token_i = [x_i, y_i], source_i = [s_i, t_i], category_i = [c_i, f_i], target_i = [t_i, x_i], summary = [s_i, t_i], class_type = [c_i, f_i], source_type = [s_i, t_i], context = [s_i, t_i], data = [s_i, t_i], class = [c_i, f_i],\n\nIn short, these are the different types of instances, the attributes, and the input, along with how they relate to each other. Formal: We start by grabbing all the instances of the class we want to classify, which we call `e_i`. Then, for each instance, we do something like this: Formal: This process is repeated for each instance `e_i`, and the result is `e_i`, which is a list of instances. Formal: The class `c_i` has a special token `i` that helps us handle instances with multiple attribute values. Formal: The source attribute is the class `c_i`. Formal: The target attribute is the class `c_i`. Formal: For the `c_i`, we use two main types of inputs: Formal: The `c_i` instance is either the source instance (`e_i`) or the target instance (`e_i`). Formal: The `c_i` instance is either the source instance (`e_i`) or the target instance (`e_i`). Formal: The `c_i` instance is either the source instance (`e_i`) or the target instance (`e_i`). Formal: The `c_i` instance is either the source instance (`e_i`) or the target instance (`e_i`). Formal: The `c_i`",
        "directory": "E12",
        "filename": "E12-1038.json"
    },
    {
        "casual_text": "In this part, we’ll dive deeper into the topic with both numbers and descriptions, plus take a closer look at the mistakes and the stuff that’s still tricky.",
        "formal_text": "In this section, we further provide more insights with quantitative and qualitative analysis, as well as error analysis for the remaining challenges.",
        "GPT2_formal_text": "ed_mode(P * Q) = softmax(h_i * h_i + b_i)  \n\nIn the end, both directions of the attention vector are combined using a normalization function. Formal: (1) Let's say the softmax function for the input x (i) is used to create a logistic vector that represents the relationship between the i-th and j-th images. Formal: (2) Also, we can use a convolutional neural network (CNN) to pull out the key features for the document. Formal: (3) The attention mechanism is applied to the output representations of the tokens in the document. Formal: (4) We can use a multi-head attention mechanism to combine the attention from both directions of the attention vector, giving us the final representation for the document. Formal: (5) Finally, to combine the attention, the output from the MGRU is combined with the output from the CNN. Formal: (6) The final input of the embedding module is the query vector, which we use as the key feature to represent the relationship between the query and the document. Formal: (7) Formal: (8) We can represent the interaction between the query and the document using the hidden states from a CNN, as explained in Algorithm 1. Formal: (9) For training, we tweak the network parameters using a linear SVM with an inner product to create a multi-class classification model. Formal: (10) Finally, we use the validation loss to measure how well our model is doing. Formal: (11) Formal: (12) Formal: (13) Formal: (14) Formal: (15) Formal: (16) Formal: (17) Formal: (18) Formal: (19) Formal: (20) Formal: (21) Formal: (22) Formal: (23) Formal: (24) Formal: (25) Formal: (26) Formal: (27) Formal: (28) Formal: (29) Formal: (30) Formal: (31) Formal: (32) Formal: (33) Formal: (34) Formal: (35) Formal: (36) Formal: (37) Formal: (38) Formal: (39) Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.361.json"
    },
    {
        "casual_text": "In this part, we're testing a language model (LM) trained with a large-margin criterion alongside a strong Chinese-to-English neural machine translation (NMT) system. The NMT model was trained on 2 million parallel sentence pairs. Following the approach of Shen et al. (2016), we used the NIST 06 newswire section (616 sentences) for development and the NIST 08 newswire section (691 sentences) for testing. We trained the model using the OpenNMT-py 2 package with its default settings: batch size is 64, word embedding size is 500, dropout rate is 0.3, target vocabulary size is 50K, and we trained for 20 epochs. After that, we got a minimum development perplexity of 7.72.",
        "formal_text": "In this section, we experiment the large-margin criterion trained LM with a competitive Chineseto-English NMT system. The NMT model is trained from 2M parallel sentence pairs. Following Shen et al. (2016), we use NIST 06 newswire portion (616 sentences) for development and NIST 08 newswire portion (691 sentences) for testing. We use OpenNMT-py 2 package with the default configuration to train the model: batch size is 64; word embedding size is 500; dropout rate is 0. 3; target vocabulary size is 50K; number of epochs is 20, after which a minimum dev perplexity of 7. 72 is achieved.",
        "GPT2_formal_text": "= { q, c, a } ∈ R n φ and η_c = 0. Here, p is the probability of picking a \"relation_i\" from the set R_i, and η_c is the same thing but for the other relations in R_i. Formal: For each set of relations in R, we use something called a Markov blanket, M, to figure out the chance that a relation r_i will be in the i-th spot. This probability is also part of the total probability we get from the cross-entropy between p and M. Formal: We set M to 10^-9. Formal: We use the cross-entropy with p as a positive example and the cross-entropy with M as a negative example. Formal: We use the softmax function, let's call it u_s, to make sure u_s is positive when we calculate the probability of a relation r_i happening. Formal: The training process of M, like this: Formal: We pick the relation with the highest probability, based on its cross-entropy with p, and use the softmax function to make sure u_s is positive. Formal: For the \"positive\" examples, we calculate the cross-entropy with p using the following formula: Formal: Here, p is the probability of picking a \"relation_i\" from the set R_i, and η_c is the same thing but for the other relations in R_i. Formal: Finally, we use the cross-entropy with M as a positive example and the cross-entropy with p as a negative example. Formal: The training process for M, like this: Formal: We pick the relation with the highest probability, based on its cross-entropy with p, and use the softmax function to make sure u_s is positive. Formal: For the \"positive\" examples, we calculate the cross-entropy with p using the formula: Formal: We use the cross-entropy with p as a positive example and the cross-entropy with p as a negative example. Formal: Lastly, we use the cross-entropy with M as a positive example and the cross-entropy with p as a negative example. Formal: Formal: We use the cross-entropy with p as a positive example and the cross-entropy with p",
        "directory": "D18",
        "filename": "D18-1150.json"
    },
    {
        "casual_text": "Hey, just a heads-up: in most SENSEVAL datasets, they usually give you separate training and test data. But for some target words in SENSEVAL1, they didn't have both. So, we decided to mix the training and test data together into one big dataset and run 10-fold cross validation instead.",
        "formal_text": "Note that, although generally in the SENSEVAL datasets, training and test data are provided separately, a few target words from SENSEVAL1 do not have both training and testing data. Thus, we opted to combine the training and test data into one dataset, and then perform 10-fold cross validation experiments.",
        "GPT2_formal_text": "ed (E_mask) and attention_normalized (E_norm) that are explained in Section 3.4. The classification part, which is often called multi-class classification or multi-label classification, is super important and can be handled pretty effectively using this approach. Let’s look at some examples of multi-class classification using the same keyphrases. We’re using the example of ICD coding (ICD) and turning it into a word-level classification problem. \n\nThe \"ICD Code\" examples come from the \"Positives\" and \"Negatives\" parts of the ICD-9 database. The main focus is on identifying the code. You can find more examples in Table 1. In this part, we’ll explain how to implement this multi-class classification task.\n\nThe main goal of multi-class classification is to tell apart positive and negative examples based on a set of labeled data. The labels come from the types of knowledge that were given to the classifier. The classifier has a few parts, like a text encoder, a semantic parser, and an entity recognition module. It’s trained using a pre-defined number of labeled examples.\n\nFor each labeled example, the classifier uses a binary classification method. In this method, the classifier labels the positive and negative instances (e1 and e2) based on the type of entity they belong to. Basically, the classifier figures out the label for each instance, like: Formal: for every example x, the classifier gives a label y = {y1, ..., yT}, which is the entity type. Formal: The label y for an instance is determined by the type of entity it belongs to. Formal: Each instance in the dataset gets its own label. Formal: The label y for an instance is determined by the type of entity it belongs to. Formal: The label y for an instance is determined by the type of entity it belongs to. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D09",
        "filename": "D09-1020.json"
    },
    {
        "casual_text": "We worked with the data from the shared task on identifying offensive language in Dravidian languages, organized by Chakravarthi and others in 2021 (and also mentioned in their earlier works in 2020). The data was nicely annotated at the comment or post level. The training data included 35,139 samples for Tamil, 16,010 for Malayalam, and 6,217 for Kannada. You can check out the details in Table 1, Table 2, and Table 3.",
        "formal_text": "We used the data provided by the organizers of shared task on Offensive Language Identification in Dravidian Languages (Chakravarthi et al. , 2021 (Chakravarthi et al. , , 2020b Hande et al. , 2020; Chakravarthi et al. , 2020a), which have been annotated well at comment or post level. The numbers of Tamil, Malayalam and Kannada training data are 35139, 16010 and 6217, respectively. The statistics of data are shown in Table 1, Table 2 and Table 3.",
        "GPT2_formal_text": "= {r_i, r_j} from the last layer, along with the embedding for the word embedding (e_w). \n\nFor comparison, we ran some experiments with different values for η, ranging from 1e-5 up to 10e-5. We picked the best values based on how well they worked in the CKY-NER model we mentioned earlier. Formal: The idea behind MLE is to predict the next word in a sentence by looking at the embedding vectors from the last hidden layer and the embedding for the word we just saw. Formal: After checking out a few examples, we figured out that MLE works best when η is set to 1. Formal: But when η is really big, like 10 or more, the performance takes a nosedive. In those cases, you don’t need MLE to predict the next word. Formal: We tested different values of η, ranging from 1e-5 up to 10e-5, with some basic tweaks like setting the dropout rate to 0.1. Formal: The results are in Figure 2. Turns out, the embedding with the highest embedding value is pretty close to the one that gets the highest score. Formal: Here’s a quick look at a histogram of the results based on the number of tokens in the current sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "dravidianlangtech",
        "filename": "2021.dravidianlangtech-1.21.json"
    },
    {
        "casual_text": "For each word, we use word embedding to capture its grammar and meaning, and position embedding to show where it is in the sentence. Then, we combine both the word embedding and position embedding to create the input vector for that word, which goes into the CNN. (Check out Figure 2 for more details.)",
        "formal_text": "For each word w i, we use word embedding to encode its syntactic and semantic meanings, and use position embedding to encode its position information. We then concatenate both word embedding and position embedding to form the input vector of w i for CNN. (See Figure 2. )",
        "GPT2_formal_text": "ed: 0x0 = [ 1, 1 ]  \nFigure 2: This shows how we handle the [ 0, 1 ] token during training. Formal: Basically, we use a convolutional neural network (CNN) to break down the input into a sequence of feature vectors. This sequence is fed into a fully-connected layer, which then spits out the actual responses. Formal: The network then uses a sigmoid function to calculate the log probability, which is then used to calculate a score for each word in the input. Formal: This system is pretty similar to the one mentioned in (Zhou et al., 2019), but they use a different setup with attention. Formal: Instead of using a softmax function to predict the current hidden state, they use a sigmoid function to calculate the log probability, and then they use a linear layer to calculate the score for each word. Formal: We’ve tweaked their algorithm to handle the [ 0, 1 ] token separately. Formal: So, in our version, we’ve added two more steps for the [ 0, 1 ] token. Formal: First, we calculate the log probability for the [ 0, 1 ] token. Then, we use a linear layer to calculate the score for each word in the input. Formal: For more info, check out (Xu et al., 2019). Formal: We also train our model using a simple convolutional neural network (CNN) to handle the [ 0, 1 ] token. Formal: This setup is kind of like what Zhou et al. (2019) did, but they added a linear layer to the convolutional function to calculate the score for each word. Formal: Formal: Our model is built using the same settings as the original Zhou et al. (2019) model, so we’re comparing our results with theirs. Formal: Formal: To show how well our model is doing, we’re using the test set from (Xu et al., 2019), which has 39 human-written responses. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D17",
        "filename": "D17-1186.json"
    },
    {
        "casual_text": "Hey, just a heads-up: SKT and SKT † are basically the same network, but they're tested with different types of knowledge—prior knowledge for SKT and posterior knowledge for SKT †. That's why the KL divergence stays the same.",
        "formal_text": "Note that SKT and SKT † are the same network tested with prior and posterior knowledge selection, respectively. Hence, the KL divergence is same.",
        "GPT2_formal_text": ") , the type of entity e_t, and the target entity e_t. This is done using a specific method. Formal: For example, we can take the sentence s = (s1, s2, ..., sn) and split it into n smaller sentences. Each of these sentences is then combined into one big sentence. Formal: We can also go a step further and figure out the F1 score for the model using a mix of model predictions from different entities. Formal: After that, we check the F1 score of the combined model at step k. Formal: The scores from these three steps are averaged out and then averaged across all entities. Formal: Also, we do this for a bigger group of target entities, let's call it m, and then combine their scores to get the final score. Formal: Lastly, we compare the F1 score of the model trained with this method versus the one trained with a teacher model. Formal: In our setup, we do all this for k = 2, 4, and 6, and the average result is calculated as ∆F1(k) = |D|k/2. Formal: Formal: Formal: For more info on how we calculate F1 scores, check out Equations 8-9. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.275.json"
    },
    {
        "casual_text": "To make sure that the sounds at the edges of words can actually connect to each other, we use a special table called the phoneme connectivity table. This table lists all the pairs of sounds that can connect in Korean, based on how the letters at the ends of words change. The table helps us figure out which sound combinations are allowed in the language.\n\nWhen we convert a word into its possible sound sequences, we often end up with a bunch of options. We then organize these sequences into something called a phoneme graph. This graph helps us find the right sequence of sounds for a given sentence. The connectivity check goes through this graph and weeds out any sequences that don’t follow the rules of Korean grammar.",
        "formal_text": "To verify the boundary phonemes' connectablity to one another, the separate phoneme connectivity table encodes the phonologically connectable pair of each morpheme which has phonologically changed boundary graphemes. This phoneme connectivity table indicates the grammatical sound combinations in Korean phonology using the defined left and right connectivity information. The morpheme-to-phoneme conversion can generate a lot of phoneme sequence candidates tbr single morpheme. We put the whole phoneme, ~equence candidates in a phoneme graph where a correct phoneme sequence path can be selected for input sentence. The phoneme connectivity check performs this selection and prunes the ungrammatical phoneme sequences in the graph.",
        "GPT2_formal_text": "_t is made up of [M-1, K, U, V, O] where the last two are either 1 or 0, which represent the source and target attention vectors. The score of the log-probability of the training event is calculated by dividing the attention weight by the average attention weight. Finally, the attention weight is calculated as the product of all the attention vectors in the input x_t. Formal: Formal: We randomly pick a group of 50 examples from the original dataset X to create a new training sample from the training set Y. The goal here is to make the attention weights for the new sample as close as possible to the original input. Formal: We also tweak the original model by adding a linear layer to make it better at handling sequential inputs. Formal: Unlike the traditional model, we don’t have to get everything perfect in the early stages because the attention weights are already figured out by the time we reach the first step in training. Formal: As we train, we keep the sequence length fixed at 50 tokens, and we use the same training dataset to test. Formal: When testing, we set the number of times we ask for clarification to 3, and we also set the sample size to 50. Formal: To get the best results, we try to keep the training time as short as possible. Formal: We use the original version of the model that was trained using the CLS algorithm and then fine-tuned using the development set. Formal: After fine-tuning, we run the test again and pick the model with the best performance from the validation set. Formal: Formal: We also use the same training data to fine-tune the model. Formal: Formal: We use the same dataset X to fine-tune the model. Formal: Formal: Formal: We do this for the same number of epochs as the original model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C98",
        "filename": "C98-1107.json"
    },
    {
        "casual_text": "Besides creating phrase alignments, the annotator also needs to give labels to these alignments. We’ve got four labels, which are based on two things: whether there’s a word order difference or not, and whether there are unaligned function words or not. Here’s what each label means and an example for each, shown in Figure 2:\n\n- **REO**: This stands for reordering without any unaligned function words (see Figure 2a).\n- **UFW**: This one is for alignments with unaligned function words (see Figure 2b).\n- **REU**: This label is for reordering that also includes unaligned function words (see Figure 2c).\n- **STD**: This is for structural divergence caused by differences between languages (see Figure 2d).\n\nLet’s break it down with the examples:\n\n- **Figure 2a**: This shows a reordering of the words under the aligned VP nodes. It’s pretty common to see this kind of word order difference between Chinese and English. In Chinese, the PP modifier comes before the verb, but in English, it comes after.\n- **Figure 2b**: Here, there’s an unaligned function word—the English infinitive marker \"to,\" which doesn’t have a match in Chinese.\n- **Figure 2c**: This one has both reordering (like the difference in the order of \"powerhouse\" and \"economy\") and unaligned function words (like \"的\" in Chinese and \"of\" in English).\n- **Figure 2d**: This example shows structural divergence caused by differences between Chinese and English. We’ll talk more about this in Section 4.",
        "formal_text": "In addition to making phrase alignments, the annotator needs to assign labels to phrase alignments. We have four labels that are designed along two dimensions: the presence/absence of word order difference and the presence/absence of unaligned function words. The name and definition of each of the four labels are listed below, and an example for each label is given in Figure 2: a REO, reordering that does not involve unaligned function words (Figure 2a ) b UFW, unaligned function words (Figure 2b) c REU, reordering that also involves unaligned function words (Figure 2c) d STD, structural divergence due to cross-linguistic differences (Figure 2d). . . . Figure 2a is an example where there is a reordering of the immediate children of the aligned VP nodes. This is a very typical word order difference between Chinese and English. In Chinese, the PP modifier is before the verb while in English the PP modifier is after the verb. The phrase alignment illustrated by Figure 2b has an unaligned function word, namely the English infinitive marker to, which has no counterpart in Chinese. There are both reordering (difference in the relative order of powerhouse and economy) and unaligned function words (Chinese 的 and English of ) in the phrase alignment in Figure 2c. Figure 2d provides an example where the aligned phrases have structural divergence caused by crosslinguistic differences between Chinese and English, which we will discuss in some detail in Section 4.",
        "GPT2_formal_text": "= {att_1, att_2, att_3, att_4}. Formal: The training data will have a bunch of nouns, and each noun will be matched with a synonym that has some info about it. Formal: We take the self-attended-entity labels from the database and stick them right next to the entities. Formal: For each entity, we get the embedding of its self-attended-entity label (att_i) and the embedding of its entity-specific self-attended-entity label (att_j). This setup means we can tweak the entity embedding by adding extra information. Formal: After we're done with training, we clean up the unlabeled data by removing any entity pairs that are not in the training data. Formal: The set of entity types we need to label for a specific dataset is called the training data. Formal: We want the model to figure out what kind of entity a given context c_i in the input X means. Formal: To do that, we train the model using something called pseudo-labeling. Formal: For the \"Disambiguated Self-attended Entity\" task, we look at the data, figure out what types of entities it covers, and then label those entities with those types. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C14",
        "filename": "C14-1143.json"
    },
    {
        "casual_text": "Okay, so I equals 1 if h(e_i) matches y_i, and I equals 0 if it doesn't. Basically, example-based precision is just 1.",
        "formal_text": "i=1 I, with I = 1 if h(e i ) = y i and I = 0 otherwise. Example-based precision is defined as 1",
        "GPT2_formal_text": ", which is basically a fixed function that tweaks the attention for each word in a tweet. This function helps the model figure out what the tweet is about, giving it a better understanding of what the words are trying to say. To get the embeddings for the words in a tweet, we take the hidden states from the last layer of the RNNG. We can write this as h = (w_s, w_t) x (W_s, W_t) = h(W_s, W_t) x d. Formal: To see how well our model can predict the sentiment of tweets, we use the TweetEval dataset, which was put together by Srikumar et al. in 2019. This dataset has tweets that are labeled with specific sentiment sentiments. For each tweet, we grab the sentiment word embeddings from its tweet embedding matrix, which are h = (w_s, w_t) x (W_s, W_t) = h(w_s, W_t) x d. Formal: To get the attention weights, we grab the last hidden state from the last layer of the RNNG. After that, we plug these weights into the attention matrix, which we get from Equation 3. Formal: Finally, we calculate the average attention weight for each word in a tweet. Formal: We created the TweetEval dataset by matching tweets from Twitter with news articles. The dataset has 567,192 tweets, and the average attention weight for each word is 0.29. Formal: We also looked into how well our model can identify different types of relationships between tweets. For this, we used a smaller dataset with 123,000 tweets, and the average attention weight for each word is 0.05. Formal: We tested our model on the TweetEval dataset to see how well it can generate sentiment responses. Formal: We created two versions of our model: one that uses the attention weight vector from Equations 3 and 4 (with the attention mask set to 0) and another that uses the attention weights from Equations 4 and 6 (with the mask set to 1). Formal: The model we used for the attention-based model (Equation 6) can generate the first result shown in Figure 2. Formal: The model we used for the attention-based model (Equation 3) can generate the second result shown in Figure 2. Formal: The model we used for the",
        "directory": "D13",
        "filename": "D13-1055.json"
    },
    {
        "casual_text": "Tweets are turned into TF-IDF representations and placed in a vector space that matches their cluster.",
        "formal_text": "Tweets are embedded into a vector space of TF-IDF representations within their corresponding cluster.",
        "GPT2_formal_text": "(i), but the pairwise mutual information doesn't help much. So, it's not really important to focus on the exact relationship between the candidates. Instead, we use the function to calculate the hidden features. Formal: From the example, we can guess that the candidate with the highest score is probably the one with the least noise. Formal: The goal of the LDA model is to figure out how important each word is for the whole group of words. Formal: The model uses a bunch of parameters (like n_s, ..., m_s) to describe this importance, and these parameters are learned from data (like the set of source words) in a supervised way. Formal: Let’s say the model tries to predict a specific word from the set {s_i, ..., s_k} using some parameters like n_s, ..., m_s. Then, we also have the sequence of words {s_i, ..., s_k-1} that describe the sentence in this language. Formal: Based on what we know from the training data, we estimate the importance of the words in the source sentence. Formal: Formal: We use a special function f to figure out the importance of each word, and we also have a set of parameters {n_s, ..., m_s} that help us with this. Formal: In this situation, the model is kind of like a sentence-level language model, where the importance of a word depends on its context. Formal: Formal: To figure out how important a word is in a context, we use a fancy algorithm. Formal: Formal: Formal: Formal: The main idea is that the main word in a sentence is usually the one that’s important. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.530.json"
    },
    {
        "casual_text": "A big earthquake hit Aceh province in Indonesia on Tuesday. Lots of houses got messed up, and a bunch of villagers got hurt. The word \"damaged\" might sound vague on its own, but when you add the info about when and where it happened, it makes more sense. We tested our system by hiding these kinds of details from the local conversation, and the results are in Table 5.",
        "formal_text": "A strong earthquake struck Indonesia's Aceh province on Tuesday. Many houses were damaged and dozens of villagers were injured. While the event \"damaged\" is ambiguous with only the context of a single sentence, it becomes much more specific when contextualized with the previous sentence which contains both a time and a location for the event. We evaluate our system with tokens of these types masked from the local discourse with results reported in Table 5.",
        "GPT2_formal_text": "ing = E_a_t/1 E_b_t/1 # p_t_masked is the max attention value for t_masked. Formal: You can find the code here: https://github.com/HIT-SCIR/AttentionMasking Formal: Alright, let’s break down how our new attention mechanism works. First, we need to figure out the hidden vector for the token y_t. We’ll call this vector h_t. Formal: The token representation h_t is calculated using this formula: Formal: The hidden representation h_t for the token y_t is the sum of all the attention vectors for the tokens in the span s_t. For instance, if we look at the source token x_t, the attention vector for the token y_t would be calculated as: Formal: This means the token gets a full attention score of 1/1 for each token in the span s_t. Formal: When we use the attention weight vector to figure out the final token representation, the attention weights for the tokens in the span s_t are added together. Formal: This gives us the final attention weight vector for the token y_t. Formal: Now, if the token isn’t in the span s_t, we can just use the final attention vector for the token to calculate the token representation h_t again. Formal: For the masked token y_t, the attention weights are added to the input token representation h_t, which gives us a new hidden representation h_t for the masked token. Formal: Finally, the final representation h_t for the masked token is calculated like this: Formal: Basically, we’re just adding up the attention weights for the tokens in the span s_t. Formal: Formal: The output vector h_t is the sum of all the attention weights for the tokens in the span s_t, which gives us the output representation h_t for the masked token. Formal: Formal: Formal: The total of all these outputs is the attention weight vector for the masked token y_t. Formal: Formal: Formal: Finally, the token representation h_t for the masked token is just the sum of all the attention weights for the tokens in the span s_t. Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.106.json"
    },
    {
        "casual_text": "Basically, our method splits pairs of parallel sentences into two parts, connects the two target parts with their correct translation from the two source parts (whether it's direct or flipped), and then keeps doing this over and over with the new pairs we get.",
        "formal_text": "In a nutshell, out method segments pairs of parallel sentences in two parts, linking the two resulting target segments with their proper translation amongst the two source segments (monotonous or inverted translation), and repeats this process recursively on the segment pairs thus obtained.",
        "GPT2_formal_text": "_t) ) = { h(t) } 9. Formal: Alright, so the first layer spits out a series of vector representations. Each of these vectors has a hidden vector and a bias vector that we can tweak during training. If you want more info on how each layer in this model works, you can check out Lu et al. (2015). Formal: Here, i represents the i-th element in the sequence. Formal: We also calculate the attention weights using cosine similarity, like in equation 1. Formal: These weights help the model focus on certain parts of the input sequence, like the token embeddings, the source context, and the word embeddings. Formal: At the end, we use a softmax layer to get the probability p(y|x). Formal: This probability is proportional to the square of the number of tokens, which tells us how often a token shows up in the input. Formal: The softmax layer gives us a probability for each token. Formal: Lastly, we use a max operation to pick the final output from all the tokens. Formal: Formal: Alright, let's break it down in simpler terms. We start with the token embeddings H t and the context embeddings c c. Formal: After that, we go through each token h t and its context embeddings, c c, to get the embedding for the next token in the sequence, h c. Formal: The last token h c gets processed through a max operation, which gives us the probability p(y|x). Formal: So, it's basically a mix of how often a token appears and how likely its context embedding is to be that token. Formal: Lastly, we use a softmax layer to get the probability for each token. Formal: Formal: Finally, we use a max operation to pick the final output from all the tokens. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2012.eamt-1.62.json"
    },
    {
        "casual_text": "We're aiming to find a group of instructions that work together across different recipes. To do this, we turn the graph (which has loops) into a collection of trees by using a maximum spanning tree algorithm. Check out Figure 4 for an example of one of these trees, made for a specific dish. A path in this tree, with no more than one node from each recipe, gives us a set of instructions that align well together. For instance, in the zoomed-in part of the tree in Figure 4, all the differently colored nodes along the path from the yellow node to the green node make up a set of instructions that align nicely across recipes.",
        "formal_text": "Our goal is to find a set of jointly-alignable instructions across different recipes. We therefore convert the graph (with cycles) into a forest by running the maximum spanning tree algorithm on the graph. Figure 4 shows an example tree derived for one of the dishes. A path in this tree, that has at most one node from each recipe, constitutes a set of jointly-alignable instructions. For example, in the magnified section of the tree in Figure 4, all unique colored nodes in the path from the yellow node to the green node constitute a set of jointly-alignable instructions.",
        "GPT2_formal_text": "(x, y) = 1 / (N_Mask + 1) (Sakai et al., 2016) and the other way around. This is shown in Equation (7). Formal: Here, i is the i-th token, and it can be either A or B. For a given token, t, we figure out how important it is to predict y by looking at it in terms of attention. Formal: We use two key pieces of info to figure out how important a token is to a specific prediction. Formal: The first part is the attention score (α_i) from the i-th token, and the second part is the attention score for the entire input sequence. Formal: To get the attention score for the i-th token, we use the token-level attention scores (α_i_1, ..., α_i_L) from the i-th token. These scores are calculated by applying a linear transformation to the input sequence, which gives us the attention scores (α_i_1, ..., α_i_L). Formal: The attention score for the i-th token can be calculated as: Formal: We take the token-level attention scores (α_i_1, ..., α_i_L) from the i-th token, and they're used as the attention scores to create the attention mask (α_i_m). Formal: For the sequence-level attention score, we use the token-level attention scores (α_i_1, ..., α_i_L) for the i-th token. Formal: Finally, we combine all these attention scores to get the overall attention score (α_i_m). Formal: Formal: To avoid mixing up the attention scores from different tokens, we use something called the attention pooling mechanism. This helps us separate the attention scores from each other. Formal: Formal: For the global attention score, we use the global attention scores (α_i_m) for the i-th token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.440.json"
    },
    {
        "casual_text": "We split the data into a training set, a development set, and a test set. We checked how well the model did on all three by looking at how accurately it could identify the antecedents of third person singular pronouns in the data. You can find the accuracy and size of each part in Table 2. \n\nNow, the big question is: are these results good enough? It's tricky to compare our results to the best in the field because we’re only focusing on third person singular pronouns. This means our system doesn’t handle coreference chains, so we can’t really measure its performance the usual way (as Luo, 2005 pointed out). Another thing that’s different from the standard approach is that we don’t have a mention detection module. Instead, we used the gold mention annotations and the singleton mentions we pulled out (check out section 3.2 for more on that).\n\nThat being said, we still want to get an idea of how well our classifier is doing. The closest study we found is Yang et al. 2004, even though they did use a mention detection module. They trained different systems to handle third person pronoun resolution and reported their accuracy using a success metric. When they tested on the MUC-6 corpus, their metric ranged from 70.0 to 74.7 for the different systems they built. On the MUC-7 corpus, the metric was between 53.8 and 62.5.",
        "formal_text": "We divided the corpus into a training set, a development set and a test set. We tested the model's performance on all three of the sets by measuring the accuracy of the identification of antecedents of the third person singular personal pronouns in the corpus. The accuracy and size for each subcorpus can be found in Table 2. An important question is whether these results are satisfactory. Our results are difficult to compare against state-of-the-art work in coreference resolution, because we concentrate on third person personal singular pronouns only. This means that our system does not form coreference chains and that its performance cannot be measured us- (Luo, 2005). A second difference with a more standard approach is that we do not have a module of mention detection. Instead, we use the gold mention annotation and the singleton mentions we extracted (see section 3. 2). This said, we still want to have an indication about the performance of our classifier. The study of Yang et al. 2004is the most comparable we found to ours, although they used a module for mention detection. Yang et al. 2004trained different types of systems to perform third person pronoun resolution and reported accuracy, in their paper indicated by the metric of success. When they tested on the MUC-6 corpus this metric was between 70. 0 and 74. 7 for the different systems they developed. When tested on the MUC-7 corpus the metric laid between 53. 8 and 62. 5.",
        "GPT2_formal_text": "= { r, t } from the training set. The aim here is to give each row in R a score that shows how well it matches the input sentence, giving higher scores to sentences that are more similar to the input sentence. Formal: We're using a simple version of the Moses decoder to turn these sentences into the target language. Formal: For the \"real\" target sentence, we start by applying regularizations to each node in the BNSL network, following the approach from (Gu et al., 2016). Formal: We use the BNSL network to figure out the probability distribution P(t|s), which is calculated as P(s|t) = exp(P(s|t)), where P(s|t) is the chance that a word s appears in a sentence t. Formal: For the \"fake\" target sentence, we use the same BNSL network but this time with the target language model instead of the target language model. Formal: To test how well our model can predict the target sentence, we add a target sentence feature vector to the BNSL network and train it using the target sentence as input. We then calculate the target sentence representation using this input vector, which we call h_t. Formal: To see how well our model can adapt, we test it on the same set of data as the original Moses model, but this time with the target language model as input. Formal: The results for the training set are shown in Figures 1 and 2. Formal: We ran our model five times to check how well it could handle the target language model as input. Formal: We ran our model five times to check how well it could handle the target language model as input. Formal: Lastly, we tested how well our model could adapt on the test set. Formal: We ran our model five times to check how well it could handle the target language model as input. Formal: Formal: Finally, we tested how well our model could adapt on the test set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E17",
        "filename": "E17-4006.json"
    },
    {
        "casual_text": "The variables s and r, which are part of the Beta distribution we use as a prior for our decision variables, are random in our model. Unfortunately, there isn’t a straightforward conjugate distribution for these parameters, so we can’t solve for them analytically. Instead, we’ll estimate the integral by repeatedly sampling these variables. Since both s and r are positive real numbers, we’ll use a Gamma prior for them, as explained in Section 3.",
        "formal_text": "The parameters s and r of the Beta distribution which serves as a prior on the decision variables are random variables in our model. Since there is no easily computable conjugate distribution for these parameters, we can not integrate them out analytically. Instead, we choose to approximate the integral through repeated sampling of these variables. Since both these variables take on values in the positive reals, we impose a Gamma prior on them as described in Section 3.",
        "GPT2_formal_text": "_num, key, value) = tf.get_feature(s, a_t).  \n\nBasically, it's just a way to show how important a specific word or token is. Formal: When picking a phrase, the model uses something called an attention mechanism to figure out the attention distribution for the whole phrase and also the parts that come before it. Formal: To make things more understandable, we came up with a simpler way to describe the attention mechanism. Formal: In the MATCH method, we think of the attention distribution for a sequence of words as a mix of different probabilities, like a bag of words. Formal: To keep things simple, we can use a simple math trick to figure out the attention weights. Formal: The model we propose combines a similarity-based method (like the one from Bahdanau et al., 2014) with a statistical approach that focuses on how often a word appears in a specific context. Formal: We looked at three different ways to measure how similar or different a word is compared to its context: co-occurrence, local co-occurrence, and global co-occurrence. Formal: We tested our model using three datasets that have been annotated with how often words appear in a context. Formal: We also tested it on two datasets where there was a lot of noise (like CNNs, RNNs, and RNNLMs) in the data. We picked the model that performed best based on these experiments. Formal: We ran a bunch of experiments on the CNN/DailyMail dataset, which was annotated using a model by Wang et al. (2016). Formal: We compared our model with four other models: the standard baseline (AN-NMT), the BERT-Tagger model (BERT-Tagger-1M), the CNN/DailyMail model (CNN/DailyMail-1M), and the BERT model (BERT-Tagger-4M). Formal: We also tested how well our model worked on datasets with more than two words. Formal: Since our model is a basic linear model, it can't take into account the special cases that involve prepositions and conjunctions. Formal: The top-performing model is the BERT-based model (BERT-Tagger-4M), which scored over 80.5% accuracy for all the datasets. Formal: This shows that our model is pretty effective and that the token",
        "directory": "C16",
        "filename": "C16-1296.json"
    },
    {
        "casual_text": "Alright, so each word has these specific features, and we use a bunch of yes-or-no variables to represent them.",
        "formal_text": "Then, each word is characterized by the following features, and is encoded by a set of boolean variables.",
        "GPT2_formal_text": "is updated. We set the maximum limit for entity mentions to 10. Formal: For any update event e, we grab all the entity mentions in the data and create an embedding for each one using a mix of word embeddings and bi-directional LSTM (BiLSTM) units, as suggested by Artetxe et al. in 2017. Formal: We do this to show the most important mentions in the document. Formal: Since we're focusing on the frequency distribution of entity mentions, we stick to the approach from Junczys-Dowmunt et al. in 2018, where they used a cluster structure to grab mentions from the document. Formal: The first row in the matrix E tells us the number of mentions for the document before the update event. The second row in the matrix E shows the count of mentions for the document after the update event. Formal: To predict the frequency of mentions for a document, we look at its entities and then calculate their normalized frequency, which we call f_e, using the entity mentions from the document. Formal: The entity mentions in the document are represented as vectors. Formal: For each entity mention x i in the document, we pick the one that's most likely to have a mention y i. Formal: Similarly, we pick the entity mentions that are most likely to have a mention y i for each mention x i. Formal: Finally, we return the normalized frequency of mentions for a document, f_i. Formal: Formal: Once we've got the normalized frequency of mentions for each mention x i, we calculate the normalized frequency of the document as f_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C04",
        "filename": "C04-1040.json"
    },
    {
        "casual_text": "In this paper, we dive into two main topics about classifying troll farms. First, we check out how three different types of features affect how well our classifier works. We're talking about content, behavioral, and stylistic features. It makes sense that troll farms pushing a certain political agenda would have common words or phrases (like #fakenews). But we also think that how they write could help improve how well we can predict things. If we assume that some people at these troll farms manage multiple accounts, even if the topics they talk about are different, their writing style should still be pretty similar. So, we're guessing that features that work for figuring out who wrote something (like Sari et al., 2018 did) could also be useful for identifying troll farms.",
        "formal_text": "In this paper, we study two specific issues related to troll farm classification. First, we analyze how three different sets of features impacts our classifier's performance. Specifically, we look at content, behavioral, and stylistic features. Based on the political agenda a troll farm is pushing, it is intuitive that there will be common tokens associated with the organization (e. g. , #fakenews). However, it is possible that writing style can improve predictive performance. Intuitively, if we assume that certain employees at a troll organization control multiple accounts, then even if the topical information (i. e. , content) varies across the accounts, the writing style should be similar. Thus, we hypothesize that features that are predictive for authorship attribution (Sari et al. , 2018), can be applied to the troll farm domain.",
        "GPT2_formal_text": "_tokens=1:1 (Sennrich et al., 2016) and attention_mask_tokens=1:2 (Sennrich et al., 2016). Formal: In this experiment, we wanted to see how well our model can understand and deal with different levels of meaning. To do this, we picked 100 random examples from the test set and made 100 comparisons between them. Formal: We also tried our model on a smaller version of the test set to see how it would do with sentences with fewer than 100 words. This smaller set had 6,977 sentences. The results are in Table 4. Our model did better than the baseline when it came to detecting metaphors. For the smaller set, our model actually made better predictions for the first level than the baseline. However, it didn’t do as well on the higher levels. Formal: We also checked how well our model would do in a zero-shot setup. We made 3 comparisons between a model trained only on the test set and a model trained only on the reference set. Our model did better than the baseline at the lower levels. However, it still didn’t perform as well as the baseline at the higher levels. Formal: Finally, we used our model to train and test a model called BERT-base-uncased, which is the one used in the original paper by Devlin et al. (2019). This model scored 0.938 on the test set and 0.999 on the reference set. Formal: Formal: Our model is built using PyTorch and is run on an NVIDIA Tesla P100 GPU. Formal: We tested it using pytorch-base-uncased for the fine-tuning phase. Formal: We trained it on the test set with 100 fine-tuned examples for 15 epochs. Formal: We used the standard gradient descent method to minimize the cross-entropy loss. Formal: We ran it for 50 epochs with the hyperparameters from the original paper. Formal: Formal: We set the batch size to 32, the learning rate to 1e-4, and the dropout rate to 0.5. Formal: Formal: Our model is built using PyTorch and is run on an NVIDIA Tesla P100 GPU. Formal: We trained it on the test set with 100 fine-tuned examples for 15 epochs. Formal: Formal: We used the standard",
        "directory": "D19",
        "filename": "D19-5003.json"
    },
    {
        "casual_text": "Since we worked with datasets from various fields, we fine-tuned multilingual BERT (from Devlin et al., 2019) on the target side of the parallel corpora to create the MLMs we used.",
        "formal_text": "Since our experiments were conducted on datasets in different domains, the MLMs we used were obtained by fine-tuning the multilingual BERT (Devlin et al. , 2019) on the target side of the parallel corpora.",
        "GPT2_formal_text": "= { 'a' => 1, 'b' => 2, ..., 'n' => n }, template = { 'a' => 1, 'b' => 2, ..., 'n' => n }, data = { 'a' => 1, 'b' => 2, ..., 'n' => n }\n\nThis basically shows the hierarchy of possible tags, with the best match being at the top and the rest falling to the bottom. Formal: A tag is considered good if it matches a description of the category. Formal: A tag is considered good if it matches a description of the category. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.267.json"
    },
    {
        "casual_text": "A compositional distributional model is all about creating a vector that captures the meaning of a whole phrase or sentence by mixing together the vectors of the individual words. The easiest way to do this is by using basic math operations on the word vectors (Mitchell and Lapata, 2010). Basically, the vector for a bunch of words w1, ..., wn is made like this:",
        "formal_text": "The purpose of a compositional distributional model is to produce a vector representing the meaning of a phrase or a sentence by combining the vectors of its words. In the simplest case, this is done by element-wise operations on the vectors of the words (Mitchell and Lapata, 2010). Specifically, the vector representation − → w of a sequence of words w 1, . . . , w n is defined to be:",
        "GPT2_formal_text": ", where |s_i| represents the number of words in the sentence s_i. Formal: LSTM-based models take an input sequence x and spit out an output sequence y that's part of a window of size W. Formal: We use the adversarial training method from Artetxe et al. (2017). Formal: We also tried using a max-margin loss, but it didn't really make things better. Formal: The best model we got is: Formal: We ran a bunch of experiments on the CoNLL-2013 dataset, which was trained on Wikipedia. We found that the model we came up with works better than the one by Ren et al. (2015). Formal: We tested our model against three other models: one that uses autoencoder components, another that uses a logistic regression classifier, and a third one that uses a mixture model. We compared them using the F1 score, and you can check out the results in Table 1. Formal: We checked how different parts of the model affect the results and how fast it is to train. Formal: For the autoencoder parts, we looked at the F1 score and the time it took to train. Formal: We did the same for the logistic regression classifier, but for the mixture model, we used the F1 score and the time it took to train. Formal: We also tested our model against the multi-head self-attention network. The results are in Table 2. Formal: We tested the model against three other top models, including the self-attention model (by Luong et al., 2016) and the Attentive-Attention model (by Luong et al., 2016). Formal: We focused on the top-1 attention matrix. Formal: We also evaluated our model against a standard extractive text classification model (Peters et al., 2018) and the BiLSTM-based one. The results are in Table 1. Formal: We tested our model against the top-1 attention matrix. Formal: Formal: The final results are in Table 3. Formal: Table 3 shows the F1 scores for our model compared to the other methods. Formal: Table 4 shows the time it took for the model to train. Formal: Formal: Formal: Formal: We also compared our model to the BiLSTM-based one. The results",
        "directory": "C16",
        "filename": "C16-1268.json"
    },
    {
        "casual_text": "In this paper, we take a look at and expand some transfer learning methods for identifying new entities. We use both the usual datasets and some new ones that haven’t been used for this before. As far as we know, this is the first time these transfer learning methods have been directly compared. \n\nWe also run experiments on seven new pairs of source and target domain corpora, which almost doubles the total number of pairs studied in all previous work. Plus, we’re sharing our code so that others can check our results and use our benchmarks to compare their own transfer learning techniques in the future.",
        "formal_text": "In this paper we replicate and extend several transfer learning methods for recognizing novel entities, using both standard corpora and datasets that have not been used for this task before. To the best of our knowledge, this is the first direct comparison of these transfer learning methods. In addition, we perform experiments on seven new source/target domain corpus pairs, nearly doubling the total number of corpus pairs that have been studied in all past work combined. We also share our code 2 so that others may verify our results and compare future transfer learning techniques against our benchmarks.",
        "GPT2_formal_text": "= (sum_total(i) for i in range(K)) n_s = (sum_num(i) for i in range(2K)) p_s = (sum_p(i) for i in range(2K)) p_n = (sum_num(i) for i in range(2K)) n_n = (sum_num(i) for i in range(K)) p_n = (sum_n(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) n_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) n_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) n_n = (sum_num(i) for i in range(K)) n_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) n_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) n_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) n_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) n_n = (sum_num(i) for i in range(K)) p_n = (sum_num(i) for i in range(K)) p_n = (sum_num",
        "directory": "C18",
        "filename": "C18-1168.json"
    },
    {
        "casual_text": "Our approach to improving the reading step by adding more detailed descriptions of relationships is kind of like what Weissenborn et al. (2017) and Mihaylov and Frank (2018) did. They both added background commonsense knowledge to reading comprehension systems. They used structured knowledge bases to pull out info about the semantic relationships between entities. In our case, instead of that, we grab text snippets that mention pairs of entities and turn them into vector representations to show the relationships between those entities.",
        "formal_text": "The idea of our baseline to improving the reading step by incorporating additional relation description spans is similar as Weissenborn et al. (2017) and Mihaylov and Frank (2018), who integrate background commonsense knowledge into readingcomprehension systems. Both rely on structured knowledge bases to extract information about semantic relations that hold between entities. On the other hand, we extract text spans that mention each pair of entities and encoded them into vector representations of the relations between entities.",
        "GPT2_formal_text": "is the attention vector for the model's input. Formal: The sigmoid function is a matrix, and W_s(t, o) is a learned parameter matrix. The final output is a sigmoid value, which is a positive number. Formal: The log probability, p_s(t, o), is calculated using equation (1). This probability is then used to calculate the importance of each token in the input. Formal: The attention mechanism uses a mix of the size of the hidden state and how the model operates. Formal: The model's output is calculated using equation (2). Formal: The attention weight vector for the token s_i_j in the input token s_i,j is calculated using equation (3). Formal: Lastly, the output is a single number, which is the attention weight vector. Formal: You can find the full code for this project at https://github.com/changqian/MUSE_Transformer. Formal: You can find the model's parameters and hyperparameters in equation (5). Formal: The model has the same parameters as the base Transformer model, but we ran some tests to see how it performs. Formal: For each sample, we calculate the attention weights using equation (6). Formal: For each sample, we calculate the attention weights using equation (5). Formal: For each sample, we calculate the attention weights using equation (4). Formal: For each sample, we calculate the attention weights using equation (3). Formal: For each sample, we calculate the attention weights using equation (2). Formal: We used MUSE for this project. Formal: For each sample, we calculate the attention weights using equation (5). Formal: For each sample, we calculate the attention weights using equation (2). Formal: For each sample, we calculate the attention weights using equation (1). Formal: The attention weights for the token s_i_j in the input token s_i,j are calculated using equation (6). Formal: The attention weights for the token s_i_j in the input token s_i,j are calculated using equation (5). Formal: Formal: Formal: For each sample, we calculate the attention weights using equation (4). Formal: Formal: For each sample, we calculate the attention weights using equation (3). Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1134.json"
    },
    {
        "casual_text": "Maximum entropy (ME) models, sometimes called log-linear or exponential learning models, are a versatile machine learning method for tasks like classification and prediction. They've been pretty successful in natural language processing, such as part of speech tagging and named entity recognition. These models can handle features from all sorts of different sources to help with classification. Each feature acts like a rule for the model. For example, in question classification, a feature might be a specific word that’s linked to a certain type of question. The maximum entropy model is the one that has the highest entropy while still following all the rules. In our work, we’re using the Stanford Maximum Entropy implementation, as described by Manning and Klein in 2003.",
        "formal_text": "Maximum entropy (ME) models (Berger et al. , 1996; Manning and Klein, 2003), also known as log-linear and exponential learning models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging, named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification. Each feature corresponds to a constraint on the model. In the context of question classification, a sample feature could be the presence of a particular word associated with a particular question type. The maximum entropy model is the model with maximum entropy of all models that satisfy the constraints. In this paper, we adopt Stanford Maximum Entropy (Manning and Klein, 2003) implementation in our experiments.",
        "GPT2_formal_text": "= {}, label_ids = {}, comment_ids = {}, and summary_ids = {}. Formal: We need to figure out the representations for the entity mentions in the text by turning them into an embedding space. We then use that embedding to represent the entity mention itself. Formal: Using the same attention for both entities and mentions gives us a better representation than just using one attention for each. Formal: The cross attention mechanism is a pretty straightforward way to deal with the issue of having a huge number of mentions that need different attention heads. Formal: We found that using a cross attention mechanism can make the model focus more on the important parts of the mentions and avoid focusing too much on generic ones. Formal: Our main goal is to automatically figure out the context of an entity mention. We think that entities and mentions should be seen as opposites, so the context should focus on the important parts of the mention. Formal: We took a closer look at the response vector representations and found that the entity mention vector is mostly focused on the context, which makes the representation of the mention pretty generic. Formal: We tested our model using the gold answer span dataset from PubMed. The results are in Table 5. Table 5 shows the entity mention vector representations from the model trained with the cross attention mechanism. Formal: Our approach keeps the attention mechanism intact, but instead of creating its own vectors, we use those from the source dataset for the model's representation. Formal: We built a strong Bi-LSTM model that handles both entity mention and mention extraction. We tested it on the news dataset, and the results are in Table 5. Formal: Our system uses a context-aware LSTM network, which learns to predict the context of entity mentions using an attention mechanism. Formal: We trained a Bi-LSTM model using the entity mention and mention extraction datasets to predict the context of a mention, and we got results in Table 5. Formal: We also built a Bi-LSTM model for entity mention extraction, and the results are in Table 5. Formal: We used a contrastive retrieval approach for sentence embeddings. This method takes the embeddings of entities and mentions as input and generates context-aware word embeddings. Formal: We used a transformer model to encode the context-aware word embeddings from the source dataset and generated the context-aware embeddings for each entity mention. Formal: We also",
        "directory": "D08",
        "filename": "D08-1097.json"
    },
    {
        "casual_text": "In 1986, she started up Hamilton Technologies, Inc. in Cambridge, Massachusetts, and became the CEO.",
        "formal_text": "In 1986, she became the founder and CEO of Hamilton Technologies, Inc. in Cambridge, Massachusetts.",
        "GPT2_formal_text": ", et al. (2019). The text is encoded using the transformer model, which was introduced by Vaswani et al. in 2017. The output is a sentence with n words, where each word gets a weight based on the attention mechanism's weight. Formal: We break down the input text into individual words and generate sequences using Transformers, which are a type of neural machine translation (NMT). Formal: To get a better understanding of how the model is performing, we check the absolute error for the base model, which is the Transformer-base model. Formal: To train the model, we use the cross-entropy loss. Formal: We also do some fine-tuning experiments using the shared task setup. The results are in Table 1. Formal: For the general evaluation, we used the same setup as in the shared task, but this time, we focused on the general NMT task. Formal: We also did some unsupervised experiments on this task. Formal: We trained our model with different amounts of data, from 1 million to 10 million tokens. Formal: We got the model's performance by running it 10 times with different random seeds and checking the accuracy on the validation set. Formal: Finally, we noticed that the model performed just as well on the validation set as it did on the test set. Formal: Table 1 shows the results of our evaluation on the general test set. Formal: In the first three columns, the top three scores are the results of the top-performing model, and the second column lists the scores of the best model on the test set. Formal: To get a better grasp of the model's performance, we ran the model 10 times with different random seeds and checked the performance on the validation set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1077.json"
    },
    {
        "casual_text": "Alright, let's keep the generator G fixed and update the discriminators DDP and DKB by tweaking them to minimize the stuff in Equation (9) using gradient descent.",
        "formal_text": "Fix generator G and update discriminators DDP and DKB by minimizing Eq. (9) via gradient descent.",
        "GPT2_formal_text": "= {'qf_1', 'qf_2', 'qf_3', 'qf_4', 'qf_5', 'qf_6'} Formal: We start with randomly chosen pre-trained word embeddings, which we call U. We train our model using an iterative gradient ascent method, following the approach described by Barhom et al. in 2018. Formal: We can tweak the embedding dimension c by adding one or more extra tokens. Formal: We take the target language embeddings X_t and use them to create different target language representations, which we call D_t. Formal: Once we have D_t, we use a linear mapping to map it to a knowledge representation, K. Formal: After that, we use a linear projection to get a sentence embedding, ŷ_t. Formal: Lastly, we combine these three embeddings to get the final embedding, e_t. Formal: In this paper, we focus on unsupervised dependency parsing. Formal: To see how well our model works, we train it on a mix of the test and development sets from the CoNLL-2009 shared task and evaluate it on the development set. Formal: We measure how well it performs using the F1 score (F1_score) and the cross-entropy between F1_score and the reference output. Formal: We also check the F1 score for top-1 accuracy (F1_acc) and top-10 accuracy (F1_top10) to see how well our model performs across different tasks. Formal: For the actual use, we train the model on the test set. Formal: We evaluate the model on the test set using the F1 score and the cross-entropy between F1_score and the reference output. Formal: Finally, we get the F1 score for top-1 accuracy and top-10 accuracy. Formal: The results for this experiment are in Table 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.281.json"
    },
    {
        "casual_text": "You can grab the word embedding initialization from this link: https://github.com/npow/ubottu.",
        "formal_text": "Initialization of word embedding can be obtained on https: //github. com/npow/ubottu",
        "GPT2_formal_text": "ing_text and image_attention_masking_image. Formal: We get rid of any words that have the attention masking feature set to zero, like \"[CLS]\". Formal: For instance, the word \"ANTONY\" gets turned into [AN] by using the attention masking. Formal: We use the model trained on the MNLI dataset (from Peters et al., 2017) to turn images into text and create text captions. Formal: To create these text captions, we start by pre-training a transformer model called g_train on MNLI using the cross-entropy loss function. We also train our model on TACRED, a dataset that includes images from Flickr30k, which includes captions by people. Formal: Next, we take the captions from TACRED and use them to train our model. We get the validation set from TACRED using the cross-entropy loss function. Formal: The validation set for TACRED has 4.8 million captions. Formal: Since we don't have the actual captions in the datasets, we use the model trained on TACRED as the context for TACRED. Formal: Finally, we add a dropout of 0.3 to the token embedding layer to prevent overfitting. Formal: The generated captions are saved in the csv format, which we put in the title of our paper. Formal: Since we don't have images for training or testing, we train the model using the cross-entropy loss on the validation set. Formal: After that, we train the model for 10 epochs on TACRED. Formal: After each epoch, we normalize the predicted probabilities and calculate the perplexity to get the final score. Formal: Finally, we train the model for 10 epochs on TACRED. Formal: Here's the dataset, how it was gathered, and how much it cost: Formal: To make the most of the training data, we grabbed 2.4 million sentences from the movie review dataset, which includes 400,000 sentences from the first 200,000 reviews. Formal: Here's the dataset and how we gathered it: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "2) In \"He paid his bill,\" the person (the subject) is giving up something valuable, like money. The \"bill\" (the object) means there's a specific amount owed, and that amount is supposed to go to some faraway person or thing (that we could figure out if we wanted to) as the one who should get it.",
        "formal_text": "2) in \"He paid his bill\" the subject gives up something of econ. value; the object implies a specific amount and that this amount is due to some not further specified (but specifiable) remote entity as the rightful receive~;",
        "GPT2_formal_text": "\": [], \"id\": 1, \"preferred_language\": [], \"model_type\": [], \"embedding\": [], \"output_features\": [], \"type\": [], \"level\": [], \"length\": [], \"shape\": [], \"start\": [], \"end\": [], \"category\": [], \"specific\": [], \"model_type\": [], \"embedding\": [], \"layer_mask\": [], \"output_shape\": [], \"mask\": [], \"sentence\": [], \"token\": [], \"reward\": [], \"target\": [], \"distance_min\": [], \"distance_max\": [], \"training_time\": [], \"adversarial_encoding\": [], \"training_mode\": [], \"adaptive\": [], \"learning_rate\": [], \"learning_rate_min\": [], \"learning_rate_max\": [], \"learning_rate_min\": [], \"learning_rate_max\": [], \"learning_rate_neu\": [], \"learning_rate_neg\": [], \"learning_rate_int\": [], \"learning_rate_n\": [], \"learning_rate_n\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [], \"learning_rate_softmax\": [",
        "directory": "C69",
        "filename": "C69-2201.json"
    },
    {
        "casual_text": "The stuff I just mentioned also applies to another type of reduction called strong reduction or 11-reduction, but I don't have room to explain it right now. You can check out Morrill et al. (1990) for more details.",
        "formal_text": "2The above remarks also extend to a second form of reduction, strong reduction/11-reduction, which we have not space to describe here. See Morrill et aL (1990).",
        "GPT2_formal_text": "ed = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6 } d_m. This model looks at the attention values for each word in the context and decides how important they are for generating the next word. Formal: We take the attention vector s and its normalized value, which we call s_m, and mix them together to get a single representation, p_t. The model then calculates the attention weight, α_t, by adding up the attention vector s_m and the normalized value s_m_t. Formal: We pick the attention weight α_t based on how the context and next word interact, and set its value to 0.8. Formal: Since the next word is the one generating the current word, we adjust the attention weights for the current word and the context words to match this setup. Formal: We compare the results from all the models using the log-likelihood ratio (log-likelihood = 1). Formal: We calculate the log-likelihood ratio (log-likelihood = 1) for different values of α_t (α_t = 0.1, α_t = 0.2, etc.). Formal: The final model uses this calculated log-likelihood ratio (log-likelihood = 1) for the generated next word. Formal: To train the model, we create a special loss function, L_m, that helps us learn the attention weights better. Formal: We take the output vector from the CRF layer and run it through a fully connected layer. After that, we add this output vector to the training data to predict the next word. Formal: After the CRF layer, we use a binary cross-entropy loss function, L_b_t, to adjust the weights in the CRF layer. Formal: Finally, we train the model using a multi-head attention mechanism, which is a linear layer with a weight vector that we can train using log-likelihood. Formal: To use this model for other tasks, like generating text, we train the model using a multi-head attention mechanism, L_a_t. Formal: We use L_b_t to train the model and the model's final output to predict the next word. Formal: Lastly, we use a cross-entropy loss function, L_a_t, to adjust the weights in the CRF",
        "directory": "E91",
        "filename": "E91-1035.json"
    },
    {
        "casual_text": "We also tweaked the sequence generation setup to work with the training pairs we’re proposing. In each layer, the attention module figures out how much weight to give to the token representations from the encoder, based on what’s already been generated in the decoder, and then spits out the context c_h. To factor in the future conversation X_f, we added another encoder to create contextualized token representations of X_f, which the attention module then pulls out as the context c_f. This new encoder uses the same parameters as the original one. The attention module’s output is basically the combination of the past context c_h and the future context c_f. Lastly, the training criterion we’re using is the negative log-likelihood, like this:",
        "formal_text": "We also redesign the sequence generation architecture to handle the proposed training pair. The attention module in each layer calculates the weight of the contextualized token representations from the encoder based on the information that has been generated in the decoder, and then returns the context c h. In order to consider the future conversation X f, we apply another encoder to produce the contextualized token representations of X f, which will be further extracted as the context c f by the attention module. The new encoder shares the parameters with the original encoder. Meanwhile, the output of the attention module is the concatenation of the past context c h and the future context c f. Finally, the training criterion is formulated as the following negative log-likelihood:",
        "GPT2_formal_text": "= {x1, ..., xn} | {x1, ..., xn-1}. Formal: This is a way to express things like a user's preferences or goals in plain, natural language. Formal: We take each dialogue-response pair and connect them using the \"Query\" part of the \"Query and Answer\" part of the NLI system. Formal: We take the initial question and use that to check if there's a query for the response. Formal: Also, we use the \"Question\" part of the \"Query and Answer\" part to get a detailed query for the response. Formal: The \"Query\" part is basically a combination of the query and the response. Formal: The \"Answer\" part is made up of the response to the query and all the other parts of the response. Formal: We use these parts to figure out the exact query for the response. Formal: To make things clearer, we also show the \"Query and Answer\" part, which is like a full conversation between the user and the system. Formal: Figure 1: Our dialogue parser, which uses NLI to pull out knowledge. Formal: The dialogue parser uses both the query and the response to look up and figure out the exact query for the response. Formal: Here's how we set up the dialogue parser using NLI: Formal: The dialogue parser works by connecting a query and a response using the \"Query\" part of the system. Formal: The parser then helps the user by showing the dialogues and the context around the query and response. Formal: We also have some extra details about the dialogue that we can use as extra context. Formal: We use two different language models (LM) to help with the query and response. Formal: First, we use a dictionary-based language model (LM), which is a statistical model that can make predictions based on a dictionary. Formal: We add this LM to the dialogue parser to get a better query for the response. Formal: We also add a document-based LM (LM-DMSC) to the parser to get a better context for the query. Formal: Lastly, we add a discourse-based LM (LM-DTMSC) to the parser to get a better context for the response. Formal: Table 2 shows how the system handles the conversation. Formal: We build the dialogue by adding the knowledge from the NLI system to",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.534.json"
    },
    {
        "casual_text": "Compared to the usual decoding approach (like the one by Zens and Ney in 2008), the method we're suggesting can skip a bunch of complicated calculations, such as figuring out rest costs and dealing with reorderings during decoding. This is because each path in the reordering lattice already has all the info about word order. So, our method ends up making the decoding process simpler than the old way.",
        "formal_text": "Compared with the conventional decoding method (Zens and Ney, 2008), the proposed method can eliminate some complex score calculations, e. g. , rest cost estimation and decoding-time reorderings, because each path in the reordering lattice holds complete information of the word order. As a result, the proposed method makes the decoding algorithm simpler than the conventional method.",
        "GPT2_formal_text": "and self-attention_mask. The attention weight for the class label h_i in the binary classification layer is calculated as: Formal: The number of classes in the input sentence is a non-negative number. If the input sentence has just one class, it's 0. If the input sentence has more than one class, it's 1. Formal: The penalty term for using the softmax function is calculated by multiplying the softmax weights of the hidden states. Formal: The negative log probability (µ) for the actual class label h_i is calculated like this: Formal: The label h_i is just the top-k hidden states from the first two layers of the pre-trained transformer model. Formal: In this project, we use the bert-base-cased model from the Hugging Face Transformers library. This model is pre-trained on the English Wikipedia dataset, and the learning rate is set to 2e-5. Formal: The training and validation data for the bert-base-cased model, which is the same as the training and validation data for the original bert-base model, is fed into the model. Formal: Formal: The learning rate is adjusted based on the validation error rate. Formal: The learning rate is adjusted based on the validation error rate. Formal: All the parameters θ are adjusted during training. Formal: All the parameters θ are adjusted during training. Formal: To get the gradient, we use the Adam optimizer (from Kingma and Ba, 2014) with a learning rate of 5e-5. Formal: Formal: Formal: We also use the bag-of-words method from the skip-gram tokenizer library (from Kalchbrenner and Blunsom, 2013) to generate the next word. Formal: Formal: To get the gradient, we use the Adam optimizer (from Kingma and Ba, 2014) with a learning rate of 5e-5. Formal: Formal: We also use the bag-of-words method from the skip-gram tokenizer library (from Kalchbrenner and Blunsom, 2013) to generate the next word. Formal: Formal: The bag-of-words method gives a word's probability as its feature representation. Formal: Formal: The feature representation for a word's probability as a vector is basically the vector of the word's word",
        "directory": "C16",
        "filename": "C16-1134.json"
    },
    {
        "casual_text": "We use these representations to figure out how similar each part of the question is to each part of the document. This is done by creating a matrix S ∈ R m×n that shows the similarities between the tokens (words or pieces) in the question and the document. We do this using a tri-linear function, as described by Seo and others in 2017.",
        "formal_text": "These representations are used to calculate the pairwise similarity matrix S ∈ R m×n between tokens in the question and those in the document by a tri-linear function (Seo et al. , 2017), i. e. ,",
        "GPT2_formal_text": ". This is a way to find the real translations of a word by looking at all the possible options and comparing them one by one. Formal: We start by making a list of translation options, which we call the \"translation\" list. It has a main heading (in this case, the main heading is a noun phrase) and a bunch of minor heading(s). These minor heading(s) are like helpers, and we use them to create a translation using the main heading. Formal: Using these helpers, we can create the translation by looking at the options created by the main heading. Formal: From this, we pick the best translation by checking its translation probability, which we call p(t|s). Formal: After that, we make a new list of translation options for the whole sentence, called the \"translation\" list, and add it to the main list. Formal: Finally, we pick the best translation from both lists, using the probability for each translation to decide the final one. Formal: So, in short, we're using helpers to get the translation, and then choosing the best one from both lists. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1237.json"
    },
    {
        "casual_text": "The dataset has a couple of cases where the answer is marked as uncertain. However, de Marneffe et al. (2010) didn't include those in their results, and we're following their lead by leaving them out too.",
        "formal_text": "The original dataset contains two additional examples where the answer is annotated as uncertain, but de Marneffe et al. (2010) exclude them from the results and so do we.",
        "GPT2_formal_text": "ed and attention_attention_masked_1D = [attn1, attn2, attn2_1D] respectively. Formal: We used the same convolutional network to grab the dialogue history from the DMRS model. We called it dialogue_history_1D. Formal: The validation set had a dialogue history that was 12 tokens long, with 36 different labels. This dataset is smaller compared to the training set, with 10 dialogues and 20 labels, so we divided it into two smaller groups, labeled dialogues and labels. The validation set has 18 dialogues and 12 labels. Formal: We picked the dialogue history from the validation set based on a specific rule. For each label, we picked the dialogue with the highest probability from the validation set (we call this \"selected_dialogue_1D_selected\"). The question we were looking at was \"What happened to Annie?\" Formal: The dialogues and labels in the validation set were all linked together, and we used the gold-standard labels for this link. Formal: We trained the dialogue history model with just one convolutional layer (like the one from Ma et al., 2015) and trained it using a batch size of 128. Formal: After training, we got the validation set back. We used the dialogue history and the main labels for validation. Formal: We trained the dialogues and labels with just one convolutional layer and trained the model using a batch size of 128. Formal: We applied the weight decay on the dialogue history to get the dialogue history and the main labels. Formal: We trained the model using the dialogue history and the main labels, using a batch size of 128. Formal: We also applied the weight decay on the dialogues and the labels, so the dialogues and labels were balanced. Formal: We trained the dialogues and labels with just one convolutional layer and trained the model with a batch size of 128. Formal: We applied the weight decay on the dialogues and the main labels, so the dialogues and labels were balanced. Formal: Formal: We trained the dialogues and labels with just one convolutional layer and trained the model using a batch size of 128. Formal: Formal: The dialogues and labels in the validation set were all linked together, and we used the gold-standard labels for this link. Formal: We trained the model using the dialogues and the main labels",
        "directory": "D18",
        "filename": "D18-1202.json"
    },
    {
        "casual_text": "When you're going through a tagged sentence T = (w 1 /p 1, . . . , w n /p n ) from the end to the beginning, you categorize each word w i into one of three groups: Left, Right, or Shift.",
        "formal_text": "While scanning a tagged sentence T = (w 1 /p 1, . . . , w n /p n ) backward from the end of the sentence, each word w i is classified into three categories: Left, Right, and Shift. 1",
        "GPT2_formal_text": "= {q_i, q_j} from i=1 to m, where q_i is the original query, q_j is the generated response, and m is the total number of sentences. Formal: Since the query and response can be really different, the algorithm tries to guess the answer by comparing the query and response. Formal: This query-response generation thing can be seen as a ranking problem, where each query rank is a combination of the response and the original query. Formal: We're using the feature-based K-means algorithm for ranking (thanks to Liu et al., 2015) to figure this out. Formal: The final ranking of the query and response is a combination of the rank of the query and the rank of the response. Formal: The relevance score is calculated by looking at how similar the query and response are. Formal: The relevance score is calculated by comparing the query and response rank, and then taking the log of the rank. Formal: The relevance score is based on the query and response rank, and then the log of the rank. Formal: Finally, the relevance score is the sum of the relevance scores for both the query and response. Formal: Here's how we calculate the relevance score, which is 1/(1 + (1 − m)) for the query and m for the response. Formal: The relevance score is calculated by comparing the rank of the query and response. Formal: The relevance score is the sum of the relevance scores for both the query and response. Formal: Here, m is the number of sentences in the query and response. Formal: Lastly, the relevance score is the sum of the relevance scores for both the query and response. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C04",
        "filename": "C04-1040.json"
    },
    {
        "casual_text": "How batch size and dropout affect things. The batch size impacts how quickly the model learns and its final accuracy, while the dropout rate also plays a big role in performance. We looked at how these two settings influence things by using a 1:1 ratio for our data (all CTB data and an equal amount of randomly selected PD data). We tracked the accuracy of our neural multi-view learning model over different training epochs, trying out different combinations of dropout rate (d) and batch size (b). The results are available here: [link 1] and [link 2].\n\nFor the stacking model, we set the batch size to 100 for the PD sub-model. The results are in Figure 5. The top two dashed lines at epoch 30 show a 20% dropout rate, the middle solid lines show no dropout, and the bottom dotted lines show a 50% dropout rate. Without dropout, the model starts off doing better but then gets worse after 10 epochs, which suggests it’s overfitting the training data. On the other hand, with a 50% dropout rate, the model starts off way worse, meaning the dropout might be too high and causing underfitting.",
        "formal_text": "Effect of batch size and dropout. The batch size affects the speed of training convergence and the final accuracies of the neural models, and the dropout rate has been shown to significantly influence the performance. We investigate the effects of these two hyper-parameters by adopting a corpus weight ratio of 1: 1 (All the CTB training data is used, while the same amount of PD is sampled randomly), drawing the accuracies of the neural multi-view learning model against the number of training epochs with various combinations of the dropout rate d and batch size b. The results are 1 https: //code. google. com/p/word2vec 2 https: //github. com/SUTDNLP/ZPar shown for the multi-view learning model. For the stacking model, we use b=100 for the PD sub model. The results are shown in Figure 5, where the two dashed lines on the top at epoch 30 represent the dropout rate of 20%, the two solid lines in the middle represent zero dropout rate, and the two dotted lines in the bottom represent a dropout rate 50%. Without using dropout, the performance increases in the beginning, but then decreases as the number of training epochs increases beyond 10. This indicates that the NN models can overfit the training data without dropout. However, when a 50% dropout rate is used, the initial performances are significantly worse, which implies that the 50% dropout rate can be too large and leads to underfitting.",
        "GPT2_formal_text": ". Formal: For the query, we grab the phrases that include the query from the text. The combined representation of the query and all the phrases is then sent to the LSTM, which creates an embedding that's a multi-dimensional version of the query. Formal: We use a convolutional neural network (CNN) to process this input. The CNN is set up to create a sequence of tokens for each query, while also keeping track of how often each token appears. Formal: Lastly, we combine the query and the phrase embedding using a linear layer. This layer makes sure the phrases are close to each other, so they can be used in a sequence. Formal: For the example in Figure 2, the input representation (image_x_i, query_x_i, a_i, image_i_j) includes the query, the embedding for the phrase \"looking,\" the embedding for \"to,\" and the token \"the.\" The resulting representation (image_x_i_i, query_x_i_i, a_i_i_j) is a multi-dimensional version of the query and the phrase embedding. Formal: Finally, we average the word embeddings for each query and phrase to get their representation (image_x_i_i, query_x_i_i, a_i_i_j). Formal: We also do a linear layer to combine these representations, which gives us the final representation (image_x_i_i, query_x_i_i, a_i_i_j). Formal: The embedding for \"looking\" is calculated based on the LSTM's hidden state. Formal: For the goal, we calculate the prediction for the correct label using the context. Formal: For the sentence embeddings, we look for the best possible embeddings for the entities, and then use an attention mechanism (like the one by Bahdanau et al., 2015) to calculate the output representation (image_x_i_i, query_x_i_i, a_i_i_j). Formal: We calculate the sentence embeddings for each sentence using the context. Formal: The sentence embeddings for each entity are then averaged. Formal: Finally, we average the sentence embeddings for each entity to get the output representation (image_x_i_i, query_",
        "directory": "D16",
        "filename": "D16-1070.json"
    },
    {
        "casual_text": "Basically, if you have a document d with a bunch of tokens like {t1, t2, ..., t|d|}, we can break it down into smaller chunks called segments. Each segment si will have c tokens, and there will be |s| of these segments in total.",
        "formal_text": "More formally, given a document d = {t 1, t 2, . . . , t |d| } of |d| tokens, we split it into |s| consecutive segments s i of c tokens:",
        "GPT2_formal_text": ", and labels that have the same label. Formal: This method can be used as a straightforward extension of the basic user model when dealing with a mix of users and groups. Formal: Formal: You can also tweak things by tweaking the features and how often they appear in the same batch. Formal: This kind of semantic representation can be made simpler by using part-of-speech tags or named entity recognition. Formal: This approach is also pretty popular in research on handling different types of data. For example, it’s been used to group data from different types of datasets into clusters (Liu and Lapata, 2006; He et al., 2006). Formal: In this part, we’ll give you a quick rundown of the data we used to train and test our models. We’ll also explain how we set up our experiments and how well we’re doing overall. Formal: Here are some examples of how we did things: Formal: The feature set we used is pretty straightforward and covers the most common types of data. We used a mix of features, including: Formal: Table 2 shows the features we used for training the model. Formal: For the user model, we looked at things like the average age of the user, how much money they spent on food and drinks, and the average number of hours they spent on TV shows. The user data comes from two sources: Formal: Here, we’ll walk you through the setup of the user model we used for analyzing our data. Formal: Formal: We’ll explain the different features we used in detail, along with the setup of our model. Formal: Formal: We’ll also talk about how well the user model performed overall and talk about the mistakes we made. Formal: We’ll also describe the training and testing process for our model. Formal: Formal: In our experiments, we focused on specific data points, but you can use more than one set of features if you want. Formal: Formal: The rest of the paper goes like this: Section 2 gives a quick rundown of the data we used, including the feature sets we used. Section 3 explains the experimental setup. Section 4 covers the experimental results. Finally, Section 5 wraps it all up. Formal: Formal: Table 3 shows the features we used for training. Formal: Formal: Formal: For training, we’",
        "directory": "clinicalnlp",
        "filename": "2022.clinicalnlp-1.2.json"
    },
    {
        "casual_text": "1a) The storm wiped out 120,000 people in Jamaica and five in the Dominican Republic, then headed west toward Mexico.",
        "formal_text": "1a) The storm killed 120, 000 people in Jamaica and five in the Dominican Republic before moving west to Mexico.",
        "GPT2_formal_text": "ed, which is basically the attention-based semantic representation of the sentence's representation vector. The rest of the parameters in the Transformer are set to 1e-4. The rest of the matrix is just a list of linear projections of the hidden representation vector into the target domain, starting from the beginning (0, 0) to the end (m, m). Formal: For the task of generating text, we use the sentence distribution q and the target domain vector h d to represent the sentence and the target domain vector, respectively. Formal: Basically, the attention matrix α_t is basically the attention matrix that gets applied to the target word, starting from the beginning (0, 0) to the end (m, m). Formal: Using this, we can calculate the attention vector for the target word, which we call the attention vector for the target word, A_t. Formal: Just like in the visual question generation task, the attention matrix A_t gets applied to the image, which we call the attention vector for the image, A_t. Formal: We then use the context vector h e_i to mix the attention vector A_t with the image context vector h_e_i, and use a softmax function to combine everything. Formal: Finally, we combine all these vectors into a single representation of the input. Formal: Formal: To get the attention vector for the question, we use the attention matrix A_q for the question and the question context vector h_q, which is the attention vector for the question and the question context vector, respectively. Formal: The attention vector for the question is calculated as the product of the attention vector A_q for the question and the question context vector h_q. Formal: Finally, we combine all these attention vectors into a single representation of the input. Formal: We then calculate the attention vector for the question, A_q, using the attention matrix A_q for the question and the question context vector h_q. Formal: Formal: After that, we use a softmax function to combine all these attention vectors. Formal: Formal: After that, we calculate the attention vector for the question, A_q, using the attention matrix A_q for the question and the question context vector h_q. Formal: Finally, we combine all these attention vectors into a single representation of the question. Formal: Formal: Formal: Formal",
        "directory": "C10",
        "filename": "C10-2170.json"
    },
    {
        "casual_text": "Before, we used these measures to figure out how two argumentative parts were connected. But since our speeches have multiple arguments and we don’t know where they are in the text, we tweaked the method to work at the speech level. This means we looked at every sentence in the supporting speech and every possible sentence in the counter speech. \n\nFor each measure, we compared one supporting speech sentence to all the potential counter speech sentences and combined the similarities using a function f. This gave us a \"sentence-to-speech\" similarity. Then, we took all those similarities and combined them again using another function g, which gave us a \"speech-to-speech\" similarity. \n\nWe call these speech-to-speech measures w f g for word-based similarities and e f g for embedding-based similarities. When combining things, we considered using the max (↑), min (↓), average (+), or product (×). For example, w ↑+ means we took the highest word-based similarity for each supporting sentence compared to all counter sentences, and then averaged those numbers.",
        "formal_text": "Previously, these measures were used to predict the relations between a pair of argumentative units. Since our speeches may contain multiple arguments, and their location within the text is unknown, we defined this method at the speech level by considering every supporting speech sentence and every candidate counter speech sentence. For each measure, the similarities of one supporting speech sentence to all candidate counter speech sentences were aggregated by applying a function f, yielding a sentence-to-speech similarity. These sentence-to-speech similarities were aggregated using another function g, yielding a speechto-speech similarity. We denote these speech-tospeech measures by w f g for word-based similarities and e f g for embedding-based similarities. As aggregation functions, the maximum (↑), minimum (↓), average (+) and product (×) were considered. For example, w ↑+ denotes taking the maximal word-based similarity of each supporting speech sentence to all candidate counter speech sentences, and averaging those values.",
        "GPT2_formal_text": ", which are basically the parts of speech tags for the text you want to translate. Formal: To create a similarity matrix, we break down the source text into smaller, meaningful parts using a seq2seq model that's been fine-tuned on the test set. Formal: We use these modified TFIDF vectors as features to calculate the phrase translation probabilities. For the regular translation model, we adjust its parameters by summing over all the possible phrase pairs in the target language. Formal: The model is trained using the cross-entropy loss function, which helps us figure out the best translation pair for each test set. The training goal is set based on the number of phrase pairs in the training set. Formal: We train the model using maximum entropy training, which is a version of the EM algorithm (from Freitag and Schapire, 1998). Formal: The translation model is built using a maximum entropy classifier. Formal: We calculate the phrase translation probabilities for all phrase pairs in a test set using the standard EM model. Formal: We train the model by minimizing the cross-entropy loss using the cross-entropy loss function from the EM algorithm. Formal: The phrase translation probabilities for each test set are shown in Figure 1. Formal: We use the phrase pairs that score the best on the development set to create phrase translation probabilities for the test set. Formal: We train the model using maximum entropy training, which is a version of the EM algorithm (from Freitag and Schapire, 1998). Formal: In this case, we combine the probability of the phrase pairs we got from the development set with the probability of the phrase pairs in the test set. Formal: Formal: We train the model using the cross-entropy loss function from the EM algorithm. Formal: We train the model using the EM algorithm, which is an EM-based model, to maximize the cross-entropy loss. Formal: Formal: In this case, we combine the probability of the phrase pairs we got from the development set with the probability of the phrase pairs in the test set. Formal: Formal: Formal: We train the model using maximum entropy training, which is a version of the EM algorithm (from Freitag and Schapire, 1998). Formal: Formal: Formal: We train the model using the cross-entropy loss function from the EM algorithm. Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "You can check out our PCRF-Seq2Seq implementation here: https://github.com/UKPLab/coling2016-pcrf-seq2seq",
        "formal_text": "Our implementation of PCRF-Seq2Seq is available at: https: //github. com/UKPLab/coling2016-pcrf-seq2seq",
        "GPT2_formal_text": "ed (C) = (1 - tanh(C * c) + tanh(C * a)). Formal: The question we're looking at is: How does the search process actually make things better? Basically, the performance of the search is all about how well it connects the question to the answer. Formal: The idea is that if a question and an answer are connected in a certain way, the answers should also match up in that same way. Formal: We're using the Stanford CoreNLP toolkit (Manning et al., 2014) to build our question-answer pairs. Formal: The feature vector for the answer s is defined as the sum of the vector representations for the question q and the answer a. Formal: If we have a question q, we can create a query q by adding the query representation q to the query representation a. Formal: After that, we use the attention (b) to predict the label y for the answer a. Formal: For each question q, we create a query q, which is basically a group of query sentences. Formal: For each answer a, we create a query a by combining the query representations q q with the query representation a. Formal: We use the attention (b) to predict the label y for the answer a. Formal: We pick the best answer and then calculate the attention (b) to get the final answer label. Formal: Using this process, the final answer label is calculated as: Formal: We use the attention (b) to predict the label y for the answer a. Formal: We pick the answer a and calculate the attention (b) to get the final answer label. Formal: We pick the best answer and then calculate the attention (b) to get the final answer label. Formal: We pick the answer a and calculate the attention (b) to get the final answer label. Formal: For the top-k most likely answers, we calculate the attention (b) to get the final answer label. Formal: We pick the answer a and calculate the attention (b) to get the final answer label. Formal: We pick the answer a and calculate the attention (b) to get the final answer label. Formal: We pick the answer a and calculate the attention (b) to get the final answer label. Formal: We pick the answer a and calculate the attention (b) to get the final answer label. Form",
        "directory": "C16",
        "filename": "C16-1160.json"
    },
    {
        "casual_text": "It's just a mix between the one-hot target distribution \\( p_v \\) and the smoothing prior \\( 1_V \\), where \\( m \\) (which is between 0 and 1) decides how much of each to use. You can figure this out using either the divergence inequality or the Lagrange multiplier method. Check out Appendix A for more details.",
        "formal_text": "which is simply a linear interpolation between the one-hot target distribution p v and the smoothing prior 1 V, with m ∈ [0, 1] being the interpolation weight. One can use either the divergence inequality or the Lagrange multiplier method to obtain this result (see Appendix A).",
        "GPT2_formal_text": "= {(p_i, f_i) | x_i} = {[CLS], [SEP]} = {[CLS], [SEP], [SEP]}, where P_i, f_i are the label-specific probabilities assigned to the input tokens. Formal: The input sentence is broken down into a bunch of tokens, each of which is given a label. So, x_i = {x_i1, ..., x_iT}, where T represents the total number of tokens in the sentence. Formal: We calculate a bunch of multi-label predictions using cross-entropy loss. Formal: We take the summaries of words at the words that have labels, like a_i = a_i1, ..., a_iT, and use the multilingual BERT model (Devlin et al., 2019) to predict these labels. Formal: In this paper, we came up with a multi-label classifier that can use the actual labels for training and prediction. Formal: Since the model can be trained and fine-tuned with just a few labels, we came up with a multi-label learning setup where we train the model using labels from a smaller set. Formal: So, basically, we have three main parts: Formal: We tested how well the multi-label classifier works by checking how well it can predict labels. Formal: We also tried adding a third part to help with classification mistakes in the multi-label case. Formal: The multi-label learning setup we used in this paper is pretty similar to the cross-entropy loss used in some previous studies (like Xu et al., 2019; Xu et al., 2020). Formal: Unlike those earlier studies, we built a multi-label classifier with fewer labels and used it to predict labels in both the supervised and unsupervised cases. Formal: We also added a few extra features to the model. Formal: For instance, we added a support vector for the non-English token (e_i) to the text segmentation layer. Formal: We also added a dropout operation to the dropout layer. Formal: Lastly, we added a weighted linear layer that combines the dropout layer and the softmax layer. Formal: Formal: We tested how well the model handles the softmax layer by training it with an extra term, λ_softmax. Formal: We",
        "directory": "aacl",
        "filename": "2020.aacl-main.25.json"
    },
    {
        "casual_text": "A bold result means it's way better (like, p < 0.01 in a t-test) compared to the other stuff in the same box.",
        "formal_text": "A result in boldface indicates that it is statistically significantly better (p < 0. 01 in pairwise t−test) than the others in the same box.",
        "GPT2_formal_text": "ed = (1, 2, 2, 3, 4)  \nFigure 5: A quick comparison between the model's outputs. The base model's output is a straight line, the attention-based model's output is a dashed line. Formal: We're gonna use the attention mechanism in two different ways: (1) A simple way where we focus on the parts of the output that are most important for the model to figure out (like the 2-gram window), and (2) A more complex way where we take into account the whole output (like the 3-gram window). Formal: We're gonna use the attention mechanism in two different ways: (1) A simple way where we focus on the parts of the output that are most important for the model to figure out (like the 2-gram window), and (2) A more complex way where we take into account the whole output (like the 3-gram window). Formal: We're gonna use the attention mechanism in two different ways: (1) A simple way where we focus on the parts of the output that are most important for the model to figure out (like the 2-gram window), and (2) A more complex way where we take into account the whole output (like the 3-gram window). Formal: We're gonna use the attention mechanism in two different ways: (1) A simple way where we focus on the parts of the output that are most important for the model to figure out (like the 2-gram window), and (2) A more complex way where we take into account the whole output (like the 3-gram window). Formal: We're gonna use the attention mechanism in two different ways: (1) A simple way where we focus on the parts of the output that are most important for the model to figure out (like the 2-gram window), and (2) A more complex way where we take into account the whole output (like the 3-gram window). Formal: We're gonna use the attention mechanism in two different ways: (1) A simple way where we focus on the parts of the output that are most important for the model to figure out (like the 2-gram window), and (2) A more complex way where we take into account the whole output (like the 3-gram window). Formal: We're gonna use the attention mechanism in two different ways: (1) A simple way where we focus on the parts of the output",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "So, we've got 450 training examples, which is way less than 1% of the whole TyDi QA training set. If we double that to 100 examples per language, we see a boost in performance, with an average F1 score of 71.7. Adding even more data—like 500 examples per language, which totals 4,500 examples—makes things even better, bringing the average F1 score up to 76.7. Basically, with less than 10% of the training data, we've closed the performance gap by over 82%. And for some languages, like Finnish, the improvement is even bigger—like over 92%.",
        "formal_text": "Results ( We note that the total 450 training instances represent less than 1% of the full TyDi QA training set! Doubling that amount of data to 100 examples per language further increases downstream performance to an average overall F1 score of 71. 7. Going further to the point of adding 500 training instances per language (for a total of 4500 examples) leads to even larger improvements for an average F1 score of 76. 7. That is, using less than 10% of the available training data we can reduce the average F1 score performance gap by more than 82%. For a few languages the gap reduction is even more notable, e. g. , more than 92% for Finnish.",
        "GPT2_formal_text": "_t = [t1, t2, ..., tN] q_t = [q_1, q_2, ..., q_T] ∆ {q_t ∈ Q_t} ∆ {q_t ∈ Q_t + 1} π_t = [σ_t1, σ_t2, ..., σ_tT] π_t + = [σ_t1, σ_t2, ..., σ_tT + 1] π_t + = [σ_t1, σ_t2, ..., σ_tT + 1]\n\nIn simpler terms, the score for (q_t, q_t + 1) is calculated by multiplying together two vectors, q_t and q_t + 1. The score for (q_t, q_t + 1) is calculated using the combination of these two vectors. The total score is π_t, which is just the sum of all these scores. Formal: The \"solve_t\" function takes the source sentence S and turns it into a solved form. Formal: In the solve_t function, the source sentence is given as the input, and the solved form is produced using the function f. Formal: The solve_t function for S has two main steps: First, the map is applied to the source sentence. Then, the solution is produced by applying the mapped source sentence to the solved form. Formal: The first step in the solve_t function is the mapping. For each source sentence in S, a mapping is applied to it. Formal: We're using the Hungarian algorithm to handle the Hungarian language. Formal: We use the global transformer (from Michel et al., 2005) to handle the Hungarian language. Formal: We also use the type-of transformer to handle the Hungarian language. Formal: For Hungarian language, we use the lexical transformer (by Vogel et al., 2005) for Hungarian language. Formal: The lexical transformer and the type-of transformer are implemented in a simple way. Formal: Formal: The type-of transformer is also implemented in a simple way. Formal: We also use the unification transformer (by Schreuder, 2007) for Hungarian language. Formal: Formal: Formal: For Hungarian language, we use the unification transformer for",
        "directory": "acl",
        "filename": "2021.acl-short.79.json"
    },
    {
        "casual_text": "Alright, let’s break this down in simpler terms. So, in the last section, we talked about how the real world—which is super complex and never-ending—is described using languages that have a limited number of words. Because of this, words in any language don’t really have super clear or exact meanings. They kind of cover a range of things or events. \n\nThis fuzziness in word meanings makes translating tricky. In one language, a single word might cover a bunch of similar things or events, but in another language, you might need different words to describe each of those things. It’s like one word doing the job of several in another language (check out Figure 6 for an example).",
        "formal_text": "6 Implicit i[nformation The discussion in the last section can be summarized thus; Because a continuously infinite physical-/mental world is described by a natural language which has only finite words, words in individual languages are used to describe certain ranges o[ events/objects. That is, 'meanings' of words a~: t quite vague. This vagueness causes difficulties of lexical choice in translation by the fact that certain families of events/objects which can be described by the same words in one language should be described by several different words in other languages (Fig. 6 ).",
        "GPT2_formal_text": "_c is equal to the cross-entropy of the attention token c_i. The matrix W_h is a matrix of size d_h, and it's in R^d. Formal: In the next part, we're gonna check if it's possible to encode the hidden state h_i using just the attention weight α_i. Formal: The attention weight α_i is calculated using equation (6). Formal: Formal: In the rest of this section, we'll talk about the hyperparameter W_h, which we can figure out using the cross-entropy loss (L_ent) instead of just using the attention weight α_i. Formal: Formal: For the experiments, we're using the same setup as in the original paper, which you can find here: https://github.com/e-int-nlp/Thin-Machine-Learning Formal: We're using the same model and set of parameters as the original paper. Formal: We're using the default settings for the multi-head attention module and the hyperparameters that were used in the original paper. Formal: Formal: For the latent variable W_h, we're using equation (8). Formal: Formal: For the multi-head attention module, we're using equation (9). Formal: Formal: Formal: We're also using the same setup as in the original paper. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1155.json"
    },
    {
        "casual_text": "The whole point of using DCGs is that they make things clearer and more organized.",
        "formal_text": "The rule based nature of a DCG result %n systems of greater clarity and modularity.",
        "GPT2_formal_text": ". It's like a recipe for making sentences that actually make sense and sound right. Formal: In this part, we'll talk about the big challenges we faced when translating from English to Chinese. Formal: We looked at the first 100,000 words of each sentence from the test set and combined them into one sentence. The sentence is made up of these 100,000 words. For a translation to sound natural, it needs to match each word with its corresponding English word. Formal: We started by translating the sentence using a phrase-based SMT system. But this approach didn't work out well because it only used the words in the translated sentence and didn't consider the surrounding context. Formal: To fix this, we switched to a sentence-level SMT system. This one helps the system deal with the bigger picture by including all the important details in the sentence. Formal: We trained this sentence-level SMT system using all the data in the test set. Formal: In this setup, we use a special phrase table to translate the sentences. The phrase table has all the translated sentences and their translations. Formal: We tested this on the test set to see how it stacks up against the system that was trained on the entire test set. Formal: Our results show that our system did way better than the system that was trained on the whole test set. Formal: For the rest of the paper, we'll just use the phrase table translation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "A83",
        "filename": "A83-1010.json"
    },
    {
        "casual_text": "Discourse relations usually have a direction. Our QA format shows this by putting discourse units either in the question or the answer. In some question types, the order is fixed by the question itself. For example, in Table 4, ex. 1, since the question asks about the condition, the condition will always be in the answer. \n\nThere’s also a pattern for symmetric relations, where the meaning stays the same no matter how you arrange the question and answer, like in ex. 2 in Table 4. \n\nLastly, some relation types, like cause-effect (reason vs. result) or certain time-related ones (before vs. after), are considered reversed. In these cases, two QA pairs with different question types can actually mean the same thing if you swap the discourse units, as seen in ex. 3 in Table 4. \n\nThese directionality patterns affect how we annotate and evaluate things, which we’ll talk about later.",
        "formal_text": "Relation Directionality Discourse relations are often directional. Our QA format introduces directionality by placing discourse units into either the question or answer. In some question prefixes, a single order is dictated by the question. As seen in ex. 1 of Table 4, because the question asks for the condition, the condition itself will always be in the answer. Another ordering pattern occurs for symmetric relations, meaning that the relation's assertion remains the same no matter how the arguments are placed into the question and answer, as in ex. 2 in Table 4. Finally, certain pairs of relation senses are considered reversed, such as for causal (reason vs. result) and some of the temporal (before vs. after) question prefixes. In this case, two QA pairs with different question prefixes can denote the same assertion when the target discourse units are reversed, as shown in ex. 3 in Table 4. These patterns of directionality impact annotation and evaluation, as would be described later on.",
        "GPT2_formal_text": "ed: The input is a masked token, and the output is a word. s: The sequence token representation at level zero. Formal: In the case of a single-word token, the position embedding can be calculated as the product of the current word's position embedding and the current input word's position embedding. Formal: In the case of a multi-word token, the position embedding can be calculated as the product of the current input word's position embedding and the previous input word's position embedding. Formal: For the multi-word token, the attention mechanism can be split into two parts: one for each word type. In the first part, the attention focuses on the input word's position and the previous input word's position, while in the second part, it focuses on the output word's position and the previous output word's position. Formal: For the multi-word token, the attention mechanism can be split into two parts: one for each word type. Formal: For the multi-word token, the attention mechanism can be split into two parts: one for each word type. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "The decor could use a little improvement, and having a small bar would make the place feel more welcoming.\n\nFigure 6 shows some examples of how BART (a machine learning model) handles different sentences. Let me break it down for you:\n\n- **(a)**: In this case, the category \"miscellaneous\" isn't mentioned in the sentence, so BART can't figure it out. But our method can still combine different sentiments from different aspects to get the right answer.\n\n- **(b)**: The sentence says, \"the value on the kids menu is good.\" BART might get confused because \"good\" is talking about the value, not the menu itself. But our method gets it right and doesn't get distracted by other parts of the sentence.\n\n- **(c)**: This one's a bit tricky because it involves \"if\" statements, which are hard for BART to handle. But our method can still figure out the negative sentiment in \"if there was... would be a bit more inviting.\"\n\nSo, our method does a better job than BART in these tricky situations.",
        "formal_text": "< menu: neutral > < incorrect output: positive > The decor could be a bit better, and if there was a small bar the overall atmosphere would be a bit more inviting. < place: negative > < incorrect output: neutral > Figure 6: Examples of BART classification. (a) is an instance with category do not occur as term in sentence. (b) represents that our method is not affected by the surrounding interference information. (c) needs conditional reasoning for analysis. Our method can obtain correct sentiment polarity. Figure 6 shows typical examples from the test set which cannot be inferred by the BART classification model. In sentence (a), the given category miscellaneous does not occur as a term in the given sentence. Our method can synthesize different sentiment polarities with different aspects to obtain correct polarity. In sentence (b), \"the value on the kids menu is good\", good modifies the value, rather than the given category menu. Our method gives the correct polarity, not being affected by the surrounding other aspect sentiments. The last instance (c) has conditional reasoning which is difficult for BART classification. In contrast, BART generation gives the correct label by correctly recognizing the negativity in \"if there was. . . would be a bit more inviting\".",
        "GPT2_formal_text": "= []. Formal: If a sentence has a verb and a noun, we make a simple vector called v_w_n (where n is the number of words in the sentence). Formal: After that, we use the averaged perceptron to figure out the importance of all the possible noun phrases that make sense. Formal: Next, we run the same weighted dot product thing on the unigram features vector. Formal: We do this for all possible noun phrases and count how many of them show up in the training set. Then, we add up all the counts and divide them by the total number of noun phrases in the training set. Formal: Lastly, we calculate the sum for each noun phrase in the training set. Formal: Next, we predict the probability of the next sentence using these probabilities and the word vectors we got in step (1). Formal: Formal: Formal: To make it easier to interpret the different probabilities, we use something called TFIDF notation. We write the probabilities like this: Formal: Here's how we do TFIDF calculations. Formal: We use TFIDF to find the TF-IDF formula that best matches the probabilities. Formal: The scores for the possible noun phrases are shown as a matrix of TF-IDF numbers. Formal: For a set of probabilities P = (p_1, . . . , p_n), we calculate the TF-IDF scores like this: Formal: The TF-IDF scores for the possible noun phrases are calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.361.json"
    },
    {
        "casual_text": "After that, we sifted out 95.5 million paper clusters using the criteria we mentioned earlier and took them out of our dataset. You can check out the breakdown of these filtered papers in Table 2. Turns out, a whole bunch of these clusters got the boot—80 million of them didn’t have any publisher-provided abstract or PDF, and 13 more came from the Unpaywall 2019-04-19 data dump. We used the cld2 tool to check the language, setting a threshold of 0.9 for the English language score. These papers without much text aren’t super helpful for our dataset right now. While they might still be useful as citation points in S2ORC, they’re generally not as high-quality, so we filtered them out to make the dataset better overall.",
        "formal_text": "Subsequently, 95. 5M paper clusters are filtered out based on the aforementioned criteria and removed from the corpus. The distribution of filtered papers is given in Table 2. We note that a large number of paper clusters are filtered out; 80. 0M of these filtered clusters have no associated publisher-provided abstract or associated PDF and 13 Unpaywall 2019-04-19 data dump 14 We use the cld2 tool for language detection with a threshold of 0. 9 over the English language score. do not provide significant value to our dataset in their current state. Although these papers that lack text may be useful as cite-able nodes in S2ORC, they are generally of lower quality and are filtered out of the corpus to improve corpus quality.",
        "GPT2_formal_text": "ed. Formal: The function f(x) is like a normalization function, and it's usually used in NLP. Formal: If the attention vector is g_e^T x, then f(x) = 1, which means the attention is focused on the first x. But if the attention vector is g_e^T x, then f(x) = 0, which means the attention is all over the place. Formal: If the input is a bunch of words in the target language, e_t, the function f(x) is the same as the cross-attention function, which we get by using the input to the network. Formal: The e_t vector for a target word x is calculated using this formula. Formal: In the context layer, we use the cross-attention function (f(x) = 1, g) to make the attention vector (g_e^T x) more focused on the context vector. Formal: In the classifier layer, we get the classification function (f(x) = 1) by using the cross-attention function (f(x) = 1, g). Formal: In the label layer, we use the classifier function (f(x) = 1) to predict the label for each target word x. Formal: We also added some extra details to this formula to make it clearer. Formal: The function f(x) is a global optimization problem, and it's usually solved with optimization problems. Formal: We used the trainable parameters from the transformer model to help the model figure out the right attention weights. Formal: We'll quickly go over the specific equations that describe how we get the attention weights. Formal: Formal: The cross-attention function (f(x) = 1, g) is defined based on Equation 2. Formal: To get the attention vector (g_e^T x), we calculate the cross-attention weights (g_e^T x) using the input. Formal: Formal: In the encoder layer, we're using the cross-attention function (f(x) = 1, g) and the attention (g_e^T x) to get the attention vector (g_e^T x). Formal: We're also including the classifier weights (f(x) = 1) and the label weights (f(x) =",
        "directory": "acl",
        "filename": "2020.acl-main.447.json"
    },
    {
        "casual_text": "In these patterns, a sentence shows how something is part of something else. They use certain verbs. The \"part\" and \"whole\" words are usually in noun phrases or prepositional phrases with specific prepositions. We use these patterns in our app.",
        "formal_text": "In these patterns, the part-whole relation is indicated in a sentence. The patterns contain specific verbs. The part word and the whole word can be found inside noun phrases or prepositional phrases which contain specific prepositions. We utilize the following patterns in our application.",
        "GPT2_formal_text": "= {u1, u2, ..., uT}. Formal: We're using a beam search decoder that works with a set of output tags T and generates a sequence of tokens with ID t. The ID tag shows which token in the reference text fits best with the current state of the system. Formal: The length of the input sequence is determined by the length of the input sequence, which is L. Formal: The state transitions π t (t) are all based on π i = π i1, ..., π j. Formal: We have N samples, each sampled from a distribution q (i) that’s part of P. Formal: For the first transition, we calculate the likelihood P(s i1, s j1, ..., s i+1, s j+1 ) by summing over all possible tag sequences (s i1, ..., s j1 ) that are in P. Formal: The first transition is also calculated as P(s i1, s j1, ..., s i+1, s j+1 ) multiplied by the average tag sequence probability P(s i1, s j1, ..., s i+1, s j+1 ). Formal: We’ve got a set of m different permutations T = {T1, T2, ..., Tm}. Formal: For each transition we’re looking at, we pick the k most likely tag sequences from T. Formal: So, the transition π t (t) is made by combining all these probabilities together. Formal: Here’s the formula for the initial tag sequence: Formal: π t (t) = (1 − π T1) * π T2 + (1 − π Tm) * π Tm Formal: Similarly, for the next transition, we calculate π t (t) using π T1 + (1 − π Tm) + (1 − π Tm) + (1 − π Tm) Formal: Formal: We also calculate a vector called π t (t) using the same setup as the previous transition, which is a vector with m units. Formal: The final tag sequence is calculated by combining all the tag sequences from T. Formal: Formal: In this paper, we’re using the definition of a transition sequence",
        "directory": "C10",
        "filename": "C10-2167.json"
    },
    {
        "casual_text": "1. Input: T is the set of MeSH terms (the topic we're searching for); n + is the number of positive examples we have; IR and θ T are a black-box machine learning-based IR system and its query settings.",
        "formal_text": "1. Input: T ← set of MeSH terms (the retrieval topic); n + ← number of labeled positive data; IR, θ T ← a black-box MLT IR engine, along with query parameters.",
        "GPT2_formal_text": "ed([]) For each word w in the source sentence, we grab the attention vector h from the response vector and add it to the output vector of the attention-based model. This is done using a fully-connected layer with a hidden size of d_m, which has an input dimension of d_m + d_t + d_s. Formal: To get the embedding for the target word x_t, we use a BERT-based model. Formal: The attention mechanism works with a hidden layer size of d_m. Formal: We’re not diving too deep into neural networks for now, but we’re planning to explore how to use other encoder-decoder models like LSTMs, RNNs, and so on. Formal: For the first experiment, we’re using an encoder-decoder model called H2O, which was introduced by Hochreiter and Schmidhuber back in 1997. Formal: We’re skipping the implementation details for this part since they’re not important for the actual project. Formal: Next, we’ll cover another language, and then the related framework we’re proposing will be explained. Formal: Here, we’ll explain the proposed setup, along with the model structures. Formal: Here’s the link to our website: http://www.cs.cmu.edu/~mflt/nlp Formal: On the flip side, we’re planning to create a few more datasets by mixing existing ones with a bit of tweaking. Formal: We’ve also got some unsupervised experiments under the experiments/ folder. Formal: Formal: To get the embedding for the target word x_t, we use a BERT-based model. Formal: We’re skipping the implementation details for this part since they’re not important for the actual project. Formal: Formal: We’ll cover another language, and then the related framework we’re proposing will be explained. Formal: Here, we’ll explain the proposed setup, along with the model structures. Formal: Here’s the link to our website: http://www.cs.cmu.edu/~mflt/nlp Formal: On the flip side, we’re planning to create a few more datasets by mixing existing",
        "directory": "eacl",
        "filename": "2021.eacl-main.47.json"
    },
    {
        "casual_text": "DISAANA keeps an eye on tweets as they happen, spotting info related to disasters and showing it in a neat, organized way. It works in two ways: QA and problem-listing. In the QA mode, you can just ask something like, \"What’s running low in Kumamoto?\" and it’ll give you a list of stuff that’s in short supply (check out Figure 2a). The answers are sorted into categories like medical supplies to make them easier to understand. You can also see the shortages on a map (Figure 2c). In the problem-listing mode, you don’t even need to ask—it just gives you a list of issues happening in a certain area, like \"people trapped under rubble,\" using a method from Varga et al. (2013) (Figure 2b).",
        "formal_text": "DISAANA analyzes tweets in real time, discovers disaster-related information, and presents it in organized formats. It has two modes: QA and problem-listing. In the QA mode, for example, a user can enumerate goods in short supply in Kumamoto merely by asking, \"What is in short supply in Kumamoto? \" (Figure 2a ). The answers are classified by such semantic categories as medical supplies for readability. A user can also enumerate them on a map (Figure 2c ). In the problem-listing mode, a user can obtain a list of problems, such as \"people were buried alive, \" which are occurring in a specified area (prefecture, city or town) without questions by using Varga et al. (2013) 's method (Figure 2b).",
        "GPT2_formal_text": "= { ..., {}, wp_1 = [ ..., {}, ..., {}, wp_n = [ ..., {}, ..., {}, ..., {}, ..., {}], h_1 = [ ..., {}, h_n = [ ..., {}, {}, ..., {}, ..., {}, ..., {}], b_1 = [ ..., {}, b_n = [ ..., {}, {}, {}, ..., {}, ..., {}, ..., {}], ..., {}] Formal: The whole idea of using monolingual embeddings to build context representations is kind of tricky to get right. But, we found that using bilingual word embeddings actually helps a bit in this process. Formal: So, the models we talked about earlier are basically a mix of word and context embeddings. They're trained together and then used to translate a source sentence into a target language. But the thing is, this process only works with one language, and that's where the similarity between the two languages ends. Formal: We used the same basic model setup as in (He et al., 2017) to evaluate the results. Formal: We also tried using something called binary cross-entropy loss on the target side. We used this as a baseline. We ran the evaluation on the target side of our monolingual models, with a batch size of 10. Formal: We also had the monolingual models trained for up to 200 epochs. Formal: We checked how well the results matched up with human judgments. Formal: Formal: We used the same evaluation methods as in (He et al., 2017) to evaluate the results. Formal: The results were basically the average of the three evaluations we made for each model. Formal: Finally, we calculated the average score based on the training set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-2055.json"
    },
    {
        "casual_text": "Psychological theories about irony, like echoic reminder theory (Kreuz and Glucksberg, 1989) and implicit display theory (Utsumi, 2000b), haven’t really been fully applied to analyzing text yet. Neuropsychology researchers who’ve looked into how the brain reacts to sarcasm say that understanding it depends a lot on not just the context of what’s being said, but also the speaker’s mood, personality, and things like facial expressions and tone of voice (Shamay-Tsoory et al., 2005). Without those non-verbal cues, figuring out sarcasm from just text has to rely mostly on the words and the situation, though you can still guess a bit about the speaker’s personality and mood from the text.\n\nThere are some fancy models that use things like the contrast between positive words and negative situations (Riloff et al., 2013), specific words, punctuation, and even emojis (González-Ibánez et al., 2011) to detect sarcasm. They work pretty well, but they usually miss out on understanding the speaker’s mindset and the broader context of what’s being said. Kreuz and Link (2002) point out that sarcasm is more likely when the speaker and listener share a lot of knowledge—like understanding the situation and knowing each other well.",
        "formal_text": "Psychological theories of irony, such as echoic reminder theory (Kreuz and Glucksberg, 1989) and implicit display theory (Utsumi, 2000b ) have yet to fully translate into text-analytic methods. Neuropsychology researchers who have sought patterns of brain activity to identify the neural correlates of sarcasm note that an understanding of sarcasm is highly dependent not just on the context of an utterance but on the state-of-mind and personality of the speaker, as well as on facial expressions and prosody (Shamay-Tsoory et al. , 2005). Without the latter markers, purely textual detection must depend largely on the content and context of an utterance, though speaker personality and state-of-mind can also be approximated via text-analytic means. Probabilistic classification models that exploit textual cues -such as the juxtaposition of positive sentiment and negative situations (Riloff et al. , 2013), discriminative words and punctuation marks, and emoticon usage (González-Ibánez et al. , 2011) have achieved good performance across domains, yet these models typically suffer from an absence of psychological insight into a speaker and topical insight into the context of utterance production. Kreuz and Link (2002) argue that the likelihood of sarcasm is proportional to the amount of knowledge shared by speaker and audience, which includes knowledge of the world and knowledge of the speaker and audience.",
        "GPT2_formal_text": "_layers(D, i, D1) = (V_T * h_t) * H(D1, i)  \n\nBasically, the attention matrix is a dense matrix with D rows and D columns. It's a dense matrix with dimensions (V_T * h_t) * H(D1, i) that's been normalized by a negative number to get a single number. Then, it's turned into a scalar vector using something called linear projection. Formal: To make sure the attention weights stay the same across different areas, we just add up the attention weights across all the areas. Formal: In our setup, the length of a sentence is set to the average of the character embeddings in its first n-gram. Formal: We use an LSTM for the output of this LSTM, which is a hidden state matrix with n rows and d columns. Formal: We use the output from the LSTM to calculate the output for a single word, which is a vector with d dimensions. Formal: We also include the word embedding vectors from the LSTM. Formal: The final representation of a word is a matrix with d rows and d columns. Formal: The response vector h_t is created by combining the output from the LSTM and the word vector vectors. Formal: The model outputs are a single number, D_R, which is calculated by multiplying the attention weights between the last hidden state and the current word embedding vector. Formal: The final output vector h_t is a matrix with d rows and d columns. Formal: The response vectors are a single number, D_R, which is calculated by multiplying the attention weights between the last hidden state and the current word embedding vector. Formal: Lastly, we use the last hidden state vector h_t to calculate the output for a single word, which is a vector with d dimensions. Formal: The model outputs are a single number, D_R, which is calculated by multiplying the attention weights between the last hidden state and the current word embedding vector. Formal: The final output vector h_t is a matrix with d rows and d columns. Formal: The response vectors are a single number, D_R, which is calculated by multiplying the attention weights between the last hidden state and the current word embedding vector. Formal: Finally, we use the last hidden state vector h",
        "directory": "D17",
        "filename": "D17-1050.json"
    },
    {
        "casual_text": "During semantic analysis, we take each word or \"leaf\" from the CCG tree we got from the syntactic analysis and give it a meaning based on some semantic templates. Then, we put these words together following the CCG tree structure to create a logical formula that shows the overall meaning of the whole sentence. This process is done using ccg2lambda for Japanese sentences.",
        "formal_text": "In semantic analysis, each leaf (lexical item) of the CCG tree obtained in the syntactic analysis is assigned a meaning from the semantic templates. The lexical items are then combined according to the CCG derivation tree to derive a logical formula that expresses the meaning of the entire sentence. The composition is performed using ccg2lambda in Japanese.",
        "GPT2_formal_text": "= { u i, j } = { u i, j }, and the embedding is basically a mix of the last hidden state, the word embedding, and the last hidden state of the last attention head h t. Formal: For a given word w i, a sequence of tokens y i, a sequence of attention head h t, and a hidden state h t, we figure out the importance score for each token in the word. Formal: To build the attention head h t, we start by making a binary logistic classifier called H t using a softmax layer. Then, we feed this H t into a linear layer to get the final attention representation for each token. Formal: We measure how well the attention works using the CoNLL F1 score (from Liu et al., 2014), which basically shows how well the token's attention weights (or embedding) match up with the attention distribution in the word's hidden representation. Formal: To figure out the attention weights for the last attention head h t, we start by applying a simple linear layer to the H t vector and then calculate the final attention weight for the word's last hidden state. Formal: We also check out how the attention weights for different tokens and attention heads are distributed for each attention head. Formal: Lastly, we check if the attention weights for the last attention head h t are correctly aligned with the attention distribution in the word's hidden representation. Formal: We check out how the attention weights for different tokens and attention heads are distributed for each attention head. Formal: Formal: We also check if the attention weights for the last attention head h t are correctly aligned with the attention distribution in the word's hidden representation. Formal: We also check if the attention weights for the last attention head h t are correctly aligned with the attention distribution in the word's hidden representation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-srw.10.json"
    },
    {
        "casual_text": "The idea that S and I are independent when we know U lets us pick any speech dataset to train P(S|U). This means we can control speaker traits and other sound features separately from the I2U system (Henter et al., 2018; Akuzawa et al., 2018). Table 1 lists the five datasets we used to train the S2U, I2U, and U2S models. We intentionally used different datasets for each part to see how well the units hold up when moving between different areas, like changes in speakers, speaking styles (scripted or spontaneous), and types of content (books, newspapers, or image descriptions). Out of the three datasets with both images and speech pairs—Places, Flickr8k, and MSCOCO—we picked the last two for training I2U models. They have five captions per image, which works better for caption evaluation metrics like SPICE (Anderson et al., 2016). Plus, they're popular image captioning datasets with lots of text-based comparisons in the research. Places only has one spoken caption per image and hasn’t been used for captioning before.",
        "formal_text": "The conditional independence assumption between S and I given the U enables us to choose any arbitrary speech dataset for training P (S | U ), therefore enabling the speaker characteristics and other acoustic properties to be independently controllable from the I2U system Henter et al. , 2018; Akuzawa et al. , 2018). Table 1 summarizes the five datasets used for training S2U, I2U, and U2S models. Note that we deliberately choose different datasets for training each module, which aims to examine the robustness of the units when transferring across domains, including shift in speaker demography, speaking style (scripted/spontaneous), and linguistic content (book/newspaper/image description). Among the three datasets with image and speech pairs: Places, Flickr8k, MSCOCO, we chose the latter two for training I2U models, because they include five captions per image, which is more suitable for caption metrics such as SPICE (Anderson et al. , 2016); moreover, they are commonly used image captioning datasets with many text-based baselines in the literature. Places only contains one spoken caption per image and has not been used for captioning.",
        "GPT2_formal_text": "ed) can be adjusted. We're planning to tweak this model for the neural machine translation (NMT) task, where the training process is a bit different. It should focus on the way we get words from the source to the target, and how that connects to the attention system. Formal: So, we’re adding another hidden layer in the system, which has two parts. One part is about how we handle the word embeddings from the source to the target, and the other part is about how the target word is shaped in the source language. Formal: So, the regular Transformer is a super basic model. We’re using an upgraded version called the Transformer encoder block. It’s got an LSTM encoder block with a length of 512, a hidden layer size of 512, and a batch size of 64. This block is super simple—it doesn’t need any fancy thinking or anything. Formal: Next, we’ll show you the final setup, where we’ve added a softmax layer to help the model learn the best output for a particular source-target context. Formal: Next, we’ll use the target word embedding space. This will help the model make the right predictions. Formal: Once we’ve got the embeddings for the target and source words, we’ll use some basic calculations to calculate the attention values for the source-target context. Formal: After that, we’ll train the model using both the embeddings for the target and source words to get the final output. Formal: To see how the model performs in a real-world scenario, we’re running some tests with different target and source word embeddings. Formal: You can check out the results in Table 2. The model has two main parts: a word embedding layer and a hidden layer that has 512 units, as shown in the figure. Formal: Figure 2: This is the word embedding layer and its hidden layer. Formal: The dataset we’re using for training and testing is called the CoNLL-2009 dataset. Formal: We’ve also included the BERT-base model from the Hugging Face Transformers library. Formal: We’ll show you how it’s trained in the next sections. Formal: We’ve updated the source-target embeddings from the dataset",
        "directory": "acl",
        "filename": "2021.acl-long.411.json"
    },
    {
        "casual_text": "After the session, the system grouped together papers with titles that had words like \"nature,\" \"development,\" \"transfer,\" \"fablizalion,\" or \"generation.\" The user's query, which was in Japanese, was saved in this specific group. This setup means that next time, the system can quickly answer the same query and also add any new papers about the same topic to this group.",
        "formal_text": "The selected papers which included in tile titles words such as nature, development, transfer, fablizalion, or generation were gathered into one subspace by the system after the session, and the user's query was registered above tile subspace. This means that, next time, the system can give the answers to the (*)The query was given in Japanese. This is a tranMation made by the authors same query at once, and can register, in the proper subspace, new papers on tile same notion.",
        "GPT2_formal_text": ") = {np_t}, which are basically the cluster-ids for the entities in the query. The weight matrix for the entity embedding layer is: Formal: The entity embedding layer has a weight matrix, W_e_t = {w_e_t1, ..., w_e_tM}, which basically shows how important each entity is in its cluster. Formal: Each entity embedding layer has a hidden state, h_e_t, and an activation function, h_e_t. Formal: We add these two activation functions together using a weighted sum, which we call Al_t. Formal: Here's the formula for the adaptation loss: Formal: In this model, we take the average of the log-likelihoods of the entity embedding and the query embedding, then average them out. Formal: For each entity embedding layer, we set a hyperparameter called k. Formal: Formal: We use the cross-entropy loss, λ_t(E_i), to measure how much the model's prediction (E_i) differs from the actual label (E_i). Formal: The cross-entropy loss is an iterative method, where we keep updating the weights λ_t for each iteration until the model's predictions stop changing much. Formal: We use the cross-entropy loss for two main tasks: entity classification and entity pair identification. Formal: Formal: We define the global attention loss, which we use to calculate the cross-entropy loss, as: Formal: Here, W_a_t_i are the weights for the entity embedding layer. Formal: Formal: The global attention loss is calculated using this formula: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1089.json"
    },
    {
        "casual_text": "While testing, we notice that people tend to focus more on the words at the beginning of the source text. To see how this \"position bias\" affects things, we picked a group of samples with the same source length (77 sentences) from the WMT15 De→En test set. We then figured out the average attention each source position got within this group. \n\nSince different source positions might be paid attention to different numbers of times in SiMT, we made sure to average the attention weights based on how many times each position was looked at. This way, every source position gets a fair evaluation. \n\nTo break it down: if α ij is the attention weight between a target word y i and a source word x j, the average attention weight A j for source position j is calculated like this:",
        "formal_text": "During testing, position bias is reflected in the preference of paying more attention to the source words in front positions. To explore the specific impact of position bias, we select the samples with the same source length (77 sentences) in WMT15 De→En test set as a bucket, and then calculated the average attention weight obtained by each source position in the bucket. Since the times of each source position being paid attention to may be different in SiMT, the average attention weight is averaged on the times of being attended, so the evaluation is fair for each source position. Specifically, give the attention weight α ij between target word y i and source word x j, the average attention weight A j at source position j is calculated as:",
        "GPT2_formal_text": ". The final output is a vector that includes the attention and its values, like a hidden representation. Formal: The generation part of the model figures out the attention weights for all the tokens in the query. Formal: For our second task, we grab the top-k words from the response that are in the topic. Formal: We start by creating a topic representation, θ t, by combining the representations of the tokens. Then, we use this to calculate the attention weights for all the words in the response. Formal: To get the topics, we start by taking the words from the response and feed them into the attention mechanism. Then, we calculate the attention weights for each topic and turn them into vectors, called topics. Formal: Finally, we use the topic vectors to calculate the attention weights for the words in the response. Formal: The model calculates the attention weights using a simple linear process, which is explained in Algorithm 1. Formal: The attention weights for the words in the response are calculated using the attention mechanism. Formal: We combine the vectors for the word topics using the multi-head attention mechanism. Formal: For the last task, we predict the sentiment of the words in the response. Formal: The model calculates the sentiment of the words in the response using a standard linear process. Formal: The final sentiment vector is a scalar that represents the sentiment of the response. Formal: We use the predictions from the model's log-likelihood function, which we call the prediction loss, to predict the sentiment of the words. Formal: Formal: The model performs best when it has a good validation loss. Formal: Formal: The model also performs best when the validation loss is small. Formal: Formal: The model takes a step back and uses the attention mechanism to predict the sentiment of the words. Formal: Formal: The model also performs best when the validation loss is high. Formal: Formal: We use the sentiment vector to predict the words in the response. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.467.json"
    },
    {
        "casual_text": "2. The candidate people look at a lot, but isn't the right one, usually comes right after the correct candidate in the list.",
        "formal_text": "2. The most frequently fixated candidate other than the correct one follows the correct candidate in the ranking.",
        "GPT2_formal_text": "= (h1, ..., hK) ∈ R m×d for all the words in the document d and the chunk i. Formal: We use a loss function L_DP to figure out the logit for all the words in the document d. Formal: To make sure we're comparing things fairly, we set the batch size to n. For each iteration, we randomly pick a group of words from the document d. Formal: The scoring function takes into account the co-occurrence of words between the chunk i and the chunk j. In short, the score for a word is how often it shows up between the chunk i and the chunk j. Formal: We tweak the model's parameters using the training data from the last round to get a better prediction. Formal: We use a linear layer to map the document pairs into a probability distribution. Formal: After that, we average the predictions from all the tokens to get a summary of the document embedding space. Formal: Using a linear layer, we map the document pair embedding space into a probability distribution for the chunk pair embedding space. Formal: Finally, we average the predictions from all the words in the document d to get a document embedding representation. Formal: To make sure we're comparing things fairly, we set the batch size to n. Formal: Formal: If we have the correct document embedding r_d, the loss function L_DP can be written as L_DP(r_d). Formal: The training data from the last round is used as the input for the optimization process. Formal: We use the LeakyReLU hyperparameter, γ, as the loss function. Formal: We adjust the values γ to get the best values for document and chunk embedding embedding space. Formal: Formal: The learning rate γ is set to 0.0001. Formal: The loss function L_DP is adjusted based on the training data from the last round. Formal: We use the log loss, L_DP, to optimize the document and chunk embedding embedding embedding spaces. Formal: Formal: Finally, we average the results from all the words in the document d to get the document embedding representation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1269.json"
    },
    {
        "casual_text": "Customized engines are usually worth the cost when there's a strong case for a good return on investment and you're only dealing with a few language pairs.",
        "formal_text": "Because of the expense of customized engines, they are more easily justified in cases where there is a clear ROI argument, and where the number of language pairs is limited.",
        "GPT2_formal_text": "= [s_mask, s_tgt] s_tgt = [s_mask, s_gt] x = [x_mask, x_tgt] n_tgt = [n_tgt, n_mask] end\n\nIn the joint model (check out equation (1)), the input and output are represented as matrices, and the attention weights are written as α_t = 1/2 for the input and α_s = 1/2 for the output. Formal: Using this joint model, we can show that: Formal: In this joint model, the input and output are represented as matrices, and the attention weights are written as α_t = 1/2 for the input and α_s = 1/2 for the output. Formal: The same idea can be applied to a mixture model, as shown in equation (2). Formal: Here, α_t = 1/2, and α_s = 1/2. Formal: Equations (1), (2), and (3) show how to combine two things: Formal: Theorem 1: If σ1 = σ2 = ... = σn, then all the attention weights are 1. Formal: Formal: This theorem basically says that the attention weights are 1 for any pair of words w_i and w_j, and a is the vector of the i-th word. Formal: Formal: We can also set σ1 = σ2 = ... = σn to be the same as σn = 1/2. Formal: Formal: Theorem 2: If σ1 = σ2 = ... = σn, then all the attention weights are 1. Formal: Formal: This theorem basically says that the attention weights are 1 for any pair of words w_i and w_j, and a is the vector of the i-th word. Formal: Formal: Formal: Formal: Formal:\n\nTheorem 3: If σ1 = σ2 = ... = σn, then all the attention weights are 1. Formal: Formal: Formal: This theorem basically says that the attention weights are 1 for any pair of words w_i and w_j, and a is the vector of the i-th word. Formal: Formal: Formal: Formal: Form",
        "directory": "amta",
        "filename": "2010.amta-commercial.5.json"
    },
    {
        "casual_text": "The JULIUS open-source speech recognition engine, which is designed for Large Vocabulary Continuous Speech Recognition (LVCSR), uses n-grams and context-dependent Hidden Markov Models (HMM) to turn spoken words into written text (Lee et al., 2008). How well it works depends on having the right language-specific stuff, like acoustic models, language models, and dictionaries. We’ve got some basic resources set up for English, German, Italian, and French, but they’re still pretty limited. To make things work better, we’ve also added online speech recognition for these languages using Google’s Speech API. This lets us run experiments with users and at the same time gather the data we need to improve and expand JULIUS’s language resources.",
        "formal_text": "The JULIUS open-source Large Vocabulary Continuous Speech Recognition engine (LVCSR) uses n-grams and context-dependent Hidden Markov Models (HMM) to transform acoustic input into text output (Lee et al. , 2008). Its recognition performance depends on the availability of language dependent resources i. e. acoustic models, language models, and language dictionaries. Our framework includes basic language resources for English, German, Italian and French. As those resources are still very limited we have also integrated online speech recognition for these four languages using the Google Speech API. This allows for conducting experiments with users while at the same time collecting the necessary data for augmenting and filling in JULIUS language resources.",
        "GPT2_formal_text": "are used to create a TFIDF vector for each query, which we then use to calculate a meta-distribution p. After that, we add a hidden layer at the query level, which is shown in equation (3). Formal: The query vector p and the resulting structured vector are combined to create a unique vector for each query q, which we call s_q. Formal: We take the hidden state vector for a specific query q and combine it with the latent representation h_q_l_t to get the embedding for the query q. Formal: Basically, the embedding for the query q is calculated like this: Formal: We apply softmax to the query vector to get the probability distribution p(q|q_l_t). Formal: The final result of the softmax is a vector that we call p_q. Formal: Finally, p_q is used to predict the query. Formal: We use the objective function p_q to find the best query vector. Formal: We use the query vector p_q to predict the query. Formal: Finally, p_q is used to predict the query. Formal: Formal: Also, we do this for each query q to figure out the probability distribution p for each query q_i, which we get using the query vector p_q. Formal: Since the query vector p_q is shaped like a square, we can simplify it like this: Formal: We just use the max pooling operation to get the vector for the query. Formal: We do this for each query q_i to get the probability distribution p_q_i, which we get using the query vector p_q. Formal: Lastly, p_q_i is used to predict the query. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E14",
        "filename": "E14-2022.json"
    },
    {
        "casual_text": "Basically, we’re saying that people from ASEAN countries can go study in other places, learn about how things work, and see what other countries have done to succeed. Then, they can bring that knowledge back home to help their own country grow. In the end, this helps some of the poorer people in ASEAN countries because they get better access to development. So, we shouldn’t get rid of ASEAN.\n\nOur last point is about keeping the peace in the region. Southeast Asia has a lot of potential for conflict—some countries are buddy-buddy with China and might like the idea of China being super powerful, while others are closer to the U.S. Some countries are communist, others are capitalist. There’s been trouble in the past, like the conflict over East Timor, and there are still tensions simmering under the surface in many places.\n\nBut one of the best ways to avoid fights between countries is to make sure everyone’s interests are tied together through trade. If everyone benefits from peace and loses out during conflict, it’s way less likely that a war will happen. That’s why we think ASEAN is a great tool for keeping the peace in Southeast Asia.\n\nSo, because ASEAN fights against colonialism, helps with development, and promotes peace in the region, we shouldn’t disband it. Thanks!",
        "formal_text": "So we tell you that people in ASEAN countries can study in other countries and learn about development and industry, and how other countries have been successful in the past, and use this in order to help their own home country. So at the end of the day, we help the people who are worse off in the world, some of them, some of these very poor people who live in ASEAN countries because we better access development so we shouldn't disband ASEAN. Our last argument is about peace in the region. Recognize that there are many potential sources of conflict within the southeast asia region. Some countries are more closely aligned with china so they see an advantage in china becoming more hegemonic, some countries are more aligned with the united states. Some countries are communists, some countries are capitalist. There's been conflict in the past over east timor, and there are other ethnic tensions boiling beneath the surface in many southeast asian countries. But one of the surest ways to prevent international conflict, is to tie everyone's interests together through trade. If everyone stands to get richer through peace and poorer through conflict, then it's much less likely that a war will break out in the region. So for that reason we think ASEAN is a tremendous tool for peace in southeast asia in the future. So because it's an anti colonial institution, because it promotes development, and because it will lead to peace in the region, we should not disband ASEAN thank you. \"",
        "GPT2_formal_text": "_layers_per_token = 2;\n\nt = (1 - e)^2 * (1 - m)^2 * tanh(s - a)^2;\n\n}\n\nThe results are in Table 1. The first row shows the numbers from our training dataset, and the second row shows the results from the first-best model. For the first model, we picked the one with the best F1 score in the validation set. For the second model, we used the model that performed best on the validation set. We also looked at the average F1 score for the first-best model. The results are all listed in Table 1. Formal: You can find the source code for this project at https://github.com/hanxiao/PARADIGMS. Formal: We've put the first-best model, the second-best model, and the validation set results into Table 1. The second-best model was trained on the training data, while the other three were trained on the validation set. Formal: The last two rows show the F1 scores of the first-best model in the validation set. Formal: In Figure 4, you can see the results for the second-best model on the validation data and the fourth-best model on the training data. Formal: The table and figure show some extra details, like the perplexity of the model on the validation set and the F1 score of the fourth-best model on the training data. Formal: Since we don't have enough room in this article, we'll just give a quick summary here. Formal: In the fourth row, there are the results from the fourth-best model on the validation set. Formal: The figure and table show some extra details, like the perplexity of the model on the validation set and the F1 score of the fourth-best model on the training data. Formal: Formal: Here, γ_s represents the standard deviation. Formal: The fifth row is the evaluation set results for the third-best model. Formal: Formal: The fourth row shows the results for the fifth-best model. Formal: The figure and table show some extra details, like the perplexity of the model on the validation set and the F1 score of the fifth-best model on the training data. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "We mix our structure-aware uncertainty sampling with the bachelor recognizer to create the final acquisition function.",
        "formal_text": "We combine our structure-aware uncertainty sampling with the bachelor recognizer to form the final acquisition function:",
        "GPT2_formal_text": "= {query, value} object_ids = {object, value} head = (Heads, Value) info = (exp, title, body) where (exp, title, body) is a tuple with (exp, title, body) instances. Formal: Let's say the label for an instance is \"O\", and for a query, it's \"q\". The dataset D is made up of pairs like (e, q). Formal: We've got a sentence from the dataset D, which we'll call D_s = {s, q}. Formal: From this sentence, we can guess the label (like \"O\" or \"q\"). Formal: Then, we have a question with the answer \"O\" from the dataset D_q. Formal: Finally, we have a set of questions with the correct answer \"O\", and a dataset D_d = {d_q, d_s, q_d}. Formal: We can use a shortcut method called the harmonic mean for this. Formal: In this case, we only look at the labels for the entities. Formal: The main goal of semantic role labeling (SRL) is to figure out the roles (like O, q) from a sentence by checking the labels of the entities. Formal: SRL is a task that focuses on recognizing both entities and their roles. Formal: One key thing to remember is that SRL doesn't need to consider any context, which is key to making it efficient. Formal: So, SRL is like a supervised system that uses a set of labeled data to help us identify the right entities and their roles. Formal: To make SRL work better, we need to use a lot of labeled data. Formal: The main goal here is to create a system that can correctly label things like (entity, role) and (query, value) from the input sentences and also match up with the predefined knowledge base (KB). Formal: Formal: Since SRL needs all the parts of the sentence—like entities and their roles—to work together, we need a bunch of labeled examples to train the system. Formal: Formal: We also need to figure out how to combine these labeled examples into a single, connected dataset for SRL. Formal: Formal: We use a cool method called the harmonic mean to get this whole SRL system. Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.270.json"
    },
    {
        "casual_text": "In the parametric methods, the first approach gives all the hidden states in a layer the same window size to look at the sequence's context. The second approach, though, figures out a different window size for each hidden state.",
        "formal_text": "Among the parametric methods, the first strategy assigns a unified window size to all the hidden states in a layer, so as to consider the context of the sequence, while the second one calculates a distinct window size for each hidden state.",
        "GPT2_formal_text": "ed: A smaller, more focused version of the input attention. Formal: This part just uses the [CLS] token to represent the prompt. Formal: You can find the code and datasets for all these methods over at https://github.com/shuangshuang/Encoder-Decoder. Formal: To help readers, we’ve included some examples in Appendix A.1. Formal: A cool thing about our model is that it doesn’t rely on any pre-existing knowledge. Instead, it learns from the user’s questions and gives them the kind of answers they’d ask. Formal: We also make sure that each token gets a weight based on how important it is in the prompt. Formal: We’ve got a working model, and we’ve included it in our paper. Formal: We’ve also put together a collection of dialogues from different domains. Formal: For future work, we’re thinking about combining the dialogue predictions with a latent semantic space that includes the answers. Formal: We’ve also included some examples in Appendix A.2. Formal: Finally, we’ve included the code and datasets for our model in Appendix A.3. Formal: We’ve also included the code and datasets for our model in Appendix A.4. Formal: We’ve got a working model, and we’ve included it in our paper. Formal: We’ve also put together a collection of dialogues from different domains. Formal: For future work, we’re thinking about combining the dialogue predictions with a latent semantic space that includes the answers. Formal: Formal: We’ve also included the code and datasets for our model in Appendix A.5. Formal: We’ve also included the code and datasets for our model in Appendix A.6. Formal: And finally, we’ve included the code and datasets for our model in Appendix A.7. Formal: We’ve got a working model, and we’ve included it in our paper. Formal: We’ve also put together a collection of dialogues from different domains. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1475.json"
    },
    {
        "casual_text": "For a deeper dive, we zeroed in on three specific features: (11) case marker, (15) intra/inter, and (18) dependency. We then calculated precision (P), recall (R), and F-measure (F) for each combination of these features in our test cases. Table 4 compares the BiReg+F syn model and the FixRank+F syn model, which had the highest overall accuracy according to Table 3. \n\nOne key difference between these two models is that the FixRank model performs better in terms of F-measure for directly dependent arguments, especially for ga arguments. When it comes to directly dependent ga arguments, the FixRank model has a better recall value while maintaining a similar precision value. On the flip side, for directly dependent arguments with other case markers, the BiReg model has better precision.\n\nWe also looked at how many directly dependent arguments each model picked as the answer. The FixRank model selected 19,612 directly dependent arguments, with 13,751 of those being ga arguments. Meanwhile, the BiReg model picked 12,520 directly dependent arguments, including 7,208 ga arguments. These numbers indicate that the FixRank model is more likely to choose directly dependent arguments without much regard for their case markers.",
        "formal_text": "For further analysis, we focused on the specific three features: (11) case marker, (15) intra/inter, and (18) dependency, and calculated precision (P), recall (R), and F-measure (F) for the test cases with each combination of these features. Table 4 shows the comparison between the BiReg+F syn model and the FixRank+F syn model that show the highest accuracy in total in Table 3. The notable difference between these two model is that the FixRank model shows the better Fmeasures for directly dependent arguments, particularly in ga arguments. Concerning the directly dependent ga argument cases, the FixRank shows the better recall value with the similar precision value. In contrast, in the cases of directly dependent 'other' case markers, the BiReg shows the better precision value. We investigated the number of directly dependent arguments that each model selected as the answer. The FixRank model selected 19, 612 directly dependent arguments, and 13, 751 ga argument among them, while the BiReg model selected 12, 520 directly dependent arguments with 7, 208 ga arguments. These numbers suggest that the FixRank model tends to choose directly dependent arguments regardless of their case markers.",
        "GPT2_formal_text": ", tokens, and tags, along with their associated probabilities. Formal: The main idea behind this is to identify the main word or phrase (the head) in the text and figure out its relation, kind of like what Klementiev and Roth did back in 2009. Formal: Once we have the relations, the text encoder figures out the parse tree, which is like a simplified version of the original text. Formal: We use the language model F_m_lm for this. Just like in Eq. (1), we calculate the likelihood by trying to minimize the cross-entropy loss. Formal: To make sure we're dealing with sentences with real entities, we treat the summary like an unsupervised paraphrase of the original document. Formal: For each sentence i, the sentence encoder creates a vector representation z_i, where z_i is a vector of labels. The model's predictions for all the entities in the text are turned into a single probability distribution p_z_i for each entity label. Then, we use the same embedding model to predict the labels for the summary. Formal: To train the model, we first use a feedforward neural network (FFNN) to pull out features like word shape, prefix, and stop token. After that, we combine these features into a single embedding p_z_i for each entity label. Formal: After that, we use the cross-entropy loss to minimize this. Formal: To check how well the model is doing, we evaluate it using the F1 score. Formal: For the entity labels, we look at the main word or phrase (the head) and calculate the vector representation for each entity label. Formal: Finally, we use the BERT embedding model (from Devlin et al. in 2019) to predict the labels. Formal: You can find more details about our model in Appendix A. Formal: We also include an evaluation metric called MSEP, which we calculate based on the document length. Formal: Finally, to compare our results with other models, we measure the F1 score using the ROUGE-1 score. Formal: Formal: We tested our model against three baseline models: CODESCRIBE (Zhang et al., 2019), ELECTRA (Clark et al., 2020), and the standard BERT-base model (from Devlin et al. in 2019). Formal: We trained",
        "directory": "C16",
        "filename": "C16-1269.json"
    },
    {
        "casual_text": "Okay, so softmax(•) is just a function that normalizes stuff along the last dimension, and c(0) is the relation-aware representation we get from the sentence s. After that, we combine this relation-aware representation c(0) with the original sentence representation s using a gate mechanism (like the one from He et al., 2016) and add a residual connection. We also throw in layer normalization (thanks to Ba et al., 2016) for good measure.",
        "formal_text": "where softmax(•) denotes a normalization function along last dimension and c (0) is the resulting relationaware representation corresponding to the sentence s. Then we merge the relation-aware representation c (0) into original sentence representation s by an element-wise gate mechanism with residual connection (He et al. , 2016) and layer normalization (Ba et al. , 2016), i. e. ,",
        "GPT2_formal_text": "_t = [CLS], which is a list of vectors, p_t = [CLS], where each [CLS] is a vector in R^d. Formal: The response function is calculated as: Formal: For each word w_i, we look at the words around it, where the first word w_i is just the start of a new word. Formal: To make sure the attention weights stay the same between different words, we multiply the attention vectors by the cosine similarity between them. Formal: For every word w_i, we check the words around it to find their top-1 vector embeddings. Formal: For a given word w_i, the words around it are sorted based on how similar their embeddings are to w_i. We look at the words in the middle and then sort them based on how similar their embeddings are to w_i. We do this for all possible words w_i, but only if w_i is a word in the vocabulary V. Formal: We use the attention weights to find the words that are most similar to w_i, and then we average them out. Formal: For each word w_i, we check the words around it to find their top-1 vector embeddings. Formal: To get the embeddings for a word w_i, we measure how similar its embedding is to the word w_i. Formal: We use the attention weights to find the words that are most similar to w_i, and then we average them out. Formal: The last part is about finding the words that are most similar to w_i, and then we average them out. Formal: We use the attention weights to find the words that are most similar to w_i, and then we average them out. Formal: For a given word w_i, the words around it are sorted based on how similar their embeddings are to w_i. Formal: We use the attention weights to find the words that are most similar to w_i, and then we average them out. Formal: For each word w_i, we check the words around it to find their top-1 vector embeddings. Formal: We use the attention weights to find the words that are most similar to w_i, and then we average them out. Formal: For a given word w_i,",
        "directory": "coling",
        "filename": "2020.coling-main.145.json"
    },
    {
        "casual_text": "We discovered that CHARAGRAM performs better on word similarity tasks when we use more character n-grams. This is covered in Section 4. Our top result came from the biggest model, SL999 Hill et al. 201452 Schwartz et al. (2015) 56 Faruqui and Dyer (2015) 58 Wieting et al. (2015) 66. 7 CHARAGRAM (large) 70. 6 (Luong et al., 2013), with SL999 for model selection. We got a Spearman's ρ of 47.1, which beats the 41.8 from Soricut and Och (2015) and is close to the 47.8 reported by Pennington et al. (2014), who trained on a 42 billion token corpus.",
        "formal_text": "We found that performance of CHARAGRAM on word similarity tasks can be improved by using more character n-grams. This is explored in Section 4. 4. Our best result from these experiments was obtained with the largest Model SL999 Hill et al. 201452 Schwartz et al. (2015) 56 Faruqui and Dyer (2015) 58 Wieting et al. (2015) 66. 7 CHARAGRAM (large) 70. 6 (Luong et al. , 2013), using SL999 for model selection. We obtained a Spearman's ρ of 47. 1, which outperforms the 41. 8 result from Soricut and Och (2015) and is competitive with the 47. 8 reported by Pennington et al. (2014), which used a 42B-token corpus for training.",
        "GPT2_formal_text": "= {'g', 'l', 'q', 'e', 'r', 's', 't', 'a', 't', 'r', 's', 'b', 'l', 'r', 'c', 'p', 't', 'a', 'r', 'l', 'c', 'p', 't', 'b', 'l', 'c', 'p', 'e', 'r', 's', 't', 'c', 'r', 'b', 's', 'e', 'r', 's', 't', 'b', 'a', 'r', 'c', 'p', 't', 'b', 'l', 'r', 'e', 'r', 'c', 'p', 'e', 'r', 's', 'b', 'l', 'c', 'p', 't', 'a', 't', 'r', 'l', 'c', 'p', 't', 'b', 'l', 'c', 'p', 'e', 'r', 's', 't', 'b', 'a', 'r', 'l', 'c', 'p', 'e', 'r', 's', 't', 'a', 'r', 's', 't', 'c', 'p', 't', 'b', 'l', 'c', 'p', 'e', 'r', 's', 't', 'b', 'a', 'r', 'l', 'c', 'p', 't', 'b', 'a', 'r', 's', 't'} Formal: We look at the generated data using the model's training data, then tweak it a bit to match the specific domain. Formal: Alright, so we do the same thing for the training data, then calculate the final score using the model's settings. Formal: We're testing this on a dataset of English sentences and different combinations of domain (like 'r' and 'e') and source (like 'g', 'l', and 'q'). Formal: We're using the same dataset as in (Burges et al., 2009) and using the same model setup, which we'll call (1). Formal: We're running the experiment with N = 5 and L = 5. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1157.json"
    },
    {
        "casual_text": "In Hebrew, when a verb has a sentential object, it's like \"n9~9.\" In German, it's \"kennen,\" and in French, it's \"conaitre.\"",
        "formal_text": "or \"n9~9\" in I lebrew, when the verb takes a sentential object) know -> kennen (ill German; or \"conaitre\" in French;",
        "GPT2_formal_text": "is one of the three hyperparameters. The loss function for the task, which we'll call L_task, is learned using the Gibbs sampler. The final loss is the one we'll use to train the model. Formal: The attention mechanism is a type of memory that keeps track of where the attention is focused. Formal: We start by creating the representation of the key and query, which we'll call h_key and h_query, respectively. Formal: The attention mechanism is basically a memory network that's been trained to focus on the most important words in the query and key. Formal: In the future, we're planning to look into models that can handle more complex semantic stuff, like a memory network that works together with a neural network. Formal: We'll talk about the three hyperparameters and the loss function later, but for now, just know that our model is set up to give a probability to each hypothesis. Formal: The attention mechanism can be seen as a kind of neural network that's been fine-tuned to figure out the probability of key and query hypotheses. Formal: Following what Barhom et al. (2017) did, we use a K-max pooling layer to get the output vector for the final classifier, which we'll call c_k. Formal: We use a LeakyReLU activation function to make sure the probability of a hypothesis is as high as possible. Formal: Finally, we combine the hidden state vector h_t for the key and the query into one single representation. Formal: We train the model using the RNNLM, which is an adaptation of the RNNLMs by Joulin et al. (2017), and also add a 1-on-1 multi-task learning setup. Formal: We also compare the best results with other methods like cross-entropy loss and L2-regularized linear unit regression loss. Formal: Here's how the main contributions of this paper are laid out. Formal: We share the results for three different setups. Formal: We also share the architecture and hyperparameters for our model. Formal: Finally, we talk about the current challenges of this task. Formal: We evaluate our model on the evaluation set created by Wang et al. Formal: We use our model on a popular benchmark dataset that's used in other studies. Formal: The results from the DUC2015 and DUC2016 tasks are in Table",
        "directory": "C88",
        "filename": "C88-1042.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. We're looking at how the Transformer model does in two different situations: the \"common-practice\" setting and the \"wug test\"-like setting.\n\nIn the \"common practice\" setting, we're basically using data from past shared tasks and related research (like the work by Cotterell et al. from 2016, 2017, McCarthy et al. in 2019, and Vylomova et al. in 2020). This data usually includes a good number of lemmas, and there's some overlap between the lemmas used in training and those in the test sets. We're using this shared task data to represent the common-practice setting.\n\nNow, in the \"wug test\" setting, things are a bit different. Here, we control the number of lemmas for training, but not the inflected forms (as explained in Section 2). The lemmas we ask the model to inflect are always new, never seen before.\n\nSurprisingly, the Transformer model performs really badly in the \"wug test\"-like setting, even though there's a lot of training data for languages like those from 2018 or the Niger-Congo languages, which have very regular and simple inflections. The performance is way worse than in the common-practice setting, even when there are seven times more training triples for languages like Finnish, Spanish, and Turkish (check out Figure 1 for details).",
        "formal_text": "Common-practice test and \"wug test\" We first compare the performance of the Transformer in the common-practice setting and the \"wug test\"-like setting. The \"common practice\" is represented by (2) hallucination (1) copy previous years' shared tasks and related work (Cotterell et al. , 2016 (Cotterell et al. , , 2017 McCarthy et al. , 2019; Vylomova et al. , 2020); here the training data usually covers a fair number of lemmata and there is overlap between lemmata in the training and test sets. We use the shared task data to represent the common-practice setting. In the \"wug test\" setting, we control the number of lemmata for training but not inflected forms (as explained in Section 2) and the lemmata to be inflected are always previously unseen. To our surprise, the performance of the Transformer at the \"wug test\"-like setting is very poor despite the large amount of training triples for 2018-languages or the very regular and straightforward inflection for Niger-Congo languages. The performance is dramatically inferior to the common-practice setting, even when the number of training triples is seven times larger for Finnish, Spanish and Turkish (see Figure 1 ).",
        "GPT2_formal_text": "are tagged with the keyphrases they identify. Figure 4 shows how this process works. Formal: Formal: There are two main types of keyphrases: those that can be used as facts or fall into the category of \"[insert obvious statement here]\" and those that don’t. Formal: The input phrase can either be a noun phrase or a non-noun phrase, depending on what kind of info is being passed. For example, you could use something like \"The United States announced that\" or \"The U.S. announced its decision to pull out of the Paris climate agreement.\" Formal: Basically, if the input phrase is a noun phrase, it’s a fact. But if it’s not a noun phrase, it’s not a fact. Formal: So, if the input phrase is a noun phrase, it’s a fact. If not, it’s not a fact. Formal: If the input phrase is a noun phrase, it’s a fact. If not, it’s not a fact. Formal: Formal: This section has been updated to include the updated stats for the CoNLL-2009 shared task dataset. Formal: The updated stats for the CoNLL-2009 shared task dataset are in Table 2. Formal: The updated stats for the CoNLL-2009 shared task dataset are in Table 2. Formal: For the CoNLL-2009 shared task, we’re looking at sentence-level performance. Formal: For the CoNLL-2009 shared task, we’re reporting the average and standard deviation for each human evaluation. Formal: We’re reporting the human evaluation results for the top three keyphrases in the development and test sets of the CoNLL-2009 shared task. Formal: We’re reporting the results for the top three keyphrases in the development and test sets of the CoNLL-2009 shared task. Formal: We’re reporting the results for the top three keyphrases in the development and test sets of the CoNLL-2009 shared task. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-short.84.json"
    },
    {
        "casual_text": "To get a better understanding of how things are going at the segment level, Figure 4 shows a comparison between our method and Thot for the first 300 segments of the Autodesk test set used in the multi-domain experiment. The plot highlights the differences in TER (Translation Edit Rate) between the MT output and our approach, as well as between MT and Thot for automatically post-edited segments. \n\nWhat we see is that our approach changes fewer segments compared to Thot. This is because our method only builds a model if it finds useful data in the knowledge base; otherwise, it leaves the MT segment as is. When Thot modifies these untouched MT segments, it often makes things worse instead of better (as you can see from the many negative peaks for Thot in Figure 4). \n\nThis suggests that, compared to other online approaches, the output from our solution is more likely to be helpful for human translators. This benefit isn’t just about making post-editing easier or more enjoyable—it also saves time by providing better suggestions overall.",
        "formal_text": "To gain further insights about the performance at the segment level, the plot in Figure 4 compares our approach against Thot for the first 300 segments of the Autodesk test set used in the multi-domain experiment. It shows the differences between the segment-level TER of the MT (T ER MT ) and our approach (T ER Our approach ), and MT and Thot (T ER T hot ) automatically post-edited segments. We notice that our approach modifies less segments compared with Thot, because it builds a model only if it finds relevant data in the knowledge base, otherwise it leaves the MT segment untouched. These untouched MT segments, when modified by Thot, often lead to deterioration rather than to improvements (as seen by many negative peaks for Thot in the Figure 4). This suggests that, compared with the other online approaches, the output obtained with our solution has a higher potential for being useful to human translators. Such usefulness comes not only in terms of a more pleasant post-editing activity, but also in terms of time savings yield by overall better suggestions.",
        "GPT2_formal_text": "ed(x, y) = self.attention_masked(x, y) * [CLS] + self.attention_normalized(x, y) + [SEP] + self.attention_layer_normalized(x, y) + [SRL] + self.attention_layer_normalized(x, y) * [DST] + [RST] + [STS] + self.attention_masked(x, y) * [DST] Formal: For each token, we turn it into a vector by doing two things: (1) we normalize it into a single number (a scalar), and (2) we combine all the vectors (a tensor) from all the tokens into a single vector. This is all defined for each token, and the size of the vector depends on the token itself. Formal: The normalized version of a token is calculated as: Formal: The attention-normalized version of a token is: Formal: The attention-layer-normalized version of a token is: Formal: The attention-masked version of a token is: Formal: The attention-masked version of a token is: Formal: The vector representation for a token is just the sum of all the vectors from all the tokens. Formal: The normalized attention-masked attention-layer-normalized vector representation is: Formal: The normalized attention-masked attention-layer-normalized vector representation is: Formal: Formal: The attention-masked attention-layer-normalized vector representation is: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "2016.amta-researchers.1.json"
    },
    {
        "casual_text": "CSCs cover general cases and syntactic rules. What sets our model apart is that it uses cases for generation, which isn't something most other generators do—they mostly just rely on syntactic rules.",
        "formal_text": "CSCs represent generalized cases and syntactic rules. Use of cases for generation is one of the unique features of our model while most generators solely depend upon syntactic rules.",
        "GPT2_formal_text": ") of the titles and authors. Formal: The model checks how similar the titles are to each other and assigns labels to these pairs, based on the similarity score. Formal: The model figures out the best way to break down each pair of titles and authors. Formal: A specific way to do this is by using a statistical model (like a neural network) that tries to make the best split for a given set of words. Formal: Another method is to just pick one split randomly. Formal: In this part, we'll talk about the dataset we used, which we'll call the HATM dataset. Formal: Here, we'll explain how we set up the model and how we trained it. Formal: We went with a multi-task learning setup and trained the model on two tasks at the same time. Formal: The model starts by figuring out the best split for the titles and authors, and then it learns how to break down those titles and authors into smaller, individual pieces. Formal: The final model we end up with is trained on a task-specific dataset, just like we mentioned. Formal: The training process is pretty straightforward and works in batches, with a learning rate of λ set to 0.1. Formal: The model is trained on a set of titles and authors, but we didn't focus on the specific split here. Formal: After that, we pick the best split for each title and author pair. Formal: We used the HATM dataset, which was trained on a task-specific dataset, to train our model. Formal: We built the model using PyTorch (Paszke et al., 2019), with the default settings for the model. Formal: We trained it for 10 epochs, using the Adam optimizer (Kingma and Ba, 2015), with an initial learning rate of λ = 0.001. Formal: We picked the best model using the development set and used it for the next experiments. Formal: Formal: Formal: In Table 2, we show how the model performed when we trained it on a task-specific dataset and then tested it on a task-independent dataset. Formal: The test dataset for the HATM model we used is the test dataset for the HATM model we used, which we call the test dataset. Formal: The test dataset is created by randomly picking pairs of titles and authors. Formal: For the H",
        "directory": "C90",
        "filename": "C90-2038.json"
    },
    {
        "casual_text": "It's clear that our vanilla depLCNN+NS, which doesn't use any extra lexical features, still beats the best systems out there—MVRNN+ and CNN+—by a big margin. Those systems actually use extra lexical features, but our approach with dependency paths works even better. When we add similar lexical features to our model, it gets even stronger, improving by 1.6%. This makes it way better than any other system we've seen.",
        "formal_text": "We can see that our vanilla depLCNN+NS, without extra lexical features, still outperforms, by a large margin, previously reported best systems, MVRNN+ and CNN+, both of which have taken extra lexical features into account, showing that our treatment to dependency path can learn a robust and effective relation representation. When augmented with similar lexical features, our de-pLCNN+NS further improves by 1. 6%, significantly better than any other systems.",
        "GPT2_formal_text": ". It’s used to adjust the attention mechanism in a discriminator. In this paper, we’re trying to make things more specific by using a few-shot learning approach. We’ve noticed that adding a few-shot learning approach (like the one by Liu et al., 2019) doesn’t always help. In fact, it can actually mess with the performance. So, we’re trying something different. We’re focusing on the task of named entity recognition (NER), which is way more complicated than what we’re dealing with here. Formal: We’re using the input sequence {s i } x i to represent the input text. We’ll call the attention mask w i, which is made up of (0, 0, 0) weights, on the input sequence {w i } x i. Here, x i represents the title, author, and the content of the sentence. Formal: The query vector p i is created by combining the input vector {s i } x i with the query vector, {q i } y i. Formal: The attention mask is calculated using this equation: Formal: Finally, we calculate the score y i by adding up the predictions of the attention mask and the query vector. Formal: Formal: \n\nTo get a better grasp of how our model works, let’s quickly look at a few examples. Formal: The input text {s i } x i is used to create a query vector p i. The query vector q i, which is created using the query vector p i, is then used to calculate the attention mask w i. Formal: The model has two main parts: the classification part and the generation part. The classification part is defined as {c(t i )}, where c(t i ) = {c(t i )1, . . . , c(t i )M }. Each token t i, j is used to create a token label t j. Formal: A bunch of labels are added to the query vector, making it longer. Formal: The model has two main parts: the classification part and the generation part. Formal: The classification part is defined as {c(t i )}, where c(t i ) = {c(t i )1, . . . , c(t i )M }. Each token t i, j is used to create a token",
        "directory": "D15",
        "filename": "D15-1062.json"
    },
    {
        "casual_text": "You can grab predicates one at a time using this interface, or you can just download them all at once from the tagging lexicon tool.",
        "formal_text": "With this interface predicates can be either acquired one by one or automatically downloaded from the tagging lexicon acquisition tool.",
        "GPT2_formal_text": "is set to a non-null integer. Formal: For each pair of nodes, the entity types are written as {type, id}, which are basically sequences of the entity types. Formal: The entity types that are common across different domains are written as [domain, type]. Formal: These common types are just the ones that show up in the training data for both domains. Formal: Let's say we have a parameter set called p, and the goal is to predict the entity type for the pair of nodes x = x 1, . . . , x m. We can do this using the training data D = {d 1, . . . , d m }. Formal: For each target node t j, we have some sentence s j = {s j 1, . . . , s j m } that has some entity entity e j. Formal: For a given pair of nodes x i, y i, y j, we'll use a bunch of target nodes that are basically the same as the target node x i, y i. Formal: The parameters p, q, r, t are called the model parameters. Formal: The goal of the model is to maximize the cross-entropy loss function, called λ θ (t j, x i, y i ). Formal: Formal: Formal: Basically, λ θ (t j, x i, y i ) is just a way to normalize the expected value, or loss, of λ θ (t j, x i, y i ) when we're dealing with the target entity t j. Formal: Formal: And λ θ (t j, x i, y i ) is the same thing as the cross-entropy loss function, λ θ (t j, x i, y i ), using the target entity t j. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2002.eamt-1.9.json"
    },
    {
        "casual_text": "This setup needs an interface that can handle a few specific tasks. First, any queries the grammar writer explicitly mentions need to be sent to DATR. Then, each of these queries, along with their \"resulthag\" values, has to be converted into a PATR path equation (which is kind of like a description of a feature structure) and sent to the PATR system. \n\nThe tricky part about this whole process is that for every unique PATR path, you need to know the exact DATR query that goes with it. It might sound easy to just figure out which paths are defined for a particular node, but that doesn't work because of inheritance—basically, the whole network could be involved. So, in practice, all the PATR structures (except for the basic values) have to be defined twice: once in the DATR statements and again in the queries. \n\nThis duplication is a pain, and it can't be avoided unless you set up types for the feature structures that can be used when writing the queries.",
        "formal_text": "Such an approach requires an interface with the following functions: Queries that the grammar writer has stated explicitly have to be passed on to DATR. Every query together with the resulthag value has to be transformed into a PATR path equation (that partially describes a feature structure) and passed on to the PATR system. What is most disturbing about this strategy is the fact that for every distinct PATR path you have to know the corresponding DATR query. It is tempting to think one could simply check which paths are defined for a given node, but this doesn't work because of inheritance: the entire network is potentially relevant. So in effect all the PATR structures except the atomic values have to be defined twice: once in the DATR statements and once in the queries. This redundancy cannot be eliminated unless types for the feature structure are declared which are consulted in formulating the queries.",
        "GPT2_formal_text": "_t):\n\n- k = 1/16 (1/16) * tanh(BATCH_WIDE + BATCH_SHIFT)\n- w = (1/16) * tanh(BATCH_WIDE + BATCH_SHIFT + 1/16) * sin(WIDE + 1/16)\n\nHere, k represents the total number of windows, and w is the width of the window.\n\nThe final model is trained using two methods: (1) the original cross-entropy loss, and (2) the convolutional neural network loss. The input for this loss is the entire text. Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: The model is trained using two main steps: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E91",
        "filename": "E91-1024.json"
    },
    {
        "casual_text": "Here, len refers to the output sequence length we're aiming for. LRPE and LDPE should be able to create sentences of any length, even if the exact length wasn't part of the training data. Takase and Okazaki (2019) took a different approach, focusing on character-based length constraints for summarization, meaning they limited the number of characters in the summary.",
        "formal_text": "where len is the given output sequence length. LRPE and LDPE are expected to generate sentences of any length even if sentences of an exact length are not included in the training data. Takase and Okazaki (2019) used character-based lengths for summarization constraints in the number of characters.",
        "GPT2_formal_text": "_sent:\n\n- For the student model, we want it to produce responses that make sense and make sense in the context of the conversation.\n\n- For the teacher model, we want it to create responses that are easy to understand and are grounded in the conversation's current situation.\n\nFor every input and output in the current dialogue, we look at the pair of the new question and answer to figure out what they mean. We use a tool called Seq2Seq Transformer (from Vaswani et al., 2017) that takes an input sequence x and turns it into a sequence y, where y is the answer to the question. The model spits out a response y * based on the conversation's current state. Formal: Since dialogues often have multiple turns, Seq2Seq models can get pretty complex and require a lot of processing power. Plus, these models usually rely on the conversation history, which is super time-consuming to process. Formal: Instead, we’re using a simpler approach that’s more efficient and doesn’t need a history. This simpler model just picks the answer from the current conversation without worrying about the history. Formal: The future of chatbots that use this approach could be something like Amazon Alexa, Google Assistant, or Apple Siri, where the conversation history acts as a guide for understanding the conversation. Formal: This paper is a step toward tackling the big challenge of understanding complex, multi-turn, multi-turn dialogues. Formal: We’re using an attention-based deep learning model, which is built on RNNs (Hochreiter and Schmidhuber, 1997), to do this. Formal: We’re introducing a new attention-based model called Seq2Seq Transformer (from Vaswani et al., 2017) to handle multi-turn dialogues. This model takes an input sequence x and turns it into a sequence y, where y is the answer to the question. Formal: To make use of the conversation history, we’re using a multi-head attention mechanism. Formal: Lastly, we’re building a model that predicts the next question and answer together based on the conversation history. Formal: These joint models are trained together to help each other out. Formal: The training process for these models is shown in Figure 1. Formal: We’ve set up our model to predict the answer to the question",
        "directory": "coling",
        "filename": "2020.coling-main.319.json"
    },
    {
        "casual_text": "Even though models like BERT have done really well in a lot of tasks, they have a couple of issues when it comes to joint entity relation extraction. First, the training methods used for these models aren’t really designed for this specific task. For example, BERT and similar models don’t focus on the kind of entity-related knowledge that’s super important for finding entities and their relationships. Second, these models only give you ready-made representations for words and sentences, but not for entities or pairs of entities. To get those, you have to add extra parameters during the fine-tuning process, which can actually make things worse and affect how well the model performs on the joint extraction task.",
        "formal_text": "Although pre-trained models, like BERT, have shown impressive performance in many downstream tasks, they have mainly two limitations when applied in the joint entity relation extraction task. One is that currently pre-trained objectives are insufficient for this task. Specifically, these commonly used universal pre-trained model (e. g. , BERT) do not consider the entity-related knowledge that is crucial for better extracting entities and relations. The other is that these models only provide pre-trained representations for tokens and sentences, but not entities and entity pairs. To obtain the representations for entities and entity pairs, additional parameters that are not pre-trained are introduced in the fine-tuning stage, which may futher impair the joint extraction performance.",
        "GPT2_formal_text": "_layers = g, [] = [], [], [], and []. Formal: In the c = 1 position, we swap out the output from the layer in the previous layer (let's call it X_p) with a random one randomly picked from a uniform distribution. Formal: To make sure everything matches up properly and makes sense, we use the standard non-linearity thing from the joint model. Formal: The final layer of the joint model has two parts: the Transformer layer (V_T) and the Concatenation layer (V_C). Formal: The full model looks like this: Formal: The joint model doesn't really go into detail about the parameters, but we'll talk about them in the next section. Formal: The first part, the Transformer layer, is a DNN layer that takes a word embedding x and turns it into a numerical vector. Formal: The second part, the Concatenation layer, uses a BiLSTM (Hochreiter and Schmidhuber, 1997) to create a kind of one-hot representation of the word embedding. Formal: The whole thing is shown in Figure 1. Formal: This joint model has two parts: the Transformer layer and the Concatenation layer. Formal: The Transformer layer is a fully connected layer that processes a sequence of words, and the Concatenation layer is there to handle the final result. Formal: Here's the figure: Formal: This joint model uses two types of parameters: the vocabulary (V_T) and the hidden layer (V_C). Formal: The input vector x for the Concatenation layer is the combination of a word embedding (x) and a bit of context information, which we call w_x. Formal: The parameters for the Transformer and Concatenation layers are described in more detail in the next sections. Formal: The first part of the joint model (V_T) has a batch size of 128. Formal: The second part of the joint model (V_C) has a batch size of 1024. Formal: Formal: The input vector for the first part is a set of vectors that are all one-hot representations of the word embedding. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.251.json"
    },
    {
        "casual_text": "Pereira (from [Percira, 1985]) came up with a way to do structure-sharing by letting the result graph share info with the original graph by storing stuff in something called the 'environment'. But this method has a log(d) overhead, where d is the number of nodes in the graph. This overhead happens when you try to rebuild the whole graph from the 'skeleton' and the updates in the 'environment'. \n\nIn the new system, though, since the arcs point directly to the original graph structures, there's no extra overhead when accessing nodes. Plus, during unification, charges are called way more times than needed—like 3299 times for sentence 9 when unify-dg was only called 480 times.",
        "formal_text": "Pereira ([Percira, 1985] ) attaills structure-sharing I)y having the rcsult graph share information with the origiw, d gr~tphs by storing charges to tile 'enviroltmerit'. There will be tlle log(d) overhead (where d is the nuM)er of nodes ill a graph) associated with Percira's ntetho<l that is rcqttircd during node access to, ~ssenfl>le the whole graph from the 'skeleton' and the updates ill thc 'environment'. Ill the proposed sdtcmc, since tim arcs dircetly lloint to the original graph structures there will be no overhead till\" node accesses. Also, during unificatioli, siltce citaltges art! called 3299 times for sentence 9 when Itnify-dg was called 480 tillies.",
        "GPT2_formal_text": "= {u1, u2, ..., ul} and the gender distributions are {gender1, gender2, ..., genderT}. The one-to-one mapping, though, doesn’t actually help with the relation extraction part.\n\nIn this project, we’re tackling the issue by using something called \"one-to-many\" relation extraction. Here, we’re defining a relation as the pair (r, e) where r is one of the first n relations and e is one of the last n relations. The pairs we’re looking at here are {gender1, gender2, ..., genderT}. If we have a pair of sentences, r and e, which include a single relation, r, we can also find the pair (r, e) that includes a single relation, r. The one-to-many mapping helps us find the right relation for sentences like this.\n\nTo do this, we first use a method called \"list-to-list\" pairwise alignment. This method is pretty straightforward to implement. For each sentence s_i, we align it with a specific pair of sentences s_j. Then, we calculate the pairwise alignment for both sentences. If the pair is found, we build the two-way relation pair {gender1, gender2, ..., genderT} for the pair. We set the length of this relation pair to T. Formal: The number of relation pairs in the pair is T times the number of sentences. Formal: The number of sentence pairs in the pair is T times the number of relation pairs. Formal: We use the one-to-many mapping to find the right relation for the pair. Formal: We use the one-to-many mapping to find the correct relation for the pair. Formal: We can break down the pairwise alignment for sentences by the number of relation pairs, resulting in the pairwise alignment for a specific pair. Formal: The pairwise alignment for a specific pair is just a one-to-one mapping. Formal: Formal: The one-to-many mapping is an unordered set of tuples, where each tuple is the pair of sentences. Formal: The one-to-many mapping is an ordered set of tuples, where each tuple is the one-to-one relation pair. Formal: Formal: Formal: We use the one-to-many mapping to find",
        "directory": "C92",
        "filename": "C92-2068.json"
    },
    {
        "casual_text": "A recent study looked at historical English letters and compared different LSTM setups. It found that bi-directional recurrent neural networks (BRNN) work better than one-directional RNNs. But using different attention models or making the architecture deeper didn’t help improve the results. Adding extra data, like social metadata or century info, actually made the accuracy worse. The study suggests that post-processing is the best way to improve a character-level NMT normalization model. This same approach has also been used successfully for OCR post-correction (Hämäläinen and Hengchen, 2019).",
        "formal_text": "A recent study on historical English letters compares different LSTM architectures finding that bi-directional recurrent neural networks (BRNN) work better than one-directional RNNs, however different attention models or deeper architecture do not have a positive effect on the results. Also providing additional data such as social metadata or century information makes the accuracy worse. Their findings suggest that post-processing is the most effective way of improving a character level NMT normalization model. The same method has been successfully applied in OCR post-correction as well (Hämäläinen and Hengchen, 2019).",
        "GPT2_formal_text": "ed) and (attention_attn_masked) to handle multi-task learning. The main goal of our approach is to train a model using a dataset made up of only high-frequency words. So, we set up the cross-lingual RE model like this: Formal: The main thing we're focusing on here is training the model with a dataset that only has words that show up a lot. We call this setup cross-lingual RE. Formal: Another approach is using the entire dataset as the training data. This works well if the task we're dealing with is really broad—like finding the exact word that describes a specific image. But if the task is more specific, like finding a specific image, we might need to get more specific, which is where we go with cross-lingual RE. Formal: The way we learn the embedding parameters Φ is by using a lot of data from both the source and target languages. Formal: Next, we try to estimate the objective function using some extra data, based on the actual or predicted embeddings. Formal: To make sure our model can work across languages, we combine the embeddings from both the source and target languages, plus the embeddings from the training set. Formal: The training set we use includes data from both the source and target languages. Formal: The cross-lingual model we're proposing is a bit different from earlier methods. Formal: Unlike other methods that just use cross-lingual data (like Filippova and Klein, 2016; Hou et al., 2018; Xuan et al., 2019), we use cross-lingual data in all the fine-tuning stages. Formal: The cross-lingual RE model we propose has three main parts. Formal: The first part is using the cross-lingual embedding parameters Φ and then updating the embeddings Φ i. Formal: The second part is a cross-lingual translation model. Formal: The third part is a cross-lingual embedding objective. Formal: In both the source and target languages, we use a Gaussian kernel to calculate the cross-lingual embedding parameters. Formal: Formal: To make sure our model can be used in all different languages, we train it using data from both the source and target languages, and then use the cross-lingual embedding parameters to train a model using the target language. Formal:",
        "directory": "D19",
        "filename": "D19-5519.json"
    },
    {
        "casual_text": "In both situations, the noun phrase (NP) either decides or has to match the overall structure. Similarly, when we're trying to find a verb phrase, we need to figure out which token (like a variable name or constant) stands for the subject (if the verb phrase is part of a sentence, or S) or the main noun (if the verb phrase is acting as a relative clause). This is done by using the SENDR function to set the subJvar register in the sub-computation to the right value.",
        "formal_text": "[n either case, the NP also determines, or must agree with, the overall Similarly, when we are lookzn8 for a verb phrase, we must know what token (i. e. variable name or constant) represents the subject (if the verb phrase is dominated by a S) or the head noun (if the verb phrase acts as a relative clause). This is done by sanding the subJvar register in the sub-computation the appropriate value via the SENDR function.",
        "GPT2_formal_text": "= {q, c}, tags = {t, e}  \nTable 4: A quick rundown of the text processing techniques we used for generating the questions and answers. Formal: We’re suggesting a new kind of classifier called QA-based Dataset Assignment (QADiscourse). It takes into account a bunch of QA-related features, like how questions are structured, how they're presented, and their reasoning. Our research on annotating QA datasets in QADiscourse is all laid out in Figure 1. Formal: This approach means that, as long as the questions and answers are both in English, the model can figure out the answers by looking at the structure of the text. Formal: We’re also introducing a method called BERT-based Dataset Assignment (BDiscourse), which is like a supervised version of QADiscourse but specifically for biomedical datasets. It helps the model pick out the right questions by using a mix of BERT and other classifiers. It’s all explained in the next sections. Formal: The QA-based dataset assignment method in QADiscourse has two main parts:  \n1. A memory cell for each question, which is made up of a bunch of words from the text.  \n2. A special token called a \"label\" that points to the answer.  \n\nThe memory cell is also used to store information about the answer, which we use as a kind of \"token\" to show where the question and answer start and end. Formal: These three parts are all combined into one thing: Formal: The goal of QADiscourse is to get the model to understand the context of the question and answer, and to figure out which parts of the text are important. Formal: The questions in QADiscourse are organized in a tree structure, and the answers are words from that same tree structure. Formal: The model uses a bunch of classifiers, like BERT and Seq2Seq, along with their special tokens, to find the best answers. Formal: We’re working on developing a new algorithm called QADiscourse-based Multitask Learning (QAML). Formal: The main idea is to combine these two parts to improve how well the model can answer questions. Formal: We’re also working on a new way to automatically adjust the model’s settings",
        "directory": "A83",
        "filename": "A83-1010.json"
    },
    {
        "casual_text": "(6) Another classic by Grisberg, I’m a huge fan of Stevie—she’s one of the best R&B singers I know. Darwin Halstead introduced me to her, so if you’re a fan too, you should definitely grab this DVD. It’s awesome and totally amazing. This woman really brings a heartfelt vibe to her R&B music.  \n\nContradiction: There’s one review (7) that seems a bit mixed up. The person says their husband isn’t a big fan of the product, but then also says he loves it. Kind of confusing, right?",
        "formal_text": "(6) another classic by grisberg, i love stevie she was one of the greatest r&b singers I know darwin halstead ment her so be a big fan please do yo self a favor and buy this dvd, its nice and it absolutly amazing this woman has a very yorfelt approch to r&b music Contradiction: We find one review (7) containing contradictions, where the subject (husband) is claimed to be not a big fan of a product but also as loving the same product.",
        "GPT2_formal_text": "(src_m, src_s) = (src_m * src_s + src_t * src_m) * d_s. Formal: We can treat the original input x as a matrix of embeddings and the generated representation as a vector. Then, we take the cosine similarity between the embeddings of the source and target tokens and average them out to get the attention vector (a_t). Formal: We can calculate the attention vector a_t by calculating the average of the embeddings of the source token x and the target token y. Formal: Lastly, we can take the average of the embeddings of the source and target tokens along with the attention vector a_t to get the embedding of the pair. Formal: We can also find the embedding of the source and target tokens along with the attention vector a_t. Formal: For instance, if we have the input x and the output y = (x_i, y_i), we can write the attention vector a_t as (a_t(x, y) + a_t(y, y_i)) = (a_t(x, y) * a_t(y, y_i)) + a_t(y, y_i). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.208.json"
    },
    {
        "casual_text": "The numbers are: 5.96 ± 0.21 (0.00 ± 0.00), 52.68 ± 0.43 (0.00 ± 0.00), 68.95 ± 0.47 (0.00 ± 0.00) for LAV N, and 30.11 ± 1.27 (2.00 ± 1.37), 85.41 ± 1.07 (48.50 ± 4.00), 94.83 ± 0.53 (83.40 ± 4.37). The blue bars on the left show how well our system did, and the green bars on the right show the baseline system's performance. The graphs break down the accuracy for tables with 1, 2, 3, 4, and more than 4 forms.",
        "formal_text": "5. 96 ± 0. 21 (0. 00 ± 0. 00) 52. 68 ± 0. 43 ( 0. 00 ± 0. 00) 68. 95 ± 0. 47 ( 0. 00 ± 0. 00) LAV N 30. 11 ± 1. 27 (2. 00 ± 1. 37) 85. 41 ± 1. 07 (48. 50 ± 4. 00) 94. 83 ± 0. 53 (83. 40 ± 4. 37) The blue bars (on the left) denote accuracy for our system and green bars (on the right) accuracy for the baseline system. The graphs show accuracy separately for tables where 1, 2, 3, 4, and > 4 forms are given.",
        "GPT2_formal_text": "= {u1, u2, ..., uk, uk+1}. Formal: The text embedding is created by pulling out the important bits from the input, using a special token called \"F\" to mark the start and end. Formal: The output tag is made by concatenating the input and output tags, using the special token \"s\" for start and end. Formal: Finally, the output is a vector with a size of d_output_t. Formal: Here, λ is a scaling factor that helps balance how much importance we give to smaller and bigger parts of the input. Formal: The tagger uses a sentence-level language model to create a probability distribution p_tag for the label l_t. Formal: To train the tagger, we create a training set by picking some randomly chosen examples from the set P. Formal: Our tagging system starts by picking the tag it thinks matches the input. Formal: After that, we use these selected examples to train a classifier θ, which figures out the probability p_tag for each label. Formal: Since the tagger can't decide if a word x_t fits the input x, we need to find a way to optimize the probability p_tag for all possible labels y_t. Formal: We use the gradient descent algorithm to find the best solution. Formal: For the tagger to work well, it needs to be able to pick the right label y_t for any input x. Formal: For more info, check out Section 5. Formal: In this part, we look at how well our model and the label guidance system work together to understand how taggers and label guidance are contributing. Formal: To train the model, we start by using the training set P for a few rounds. Formal: After that, we update the model by training it on the updated set P, following the steps in Section 5. Formal: Our tagger is trained using the cross-entropy loss function, which is explained in Section 6. Formal: The final tag is chosen by finding the best label y_t that maximizes the cross-entropy loss. Formal: We also used a linear kernel function θ for the tagger's predictions. Formal: The final tag we pick is the one that minimizes the cross-entropy loss. Formal: The training for the classifier θ is done by",
        "directory": "D18",
        "filename": "D18-1315.json"
    },
    {
        "casual_text": "So, like, the stuff we talked about earlier? It can be explained using the example in Figure 1. Let's say we have a sentence in Chinese that says \"在",
        "formal_text": "As stated above, our motivations may be explained by the example shown in Figure 1. Given a source sentence in Chinese \"在",
        "GPT2_formal_text": "ed = t * [t + 1; i] y_masked = t * [t + 1; i] p_masked = [p_masked_1; p_masked_2; ...; p_masked_m; p_masked_m+1; ...; p_masked_m+l; p_masked_m+l+1] p_masked_i = [p_masked_i1; p_masked_i2; ...; p_masked_i+1; p_masked_i+l; p_masked_i+l+1] Formal: This model can figure out the attention weights in an image by using a specific function f_mask. Formal: Another model we're using, which we'll call G, takes in a sequence of pixels. Formal: After G, we use a second feed-forward network to handle the whole sequence. Formal: Finally, we combine the values of the hidden parts of these two models to get the final representation. Formal: This is what we end up with after feeding the image into a neural network. Formal: This is basically the equation we get after running the network, with the parameters we set and the attention weights we calculated. Formal: So, we end up with the final hidden representation of the image as f_mask_i, which is the combination of the image's hidden parts after G. Formal: After we've got that, we plug f_mask_i into the attention mechanism to get the final attention weight. Formal: We use G to generate the representations f_mask_i and a feed-forward network G to process the image. Formal: We'll explain the process step by step in the next section. Formal: We keep going like this for a while and then average out the values of the hidden parts. Formal: After that, we combine the values of the hidden parts by applying a second feed-forward network to get the final representation. Formal: Finally, we plug f_mask_i back into the attention mechanism to get the final attention weight. Formal: To make sure the model learns from examples that are similar to the image, we use a regularization parameter α. Formal: If the model is trained with a small β, it'll do well on similar examples. Formal: If β is too big, the model",
        "directory": "amta",
        "filename": "2008.amta-papers.10.json"
    },
    {
        "casual_text": "Sorry, we can’t go into more examples because we’re running out of room.",
        "formal_text": "Due to space limitations, we will not present additional examples in detail.",
        "GPT2_formal_text": "= { ... }  \nHere's how we'll create these queries:  \n- We'll set the basic info like the query and the question to be in the text.  \n- We'll also use the first result for the query (the \"Result\" part).  \n- To make sure we're getting the right answer, we'll use the \"Answer\" part.  \n- We'll use the user's question to figure out the answer. Formal: Here's how we'll use the text in these queries:  \n- We'll start by setting the basic info like the query and the question to be in the text. Formal: To get the text, we'll use the \"Query\" part and the text \"\". Formal: For the first result, we'll look at the first question in the text to find the answer. Formal: The last result will be the last question in the text. Formal: Lastly, the result from the last question will be the last question in the text. Formal: The final result will be the last question in the text. Formal: The query and the question will be in the same text, so we'll just use the first result and the last result together. Formal: We'll take the text \"Query\" and add the query and the question together. Formal: Lastly, we'll use the final result from the last question and the last text \"\". Formal: Formal: Formal:\n\nUsing \"?\". Formal: Formal:\n\nUsing the query and the question in the same text, we'll use the last result from the last question to get the text. Formal: Formal: Formal: Formal:\n\nFor the first result, we'll use the first question in the text to find the answer. Formal: Formal: Formal: Formal:\n\nUsing the query and the question in the same text, we'll use the last result from the last question to get the text. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.241.json"
    },
    {
        "casual_text": "Alright, so we've got a target and a relative, and we need to figure out how their parts-of-speech (POS) N-grams and positions are distributed. Using a labeled corpus would be way too limited for this, so instead, we turn to the much bigger, unlabeled Google Ngrams corpus (shoutout to Franz and Brants, 2006).",
        "formal_text": "Given a target and relative, we need to estimate their distribution of POS N-grams and positions. A labeled corpus is too sparse for this purpose, so we use the much larger unlabeled Google Ngrams corpus (Franz and Brants, 2006).",
        "GPT2_formal_text": "ed. After removing the node representations, we get the contextualized representations shown in Figure 3, which are basically the representation representations for the head entity (h_head) and its relation (r_relation). Formal: In the original model, the attention weight for the head entity is calculated based on the loss. We tweaked the attention weight for the relation vector r_relation by combining the attention weight from the classifier for the relation vector with the weight from the latent variable, which we call r_class. Formal: For classification, we use the standard cross-entropy loss function, which we call C(x_pred), to calculate the classification loss, L_pred. Formal: We also add a linear layer on top of the cross-entropy loss to handle the loss of the word embedding vector, y_vec. Formal: Also, we add a linear layer on top of the cross-entropy loss to focus on the word embedding vector. Formal: From these results, we noticed that using a linear model gives better results than a convolutional model for both datasets, but the gains are smaller for English data. Formal: For the validation sets, we trained a model using the maximum conditional likelihood (MC) objective, which is a linear model. Formal: In the evaluation set, the model performed better than the baseline model for both datasets. Formal: After training, we checked how the model performed on the validation sets and the test sets. Formal: We used the evaluation set results for our experiments. Formal: The attention weights were trained for a set number of epochs, and the cross-entropy loss was kept constant at 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E12",
        "filename": "E12-1038.json"
    },
    {
        "casual_text": "Swimming is super beneficial, right? It’s got loads of benefits. You know, infinitival verbs and verbal nouns look the same, so sometimes it’s tricky to tell them apart. But when a verbal noun is in the oblique form and followed by a postposition, like in example 6, it’s easier to figure out. The real challenge is when it’s in the direct form. In example 7, the verb \"jānā\" is part of a verb group (VG), so it’s tagged as an infinitival verb. But in example 8, it’s used as a verbal noun and even has a possessive pronoun, \"merā,\" modifying it. This makes it confusing—is it a noun or a verb? Well, if an infinitival verb is right before a possessive pronoun or a genitive postposition, like in 8, it should be tagged as a verbal noun. And this info can help the POS tagger do its job better.",
        "formal_text": "tair-nā bəhut lābhkārī hai swim-Inf very beneficial be-pres 'Swimming is very beneficial' 6) tair-n-e ke bəhut lābh haῖ swim-Inf-obl Poss many benefits be-pres, pl 'Swimming has many benefits' Infinitival verbs as either main verbs or verbal nouns have identical forms. The POS ambiguity is easy to resolve when the verbal noun is in the oblique and is followed by a postposition as shown in 6 above. More difficult are sentences where it appears in the direct form. In 7 the verb jānā appears inside a VG and should be tagged as an infinitival verb. While in 8, it appears as a verbal noun and is also modified by a possessive pronoun merā. These two occurrences of jānā as a noun and an infinitival verb yields POS ambiguity (N or V). However, when an infinitival verb is immediately preceded by a possessive pronoun or a genitive postposition, as in 8, it should be tagged as a verbal noun, and the NG information can be successfully exploited by the POS tagger.",
        "GPT2_formal_text": "= {% i j %}  \n\n| S | i is the i-th word in the source sentence, and j is the j-th word in the target sentence. Formal: To tackle the issue of forgetting words, we use a hierarchical attention mechanism to take advantage of the connections between different levels of the sentence. Formal: In this paper, we introduce a new LSTM-based attention mechanism that helps us understand context better by looking at both word and sentence levels. This mechanism helps in capturing the connections between words, which helps us better understand the overall meaning of the sentence. To make this happen, we came up with a model that does both local and global word-level attention. The results show that the model, which only uses local word-level attention, performs better than the one that uses global attention. Formal: To show how well our model works, we used its performance on SST-2 dataset to train a model using BERT (Devlin et al., 2019). We found that this improved BERT model works well in the area of NLP tasks, especially for detecting sarcasm. Formal: We also looked at how to use sentence-level attention and found that using both layers together works better than just one layer. Formal: We tested our model on five different NLP tasks, including those like SST-2 (Peng et al., 2018), BERT, and SQuAD (Rajpurkar et al., 2019). Formal: The results show that our model performs really well on the SST-2 dataset, showing its effectiveness for detecting sarcasm. Formal: We also added a multi-task learning approach to our model to handle various NLP tasks. Our multi-task learning method boosts the performance by learning a representation of the source and target sentences, and then combining it with the previous sentence-level attention mechanism. The results show that our model improves performance across all NLP tasks, including SST-2. Formal: Using the cross-attention mechanism, we trained a model using BERT (Devlin et al., 2019) and SQuAD (Rajpurkar et al., 2019). The results showed that this model was better than the previous one. Formal: Our multi-task learning method helped improve the performance on SST-2 and BERT. This shows that our multi-task learning approach can be used in other NLP tasks, especially for detecting sarcasm. Formal: We",
        "directory": "C12",
        "filename": "C12-1152.json"
    },
    {
        "casual_text": "We’ve got two main systems in our experiments: the regular HPB SMT baseline and the CCG-augmented HPB SMT baseline. The CCG-augmented one uses single-category CCG labels and adds some strict syntactic rules (shoutout to Almaghout et al., 2010). For the regular HPB SMT baseline, we used the Moses Chart Decoder. We also used the GIZA++ toolkit to handle word and phrase alignment, and we went with the \"grow-diag-final\" method for refinement (thanks, Koehn et al., 2003). We set both the maximum phrase length and rule span to 12 words. For decoding, the chart maxes out at 20 words, and anything beyond that only uses glue grammar rules. The hierarchical rules we extracted can have up to 2 nonterminals. To fine-tune everything, we did minimum error rate training (props to Och, 2003). For the language model, we used a 5-gram trained on the target side of the parallel corpus with the SRILM toolkit and modified Kneser-Ney smoothing.\n\nThe CCG-augmented HPB system also runs on the Moses Chart Decoder, which has a feature to pull syntax-augmented rules from annotated data. We kept the same rule extraction and decoding settings as the regular HPB baseline. For the CCG-augmented experiments, we used the CCG parser from the C&C tools to handle the training data and combine CCG categories during glue grammar rule application.",
        "formal_text": "We have two baseline systems in our experiments: the HPB SMT baseline system and the CCGaugmented HPB SMT baseline system which uses single-category CCG labels and applies strong syntactic constraints (Almaghout et al. , 2010). We built our HPB SMT baseline system using the Moses Chart Decoder. 3 The GIZA++ toolkit 4 is used to perform word and phrase alignment and the \"grow-diag-final\" refinement method is adopted (Koehn et al. , 2003). Maximum phrase length and maximum rule span are both set to 12 words. The maximum span for the chart during decoding is set to 20 words, above which only glue grammar rules are applied. Hierarchical rules extracted contain up to 2 nonterminals. Minimum error rate training (Och, 2003) is performed to tune all our SMT systems. The 5-gram language model in all experiments was trained on the target side of the parallel corpus using the SRILM toolkit 5 with modified Kneser-Ney smoothing. Our CCGaugmented HPB system was also built using the Moses Chart Decoder, which has an option to extract syntax-augmented rules from an annotated corpus. We used the same rule extraction and decoding settings as for the HPB baseline system. We use the CCG parser from C&C tools 6 to parse the training data for our CCG-augmented HPB system experiments and to combine CCG categories during glue grammar rule application.",
        "GPT2_formal_text": "_x11, attention_mask_x2, and attention_mask_x3, where each x_i is a vector in R^m. The pooling layer, which is a 12-layer Transformer, helps the model learn useful information from different sources. For the RNN-based model, we use a batch size of 64 and a learning rate of 0.1. The model is trained for 100,000 steps, and we set the dropout rate to 0.1. We tested it on two datasets—BERT-100k and BERT-500k—with a batch size of 128 and a learning rate of 0.00002. Formal: We ran our experiments on the TACRED benchmark dataset (from Su et al., 2019) for TACRED. Formal: We tweaked the model using the training set, including extra human-labeled examples from the MultiNLI dataset. To get the model's predictions for all the test instances, we used the word2vec toolkit (Mikolov et al., 2013) on the TACRED training set. Formal: For the open-source system, we used a 12-layer T5-based model with a learning rate of 0.003. Formal: We used the training set of TACRED to fine-tune the GAN-based model. Formal: The performance results on the OpenWebText dataset are in Table 2. Formal: We tested the GAN-based model with different parameters and values of the attention mask. We found that the best settings were 0.1 and 0.3, which worked best in our tests. Formal: We used the same pretrained model on the TACRED test set as the original GAN-based model. Formal: We used the same model for decoding and used the same dropout rate for the CNN-based model. Formal: For the word-based model, we set the batch size to 64 and the learning rate to 0.1. We tested it with different values of the context pooling layer and the attention mask. The word-based model is trained for 100,000 steps and the CNN-based model for 100,000 steps. Formal: We trained the word-based model for 100,000 steps and then tested it on the OpenWebText test set. Formal: We found that the best settings for the attention mask and the context pooling layer were 0.1 and 0.",
        "directory": "eamt",
        "filename": "2012.eamt-1.44.json"
    },
    {
        "casual_text": "The supervised model we got wasn't as great as the fully-lexicalized DMV, but it was still over five points more accurate than using gold part-of-speech tags (check out Table 1: flat). Unsupervised accuracy was lower compared to gold tags, and the main perk of manually created categories might be that they can tag a word differently depending on the context.",
        "formal_text": "The resulting supervised model, though not as good as the fully-lexicalized DMV, was more than five points more accurate than with gold part-ofspeech tags (see Table 1: flat). Unsupervised accuracy was lower than with gold tags (see also tag a word differently in context) may be the primary advantage of manually constructed categorizations.",
        "GPT2_formal_text": "_list = {},  \nend\n\nlocal function pick_count(query, sentence):",
        "directory": "D11",
        "filename": "D11-1118.json"
    },
    {
        "casual_text": "The Computational Linguistics model seems to be doing pretty well. For instance, it can correctly generate \"a big rabbit\" when it focuses on the object \"rabbit\". Similarly, it can accurately recognize actions like \"crouching\" and the surroundings like \"grass and flowers\" when it pays attention to the different parts of the sketchy scene. However, it doesn’t mention anything about the weather, like \"on a cloudy day\", which might be because the \"cloud\" is too small and not very noticeable. \n\nIt’s also interesting that the model doesn’t describe the \"trees\" even though they take up a big part of the scene. As mentioned earlier, the model tends to focus on the most noticeable parts of a sketchy scene. In this case, the \"rabbit\" was considered the main object, and since \"rabbit\" often appears with \"grass\" and \"flowers\" in the dataset, the \"trees\" didn’t get much attention.\n\nThere are a few more examples in Figure 5 that show how the model works. Overall, it does a good job identifying the main objects in the scenes, except for the \"woman\" and \"house\" in the first example. The reason for this might be that the \"woman\" is hidden behind a \"tree\" in the sketch, making it hard for the model to recognize her properly.",
        "formal_text": "Computational Linguistics scene quite well. Specifically, \"a big rabbit\" can be generated correctly when the model focuses on the object \"rabbit\". Meanwhile, the action \"crouching\" and the surroundings \"grass and flowers\" can be correctly recognized as well when the model focuses on the two sides of the sketchy scene. In addition, the model does not generate the description about the weather, such as \"on a cloudy day\", which may be due to the reason that the \"cloud\" is too small to be salient enough. It is worth noting that the model does not generate the description about the \"trees\" which occupy a large area of the sketchy scene. As mentioned before, a sentence-level caption only describes the most salient parts of a sketchy scene. In the example, the \"rabbit\" has been treated as the salient object and the word \"rabbit\" may co-occur with \"grass\" and \"flowers\" more frequently in the dataset, and thus the \"trees\" are not treated as the salient objects by the captioning model. Another three representative examples of the sentence-level captioning are given in Figure 5. Generally, the salient objects in the selected sketchy scenes can be well identified except the objects \"woman\" and \"house\" in the first one, and this bad result may be caused by the imprecise visual representation of the sketchy scene. That is, the \"woman\" is occluded by the \"tree\" in front of her, making the captioning model fail to recognize the \"woman\" correctly.",
        "GPT2_formal_text": ". But, the thing is, the original prompt wasn't there. So, we had to figure out what the prompt was supposed to be. We did this by using a method from the field of information retrieval. Formal: For each task, we calculated the average precision (P), recall (R), and F-score (F) for each domain for each given prompt (P, R, F) and token (T). This was all done using the code we shared in the paper. Formal: Since each domain had a different mix of documents, we decided to keep it separate for now. We also tried a different approach where we used a one-hot encoding for all domains and a weighted sum for the ones that overlap. Formal: Formal: Based on this, we tweaked the model for each domain using a separate development set. Each domain, D_i, got its own representation and had its own training set. Formal: Once we were happy with the settings, we trained the model for 10 epochs on D_i and got the final results. Formal: The model was then tested on D_l, which has 1,000 documents, and the results were pretty much the same as the one we got with the one-hot encoding. Formal: Formal: Finally, we trained the model with D_l and D_l+1,000 documents, and the results are shown in Figure 4. Formal: When we tested the model on D_l+1,000 documents, we got results that were pretty much the same as the one we got with the one-hot encoding. Formal: Formal: We compared the D_l model to D_l+1,000 documents using the same setup as in the original paper. Formal: The results are in Figures 5, 6, and 7. Formal: We picked the top two results for each domain and then averaged the results across all domains. Formal: Figure 7 shows the average precision (P) and recall (R) for the top two results (D_l, D_l+1,000 documents) from the D_l test set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "ccl",
        "filename": "2021.ccl-1.104.json"
    },
    {
        "casual_text": "In Section 3, we told the annotators to pick the word that most reminded them of an experiment and label it as EX-PERIMENT. They also had to connect any related details to that word. Plus, the EXPERIMENT labels could be linked together if they were about the same experiment or a variation of it. Table 10 gives some numbers on how many EXPERIMENT labels were in each sentence and how often the main annotator actually connected them. \n\nIn the training data, there were 703 sentences about experiments, and 135 of those had more than one word that felt like an experiment. Specifically, 114 sentences had two experiment-related words, 18 had three, and 3 had four (check Table 10 for the details). Out of those 114 sentences with two experiment labels, only 2 didn’t link the labels together. When we showed these cases to our main annotator, they thought one of them should have been linked after all.",
        "formal_text": "As stated in Section 3, we instructed annotators to mark the closest experiment-evoking word as EX-PERIMENT and link the respective slot arguments to this mention. In addition, the EXPERIMENT annotations could then be linked either by same exp or exp variation links. Table 10 shows some statistics on the number of EXPERIMENT annotations per sentence and how often the primary annotator actually made use of the possibility to link experiments. In the training data, out of 703 sentences describing experiments, 135 contain more than one experiment-evoking word, with 114 sentences containing two, 18 sentences containing three, and 3 sentences containing four EXPERIMENT annotations (see Table 10 ). In the 114 sentences containing two experiment annotations, only in 2 sentences, the EXPERIMENTs were not linked to any others. Upon being shown these cases, our primary annotator judged that one of them should actually have been linked.",
        "GPT2_formal_text": "= { u_q, u_d, u_a }, d_q, d_a } Formal: This function is based on the idea that a lot of people tend to talk about the same things (like people talking about family or friends) when they’re sharing their thoughts. Formal: We’ve found that adding this kind of \"context\" in text can really boost how well people recall stuff (check out Section 3.2 for more details). Formal: So, we’ve come up with a new model called STATENet, which takes the model from (1) and makes it even better by using a special attention mechanism on the text we’re trying to predict. Formal: When we tested it out, we saw that this new attention helps a lot in predicting things (see Section 3.3 for the details). Formal: There are a bunch of ways people have tried to improve how well people recall stuff, but STATENet is our best approach so far. Formal: Here’s a quick rundown of the other stuff we’ve worked on: Formal: In Figure 1, we’ve shown the current state-of-the-art (SOTA) for a specific conversation in a restaurant. The red points represent what the model was trained on, and the green points show the actual probabilities it got from the annotators. Formal: We’ve also included the average accuracy and what percentage of the time the model got the right answer. Formal: The cool thing about STATENet is that it’s a universal model that can be easily adapted to different areas. Formal: We tested STATENet on the restaurant conversation dataset (check out Section 3.1 for more details). Formal: We got the accuracy from these tests and the performance is shown in Figure 3. Formal: Here’s what we’re really excited about from this work: Formal: We’ve developed a new model for retrieval-based text style transfer that can actually work across different areas. Formal: We’ve made some major improvements to the model, which we’ll share in Section 4. Formal: We’ve also created a dataset for text style transfer, and in Section 5, we’ll show that the model we’re proposing performs better than the usual retrieval-based methods. Formal: We also share the average accuracy and what",
        "directory": "acl",
        "filename": "2020.acl-main.116.json"
    },
    {
        "casual_text": "Hey, so for all the questions I just answered, I think I’m pretty good at getting, understanding, and using basic health info to make smart decisions. I mean, I trust doctors a lot (TD), but I also like to stay informed and make choices that make sense for me. So yeah, I feel confident in my ability to handle that stuff.",
        "formal_text": "Regarding all the questions you just answered, to what degree do you feel you have capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions? Please explain you answer in a few sentences. Trust in physicians (TD)",
        "GPT2_formal_text": "_t1, ..., attention_mask_tL) for a sample_t. The model's output, which is a vector, is built using a feed-forward neural network (FFNN) with a hidden dimension of d_s. Formal: We're looking at the input representations of the y_i that come from the attention module. Each attention head is a bi-directional GRU, and each input is a matrix with dimensions d_i. The output of the attention module is a vector that has the same number of dimensions as the input. Formal: This is done with the help of a linear mapping from X to Y and a two-layer feed-forward neural network (FFNN) with a hidden dimension of d_s. Formal: Since the input and output are the same size, the model can easily get the same output. Formal: The input x_i is represented as the dot product of the input representation x_i with the attention head attention_t1, ..., attention_mask_tL. Formal: We're assuming that the attention weights, f(x_i), are already known for the sample_t. Formal: We calculate the attention weights f(x_i) by averaging the input representations x_i and the attention head attention_t1, ..., attention_mask_tL. Formal: Finally, we take the attention weights f(x_i) and multiply them by the combined representation x_i. Formal: The model output, y_i, is a matrix that has the same number of dimensions as the input. Formal: The attention weights f(x_i) are also known for the sample_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.304.json"
    },
    {
        "casual_text": "Lately, a lot of normalization techniques have been using neural machine translation (NMT) on a character level, kind of like how older SMT-based methods did it. Bollmann and Søgaard (2016) used a bidirectional long short-term memory (bi-LSTM) deep neural network to normalize historical German text at the character level. They also checked how well the model worked when extra data was added during training (like multi-task learning). Their results showed that neural network-based normalizations were better than those done by conditional random fields (CRF) and Norma, especially when the models were trained with extra data.\n\nTursun and Cakici (2017) tried out LSTM and the noisy channel model (NCM), which is often used for spell-checking, to normalize Uyghur text. They used a base dataset of around 200 sentences from social media, both automatically and manually normalized. They also created synthetic data by scraping news websites and adding noise to it by randomly swapping characters with corrupted ones. Both methods did a great job normalizing the text, showing their effectiveness.\n\nSimilarly, Mandal and Nanmaran (2018) used an LSTM network to successfully normalize code-mixed data with an accuracy of 90.27%.",
        "formal_text": "Recently, many normalization methods utilized neural machine translation (NMT) analogously to the previous SMT based approaches on a character level due to its considerable ability in addressing the task. Bollmann and Søgaard (2016) have used a bidirectional long short-term memory (bi-LSTM) deep neural network to normalize historical German on a character level. The authors have also tested the efficiency of the model when additional auxiliary data is used during the training phase (i. e. multi-task learning). Based on their benchmarks, normalizations using the neural network approach outperformed the ones by conditional random fields (CRF) and Norma, where models trained with the auxiliary data generally had the best accuracy. Tursun and Cakici (2017) test out LSTM and noisy channel model (NCM), a method commonly used for spell-checking text, to normalize Uyghur text. In addition to the base dataset (≈ 200 sentences obtained from social networks, automatically and manually normalized), the authors have generated synthetic data by crawling news websites and introducing noise in it by substituting characters with their corresponding corrupted characters at random. Both of the methods have normalized the text with high accuracy which illustrates the their effectiveness. Similarly, Mandal and Nanmaran (2018) had employed an LSTM network and successfully normalized code-mixed data with an accuracy of 90. 27%.",
        "GPT2_formal_text": "(0.1). The formula for the thing we're looking at here is:\n\n|v_t| = (v_a + v_c + ... + v_T) - 1 / (1 - v_{t-1}) + a_t.\n\nSo, it's like summing up (v_a + v_c + ... + v_T) over (1 - v_{t-1}) and then adding a term (a_t) there. Formal: Basically, there's a function that takes inputs (v_t, v_a, v_c), turns them into vectors, and gives you back a result (a_t). Formal: But, there's a catch. The input for this function, v_{t-1}, is the same as v_t itself. Formal: To make it work, we need to throw in some constraints (like, v_{t-1} is greater than 0). Formal: But here's the thing: that's way too much work. It's not easy to expand these constraints, and we might not even know the full set of them. Formal: So, instead, we just pick the x_t that gives the best result based on the input, which is v_t, and run that through the function. Formal: Now, if we want to use these inputs for something specific, we just go ahead and use them. Formal: Formal: So, we have this setup where (v_t, v_a, v_c) are all vectors that fit into a certain category (C). Formal: And the formula for the feature vector is just v_t, v_a, v_c, ..., v_{t-1} + a_t. Formal: But, we can't use the whole set of constraints here. Formal: Also, we could just combine all the results of the function we're using. Formal: We can do this using the function we just defined. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-5519.json"
    },
    {
        "casual_text": "Alright, big thanks to our sponsors, the Japanese Society for the Promotion of Sciences and the Laboratoire Parole et Langage. Their awesome support helped us give fee waivers to PhD students who were first authors of accepted papers and cover the costs for our invited speakers.\n\nHere’s a quick rundown of some of the teams and their work:\n\n- **LangResearchLab_NC at CMCL2021 Shared Task**: Predicting Gaze Behaviour Using Linguistic Features and Tree Regressors by Raksha Agarwal and Niladri Chatterjee.\n- **TorontoCL at CMCL 2021 Shared Task**: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction by Bai Li and Frank Rudzicz.\n- **LAST at CMCL 2021 Shared Task**: Predicting Gaze Data During Reading with a Gradient Boosting Decision Tree Approach by Yves Bestgen.\n\nAnd that’s a wrap for the teams!",
        "formal_text": "Finally, thanks again to our sponsors: the Japanese Society for the Promotion of Sciences and the Laboratoire Parole et Langage. Through their generous support, we have been able to offer fee waivers to PhD students who were first authors of accepted papers, and to offset the participation costs of the invited speakers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 LangResearchLab_NC at CMCL2021 Shared Task: Predicting Gaze Behaviour Using Linguistic Features and Tree Regressors Raksha Agarwal and Niladri Chatterjee. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction Bai Li and Frank Rudzicz. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 LAST at CMCL 2021 Shared Task: Predicting Gaze Data During Reading with a Gradient Boosting Decision Tree Approach Yves Bestgen. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 Team",
        "GPT2_formal_text": "ed = attention_masked(h_t) + attention_masked_multiply(h_t, m_t) + attention_masked_multiply(h_t, p_t). Formal: In this setup, we take the input h_t and turn it into a sentence using a language model. The system then checks if the sentence is grammatically correct or not. Formal: To keep things simple, we'll only talk about the whole process of generating a sentence. Formal: We train a language model by getting a bunch of labeled examples for a new sentence. We take these labeled examples from the training set and use them to create the sentence. Formal: After generating the sentence, we have a few options: the grammar is correct, the word order is right, and the word is in the right spot. But we don't know anything about the lexicon, which is the big part of understanding what the sentence means. So, we look for the best possible options in the whole sentence and pick the one that seems most likely. Formal: We use a statistical language model (LM) to find the best possible sentence by minimizing something called the Dirichlet prior. Formal: To make sure we get the most accurate sentence, we use a statistical language model (LM) to minimize a non-linear function called the posterior probability of the correct grammar. Formal: We also add the Dirichlet prior to the LM to make sure the model doesn't get influenced by the context. Formal: Formal: We run the generation process on the same set of labeled examples and the results are shown in Figure 1. Formal: Here, λ is a parameter that helps us make sure the generated sentence is grammatically correct. Formal: The final sentence we end up with is the one we want to use for retrieval. Formal: But here's the thing: the translation model doesn't really focus on the meaning of the input sentence. It only looks at how the words are arranged in the source language. Formal: So, to make sure we get the best possible translation, we train a statistical language model (LM) to find the best translation in the whole sentence. Formal: Formal: Formal: We train the LM using a specific loss function and also add the Dirichlet prior to the LM. Formal: Formal: Finally, the best translation is the one that minimizes the LM loss. Formal:",
        "directory": "cmcl",
        "filename": "2021.cmcl-1.0.json"
    },
    {
        "casual_text": "We came up with two types of features for splitting sentences: the phrase-embedding feature and the segmentation-specific feature. The final feature for each segmentation is just a mix of these two. Oh, and the phrase-embedding feature is also used for sentiment classification, not just sentence segmentation.",
        "formal_text": "We design two kinds of features for sentence segmentation, namely the phrase-embedding feature and the segmentation-specific feature. The final feature representation of each segmentation is the concatenation of these two features. It is worth noting that, the phrase-embedding feature is used in both sentence segmentation and sentiment classification.",
        "GPT2_formal_text": "= {query_id: query, key: key_1, value: value_1} 2. Formal: So, for a query with ID = k, the first k words are the keys, and the last k are the values. Formal: For each token in the source sentence, let's call it token1, token2, ..., tokenN, we grab the top k tokens with the highest scores. Formal: Then, we use a classification task (or a regression task) to figure out the probability of the next word. Formal: The probability for a given token (token1, token2, ..., tokenN) is calculated by the logistic regression model. Formal: Formal: Basically, the goal here is to predict the probability of the next word. Formal: Formal: For a given token, the logistic regression model gives us the probability of the next word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D14",
        "filename": "D14-1054.json"
    },
    {
        "casual_text": "In this paper, we treat both the translation and generation methods as ways to boost the amount of data. Our main goal is to deal with the issue of noise in these augmented datasets. We come up with a structured way to learn from multiple noisy augmented datasets for cross-lingual SLU, especially when there's no perfectly labeled data in the target language. Our key technical contribution is a set of methods to clean up the noise, like relabeling instances, co-training, and re-weighting instances.",
        "formal_text": "In this paper, we regard both the translation approach and the generation approach as data augmentation methods. We tackle the problem of reducing the impact of noise in augmented data sets. We develop a principled method to learn from multiple noisy augmented data sets for cross-lingual SLU, where no golden-labeled target language data exists. Our major technical contribution consists of a series of denoising methods including instance relabeling, co-training and instance re-weighting.",
        "GPT2_formal_text": "can be adjusted depending on how many input tokens you want to adjust the attention mask. Formal: In the next section, we'll talk about the strengths of each of these pruning methods. Formal: We use the average of the multi-head attention scores for each token in the test set to calculate the sentence attention score. This score shows how much attention the input sentence gets for each token. Formal: Following what Rubinstein and his team did in 2017, we apply this attention score pruning method to the first model (M) to get the sentence attention score for the token y_t, which is a 1 in the formula. This score is the same for the second model (M) as well. Formal: We calculate the sentence attention score by dividing the multi-head attention score for the token y_t by the sentence attention score for the whole sequence. Formal: Formal: We've got three different versions of these sentence attention scores. Formal: First, we calculate the attention score using the average of the multi-head attention scores for each token in the test set. Then, we calculate the sentence attention score by dividing the multi-head attention score for the token y_t by the sentence attention score for the whole sequence. Formal: We pick the model with the best sentence attention score by maximizing the best possible sentence attention score for the token y_t. Formal: Next, we calculate the sentence attention score using the sentence attention score for the token y_t. Formal: Formal: We pick the model with the best sentence attention score by maximizing the best possible sentence attention score for the token y_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.259.json"
    },
    {
        "casual_text": "We figure that for a perfect system (or what we call the human gold standard), the correlation index should be 1, meaning it gets 100% accuracy on both the monothematic dataset and the original RTE dataset. So, we think of CI = 1 as the ideal situation. To see how close a system S is to being perfect, we just subtract its actual correlation from this ideal CI.",
        "formal_text": "We expect the correlation index of an optimal ideal system (or the human goldstandard) to be equal to 1, i. e. 100% accuracy on the monothematic dataset should correspond to 100% accuracy on the original RTE dataset. For this reason, we consider CI = 1 as the ideal correlation, and we calculate the difference between such ideal CI and the correlation obtained for a system S.",
        "GPT2_formal_text": "= (t) i, j, k, l) and sets a limit for how many edges can be processed at once. Formal: In this project, we use the TF×IDF matrix A to represent a discourse tree, which is basically a sequence of triplets that include a discourse label. Following the method suggested by Schumann et al. (2017), we calculate the model parameters using the L2 norms of a self-loops in a feed-forward network. This setup gives us an embedding for each discourse relation as h(t). Formal: We also run a bunch of experiments to figure out the best weights to use for the Dirichlet priors λ0, λ1, and λ2, which are the parameters for the co-occurrence-based model. Formal: To make sure our method works well with all types of discourse trees, we set up some experiments where we switch the discourse label from t to y if the model can't tell the difference between t and y. In these cases, we keep the graph constant. Formal: We also test how well our methods work for parsing text using two different datasets. Formal: We also look at how well our algorithm can handle text and image data to figure out the best settings for the model. Formal: The feature weights for the discourse relation embedding we get are shown in Table 2. Formal: We picked the first four features that show the most differences between t and y in terms of discourse relation label distribution. Formal: The relation term vector h(t) shows the probability of a relation term t showing up in a particular context c, as predicted by the discourse relation label distribution h(t). Formal: We use a GAT-based model (Bahdanau et al., 2014) to extract discourse relation embeddings. Formal: We use a GAT-based model (Bahdanau et al., 2014) to extract discourse relation embeddings. Formal: Formal: For the image and text datasets, we just use the embeddings from GAT to get the embeddings for each relation. Formal: Formal: Formal: We calculate the model parameters by averaging the embeddings from the four representative GAT-based models, using GAT's learning rate (λ) to control the learning rate. Formal: Formal: We use a general neural network (GAN) (Kim et al., 2014) to learn the parameters",
        "directory": "C10",
        "filename": "C10-2012.json"
    },
    {
        "casual_text": "Sure, when we use a model trained on this kind of transformed data for new inputs, it’ll produce transformed outputs. So, the transformation needs to be reversible, meaning we can convert the model’s output back to the original, un-transformed value.",
        "formal_text": "Of course, when we apply a model trained on this type of transformed output to new inputs, it will generate transformed output values. Thus, the transformation must be reversible, so that we can map the output of the model back to an un-transformed output value.",
        "GPT2_formal_text": "ed (att) and attention_masked_mult (attm) to the input sequence. If the input has multiple tokens, we use a linear layer to get the final representation for each token. Formal: To add a linear layer to the input, we first normalize the attention vector and then multiply it by a linear layer with the sigmoid function. Formal: The token representation is basically a mix of the attention values and the output token representation. Formal: In both the CNN and transformer models, the final representation is created by mixing the attention vectors and the output token representation together. Formal: We can also see how the model's parameters interact and change based on how well it's doing on the validation set. Formal: We break down the model's parameters into two types: feature weights and attention weights. Formal: In the original model, the attention weights for each token are tied to the attention scores for the previous and next tokens. However, in this new model, we use global attention to focus on the most important tokens. The global attention mechanism helps the model learn better representations for the tokens. Formal: Here's how the attention weights are set up in the CNN model: Formal: We train the CNN model by treating it as a classifier. This means we create a feature vector for each token and then use that vector to predict the class for the token. Formal: We also train the transformer model by predicting the class based on the hidden state. Formal: For the regular transformer model, the feature weights are set to a fixed value, while for the convolutional neural network model, we use the attention weights. Formal: We use a linear layer to create the representation for each token. Formal: Formal: The output is a mix of the attention values and the output token representation. Formal: We test this on the validation set to see how well the model performs. Formal: The validation set helps us see how well the model performs overall. Formal: Formal: We add linear layers to the input sequence to get the final representation for each token. Formal: For the CNN model, we treat it as a classifier, using the input sequence to create the feature vector for the token. Formal: We train the CNN model with a convolutional neural network. Formal: The final representation is a mix of the attention vectors and the output token representation. Formal: Formal: We also train the transformer model using the",
        "directory": "D07",
        "filename": "D07-1084.json"
    },
    {
        "casual_text": "Li et al. (2020) had a similar idea to ours, which is to make word representations better by using context. But their work isn't open source, and they didn't use the same dataset, so we can't really compare it to MINER. Instead, we compared our method to some baselines like this:",
        "formal_text": "Li et al. (2020) share the same intuition as us, enriching word representations with context. However, the work is neither open source nor reported on the same dataset, so this method cannot be compared with MINER. We compare our method with baselines as follows:",
        "GPT2_formal_text": "= [q, w, b]  \n\nNow, for each token `t` in `T`, we use a TF-IDF vector `v_t`, which is in the range [-1, 1] for `T` and `|C|` for `T`. Formal: In Figure 4, the two models are combined. The last column shows the average performance across different domains. Formal: This model builds on the idea that the labels are usually more informative than the actual content, but the actual content often doesn't match the labels. Formal: This model uses something called the Informedness Penalty (IPS) to decide how much to focus on the labels and how much to ignore the actual content. Formal: We use a graph-based method called graph-based attention to combine the two representations. We use a model called GAT (GloVe) to do this. Formal: A similar idea was suggested by Mikolov et al. in 2013, so we decided to go with it as our attention mechanism. Formal: The discriminative model we're using here is based on a version of the Penn Treebank (PTB) that was trained on a mix of news and Wikipedia data. Formal: Unlike the standard RNN (Hochreiter and Schmidhuber, 1997), we use self-attention mechanisms to calculate the scores for each token. Formal: To make things fair, we use two versions of the same model. Formal: Following what Chiang (2018) and Xie et al. (2019) did, we apply the usual binary cross-entropy loss to the attention scores. Formal: We run the model for 100 epochs on a single NVIDIA Tesla K80 GPU. Formal: The model is built using PyTorch (Paszke et al., 2019) and our own approach (Section 4.2). Formal: We train the model on a single Titan RTX GPU. Formal: Formal: We tested our model on five different datasets: Yelp-11, Yelp-13, Yelp-14, Yelp-15, and Yelp-16. Formal: Formal: We compared our model to the basic model using the same evaluation method. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.383.json"
    },
    {
        "casual_text": "The interface stacks the source and target language segments, with the source on top and the target below for editing. In Figure 2, you can see the PosEdiOn interface: the upper box shows the source segment, and the lower one lets the translator make changes. There are toolbar buttons at the bottom that let you scroll through the whole document to see more context.",
        "formal_text": "The interface displays the source and target language segments one on top of the other. Figure 2 shows the PosEdiOn interface, where the upper window contains the source segment and the lower window enables the translator to edit the text. Translators can see a wider context using the toolbar buttons located on the lower part, which can be used to move along the whole document.",
        "GPT2_formal_text": ". For our neural network setup, we use the original Transformer (from Vaswani et al., 2017) as the encoder. The final hidden state h_t is created by combining the output from the input embedding module and the output from the output layer. Formal: The goal is to figure out the probability of a token i in a sentence y given a source sequence y_i. Formal: The model creates a probability distribution P(y_i | x_i) by looking at the sequence of tokens that are connected to y_i. Formal: The model uses the embedding of the source token x_i as its input. The output is made by combining the output from the input embedding module and the output from the output layer. Formal: This whole process is repeated for each token in the target sentence y. Formal: The model also uses the context to predict the probability distribution P(y_i | x_i). Formal: Since each token x_i has a unique context vector, the model can directly calculate the probability P(y_i | x_i) for the target token x_i. Formal: To get the probability P(y_i | x_i), the model checks the embedding vector of x_i to find the context embedding for the target token x_i. Formal: The model combines the input embedding module with the output layer to generate the probability distribution P(y_i | x_i) for the target token x_i. Formal: We calculate the relevance score to figure out the probability of the target token y_i based on the input token x_i. Formal: Lastly, the relevance score is calculated using the output layer output, and this helps us determine the relevance score for the token x_i. Formal: Finally, the relevance score is calculated using the output layer output, and this helps us determine the relevance score for the token x_i. Formal: We’ve got three main goals for designing a neural network model that can make this kind of prediction: Formal: First, to generate the probability distribution P(y_i | x_i) for the target token x_i, we need to learn the embedding vectors of x_i. Formal: Then, to get the probability P(y_i | x_i), we’re also dealing with the context embedding for the target token x_",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "Okay, so here's the deal: the monitor keeps an eye on the whole reconstruction process. If it thinks things are taking too long, it steps in by raising the threshold to stop the process when it detects inconsistency. Figure 3 gives an example of how this space reconstruction works.\n\nIn Fig. 3(a), the distance between entities A and B is 10. Now, let's say a new estimate puts that distance at 5. The reconstructor checks out the neighbors of both A and B. It decides to move entity B because B's neighbors are less crowded than A's. The reconstructor picks one spot out of eight possible positions around A where B can go with the least amount of new inconsistency.\n\nIn Fig. 3(b), B is placed to the left of A. Any new inconsistencies, like those involving B and G, are checked and noted in *inconsistent. After a few rounds of trial and error to reduce inconsistency, the space settles into the final configuration shown in Fig. 3(c), which is all good and consistent.",
        "formal_text": "in *inconsistent. 6. Go to step 1. The monitor monitors the whole reconstruction process and stops the process by raising the threshold to judge the inconsistency when it judges that the reconstrnetion takes too much time. Fig. 3 shows an example of the process of space reconstruction. Ill Fig. 3(a), the distance between the entities A and B was 10. Let's assume that a new estimation for tile distance is 5. The reconstructor looks around the neighbors of both entities, and decides to move tile entity B because the neighbors of B are less dense than those of A. The reconstructor selects one position that causes the least new inconsistency, from among eight positions around A, for B to be placed in. In Fig. g(b), ]: 3 is placed to the left of A. New inconsistencies ill the scope of *attention such as inconsistency about B and G are checked and registered in *inconsistent. After a few 3\"77 trial loops to decrease inconsistency, the space settles in the configuration shown in Fig. 3(c), which includes no inconsistency.",
        "GPT2_formal_text": ", and V_a is just the average of all the attention scores for all the local entity mentions. Formal: (2) To pick the best answer, we use a multi-view AdaBoost algorithm on the hidden states of the entity features. Formal: We use cosine similarity with a non-linear function to calculate the cosine similarity for both the input and output representations. We also calculate the score for each query by averaging the scores from the different views. Formal: To make sure the attention weights are correctly assigned, we calculate the dot product between the query and the entity features using the cosine similarity. We also check how similar each query is to the entity features by calculating the cosine similarity between the attention weights for each query. Formal: We use Adam (from Kingma and Ba, 2014) to train the model using the training set. Formal: The best result is the one with the highest cosine similarity score on the validation set. Formal: We use the parameters α, β, and γ, along with a learning rate of 1e-4 for training and 1e-3 for validation, to tweak the parameters. Formal: The final version of the model is trained using SGD with an early stopping strategy. Formal: We also calculate the score for each query by averaging the scores from the different views. Formal: We calculate the score for each entity by averaging the scores from the different views. Formal: Lastly, we pick the top answer using the model trained on the validation set. Formal: We use Adam (from Kingma and Ba, 2014) to train the model using the training set. Formal: The best result is the one with the highest cosine similarity score on the validation set. Formal: We use the parameters α, β, and γ, along with a learning rate of 1e-4 for training and 1e-3 for validation, to tweak the parameters. Formal: The final version of the model is trained using SGD with an early stopping strategy. Formal: We also calculate the score for each query by averaging the scores from the different views. Formal: Finally, we pick the top answer using the model trained on the validation set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C86",
        "filename": "C86-1089.json"
    },
    {
        "casual_text": "Ukrainian President says it's not true when @cnnbrk shared that Russia claims their records show a Ukrainian warplane was within 5 km of #MH17 on the day it crashed.",
        "formal_text": "Ukrainian Pres. says it's false \"@cnnbrk: Russia says records indicate Ukrainian warplane was flying within 5 km of #MH17 on day of crash.",
        "GPT2_formal_text": ", adjacency_mask, and attention_level to get the full picture of the document. Formal: We can use attention to model the entire document, while also paying attention to specific parts of it. Formal: For the rest of the paper, we'll use the term \"attention\" to mean both the attention mechanism and the attention-based model. Formal: From now on, let's just call the entity-aware attention mechanism an \"attention_mask.\" For a specific entity, let's say e, the entity-aware attention_mask can be written like this: Formal: This approach has two main benefits. First, it makes the model more flexible—you can easily swap out the model for other ones, like the entity-aware attention mechanism. Second, the model can be tweaked to handle different situations better, like dealing with changes in the input or the data itself. Formal: When generating text, the model can focus on the important parts of the text and decide when to stop generating. Formal: We can also think of the attention mechanism as a kind of memory storage that keeps track of the important parts of the text, and it can be used later to give feedback to the model. Formal: Using this approach, we can use the model to \"remember\" important info from the past, which will be super useful for later tasks. Formal: Formal: For more info, check out section B. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-5006.json"
    },
    {
        "casual_text": "When you plug this IF into the mapper, it spits out the FS structure you see in Figure 6.",
        "formal_text": "When this IF is fed into the mapper, the FS structure shown in Figure 6 is produced.",
        "GPT2_formal_text": "is a group of ID tags that have a specific meaning, like the one in Figure 1. Each position in the sentence has a set of overlapping words. A word can be part of more than one relationship. For example, the word \"the\" can be linked to: Formal: We can think of the translation model as a three-layer feedforward neural network, where each layer has a hidden state and a bias vector. Formal: To make sure the model can pick up on the relationships between words and phrases, we set up two loss functions, α and β. Formal: When we're decoding, if the model's prediction (y) is wrong, we'll just ditch the prediction (x) and use the original translation (y) instead. Formal: The lower the loss function, the better the model's performance. Formal: Also, if we have a lot of training data, the model tends to repeat words more often. Formal: If we have a lot of training data, the model might focus more on finding the right translation for each word. Formal: We think this pattern might show up because the model is being trained on a huge amount of data, which could explain why it often produces the same translation for a word over and over. Formal: When we calculate the final scores, we only look at how well the model's predictions match up with the actual translation. This way, we can focus on the best translation without worrying about the exact words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "bcs",
        "filename": "2006.bcs-1.12.json"
    },
    {
        "casual_text": "So, the tasks in CLUES-Real have explanations from different teachers and students for each teacher. This gives us a lot of info about how teachers and students perform differently, showing how well different tasks can be learned through language. In section 4, we share some details about how teachers and students did in our setup.",
        "formal_text": "Thus, the tasks in CLUES-Real contain explanations from multiple teachers and multiple students corresponding to a teacher. This provides rich information about variance in teacher and student performance indicating how amenable different tasks are for learning via language. We provide insights into the performance of teachers and students of our setup in §4.",
        "GPT2_formal_text": "_v2(x, y) for the set of attention weights. Formal: In this paper, we came up with a new way to measure how important a word is. We use a soft attention mechanism that looks at both the word itself and the context around it. We created a word embedding model using an RNN and a CNN. The model takes the input word x and turns it into a vector, which we call h_x. Then, it uses something called the softmax function to figure out how important the word is. In our experiments, we used a vocabulary of around 30,000 words, and we used two different versions of the CNN. Formal: In this paper, we also came up with two ways to measure how related words are, which we call the dot product and the cosine similarity. We tested these measures on three different datasets. Formal: The dot product is basically the average of the dot product between two vectors, x and y. The cosine similarity, on the other hand, is a way to measure how similar two vectors are by looking at the cosine similarity between their vectors. Formal: The similarity measure uses the cosine similarity between two vectors, x and y. Formal: Lastly, we mentioned that we plan to use these scores to build a new dataset that focuses on text based on word relationships. Formal: We also showed how you can use the similarity measure to add more information to the input text. Formal: We tested how well the method works with different text types. Formal: We used our method to measure how similar the input text is to the target text, like in a single-label setup (SLS). Formal: We also looked at how different types of relationships can help with the overall accuracy of the data. Formal: Lastly, we talked about the challenges we faced when trying to create a dataset specifically for text-based relation extraction. Formal: When we tested our method on a bigger dataset, we found that the accuracy was still pretty low. Formal: Lastly, we mentioned that we’re thinking about developing a new method to add in more relation types and improve the overall accuracy of the data. Formal: We also mentioned that there might be other types of relationships that don’t fit well into the SLS setup. Formal: We also looked at how different types of relationships can help improve the accuracy of the data. Formal: Formal: We also looked at how different types of",
        "directory": "acl",
        "filename": "2022.acl-long.451.json"
    },
    {
        "casual_text": "We think the main reason DIORA struggles with supervised parsing is because it can't really fix mistakes once they happen. DIORA is great at finding the best tree based on the numbers it gets for each part of the sentence, but the problem is in how it assigns those numbers. It only uses local information, so it might give a low score to a part of the tree that should actually be higher if it had more context. This can be a big deal when a sentence is ambiguous and needs more context to make sense. For example, take the sentence \"We saw the dog with my phone.\" The way it's parsed depends on the context. In the next part, we'll show how we fixed this issue in DIORA.",
        "formal_text": "We posit the low performance in supervised parsing is due to DIORA's inability to effectively recover from local errors. Predicting trees in DIORA is exact -you are guaranteed to find the highest scoring tree given the scalar values associated with each span, but there is a weakness when assigning the scalar values. Specifically, the scalar values are assigned using local information, and may assign a low weight to a subtree which, when given more information, deserves to be given higher weight. Said plainly, this might occur when the sentence has structural ambiguity that requires context to resolve. For instance, the clause 'We saw the dog with my phone, ' has a more likely parse tree depending on the context. 2, 3 In the next section we present our extension to DIORA that addresses this downside.",
        "GPT2_formal_text": ".py):\n\n- For the SLU (Spoken Language Understanding) stuff:\n  - For the code snippets and machine translation (MT) outputs:\n  - For the gold texts:\n  - For the gold monolingual texts:\n  - For the gold English-French translations:\n  - For the gold multilingual texts:\n  - For the gold monolingual monolingual English-French pairs:\n  - For the gold multilingual English-French pairs:\n  - For the gold multilingual texts:\n\nWe use the OpenNLP toolkit (http://www.statmt.org/) to figure out the source text, tokenization, POS tags, and the code tokenization. The final tokens are calculated based on the source text.\n\nOur models are trained on the corpus of the SLU task. We also use an English-French translation model that was trained on the corpus (C-F) to handle the target language. Formal: We also use the OpenNLP toolkit to figure out the source text, tokenization, POS tags, and the code tokenization. The final tokens are calculated based on the source text. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.392.json"
    },
    {
        "casual_text": "Since there aren't many examples of split-antecedent anaphora in ARRAU, we decided to use four extra corpora to help the system perform better. These extra corpora came from either the Phrase Detectives (PD) corpus, which was annotated by lots of people, or the gold-standard annotated version of ARRAU.",
        "formal_text": "Since the number of examples of split-antecedent anaphora in ARRAU is small, we deployed four auxiliary corpora created from either the crowd annotated Phrase Detectives (PD) corpus or the gold annotated ARRAU corpus to improve the performance of the system.",
        "GPT2_formal_text": "_t is like a way to change the attention weight in the input layer. The last hidden state vector is a mix of the attention weights for each token in the input. Formal: Here, S = (s_i, t_i) is the attention weight for the i-th token, and T = (t_i, s_i) is the attention weight for the s-th token. The final output vector f_s is a mix of the token attention vectors. Formal: We also use a Bi-directional Long Short-Term Memory (Bi-LSTM) (Hochreiter and Schmidhuber, 1997) to process each token, which is used to estimate the token attention weights. Formal: We then combine the token attention vectors with the output vector f_s to get the final representation for the whole input token. Formal: Here, we train a linear classifier to pick out the top-N most likely tokens as the response, based on the attention vector f_s. Formal: So, the main idea is to look for the best response from the top k possible options. Formal: Let’s say W_s_i is the response vector w_s_i for the i-th token, and W_s_k is the response vector w_s_k for the k-th token. Formal: The alignment matrix A_s_k is a linear combination of A_s_i and A_s_k, where each A_s_k_i is a row in A. Formal: In the next part, we’ll look at the alignment matrix A_s_k for the input sequence x_i. Formal: Let’s say q_i is the query vector q_i for the i-th token. Formal: The input sequence is represented as a vector with n elements, like (x_i, q_i). Formal: The output representation is a matrix with dimensions d_s, where d_s = (d_s_1, ..., d_s_n). Formal: We also use a Bi-directional Long Short-Term Memory (Bi-LSTM) to process each token, which is used to estimate the token attention weights. Formal: Following the approach by Guo et al. (2017b), we also combine the token attention vectors with the output vector f_",
        "directory": "coling",
        "filename": "2020.coling-main.538.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\n- **w_i**: Just think of this as a single word in a sentence.\n- **w_i w_i+1**: This is called a bigram, which means two words in a row, like \"word one\" and \"word two.\"\n- **S**: This is the whole sentence.\n- **tf_d(w_i)**: This is how many times the word **w_i** shows up in a specific document or text (let's call it **d**).\n- **df_D(w_i)**: This is how many documents in a collection (let's call it **D**) contain the word **w_i**.\n\nSo, in short:\n- **w_i** = a word\n- **w_i w_i+1** = two words together\n- **S** = the full sentence\n- **tf_d(w_i)** = how often a word appears in a specific text\n- **df_D(w_i)** = how many texts have that word in them",
        "formal_text": "w i unigram w i w i+1 bigram S sentence tf d (w i ) the frequency of w i occurring in d df D (w i )",
        "GPT2_formal_text": ", rows, and columns, with values of {k}, {l}, and {m}. Formal: We can use a feature extractor with three different kernels to get the features we need. Formal: For each column in the table, we grab the values for the i-th and j-th rows (and the column names). Formal: Next, we turn the table's features into vectors and map them to something called ILP vectors. Formal: Finally, we create a logistic regression classifier using those ILP vectors. Formal: Formal: The table features are represented as a matrix, and the ILP predictions are a vector that includes the features. Formal: Formal: The classifier is trained using a feature extractor that uses three different kernels. Formal: For each of the three kernel choices, we predict the feature vector for the i-th row and column. Formal: The classifier is trained with the training set for two rounds. Formal: Formal: The classifier uses these predictions as features to train a linear model. Formal: Formal: This linear model calculates the logistic regression probability for the feature vectors. Formal: Formal: Here's how the classifier is trained. Formal: For each of the three kernel choices, we predict the feature vector for the i-th row and column. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C10",
        "filename": "C10-2060.json"
    },
    {
        "casual_text": "We're using this version: praw.readthedocs.io/en/latest. You can check out all our code here: http://www.cs.cmu.edu/pmichel1/mtnt/.",
        "formal_text": "In particular, we use this implementation: praw. readthedocs. io/en/latest, and our complete code is available at http: //www. cs. cmu. edu/ pmichel1/mtnt/.",
        "GPT2_formal_text": ", ai_mask, aj_mask, and aj_mask. Each of these is calculated using a mix of the predicted and actual attention values. Finally, the output representation gets combined with the input to create the final representation for the image. Formal: So, we take the word representations (w_i, w_j) from the input image (x) and turn them into word vectors (x_i, x_j). To get the attention mask, we average the attention vectors from the word vectors (x_i, x_j). This approach is pretty common in most recent methods for image captioning (like Lample et al., 2016; Ma and Hovy, 2016; Li et al., 2016b; Ma and Hovy, 2017). Formal: After that, the output from the first attention layer (a_1) is passed to the second attention layer (a_2), and so on, until the output from the last attention layer (b_1) is passed to the last attention layer (b_2). Formal: In Figure 2, the size of the token embedding D_t for the image (x) is calculated using this approach. Formal: In Figure 4, the size of the token embedding D_t for the image (x) is also calculated using this approach. Formal: For each of the attention masks, we calculate the attention vector by averaging the attention vectors from the mask's output. Formal: To get the attention mask, we average the attention vectors from the mask's output. Formal: Finally, the output from the second attention layer (a_2) is passed to the first attention layer (a_1), and so on, until the output from the last attention layer (b_2) is passed to the last attention layer (b_1). Formal: Formal: To get the attention mask, we average the attention vectors from the mask's output. Formal: To get the attention mask, we average the attention vectors from the mask's output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1050.json"
    },
    {
        "casual_text": "Okay, let's break it down in simpler terms.\n\nSo, let's say τ 0 turns into τ j after a certain number of steps, and that number of steps is at least 1. In each step...",
        "formal_text": "Proof Assume τ 0 factorizes to τ j in a number of steps j ≥ 1. In every step",
        "GPT2_formal_text": "= {0, 1, ..., k}, from k = 1 to k_d. Formal: We've got two types of relations: S and R, and they're shown as (S, R). \n\nThe same idea applies to k_i, k_l, and k_u. Formal: (i, j, k) is just the element from the set `i` to `j`, and (k, i, j, k) is the element from the set `j` to `k`. Formal: For R, the set `R` is a subset of `R^d`, which is d in size. Formal: In the model, the position `i` is always at the beginning, and the position `k` is at the end. Formal: The position `i` is always at the beginning, and the position `k` is at the end. Formal: In each step, we look at the last two elements from `R` and combine them. Formal: So, we keep combining stuff until everything is combined. Formal: In each step, we add up all the combination expressions. Formal: The set `R` includes all the sets that are part of `R^d`. Formal: The set `R` includes all the sets that are part of `R^d`. Formal: The set `R` includes all the sets that are part of `R^d`. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1204.json"
    },
    {
        "casual_text": "A6. Did they copy the previous work instead of reproducing it? And did they make sure that all the MT systems used the exact same pre-processed training, validating, and testing data? Just answer yes or no.",
        "formal_text": "A6. If previous work has not been reproduced but copied, whether it has been confirmed that all the compared MT systems used exactly the same pre-processed training, validating, and testing data: yes or no.",
        "GPT2_formal_text": "ed, attention_encoded, and attention_output masks are used to predict the responses. The time it takes to calculate the attention matrix is O(|T|n). To make things easier and avoid overfitting, we use mini-batches of 200 response tokens and a batch size of 64. Formal: The response embeddings we get are fed into the encoder and the decoder to turn them into a fixed-length vector. Then, they're passed into a fully-connected layer to give us the actual response. Formal: For the response embeddings, we go through the whole sentence and pick the top k response tokens to get their embeddings. Formal: Here, |T| stands for the total number of sentences, and n is the number of response tokens. Formal: A small part of the encoded response vector gets squashed down to a fixed size. Formal: For the input embeddings, we combine the context embeddings and the embeddings of the response token to get the latent representation. Formal: To get the probability distribution P, we multiply the embedding vectors of the response tokens and the response context together. Formal: Formal: We also calculate a latent representation of the response, which is the cross-entropy between the response context and the embedding vectors of the response token. Formal: Formal: Lastly, we use a low-rank linear projection to get the response embedding. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.566.json"
    },
    {
        "casual_text": "The Online Primal Subgradient method, introduced by Ratliff et al. in 2007, calculates the subgradient for the margin objective for each instance by doing a loss-augmented decode. It then uses these subgradients for each instance to optimize the overall objective using Ada-Grad, which was developed by Duchi et al. in 2011. Ada-Grad can be used with either L1 or L2 regularization.\n\nThe basic version of Ada-Grad updates every weight when processing a batch. To speed things up, we split the updates into two types. When the subgradient isn't zero, we do the regular update. But when the subgradient is zero, we delay the update until the weight is needed again. This approach saves time because we only update the weights that have a nonzero subgradient in the current batch, which is usually a smaller number.\n\nAlgorithm 1 provides a pseudocode for our implementation, which was inspired by Dyer's work in 2013.",
        "formal_text": "Online Primal Subgradient (Ratliff et al. , 2007) Computes the subgradient of the margin objective on each instance by performing a loss-augmented decode, then uses these instance-wise subgradients to optimize the global objective using Ada-Grad (Duchi et al. , 2011) with either L 1 or L 2 regularization. The simplest implementation of Ada-Grad touches every weight when doing the update for a batch. To save time, we distinguish between two different types of update. When the subgradient is nonzero, we apply the usual update. When the subgradient is zero, we apply a numerically equivalent update later, at the next time the weight is queried. This saves time, as we only touch the weights corresponding to the (usually sparse) nonzero directions in the current batch's subgradient. Algorithm 1 gives pseudocode for our implementation, which was based on Dyer (2013).",
        "GPT2_formal_text": "= { q i } for i in {0, 1, ..., n}. Formal: We use a mix of regular and strong negative sampling techniques to get samples with different probabilities. Formal: The goal of this approach is to make the negative sampling more reliable and better at handling noisy or unclear data. Formal: We took a closer look at how the model's behavior changes when we use different beam sizes. Figure 6 shows the average changes in ROUGE-2 scores for each dataset after using different beam sizes. Formal: We compared the model's behavior when we use different beam sizes, comparing the model's performance when using 20-BPE (1 × 20) and 25-BPE (1 × 25) beams. The findings show that the model's performance changes depending on the beam size. Specifically, the BPE model's performance drops when using 5-BPE, which matches what we noticed earlier. But when we use 20-BPE, things change a bit because it has fewer possible word options, making it harder to get a high-quality sample. The same thing happens when we use 50-BPE, and then when we use 100-BPE, and so on. Formal: We also did a study to see how using different beam sizes affects the results. We ran a test on Yelp-EL, where we looked at the ROUGE scores for different beam sizes (5-BPE, 25-BPE, and 100-BPE). Formal: We compared how the model performed in the test set (YELP) and the actual final results (YELP) when we use different beam sizes. Formal: Lastly, we checked out how the model's performance changes when we use different beam sizes on the TAC dataset. Formal: We also did a study to see how different beam sizes affect the results. We ran a test on Yelp-EL, where we looked at the ROUGE scores for different beam sizes (5-BPE, 25-BPE, and 100-BPE). Formal: We compared how the model's performance changes when we use different beam sizes. Formal: Lastly, we checked out how the model's performance changes when we use different beam sizes. Formal: Lastly, we checked out how the model's performance changes when we use different beam sizes. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D15",
        "filename": "D15-1032.json"
    },
    {
        "casual_text": "Okay, so we've got this thing where S_k is calculated by adding up U(k-j+1) {l_j } for j from 1 to k-1. After that, we tweak M_k by combining S_k with S(1) {l_k } and using the task loss L_T to fine-tune it.",
        "formal_text": "S k = k−1 j=1 {U (k−j+1) {l j } }. M k is then fine-tuned by using S k = S (1) {l k } ∪S k with the task loss L T.",
        "GPT2_formal_text": "ed: yes; attention_attention: yes; direction_masked: yes; slot_masked: yes; slot_attention: yes; position_masked: yes; position_attention: yes; position_masked: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; slot_masked: yes; position_attention: yes; Slot: yes; Type: yes; Slot: yes",
        "directory": "acl",
        "filename": "2021.acl-short.106.json"
    },
    {
        "casual_text": "When translating from right to left, the left side of the sentence (which is the later part in this case) is actually more accurate than the right side. This is kind of weird because earlier studies said that mistakes build up over time, making the end of the translation worse. But here, the end is better, which doesn't really match what we thought before.",
        "formal_text": "• When translating from right-to-left, the accuracy of the left half (in this way it's the later part of the generated sentence) is still higher than the right half. Such an observation is contradictory to the previous analyses between error propagation and accuracy drop, which regard that accumulated error brought by exposure bias will deteriorate the quality in later part of translation (i. e. , the left half).",
        "GPT2_formal_text": "_t = [s_t1, ..., s_tM] (where s_t1 is the start token, s_tM is the end token, and w_t is a matrix of weights). Formal: Basically, it's a way to figure out how important each token in a sentence is by looking at the attention weights. Formal: • W_t is the weight matrix for the model. Formal: This is kind of like the attention weights we talked about in Section 2. Formal: We're using a linear transformation for the input sentences, which we get using mean-field methods. Formal: We get the attention weights using a linear transformation that connects each token to its own hidden vector. Formal: Lastly, we get the model's output using the same linear transformation but this time using a single attention head. Formal: In Table 1, you can see the results of the attention weights we got. The numbers in bold are the results we got right after applying the transformation. Formal: In this setup, we're using mean-field methods for token attention, and we're sticking with linear transformations for the input sentences. Formal: We figure out the attention weights for each token by using mean-field methods, which is a way to do attention in a structured way. Formal: After applying the transformation, we get the model's output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1396.json"
    },
    {
        "casual_text": "A response with a bigger s mtv (y = 1|c, r) is more likely to get picked. We'll look into other combination models later on.",
        "formal_text": "The response with larger s mtv (y = 1|c, r) is more likely to be selected. We will investigate other combination models in our future work.",
        "GPT2_formal_text": "ing. This method, introduced by Sutskever et al. in 2014, is trained with an attention mechanism that’s been tweaked to work with text. While the usual approach of directly encoding the text, like in the decoder, doesn’t really capture the meaning, it does have some useful shortcuts to make it easier to explain. \n\nThe goal here is to make the attention matrix that focuses on the masked token look like this: Formal: Here, M is the size of the hidden state vector. A is the weight matrix that gets updated during training. The attention matrix is a kind of normalized version of the hidden state vector, kind of like a second-order version. Formal: Also, the hidden state vector is normalized based on the vector from the source context token's representation. This normalized version gives the model the chance to figure out the probability of the masked token by looking at its embedding vector. Formal: For instance, in Figure 1, the red part shows how the attention matrix connects to the hidden state vector of the token's representation, while the blue part shows how it connects to the token itself. Formal: The masked token representation is created by applying a projection to the attention matrix, which is a multi-layer perceptron. Formal: Formal: The attention matrix for the masked token is normalized based on the output vector from the source context token's representation. Formal: Formal: If you need to, you can use an attention-based function to handle the normalization. Formal: Formal: The masked token representation is normalized based on the output vector from the source context token's representation, which is a multi-layer perceptron. Formal: Formal: A common approach is to normalize the input embeddings of the target token by taking their dot product, so they end up being normalized. Formal: Formal: This normalization is done using a projection to the attention matrix, which is a multi-layer perceptron. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "In Section 1, we start by using Bleualign for unsupervised alignment. Instead of using length-based methods like Sennrich and Volk (2011) to create an initial training set, we train Monoses and use it to give Bleualign machine translations of the sentences we're trying to align. Monoses is trained by creating crosslingual word embeddings from monolingual corpora using word2vec and Vecmap (Artetxe et al., 2018a), which helps build a phrase table. We then train a Statistical Machine Translation (SMT) system with this data and use it to translate a monolingual corpus from one of the two languages. The translated data is then used to train an SMT system in the other direction. We build a new phrase table and repeat the process three times to get the final model.",
        "formal_text": "As stated in Section 1, we initially employ Bleualign for unsupervised alignment, but instead of bootstrapping an initial training set with lengthbased methods like Sennrich and Volk (2011), we train Monoses and use that to provide Bleualign with machine translations of the sentences being aligned. Monoses is trained by building crosslingual word embeddings from monolingual corpora using word2vec and Vecmap (Artetxe et al. , 2018a), inducing a phrase table. An SMT system is then trained on this data and used to translate the monolingual corpus in one of the two languages. The translated data is then used to train a standard SMT system in the opposite direction. A new phrase table is built and the process iterated three times for a final model.",
        "GPT2_formal_text": "= {e1, ..., en}. Formal: To make things clearer, we'll use two types of annotations instead of just one. Formal: Sure, let's break this down in simpler terms. Formal: A token is considered a \"node\" if it has a label (like a label) and can be connected to other nodes by a label too. Formal: The embedding function f(n) is the same as the one f(n) = f(n-1) for a single node (n). We'll talk more about how to set up the embedding function in the next sections. Formal: A token is considered a \"node\" if it has a label (like a label) and can be connected to other nodes by a label too. The embedding function f(n) is the same as the one f(n) = f(n-1) for a single node (n). The embedding function f(n-1) is like the embedding function f(n) = f(n-1) for a single node (n). We'll look at how to use these functions more clearly in Section 4. Formal: A token is considered a \"node\" if it has a label (like a label) and can be connected to other nodes by a label too. The embedding function f(n) is the same as the one f(n) = f(n-1) for a single node (n). Formal: A token is considered a \"node\" if it has a label (like a label) and can be connected to other nodes by a label too. Formal: A token is considered a \"node\" if it has a label (like a label) and can be connected to other nodes by a label too. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-srw.25.json"
    },
    {
        "casual_text": "The attention weights determine how much each hidden state contributes. In this case, the connection between the words \"sat\" and \"mat\" can be clearly picked up.",
        "formal_text": "The attention weights controls the individual contribution of each hidden states. In this example, the semantic dependency between the word \"sat\" and \"mat\" can be be explicitly captured.",
        "GPT2_formal_text": ", and the two hidden layers in the model are called h_int and h_int_t. Formal: For the positive examples, the text is broken down into words, and then each word is turned into a representation. Formal: The model works by processing the input sequence x and predicting a label y_t using the attention mechanism. Formal: The model picks a value from a set called φ_t for each token x_i. Formal: If the model has already predicted a label y_t for a token x_i, it sets the probability of y_t to φ_t(x_i) / (1 + φ_t(x_i)), which is calculated by adding up all the probabilities from the model. Formal: For the negative examples, the text is broken down into words, and the final representation for each token x_i is made up of the word representations from the two hidden layers in the model. Formal: Finally, the model uses the sum of all these representations to predict a label y_t for each token. Formal: The whole process for figuring out the label y_t for a token x_i is shown in Figure 1. Formal: After going through these steps, the model picks the label y_t for a token x_i based on the attention mechanism, and that label is written as y_t = {y_t_i | y_t_i + φ_t(x_i)}. Formal: In Figure 1, the model can pick up on important words and phrases, like \"I am\", \"this is\", and \"to be\". Formal: Formal: This paper is the first to actually look into how to represent feedback comments in a structured way. Formal: There are a few existing datasets for this, like the Feedback Comment Corpus, Feedback Comment Generation, and Feedback Text Generation datasets. Formal: Another important dataset is the Feedback Attachment Model, which was created by Lai et al. in 2015. Formal: Lastly, the Feedback Text Generation dataset has annotations for both text-based and image-based feedback comments. Formal: Lastly, the Feedback Comment Generation dataset has annotations for both text-based and image-based feedback comments. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C18",
        "filename": "C18-1124.json"
    },
    {
        "casual_text": "Check out Figure 3 for how the joint system is set up. The parser and semantic role labeler are linked in the embedding layer, using the same vector lookup tables for characters, words, and POS tags. Basically, the Bi-LSTM model from Section 2.1 is used for the SRL task, while the Stack-LSTM model from Section 2.2 handles the parsing task. Both the Bi-LSTM labeler and Stack-LSTM parser share the embedding layer. \n\nDuring training, we focus on minimizing the loss from both the semantic role labeler and the parser, which both affect the embedding layer. This helps create better vector representations for each token, making both tasks more accurate. However, since the neural structures are different, they don’t share any other parameters. \n\nThis joint model is a simpler version of shared training (like what Collobert et al. did in 2011), but it doesn’t use shared decoding (like Sutton and McCallum in 2005 or Zhang and Clark in 2008b). Instead, syntax and semantic roles are assigned separately, which avoids messing things up due to errors.",
        "formal_text": "The structure of the joint system is shown in Figure 3. Here the parser and semantic role labeler are coupled in the embedding layer, sharing the vector lookup tables for characters, words and POS. More specifically, the Bi-LSTM model of Section 2. 1 and the Stack-LSTM model of Section 2. 2 are used for the SRL task and the parsing task, respectively. The Bi-LSTM labeler and Stack-LSTM parser share the embedding layer. During training, we maximize the The loss from the semantic role labeler and the parser both propagate to the embedding layer, resulting in a better vector representation of each token, which benefits both tasks at the same time. On the other hand, due to different neural structures, there is no sharing of other parameters. The joint model offers the simplest version of shared training (Collobert et al. , 2011), but does not employ shared decoding (Sutton and McCallum, 2005; Zhang and Clark, 2008b). Syntax and semantic roles are assigned separately, avoiding error propagation.",
        "GPT2_formal_text": "= {v_t, t_i} | (v_t, t_i) ∩ {0, 1} | i = 1, 2, . . . , | | | | | | |  \n\nSo, basically, this is saying that the probability of a word (v_t) happening is equal to the probability of the whole sequence (t_i). Formal: Formal: Basically, for each pair of words (v_t, t_i) in the input, there are two possible meanings. Formal: Also, the probability is calculated based on the total length of the sequence. Formal: Here, t_i is the first word in the sequence (t_i = 1, 2, . . . , t_i = t_i−1), and v_t is the second word (v_t = t_i, t_i−1). The probability for a word (v_t) happening is just the probability of the whole sequence (t_i). Formal: We add a \"-\" to show that we're ignoring the first word if t_i is longer than v_t. Formal: Formal: In the second case, the probability is the whole sequence multiplied by the probability of the whole sequence, which is (v_t − t_i), plus the probability of the word \"YES\" (v_t = 1, 2, . . . , t_i = t_i−1). Formal: The final result for the input is (v_t − t_i) plus the probability of the first word \"YES\" (v_t = 1, 2, . . . , t_i = t_i−1). Formal: Formal: So, it's like saying, for each word (v_t, t_i), there's a chance of the whole sequence happening, plus how likely \"YES\" is (v_t = 1, 2, . . . , t_i = t_i−1). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1098.json"
    },
    {
        "casual_text": "The integration of existing morphological processing tools has resulted in a really cool CAI tool. This tool offers a dictionary lookup, shows examples from real texts, and displays morphological info—all online. Adding support for other languages would be pretty straightforward since the basic framework is already in place. Even though the prototype is still a work in progress, the early results are looking great. By February, it was advanced enough for some communication students to do a user study. We'll share the full details later, but the study showed that people are interested in using it. In the near future, we're planning to organize the text data by lexemes and also want to add features like a teaching or diagnosing module to make the tool even better and more like real CAL software.",
        "formal_text": "The intergration of existing morphologica. I proeessing tools has led to a powerful CAI, I, tool. The tool provides a dictionary lookup, it, gives examples fl'om corpora and displayes morl~hological information, all on-line. Other languages could be easily imlflemented in the overall skeleton of (; I, ()SS I'; IUI{uC. Although development of the l)rototype ({ I, ()SSI~; l{. -[{, u(l is still ongoing, these first results look very promising. The prototype -( ' w~ts sul~ticiently advanced in li'ehruary ror (, n mgen communications stndellts to conduct an in vestigatiw~ user study. Although we. 'll report on this seperately, it indicated user interest. In the uear fllture we're planning to index the corpora on basis of lexemes, l, al, er we wish to extend tim' software with for example a teaching; rod diagnosting module so that the tool matures to real (: ALl, software.",
        "GPT2_formal_text": ", attention_type, and attn_type. After that, we use a softmax function to figure out the meaning behind each token, which we then use to calculate the loss. Formal: In a sequential setup, the graph representation, f_s, for each token x_i in the input sequence x is determined by a bunch of features, which we call a_t. These features come from two things: the pooling layer's output for the previous token (x_1) and the output of the token we're looking at right now (x_i). Formal: The term L_w is the weight matrix for the attention layer. Formal: The loss for the whole sequence is calculated by finding the overall loss for all the tokens. Formal: Finally, the final loss is calculated by summing up the losses for all the tokens. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C96",
        "filename": "C96-2140.json"
    },
    {
        "casual_text": "Figuring out what things in a text are referring to and connecting them to the right entries helps us make sense of documents and search queries. A lot of research uses knowledge bases like Freebase (Chiu et al., 2014), YAGO (Yosef et al., 2011), and Dbpedia (Olieman et al., 2014). Wikify (Mihalcea and Csomai, 2007) was one of the first projects to link text to Wikipedia pages. It looks at all possible n-grams that match Wikipedia concepts, like links and titles, and considers them as potential candidates. They use a voting system that combines knowledge-based and data-driven methods to figure out the best match. Cucerzan (2007) uses four types of resources to find candidates: entity pages, redirect pages, disambiguation pages, and list pages. Then, they compare the context of the text with the information on Wikipedia pages, including category tags, to pick the right candidate. They also look at all the n-grams in the document and keep the ones that have a high enough probability. To narrow down the candidates, they use the structure of Wikipedia links, focusing on how common and related the terms are.",
        "formal_text": "Recognizing entity mentions in text and linking them to the corresponding entries helps to understand documents and queries. Most work uses the knowledge base including Freebase (Chiu et al. , 2014), YAGO (Yosef et al. , 2011) and Dbpedia (Olieman et al. , 2014). Wikify (Mihalcea and Csomai, 2007) is the very early work on linking anchor texts to Wikipedia pages. It extracts all ngrams that match Wikipedia concepts such as anchors and titles as candidates. They implement a voting scheme based on the knowledge-based and data-driven method to disambiguate candidates. Cucerzan (2007) uses four recourses to generate candidates, namely entity pages, redirecting pages, disambiguation pages, and list pages. Then they disambiguate candidates by calculating the similarity between the contextual information and the document as well as category tags on Wikipedia pages. generate candidates by gathering all n-grams in the document, and retaining those whose probability exceeds a low threshold. Then they define commonness and relatedness on the hyper-link structure of Wikipedia to disambiguate candidates.",
        "GPT2_formal_text": "= {h1, h2, ..., hm, hn, ...}  \n\nThis layer basically turns the sentence into a bunch of numbers and keeps track of how well the input matches the answer. Then, it calculates something called the classification accuracy, which is the chance of a correct classification happening. Formal: The idea behind this is to figure out if the answer in the input is a positive example or a negative one. Formal: We need to compare the input with the answer to see how similar they are. Formal: The model then picks the best match from all possible options. Formal: This method has two main parts: Formal: We need to compare the input with the answer to see how similar they are. Formal: The model then picks the best match from all possible options. Formal: The tricky part is that this method works on a linear scale, which makes it tricky to handle the issue of better and worse matches. Formal: A simpler approach is to calculate the predicted probabilities on the target side. Formal: This method has two parts: Formal: We need to compare the input with the answer to see how similar they are. Formal: The model then picks the best match from all possible options. Formal: The tricky part is that this method works on a linear scale, which makes it tricky to handle the issue of better and worse matches. Formal: A simpler approach is to calculate the predicted probabilities on the target side. Formal: The model also picks the best match from all possible options. Formal: Formal: To make things clearer, we’ll use a more general version of Equation (1) that’s more intuitive: Formal: We’ll use the output from Equation (1) to predict the probability of a positive answer. Formal: This gives us the probability that a positive answer is correct. Formal: Formal: This probability is calculated based on the model’s prediction for the probability that a positive answer is correct. Formal: Formal: Finally, we’ll use this probability to predict the probability that a negative answer is correct. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1007.json"
    },
    {
        "casual_text": "Pretty much anything that can be thought of or talked about individually can be a discourse object. So, stuff like objects and facts stored in a database, what the user types in, the commands they give, and the system's responses—all of these are potential discourse objects. Terms like \"discourse elements\" (Sidner, 1984) and \"discourse entities\" (Webber, 1984) are used to describe the things that are \"specified\" or brought up in a conversation. These things and how they relate to each other make up the discourse model of the person speaking. Hayes (1984) calls the objects, events, commands, states (and so on) that a system needs to understand \"entities.\" Following that idea, I think of a discourse object as something very broad—basically, the whole universe of discourse is just a bunch of these objects. Even a relationship between discourse objects counts as a discourse object. Sometimes, that relationship might also be part of the description of one or more of the objects involved.",
        "formal_text": "Virtually anything that can be perceived as and talked about as an individual may serve as a discourse object. Thus, objects and facts represented in a database as well as the user's inputs, the commands to be executed and the responses of the system are all (potential) discourse objects. Notions such as discourse elements (Sidner, 1984) and discourse entities (Webber, 1984) have been employed to denote the entities that are =specified\" or evoked by the constituents of a discourse, they and their relations then constituting the discourse model of a speaker. Hayes (1984) refers to the objects, events, commands, states (and so on) that an interface system needs to recognize collectively as \"entitities ~. In the same vein I ta~e the notion of a discourse object to apply in the most general sense; the universe of discourse is in principle just a collection of discourse objects. A relation between discourse objects is also a discourse object although it may also, or alternatively, be attributed to one or more of its constituents as part of their descriptions.",
        "GPT2_formal_text": "ed = attention_masked;\n\nstruct attention_masked {\n\nint x_masked;\n\nstruct input {\n\nint a_masked;\n\nint s_masked;\n\n} = {\n\nx_masked = attention_masked;\n\ns_masked = s_masked;\n\n};\n\n}\n\nOutput:\n\n- A(A(B(C(D(E(F(G(H(i)))))\n\n---\n\nFigure 1: The attention-based QA model. It uses a CNN with attention to focus on the key parts of a question. Formal: A = (A1, A2, ..., A), S = (S1, S2, ..., S), T = (T1, T2, ..., T), V = (V1, V2, ..., V), D = (D1, D2, ..., D),\n\nFor example, to figure out if \"Mary\" is the subject or object of a question, the model might use: Formal: S = (S1, S2, ..., S), T = (T1, T2, ..., T),\n\nor \"Mary is the subject of the question.\" Formal: T = (T1, T2, ..., T),\n\nor \"Mary is the object of the question.\" Formal: V = (V1, V2, ..., V). Formal: D = (D1, D2, ..., D),\n\nor \"Mary is the subject of the question.\" Formal: V = (V1, V2, ..., V). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E87",
        "filename": "E87-1024.json"
    },
    {
        "casual_text": "Basically, when k = 100 and p = 0.9, we used a loss truncation method with c = 0.6 for summarization. This gave us entropies of 18.08, 20.01, and 17.93. On the other hand, with k = 2 and p = 0.4, we used rejection sampling for summarization at c = 0.6 and α = 0.1, which resulted in entropies of 3.71, 4.02, and 3.84.",
        "formal_text": "Specifically, k = 100 and p = 0. 9 matched loss truncation at c = 0. 6 for summarization (entropies of 18. 08, 20. 01, and 17. 93 respectively). k = 2 and p = 0. 4 matched rejection sampling for summarization at c = 0. 6, α = 0. 1 (entropies of 3. 71, 4. 02, and 3. 84 respectively).",
        "GPT2_formal_text": "\" to create a copy of the query. Formal: Alright, let's break this down in simpler terms. \n\n- The user's query is made up of a bunch of words, like w_i, w_j, and so on, up to w_k.\n- There's also a group of related documents called D_j, which includes D_i, D_j_1, and so on, up to D_j_l. Formal: For each word in the query, we figure out how important it is to show up in this group. We do this by looking at the words around it (like in Figure 1) and doing some math. We use a logistic regression model to make this determination. Formal: Basically, we're checking how well the word fits into its group, based on its position in the document. Formal: The word w_i is considered \"important\" if it can make its way into that group and get promoted. Formal: Also, the phrase w_j is considered \"important\" if it can also make its way into that group and get promoted. Formal: Lastly, the document D_j is considered important if it can create a clear version of the word w_i and the phrase w_j. Formal: Formal: And finally, the final word w_l is important if it can also get promoted. Formal: Formal: Formal: Formal:\n\nSo, in short, we're trying to find the most important word in the query that can push its way into the important group. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.66.json"
    },
    {
        "casual_text": "Okay, so let’s break this down in simpler terms. \n\nFirst, we’re using something called a Siamese dual-encoder (Lowe et al., 2017) to encode both the context and the responses. \n\nNow, let’s talk about the decoder. The decoder starts with the context encoding and generates a response one token at a time. Each new token depends on the ones that came before it. The decoder is trained to make sure it’s generating the right words by minimizing something called log-perplexity for each word in the correct response.\n\nBut here’s the issue: when we train the decoder on real conversation logs, it often ends up giving generic or totally irrelevant responses. To fix this, we add a classifier to the decoder. At each step of decoding, this classifier checks if the response so far is relevant to the conversation context. The classifier is trained to say \"yes\" (relevance of 1) if the response matches the correct answer and \"no\" (relevance of 0) if it’s just some random response. \n\nAt the same time, the decoder is still working on minimizing the word loss, which is the log-perplexity for each word in the correct response. \n\nSo, for any response r, the relevance loss can be written like this:",
        "formal_text": "(3) Using the encoded representations in a Siamese dual-encoder (Lowe et al. , 2017) that also encodes the responses. Vanilla Decoder: The decoder is initialized using the context encoding. The decoder generates the response autoregressively, that is, the token at each time-step is generated conditioned on the previous tokens of the response. The decoder is trained to minimize the log-perplexity of each word in the gold response. Decoder with sub-sequence validation: When trained on actual conversation logs, vanilla decoders often resort to generic responses or responses that are irrelevant to the context. Hence, to enforce relevance, we enhance the decoder with a classifier for each time-step of decoding. At each time-step, the classifier predicts the relevance of the response so-far for the given conversational context. The classifier is trained to predict a relevance of 1 for a prefix of the gold response and 0 for a prefix of any other randomly sampled response at each time-step of decoding. Simultaneously, the decoder is also trained to minimize the word loss, that is, log-perplexity of each word in the gold response. For any response r, the relevance loss can be written as follows:",
        "GPT2_formal_text": "(h_i) = (1 - s_a) * h_i. To make sure we don't end up with totally unrelated answers, we use something called Dirichlet priors to adjust the attention matrix. To get the final output y_t, we use a basic LSTM with a hidden layer size of 512. Formal: The method we're talking about in this paper is pretty much like the one Dyer et al. (2017) used for extractive summarization. They used a multi-layer perceptron (MLP) with a linear layer at the top and a linear dropout at the bottom. The key difference is that ours focuses on generating the actual summary instead of just copying the input. Formal: We've come up with a new approach to extractive summarization that uses an attention-based structured perceptron to create the final summary. Formal: We're not sharing the code for the model yet, but you can check it out at https://github.com/sao-jun/mlt. Formal: We built a basic MLP using the Java code mentioned in the paper. The model has a hidden layer size of 512 and a batch size of 32. Formal: We tested our method on the TAC2014 dataset, and the results are in Table 2. For a more detailed look, we also tested it on the TAC2015 dataset. Formal: We ran a bunch of experiments with different values of s, and the results are in Table 3. Table 3 shows how we performed on the TAC2014 dataset. Formal: To help with accuracy and diversity, we created two datasets, TAC2014-TRAIN and TAC2014-DISEASE, for our experiments. Formal: The datasets have a total of 7,894 examples in TAC2014-TRAIN and 9,019 in TAC2014-DISEASE. Formal: We also tried using adversarial training with the datasets to boost accuracy. Formal: The results from our adversarial training are in Table 4. Formal: The results from our adversarial training are in Table 5. Formal: We also used the Levenshtein distance to check how similar the input and output were. Formal: We found that the loss terms from the adversarial training help with fluency and diversity. Formal: For the best results, we tried running our experiments on the GENIA dataset. Formal: We ran our experiments on",
        "directory": "emnlp",
        "filename": "2020.emnlp-demos.20.json"
    },
    {
        "casual_text": "To make it easier to explain and understand how complicated the algorithm is, we need to introduce a few more terms related to the hierarchy. Besides the sequence length L, the number of hierarchy levels M, and the embedding or feature size d from Eq. (1), we’re adding these new terms: \n\n1) N r: the numerical rank of the off-diagonal blocks (for example, it’s 2 in Eq. (20)). This also happens to be the size of the diagonal blocks at the lowest level (level-0); \n2)",
        "formal_text": "To facilitate the description and the complexity analysis of the algorithm, we define a few more hierarchy-related notations. In addition to sequence length L, number of hierarchy levels M and embedding or feature size d in Eq. (1), the new notations include: 1) N r: numerical rank of the off-diagonal blocks (for instance, 2 in Eq. (20)). This is also the diagonal block size at level-0; 2) N",
        "GPT2_formal_text": "= []) and then expand the similarity matrix to make sure the matching vectors are as close as possible. Formal: We're using an RNNG model with n hidden units, where each hidden state has m = 1 k-1 units. This model is trained to find the best match for the query, using the classifier's predictions for the previous sentence as guidance. Formal: We're using a cross-entropy loss function to figure out the losses for the pairs of sentences, as explained in Section 2. Formal: You can find more details in the paper. Formal: We're using the top-k nearest neighbor classifier from (Sennrich et al., 2016) as our feature extractor, which helps us retrieve matching entity pairs. Formal: To make sure the results are reliable, we test our model on two different datasets: one is the CNBL dataset (Wu and Mooney, 2003), and the other is the MSRA dataset (Krishna and Roth, 2000). Formal: We train our model using 10 rounds of 100 training steps with a learning rate of 1e-5, setting α to 0.3. Formal: To check how well it's doing, we take the average of the accuracy (Acc) and the precision (Pre). Formal: We tested our model on the MSRA dataset, specifically the test set (MT), and the results are in Table 3. Formal: We looked at the precision (Pre) and the accuracy (Acc) for the MRPC and the JRC-Acquis datasets, respectively. Formal: To measure how well our model is performing, we used the BLEU score (Papineni et al., 2002) and the NIST14 MUC-6 metric (McCallum et al., 1999). Formal: To get the BLEU score (Papineni et al., 2002), we divide the BLEU score for each training example by the total number of examples in the dataset. Formal: We also calculated the precision (Pre) and the recall (Rec) for the MSRA dataset, which you can find in Table 3. Formal: The results for the JRC-Acquis dataset are in Table 4. Formal: We report the top-k closest neighbors with a learning rate of 1e-5, the NIST14 MUC-6 metric, and the BLEU score (Papineni et al., 2002",
        "directory": "acl",
        "filename": "2021.acl-long.294.json"
    },
    {
        "casual_text": "In Section 3, we talked about how CDRNN uses distributional regression, which means it has an IRF that shows how predictors affect the spread of the prediction over time. You can visualize the IRF for the variance just like you would for the mean.",
        "formal_text": "As discussed in Section 3, CDRNN implements distributional regression and thus also contains an IRF describing the influence of predictors on the variance of the predictive distribution as a function of time. IRFs of the variance can be visualized identically to IRFs of the mean.",
        "GPT2_formal_text": ", which we'll call m_m in this paper. Formal: Basically, x_i, x_j, and x_k represent the i-th, j-th, and k-th elements, respectively. Formal: The Jaccard similarity measure, which we'll call m_j, is calculated by multiplying the cross-entropy between x_i and the input y_i. Formal: The dimension of the discriminator's grid D_k is set to D_i. Formal: The decoder's output vector, which we'll call h_d, is made up of vectors h_i and h_j. Formal: To see how the model behaves in different scenarios, we use the 'Random' setting and train it on the training set for different datasets. Formal: We've outlined all the models in Figure 3. The 'Random' setting gives the model the same training data it gets in the regular training setup, but it doesn't use the latent representations learned from the token embeddings. Formal: For the 'Random' setting, the model gets the same training data as the regular Transformer model, but it doesn't use the latent representations learned from the token embeddings. Formal: We've also included a simple evaluation called 'Copy Embeddings' to compare the Transformer model's output with the latent representation. Formal: These datasets we picked are the only ones where we've seen a clear difference between the 'Random' and 'Copy Embeddings' evaluations. Formal: Figure 3: The encoder-decoder setup. The input text x_i is broken down into tokens using a bidirectional LSTM network. Then, the decoder's output is built using a CNN to create a token representation h_d. Formal: The encoder-decoder setup is shown in Figure 4. The input text x_i is broken down into tokens using a bidirectional LSTM network. Formal: The decoder's output is created using a CNN to generate a token representation h_d. Formal: We've included two versions of the model for comparison. Formal: The 'Random' setting gives the model the same training data as the regular Transformer model, but it doesn't use the latent representations learned from the token embeddings. Formal: We've also included two versions of the model for comparison. Formal: The 'Copy Embeddings' evaluation",
        "directory": "acl",
        "filename": "2021.acl-long.288.json"
    },
    {
        "casual_text": "In this paper, we introduced a new word-based labelling method for Thai sentence boundary detection (SBD) for the first time. We used a Linear Chain Conditional Random Field (LCRF) for sequence labelling, which worked really well compared to previous results on the ORCHID dataset. We also looked at how POS tagging could help with SBD using the TaLAPi corpus. We compared cascade models and joint models, and came up with a \"2-step\" joint model that combines POS tagging and SBD. This new model was more than twice as fast as the \"1-step\" joint model, but still got almost the same accuracy for SBD when using the same features. With the extra speed, we could add more features to improve SBD while keeping POS tagging performance on par.",
        "formal_text": "In this paper, we demonstrated for the first time a word-based labelling approach to Thai SBD. The word-based labelling approach was proposed to leverage LCRF to do sequence labelling which achieved very good performance compared to reported results on ORCHID data. Furthermore, the performance of SBD with the help of POS tagging was investigated on the corpus TaLAPi. Cascade models and Joint models were compared and the \"2-step\" Joint POS tagging with SB detection was proposed. This proposed model saved more than half of the time, while obtaining almost the same accuracy for SBD as \"1-step\" Joint model, when using the same feature set. With increased speed, more features were therefore used to improve SBD and yields comparable POS tagging performance.",
        "GPT2_formal_text": ", p_l = {}, p_l2 = {}, s_l = {}, and s_l2 = {}, all being part of the set {1, . . . , |C|}. The output of a model can be represented using a special matrix called W. Formal: Alright, let's break this down in a simpler way. We're dealing with two documents, let's call them D1 and D2. D1 is an older document and has a bunch of n-grams, while D2 is newer and has way more n-grams. We're trying to match up the document entities in D1 and D2 with the ones in D2. Formal: To do this, we calculate the dot product between D1 and D2 using something called the Frobenius norm. Formal: For each pair of entities, we look at both the top n-grams from D1 and the top n-grams from D2, sum them up, and then use a standard linear transformation to get their representations. Formal: Finally, we pass that information through a fully connected layer to get the final representation for the document entity pair, like this: Formal: To calculate the cosine similarity between two entities, we calculate the cosine similarity between their representation vectors using a nonlinear transformation. Formal: To figure out the document similarity score between D1 and D2, we use a convolutional neural network (CNN) (developed by Hochreiter and Schmidhuber in 1997) with a window size of 50. Formal: To find the max value of the dot product for document similarity, we use a simple linear function. Formal: Lastly, to update the document similarity score, we use the same CNN as before. Formal: To get the full representation for document entity pairs, we use a fully connected layer with a window size of 50. Formal: Oh, and by the way, in Appendix A.2, there's a table with all the n-gram embeddings for each entity pair. Formal: In the next part, we'll give you the formula for calculating the dot product between D1 and D2 to get their representation vectors. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1031.json"
    },
    {
        "casual_text": "So, on the brain cancer test set, the F1 score was 566, which is way lower than what the MTL Model got. Here's a breakdown of the flops/inst and total instances for different models:\n\n- **MTL Model**: 218,767,889 flops/inst, 20k instances, Ratio OP 1\n- **OP+MTL**: 218,783,260 flops/inst, 20k instances, Ratio OP 1\n- **Multi-pass**: 218,724,880 flops/inst, 427k instances, Ratio OP 23\n- **Multi-pass+Silver**: 218,724,880 flops/inst, 497k instances, Ratio OP 25\n\nTable 4 shows the computational complexity in terms of flops per instance and the total number of instances.",
        "formal_text": "566 F1 on the brain cancer test set, significantly lower than the MTL Model flops/inst inst # Ratio OP 218, 767, 889 20k 1 OP+MTL 218, 783, 260 20k 1 Multi-pass 218, 724, 880 427k 23 Multi-pass+Silver 218, 724, 880 497k 25 Table 4: Computational complexity in flops per instance (flops/inst)×total number of instances (inst#).",
        "GPT2_formal_text": ", dim(t_i) = k, q_i = n. The function f_sim(t, a) is calculated by multiplying the softmax of (t_i, a) with (a_i, t). This helps the model learn to predict the next word in the sequence. Formal: When the encoder is trying to decide whether to generate the next token, it uses the attention matrix φ_c. This helps it learn to generate different words in the next token's context. Formal: You can calculate f_sim(t, a) by using the joint probability distribution P(t, a) for the current token (t_i) and the context tokens (a_i). Formal: The score h_sim(t, a) is calculated based on this joint probability distribution P(t, a) and the current token's state (t_i). Formal: We put the encoder, decoder, and the student model together to get the final model. Formal: So, P_c(a_i) is just the shared probability distribution for all the context tokens, and the probability P(t_i) is how well the model predicts the next word. Formal: We also add a parameter α, which is the log probability that the current token is the correct answer, given the context and the current word. Formal: Finally, f_sim(t, a) is the probability of the correct answer when the current word is the correct answer. Formal: The model we train using the joint probability distribution P(t, a) learns to generate different tokens in the next token's context. Formal: Formal: Here’s what we’ve done: Formal: We’ve created a new dataset called the Indo-Este-Uni-Biligature (I²U). This dataset includes examples of dialogues where the next word is unclear. Formal: We’ve created a new dataset called the Indo-Este-Uni-Biligature (I²U). This dataset includes examples of dialogues where the next word is unclear. Formal: We’ve included two new datasets, T_MUSIC and T_SOCIAL, and two new decoders, T_DPC and T_DPC_DPC. Formal: To train the student model, we use the joint probability distribution P(",
        "directory": "bionlp",
        "filename": "2020.bionlp-1.7.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\nWe have a set of sentence pairs: C = {(S₁, T₁), ..., (S_N, T_N)}. We also have a maximum number of iterations we want to do, which is M, and a depth value δ for something called \"seg rev.\" The goal is to get a word alignment A = {A₁, ..., A_N}.\n\nHere's how we do it step by step:\n\n1. Start with an empty alignment, so A is empty.\n2. Loop through the iterations from 1 to M:\n   - Set I and C to empty.\n   - If A is still empty:\n     - Go through each sentence pair from 1 to N.\n     - Use \"seg rev\" with depth δ on the pair (S_n, T_n, A_n) to get RS and RI.\n     - Add (RS, T_n) to the set C.",
        "formal_text": "C = {(S 1, T 1 ) • • •, (S N, T N )}; maximum iteration M; depth δ for seg rev; output: word alignment A = {A 1, • • •, A N }; 1 A ← ∅; 2 for iter ← 1 to M do 3 I ← ∅; C ← ∅; 4 if A = ∅ then 5 for n ← 1 to N do 6 RS, RI ← seg rev δ (S n, T n, A n ); 7 append (RS, T n ) to C;",
        "GPT2_formal_text": "= [t_i, t_j]  \n\nTokenized:  \nInput:  \n1. **A**  \n2. **B**  \n3. **C**  \n4. **D**  \n5. **E**  \n6. **F**  \n7. **G**  \n8. **H**  \n9. **I**  \n10. **J**  \n11. **K**  \n12. **L**  \n13. **M**  \n14. **N**  \n15. **O**  \n16. **P**  \n17. **Q**  \n18. **R**  \n19. **S**  \n20. **T**  \n21. **U**  \n22. **V**  \n23. **W**  \n\nThe plan is to train the model using a mix of data from both the source and target domains. The final setup will include both the input and output domains combined in one model. Formal: Once we have the domain distribution, we use a supervised clustering method to group similar text pieces together into clusters. Formal: After doing this clustering, we can use the conditional probability to figure out which clusters are likely to be the most similar in terms of domain. Formal: The final hidden state of the model is basically the probability of the next cluster, given the current input and the previous input. Formal: We also calculate a weighted coefficient for each cluster. Formal: We combine the weights from the first and second coefficients to get the final distribution. Formal: Finally, we apply the cross-entropy loss to see how well we're doing. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D15",
        "filename": "D15-1119.json"
    },
    {
        "casual_text": "Alright, so P(f) doesn't depend on the domain D. Also, we're just assuming that P(D) stays the same, like a constant. The formula we're looking at here is all about finding the best domain. It's like trying to figure out the argmax.",
        "formal_text": "Here, P (f ) is independent of domain D. Furthermore, we assume P (D) to be constant. The following formula embodies the search for the optimal domain. argmax",
        "GPT2_formal_text": "_2d = [u, v] = (u * [i, j]) with s = 2d. Formal: The attention network takes in a vector, and then it uses something called the gradient descent algorithm to figure out the overall probability distribution P(W_i | b_i). Formal: The attention weights for the tokens in W_i are defined as: Formal: The attention weights for the tokens in the query are: Formal: To figure out the probability of the query token, we use a cross-entropy loss function, which we'll explain in Section 3. Formal: To make sure the attention weights are the same for all tokens in the query, we use a linear mapping. Formal: Lastly, to make sure the query and response are related, we use a linear-chain CRF (Hochreiter and Schmidhuber, 1997). Formal: In this setup, the token's representation is passed through a multi-layer perceptron (MLP) with a softmax layer. Formal: The softmax layer gives us a probability distribution P(W_i | b_i | s), where s = 2d. Formal: Each token is represented by its embedding in the embedding space. Formal: The final attention weights, W_i and b_i, are then adjusted to minimize the cross-entropy loss, which we'll explain in Section 3. Formal: Let's call the final value of the attention weights W_i. Formal: Formal: Formal: Let's define the softmax layer for the attention weights. Formal: Here, p_i is the probability of the token being generated, and u is the attention weight for token i. Formal: We start by setting the token embedding embedding dimensions as d_i = d_i + d_i, and then we figure out the attention weights based on the token embedding dimension. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D07",
        "filename": "D07-1054.json"
    },
    {
        "casual_text": "The kappa scores for the petfoodrecall and vtech datasets were pretty consistent among the raters, showing good agreement. However, the spitzer dataset didn't do as well—there was only fair agreement. As for the iPhone dataset, the evaluators had a lot of disagreement on what they thought were subtopics.",
        "formal_text": "The kappa scores for the petfoodrecall and vtech datasets showed good agreement among the raters, while the spitzer data set had only fair agreement. For the iPhone data set, both evaluators had a large amount of disagreement on what they considered subtopics.",
        "GPT2_formal_text": "= [ ] for i in range ( 0, n) from i=1 to n, d_i = d_i + 1, while f_s = [ ] for i in range ( 0, n) from i=1 to n. Formal: For each sentence s in the summary, we figure out the log-likelihood P(s|x_i, y_i). Formal: After training, we look at how well the model performs on the development set by calculating the average F1 score (using the validation set) and the F1 score for a specific model (based on the model itself) using the validation set. Formal: For each sentence s in the summary, we calculate the log-likelihood P(s|x_i, y_i). Formal: For each model p_i in the model set, we calculate the log-likelihood P(p_i|s). Formal: After training, we check how the model performs on the development set by calculating the average F1 score (using the validation set) and the F1 score for a specific model (based on the model itself) using the validation set. Formal: For each model p_i in the model set, we calculate the log-likelihood P(p_i|s). Formal: After training, we check how the model performs on the development set by calculating the average F1 score (using the validation set) and the F1 score for a specific model (based on the model itself) using the validation set. Formal: After training, we check how the model performs on the development set by calculating the average F1 score (using the validation set) and the F1 score for a specific model (based on the model itself) using the validation set. Formal: Formal: Formal: Finally, we combine all the individual F1 scores from the different models to get the final score for each model. Formal: Formal: Formal: We also use the model p_i to predict the target labels for each target sentence s. Formal: Formal: Formal: Formal: The probability p(s|x_i, y_i) is calculated based on these predictions. Formal: Formal: Formal: Formal: Formal: The probability p(s|x_i, y_i) is calculated based on these predictions. Formal: Formal: Formal: Form",
        "directory": "C08",
        "filename": "C08-1077.json"
    },
    {
        "casual_text": "We tackled this optimization problem using a branch-and-bound algorithm, which was first introduced by Land and Doig back in 1960. This graph alignment problem is generally considered NP-hard, as Klau pointed out in 2009, and is typically solved in an approximate way, kind of like how beam search works. But, the unique structure of constraints 1 to 3, which come from the requirement that the alignment function must be injective (one-to-one), lets us solve this optimization exactly.\n\nNow, our version of the branch-and-bound algorithm doesn’t usually run in polynomial time, but in practice, we found it works pretty well. We were able to compute optimal alignments efficiently in almost all cases—only less than 0.1% of the alignment pairs in our data took too long to solve. This good performance is mainly because we don’t have to deal with aligning entire graphs, and the number of nodes in the subgraphs we’re aligning is limited.",
        "formal_text": "We solve this optimization problem with a version of the branch-and-bound algorithm (Land and Doig, 1960). In general, this graph alignment problem is NP-hard (Klau, 2009) and usually solved approximately following a procedure similar to beam search. However, the special structure of constraints 1 to 3, originating from the required injectivity of the alignment function, allows us to solve the optimization exactly. Our implementation of the branch-and-bound algorithm does not generally run in polynomial time, however, we found that in practice we could efficiently compute optimal alignments in almost all cases (less than 0. 1% of alignment pairs in our data could not be solved in reasonable time). This relatively benign behavior depends crucially on the fact that we do not have to consider alignments between full graphs, and the number of nodes in the aligned subgraphs is limited.",
        "GPT2_formal_text": "ing, it’s a bit different from the regular masking operation. First, you’d need to set up a linear transformation matrix that connects the word vectors from the target text to the word vectors in the source text. After that, you’d use a linear transformation to turn the attention vector from the source text into the target text. Formal: For the ECOND-HM part, we use a matrix A that’s in R^n×d to represent the input sequence of tokens. The idea here is that the model can keep updating its learned representations by learning to predict the next token in the sequence. Formal: We’ve got two training sets: one for the target language and one for the source language. Our experiments show that this setup works really well, especially when dealing with tasks like text classification. Formal: Since the source and target languages are pretty different, we wanted to see if a model trained on the source language and the target language could still learn useful representations from the source language. We tried two methods: one where we used the target language as the ground data (let’s call this the \"source language-only\" setup), and another where we used the source language as the ground (let’s call this the \"target-only\" setup). The results? The model that used the source language as the ground data did way better than the one that used the target language as the ground data. Formal: Our experiments also suggest that a model trained on the target language as the ground data could improve its performance even more if it learned representations from the source language as well. Formal: To see how well the source-language-only model would perform if it had access to all the source-language training data, we tested it on both the source and target languages. Formal: We trained the model using a batch size of 32, and the results are shown in Figure 1. Formal: We ran this test with a word error rate of 3% and a vocabulary size of 300k. Formal: The source-language-only model performed better than the target-language-only one in terms of BLEU scores. This was true whether we used the source-language as the ground or the target-language as the ground. Formal: The target-language-only model also improved, though not by as much compared to the source-language-only one. Formal: We ran our experiments with a word",
        "directory": "D09",
        "filename": "D09-1002.json"
    },
    {
        "casual_text": "Back in 1991, Elman pointed out that messing around with artificial systems can help us cut through the noise of real-world language data. But to make sure our model actually learns recursive patterns and not just simple, limited ones, we need to test it on structures that are more deeply nested than what it was trained on. That’s exactly what we did. Generalised Dyck languages are perfect for this kind of testing (Bernardy, 2018). \n\nNow, LSTMs (and GRUs) can handle some deeper nesting, but their performance drops as the nesting gets more complex, just like how they struggle with natural language agreement stuff. Other studies have shown similar results (Hewitt et al., 2020; Sennhauser and Berwick, 2018). The same thing happens with generative self-attention models (Yu et al., 2019), while BERT-like models, which aren’t generative, just can’t handle this task at all (Bernardy et al., 2021).",
        "formal_text": "Dyck Languages Elman (1991) already ob-served that it is useful to experiment with artificial systems to filter out the noise of real world natural language data. However, to ensure that the model actually learns recursive patterns instead of bounded-level ones, it is necessary to test on more deeply nested structures than the ones that the model is trained on, as we did. Generalised Dyck languages are ideal for this purpose (Bernardy, 2018). While LSTMs (and GRUs) exhibit a certain capacity to generalise to deeper nesting their performance declines in proportion to the depth of the nesting, as is the case with their handling of natural language agreement data. Other experimental work has also illustrated this effect (Hewitt et al. , 2020; Sennhauser and Berwick, 2018). Similar conclusions are observed for generative self-attention architectures (Yu et al. , 2019), while BERT-like, non-generative self-attention architectures simply fail at this task (Bernardy et al. , 2021).",
        "GPT2_formal_text": ")) { [CLS] = e1, e2, ..., ek } end for i = 1, ..., n-1; I_n = 1; while looping { [CLS] = i; [SEP] = e1, e2, ..., ek } end for i = 1, ..., n-1; I_n = 1;\n\nBasically, it's a looping process where you match each token with a label (e1, e2, ..., ek) to form a set of predicates. You do this for each token in the sequence. Finally, you get the output that includes both the predicates you matched and the actual value. Formal: We use both sequence-level and state-level features to map each token to a separate cluster. Formal: We create a state for each token by combining information from the two sequence levels. Formal: The state is created by combining local and global features, kind of like how you combine the local and global features in a sentence. Formal: The global features are used to add state info to the cluster at the sentence level, and the local features are used to change the state from the cluster at the token level. Formal: The global features are like the predicates in the predicate space, while the local features are like the predicates in the value space. Formal: So, we end up with a global state for each token, with the state being the combination of local and global features. Formal: The final state is used to update the state at the token level and also to update the state at the state level. Formal: The global features help make the state at the token level more meaningful. Formal: Each state is represented by a vector, which is created by combining a bunch of local and global features. Formal: At the state level, we add information from the two sequence levels. Formal: The global features are like predicates in the predicate space, while the local features are like predicates in the value space. Formal: Formal: The final state is a combination of global and local features. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "cmcl",
        "filename": "2022.cmcl-1.2.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nThis is an interactive argument pair. For instance, in Figure 1, the review argument spans are set X, which is {â v 1, â v 2} = {(3, 5), (6, 9)}, and the rebuttal argument spans are set Ŷ.",
        "formal_text": "is an interactive argument pair. For example, in Figure 1, the review argument spans set X is {â v 1, â v 2 } = {(3, 5), (6, 9)} and the rebuttal argument spans setŶ is",
        "GPT2_formal_text": ". And for the final layer, we swap out the max layer with a softmax function. Formal: For the training data, we use the generated text as the label for the test data. We set the word embeddings to 64 and the hidden dimensions to 200. To get the dataset ready for training, we set the dropout rate at 0.1. \n\nFor testing, we swap out the word embeddings for GloVe vectors. For the word embeddings, we use the 7-gram corpus. For the hidden dimension, we use 200. The learning rate for the activation function is 0.001, and for the linear layer, we use 0.00005. We train our model with a batch size of 32 and a learning rate decay of 0.0001. For the back-propagation, we use a momentum of 0.1, and we set the dropout rate to 0.1. Formal: We trained the model using a learning rate of 0.001, and the batch size was 32. We played around with different dropout rates and dropout rates, ranging from 0.001 to 0.99, until the perplexity on the validation set stabilized. Formal: After training, we tested it on the test set to see how well it could distinguish between entities with similar contexts. Formal: The model was trained with a learning rate of 0.001. We tested the model on the validation set using a batch size of 32. The perplexity on the validation set stabilized after training for 2 epochs. Formal: We trained the model with a learning rate of 0.01, and the batch size was 32. We tested the model on the test set using a batch size of 32. Formal: The model was trained with a learning rate of 0.1. Formal: Formal: Formal: We trained the model with a learning rate of 0.001. We trained the model for 20 epochs, and the perplexity on the validation set stabilized after 20 epochs. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.319.json"
    },
    {
        "casual_text": "Creating hand-crafted features is important, but it’s also super time-consuming. It’d be great if we could just automatically learn useful features from text. For figuring out how readable a whole document is, a good approach is to combine the representations of the words in it (Kim, 2014). When it comes to representing words, a cool trick is to turn them into dense, low-dimensional vectors, which is called word embedding. There are already some word embedding models out there (like the ones by Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) that could help with readability assessment, but they have a downside: they mainly focus on how words relate to each other in terms of syntax or meaning, and they don’t really consider how easy or hard those words are to read. So, words like \"man\" and \"gentleman\" might end up close in the vector space because they’re similar in function or topic, even though they’re not equally easy to understand. This means we need to find a way to include reading difficulty when training these word embeddings.",
        "formal_text": "Designing hand-crafted features are essential but labor-intensive. It is desirable to learn representative features from the texts automatically. For document-level readability assessment, an effective feature learning method is to construct the representation of documents by combining the representation of the words contained (Kim, 2014). For the representation of word, a useful technique is to learn the word representation as a dense and low-dimensional vector, which is called word embedding. Existing word embedding models (Collobert et al. , 2011; Mikolov et al. , 2013; Pennington et al. , 2014) can be used for readability assessment, but the effectiveness is compromised by the fact that these models typically focus on the syntactic or semantic relations of words, while ignoring the reading difficulty. As a result, words with similar functions or topics, such as \"man\" and \"gentleman\", are mapped into close vectors although their reading difficulties are different. It calls for incorporating the knowledge on reading difficulty when training the word embedding.",
        "GPT2_formal_text": "= {\n\n- v = (w_v, ..., w_v)\n- r = (w_r, ..., w_r)\n- s = (w_s, ..., w_s)\n- t = (w_t, ..., w_t)\n\nIn our setup, the input texts are split into two parts: the first part is for the question, and the second part is for the answer. This way, we can use the question-answer text to train a question answering model. The first part is called the Question-Answer Text Sequence (QAS), and the second part is the Answer-Question Text Sequence (AQT). The QAS and AQT are like sets of words, each with a length of k. We use the question-answer text sequence (q_t, a_t) to train a question answering model. Formal: Formal: The question and answer text sequences are shown as (q_t, a_t), with a length of k, and we use the question-answer text sequence (q_t, a_t). Formal: Formal: Each sentence is connected by a directed edge from the question-answer text sequence (q_t, a_t) to the answer sentence (a_t). Formal: Formal: We take the question-answer text sequence (q_t, a_t) and turn it into a set of queries (qs_t, a_t). Formal: Formal: The generated queries (qs_t, a_t) are then used as the input for the question answering model. Formal: Formal: Formal: We pick the query that gives the highest probability to the question, based on the scores from the question-answer text sequence (q_t, a_t). Formal: Formal: If we can't find a query that satisfies all the conditions, we just pick the question that doesn't have the highest probability to the answer. Formal: Formal: Formal: Lastly, we set the query to a maximum of k queries. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C18",
        "filename": "C18-1031.json"
    },
    {
        "casual_text": "Orthogonal transforms have some pretty cool features. For instance, you can get the inverse matrix by just flipping it around (transposing it). Plus, it keeps the L2 norm of a vector intact after the transformation. In our case, we're mainly interested in that handy trick of finding the inverse by transposing. This helps cut down on the number of model parameters we need to deal with (check out Table 3 for details).",
        "formal_text": "Orthogonal transform has many desired properties, for example, the inverse matrix is obtained by simply transposing itself. It also preserves the L 2 norm of a vector after the transform. For our work, we are just interested in its property to obtain inverse matrix by simple transposing. This saves the number of model parameters (see Table 3 ).",
        "GPT2_formal_text": "and attention_hidden_mask. This helps the model focus on the key parts of the input sequence. Figure 4 shows the cross-entropy loss for the sequence sequence classification task using the data shown in Figure 2. The inputs for each layer are {0, 1} × k = 3. The objective function we use to score the responses is: Formal: To make things easier, the model figures out the losses for each epoch using a gradient descent algorithm. Formal: We use a linear layer to grab the best values from the latent representation. Formal: To make things even better, we add an adversarial training loss to keep things in check. Formal: Here, h_c is the hidden layer we use, and λ is the learning rate. Formal: The loss function, L_a, tells us how sensitive the model is to the predicted label. Formal: The model's parameters are trained during each training epoch to get the best possible results. Formal: Finally, the whole model is trained to minimize the cross-entropy loss. Formal: To find the best response, we focus on finding the path with the lowest cross-entropy loss. Formal: The loss function we use to score the responses is: Formal: This approach means the model is trained to minimize the cross-entropy loss. Formal: Formal: The loss function L_a is calculated by looking at how similar the generated responses are to the original ones. Formal: Formal: Formal: The loss function L_a is calculated by comparing the generated responses to the original ones. Formal: Formal: We add an adversarial training loss to keep the model from overfitting. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.241.json"
    },
    {
        "casual_text": "For the scoring experiment, we found out that scoring is super tough, and we still need to do more work to get better results. But hey, we did get some promising stuff—almost a 0.7 correlation for the advanced class in a task where people often disagree a lot.",
        "formal_text": "In terms of the scoring experiment, we observed that scoring is a very challenging task and that there is still need for further study before we can get good results. Even so, we did get some encouraging results, reaching almost 0. 7 correlation for the advanced class in a task that is known to present high discordance among evaluators.",
        "GPT2_formal_text": "= []. Each sentence is a graph made up of three parts: an entity type, a relation type, and a slot type. Formal: The model can predict up to three types of entities. The prediction is based on how often each entity type shows up in the data, as defined by Sim and Zhang in 2017. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of relations. Formal: The model can predict up to three types of",
        "directory": "C18",
        "filename": "C18-1294.json"
    },
    {
        "casual_text": "Using hints while learning, especially when you've got 1000 or fewer data points, makes a big difference and can even work better than just trying to teach yourself.",
        "formal_text": "• Learning with hints, especially for ≤ 1000 training data points, helps significantly, even over self-training.",
        "GPT2_formal_text": "ed. On the flip side, the C-GCN has a different setup. It doesn't need to explicitly learn attention, which is why it performs almost as well as the flat model. Formal: For the query-focused GCN, we have a convolutional layer that figures out the attention distribution for each query, which then helps create the query vector. This vector can be seen as a 1D graph that shows how different entities relate to each other. Formal: Our graph convolutional layer is built on the regular GCN, but instead of combining it with the GCN's hidden state, we swap it out for a convolutional one. Formal: After that, the graph convolutional layer spits out the entity embedding vectors and the query vector. Formal: For the entity-focused GCN, we use the same convolutional layer but this time we use the entity embedding vectors to figure out the attention distribution for each entity, which then helps create the entity vector. Formal: Finally, we combine all these entity vectors into a single representation vector for the query vector. Formal: Figure 1 shows how our GCN works in a simplified form. It takes a query and turns it into a series of entity embeddings, which then give us a query vector. Formal: The entity graph (Equation 1) has four parts: the start, middle, and end nodes, which are convolutional nodes, and their three sub-graphs. Formal: We use the query vector to figure out the attention distribution for each entity, which then helps create the entity vector. Formal: We’ve also shared our code for anyone to use. Formal: The entity-focused GCN, on the other hand, uses the entity embedding vectors to figure out the attention distribution for each entity, which then helps create the entity vector. Formal: The query-focused GCN also uses entity embeddings to learn the attention distribution for each entity, but this time we only use the entity embedding vectors to do it. Formal: The entity-focused GCN is designed for queries that have a lot of entities, while the query-focused GCN is built for queries that only have a few entities. Formal: Formal: We tested both GCNs using our own datasets for the queries they handle. Formal: We trained both GCNs with the same settings and we didn’t tweak the hyperparameters. Form",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "So, our model needs to consider how both predicates and variables affect the sentences in the documents. A good approach is to let the predicates guide the words we use and let the variables decide the sentence structure. For instance, the formula card(x) ∧ freecell(y) ∧ empty(y) has three predicates and two variables. The predicates directly influence the words: sentences based on this formula are more likely to include words related to each predicate. So, a sentence like \"Put the cards on the empty freecells\" makes more sense than something like \"Columns are constructed by playing cards in alternating colors.\"",
        "formal_text": "Thus, our generative model must account for the influence of both predicates and variables on the sentences in the documents. A natural choice is to use the predicates to influence the lexical items, while letting the variables determine the syntactic structure. For example, the formula card(x) ∧ freecell(y) ∧ empty(y) contains three predicates and two variables. The predicates influence the lexical items in a direct way: we expect that sentences generated from this formula will include a member of the gloss set for each predicatethe sentence \"Put the cards on the empty freecells\" should be more likely than \"Columns are constructed by playing cards in alternating colors. \"",
        "GPT2_formal_text": ". For instance, the second example could be rewritten as... Formal: We look at the whole dataset, not just the parts we're interested in. For example, the first example is linked to a part of it, but the second one isn't. Formal: We usually just grab the smaller parts of a document. For instance, the first example in Figure 1 is tied to the first sentence of the summary, but the second example is tied to the second sentence. Formal: We also use something called Maximum Likelihood Estimation (MLE). Formal: We use a scoring function called Φ to measure how well the evidence matches the hypothesis. Formal: For each evidence span (x i) in the document D, the score is calculated by... Formal: The score is basically the average of how well (or not so well) the evidence fits with the hypothesis, divided by the total number of sentences in the document. Formal: We calculate this score using the formula shown in equation 2. Formal: To get the evidence span score, we start by running the algorithm Φ on the document D. The score for a span x i is calculated by... Formal: Formal: We then use the average of these scores to rank all the evidence spans in D. Formal: Finally, we pick the best span x i based on how well it fits with the hypothesis. Formal: Formal: We can use this idea of ranking to rank the evidence spans in D. Formal: Formal: We can also use the type of sentence that leads to the hypothesis to sort the evidence spans. Formal: Formal: Formal: We put all this together in a way that shows how much the evidence matches the hypothesis and how well the hypothesis fits with the evidence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1100.json"
    },
    {
        "casual_text": "Results. Figure 7 shows the same trend as the BLEU score, but with a smaller improvement of 23% when using the adversarial term.",
        "formal_text": "Results. The results in Figure 7 show the same pattern as for the BLEU score, although with a smaller relative gain of 23% when using the adversarial term.",
        "GPT2_formal_text": "_t f_mask = max(1, H_t * M * (1 + p), [1, h])  \n\ndef c1(t_i):\n\nif i is not in set A and i is not in set B:\n\nreturn 0\n\nreturn 1 - c1(t_i) * H * M * (1 + p) * h\n\ndef c2(t_i):\n\nif i is not in set A and i is not in set B:\n\nreturn 0\n\nreturn 1 - c2(t_i) * H * M * (1 + p) * h\n\ndef c3(t_i):\n\nif i is not in set A and i is not in set B:\n\nreturn 0\n\nreturn 1 - c3(t_i) * H * M * (1 + p) * h\n\ndef c4(t_i):\n\nif i is not in set A and i is not in set B:\n\nreturn 0\n\nreturn 1 - c4(t_i) * H * M * (1 + p) * h\n\ndef c5(t_i):\n\nif i is not in set A and i is not in set B:\n\nreturn 0\n\nreturn 1 - c5(t_i) * H * M * (1 + p) * h\n\ndef c6(t_i):\n\nif i is not in set A and i is not in set B:\n\nreturn 0\n\nreturn 1 - c6(t_i) * H * M * (1 + p) * h\n\ndef c7(t_i):\n\nif i is not in set A and i is not in set B:\n\nreturn 0\n\nreturn 1 - c7(t_i) * H * M * (1 + p) * h\n\ndef c8(t_i):\n\nif i is not in set A and i is not in set B:\n\nreturn 0\n\nreturn 1 - c8(t_i) * H * M * (1 + p) * h\n\ndef c9(t_i):\n\nif i is not in set A and i is not in set B:\n\nreturn 0\n\nreturn 1 - c9(t_i) *",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.491.json"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. We're talking about a pre-trained word vector model called *look-up pre−trained*, which is fixed and doesn't change. Then, we have two word vectors for the input s: *x t* and *x * t*. These vectors represent the input s, and *h * t* is the current hidden state, while *h * t−1* is the hidden state from the previous step.\n\nFor instance, if we take an input s from the ID training data, we can get these two word vectors. After that, we can feed these vectors into two different hidden layers (RNN and RNN * 3).\n\nNow, here's something interesting: the language models (LMs) we're using are trained from scratch using the ID training data, but they also borrow some knowledge from a different word vector model called GD. So, the hidden state in Equation 6 isn't exactly the \"GD hidden state\"; it uses the word embeddings from the GD model, but the actual data being fed in is still from the ID dataset.\n\nIn Figure 1, you can see the process we're proposing in this paper. The domain-adapted training is explained in Sections 3.1, 3.2, and 3.3.",
        "formal_text": "where look-up * pre−trained is a pre-trained word vector model and static. 2 x * t is the word vector representation of input s, h * t is the hidden state and h * t−1 is the previous hidden state of word s. For example, given input s from the ID training data, we can obtain two word vector representations (x t and x * t ). In addition, the two representations can then be fed into the corresponding hidden layer (RNN and RNN * 3 ). It is also worth mentioning that the proposed LMs are trained from scratch on ID training data, but adapting knowledge from the GD word vector model. Thus, the hidden state in Equation 6is not strictly the \"GD hidden state\"; it uses the word embeddings from the GD pre-trained word vector model, but the inputs are still ID data. 4 Figure 1 shows the adaptation flow proposed in this paper, where the domain-adapted training is presented in Section 3. 1, 3. 2 and 3. 3.",
        "GPT2_formal_text": ", and it's a 2-to-1 match (like, the query is both query and id). Formal: The query is split into individual pieces, and each piece gets a unique title. Formal: The title is picked based on how similar it is to the pieces in the query. Formal: In a \"no match\" case, the title might not even be there. Formal: You can pick any number of queries, from 1 to K, and pick K different parts. For each query, you pick the parts that are most similar to the query. Formal: You can also choose to include the title, the document title, or both as part of the query. This is done by combining the query and the query parts. Formal: If the query is a named entity (NE), you can also pick the title of the NE entity if it's available. Formal: The title of the NE entity is picked based on how related it is to the query. Formal: Using the title, the entity is identified based on the query and the query parts. Formal: If the query and the query parts are identical, you pick the title of the entity. Formal: The title of the entity is chosen by how similar it is to the query. Formal: For example, if you have a query and the query parts are identical, the title of the entity is the query and the query parts. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1131.json"
    },
    {
        "casual_text": "In this part, we're sharing the results of our experiments for spotting sentences that describe experiments, pulling out mentions of entities, and identifying specific details about the experiments. For breaking down the text into smaller pieces (tokenization), we used ChemDataExtractor, which is really good at handling chemical formulas and units.",
        "formal_text": "In this section, we present the experimental results for detecting experiment-describing sentences, entity mention extraction and experiment slot identification. For tokenization, we employ ChemDataExtractor, 8 which is optimized for dealing with chemical formulas and unit mentions.",
        "GPT2_formal_text": "ing and attention_reconstruction_masking are both the same thing as the regular attention mechanism, which helps in handling long-tail dependencies. Formal: This part talks about the pre-training, fine-tuning, and inference models for the Bengali-English NER task. Formal: The Bengali-English model was trained using the entire Bengali corpus, except for the part where it deals with the target language. Formal: Formal: We take the English sentences from the parallel text and feed them into the model. We use a maximum entropy classifier to figure out the probability of each English word. Formal: After that, we calculate the weighted average of the predictions from the NER model to make the prediction. Formal: We also add the word-level NER predictions from the bilingual text to the model, and then calculate the weighted average for each word. Formal: Finally, we calculate the similarity score between a pair of English sentences to see how similar they are. Formal: So, basically, Formal: Formal: We take the English sentences from the parallel text and feed them into the model. We use a maximum entropy classifier to figure out the probability of each English word. Formal: After that, we calculate the weighted average of the predictions from the NER model to make the prediction. Formal: Finally, we calculate the similarity score between a pair of English sentences to see how similar they are. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.116.json"
    },
    {
        "casual_text": "For the training data, we grabbed a parallel corpus from the web using our own crawler. This corpus has around 9.5 million sentences and 160 million words on average, with at least 8 million sentences and 140 million words for each language pair. For the development and test data, we picked and manually translated 3,000 and 5,000 sentences, respectively, from other web sources for each language pair. We tweaked all the hyperparameters for each method using the development data, and then did the final evaluation with the test data. \n\nFor word alignment, we used IBM Model 1 (from Brown et al., 1993) and HMM alignment (Vogel et al., 1996) on the best-ranked source sentences and their corresponding target sentences. Based on these alignment results, we built the phrase table, which was shared across all decoding methods. For the English language model, we used a 4-gram model with stupid backoff smoothing (Brants et al., 2007), which was the same for all setups. We decided on each configuration for word alignment and the language model based on some preliminary experiments with the baseline system.\n\nFor the baseline system, we used a standard PBMT system, similar to what Och and Ney (2004) did, but with a lexical reordering model (Zens and Ney, 2006) boosted by a cutting-edge preordering method using bracketing transduction grammar (Nakagawa, 2015). We also used a similar decoding strategy and other basic feature functions as in Moses (Koehn et al.).",
        "formal_text": "For the training data, we used a parallel corpus by mining from the Web using an in-house crawler. The corpus contains 9. 5M sentences and 160M words on average, at least 8. 0M sentences and 140M words for each language pair. For the development/test data, we separately sampled and manually translated 3, 000/5, 000 sentences from other data sources on the Web for each language pair. All hyperparameters for each method are optimized using the development data and final evaluation is performed using the test data. During word alignment, IBM Model 1 (Brown et al. , 1993) and HMM alignment (Vogel et al. , 1996) were performed using one-best preordered source sentences and corresponding target sentences. The phrase table was built according to the alignment results, and shared with all decoding methods. For the English language model, a 4-gram model with stupid backoff smoothing (Brants et al. , 2007) was built and commonly used for all settings. Each configuration of the word alignment and the language model was decided according to the preliminary experiments on the baseline system. For the baseline system, we employed a standard PBMT system, similar to that of (Och and Ney, 2004) with a lexical reordering model (Zens and Ney, 2006) enhanced by a state-of-the-art preordering method based on bracketing transduction grammar (Nakagawa, 2015). We used similar decoding strategy and other basic feature functions to Moses (Koehn et al.",
        "GPT2_formal_text": "= list(c) for i in range(n) - 1: input_ids = input_ids + [c] + [n] for j in range(n) - 1: output_ids = output_ids + [c] + [n] return [c] + [n] def get_sent_key_values(c): for i, j in c: return [c] + [i, j] def get_sent_values(c): for i, j in c: return [c] + [i, j] classifier = (c - 1) / (n - 1) for i in range(n) - 1: for j in range(n) - 1: return [c] + [i, j] + classifier # \n\nThis is an example of how to use a greedy algorithm to find all the important pieces of information (keyphrases) for a given text. It starts by creating a list of keyphrases. Then, it picks the ones that are most likely to give useful information. Finally, it uses a classifier to figure out the most likely answers for each keyphrase. Formal: It looks at the text from start to finish and then picks the most important phrases (keyphrases) for the whole document. Formal: It looks at the whole document and then picks the most important phrases (keyphrases) that match the most important phrases in the keyphrases. Formal: The most important phrases (keyphrases) are the ones that match the most important phrases in the keyphrases. Formal: The most important phrases (keyphrases) are the ones that match the most important phrases in the keyphrases. Formal: We can use the same approach to find all the important phrases (keyphrases) for a document by doing something similar but for each document separately. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1134.json"
    },
    {
        "casual_text": "We're focusing on finding how ideas connect in a text, not just how often words repeat. Instead of looking at how words link together, we're looking at how the overall meaning ties everything together. We’re suggesting a way to measure how similar different parts of a text are by considering how words relate to each other across those parts. This method kind of acts like those \"lexical chains\" people talk about, but we’re doing it by averaging how similar a part of the text is to a few parts that came before it. We also handle synonyms by using a mix of different ways to index the text. Our tests on breaking up text into segments show that this approach works way better than the basic method we started with.",
        "formal_text": "Our approach aims at discovering semantic relatedness beyond word repetition. It is based on the notion of semantic cohesion rather than lexical cohesion. We propose to use a similarity metric between segments of text that takes into account semantic associations between words spanning a number of segments. This method approximates lexical chains by averaging the similarity to a number of previous text segments and accounts for synonymy by using a hybrid document indexing scheme. Our text segmentation experiments show a significant performance improvement over the baseline.",
        "GPT2_formal_text": "_x, which is the attention mask for the x-th word. Then, it calculates the attention weights for the words before and after the x-th token. Formal: To get the attention weights, the classifier first looks at the words before and after the x-th token and decides if the word is important or not. After that, the attention weights are calculated using the softmax function on the tokens that make up the predicted labels. Formal: For a given input x, the classifier takes the attention weights for the words before and after the x-th token and gives us the predicted labels y = {y1, ..., yT}. Formal: The LSTM decoder has a state transition function f and a gate g. At each time step t = t 1, ..., t T, the following happens: Formal: The LSTM decoder works like this: Formal: During the pre-training phase, the classifier calculates the attention weights for the tokens and the words before and after the x-th token. Then, it calculates the attention weights for the words before and after the x-th token. Formal: The teacher model has a hidden layer h t ∈ {0, 1} with dimensions D t. Formal: The teacher model has a hidden layer h t ∈ {0, 1} with dimensions D t. Formal: At each time step t = t 1, ..., t T, the following happens: Formal: The attention weights are calculated using softmax. Formal: The classifier then uses a cross-modal GRU to process the data. Formal: The student model has a hidden layer h t ∈ {0, 1} with dimensions D t. Formal: The student model has a hidden layer h t ∈ {0, 1} with dimensions D t. Formal: At each time step t = t 1, ..., t T, the following happens: Formal: The attention weights are calculated using softmax. Formal: The student model then uses a cross-modal GRU to process the data. Formal: The attention weights are calculated using softmax. Formal: The classifier then uses a cross-modal GRU to process the data. Formal: The student model has a hidden layer h t ∈ {0, 1} with dimensions D t. Formal: The student model has a hidden layer h t ∈ {0",
        "directory": "D07",
        "filename": "D07-1037.json"
    },
    {
        "casual_text": "A simple way to do this would be to just use noun phrases as they are. For instance, ipadic 1 treats a bunch of long proper nouns as single words, like \" \" (which means Kansai International Airport Company Connecting Bridge). But this method has some issues. For example, if you're searching for \"Kansai International Airport,\" it won't match the single word for the bridge. That's why we need to break it down into smaller parts.",
        "formal_text": "A naïve implementation would be to use noun phrases as they are. In fact, ipadic 1 regards as single words a large number of long proper nouns like \" \" (literally, Kansai International Airport Company Connecting Bridge). However, this approach has various drawbacks. For example, in information retrieval, the query \"Kansai International Airport\" does not match the \"single\" word for the bridge. So we apply segmentation.",
        "GPT2_formal_text": "= {r_i, r_j, ..., r_T}. Formal: The whole thing is about finding all the keyphrases, starting with the ones that are most relevant. Formal: Basically, the goal is to pick out keyphrases that help us figure out the actual target keyword, p_k. Formal: In this paper, we suggest using the WSI model to get a better grasp of the keyphrases in the text. Formal: Using the WSI model to understand the keyphrases is pretty useful because it helps us dig deeper into how the text is being used. Formal: We also use the WSI model to try and predict the target keywords. We use a softmax classifier that checks if a phrase is important for the target keyword, p_t. If it is, the target keyword gets a probability of 1. Formal: Our approach can be adjusted to work with different target keywords, like phrase-based ones. Formal: We tested how well it works on two datasets: WSI-d, which is a collection of keyphrases from the Dundee corpus, and WSI-p, which is a dataset from the PARC corpus. We used the same setup for both datasets. Formal: The results show that we can get a pretty good idea of what the text is about by using this method. Formal: We also added a list of target keywords to the WSI model. We picked the keywords that seemed most likely to be the target keywords, p_t, based on how often they appeared in the text. Formal: Since the WSI model helps us to see more of the context, we think it could be a good tool for generating keyphrases. Formal: For generating more keyphrases, we look at two ways to do it: Formal: We created an English version of WSI using a basic word embedding model, called the Bag of Words model. This model has some similarities with the Berkeley parser, but it has a couple of issues: Formal: First, the word embedding model doesn’t really capture the word’s meaning properly, which makes it tricky to use for creating keyphrases. Formal: Second, the BOW model doesn’t really work well when it’s trained on a noisy dataset that has a lot of noise. Formal: We found a solution: we added a noise",
        "directory": "D11",
        "filename": "D11-1056.json"
    },
    {
        "casual_text": "In this part, we're going to talk about our new method for classifying sentiments in questions and answers. Check out Figure 2 for a quick look at how this method works.",
        "formal_text": "In this section, we introduce the proposed hierarchical matching network approach for QAstyle sentiment classification. Figure 2 depicts the overview of the proposed approach.",
        "GPT2_formal_text": "_p@1 (src, dst, attn, name) (14:30) (14:30) ... (14:50) Formal: For a single model, this means picking the word that gives the biggest difference (let's call it the \"target\") compared to the rest of the words in the sequence. Formal: Also, check out Figure 1: This is the attention matrix for the sentence \"John is hungry\". Formal: The attention is calculated using equation 1. Formal: This method doesn't need any knowledge about the input because it doesn't need to figure out the hidden state, S. Formal: We're assuming that, with the model S, the attention vector for the source word in the sentence is figured out using equation 1. Formal: Basically, we're saying that if you have two things, s_i and s_j, with different amounts of attention (attn and name) for the source word in a sentence, then the attention vector for s_i is the sum of these two vectors. Formal: We use the attention to help the model focus on the important words, like \"John is hungry\" in Figure 1. Formal: We pick the first important word, w_j, that's closest to the source word. Formal: This helps the model pick the most important words from the whole sentence. Formal: Basically, we're saying that if you have two things, s_i and s_j, with different amounts of attention (attn and name) for the source word in a sentence, then the attention vector for s_i is the sum of these two vectors. Formal: This helps the model pick the most important words from the whole sentence. Formal: We're using the attention to help the model focus on the important words, like \"John is hungry\" in Figure 1. Formal: We choose the first important word, w_j, that's closest to the target word, v_i. Formal: This helps the model pick the most important words from the whole sentence. Formal: We're using the attention to help the model focus on the important words, like \"John is hungry\" in Figure 1. Formal: We choose the first important word, w_j, that's closest to the target word, v_i. Formal: This helps the model pick the most important words from the whole sentence. Formal: We're using the attention to help the model",
        "directory": "D18",
        "filename": "D18-1401.json"
    },
    {
        "casual_text": "The team has a few key roles: the operators (UGV-1, UGV-2, UAV) who actually control the robots, a team leader (TL), and sometimes a mission commander (MC) depending on the mission. The MC is responsible for the whole mission and assigns tasks to the teams. The TL takes those tasks and splits them up among the operators, making sure everyone is on the same page and keeping the MC in the loop if they're around. The operators use the robots to do the tasks they're given and let the TL know how it's going, including any issues that come up.\n\nEveryone on the team uses a shared system that shows a digital map with points of interest (POIs) and where the robots are located. There's also a collection of photos taken by the robot cameras, and since 2017, a task list that the TL can update manually.",
        "formal_text": "The team consists of operators (UGV-1, UGV-2, UAV) who control the robots, a team leader (TL) and in some missions also a mission commander (MC). A MC is in charge of the whole mission and gives tasks to teams. The TL distributes the tasks between the operators, coordinates their actions and reports to the MC (if present). The operators use robots to perform the tasks assigned to them and report to the TL about the results or possible difficulties. The team members use a shared situation awareness interface, consisting of a digital map on which POIs are marked and robots' positions are displayed; a repository of shared photos made with the robot camera; and in 2017 also a task list which the TL can manually edit.",
        "GPT2_formal_text": "= {}; for (x = 0; x < n; x++) { w_i = [w_i1, ..., w_iT]; }  \n\nFigure 2 shows how the n-gram model and the back-off model work together. The back-off model can make the model more stable, but it can also mess up the results a bit. To fix this, we suggest a process where we gradually add more negative samples to the training set, using a minimum learning rate α. This helps the model learn to avoid making the wrong predictions by gradually moving away from the original, positive samples. Formal: So, we can just add more positive samples, right? Formal: In a real-world scenario, the model will use negative samples to train and boost its accuracy, but when we're testing, it might use positive samples to train and check the accuracy. Formal: The model gets better over time because it learns to copy positive samples, which helps it perform better. Formal: We can also think of the model's performance as a type of random walk, like in equation (9), where w_i = [w_i1, ..., w_ik] and t = 1. Formal: We can add more negative samples using the back-off model, but this will make the model less stable and less accurate. Formal: The model will start using more positive samples, but this will also lead to more errors and higher losses. Formal: The model will then reach a stage where it can only use the original positive samples, making the model less accurate. Formal: Formal: The model will use the best positive samples, but this will also lead to more mistakes and lower accuracy. Formal: The model will use the worst negative samples, but this will also lead to more mistakes and lower accuracy. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "crac",
        "filename": "2020.crac-1.13.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nDiplomacy (DM) is a dataset based on conversation logs from the online text-based game Diplomacy. It was put together by Peskov et al. in 2020. The dataset includes stuff like whether someone is lying or not during the game. For example:\n\n- DM 13, 132: Lies, True\n- MS 9, 676: Liar, True\n- OD 7, 168: Lies, False\n- LIAR 4, 560: Lies, False\n- BOL 502: Lies, True\n- MU3D 320: Lies, True\n- RLT 121: Liar, True\n\nBasically, it’s all about figuring out who’s being honest or not in the game.",
        "formal_text": "• Diplomacy (DM) (Peskov et al. , 2020): conversation logs collected from Diplomacy, an online text-based board game. Statement Context? DM 13, 132 Lies T Yes MS 9, 676 Liar T Yes OD 7, 168 Lies T No LIAR 4, 560 Lies A, T No BOL 502 Lies A Yes MU3D 320 Lies A Yes RLT 121 Liar A Yes",
        "GPT2_formal_text": ". Attention-based systems, like the ones by Li et al. (2015) and Li et al. (2016), do a great job of capturing the context when generating text. But when it comes to understanding natural language text, things get a bit more complex. A model that can handle context is super important for tasks like natural language inference (NLI), which involves understanding the overall meaning of a conversation. Formal: To break down the text into sentences, we use a parser to break it down into phrases (grams). Then, to handle how the words connect, we use a grammar that looks at both the words themselves and how they're put together. Formal: After generating the text, the parser figures out the final meaning of each sentence using this grammar. Formal: Once we've got the final meaning for each sentence, we use this grammar to match the words in the text to what the sentence's meaning is. Formal: We tested our system on the TREC-6 dataset (thanks to Lin et al., 2006) to see how it stacks up against the best models out there. Formal: Even though these models are trained on lots of data, they still don't perform as well as human-level performance. Formal: We looked at how well our model did on two popular datasets and also checked how much it can perform in different situations, like when dealing with lots of data or when there's a lot of text. Formal: We showed how our system can create paraphrased versions of text, which is pretty cool. Formal: We also did some experiments to see how much human-level performance matters for different datasets. Formal: Our system does way better than other models for TREC-6 (like the best one out there). Formal: We also looked at how different datasets affect the performance of the best models. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.370.json"
    },
    {
        "casual_text": "Comparing the proposed method to two reranking systems (DL > 0 or DL = 0), the version without distortion (DL = 0) usually gets a better BLEU score than the one with DL > 0. We think this might be because it uses a smarter way of preordering multiple options instead of just doing reordering during decoding based on distortion. These results make it pretty clear that you don’t really need decoding-time reordering if you already have a good preordering setup in place.",
        "formal_text": "In comparison between the proposed method and two reranking systems (DL> 0 or = 0), the proposed method without distortion (DL= 0) often achieves higher BLEU score than DL> 0. We conjecture that the tendencies may come from the use of better preordering among multiple candidates instead of a distortion-wise decoding-time reordering. These results clearly show that the decoding-time reordering is not necessary if better reordering is encoded in a preordering lattice.",
        "GPT2_formal_text": "= [], main = [], output = [], extract = [], keys = [], values = [], p = p_i, t = t_i, q = q_i, template = [], category = category_i, class = class_i, stem = stem, parent = parent_i, and student = student_i. Formal: The P-feature representation for the input sequence (like the ones in Figure 1) is made by taking a feature vector and a vector representation of the word embedding. For instance, the embedding of [CLS] can be created by combining the embedding of the start position (e_start) and the vector of the word embedding. Formal: After running a few steps to create the feature vector, we return the output vector as the final feature representation. The final result is the feature representation vector, which is in the form of a set of vectors where each is a (i, j) tuple, where i is the feature vector and j is the word embedding. Formal: We also use the output vector as the feature representation for the query, which we get by applying the P-feature representation to the output vector. Formal: Finally, the query representation, which we get by applying the P-feature representation to the query vector, is a set of vectors where each is a (i, j) tuple, where i is the query vector and j is the query embedding. Formal: The query is made by combining the output vector and the query representation vector, along with the feature representation vector, the P-feature representation vector, and the P-feature vector vector, all in the form of a (i, j) tuple. Formal: We can use the P-feature representation vector and the P-feature vector representation vector as input features for a CNN to create the final feature representation. This gives us a matrix (h, r, t) that represents the P-feature vector for the query. Formal: In this paper, we call this new representation (h, r, t) P_P_t. Formal: To get the P-feature vector and the P-feature vector representation vectors for the query and the query vector, we use the P-feature vectors for the input features as input features. Formal: We can also use the P-feature vectors as input features for a CNN to make the final feature representation for the query. Formal: Finally, the",
        "directory": "C16",
        "filename": "C16-1134.json"
    },
    {
        "casual_text": "Alright, so we're looking at a situation where |S| is greater than 2ε, and there's no chance of misclassifying any training points. Using our definition, F(X) = {f = w, ∆Φ(X ) |w ∈ R d+ }, with ∆Φ being a projection from X to R d, the L1-norm for (f, g f ) is:",
        "formal_text": "provided |S| > 2 ε, and there is no discrete probability on misclassified training points. Based on our definition F (X ) = {f = w, ∆Φ(X ) |w ∈ R d+ } with respect to a given projection ∆Φ: X → R d, the L 1 -norm of (f, g f ) is then given by:",
        "GPT2_formal_text": ". Formal: We use F1 scores to check how well the text representations are performing. Formal: We test our method on the CoNLL 2000 dataset to figure out the hidden representation representation. Formal: A sentence is considered \"distracting\" if its constituent is the least important part of the sentence. We use the CoNLL 2000 dataset (thanks to Koehn et al. for that in 2003) to test our approach. For training, we go with the linear SVM model and set the learning rate to 0.0001 for both the teacher and student. We keep the other parameters as default. Formal: We use the F1 score to check how well the text representations are performing. Formal: We test our method on the CoNLL 2000 dataset, specifically on the sentence \"The girl with the telescope.\" Formal: The objective function we're looking at, Φ_ij, is a matrix with dimensions n1d × dwn. Formal: We train a supervised classifier using this model. Formal: We use a transformer model to get the output, called h_T. Formal: We also check how well the model's predictions match up with the actual labels in the test set. Formal: We train the model using a development set that includes 10% of the test set and retrain it on the remaining 90%. Formal: We test our method on the CoNLL 2003 dataset. Formal: We train the model using the CoNLL 2003 dataset. Formal: We test our method on the CoNLL 2003 dataset. Formal: We train the model using the CoNLL 2003 dataset. Formal: We test our method on the CoNLL 2003 dataset. Formal: We train the model using the CoNLL 2003 dataset. Formal: We test our method on the CoNLL 2003 dataset. Formal: We train the model using the CoNLL 2003 dataset. Formal: We test our method on the CoNLL 2003 dataset. Formal: We test our method on the CoNLL 2003 dataset. Formal: We train the model using the CoNLL 2003 dataset. Formal: We test our method on the CoNLL 2003 dataset. Formal: We test our method on the CoNLL 2003 dataset. Formal: We train the model using the CoNLL 2003 dataset. Formal: We test our method on the CoNLL 2003 dataset. Formal: We test",
        "directory": "eamt",
        "filename": "2009.eamt-smart.9.json"
    },
    {
        "casual_text": "To fix this, we’re tweaking the translation process like this:",
        "formal_text": "As a solution to this issue, we change the translation pipeline as follows:",
        "GPT2_formal_text": "= {id1, id2, id3, ..., idT}  \n\nSimilarly, entity span detection works by first looking at the entity span and then checking if it meets the constraint. Formal: Each entity span is considered a \"query\" if it can be turned into a form, and all the other spans are considered \"non-query\" spans. Formal: A query is like a list of words, like a title. It might have some extra stuff like: Formal: So, the query spans are like special codebooks, and the non-query spans are like basic forms. Formal: To get a query, you start with a basic form like \"John is a dog.\" Formal: The goal of entity span extraction is to give you all the forms of the query you've identified. Formal: Using the information from all the queries, we pull out the info for each entity span. Formal: The final result, called the query, is basically the combined result of all the queries. Formal: Formal: Formal: In this paper, we didn't use any fancy word embedding techniques. Formal: We tested our system on a real-world dataset and reported the results. Formal: We used two different pre-trained models: a Bi-LSTM (which was trained on Wikipedia) and an RNN (trained on Wiki). Formal: The Bi-LSTM uses a CNN with two layers of convolution to pull out features. Formal: The RNN then figures out the probability of each query. Formal: Formal: We call the result of the query the \"query\" for the entity span. Formal: We used a human reviewer to check the queries and double-checked the results. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2014.eamt-1.37.json"
    },
    {
        "casual_text": "Zhang et al. (2018b) came up with this cool graph RNN called S-LSTM, which looks at how words in a sentence connect to each other. We took that idea and added our own twist by incorporating task-specific stuff, like slots and intents, using something we call collaborative memories. \n\nNow, the S-LSTM already does a good job with local features, but it kind of misses the bigger picture when it comes to the whole sentence's sequence. So, we brought in external BiLSTMs to handle that global sequential information. Our experiments showed that this extra step was super important for both tasks we were working on.",
        "formal_text": "Sentence-State LSTM Zhang et al. 2018b propose a novel graph RNN named S-LSTM, which models sentence between words simultaneously. Inspired by the new perspective of state transition in the S-LSTM, we further extend it with task-specific (i. e. , slots and intents) representations via our collaborative memories. In addition, the global information in S-LSTM is modeled by aggregating the local features with gating mechanisms, which may lose sight of sequential information of the whole sentence. Therefore, We apply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments.",
        "GPT2_formal_text": ". It can be used either as a single input or as a filter that decides what features to use when generating the responses. Formal: A module that turns keyphrase embeddings into a fixed-length vector, like P_embedding, P_epsilon, or P_mask. This module creates a fixed-length vector by combining keyphrases into a matrix. Formal: A module that creates keyphrase embeddings, P_enc, P_epsilon, or P_mask. These keyphrases are then fed into a multi-layer perceptron (MLP) to get the final representation. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_epsilon. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_mask. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_epsilon. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_mask. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_epsilon. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_mask. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_mask. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_epsilon. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_mask. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_epsilon. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_epsilon. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_mask. Formal: A module that combines vectors from the encoder and decoder to create a fixed-length vector, P_epsilon. Formal: A module that combines vectors from the encoder and decoder",
        "directory": "D19",
        "filename": "D19-1097.json"
    },
    {
        "casual_text": "Okay, let’s break this down in simpler terms. So, there’s a diagram (Figure 4) that shows how predictions work based on Wikipedia data, as mentioned in Table 7. Here’s what the colors mean:\n\n- **Green circles**: These represent the \"facet embeddings\" from something called MFS.\n- **Orange circle**: This is the average of all those green circles (called MFS Avg).\n- **Blue circles**: These are word embeddings that are close to both the green and orange circles.\n\nThe word \"project\" is highlighted because it’s the next word in the actual data we’re looking at.",
        "formal_text": "Wikipedia 2021 Improvement Model S3I9P4 S3I9P4 S3I9P4 S3I9P1 S3I1P1 S3I9P4 S3I9P4 S3I9P4 S3I9P1 S3I1P1 Reference Model S3I9P1 S3I1P1 S1I1P1 S1I9P1 S1I1P1 S3I9P1 S3I1P1 S1I1P1 S1I9P1 S1I1P1 Figure 4: Illustration of the MFS predictions given the Wikipedia context in the second column of Table 7. The green circles mean the facet embeddings from MFS. The orange circle is the average of the facet embeddings (MFS Avg). The blue circles are the word embeddings that are close to the facet embeddings and MFS Avg. The word project is highlighted because it is the next word in our ground truth.",
        "GPT2_formal_text": ", position, and the position of the first sentence in the target language. The score for each possible alignment is calculated using this formula: Formal: For a given alignment, the score of the first word in the target language is figured out by adding up the scores of all the words that start with that word. Formal: The score of a word is calculated by adding up the scores of its parts. Formal: We also add up the scores of all the words that start with the target word, like w_i and w_j. Formal: In the translation model, we add up the scores of all the words in the source sentence, like w_s and w_t. Formal: If the translation model gets an alignment that matches the target sentence's position, the score of the word at that position is also the same as the score of the words that start with the target word. Formal: To find the best translation, we use a maximum entropy model that adjusts for the length of the target sentence. Formal: The translation model we use for an alignment is based on the one in Figure 1, but we tweak it a bit by adding a lower bound for the length of the target sentence. Formal: The best translation for a target sentence can be figured out using this formula: Formal: If the target sentence has more words than the source sentence, the score for a word is calculated by adding up the scores of all the words that start with that word. Formal: The target sentence's length is determined by the sum of the length of each word in the target sentence and the length of the target sentence. Formal: Finally, we use a transition-based translation model to translate the source sentence. Formal: In this model, we use the final translation of the source sentence as the score for the word at position t. Formal: The length of the target sentence is also determined by the sum of the length of each word in the source sentence and the length of the target sentence. Formal: Formal: To find the best translation for a source sentence, we use a maximum entropy model that adjusts for the length of the source sentence. Formal: We add up the scores of all the words in the source sentence, like w_s and w_t. Formal: Formal: The length of the source sentence is also determined by the sum of the length of each word in the source sentence and the length of the target sentence. Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.554.json"
    },
    {
        "casual_text": "The distributional inclusion hypothesis (DIH), which has been around since the late 90s and has been studied by folks like Dagan, Geffet, and others, basically says that if one word (let's call it u) means something that includes another word (v), then you can swap u for v in a sentence and it still makes sense. For example, \"cat\" includes \"animal,\" so in the sentence \"a cat is asleep,\" you can replace \"cat\" with \"animal\" and get \"an animal is asleep.\" Makes sense, right? But if you try to replace \"cat\" with something it doesn't include, like \"butterfly,\" you end up with \"a butterfly is asleep,\" which doesn't really work.\n\nNow, this idea has its limits. It only really works in situations where the meaning of the sentence goes \"upward,\" like when you're talking about something getting bigger or more general. It doesn't work so well in sentences with negations or words like \"all\" and \"none.\" So, you can't just swap \"cat\" for \"animal\" in sentences like \"all cats are asleep\" or \"a cat is not asleep.\"\n\nEven with these limitations, the DIH has been a pretty big deal in the world of distributional semantics. A lot of research has been done on it, and it's been shown to hold up pretty well in many cases.",
        "formal_text": "The distributional inclusion hypothesis (DIH) (Dagan et al. , 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that whenever a word u entails a word v, then it makes sense to replace instances of u with v. For example, 'cat' entails 'animal', hence in the sentence 'a cat is asleep', it makes sense to replace 'cat' with 'animal' and obtain 'an animal is asleep'. On the other hand, 'cat' does not entail 'butterfly', and indeed it does not make sense to do a similar substitution and obtain the sentence 'a butterfly is asleep'. This hypothesis has limitations, the main one being that it only makes sense in upward monotone contexts. For instance, the substitution of u for v would not work for sentences that have negations or quantifiers such as 'all' and 'none'. As a result, one cannot replace 'cat' with 'animal' in sentences such as 'all cats are asleep' or 'a cat is not asleep'. Despite this and other limitations, the DIH has been subject to a good amount of study in the distributional semantics community and its predictions have been validated (Geffet and Dagan, 2005; Kotlerman et al. , 2010).",
        "GPT2_formal_text": "is the key to figure out the attention for an entity e in a sentence s. Formal: We take an entity e and its contextual vector representation h t and run it through the attention mechanism to get the contextual representation for the entity e. After that, we do something similar but for the context vector representation of a local entity t. Formal: The Entity Pair Encoder (EPE) takes the embedding vectors of the entity pair e i and e j and turns them into a vector representation called e e i, e j. The entity pair embedding has three parts to it. Formal: The entity pair embedding is calculated by taking the average of the embeddings of the entity pair e i and e j, plus the embeddings of the local entities t i and t j. Formal: The local entity embedding is calculated by the sum of all the embeddings of the local entities t i and t j. Formal: After the entity pair embedding is calculated, we just add a vector representation of the entity pair e e i to our dataset E t. Formal: The final entity pair representation e e i, e e j is then used to train the encoder. Formal: The embedding and embedding representation of the entity pair are used to train the model, which is the entity pair embedding in the next part. Formal: After that, the model decides the entity pair type. Formal: Lastly, the entity pair embedding, which we got from the entity pair embedding, gets passed to the classifier to predict the type of entity pair e e i, e e j. Formal: Formal: We trained the model using E t with different values of α. Formal: We used the gold label dataset M t and the target dataset D t as the test set for our experiments. Formal: We trained the model using E t with different values of α. Formal: We trained the model using E t with different values of α. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1268.json"
    },
    {
        "casual_text": "Here’s the informal version:\n\nSo, in the paper, they tested some models on the Penn Treebank validation set using F1 scores for binarized trees. The results are like this:\n\n- **Kitaev and Klein (2018)**: Binary - 87.5, N-ary - 84.0, Binary N-ary - 85.9, N-ary Binary - 83.6.\n- **DIORA**: Binary - 86.0, N-ary - 73.9, Binary N-ary - 81.7, N-ary Binary - 69.1.\n- **S-DIORA**: Binary - 89.9, N-ary - 77.5, Binary N-ary - 84.8, N-ary Binary - 73.2.\n\nBasically, DIORA didn’t perform that great due to some weaknesses, but S-DIORA did better, especially in the best setup they found (check Table 2 for more details).",
        "formal_text": "Model Binary N-ary Binary N-ary Kitaev and Klein (2018) 87. 5 84. 0 85. 9 83. 6 DIORA 86. 0 73. 9 81. 7 69. 1 S-DIORA 89. 9 77. 5 84. 8 73. 2 Table 1: Supervised parsing results on the validation set of PTB using parsing F1 with binarized trees. DIORA does not do well because of its inherent weakness, and the best setting from S-DIORA (Table 2) is superior.",
        "GPT2_formal_text": "= [s_i1, s_i2, ..., s_iT]  \n\nFor each query q_i, the model grabs all the possible values of k that are related to the query and the current question q_i.  \n\nLet’s say p(q_i | q) represents the probability of a question having a specific answer. To make things easier, let’s say q_i is in the set {1, 2, ..., n}, and q_i is a proper left-branching question. So, p(q_i | q) = p(q_i) ∪ p(q_i|q) ∪ exp(p(q_i | q)).  \n\nThe probability of a question having a specific answer can be broken down into these probabilities:\n- p(q_i | q) = p(q_i|q) ∪ p(q_i|q) ∪ p(q_i|q) ∪ exp(p(q_i | q))\n- p(q_i | q_i) = p(q_i|q_i) ∪ p(q_i|q_i) ∪ p(q_i|q_i) ∪ p(q_i|q_i)\n\nThe sum of these probabilities is also p(q_i | q_i) ∪ p(q_i|q_i) ∪ p(q_i|q_i) ∪ p(q_i|q_i). Formal: Let’s say p(q_i | q) is the probability of a question having a specific answer, and p(q_i|q_i) is the probability of a question having a specific answer. Formal: The probabilities of a query and its answer can be broken down into these probabilities: p(q_i | q_i) = p(q_i|q_i) ∪ p(q_i|q_i) ∪ p(q_i|q_i) ∪ p(q_i|q_i) Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.392.json"
    },
    {
        "casual_text": "We also did some tests to see how different feature sets work together. Turns out, combining WLS with vectors from the MUSE model works really well. We saw improvements for eight out of thirteen language pairs, with the biggest jump being 5% (for Hi-Or) and the smallest being 1% (for Hi-Ko, Hi-Ml, Hi-Ta, Hi-Te). It's worth mentioning that this combo didn't hurt performance for any language pair, which is why we included it in Table 3. Other combinations, like MUSE + VecMap, MUSE + XLM-R, or MUSE + PVS, didn't do as well and hurt performance for at least one language pair.\n\nWhen we compared our best model (MUSE + WLS) to the strongest baseline (from Kanojia et al., 2019b), we saw an average improvement of 9%, with the biggest improvement being 18% (for Hi-Ta). Compared to the weakest baseline (just WLS), our best model showed an average improvement of 50%, with the highest being 61% (for Hi-Or).",
        "formal_text": "Additionally, we perform an ablation test with our feature sets for further experimentation. We observe that the combination of WLS and vectors from the MUSE model performs even better. An improvement is observed for eight language pairs out of thirteen ranging from 1% point (Hi-Ko, Hi-Ml, Hi-Ta, Hi-Te) to 5% points (Hi-Or). It should be noted that this is the only combination where no degradation in performance was observed for any language pair and hence, is reported in Table 3. Any other combination (MUSE + VecMap, MUSE + XLM-R, MUSE + PVS, and so on) degrades the performance of the cognate detection task, on at least one language pair. The average improvement observed by using our best model (MUSE + WLS) over the strongest baseline approach (Kanojia et al. , 2019b) is 9% points with the highest being 18% points (Hi-Ta). Over the weakest baseline approach (WLS), our best model obtains an average improvement of 50%, peaking at 61% points (Hi-Or).",
        "GPT2_formal_text": ". In this paper, we use a bunch of different attention methods, and they help us figure out what the user is focusing on. We build this system by training it on a dataset of 1 million tweets for the IDS and another 1 million tweets for the NIA. We train this system using a method called 'multi-task learning', which has been looked at before for a bunch of different natural language processing tasks. We also use a method called 'cross-modal vision' to figure out which parts of the user's tweet are relevant to what the model is asking. Formal: The model's training goal is to train the attention module and get the final representation that reflects the user's focus. Formal: The model has a hidden state h_t, which is made up of three parts: h_t1, h_t2, and h_t3. Formal: Each h_t in the hidden state h_t1 is represented as a vector. Formal: Then, we use the model's parameters (h_1, ..., h_m) to get the attention vector for the user. Formal: In the training process, we first create a representation h_t by combining the attention from the model and the user's attention, which we call h_t_i. Formal: The goal here is to minimize the cross-entropy loss, which is the gradient of the attention matrix. Formal: Finally, we compare the model's output to the user's attention vector. Formal: We train the model using the development set and calculate the attention loss during inference using the validation set. Formal: The model's parameters are hidden variables that we can figure out by running the training process again. Formal: We use the user's attention vector h_t_i to guide the model's attention during training. Formal: Lastly, we check how well the model's output matches the user's attention vector h_t_i. Formal: Formal: We then use the model's parameters to update the attention vector h_t_i. Formal: Formal: Finally, we use the model's parameters to update the attention vector h_t_i. Formal: Formal: Formal: We add two more hidden variables: h_n_i and h_n_m. Formal: The goal is to minimize the cross-entropy loss by minimizing the cross-entropy loss. Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.119.json"
    },
    {
        "casual_text": "The dataset has around 50,000 documents in three languages: English, French, and Spanish. Out of these, about 35,000 are labeled with useful snippets, which can be used to train and test models for extractive summarization. We tested how well three different models work on this dataset: LEAD4, TextRank (an unsupervised graph-based model from Mihalcea and Tarau, 2004), and NeuSum (a supervised neural model created by Qingyu et al. in 2018).",
        "formal_text": "The dataset consists of approximately 50K documents in three languages: English, French, and Spanish. Among these documents, approximately 35K are annotated with informative snippets and can be used for the training and evaluation of ex-tractive summarization models. We evaluate the performance of LEAD4, TextRank (Mihalcea and Tarau, 2004) -an unsupervised graph-based model -, and NeuSum (Qingyu et al. , 2018 ) -a recently created supervised neural model in the dataset.",
        "GPT2_formal_text": "= {a_i} in the input text. If we don't know the i-th label, we just set the bias to 0. Formal: We tweak the attention weights to handle the entire sequence of tokens. Formal: The models we use for token classification are LSTM-CRF (Dyer et al., 2016). In this setup, we first calculate a hidden state h_t to grab some context, and then we update the attention weights like this: Formal: The loss function for the token classification task is basically the same as L1-regularized pooling. Formal: You can find the code for our model, hyperparameters, and the training data for our experiments over at https://github.com/liugangfei/supervised-decoding. Formal: We picked the LSTM-CRF as our model for two main reasons. First, it's trained on a ton of labeled data and has a great embedding for different kinds of text. Second, LSTM-CRF's memory-enhanced multi-head attention mechanism works really well for this task. Formal: We also tried out some other models, like RNN-CRF (Zhang et al., 2016), LSTM-CRF (Xu et al., 2016), and LSTM-BERT (Gan et al., 2017), but they didn't perform as well. Formal: For the objective function, we use a linear layer with a window size of 5. Formal: Lastly, we used a transform-based kernel with a size of 10 for the final layer. Formal: The main goal of this paper was to show that supervised neural machine translation (NMT) can help with automatic taxonomies and classification tasks. Formal: The main idea behind this research is that NMT can learn better representations of words and phrases that are related to taxonomies and classification tasks. Formal: This is a big deal because it could lead to better, more accurate representations for a bunch of NLP tasks. Formal: There are two main ways people have tried to improve NMT for taxonomy/classification tasks. Formal: One approach is using deep learning methods to automatically create taxonomy/classification structures. Formal: Another approach is using neural networks that learn representations for words and phrases. Formal: Formal: Formal: Formal: We also looked at two other approaches. Formal: We",
        "directory": "eacl",
        "filename": "2021.eacl-main.146.json"
    },
    {
        "casual_text": "In this project, we tested our networks at four different depths: 9, 17, 29, and 49 layers. Here, \"depth\" refers to the total number of convolutional layers. To calculate the depth, we add up the number of blocks with 64, 128, 256, and 512 filters, where each block has two convolutional layers. In Figure 1, for example, there are 2 blocks of each type, so the depth is 2 × (2 + 2 + 2 + 2) = 16. If you include the very first convolutional layer, that brings the total to 17 layers. You can adjust the depth by adding or removing blocks with specific filter sizes. Table 2 shows the best setups we found for depths 9, 17, 29, and 49, along with the number of parameters for all the convolutional layers.",
        "formal_text": "In this work, we have explored four depths for our networks: 9, 17, 29 and 49, which we define as being the number of convolutional layers. The depth of a network is obtained by summing the number of blocks with 64, 128, 256 and 512 filters, with each block containing two convolutional layers. In Figure 1, the network has 2 blocks of each type, resulting in a depth of 2 × (2 + 2 + 2 + 2) = 16. Adding the very first convolutional layer, this sums to a depth of 17 convolutional layers. The depth can thus be increased or decreased by adding or removing convolutional blocks with a certain number of filters. The best configurations we observed for depths 9, 17, 29 and 49 are described in Table 2. We also give the number of parameters of all convolutional layers.",
        "GPT2_formal_text": "ed_robot, which is basically the attention masking process we talked about in Section 2. We're using attention to take into account the context around a specific object mention, as well as the words in that mention. Basically, the attention is calculated using the distribution p_t for the attention score. Formal: There's a correlation between the attention score and the eye-tracking results. Formal: We see this correlation as a way to measure how well the attention masking is working. Formal: In Section 3.2, we mentioned how important it is to measure how well the attention masking is doing. So, to do that, we first calculate the attention score using the distribution p_t for the attention masking process, and then we use this attention score to calculate the eye-tracking results. Formal: To figure out how much the attention masking helps, we use a linear mixed-effects model that considers all the possible combinations of attention masks and the target words. This model is calculated using a log-likelihood function, like the one from a neural network (Toutanova and Klein, 2012). Formal: This model is made to simulate the learning process of an expert system that assigns attention to the correct token. Formal: Formal: The attention masking process is different from the usual binary classification in the usual image classification task, like classification by question or image feature vectors (Li et al., 2017; Su et al., 2019). Formal: In image classification, the binary classification problem doesn't really focus on how well the model predicts the labels. Instead, it just checks if the prediction matches the labels. Formal: In this task, we're checking how well the attention masking is performing, not just how well the model predicts the labels. Formal: Formal: We set k to 10 to make sure the model doesn't get too complicated. Formal: We run the experiments five times with different random seeds and see how well the model performs on the validation set. Formal: The results show that the model is better at predicting the labels compared to the usual binary classification. Formal: Formal: We calculate the correlation between the attention score and the eye-tracking results by multiplying the attention score and the eye-tracking score. Formal: In Section 4, we talk about two ways to measure the attention masking performance: Formal: There's a correlation between the attention masking score and the eye-tracking results, and we calculate it by",
        "directory": "E17",
        "filename": "E17-1104.json"
    },
    {
        "casual_text": "We tested a neural network setup using graph convolutional networks for relation extraction and it worked well. We also came up with a method called path-centric pruning to make dependency-based models more reliable by cutting out unnecessary stuff without losing important details. After looking into it closely, we found that our model works really well alongside sequence models, and the pruning trick can be used on other dependency-based models too.",
        "formal_text": "We showed the success of a neural architecture based on a graph convolutional network for relation extraction. We also proposed path-centric pruning to improve the robustness of dependencybased models by removing irrelevant content without ignoring crucial information. We showed through detailed analysis that our model has complementary strengths to sequence models, and that the proposed pruning technique can be effectively applied to other dependency-based models.",
        "GPT2_formal_text": "to create a dictionary of the text's subtopics. These topics are then used to train a model that predicts the next word. Formal: To make it easier to compare with earlier studies, we made our dataset public. You can check out the stats for this dataset in Table 1. Formal: The next step is to figure out how to estimate the entropy for a specific topic. Formal: To get an entropy estimate for a topic, we calculate the number of times the current word w_i shows up in the text t. Formal: First, we calculate the topic distribution for the whole text t. Then, we estimate the entropy of the topic p_t_i for the word w_i based on the probability distribution p_t_i(w_i). Formal: Next, we estimate the entropy of the topic p_t_i(p_t_i(w_i)). We do this by looking at the difference in probability distributions between the current word w_i and the previous word p_t_i(w_i). Formal: Finally, we can find the entropy for the topic p_t_i(p_t_i(w_i)). Formal: To be more specific, we can estimate the entropy for the topic p_t_i(p_t_i(w_i)). We do this by looking at the difference in probability distributions between the current word w_i and the previous word p_t_i(w_i). Formal: Finally, the total probability p_t_i(w_i) is found by taking the difference in distributions from the current word w_i and the previous word p_t_i(w_i). Formal: The hyperparameters for the topic distribution and the language model we use are given in Table 1. Formal: To build the language model, we use a standard language model from a pre-trained model, like GPT-2 (Radford et al., 2019). Formal: The final model is trained using a cross-entropy loss function. Formal: The weight parameter θ is set to 1, which means we only train the model using the probability of the words w_i. Formal: We tested our method on the test set and the results are in Table 1. Formal: The log loss function τ is set to 0.1, which means we only use the log probabilities of words w_",
        "directory": "D18",
        "filename": "D18-1244.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nThe connection between these split-antecedent anaphors and their individual parts is kind of like the opposite of \"element-of.\"",
        "formal_text": "https: //www. phrasedetectives. org/ 3 The relation between the split-antecedent anaphors and the individual antecedents is the inverse of element-of.",
        "GPT2_formal_text": "ed: a. {a j }, b. {b j }, c. {c j } where a j is the i-th token in the text and b j is the j-th token. Formal: The importance of a word w i in a sentence s i is calculated using the Frobenius norm of the attention matrix A. Formal: To see how the composition function φ affects attention, we use the Adam optimizer with a learning rate of 5e-4 and some nonlinearities β, γ, and γ (as suggested by Kingma and Ba in 2015). \n\nFirst, we create a fixed-length vector called A (or A^T) that we use to represent the sequence of tokens. We build this vector by averaging the attention weights that the model produces. We can do this in two steps: first, we calculate the attention weights and then we normalize them using the Frobenius norm. To keep things simple, we set the output matrix A^T to 1. Formal: To see how well the model understands the context around a word, we train the model using its own internal hidden state to make a representation of the word. Formal: The model calculates the attention weights using the Frobenius norm of the attention matrix A. Formal: Then, we normalize this representation using the Frobenius norm. This process is repeated for the entire text. \n\nWe use these attention weights to calculate the attention distribution α c i, j and the attention span, which we call α c i, j. Formal: After applying this attention weight function, the composition function φ (where the j-th word in the text) helps the model understand the context better. Formal: Finally, we add the attention weights for the text to the current hidden state and normalize them using the Frobenius norm. Formal: Finally, we add the attention weights for the text to the current hidden state and normalize them using the Frobenius norm. Formal: Formal: Finally, we add the attention weights for the text to the current hidden state and normalize them using the Frobenius norm. Formal: Formal: Finally, we add the attention weights for the text to the current hidden state and normalize them using the Frobenius norm. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.538.json"
    },
    {
        "casual_text": "The goal here is to boost the score for the correct tags in the sequence, which we write as s θ (x T 1, ŷ T 1 ). One thing to point out is that the first part and the second part of this new sequence score don’t really depend on each other. This means we can train the BLSTM-based tagging model and the LSTM-based language model separately using cross-entropy.",
        "formal_text": "The training objective is to maximize the sequence score of the true tags, i. e. s θ (x T 1, ŷ T 1 ). We note that the first term and the second term of the new sequence score are independent of each other, eventually leading to cross-entropy training of the BLSTM-based tagging model and the LSTM-based LM separately:",
        "GPT2_formal_text": ", m, n) = {m|1, ..., n|N}, where n is the number of documents in each type. Formal: Here, p_ij is the classification probability for the j-th document, given the i-th document. Formal: We also train a binary classifier for each token, using k-1-1-1 weights to do that. Formal: We add a linear layer to this model, which helps reduce the negative log-likelihood. Formal: We use label smoothing (from Gimpel and Smith, 2010) with a parameter α to prevent overfitting. Formal: To keep the model from overfitting, we make sure the log loss stays below a certain value, γ. Formal: We create a softmax classifier using a one-vs-all loss function, and we train it on the whole dataset using a parameter γ. Formal: To train the joint model, we adjust the log loss for the two classes to keep the model from overfitting. Formal: We train the binary classifier using the cross-entropy loss, which we calculate using the cross-entropy between the labels in the dataset. Formal: We use two different learning rates: α and γ, which are also adjustable. Formal: We add a linear layer between the two classifiers to keep the loss below a certain value. Formal: We use BERT as the pre-trained word embedding layer to get word representations. Formal: The latent variables for the entity type, type_of, and document type are combined to form the entity type, type_of_s, and document type, respectively. Formal: We use the learned entity type vector to estimate the type_of and document type, then use cross-entropy to calculate the cross-entropy loss. Formal: We use the cross-entropy loss to calculate the cross-entropy loss between the learned entity type and type_of vectors. Formal: To train the model using this process, we set the initial learning rate to γ = 0.1, which is a good starting point. Formal: The final model outputs are made up of entity types, type_of, and document types, respectively. Formal: This model is trained using the cross-entropy loss, which we calculate using the cross-entropy between the learned entity type and type_of vectors. Formal: To train the",
        "directory": "coling",
        "filename": "2020.coling-main.612.json"
    },
    {
        "casual_text": "Alright, so we've got an encoder E and a decoder D. Our job is to take a bunch of inputs {x 1. . . x N } and turn them into outputs {ȳ 1. . . ȳ N }, where N is the number of data points. For instance, in machine translation, each x i is a sentence in the source language, made up of words or tokens, and each y i is the translated version. The decoder D works by taking e i = E(x i ) and a partial translation y i as input, and it builds the full translation y i one token at a time.",
        "formal_text": "Given encoder E and decoder D, our task is to convert inputs {x 1. . . x N } into corresponding outputs {ȳ 1. . . ȳ N }, for data size N. For example, in machine translation, each x i is a source sentence consisting of a sequence of tokens and each y i is a translation. We assume D(e i, y i ) receives e i = E(x i ) and a partial y i as input, constructinḡ y i one token at a time.",
        "GPT2_formal_text": "ing. But, it's tricky to figure out the exact values for the bias term, so we use the loss function from L2CT to do that. This lets us simplify things by giving higher importance to some examples than others. \n\nTo sum it up, we picked the sentence y = {x 1, . . . , x m } using the method from Liu et al. (2019). Then, we trained the sentence representation model using the attention masking function we just learned. \n\nNow, let's talk about our results. We ran the validation sets for all three datasets and the results are in Table 2. We used the model we trained with the results from the first set of validation sets. Table 3 shows the confidence scores from these sets. \n\nThe table shows that all three datasets have pretty similar performance scores. Formal: Our model has a bit of a bias issue, which is why our validation performance is lower compared to the first validation set. Formal: Our model doesn't have that problem, so our validation performance is better than the first validation set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.366.json"
    },
    {
        "casual_text": "Alright, so we've got two vectors, and we calculate the cosine similarity between them. We do the same thing for the Question-language Features. Earlier, we talked about translating the sentence (s 1best) and finding the cosine similarity with the original question (q). We use Equation 2 to get v q and v s 1best. Even though we could translate the sentence into English using four different methods, we only went with the one-best translation because it’s less heavy on the computer. So, in the QL view, we just have one lexical similarity feature (let’s call it LexQL). The whole process for these five lexical similarity features is laid out in Table 1. Once we’ve got all the numbers, we use a maximum-entropy model to figure out the feature weights. Oh, and even though it’s not shown in the figure or table, we also include the same set of features from the sentence right before the answer in the forum post to give a better idea of the bigger picture.",
        "formal_text": "Given the two vectors, we compute the cosine similarity. Same process is repeated for the Question-language Features. As mentioned before, we also obtain a similarity value by translating the sentence (s 1best ) and computing the cosine similarity with the original question (q). v q and v s 1best are computed using Equation 2. Although it is possible to translate the sentence into English using the same four methods, we only used the one-best translation due to the computational cost. Hence, we have only one lexical similarity feature in the QL view (call LexQL). The computation process for the five lexical similarity features is summarized in Table 1. After computation, feature weights are learned via a maximum-entropy model. 7 Although not included in the figure or table, we also include the same set of features from the sentence preceding the answer (within the corresponding forum post), in order to represent the larger discourse.",
        "GPT2_formal_text": ", and the hidden state representation for the cross-attention part is just a bunch of n-gram vectors. Formal: The cross-attention part here is basically a 2D transform that uses an input embedding x (like x = x 1, . . . , x m ) to create a response vector y (like y = y 1, . . . , y n ). Formal: The training process for the cross-attention part is shown in Figure 1. In this case, we're dealing with a sentence x and its corresponding word embedding y. Formal: The cross-attention model is a 2D transform that uses a hidden layer dimension of n. Formal: The training process for the cross-attention part is shown in Figure 1. In this case, we're dealing with a sentence x and its corresponding word embedding y. Formal: The training process for the cross-attention part is shown in Figure 1. In this case, we're dealing with a sentence x and its corresponding word embedding y. Formal: The cross-attention model is a 2D transform that uses a hidden layer dimension of n. Formal: The training process for the cross-attention part is shown in Figure 1. In this case, we're dealing with a sentence x and its corresponding word embedding y. Formal: The training process for the cross-attention part is shown in Figure 1. In this case, we're dealing with a sentence x and its corresponding word embedding y. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1055.json"
    },
    {
        "casual_text": "- **Usability**: Making it easy for developers and users from all kinds of backgrounds to use the stuff we’ve built in TSNLP for all sorts of diagnosis and evaluation tasks.\n- **Suitability**: Making sure the system can handle storing and managing natural language test data (like in string processing) and giving really flexible tools to work with.",
        "formal_text": "• usability: to facilitate the application of the methodology, technology, and test; data developed in TSNLP to a wide variety of diagnosis and evaluation purposes for ditferent applications by developers or users with varied backgrounds; • suitability: to meet the specific necessities of storing and maintaining natural language t; est data (e. g. in string 1)recessing) and to provide maximally flexible interfaces;",
        "GPT2_formal_text": ". The same idea applies to the attention weights from the encoder and decoder. Instead of the usual teacher forcing the model to generate responses that all look alike, we use something called adversarial training (shoutout to Vaswani et al., 2017). We get the student model to generate responses by using the output of a specific classifier that's been tweaked for that specific task. Formal: We train the student model using this setup. Formal: We use the student model to generate responses for a specific task, which is the input we're focusing on. Formal: Lastly, we add a dynamic penalty to the loss function, like this: Formal: Basically, we make the loss function more likely to focus on the target class's output by using this dynamic penalty. Formal: We use this dynamic penalty to adjust the model's output distribution, making sure it stays focused on the task's input. Formal: We use this dynamic penalty to adjust the model's output distribution, making sure it stays focused on the task's input. Formal: Finally, we add a dynamic penalty to the loss function, like this: Formal: Basically, we make the loss function more likely to focus on the target class's output by using this dynamic penalty. Formal: We use this dynamic penalty to adjust the model's output distribution, making sure it stays focused on the task's input. Formal: Finally, we add a dynamic penalty to the loss function, like this: Formal: Basically, we make the loss function more likely to focus on the target class's output by using this dynamic penalty. Formal: We use this dynamic penalty to adjust the model's output distribution, making sure it stays focused on the task's input. Formal: Finally, we add a dynamic penalty to the loss function, like this: Formal: Basically, we make the loss function more likely to focus on the target class's output by using this dynamic penalty. Formal: We use this dynamic penalty to adjust the model's output distribution, making sure it stays focused on the task's input. Formal: Finally, we add a dynamic penalty to the loss function, like this: Formal: Basically, we make the loss function more likely to focus on the target class's output by using this dynamic penalty. Formal: We use this dynamic penalty to adjust the model's output distribution, making sure it stays focused on the task's input. Formal: Finally, we add a dynamic penalty to the loss function",
        "directory": "C96",
        "filename": "C96-2120.json"
    },
    {
        "casual_text": "Clearly, these two tasks are connected because the info you need to answer a new question often comes from threads of similar questions. We’re looking at tackling both tasks together, with the help of another related task: figuring out if a comment in a question-comment thread actually answers the main question.",
        "formal_text": "Obviously, the two subtasks are interrelated as the information needed to answer a new question is usually found in the threads of highly related questions. Here, we focus on jointly solving the two subtasks with the help of yet another related subtask, i. e. , determining whether a comment within a question-comment thread is a good answer to the question heading that thread.",
        "GPT2_formal_text": ". For a given document d, the relevance score l_i is the highest similarity score between the query and the document we're looking at. We use two popular ways to measure relevance: mean reciprocal rank (MRR) and TFIDF (thanks to Grefenstette and Sadrzadeh, 2011). Formal: The relevance score for a document d in our dataset is calculated like this: Formal: In our setup, we're sticking with the usual evaluation metrics like F1-score, micro-average, and overall evaluation scores. Formal: The relevance score for the query query is calculated using these methods: Formal: The relevance score for the document d_q is calculated as the average of the relevance scores from the query. Formal: The relevance score for the document d_i is calculated as the average of the relevance scores from the document itself. Formal: The relevance score for the query query is the average of the relevance scores from all the documents in our dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1452.json"
    },
    {
        "casual_text": "Figure 2 shows how the different versions of the DeReKo corpus—DeReKo-2018-I to DeReKo-2019-I—are connected, along with three virtual corpora (let’s call them 1, 2, and 3) and some texts. Corpus 1 was first defined based on DeReKo-2018-I and included two texts: HMP17/FEB. 18387 and AZM18/MAI. 11491. When DeReKo-2018-II came out, another text, GAZ18/JAN. 12539, was added to Corpus 1 because it fit the criteria for that corpus. At the same time, based on DeReKo-2018-II, Corpus 2 was created, and it included GAZ18/JAN. 12539. Later, with DeReKo-2019-I, Corpus 3 was added, and it included AZM18/MAI. 1149. The point here is that texts in DeReKo can be part of multiple corpora, which can make things complicated if you start removing texts.",
        "formal_text": "Figure 2 shows the relationships between the DeReKo corpus releases DeReKo-2018-I -DeReKo-2019-I, three persistent virtual corpora 1, …, 3, respectively initially defined on one of the releases, and three texts. 7 Based on DeReKo-2018-I, 1 was intensionally defined, already containing the texts HMP17/FEB. 18387 and AZM18/MAI. 11491. With DeReKo-2018-II, GAZ18/JAN. 12539 was added to 1 because the text matches the intensional definition of 1. In addition, based on DeReKo-2018-II, 2 was defined, containing the text GAZ18/JAN. 12539. Based on DeReKo-2019-I, then 3 was added, containing AZM18/MAI. 1149. You can see here that texts in DeReKo can belong to many different corpora so that the removal of texts can have complex consequences.",
        "GPT2_formal_text": "_tokens(len(src)) } r = [r1, ..., rK, ..., rT] T = [T1, ..., TK] φ = [φ1, ..., φK]  \n\nBasically, this equation is saying that the word vector φ is made up of two parts, φ1 and φ2, and they're related to the words they overlap with. Formal: If the word vector φ1 is different from φ2, we say that the similarity is 1. If the word vector φ2 is different from φ1, we say that the similarity is 0. If both are the same, we just use 0. Formal: In equation (11), the attention matrix A is used to measure how similar a source sentence is to a target sentence, called T. The output from the attention layer is then sent to the source sentence. Formal: Since the attention matrix A is based on the source sentence and the connection between the source and target sentences, it's pretty important to handle these connections effectively. Formal: Lastly, the output from the model is passed to the target sentence. Formal: The weights for the attention and the output from the model are calculated using the loss function from equation (11). Formal: The model is set up to learn the correlation between the source and target sentences. Formal: The model learns to predict how much attention is given to each source sentence. Formal: The model then uses this attention to predict the similarity between the source and target sentences. Formal: The model is trained using the cross-entropy loss function and the rewards it gets from these predictions. Formal: Finally, the model is tested on two datasets: one for training and the other for testing. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "cmlc",
        "filename": "2020.cmlc-1.1.json"
    },
    {
        "casual_text": "In Daxenberger and Gurevych's 2012 paper, they broke down a 21-category system into three main types of edits: text-based (which change meaning), surface (edits that don't change meaning), and Wikipedia policy edits (like VANDALISM and RE-VERT). Within the text-based edits, they grouped things like templates, references (both internal and external links), files, and information. Each of these was then split into three subcategories: insertions (I), deletions (D), and modifications (M). \n\nSurface edits include stuff like paraphrasing, fixing spelling and grammar, moving things around, and markup changes. The markup edits cover anything that tweaks markup elements that don’t fit into the other categories, like adding, deleting, or modifying things. An example would be messing with apostrophes in '''bold text'''. They also added an \"OTHER\" category for edits that couldn’t be labeled due to segmentation issues.\n\nFigure 1 gives an example of an edit from WPEC, labeled as REFERENCE-M. WPEC was made through a manual annotation process with three people working on it. The agreement between the annotators, measured using Krippendorf's α, was .67. The experiments in the study used the gold standard annotations from WPEC, which were decided by a majority vote for each edit.",
        "formal_text": "In Daxenberger and Gurevych (2012), we divide the 21-category taxonomy into text-base (meaningchanging edits), surface (non meaning-changing edits) and Wikipedia policy (VANDALISM and RE-VERT) edits. Among the text-base edits, we include categories for templates, references (internal and external links), files and information, each of which is further divided into an insertion (I), deletion (D) and modification (M) category. Surface edits consist of paraphrases, spelling and grammar corrections, relocations and markup edits. The latter category contains all edits which affect markup elements that are not covered by any of the other categories and is divided into insertions, deletions and modifications. This includes, for example, apostrophes in '''bold text'''. We also suggested an OTHER category, which is intended for edits which cannot be labeled due to segmentation errors. Figure 1 shows an example edit from WPEC, labeled with the REFERENCE- M category. WPEC was created in a manual annotation study with three annotators. The overall interannotator agreement measured as Krippendorf's α is. 67 (Daxenberger and Gurevych, 2012). The experiments in this study are based on the gold standard annotations in WPEC, which have been derived by means of a majority vote for each edit.",
        "GPT2_formal_text": ", the coreference model gives us a list of entity names. Formal: We take the info from the entity names and make sure they all end up in the same document. Formal: Lastly, we add some general knowledge about the entities, like how they might be connected or different. Formal: Once we get that, we turn each of these entity names into a vector with a hidden size of d_e. Formal: Next, we train a model that helps us understand and classify these entity mentions. Formal: The final result is a vector with a hidden size of d_c. Formal: Using our new vector, we can automatically spot mentions of entities in the text and figure out their types. Formal: For instance, if we look at \"John Smith,\" we can figure out that it's a Person mention. Formal: In this paper, we use a combined BIO tagging system to handle both entity mentions and their types. Formal: We use the Knowledge Base Network (KBN) to connect mentions to their types. In this setup, the mentions are linked to the entities, while the entities are linked to the KB. Formal: The entity types we use are: Person, Organization, Location, and Misc. Formal: So, if a mention has a specific type, like \"McDonalds,\" we can label it with that type. Formal: We also figure out the types for an entity by looking at the mentions and the entities. Formal: We use the Entity Type Induction Kit (ETK) to do this. Formal: We do this by finding the entity mentions in the text and then figuring out the types of the mentions. Formal: If the mentions are the same type, we can also pick the entity mentions that belong to the same type. Formal: For instance, if \"John Smith\" is an Organization mention, we can label \"John Smith\" as an Organization type, \"Smith\" as a Person type, and \"John\" as a Location type. Formal: We also use the Hierarchical Attention Network (HAN) to do this. Formal: We don't bother with the regular hierarchical attention mechanism because it doesn't really help us understand the entity types. Formal: Formal: We only use the entity type info to fix the sparsity issues in the original NER model. Formal: Formal: The Entity Type Induction Kit (ETK) is a pretty good tool for learning entity",
        "directory": "D13",
        "filename": "D13-1055.json"
    },
    {
        "casual_text": "To go along with Table 1, we’ve put the correlations for the best baselines in Figure 2. We looked at what happens when we reduce the number of gold references from 11 down to 1. For all four dimensions and all the baselines, we noticed that fewer references lead to lower correlation and more variance. But QU E S TEV A L doesn’t need any references at all. So, as the number of references drops, QU E S TEV A L does better compared to the other metrics. Plus, QU E S TEV A L can still evaluate systems even when there are no gold references available.",
        "formal_text": "To complement Table 1, we report in Figure 2 the correlations for the best baselines as we progressively decrease the number of available gold references from 11 to 1. For all four dimensions and all the baselines, we observe that less references result in decreased correlation and increased variance. However, QU E S TEV A L does not require any reference. Therefore, the improvement over the other metrics grows larger as the number of references used decreases. Furthermore, QU E S TEV A L enables the evaluation of systems even when no gold reference is available.",
        "GPT2_formal_text": "ed, attention_pooling, and max_pooling layers, respectively. For the Bi-LSTM model, we use the same setup as in Barhom et al. (2018) to get the hidden state h = {h_i, . . . , h_i+1}. Formal: We use a non-linear function f(a_t, e_t) to calculate the attention weights. Formal: We also use a linear transformation of the attention weights to get the final representation for the tokens in the text. Formal: In this project, we're using a BiLSTM model that uses attention mechanism, so we call it BiLSTM-ATT. It has three layers, each with 12 attention heads. The hidden state h for the first layer is h_0 = {h_0, . . . , h_0+1}. Formal: For the second layer, we calculate the hidden state h_1 = {h_1, . . . , h_1+1}. Formal: Finally, for the last layer, we calculate the hidden state h_2 = {h_2, . . . , h_2+1}. Formal: For our experiments, we use the En-De dataset from the Microsoft Azure Machine Translation (MT) service. Formal: We train the BiLSTM model using the target language's development set, and then we test it on the test set. Formal: We fine-tune the model for 1 epoch using the development set for German to English. Formal: The cross-entropy loss is calculated using the test set for German to English. Formal: For the final evaluation, we check how well the model performs on the test set for German to English. Formal: To keep things fair, we use the same evaluation metrics (like BLEU, METEOR, and TER) as the reference model. Formal: Lastly, we compare the model's performance to the reference model and the general-domain baseline. Formal: Formal: Formal: We use the development set for German to English and the test set for German to English. Formal: We fine-tune the model for 1 epoch using the development set for German to English. Formal: Formal: Formal: Formal: Formal: We apply the same evaluation metrics as the reference model. Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.529.json"
    },
    {
        "casual_text": "You can use TexSmart in two ways: either by calling the HTTP API directly or by downloading the offline SDK. Just a heads-up, the results from the HTTP API and the SDK might be a bit different for the same input text. This is because the HTTP API uses a bigger knowledge base and supports more text understanding tasks and algorithms. If you want a detailed comparison between the SDK and the HTTP API, you can check it out here: https://ai.tencent.com/ailab/nlp/texsmart/en/instructions.html.\n\n**Offline Toolkit (SDK)**  \nRight now, the SDK works on Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support is coming in version 0.3.0. The SDK supports a bunch of programming languages, including C, C++, Python (both versions 2 and 3), and Java (version 1.6.0 or higher). If you want to see how to use the SDK with different languages, check out the example codes in the `./examples` sub-folder. For instance, the Python example in `./examples/python/en_nlu_example1.py` shows how to process an English sentence using the TexSmart SDK. And the C++ example in `./examples/c_cpp/src/nlu_cpp_example1.cc` demonstrates how to analyze both an English and a Chinese sentence.\n\n**HTTP API**  \nThe HTTP API for TexSmart has two parts: the text understanding API and the text matching API. The text understanding API can be accessed using HTTP-POST, and the URL is available on the webpage. The text matching API is used to calculate how similar two sentences are.",
        "formal_text": "Two ways are available to use TexSmart: Calling the HTTP API directly, or downloading one version of the offline SDK. Note that for the same input text, the results from the HTTP API and the SDK may be slightly different, because the HTTP API employs a larger knowledge base and supports more text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https: //ai. tencent. com/ ailab/nlp/texsmart/en/instructions. html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will be added in v0. 3. 0. Programming languages supported include C, C++, Python (both version 2 and version 3) and Java (version ≥ 1. 6. 0). Example codes for using the SDK with different programming languages are in the. /examples sub-folder. For example, the Python codes in. /examples/python/en_nlu_example1. py show how to use the TexSmart SDK to process an English sentence. The C++ codes in. /examples/c_cpp/src/nlu_cpp_example1. cc show how to use the SDK to analyze both an English sentence and a Chinese sentence. HTTP API The HTTP API of TexSmart contains two parts: the text understanding API and the text matching API. The text understanding API can be accessed via HTTP-POST and the URL is available on the web page. 4 The text matching API is used to calculate the similarity between a pair of sentences.",
        "GPT2_formal_text": "= { u1, u2, ..., uk }  \n\nNow, let's talk about how to combine all these different entities into one neat, organized structure. Formal: For each group, we figure out the best mix of groups that will work together to create the most coherent summary. Formal: Basically, we train a bunch of models using different scenarios, each one trying to match up each example. Formal: Once we have this structured information, we use it to make a summary that answers all the questions. Formal: For each question, we look at the entities in the dataset and see if they match up with any entities in the Knowledge Graph. The last two parts are based on a real example where we grabbed the entities from the Knowledge Graph. Formal: For each entity in the Knowledge Graph, we see if it's in a group with other entities. If it is, we say the entity is in that group. Formal: We use the features from each entity to help make the summary better. Formal: To see how useful each entity is, we check how often it shows up in other groups. We then figure out how often each entity appears with each group. Formal: We're curious about how entities that appear in different groups are connected. Formal: We look at how often each entity shows up with other entities to get a sense of how often they're actually related. Formal: We check how often entities appear with each other to see if they're in the same group or not. Formal: We keep track of all the entities that are linked to each group, and we figure out how often each entity appears with each group. Formal: After that, we combine all the entities in the Knowledge Graph into one cohesive group. Formal: Finally, we create the summary by using all the information from all the groups to help create a coherent summary. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-demo.1.json"
    },
    {
        "casual_text": "A popular and straightforward way to measure distance is using the l1 norm. This method just adds up the differences in brightness between corresponding pixels in two images, assuming they're lined up properly. Luckily, all kanji are designed to fit into the same size square, so aligning them is pretty straightforward—it's just a grid that works for all kanji. If we say p_x(i, j) is the brightness of the pixel at position (i, j) in the kanji x, then we calculate the l1 norm like this:",
        "formal_text": "A common and simple distance metric is the l 1 norm, which simply sums the difference in luminance between pixels of the two images for some alignment. Fortunately, all kanji are intended to occupy an identically sized block, so alignment is via a grid, constant across all kanji. Considering p x (i, j) to be the luminance of the pixel at position (i, j) of rendered kanji x, we evaluate the l 1 norm as follows:",
        "GPT2_formal_text": "_tokens) = [ [mask_tokens, [mask, [tokens]]] ] token_tokens = [tokens, [tokens] ] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [tokens, [tokens]] token_tokens = [",
        "directory": "C08",
        "filename": "C08-1131.json"
    },
    {
        "casual_text": "The process involves picking from a list of thirteen options that look like this: H (modal) [P true/false], where H stands for the person involved and P is some statement or event. The job of the annotators is to choose the option that best shows the meaning of the modal in a given situation. The agreement rates between different annotators are pretty good—0.82 for triggers and 0.76 for targets.",
        "formal_text": "The procedure relies on a list of thirteen choices of the form of H (modal) [P true/false] where H is a holder and P is a proposition or an event. The annotators' task is then to select the best form to represent the modality meaning of a given trigger. Reported kappa κ inter-annotator agreement rates are 0. 82 for triggers and 0. 76 for targets.",
        "GPT2_formal_text": "= [], tags = {}, class_ids = {}, slots = {}, values = {}, and unigrams = [], where unigrams are just words from a dictionary with no special characters. Formal: These are the metrics we use. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the one that uses the least amount of memory. Formal: The best MT system is the",
        "directory": "C14",
        "filename": "C14-1144.json"
    },
    {
        "casual_text": "Sure, there are algorithms with a = 2.3727, but the ones we actually use have a = 2.807 or a = 3 (thanks to Coppersmith and Winograd, 1990).",
        "formal_text": "Algorithms with a = 2. 3727 are known but practical algorithms have a = 2. 807 or a = 3(Coppersmith and Winograd, 1990)",
        "GPT2_formal_text": "ed = [], b = {b1, ..., bT}, c = {c1, ..., cT}, h = {h1, ..., hT}. Formal: Using the softmax layer, we can figure out the hidden representation vector h. Formal: We can also find the correct answer by looking at the hidden representation vector h that’s been masked, based on the given input x. Formal: Formal: We can also find the correct answer by looking at the hidden representation vector h that’s been masked, based on the given input x. Formal: We can also find the correct answer by looking at the hidden representation vector h that’s been masked, based on the given input x. Formal: Formal: The whole idea of combining multiple-choice questions with multiple answers is kind of like what we do with multi-choice questions. Formal: Formal: The whole idea of combining multiple-choice questions with multiple answers is kind of like what we do with multi-choice questions. Formal: Formal: In this paper, we’re introducing a new multi-choice question generation task. We train a model using question pairs from different sources, like Wikipedia, news articles, and forum posts, and use the first question in the set to generate the next question. This new task helps us get the answer we’re looking for in just one go. Formal: Formal: Formal: We created a dataset that includes question-answer pairs. We ran the dataset through three different models, each focusing on different aspects of the input data. Formal: Formal: After training, we used this data to test how well our model could handle multiple-choice questions. Formal: Formal: Formal: The results from this test show that our model works well. Formal: Formal: Formal: The results from this test show that our model works well. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D13",
        "filename": "D13-1179.json"
    },
    {
        "casual_text": "For our next steps, we're thinking of focusing on three main areas. First, we want to work on improving how we collect data. Right now, we're using screenshots and OCR, which can mess things up with lots of non-text stuff and broken-up text due to weird webpage designs. Second, we'd like to get more people involved in our studies. And third, we're curious to see how our word2vec results stack up against newer models like BERT (Devlin et al., 2019). BERT has been doing really well across a bunch of NLP tasks, better than models with static word embeddings. \n\nUsing BERT for language modeling isn't straightforward because of its bi-directional setup and masking, but Salazar et al. (2020) recently figured out how to get prediction values from BERT and similar models trained with masking loss. BERT's subword representations might also help clean up OCR errors, especially when only a few letters are wrong. \n\nThe downside is, with our current corpus sizes of 300/500K tokens, it's not clear if we have enough data to properly handle all of BERT's parameters. One idea is to use a BERT model that's already been trained on a huge corpus and then fine-tune it with our smaller datasets.",
        "formal_text": "For future work, we would like to proceed in three directions. First, we would like to improve the collection procedure: the corpora collected via screenshots and OCR contain a high number of artifacts stemming from non-textual material, as well as non-contiguous texts as a result from complex webpage layouts. Second, we like to increase the number of participants in future studies. Third, it would be interesting to compare our word2vec results with more recent contextual embeddings such as BERT (Devlin et al. , 2019), which have been shown to achieve better performance across a wide range of natural language processing tasks than language models with static word embeddings. While it is nontrivial to use BERT's bi-directional architecture and its masking mechanism for language modelling tasks, Salazar et al. , (2020) have recently shown how to obtain prediction values for BERT and other architectures trained with masking loss. Subword representations as used in BERT may also help to compensate OCR-based errors, when only a few letters have been falsely recognized. On the downside, it is questionable whether the present corpus sizes of 300/500K token are large enough to obtain reliable estimates for the large number of BERT's parameters. A potential solution is to rely on a BERT model pre-trained by a large corpus, and to use the individual corpora to fine-tune the language model.",
        "GPT2_formal_text": ".py -f ctx.txt data/treebank/train.txt, you can find the original corpus and the pre-trained models over here: https://github.com/facebookresearch/bert-base/tree/master/data/treebank/train.txt. Formal: The main aim of this project is to figure out how well a pre-trained model can understand and generate natural language, even when the inputs are messy and the model can't directly connect the text with the image. By doing this, we can create a dataset of caption examples that are different from each other, which we call a noisy dataset. Our method involves checking the model's performance by seeing if it can predict the next word based on the image. We tried this method on a bunch of image captioning datasets and found that it works really well. Formal: We collected and labeled three different datasets: Image-Based Captioning Dataset (IBC), The CoNLL-2012 Image Captioning Dataset (ICCD), and the One Billion Word dataset, which has 10 million Chinese words and 1000 million English words. Formal: We used a top-of-the-line BERT-base model (base-uncased) with the image encoder and the word embedding layer set to 512. We ran the model on an NVIDIA GTX 1080 Ti GPU (10GB) for ten epochs with a batch size of 128. Formal: To see how much the model understands and generates, we used the OpenNMT model to extract both the title and the caption, which we then used to train the model. We picked the model that did the best on the development set for training. Formal: The results show that our method can help the model understand and generate captions that are different from each other. We also created a system to clean up the captions generated by the model. Formal: The main goal is to give the model enough context to generate captions that are different from the ones it already knows. We tested different settings for the hyperparameters to see how much each one helps improve the model's performance. Formal: We ran our model on the Image-Based Captioning dataset and then used the tokenization method from the CoNLL-2012 dataset to create a clean caption dataset. We also applied our method to the One Billion Word dataset and noticed that it worked better than the basic model. Formal: We tested our method on the Image-Based Captioning dataset",
        "directory": "cogalex",
        "filename": "2020.cogalex-1.1.json"
    },
    {
        "casual_text": "We showed that our approach works well across different languages and test sets, and it even improved how pronouns were translated. While we mainly focused on pronouns, the fine-tuning method we used isn’t limited to just that—it can also fix other types of errors in machine translations, like named entities or rare words. For future projects, we plan to test out how this method can be applied to other kinds of issues too.",
        "formal_text": "We demonstrated the effectiveness of our methods on different languages and testsets, also reporting improved pronoun translations. Although we focus on pronoun translations, our fine-tuning method is generic and can be used to correct other kinds of errors in machine translations, like named entities or other rare words. In future work, we will explore other such applications of our proposed methods.",
        "GPT2_formal_text": "ing, and the cross-attention_outputs methods used in the cross-attention model. Formal: • Interference-based masking: We pick the best possible representation for each input token at every step to make sure the prediction of the next token doesn't change based on the input. Formal: • No-attention masking: Instead of just using the output, we use a global variable to keep track of the current hidden state and its relation vectors. Formal: • Unsupervised training: We train the model without any supervision using a mix of labeled data and the unlabeled data generated by the model. Formal: • Multi-task learning: For each task, we combine the features from two tasks to predict the output, which helps us understand how the two tasks are related. Formal: • Label attention: We use the output of each task's token to pick out the top n features that make up the output. Formal: • FastText text embedding: We grab the most relevant words from the input to generate the text embedding, which helps us learn how to represent the input. Formal: • Transformer text embedding: We use the outputs from both tasks to get the text embedding, which we then use as the input for the transformers. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.177.json"
    },
    {
        "casual_text": "In this case, Lake and Baroni (2018) came up with a straightforward experiment to check how well machine translation handles compositionality. Chen et al. (2020) and Li et al. (2019b) also looked into this. They created a new word called \"dax\" and used a simple pattern of sentence pairs in their training data, like \"I am daxy\" and \"je suis daxiste\". But for the test set, they used different patterns. The problem is, their test set only had 8 sentences, which isn’t much. Raunak et al. (2019) noticed a dip in performance when dealing with concatenated source sentences. Fadaee and Monz (2020b) played around with the source sentences by removing adverbs, swapping numbers, adding words that keep the syntax intact (like \"very\"), and changing the gender, and found some weird translation issues. Unlike these studies, we’re using a compound translation error rate to measure how well NMT handles compositionality.",
        "formal_text": "In this respect, Lake and Baroni 2018propose a simple experiment to analyze compositionality in MT, followed by Chen et al. (2020) and Li et al. (2019b). Specifically, they introduce a novel word \"dax\", and their training data contains a single pattern of sentence pairs (e. g. \"I am daxy\", \"je suis daxiste\") while the test set contains different patterns. However, their work is limited in that there are only 8 sentences in the test set. Raunak et al. (2019) observe a performance drop on a dataset of concatenated source sentences. Fadaee and Monz (2020b) modify source sentences by removing adverbs, substituting numbers, inserting words that tend to keep syntax correct (e. g. \"very\"), and changing the gender, and find unexpected changes in the translation. In contrast to these studies, we quantitatively measure compositionality of NMT under compound translation error rate.",
        "GPT2_formal_text": "= { \"user_id_1\" , \"user_id_2\" }  \n\nYou can also calculate a tf-idf similarity between any two items. This similarity is then used to figure out the overall similarity score for the whole set of recipes. Formal: The entropy of the TFIDF value for a sequence (w) is calculated by multiplying the TFIDF similarity (s) for the whole set (W) by the TFIDF similarity for each recipe (q) in that set (c). Formal: The entropy of the TFIDF value for a sequence (w) is calculated by multiplying the TFIDF similarity (s) for the whole set (W) by the TFIDF similarity for each recipe (q) in that set (c). Formal:  \n\nThe model can be updated using a greedy search algorithm that picks the best value of (w) for each item in a set (c). Formal: The entropy of the TFIDF value for a sequence (w) is calculated by multiplying the TFIDF similarity (s) for the whole set (W) by the TFIDF similarity for each recipe (q) in that set (c). Formal:  \n\nThe model can be updated using a greedy search algorithm that picks the best value of (w) for each item in a set (c). Formal: Formal:\n\nQ: What's the TFIDF similarity for the whole set (W) compared to the recipe (q) in that set (c)? Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.368.json"
    },
    {
        "casual_text": "Here’s everything from EFCAMDAT for C1 and C2 combined, focusing on scores above 90%, after removing the ones used to calculate divergence scores. We split the whole dataset into eleven smaller groups: 0-9, 10-19, and so on, up to 90-99, with 100 as its own separate group. This was done for each of the three levels: beginner, intermediate, and advanced. \n\nEFCAMDAT has a lot of documents with high scores, but not as many with lower ones. For some of the lower score ranges, there weren’t 70 documents available, so we just used whatever was there. None of these documents were part of the production corpus used to create the language profile.",
        "formal_text": "This is the whole of EFCAMDAT for C1 and C2 combined for scores above 90% after excluding those that were used for calculating the divergence scores. 8 We divided the whole corpus in eleven truncated ranges: from 0 to 9, 10 to 19 etc. , up to 90-99, and 100 as a separate range, for each of the three classes (beginner, intermediate, and advanced). The EFCAMDAT contains lots of documents evaluated with higher notes, but not so many on the lower side. For some of the lower score ranges, there was not 70 documents, so we used all those available. None of these documents were present in the production corpus that was used for developing the language profile.",
        "GPT2_formal_text": "ed(base, data_d) = input_dim + attention_shape(data_d) + softmax(1 - cosine(z)) ∈ R d×d. The probability of a word being chosen as an input embedding is estimated using cross-entropy loss. Formal: The f(m) thing can be written as P(w|m) = P(w|m) + P(m), where P(w|m) is the softmax function applied to the input embedding w. Formal: The f(m) thing can be rewritten as P(w|m) = P(w|m) + P(m), where P(w|m) is the softmax function applied to the input embedding w. Formal: The cross-entropy loss is calculated like this: Formal: The cross-entropy loss is calculated like this: Formal: The cross-entropy loss is calculated like this: Formal: The cross-entropy loss is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C18",
        "filename": "C18-1294.json"
    },
    {
        "casual_text": "So, the model has a pretty high Self-BLEU score, which means it’s more likely to give the same response to different input reviews compared to models trained on more data. On the other hand, it does better in other areas like chrF-tgt, chrF-src, DIST-1, and Uniq., and the responses are also longer.",
        "formal_text": "That said, the relatively high Self-BLEU score indicates that this model tends to generate the same response to different input reviews to a greater extent than those trained on more data. chrF-tgt ↑ chrF-src ↑ DIST-1 ↑ Self-BLEU ↓ Uniq. ↑ Len ↑",
        "GPT2_formal_text": ", and a teacher model p_train that predicts the label for each instance. This can be done with the help of data augmentation. The text-to-image generation (T2I) model was introduced by Berant et al. in 2018. This model creates both the T2I image and the caption by creating a continuous sequence of vectors. The output from this T2I model is a fixed-size vector with dimensions d_i, which is a matrix of size d_i. The input for the model is a sequential image encoder x_i, a response encoder y_i, a text encoder h_i, and a sentence encoder s_i. We’re going to use the hyperparameters from this model in our experiments. We’ve already talked about the T2I model in Section 2. In this paper, we’re not diving deep into how the model learns its parameters. Instead, we’re focusing on the process of generating text. For the generation part, we’ll use the T2I model to generate text from an input image, then feed this text into a network for the response generation. The goal here is to train the model to create text that includes some specific information (like the actual image), while also keeping the diversity high. Formal: Basically, the goal here is to create a text that includes the main info from the image, without changing the original meaning or altering the order of sentences. Formal: The generation part is a straightforward process where we generate the main sentence in the image, then use the network to generate the response. Formal: The training process for T2I and T2R models is pretty similar. The model tries to learn representations that represent the two things it’s generating (text and image). Formal: The input to the T2I model is just the word embedding x_i. Formal: The model learns two representations, h_i and h_i^T, which are basically the embeddings of the image and the response, respectively. Formal: The training of the model begins with a regularization parameter λ, which adjusts the learning rate α to help it learn better. Formal: The training for T2R models starts with a softmax classifier p_train. Formal: To make sure the model can correctly identify the correct image, we use a discriminative classifier called p_divergence to",
        "directory": "ecnlp",
        "filename": "2022.ecnlp-1.15.json"
    },
    {
        "casual_text": "The NEL performance on the In Media Res corpus is way worse compared to traditional datasets. This wasn't a surprise since adding creative works to the mix introduces a lot of errors. As expected, the tools struggled to tell the difference between a character and the franchise with the same name, and they didn't perform well on the YouTube influencer part of the corpus either. The influencer section is particularly tricky because some of the stuff mentioned there—like YouTube channels that got shut down or early work from famous influencers—are NIL, meaning they’re not in Wikipedia or related knowledge graphs like DBpedia and Wikidata.",
        "formal_text": "NEL performance on the In Media Res corpus is considerably lower than the results obtained on traditional data sets. This was expected due to the large amount of errors introduced by adding creative works to the corpora. Also as expected, the tools were not able to distinguish well between a character and the franchise that bears its name or offer good results on the YouTube influencer partition of the corpus. The influencer partition is especially difficult due to the fact that some of the works mentioned there (e. g. , YouTube channels that were shut down or early gigs for famous influencers) are NIL (i. e. entities that have not been included in Wikipedia or related KGs such as DBpedia and Wikidata).",
        "GPT2_formal_text": "ed_x. Formal: The main idea behind this attention-based approach is to give each word in the input a vector that shows its meaning, kind of like a summary. We take the hidden state of the word and use it to create the vector for the word. Formal: We also use a cross-entropy loss function to measure how well the model is doing. Formal: The second part of the process involves generating the answer sentence using the model's output. We start by using a softmax classifier to create the answer embedding vector, which we get by adding together a bunch of hidden states from the attention part. The embedding of the answer is just the product of two vectors, s(e) and s(e', r), where s(e) is the representation of the answer, and s(e', r) is the representation of the question. Then, we put all this into a multi-layer perceptron and calculate the classification probabilities for each possible answer span. Formal: Next, we use a cross-entropy loss to train the model. Finally, we pick the answer span with the highest probability for the question and use it to generate the answer sentence. Formal: We're using the same setup as the original paper by Filippova et al. (2018), which uses the same question embedding and cross-entropy loss as our method. Formal: The best scores we got were 0.66 and 0.73, which shows that our method can produce a better summary than the current top model. Formal: We also tried out our method on the dataset from the CoQA challenge (Wang et al., 2018), and the results are in Table 1. Formal: In this paper, we focus on summarizing argumentative questions from two sources: WikiQA and TREC. Formal: WikiQA is a dataset from Wikipedia that has document summaries from different areas, and TREC is a test-bed for extractive question answering (QA) with multiple-choice questions (Chen et al., 2016). We picked WikiQA because the test set has way more questions than the training set, which is usually more relevant and closely tied to the document. Formal: For TREC, we used the same training set and test set as the original paper by Filippova et al. (2018), which has 7,764 questions and 17,932 answers. Formal: Table 1 shows how our",
        "directory": "conll",
        "filename": "2020.conll-1.28.json"
    },
    {
        "casual_text": "• If two arguments don’t have any words in common or similar meanings, it’s tough to pair them up. In this case, our ISRG method, which relies on matching words, just doesn’t work. Plus, even a pre-trained model struggles to connect these kinds of argument pairs.  \n\n• Sometimes, our model only picks out a couple of key sentences instead of the whole argument. Other times, it lumps together multiple arguments into one big chunk. This happens because we treat both AM and APE as sentence-level tasks. Since argument boundaries can be all over the place, the model often gets confused and mixes things up.",
        "formal_text": "• It is difficult to extract argument pairs if there are no co-occurring or semantically similar words in two arguments. In this scenario, our proposed ISRG based on co-occurring words cannot provide valid information. Also, it is hard for the pre-trained model to capture the association between such argument pairs. • In some cases, our model identifies only a few important sentences instead of a complete argument. However, in some other cases, multiple consecutive arguments are identified as one argument. The reason is that we frame both AM and APE as sentence-level sequence tagging tasks. For such a task, the boundaries of arguments are often diverse and difficult to determine, so the model often misidentifies them.",
        "GPT2_formal_text": "= { u, v } with 1 ≤ i < n. If the answer u is a correct one, we can use the expected value of the sentence-level attention vector v to figure out the average of the attention scores for each pair of tokens, which gives us the attention weights. Formal: We use the training examples from the adversarial dataset to train the model, and the adversarial examples are used to test how well the model is doing. Formal: We train the model using the adversarial examples, using the weights that are learned during training on the adversarial examples. Formal: To keep track of how much each aspect contributes to the overall prediction, we use the attention weights for each aspect as the prediction for the whole prediction. Formal: We combine the predicted aspect vector v_a, ..., v_h with the attention weights for all the tokens, making sure to keep the aspect vectors in order (like v_a, ..., v_h). Formal: The attention weights are calculated based on the prediction for a specific aspect, based on the representations from the models that were trained on that aspect. Formal: Figure 3 shows how we calculate the attention weights for each aspect. Formal: We train the model using the adversarial examples, using the weights learned during training on the adversarial examples. Formal: We train the model using the adversarial examples, using the weights learned during training on the adversarial examples. Formal: We train the model using the adversarial examples, using the weights learned during training on the adversarial examples. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.319.json"
    },
    {
        "casual_text": "For beginners or regular users, there’s a straightforward interface where you can just type in a text query without needing to specify anything else. The results show a list of matches with small images (thumbnails) of the video clips related to your query, along with the text from those clips. Clicking on a match lets you watch the full video clip. If you want more details—like what an advanced user might see—you can click the \"advance information\" link. \n\nThe way the results are sorted is kind of sneaky. It’s based on how much the words in the transcript or the visual labels connect to your query across different media. The more connected they are, the higher up they appear in the list. The goal of the COSMOROE approach is to make all this happen automatically—processing video and audio files to figure out these connections and make searching through digital video archives smarter.",
        "formal_text": "For the novice or general user, a simple interface is provided that allows the user to submit a text query, with no other specifications. The results consist of a hit list with thumbnails of the video-clips related to the query and the corresponding transcribed utterance. Individual hits lead to full viewing of the video clip. Further details on the hit, i. e. information an advanced user would get, are available following the advanceinformation link. The use of semantic relations in multimedia data, in this case, is hidden in the way results are sorted in the results list. The sorting follows a highly to less informative pattern relying on whether the transcript words or visual labels matched to the query participate in cross-media relations or not, and in which relation. Automating the processing of audiovisual files for the extraction of cross-media semantics, in order to get this type of \"intelligence\" in search and retrieval within digital video archives, is the ultimate objective of the COSMOROE approach.",
        "GPT2_formal_text": ". This helps the model figure out the best way to generate text in the next step. Formal: We also keep track of the recall values for each attention head during training. Formal: The model takes the output from the head it’s working on and turns it into a sentence. Formal: In the next step, we use the label embedding to figure out the probability distribution for the next word in the document. Formal: We calculate the loss for this sentence selection using the output from the last attention head and the Nth hidden state in the input sequence. Formal: We fine-tune the model for 100,000 steps, using a learning rate of 0.1 for the Nth hidden state and 0.01 for the E1th hidden state. Formal: To keep the model from getting too focused on the same sequence, we remove it from the dataset and re-implement it with a different setup. Formal: We use the best model trained on the last dataset to generate the next sentence. Formal: Finally, we run this whole process 100,000 times with a learning rate of 0.1. Formal: The results for the two datasets we used are shown in Figure 1. Formal: We also keep track of the recall values for each attention head during training. Formal: We calculate the loss for this sentence selection using the output from the last attention head and the Nth hidden state in the input sequence. Formal: To keep the model from getting too focused on the same sequence, we remove it from the dataset and re-implement it with a different setup. Formal: We use the best model trained on the last dataset to generate the next sentence. Formal: Finally, we run this whole process 100,000 times with a learning rate of 0.1. Formal: Formal: The results for the two datasets we used are shown in Figure 1. Formal: We also keep track of the recall values for each attention head during training. Formal: We calculate the loss for this sentence selection using the output from the last attention head and the Nth hidden state in the input sequence. Formal: To keep the model from getting too focused on the same sequence, we remove it from the dataset and re-implement it with a different setup. Formal: We use the best model trained on the last dataset to generate the next sentence. Formal: Finally, we run this whole process 100,000 times with a learning",
        "directory": "E09",
        "filename": "E09-2014.json"
    },
    {
        "casual_text": "So, let's look at a couple of sentences, S and T, from a parallel text. S is the source sentence with I source words, and T is the target sentence with J target words.",
        "formal_text": "To this end, we consider a pair of sentences (S, T ) from the parallel corpus, where the source sentence S is made up of I source words and the target sentence T is made up of J target words:",
        "GPT2_formal_text": "= {s1, ..., s|T}. You can figure out the starting and stopping nodes using the CKY algorithm, which takes O(|T|log p(T|)) time to run. This algorithm has a complexity of O(|T|log p(|T|)) for a case of |T| = |t| = |S|. Formal: The decoder uses the decoding model from the encoder and does a forward pass, where it uses an attention mechanism with a weighted sum of attention weights. Formal: The decoder also has a backward pass, where it uses a special type of attention mechanism called a bidirectional LSTM. Formal: To calculate the decoding loss, it takes O(|T|log p(|T|)) time, where |T| is |S|. Formal: Lastly, to calculate the generation loss, it takes O(|T|log p(|T|)) time, where |T| is |S|. Formal: Formal: If you have a parse tree with T tokens and a sequence of target tokens, and you want to figure out the generation probabilities for each token, you can use the template decoder. It uses the template-based decoder for the task. Formal: The template decoder is similar to the one used in the model, but instead of using a BiLSTM with a convolutional neural network, it uses a pre-trained BiLSTM to do the generation. Formal: Formal: The template decoder is built using a bidirectional LSTM, which is a type of attention mechanism that combines forward and backward attention. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2012.eamt-1.62.json"
    },
    {
        "casual_text": "To get a grasp on how global inheritance works in DATR, we need to talk about the idea of global context. Let's say we're trying to figure out the value of \"Dog: (sing)\" in a DATR theory. At first, the global context is just the pair (Dog, sing). From the theory, we know that \"Dog: (sing)\" gets its value locally from \"Noun: (sing)\", which in turn gets its value globally from the path \"(root)\". To figure out what \"(root)\" means, we look at the global context to find the current global node, which is \"Dog\". Then, we evaluate \"Dog: (root)\" to get \"dog\", which is what we needed.",
        "formal_text": "To understand the way in which global inheritance works, it is necessary to introduce DATR's notion of global contea't. Suppose that we wish to determine the value of Dog: (sing) in the exalnt)le DATRc; theory. Initially, the global context will be the pair (Dog, sing), bS\"om tile theory, the value of Dog: (sing} is to be inherited (locally) fl'om Noun: (sing), which ill turn inherits its value (globally) from the quoted path \"(root)\". rio evaluate the quoted path, the global context is examined to find the current global node (this is Dog) and the vahle of \"(root)\" is then obtained by evaluating Dog: (root), which yields dog as required.",
        "GPT2_formal_text": "_set, which gives the best possible score for the attention weights. If you want more info, check out the original paper. Formal: This method is pretty straightforward and can be used with any kernel function you want. Formal: We use the shared-private binary optimization problem (shoutout to Shi et al., 2014) for the attention weights. The problem is set up like this: Formal: In this case, we're dealing with an entity's description. If the entity description is labeled, the relation path is usually more likely to have the relation type we're interested in. In this case, the label of the entity description is pretty important. Formal: A common issue in knowledge graph embedding is that there's not enough attention on the right edge when there are multiple relation types. For instance, if you have two entity descriptions and you know the label of the entity, you might want the path to connect the entity with the description of the relation type you're interested in. Formal: To deal with this, we add an edge-aware attention mechanism, which learns how to focus on the right edge of the attention weights. Formal: We also use a smart attention mechanism to adjust the weights based on the document, specifically to the node or edge that has the closest match with the entity's description. Formal: To pick the right edges, we use the adaptive loss function. The idea is that the entity description should be tied to a specific node in the knowledge graph. Formal: Here's how our model is set up. Formal: We use the cross attention model (shoutout to Jiao et al., 2015) with the hidden states from the attention mechanism, as explained in Section 5. Formal: For the cross attention model, we fine-tune the model using the training set of the target domain. Formal: The cross attention model takes the attention weights from the embeddings of the target node and the embeddings of the entity description and assigns them to the embedding of the entity description. Formal: For the cross attention model, the embedding of the entity description is then used to calculate the cross attention weights. Formal: After fine-tuning, we use the cross attention model again to calculate the cross attention weights for the target domain. Formal: The cross attention model is trained using a cross-entropy loss function (shoutout to Jiao et al., 2015). Formal: We can tweak the cross-entropy",
        "directory": "C96",
        "filename": "C96-2109.json"
    },
    {
        "casual_text": "Check out Table 9 for the results. Basically, the multi-embedding methods keep beating the single-embedding ones when it comes to training, validation, and testing perplexity. The difference isn't as big as with similar words, though. Also, the gap gets wider when the nouns are really different. We think that when the word embeddings for nouns are farther apart, the next word distribution tends to be more complex, and multiple embeddings do a better job of capturing that.",
        "formal_text": "The results are presented in Table 9. In terms of the training, validation, and testing perplexity, multi-embedding approaches consistently outperform the single-embedding baselines, though the margins are smaller than those from the analogous words. Moreover, the improvement gap is larger when the nouns are dissimilar. We hypothesize that as the word embeddings of nouns become further away from each other, the next word distribution is more likely to be multi-mode and thus could be better captured by multiple embeddings.",
        "GPT2_formal_text": "= { id_t, id_m, id_l }, { id_s, id_t }, ..., { id_s, id_t, id_t }). Formal: On the flip side, if you have a sentence with a noun phrase or a phrase that could be a noun or a phrase, you can figure out its ID using this formula. Formal: For each node in the sentence, you look for the longest matching part in the whole sentence. Formal: For each noun phrase or phrase in the sentence, you grab the longest matching word or phrase from the whole sentence. Formal: Finally, for each possible sentence structure, you pick the best one by considering all the possible options. Formal: So, if you have a source sentence x, a target sentence y, and a bunch of possible sentences z = {z1, z2, ..., zn}, you can figure out the ID of the best sentence by checking all the sentences in the sentence together and picking the one that matches the longest. Formal: Formal: Here's how you calculate the longest matching part in the whole sentence: Formal: The longest matching word or phrase from the whole sentence is the longest matching part in the target sentence. Formal: The longest matching word or phrase in the target sentence is the longest matching part in the source sentence. Formal: The shortest matching part in the target sentence is the longest matching part in the source sentence. Formal: And the shortest matching part in the source sentence is the longest matching part in the target sentence. Formal: Formal: Formal: Formal: The combined length of all the words or phrases in the source sentence is the length of the longest matching word or phrase. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.554.json"
    },
    {
        "casual_text": "On the essay datasets, Target Embedding (learning) works really well in the optimistic scenario, especially for most scores. The hybrid approach doesn’t really show much improvement, though. This might be because the dataset is pretty small, so it’s hard to tell if there’s any real difference. In the pessimistic scenario, Premise Target (ranking) seems to be a better fit. The lower scores for Essay Conclusions could be due to having fewer premises (check out Figure 6), which makes it harder to pick a good conclusion target from the available premise targets. Figure 7 shows three examples of premise targets from the datasets, along with the actual conclusion target and the ones our approaches came up with.",
        "formal_text": "In the optimistic scenario on the essay datasets, Target Embedding (learning) is strongest for most scores. The hybrid approach hardly achieves any improvement. Due to the small dataset size, no significance was found, though. In the pessimistic scenario, Premise Target (ranking) seems more suitable. The lower scores on Essay Conclusions can be attributed to the low number of premises (see Figure 6 ), which makes finding an adequate conclusion target among the premise targets less likely. Figure 7: Three examples of premise targets from the datasets, the associated ground-truth conclusion target, and the conclusion targets inferred by our approaches.",
        "GPT2_formal_text": "layer. We swap out the original attention parameter h (or embedding) in the input with the attention mask h, which we set to 1 for this paper. Formal: We’ve taken all the text and image captions we’ve gathered and thrown them into a single big training set called T. Formal: We use an encoder-decoder model (like the one by Vaswani et al., 2017) to turn the text and image captions into vectors. Each word gets turned into a hidden vector, which we call h_w_t. The vectors of the captions we’ve generated so far, h_w_1 to h_w_n, are all d-dimensional. Formal: The encoder uses a bidirectional GRU (bi-GRU) for the attention mask. The output of the GRU is h_c, which tells us the attention mask for the first word in the caption. After that, we send this h_c through an LSTM layer to get the hidden representation of the entire caption. Formal: To make it easier to compare our model to other methods, we randomly pick some of the selected captions to use as our test set. Formal: If we’ve got an image caption (c_i) along with its image caption (c_i') and its generated caption (e_c), we mix them together as the input for our model. Formal: For our model, we pick the caption with the highest attention mask h_m. Formal: The input for the model is a sequence of image words, x_i, which we give to the encoder. Formal: To get the hidden representation of the image caption, we use the output from the GRU layer in the encoder. Formal: For the LSTM layer, we use the output from the Bi-GRU layer in the encoder. Formal: To get the output word embedding for the caption, we use the output from the LSTM layer in the decoder. Formal: For the LSTM layer in the decoder, we use the output from the Bi-GRU layer in the decoder. Formal: Formal: The decoder uses the output from the Bi-GRU layer in the decoder. Formal: The generated caption is created by combining the attention mask h_c with the attention mask h_c', and the",
        "directory": "acl",
        "filename": "2020.acl-main.399.json"
    },
    {
        "casual_text": "InLäubli et al. (2018) used the Sign test instead of the Wilcoxon rank sum, and it turned out to have similar statistical power for the effect size they were looking at. On the other hand, Toral et al. (2018) took a different approach—they only had document context for the source document, not for the MT output.",
        "formal_text": "InLäubli et al. (2018) the Sign test was used as opposed to Wilcoxon rank sum and has similar statistical power for such an effect size. 3 This approach is not that ofToral et al. (2018), where document context was only available in for the source input document as opposed to MT output document.",
        "GPT2_formal_text": "= {get_id(), ..., get_constraints_id_tok}, which is the same as the instruction set we set aside. Formal: In the past, it was pretty straightforward to use language resources for different tasks. But now, thanks to stuff like semantic role labeling (SRL) (Bordes et al., 2011), constituency parsing (SRL) (Che et al., 2011), and dependency parsing (Parsing) (Ganin et al., 2015), the amount of training data needed for each task is way less. Plus, the methods that use external resources—like adding them to a pretrained model (Li et al., 2016; Tenney et al., 2017; He et al., 2018b; He et al., 2018a) or tweaking it for a specific task (Li et al., 2016; Tenney et al., 2017; He et al., 2018a)—can work with a much smaller set of instructions. Formal: Unlike the methods that use external resources, our approach combines the knowledge we get from the knowledge bank with the internal structure of the instruction set. This method is different from most of the ones that use external resources because it builds on the knowledge already in the knowledge bank. Formal: While the results for the CoNLL-2012 shared task show that external resources are helpful, recent studies have been mixed up about how much they actually help. For example, some people think external resources only help with fixing mistakes in syntax (Wu et al., 2020), while others think they actually help with understanding the instruction structure (Jia et al., 2020). Formal: We tested our model against a few other methods that use external resources, like Seq2Seq (Kocisky et al., 2011) and BERT-COREF (Bordes et al., 2020). They all work with a sequential format, where the output for each input instruction is a sequence of characters. Formal: We created two types of constraints, SRL constraints and dependency constraints. Formal: The SRL constraints are used in an experiment to figure out how well a grammar can generate instructions. Formal: In a related experiment, we used external resources for the CoNLL-2016 shared task on SRL. Formal: The dependency constraints are used in a study to learn how much better a grammar can learn. Formal: The CoNLL-2016 shared task on dependency parsing is kind of like the CoNLL-2016 shared task on",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.6.json"
    },
    {
        "casual_text": "These issues come up because LDA’s math doesn’t really focus on capturing how words relate to each other in terms of meaning. A cool next step could be tweaking the parts of the model that use probabilities, training it on a good dataset, and creating a vector-based model that’s tailored specifically for combining words into meaningful phrases. We’re also thinking about exploring more advanced models that consider the structure of sentences. Our experiments so far, where we mixed these models with a parser, show that blending syntax and semantics gives us a lot of potential for improvement.",
        "formal_text": "These shortcomings arise from the mathematical formulation of LDA, which is not directed at modeling the semantic interaction between words. An interesting future direction would be to optimize the vector components of the probabilistic model over a suitable training corpus, in order to derive a vector model of semantics adapted specifically to the task of composition. We also plan to investigate more sophisticated composition models that take syntactic structure into account. Our results on interpolating the compositional models with a parser indicate that there is substantial mileage to be gained by combining syntactic and semantic dependencies.",
        "GPT2_formal_text": "ed(u_i) = cosine(u_i) + tanh(u_i) * tanh(u_i) + ... + cosine(u_i) * tanh(u_i) + a_i.\n\nYou can find more details in Appendix A.2.\n\nAs mentioned in Section 2.1, some researchers have started looking into how to pull out knowledge from text automatically. For instance, Berant and his team (2016) noticed that a lot of computer-generated text has knowledge that's not there in the text itself. They did a bunch of experiments using machine learning (ML) to figure out the specific bits of knowledge in the text. However, these methods struggle to create text that really connects with the idea being described.\n\nFor instance, if the same kind of knowledge is mentioned in multiple sentences, it's not always clear which sentence it's from. To fix this, they created a new task: figuring out which sentences the model was trained on by looking at the knowledge it picked up. They showed that by using a method that considers the context, it's possible to create text that accurately shows the relationship between the two ideas being described.\n\nAnother way to figure out the connection between ideas is to use a knowledge graph (KG) as a guide. This method builds on the idea of a knowledge graph (KG) by looking at the connections between entities, which is way more helpful. For instance, this method can help create text that clearly explains how the two entities are related. Formal: A GCN can group knowledge into meaningful subgraphs for understanding the relationship. Formal: A GCN can group knowledge into meaningful subgraphs for understanding the relationship. Formal: A GCN can group knowledge into meaningful subgraphs for understanding the relationship. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D09",
        "filename": "D09-1045.json"
    },
    {
        "casual_text": "In terms of how much work it takes to get this done, labeling counterfactuals is basically the same as labeling regular examples. So, no extra training for annotators or separate systems are needed. On the other hand, Kaushik et al. (2020) had to set up two different crowdsourcing tasks to create and label counterfactuals. Plus, it’s way less effort for annotators to just evaluate examples instead of making them. Kaushik et al. (2020) say it took about 2 minutes per counterfactual for NLI tasks before checking the quality, but for us, it was only 10 seconds per counterfactual on average. Even after we cleaned up the data (kicked out bad annotators and removed unclear counterfactuals), our time per NLI counterfactual was around 36 seconds (this is what we used in Table 5).",
        "formal_text": "terms of implementation effort, the process of just labeling counterfactuals is the same as labeling original examples, such that no additional annotator training or separate pipelines are required; in contrast, Kaushik et al. (2020) set up two separate crowdsourcing tasks for creating and labeling the counterfactuals. Further, annotator effort is much lower, as evaluating examples is easier than creating them - Kaushik et al. (2020) report an average of ≈2 minutes per NLI counterfactual prior to quality validation, while our median time was 10 seconds per counterfactual. Even after our quality validation (removing noisy annotators, disregarding non-fluent counterfactuals), our rate for NLI is ≈36 seconds per counterfactual (used in Table 5 ).",
        "GPT2_formal_text": "= [] for e in o_p; e:  \nb = tf.get_output(e)\no_p = [o_p1, ..., o_pN]  \na_p = [a_p1, ..., a_pN]  \na_p1 = [a_p111, ..., a_pN1]  \na_pN = [a_pN1, ..., a_pN]  \n\nThe idea behind this is that the probability of a phrase pair (p1, p2) given a specific context (c) is calculated by multiplying the probabilities of p1, p2 given the context c. The main goal here is to make sure that phrases in the training data are similar to the context by using a beam search, which is like a computer search. The probability p for a phrase pair is calculated as: Formal: We use a part-of-speech (POS) tagger (like the one Bosc and Neubig, 2004, used) to figure out the chances of a phrase pair (p1, p2) and the word w1, w2 happening based on the context. Formal: Formal: To make sure the word w2 doesn’t repeat, we also check the word w1 that’s next to it. Formal: Formal: We also check if the word w1 isn’t near w2. We do this by checking the next word in the pair, w1, that’s not next to w2. If w1 is closer to w2 than w1 is to w1, then w1 shouldn’t be next to w2. Formal: The context c is a set of stuff like the context words, which we call c_i, and the input words, X_i. Formal: In this setup, the context and the input words are already analyzed to create a probability p, which we call p_c. Formal: So, the context words (c_i) and the input words (X_i) are already known to us. Formal: Formal: Lastly, we only check if the word w2 is near the word w1, w2, and vice versa. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "We tried out a few different versions of our model as a starting point: BERT-SeqWD, which only looks at coreference scores based on entity or event representations without any cross-document linking, and BERT-SeqXdoc, which calculates coreference scores across documents but doesn’t use candidate composition. So, BERT-SeqXdoc just compares the query mention to all antecedent mentions from previous documents, instead of comparing it to clusters created with candidate composition. We also tested our model, SeqXdoc+IC, with and without adaptive pre-training for both event and entity coreference. For entity coreference, we compared our model against a few others:",
        "formal_text": "We experiment with the following baseline variations of our model: BERT-SeqWD -computes coreference scores using only the entity (or event) representations, without any cross-document linking, and BERT-SeqXdoc -computes coreference scores across documents but without candidate composition. This means the baseline BERT-SeqXdoc computes scores between the query mention and all antecedent mentions across all prior documents, rather than between the query and the clusters computed with candidate composition. For both event and entity coreference we experiment with our model, SeqXdoc+IC with (+Adapt) and without adaptive pre-training. For entity coreference we compare against the following models:",
        "GPT2_formal_text": "ed is a meta-learning method, and it's trained on the input sequence x = [x1, ..., xI, ..., xM]. It's calculated using the log-likelihood of the correct answer and the class label y = {y1, ..., yI}. Formal: To get a good representation for the input x, we calculate a log-likelihood for the answer y using the cosine of the attention mask (a_mask) and the label y. The cosine similarity between the mask and label is calculated using the tanh function. The result from this is a parameter vector h_t, which is an n×n matrix with dimensions m×n. Formal: Formal: The attention mask (a_mask) is a learned vector, and the hidden representation (h_t) is a matrix. The cosine similarity between the vector and the label (a_mask) is calculated using the tanh function. The identity matrix (h_t) is a matrix with dimensions m×n. Formal: In the distillation setup, we also have a linear layer that predicts the attention mask. The linear layer is trained on the whole input sequence x = [x1, ..., xI, ..., xM]. After applying the softmax function to the prediction, the hidden representation of the token y is created. Formal: To train the linear layer, we set the size of the embedding dimension to 100. Formal: Finally, the final attention vector h_t is a scalar with dimensions m×n, which is then passed through a softmax layer. Formal: Here's a quick example of how this works for the sentence \"John loves Bill.\" Formal: Here's a quick example of how this works for the sentence \"John loves Bill.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.382.json"
    },
    {
        "casual_text": "EmotionPush 1: This dataset includes conversations pulled from Facebook Messenger chats, with all the private stuff taken out. The emotion categories are the same as in the Friends dataset.",
        "formal_text": "EmotionPush 1: The conversations in this dataset are from the facebook messenger logs after processing the private information. Emotion categories are the same as Friends dataset.",
        "GPT2_formal_text": "= {'base': [], 'mnemonic': [], 'parse': [], 'speech': [], 'speech_text': [], 'speech_image': [], 'speech_image_text': [], 'speech_text_text': [], 'speech_image_text_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text_image': [], 'speech_image_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text_image': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text_image': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text_image': [], 'speech_text': [], 'speech_text_image': [], 'speech_text_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text_image': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text_image': [], 'speech_text_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text_image': [], 'speech_text': [], 'speech_text': [], 'speech_text': [], 'speech_image_text': [], 'speech_text': [], 'speech_",
        "directory": "ccl",
        "filename": "2020.ccl-1.102.json"
    },
    {
        "casual_text": "We used GraphChi, a cool library for processing graphs on a single machine (shoutout to Kyrola et al. in 2012), to handle both the feature selection and feature computation parts of PRA. For the logistic regression, we went with MAL-LET's setup, including both L1 and L2 regularization (props to McCallum in 2002). To figure out what counts as negative evidence, we made a closed world assumption, meaning any (source, target) pair we found during feature computation that wasn’t labeled as positive was treated as negative.\n\nWe tweaked the parameters for our methods by doing a rough, manual grid search with cross-validation on the training data we’ll talk about later. The parameters we messed with included the L1 and L2 regularization settings, how many random walks we did for feature selection and computation in PRA, and some spikiness and restart parameters for vector space walks. Turns out, the results didn’t change much even if we fiddled with these parameters a bit.",
        "formal_text": "We perform both the feature selection step and the feature computation step of PRA using GraphChi, an efficient single-machine graph processing library (Kyrola et al. , 2012). We use MAL-LET's implementation of logistic regression, with both L1 and L2 regularization (McCallum, 2002). To obtain negative evidence, we used a closed world assumption, treating any (source, target) pair found during the feature computation step as a negative example if it was not given as a positive example. We tuned the parameters to our methods using a coarse, manual grid search with cross validation on the training data described below. The parameters we tuned were the L1 and L2 regularization parameters, how many random walks to perform in the feature selection and computation steps of PRA, and spikiness and restart parameters for vector space walks. The results presented were not very sensitive to changes in these parameters.",
        "GPT2_formal_text": ", duration_mask, and state_mask) with h in T, and e_i in E. In this paper, we create attention-based embeddings by averaging the attention scores from each input token. Formal: We take the entity embedding vector (e_e) and the topic embedding vector (e_t) from each sentence and feed them into the classifier. The main goal here is to predict the label for the entity entity e_e and the topic vector e_t. We use some baseline methods like knowledge distillation (Zhang and Yang, 2015) and cross-entropy loss (Yu et al., 2017) to calculate the attention weights for the two input words. Formal: The final weights we get are the average of the weights for all the tokens in the sentence. Formal: We compare the final word embeddings from the input and output sentences to the best embeddings from the LSTM and attention layers. Formal: We also check how well the LSTM and attention layers work together in the model. Formal: Lastly, we measure how similar the outputs are using two different methods: Krippendorff's α (Blei et al., 2003) and Gini Coefficient (Gal and Ghahramani, 2005). Formal: Finally, we test if this model can do a good job identifying topics in the topics dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D14",
        "filename": "D14-1044.json"
    },
    {
        "casual_text": "Lastly, we take a look at how this affects search and scoring, and we think there's a lot more to explore about the difference between how things are trained and how they're tested.",
        "formal_text": "• Finally, we examine the implications in search and scoring and motivate further research into the mismatch between training and testing.",
        "GPT2_formal_text": ", q, and i, which are based on the id and s of the query, respectively. Then, for each part of the query, we create something like this: Formal: The model learns to generate query responses that make sense based on the given context, just like humans do. Formal: The model has two parts: a language model (LM) and a query model (QM). The language model is trained using a bunch of text data. The query model is trained using a set of question-answer pairs, which come from the QA model. Formal: The whole process of generating the response, which includes the language model, the query model, and the QM, is shown in Figure 1. Formal: Once we have the response, we pick out the important parts and combine them into a single sentence. Formal: The model helps us understand the context of the query better, which helps us pick the right answer. Formal: Finally, we take the sentence generated by the language model and turn it into a list of concepts, which we call the query. Formal: Formal: The system we're talking about in this paper is based on the work of Rajpurkar et al. (2019). Formal: They used something called a DistilBERT base to process the input data, and then they fed the result into a Bi-GRU network. Formal: Formal: Later, they added a Sigmoid activation function to create a likelihood function to weigh the response's importance. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "aacl",
        "filename": "2020.aacl-main.25.json"
    },
    {
        "casual_text": "Okay, let me break this down in a simpler way:\n\n2. s = c (G, i)  \n   G = ⊥ ([i, i + 1], i, τG): s  \n   Init ([i, k], r, τ ): s  \n   s = c (⊥, k) + c 0  \n   IGNORE − −−− → k ([i, k + 1], r, τ ): s + s  \n   Skip-R ([i, k], r, τ ): s  \n   s = c (⊥, i − 1) + c 0  \n   IGNORE − −−− → i − 1  \n   Skip-L ([i − 1, k], r, τ ): s + s  \n   ([i, j], r1, τ1): s1  \n   ([j, k], r2, τ2): s2  \n   τ = (τ1, τ2) defined  \n   s = c r1 − → r2  \n   Arc-R [ ] ([i, k], r1, τ ): s1 + s2 + s  \n   ([i, j], r1, τ1): s1  \n   ([j, k], r2, τ2): s2  \n   τ = (τ2, τ1) defined  \n   s = c r2 − → r1  \n   Arc-L [ ] ([i, k], r2, τ ): s1 + s2 + s  \n   ([1, n + 1], r, [ ]): s = c 0  \n   ROOT − −− → r ([0, n + 1], r, [ ]): s + s\n\nAlright, let’s make this more conversational:\n\n2. s is calculated based on G and i.  \n   G is defined as ⊥ over the range [i, i + 1], with i and τG.  \n   Init sets s over [i, k], r, and τ.  \n   Then, s is updated to include c(⊥, k) and c0.  \n   IGNORE leads to k, updating s over [i, k + 1], r, and τ.  \n   Skip-R adjusts s over [i, k], r, and τ, with s now including c(⊥, i - 1) and c0.  \n   IGNORE then points to i - 1, and Skip-L updates s over [i - 1, k], r, and τ.  \n   For ranges [i, j] and [j, k], we have s1 and s2 with r1, τ1 and r2, τ2 respectively.  \n   τ is defined as (τ1, τ2), and s connects r1 to r2.  \n   Arc-R combines s1 and s2 over [i, k], r1, and τ.  \n   Similarly, for [i, j] and [j, k], s1 and s2 are defined with τ2 and τ1, and s connects r2 to r1.  \n   Arc-L combines s1 and s2 over [i, k], r2, and τ.  \n   For the range [1, n + 1], s is set to c0, and ROOT connects to r.  \n   Finally, s is updated over [0, n + 1], r, and an empty list.\n\nThis should be easier to follow!",
        "formal_text": "2. s = c (G, i) G = ⊥ ([i, i + 1], i, τG): s Init ([i, k], r, τ ): s s = c (⊥, k) + c 0 IGNORE − −−− → k ([i, k + 1], r, τ ): s + s Skip-R ([i, k], r, τ ): s s = c (⊥, i − 1) + c 0 IGNORE − −−− → i − 1 Skip-L ([i − 1, k], r, τ ): s + s ([i, j], r1, τ1): s1 ([j, k], r2, τ2): s2 τ = (τ1, τ2) defined s = c r1 − → r2 Arc-R [ ] ([i, k], r1, τ ): s1 + s2 + s ([i, j], r1, τ1): s1 ([j, k], r2, τ2): s2 τ = (τ2, τ1) defined s = c r2 − → r1 Arc-L [ ] ([i, k], r2, τ ): s1 + s2 + s ([1, n + 1], r, [ ]): s = c 0 ROOT − −− → r ([0, n + 1], r, [ ]): s + s",
        "GPT2_formal_text": ". Formal: If we get a sentence that has a pair of (s1, s2) in both the source and target languages, we can figure out the attention value for the target word using this formula: Formal: We add the attention weight vectors to the current hidden state and then use the softmax function to update the attention feature vector. Formal: The attention weight vector can be written like this: Formal: We can't predict the target language's word embedding because it depends on the context and might change depending on the target language. Formal: Instead, we use the context from the source language to help with the target embedding. Formal: To predict the target word embedding for each target sentence, we use a BiLSTM with a hidden state dimension of d. Formal: The original BERT model had a hidden state dimension of 768, but we swapped it out for a 1024-dimensional embedding. Formal: The first layer in the BERT model has 512 neurons and a hidden state dimension of 8192. Formal: We added another 1024-dimensional embedding to this BERT model and trained it using a hybrid sequence-to-sequence (seq2seq) setup. Formal: To get the target embedding for each target sentence, we use a BiLSTM with a hidden state dimension of 512 neurons and a hidden state dimension of 512. Formal: Using the target language's word embedding gives us the best results, as shown in Table 1, since it gives the best context. Formal: To predict the target language's word embedding for each source sentence, we use a BiLSTM with a hidden state dimension of d. Formal: The parameters α and β are like weights that decide how much to focus on the target embedding. Formal: We tested different values of α and β in our experiments. We found that α = 0.4 works the best for predicting the target embedding for the source sentence. Formal: The loss function we used to adjust the target embedding weights is called L_adapted, and it's in R^d. Formal: We trained the BERT model using different weights for the target embedding. Formal: The loss function L_adapted is based on the probability of the target embedding being correct and the source sentence embedding being correct. Formal: We followed the same process as in the training phase of the BERT model. Formal: The",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "Alright, so we've got W ao, W ad, and W ae i, which are all learnable parameters. Here, W ao and W ad are in R m×b, and W ae i is in R m×d. The variable m represents the hidden size of the attention mechanism, b is the hidden size of BERT, and d is the size of the word embedding. As you can see in Figure 1, the attention process works like this:",
        "formal_text": "where W ao ∈ R m×b, W ad ∈ R m×b, and W ae i ∈ R m×d are learnable parameters; m denotes the hidden size of attention, b denotes the hidden size of BERT, d denotes the size of word embedding. As shown in Figure 1, the attention goes through as:",
        "GPT2_formal_text": "and index_ids. Formal: We've calculated the length-to-word ratios for each language pair using the exact same training data as the original training data. Formal: The word counts are 100% for English and 0% for Chinese. We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language pair count of 1,200 for English and 1,000 for Chinese. Formal: We also ran the same experiments using 20% and 0% of the training data, which gives us a language",
        "directory": "deelio",
        "filename": "2020.deelio-1.1.json"
    },
    {
        "casual_text": "Alright, let's dive into what we found when we used one-sided discriminative labeling with some hints. We grabbed the real syntactic labels from the Penn Treebank—about 9000 sentences worth. Then, we plugged them into the LaSO sequence labeling software (shoutout to Daumé III and Marcu, 2005) with its default features. Our main aim here was to look at two key things:",
        "formal_text": "In this section, we describe the results of one-sided discriminative labeling with hints. We use the true syntactic labels from the Penn Treebank to derive the constraints (this is roughly 9000 sentences). We use the LaSO sequence labeling software (Daumé III and Marcu, 2005), with its built-in feature set. Our goal is to analyze two things:",
        "GPT2_formal_text": "_t), where t is the length of the input sequence. Next, we have the attention pooling layer that processes the input sequence x, following Equation (1). Formal: Here, c is the number of characters, and n is the total number of possible responses. Formal: But here's the thing: the length of the input response can vary a lot. Take the example in Figure 1, where the length is 7.3. Formal: We also have a sentence generator (G) that takes the whole input and generates the response. Formal: The model we're using is called G = (W h, W f, W g, U, U s, U t ), where h and f are the starting and ending words, respectively. The parameters W h, w f, w g, U, U s, U t are all set up like in Equations (1) to (4), as explained in Section 5. Formal: Basically, we're just building the model to create the output. Formal: We're using the best parameters (W h, w f, w g, U s, U t ) for this model. Formal: Basically, we're using the best parameters (W h, w f, w g, U s, U t ) to generate the response. Formal: Basically, we're using the best parameters (W h, w f, w g, U s, U t ) to generate the response. Formal: The final response for the input x is the combination of all the responses that fit together. Formal: Formal: Using the best parameters (W h, w f, w g, U s, U t ), we can generate the final response for the input x. Formal: Formal: We train the model using a batch size of 32, and then we do a selective attention layer on the final response. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "The VQA model by Johnson and team (2017) has two main parts: a program generator (G) and an execution engine (E). The program generator takes a question (q) and predicts a program (p) to answer it. The execution engine then uses this program to combine different modules and runs the whole thing on an image to get the answer. Johnson and colleagues (2017) trained the model using a semi-supervised method. They showed that the program generator can work well even when trained on just a small number of possible programs (like 4% or less).\n\nTo test how well the execution engine (E) performs with limited data, we did some experiments using regular supervised training but with smaller training sets. In all these experiments, we fed E the correct program and image pairs. Figure 1 shows the best accuracy we got for each experiment on CLEVR's validation set, where the execution engine was trained on a portion of CLEVR's training set, like 50% (check out Figure 2 for some examples from the CLEVR dataset). The results show that the execution engine doesn't do so well when it's trained on smaller subsets of data.",
        "formal_text": "The VQA model (Johnson et al. , 2017b) includes two main components: a program generator G and an execution engine E. The program generator predicts a program p to address a question q. The execution engine combines the modules according to the program, and executes the obtained network on the image to produce an answer. Johnson et al. (2017b) train the model using a semi-supervised learning approach. They demonstrate that the program generator can produce acceptable programs while training on only a small fraction of possible programs (≤ 4%). To evaluate E's performance in a low data regime, we conducted a number of vanilla supervised training experiments with decreasing sized training sets. Note that we use ground truth program and image pairs as the input to E in all experiments. Figure 1 shows the best accuracy of each experiment on CLEVR's validation set while the execution engine is trained on a subset of the CLEVR's train set e. g. , 50% (See Figure 2 for some examples of the CLEVR dataset). The results verify execution engine's poor performance on the small sized training subsets.",
        "GPT2_formal_text": "= {'a': {'f1': {'f2': {'f3': {'f4': {'a': {'b': {'c': {'d': {'e': {'f5': {'c': {'f6': {'e': {'f7': {'e': {'f8': {'f9': {'f10': {'f11': {'f12': {'f13': {'f14': {'f15': {'f16': {'f17': {'f18': {'f19': {'f20': {'f21': {'f22': {'f23': {'f24': {'f25': {'f26': {'f27': {'f28': {'f29': {'f30': {'f31': {'f32': {'f33': {'f34': {'f35': {'f36': {'f37': {'f38': {'f39': {'f40': {'f41': {'f42': {'f43': {'f44': {'f45': {'f46': {'f47': {'f48': {'f49': {'f50': {'f51': {'f52': {'f53': {'f54': {'f55': {'f56': {'f57': {'f58': {'f59': {'f60': {'f61': {'f62': {'f63': {'f64': {'f65': {'f66': {'f67': {'f68': {'f69': {'f70': {'f71': {'f72': {'f73': {'f74': {'f75': {'f76': {'f77': {'f78': {'f79': {'f80': {'f81': {'f82': {'f83': {'f84': {'f85': {'f86': {'f87': {'f88': {'f89': {'f90': {'f91': {'f92': {'f93': {'f94': {'f95':",
        "directory": "alta",
        "filename": "2021.alta-1.3.json"
    },
    {
        "casual_text": "FB15k-237 #θ MR MRR Hits@10 @3 @1 MR MRR Hits@10 @3 @1 TuckER (Balazevic et al.). We also saw some cool improvements. Plus, our local-cognitive negative sampling method, which combines the best of both worlds, really shines in structured knowledge learning.",
        "formal_text": "FB15k-237 #θ MR MRR Hits@10 @3 @1 MR MRR Hits@10 @3 @1 TuckER (Balazevic et al. , improvement. The results also emphasize the effectiveness of our proposed local-cognitive negative sampling method, a non-trivial integration of the both above, in structured knowledge learning.",
        "GPT2_formal_text": "ed_tasks, and the final embedding vector for the sentence. Formal: The model uses a mean pooling layer and a max pooling layer to create the sentence embedding vector. After that, the encoded_s is sent to the encoder layer. Formal: We use an EM-based model for the latent representation, which is trained using cross-entropy loss. The goal is to maximize the log likelihood of the hidden states, which we can think of as an embedding representation. Formal: To handle the issue of sparse data and avoid exploding gradients, we add the sigmoid function to the embedding representation of each token. This helps fix the problem of not having enough data. Formal: We use a multi-layer perceptron (MLP) to figure out the hidden state representations for each token in the input sequence. The hidden state vector h_t_i for a token is found by minimizing the log loss across all the tokens. Formal: To make sure the model can learn useful representations from multiple directions, we add the non-linearity L(•) to the embedding representation. This helps the model learn better representations for both forward and backward directions. Formal: We combine the learned representations using a weighted sum. This helps the model pick the best representations that are useful for the next step. Formal: The final embedding vector is then fed into a fully-connected layer, which gives us the latent representation for the sentence. Formal: Finally, we use the decoder to generate the actual output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.48.json"
    },
    {
        "casual_text": "In this part, we’re introducing our clickbait detection method called SATC, which stands for style-aware title modeling and co-attention. The whole setup of SATC is shown in Fig. 2. It has four main parts: a content modeling module that learns how titles and bodies are represented based on their content, a style modeling module that picks up on the stylistic patterns in the title, an interaction modeling module that looks at how the title and body connect, and a clickbait prediction module that calculates the clickbait score. Let’s break down each of these modules in more detail. Oh, and Fig. 2 shows the full structure of our SATC approach for detecting clickbait.",
        "formal_text": "In this section, we introduce our proposed clickbait detection approach with style-aware title modeling and co-attention (SATC). The framework of our proposed SATC approach is illustrated in Fig. 2. It consists of four core modules, i. e. , a content modeling module to learn representations of title and body from their content, a style modeling module to capture the stylistic patterns in the title, an interaction modeling module to capture the interactions between the contexts within title and body, and a clickbait prediction module to compute the clickbait score. The details of each module are introduced as follows. Figure 2: The architecture of our SATC approach for clickbait detection.",
        "GPT2_formal_text": "= {'s1', 's2', 's3', 's4', 's5'}, where each s1 is an entity and each s2, s3, and so on, are its related entities. Formal: To train a classifier, we first create a basic vector with the specific features we want. Then, we use a neural network to guess the class label for each sentence. Formal: We use the input sentence to get the representation for each entity. This gives us the vector representation for the entity. Formal: We take the input sentence, turn it into a vector, and then use a neural network to guess the class label for each sentence. Formal: Finally, we combine all these vectors to get the entity representation. Formal: The number of entities in each sentence is basically the average of the vector representations for all the entities in that sentence. Formal: The classifier learns from the entity representations using the data it gets. Formal: Since the classifier focuses on the entities in the input, it might miss some entity-related tokens. Formal: To fix this, we add entity embedding to the original embedding vector of the entity to make sure it gets included in the classifier's attention. Formal: For any sentence, the classifier gives us the entity vector. Formal: The classifier learns from the entity representations using the data it gets. Formal: Formal: The classifier can't learn from the entity embedding because the entity embedding is tied to the token embedding. Formal: Formal: The classifier can't learn from the entity embedding because the entity embedding is tied to the token embedding. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "ccl",
        "filename": "2020.ccl-1.106.json"
    },
    {
        "casual_text": "Alright, so the goal here is to show how to use \"locus\" to break down the interpellation of a phrase into two parts. One part is the interpellation of the addressed item, and the other is something related to it that this can combine with. Let's say, for example, we're looking at the VP \"a peach should be interpreted as.\"",
        "formal_text": "The g, ~mera. I aim of ISis l)a. l~er is to show h()w t, o use locus I; o decotnpose the ii~l, erpreL: . ~t, ioJl of a. phra, se iul; o two) pa. . rl, s, where, oue pa, rt is I, he iittez'prel; . ; : d, ion, . ff I, he ['o, : : t+ssed item a, n(l the or; her is sotn, : ; o1: , ], : : , , : : 1; with which this ca. n comlJine. Sup-pose, ['or exa. ml>le, we t. h(+ughl, l. ha, t; the VP. /. , a pea. oh shoutd Im iul: ; rF, reted ~ts:",
        "GPT2_formal_text": "_tensor ( s_t, t, g_t ). Here, s_t represents the sequence of words in the input, while t and g_t are the sequences of words in the target and source, respectively. Formal: To figure out the attention matrix, we first use the tanh function on the input sequence s_i, which gives us the attention matrix A = (r_s, t, g_t) = tanh(r_s, t, g_t). Formal: The attention matrix A for the sequence s_i, which is based on the input sequence s_i. Formal: There's also another linear layer called ReLU, which helps the model focus more on the important parts of the input. Formal: To get the input hidden states h_i, we first use the LSTM (H) to process the input data. Formal: Formal: This approach works better than using a separate attention layer and is actually faster than the traditional attention-based model. Formal: Formal: This layer keeps the attention weights from the original attention layer, but adds a linear layer that focuses on the important bits of the input. Formal: Formal: This layer is trained using the cross-entropy loss on the target side, specifically using the attention weights from the original attention layer. Formal: The final hidden state h_i is calculated by multiplying the cross-entropy loss with the learned attention weights. Formal: This version of the attention-based model makes the model more flexible in terms of computation and can be used to calculate scores for other tasks like word similarity. Formal: Formal: The final hidden state h_i is computed by multiplying the cross-entropy loss with the learned attention weights. Formal: Figure 1 shows how our model works for each task. Formal: For each task, we use the model trained on the development set to evaluate its performance on the test set. Formal: Formal: The attention weights are learned from the target side, so the model can pick the right attention weights for different tasks based on the training data. Formal: Formal: To train the attention layer, we use a cross-entropy loss on the target side. Formal: Formal: Since this is a linear model, the model can better learn how to pay attention to the important parts of the input by focusing on the input that has the most important words. Formal: Form",
        "directory": "C94",
        "filename": "C94-2142.json"
    },
    {
        "casual_text": "It means that in any ordered tree, for any node u, if the feature structure linked to u begins with the point w and inv is true at w, then auz is also true at w. Plus, you can get to the propositional info fin from w by taking a (VFORM) transition to another node w'.",
        "formal_text": "This says that for any ordered tree and any node u in this tree, if the feature structure associated with u starts with the point w and inv is true at w, then auz is also true at w and furthermore, the propositional information fin is reachable from w by making a (VFORM) transition to some other node w'.",
        "GPT2_formal_text": "= (x_i, x_j, ..., x_n) } [/] Formal: We use these NLP tools to figure out which questions and documents are about the same thing. Formal: We then use a similarity measure, f(•), to compare these pairs to the original ones. Formal: We're not using any fancy fancy semantic processing to identify the connections between these pairs. Formal: As mentioned in (Kiddon et al., 2017), there are some issues with how the NLP systems might work. For example, the similarity measure f(•) isn't super accurate when the test set is small, like less than 1% of the total. Plus, some of the names in the questions might be too short or too long to be unique in the full text. Formal: For instance, the set of questions in the second dataset (left column in Figure 1) has all the names from the first dataset (right column). Formal: This might work fine for a regular QA system, but when we want to see how accurate the scores from the systems are for a specific question, we can't just rely on this basic approach. Formal: The multi-turn QA dataset, like the one in Table 1, is a good example of this. Formal: We're using all the training data for both questions and documents, so the current model is trained for each turn. Formal: We're also using the multi-turn dataset for testing. Formal: We want to make sure our system can handle natural language questions that have a lot of information, so we're using a QA system with six levels (like Collins and Singer talked about in 2007) and the Multi-turn dataset for this purpose. Formal: Formal: We're testing three systems: (1) an LSTM-based system (from Gu et al., 2015), (2) a CNN-based system (by Chen and Duh, 2015), and (3) an RNN-based system. Formal: For each of these systems, we're testing on the same dataset. Formal: We're mainly using the test set for comparison, but we're also testing on the full test set. Formal: Here, we're comparing these systems with the best results from our method. Formal: The multi-turn dataset has some pairs with less than 10 words, so we're only testing on the full test set. Formal: For a fair",
        "directory": "E93",
        "filename": "E93-1004.json"
    },
    {
        "casual_text": "This paper introduces a phrase structure parsing algorithm that’s kind of a mashup of ideas from LR (Knuth, 1965), GLR (Tomita, 1988), and some newer stuff on dependency grammar by Huang and Sagae (2010). It also uses a discriminative weighting system inspired by Collins (2002). Now, discriminative phrase structure parsing can be a real pain when it comes to speed and efficiency, as Turian and Melamed (2006) and Finkel et al. (2008) pointed out. But we’ve figured out some shortcuts that not only make this approach manageable but also super efficient and accurate, especially for a language like French, which is packed with tons of vocabulary.",
        "formal_text": "The paper provides a phrase structure parsing algorithm inspired by LR (Knuth, 1965), GLR (Tomita, 1988) and the recent developments of (Huang and Sagae, 2010) for dependency grammar. The parsing algorithm comes with a discriminative weighting framework inspired by (Collins, 2002). Although discriminative phrase structure parsing has been shown to be challenging when it comes to efficiency issues (Turian and Melamed, 2006; Finkel et al. , 2008), we use here several approximations that make the framework not only tractable but also efficient and accurate on a lexically rich language such as French.",
        "GPT2_formal_text": "ing. In both setups, the layers are randomly initialized, and the final hidden state is represented by the average of the K input hidden states. Formal: The raw token representation, X = (x1, ..., xN), is created based on the initial hidden state x1. Formal: We use the same setup as in the original paper. Basically, we start by training a linear classifier using the input data and the predicted hidden states. The model learns to predict the next token based on its own hidden states. Then, we update the weights, w(x), using the original label embeddings, x. The original paper showed some cool results with both MLE and LM adaptation (Li et al., 2016b) for sequence labeling tasks. Formal: The encoder gets trained using the same steps as the original paper. After that, the final hidden state is represented by the average of the K input hidden states, which we calculate using an LSTM. Formal: We train this model using the encoder's outputs (y = (y1, ..., yK)). Formal: Finally, the parameters of this LM adaptation are what we use for the prediction. Formal: We apply the same LM adaptation method to both MLE and LM adaptation tasks. We first train the linear classifier using the input data and the predicted hidden states. We then update the weights, w(y), using the original label embeddings, x. Formal: The main difference between our approach and the original paper is that we use the entire predicted sequence to train the LM adaptation. Formal: We also think it's worth considering combining LM adaptation with MLE adaptation to get the best of both worlds. Formal: We tested our approach on a bunch of different sequence labeling tasks (check out Table 2) and found that it works really well. Formal: Formal: We also did some experiments to see how the weight parameters, w(x), affect the LM adaptation. Formal: Formal: We tried different weights for the LSTM layers to see how they affect things. Formal: We compared our LM adaptation to the usual, two-step process (Li et al., 2016a). Formal: For instance, we noticed that a simple linear layer can be too basic or not helpful, while the hierarchical one can be too complex. Formal: Formal: We also compared our approach to a few other methods, like linear sequence labeling, two-step word",
        "directory": "C14",
        "filename": "C14-1052.json"
    },
    {
        "casual_text": "2 So, the overall change on the main test set (Total) is basically nothing, but when we look closer, we see that the new data helps reduce the error rate for low-frequency phrases by up to 3%. And, as we hoped, it doesn’t mess things up for the high-frequency stuff. Things get even better for the MARUPA examples we held back, where the baseline had a high error rate on purpose. We noticed that a lot of the collected phrases are so rare that they don’t even show up in the lowest-frequency category of the main test set, so it’s hard to fully understand how much MARUPA is helping. Across different languages, we found that for newer ones like Hindi, the examples we get aren’t as helpful and the quality is lower. This is because there are fewer user interactions and the underlying IC/SL system isn’t as developed yet. Overall, the results show that MARUPA can boost accuracy for those less common phrases.",
        "formal_text": "2 While the overall change on the main test set (Total) is negligible, the break down reveals that for low-frequency utterances the new data leads to error rate reductions of up to 3%. As desired, this comes with no negative effects for high-frequency utterances. Even stronger error reductions can be seen for the heldout MARUPA examples, on which the baseline's error rate is by design high. We observed that many collected utterances are so rare that they are not captured even by the lowest-frequency bin of the main test set, making it difficult to assess the full impact of MARUPA. Across languages, we observe that for newer languages like Hindi, less useful examples can be collected and their quality is lower, which is because fewer user interactions are available and the underlying IC/SL is less mature. Overall, the results demonstrate that MARUPA can improve accuracy on long-tail utterances.",
        "GPT2_formal_text": "= entity_ids (q) for each entity_id in q. Formal: We're looking for the best translation that performs best on the development set, but the performance here depends on the exact type of relation and how it's spread out in the data. Formal: The goal here is to find the best translation, which we can use for training. We picked these embedding layers because they help model the relationships between entities. Formal: In a text, words are represented by vectors, and the type of relation is also figured out through these vectors. Formal: Using these embedding layers, we create vectors for the relations. Formal: For the first layer, we start by setting up a basic model for how words and relations are related, which is based on a few ideas. Formal: For the second layer, we do something a bit different. We use a model that connects the words to the relation vectors, so we can work with a wider range of relationships. Formal: Finally, for the last layer, we learn a straightforward model for how to match up the words in the sentences. Formal: The final model we use for training comes from this setup. Formal: It's pretty straightforward to use these model-based embeddings, and the best results so far have been measured by comparing the translation to the reference. Formal: Formal: To see how each layer affects things, we ran some experiments. We did experiments on the MIMIC-III dataset (from Johnson et al., 2008) and compared our method to other approaches based on the embedding layers used in (Mikolov et al., 2013). Formal: We measured the accuracy of the translation by calculating the BLEU score. Formal: We also took a look at the BLEU score for a few other language pairs. Formal: We also tested the model's performance on three different relational datasets and compared our method to two baselines. Formal: We ran some experiments on the Stanford NER dataset. Formal: In our experiments, we used the Stanford NER dataset (Ng and Grishman, 2008) for training and tested it on the MIMIC-III dataset. Formal: We also did some experiments on the TIGER dataset. Formal: Finally, we tested the model on two other relational datasets and compared it to two baselines. Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-industry.3.json"
    },
    {
        "casual_text": "So, we’re talking about a survey by Gao et al. (2018) that gives a big-picture look at neural methods in conversational AI. In this paper, we’re focusing on similar work, specifically semantic parsing approaches for conversations. \n\nLiang et al. (2017) came up with a Neural Symbolic Machine (NSM) that’s enhanced with a key-value memory network. In this setup, the keys and values come from a sequence model during different encoding or decoding stages. The NSM is trained using the REINFORCE algorithm with weak supervision and tested on the WebQuestionsSP dataset (Yih et al., 2016).\n\nThen, Saha et al. (2018) introduced a hybrid model that combines the HRED model (Serban et al., 2016) and the key-value memory network model (Miller et al., 2016). This model has three main parts:\n\n1. The Hierarchical Encoder, which creates a representation for each utterance.\n2. A higher-level encoder that makes a representation for the overall context.\n3. The Key-Value Memory Network. It stores each candidate tuple as a key-value pair. The key is made up of the combined embeddings of the relation and the subject, while the value includes things like:\n   - Finding objects in triples with a specific subject and predicate.\n   - Finding subjects in triples with a specific object and predicate.\n   - Filtering entities based on a given type or multiple types.",
        "formal_text": "We point to the survey by (Gao et al. , 2018) that provides a holistic overview of neural approaches in conversational AI. In this paper, we stick to our closely related work, i. e. , semantic parsing-based approaches in conversations. (Liang et al. , 2017) introduce a neural symbolic machine (NSM) extended with a key-value memory network, where keys and values are the output of the sequence model in different encoding or decoding steps. The NSM model is trained using the REINFORCE algorithm with weak supervision and evaluated on the WebQuestionsSP dataset (Yih et al. , 2016). (Saha et al. , 2018 ) propose a hybrid model of the HRED model (Serban et al. , 2016) and the keyvalue memory network model (Miller et al. , 2016). The model consists of three components. The first one is the Hierarchical Encoder, which computes a representation for each utterance. The next module is a higher-level encoder that computes a representation for the context. The second component is the Key-Value Memory Network. It stores each of the candidate tuples as a key-value pair where the key contains the concatenated embedding of the relation and the subject. In contrast, the value contains Action Description set → find(e, p) set of objects part of the triples with subject e and predicate p set → find reverse(e, p) set of subjects part of the triples with object e and predicate p set → filter type (set, tp) filter the given set of entities based on the given type set → filter multi types(set1, set2)",
        "GPT2_formal_text": "layer, which is set to 512, turns all the input messages into numbers. This output is then fed into the BERT model. Formal: In the process of normalizing words, we can pick the number of changes (|C|) that get rid of a word as long as the input is the same. We'll call this method FastText normalization (from Wieting et al., 2017). Formal: You can find more info in the paper by Wieting et al. Formal: We start by using a BERT model (from Devlin et al., 2019) to figure out the probability P(C|w i |w i+1), which is calculated using the log of the log probability p(C|w i |w i+1). The next thing we do is pick the number of changes (|C|) that will remove a word from the input based on these probabilities. Formal: You can find more info in the paper by Wieting et al. Formal: We use a FastText embedding (from Peters et al., 2018) to get the probabilities P(C|w i |w i+1). Formal: After that, we calculate the normalized word embedding, z i, for each input word w i by: Formal: We use a FastText embedding (from Peters et al., 2018) to get the probabilities P(C|w i |w i+1). Formal: We use the normalized word embedding z i to get the probabilities P(C|w i |w i+1). Formal: We use the normalized word embedding z i to get the probabilities P(C|w i |w i+1). Formal: We calculate the normalized word embedding, z i, for each input word w i by: Formal: We calculate the normalized word embedding, z i, for each input word w i by: Formal: Formal: We use the normalized word embedding z i to get the probabilities P(C|w i |w i+1). Formal: We calculate the normalized word embedding, z i, for each input word w i by: Formal: We use the normalized word embedding z i to get the probabilities P(C|w i |w i+1). Formal: Formal: We calculate the normalized word embedding, z i, for each input word w i by: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.72.json"
    },
    {
        "casual_text": "Most spelling systems these days need some special, language-specific stuff, like dictionaries, lists of common typos, or rule books. Systems that use statistical models need a lot of data with examples of spelling mistakes to learn from. But our system doesn’t need any of that fancy annotated data. Instead, we just use the internet as a big, messy source of information. Here’s how we do it:\n\n1) We figure out stuff about typos by looking at how words are used on the web and use that to create a model of errors.\n2) The words we see the most online become a messy list of possible corrections.\n3) We use n-grams (little chunks of words) to build a language model (LM) that helps us make corrections that sound right in context.\n\nSince our error model works by looking at parts of words, we don’t have a fixed list of \"correct\" words to compare things to. So, whether a word is spelled wrong or right, it’s all good. Plus, when we combine this with our n-gram LM, the system can catch and fix real-word mistakes—like when you use the wrong word or mess up grammar.",
        "formal_text": "Most spelling systems today require some handcrafted language-specific resources, such as lexica, lists of misspellings, or rule bases. Systems using statistical models require large annotated corpora of spelling errors for training. Our statistical models require no annotated data. Instead, we rely on the Web as a large noisy corpus in the following ways. 1) We infer information about misspellings from term usage observed on the Web, and use this to build an error model. 2) The most frequently observed terms are taken as a noisy list of potential candidate corrections. 3) Token n-grams are used to build an LM, which we use to make context-appropriate corrections. Because our error model is based on scoring substrings, there is no fixed lexicon of well-spelled words to determine misspellings. Hence, both novel misspelled or well-spelled words are allowable. Moreover, in combination with an n-gram LM component, our system can detect and correct real-word substitutions, ie, word usage and grammatical errors.",
        "GPT2_formal_text": "=1; type_ids=1; title_ids=1; meta_ids=1; form_ids=1; body_ids=1; footer_ids=1; where T is the total number of entities, A is the total number of sentences, and W_m is the number of words in the sentence. Formal: The IOB2 token, like \"abcdefghijklmnopqrstuvwxyz0123456,\" is considered a specific example for the given entity type. Formal: The title and meta-data for a document are the same as the entity type they describe. Formal: Figure 1 shows the structure of our GRU-based entity detection model, which uses an encoder-decoder setup. Formal: The title and meta-data for a document are the same as the entity type they describe. Formal: Formal: The title and meta-data for a document are the same as the entity type they describe. Formal: Figure 2 shows the GRU-based entity detection model for news articles. Formal: The title and meta-data for a news article are the same as the entity type they describe. Formal: Figure 3 shows the GRU-based entity detection model for Wikipedia articles. Formal: The title and meta-data for a Wikipedia article are the same as the entity type they describe. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D09",
        "filename": "D09-1093.json"
    },
    {
        "casual_text": "Check out the annotation templates and interfaces we used for the explanation collection and verification stages in Figures 12, 13, 14, 15, and 16.",
        "formal_text": "We present the different annotation templates and interfaces used for our explanation collection and verification stages in Figures 12, 13, 14, 15 and Figure 16 respectively.",
        "GPT2_formal_text": "_tgt: 1.0; response_type_tgt: 1.0; label_type_tgt: 1.0; weight: 1.0; min_iter: 5; max_iter: 50; shuffle: true; } Formal: So, f_tgt and f_mask_tgt represent the weights for the attention and mask parts, respectively. Formal: We're using the LSTM model with a sequence length of 5, and we use a learning rate of 1e-5. The learning rate is adjusted based on the sigmoid function. We set the dropout rate at 0.2. Formal: We set the hidden size to 128 and the number of layers to 3. For generating text, we use the Adam optimizer (from Kingma and Ba, 2015) with a learning rate of 1e-4. Formal: We're using the standard LSTM model with a learning rate of 1e-5 and a dropout rate of 0.2, with a batch size of 32. Formal: The model parameters are d = 3, s = 128, and L = 3. Formal: For the BERT model, we used a learning rate of 1e-5, a batch size of 64, and d = 128. We trained the BERT model on a single Tesla V100 GPU and tested it on a 4-way classification task. Formal: The model parameters were d = 3, s = 128, and L = 3. Formal: Formal: To optimize the model parameters, we used the Adam optimizer (from Kingma and Ba, 2015) with a learning rate of 1e-5 and a batch size of 64. Formal: We trained the BERT model with a learning rate of 1e-5 and a batch size of 64. Formal: We then applied the mini-batch distillation (from Ma et al., 2016) to the batch of tokens. Formal: Formal: The mini-batch distillation is a method where we take the last hidden state from the last batch of tokens, h_t, and use it as the new hidden state for the current batch. Formal: We trained the BERT model with a learning rate of 1e-5 and a batch size of 64. Formal: We also applied the mini-batch distillation to the batch of tokens. Formal: Finally, we ran the BERT model on the validation",
        "directory": "acl",
        "filename": "2022.acl-long.451.json"
    },
    {
        "casual_text": "We used Pytorch 1.4.0 (Paszke et al., 2019) along with libraries like numpy (Harris et al., 2020) and scipy (Jones et al., 2001-) to code all the models. For running the experiments, we had two setups: (1) a GeForce RTX 2080 GPU with 12 GB of memory, 256 GB of RAM, and 40 CPU cores, or (2) a Tesla V100-SXM2 GPU with 16 GB of memory, 250 GB of RAM, and 40 CPU cores.",
        "formal_text": "All the models are coded using Pytorch 1. 4. 0 14 (Paszke et al. , 2019) and related libraries like numpy (Harris et al. , 2020), scipy (Jones et al. , 2001-) etc. We run all experiments on one of the following two systems -(1) GeForce RTX 2080 GPU of size 12 GB, 256 GB RAM and 40 CPU cores (2) Tesla V100-SXM2 GPU of size 16GB, 250 GB RAM and 40 CPU cores.",
        "GPT2_formal_text": ", query_ids, and slot_ids for each i. So, for each i, the LSTM decoder creates a sequence of vectors X = (x_i, x_i+1, ..., x_i+m). Formal: The sentence encoder uses three main things to create its representations. Formal: The first part is a linear transformation that turns the input sentence into word representations. Formal: The second part is a multi-head attention mechanism that helps the model focus on different parts of the input sentence, which helps it achieve better results. Formal: Finally, the last part is a multi-layer perceptron that takes the output from the first two steps to create the final hidden state for the sentence. Formal: To put it more technically, the representation for the i-th token gets created by the sentence encoder, using the representations from the encoder for the i-th token. Formal: So, in short, the LSTM decoder gives you the sequence of word vectors, the attention mechanism helps it focus on important parts, and the multi-layer perceptron helps it create the final representation. Formal: Formal: The embedding matrix A(w_1, x_1, ..., x_i, m) for the token w_i is set up like this: Formal: The hidden state h_i is calculated as the sum of the dot products from the first three hidden states for each token. Formal: The final hidden state h_i is a mix of the output of the attention mechanism and the multi-layer perceptron. Formal: The original version of the paper was published by the authors on April 13, 2017. Formal: Formal: There are also some different versions of the algorithm, like the one by Nguyen et al. in 2015. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.451.json"
    },
    {
        "casual_text": "Okay, let’s say c₁, c₂, and so on represent the sequence of vector embeddings for the parts of the new phrase. The size of these vectors depends on the number of dimensions in the bidirectional LSTM that was used in the original composition function, as shown in Figure 2. We’ll use a semicolon (;) to show when we’re sticking two vectors together, like gluing them end-to-end.",
        "formal_text": "Now let c 1, c 2, . . . denote the sequence of vector embeddings for the constituents of the new phrase. The length of these vectors is defined by the dimensionality of the bidirectional LSTM used in the original composition function (Fig. 2). We use semicolon (; ) to denote vector concatenation operations.",
        "GPT2_formal_text": "_t and attention_mask_t. The network is trained using the cross-entropy loss function to minimize the cross-entropy loss, which is 0.95 for Task A and 0.91 for Task B. Formal: For each tag, the model gets a score from 0 to 1, where 0 means the tag is off, and 1 means it's on. Formal: For each name in the dataset, we calculate a score for each token. We pick a token as the key if it has a higher score than the rest. \n\nThere's also this thing called \"retention loss,\" which is basically the F1 score for each token. It's calculated as 2 divided by the number of tokens in the key, which is a little over 11.5. Formal: The main idea is to keep the model guessing, not letting it overfit to the input and avoid overfitting. Formal: We use a loss function called L_z(c) that's specific to each task. It's calculated using a weighted linear function. Formal: The weight of the input layer is normalized using something called \"norm\" on the token embeddings. Formal: When we're looking at a few tokens, we take the average of the confidence scores for the tokens before and after that particular token. Formal: The weighted linear function we use is L_z(c). Formal: We also calculate the loss for a \"learning rate\" of 0.00002, which is the best parameter we're trying to minimize. Formal: L_z(c) and L_z(1) are the loss functions for the input layers, and the \"learning rate\" is the best parameter we're trying to minimize. Formal: In each step, the model learns to minimize these loss functions. Formal: We use an L2 regularization term with a magnitude of λ. Formal: For a given token, we calculate the loss by adding up the confidence scores for the previous and next tokens. Formal: We also use a loss function called L_z(c) that's specific to each task. It's calculated using a weighted linear function. Formal: We use a loss function called L_z(c) that's specific to each task. Formal: We also calculate the loss for a \"learning rate\" of 0.00002, which is the best parameter we're trying to minimize. Formal: Formal: Form",
        "directory": "E17",
        "filename": "E17-1117.json"
    },
    {
        "casual_text": "The most common approach for semi-supervised sentiment classification is using some labeled data to help guide the process (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008; Wan, 2009; Li et al., 2011). But in a lot of cases, we don’t have any labeled data, which opens the door to unsupervised methods. The typical way to do unsupervised sentiment classification is by using a sentiment lexicon (Turney, 2002; Taboada et al., 2011) or by figuring out sentiment orientation through matrix factorization and clustering (Li et al., 2009; Hu et al., 2013). \n\nInstead of following those methods, we’ve come up with a different approach that combines word-level and document-level sentiment information (like how nearby words or documents often share the same sentiment) into one framework. Our framework uses geometric information to make up for the lack of labeled data, which is a big deal for sentiment classification. \n\nOn top of that, some researchers have used matrix factorization for other NLP tasks, like relation extraction (Peng and Park, 2013) and question answering (Zhou et al., 2013). There are also a bunch of studies focusing on other parts of sentiment analysis, such as cross-domain sentiment classification (Blitzer et al., 2007; Pan et al., 2010; Hu et al., 2011; Bollegala et al.).",
        "formal_text": "The most representative way to perform semi-supervised paradigm is to employ partial labeled data to guide the sentiment classification (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008; Wan, 2009; Li et al. , 2011). However, we do not have any labeled data at hand in many situations, which makes the unsupervised paradigm possible. The most representative way to perform unsupervised paradigm is to use a sentiment lexicon to guide the sentiment classification (Turney, 2002; Taboada et al. , 2011) or learn sentiment orientation via a matrix factorization clustering framework (Li et al. , 2009; ? ; Hu et al. , 2013). In contrast, we perform sentiment classification with the different model formulation and learning algorithm, which considers both word-level and document-level sentiment-related contextual information (e. g. , the neighboring words or documents tend to share the same sentiment polarity) into a unified framework. The proposed framework makes use of the valuable geometric information to compensate the problem of lack of labeled data for sentiment classification. In addition, some researchers also explored the matrix factorization techniques for other NLP tasks, such as relation extraction (Peng and Park, 2013) and question answering (Zhou et al. , 2013) Besides, many studies address some other aspects of sentiment analysis, such as cross-domain sentiment classification (Blitzer et al. , 2007; Pan et al. , 2010; Hu et al. , 2011; Bollegala et al.",
        "GPT2_formal_text": ", which are like basic info about the entities in the dataset. The scores from these two methods are then combined to create a final score for the entity pair, which we use as the query for a BLI (BiLingual Inference). To keep things simple and not require a ton of steps to set up, we're just going to focus on the answer tokens in the answer sentence. Formal: To train the model for a specific BLI task, we first calculate the entity-specific loss. Then, we calculate the query loss and combine it with the entity-specific loss to train the whole model. Formal: We can train the model for a specific BLI task using the input set in the usual way. Formal: The loss for the BLI task can be used to train the model for other BLI tasks too. Formal: We're not worrying about whether the model can generate the right answer or not. Formal: The word embeddings are processed using CNN (Cho et al., 2014) with a word length of 50. Formal: To get the embedding of an entity pair, we first use an LSTM (Hochreiter and Schmidhuber, 1997) to get its hidden representation. Then, we calculate the attention weights from the LSTM's output by minimizing the dot product between the entity embedding vector and the query word embedding vector. Formal: For each pair of entities in the dataset, we calculate the TFIDF score and the entity-specific loss separately using the input set. Formal: For the entity pair embedding, we consider a pair of entities as a single unit if their embedding vectors match exactly (meaning the dot product is 1). We calculate the TFIDF score for the entity pair embedding. Formal: We're not training the model to predict the correct answer. Formal: We'll show you how to use the BLI task to build a model that can predict the correct answer. Formal: We can train the model for a specific BLI task using the input set in the usual way. Formal: The loss for the BLI task can be used to train the model for other BLI tasks too. Formal: We're not worrying about whether the model can generate the right answer or not. Formal: The word embeddings are processed using CNN with a word length of 50. Formal: To get the embedding of an entity pair, we first use an L",
        "directory": "C14",
        "filename": "C14-1126.json"
    },
    {
        "casual_text": "Plus, the LSTM-based language model (LM) might do a better job because it captures more context than the CRF layer. To really make the most of the LM, we also tried training the tagging model and the LM together at the sequence level. Sure, this slows things down a bit, but the combined system performs just as well as the top-notch NER models on four different datasets.",
        "formal_text": "In addition, the LSTM-based LM potentially can perform better as it encodes more contextual information than the CRF layer. To unlock the full power of the LM, we also try to train the tagging model and LM jointly at the sequence level. In this case we lose the speed advantage, but the jointly trained system achieves comparable performance as the state-of-the-art NER model on four different corpora.",
        "GPT2_formal_text": "ed_elem = attn_masked_elem; }\n\nAttention Multi-head Learning (AML), introduced by Bahdanau et al. in 2014, is a type of multi-head attention that works across different types of inputs. It makes it easier to handle different types of text information. Basically, if you have a sentence x that has attention labels y, the AML model tries to make the model's output h D (y) as similar to y as possible by adding up the attention values for every head in the sequence. Basically, the AML model figures out the closest matching sequence by combining the values from all the heads. Formal: In Figure 1, the attention weight matrix for the title and authors is written as A = {a_t, b_t, c_t}, where a is the number of heads (h) and b, c, and d are the names of the authors, the title, and the context. Formal: But here's the thing: the AML model can't figure out the exact sequence for each head. That's because it doesn't use a beam search process, so it can't make the best decision on each head. To fix this, we came up with a second way to calculate attention weights using a hierarchical attention mechanism. Formal: Here's how it works: each head in the sequence is calculated based on the information from its parent. The parent of the first head is the one with the highest attention weight, and the last head is the one with the lowest. Formal: In Figure 1, the attention weight matrix for the title and authors is written as A = {a_t, b_t, c_t}, where a is the number of heads (h) and b, c, and d are the names of the authors, the title, and the context. Formal: Here's how we tweak the attention weights for each head: Formal: We do this by picking the head with the highest attention weight and then reducing the other heads' weights. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.612.json"
    },
    {
        "casual_text": "Alright, so basically, D' is about recalculating the data from the original dataset (let's call it Dataset 1) by focusing on specific parts of it. These parts are related to certain features or variables that come from a different source, like the Å ÖÓ×Ó Ø Ü Ð ÐÔ Ø ÜØ. The idea is to adjust or refine the original dataset based on this new information. It's kind of like tweaking the data to make it more accurate or relevant by incorporating these extra details.",
        "formal_text": "D' = recalculate ÙÖ ¾ Ë Ø Ó ÌÖ Ò× Ö ÊÙÐ × È Ö× × Ò ÌÖ Ò× Ö ÊÙÐ × ÙÖ ½ × Ô Ö Ó Ö ÙÐ Ö Þ Ô Ö× × ÓÖ ÓÖÖ ×ÔÓÒ Ò Ô Ö Ó ËÔ Ò × Ò Ò Ð × × Ò¹ Ø Ò × ÖÓÑ Å ÖÓ×Ó Ø Ü Ð ÐÔ Ø ÜØº Ì × Ö ¹×ØÖÙ ØÙÖ ¹Ð Ô Ò Ò Ý Ò ÐÝ× × Ó × Ò¹ Ø Ò × Ø Ø Ö",
        "GPT2_formal_text": ", uniques, and typos are all named entities, and we're using the ones from the dataset mentioned. Formal: We use a linear model to figure out the title and its surrounding text. Formal: After breaking it down into smaller pieces, we use a classifier to figure out the label for each one. Formal: We use a linear model to figure out the title and its surrounding text. Formal: We use a classifier to spot the main entities and figure out the label for each one. Formal: We use a linear model to spot the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify the main entities and figure out the label for each one. Formal: We use a classifier to identify",
        "directory": "C00",
        "filename": "C00-1078.json"
    },
    {
        "casual_text": "Experiment 4: This one compares two different update methods—early update and max violation update. The max violation update gets trained for 12 epochs, but the early update goes through 25 epochs.",
        "formal_text": "Experiment 4 This experiment contrasts early update with max violation update. Max violation update is trained over 12 epochs, while early update is trained over 25 epochs",
        "GPT2_formal_text": "ed(. , . ) ensures that the attention is shared across the different attention heads. To use this, we use the max-pooling method with a window size of 5, which gives us a length of (5 - 10) tokens. Formal: We use the model's outputs to initialize a set of common values. Formal: The input representation for each token gets turned into a vector called h_i_t for every instance of that token. For instance, in Figure 1, the input representation for the token \"The\" is h_i_t = {x_1, ..., x_T}. Formal: The input representation for an instance of \"The\" is also a vector called h_i_t, which is defined as Formal: The input representation for an instance of \"The\" is a vector called h_i_t, which is defined as Formal: Formal: Formal: When using a multi-head attention model, it's important to include the input representation and the head representations for each token in the input. This helps the model focus on the important words or tokens. For instance, the input representation for the token \"The\" in Figure 1 is h_i_t = {x_1, ..., x_T}. Formal: In the multi-head attention model, we can also use a second-order bidirectional GRU to get the sequence representations of the input and the head representations for each token. Formal: This second-order bidirectional GRU can be thought of as a bidirectional GRU. Formal: Formal: If the output is \"x_t\" for a token, we can just set the parameters to zero. Formal: The input representations for a token can be either a single vector or a multi-dimensional vector. We use a linear transformation for these. Formal: The output from the linear transformation can be either a single vector or a multi-dimensional vector. We use a linear transformation for these. Formal: The output from the linear transformation can be either a single vector or a multi-dimensional vector. Formal: Formal: For the loss function, the parameters include the attention parameters for the attention heads and the hidden state parameters for the latent representations. Formal: Formal: We also add a bias term to the loss function that adjusts the magnitude of the objective function. Formal: Formal: Formal: The loss function for the multi-head",
        "directory": "C14",
        "filename": "C14-1052.json"
    },
    {
        "casual_text": "We're testing our zero-shot activity recognition against a few other baseline models that also learn from attributes and embeddings. One of them is the \"Embarrassingly Simple Zero-shot Learning\" (ESZL) model by Romera-Paredes and Torr (2015). It's a straightforward linear model that predicts class labels using attributes and includes some regularization techniques.\n\nWe're also comparing our results to a version of the DAP model by Lampert et al. (2014), which we talked about in Section 4.1.1. Plus, we're including DeVISE (Frome et al., 2013) in our comparisons, as mentioned in Section 4.2.\n\nFor all these baselines, we're using visual features from a Resnet-152 CNN that's been fine-tuned on the imSitu V train classes. This is the same setup we discussed in Section 4.1.",
        "formal_text": "Zero-shot activity recognition baselines We compare against several additional baseline models for learning from attributes and embeddings. Romera-Paredes and Torr (2015) propose \"Embarassingly Simple Zero-shot Learning\" (ESZL), a linear model that directly predicts class labels through attributes and incorporates several types of regularization. We compare against a variant of Lampert et al. (2014)'s DAP model discussed in Section 4. 1. 1. We additionally compare against DeVISE (Frome et al. , 2013), as mentioned in Section 4. 2. We use a Resnet-152 CNN finetuned on the imSitu V train classes as the visual features for these baselines (the same as discussed in Section 4. 1).",
        "GPT2_formal_text": "= [], results = [], seq_tokens = [], sig_tokens = [], ct = [], n_seg = [], tokens = [], input_seq = [], input = [], output = [], n_eos = [], token_enc = [], output_enc = [], next = [], skip = [], c = [], t = [], mt = [], mt = [], mt = [], stack_size = [], stack = [], push = [], pop = [], string = [], line_length = [], [] = [], input = [], output = [], output = [], id = [], id = [], id = [], seq_id = [], seq_id = [], seq_id = []], stack = []\n\nHere's the data:\n- **Input:** k = 10, t = 0.1, c = 0.1, n_seg = 0.1, tokens = [], t = 0.1, seq_tokens = [], c = 0.1, n_eos = 0.1, sig_tokens = [], ct = 0.1, n_eos = 0.1, tokens = [], next = [], skip = [], c = [], t = [], mt = [], mt = [], mt = [] = []\n- **Output:** k = 10, t = 0.1, c = 0.1, n_seg = 0.1, tokens = [], t = 0.1, seq_tokens = [], ct = 0.1, n_eos = 0.1, sig_tokens = [], ct = 0.1, n_eos = 0.1, tokens = [], next = [], skip = [], c = [], t = [] = []\n- **Encoder:** mt = [], mt = [], mt = [] = []\n- **Transformer:** t = [], t = [] = []\n\nFigure 1: Some examples of how to create a model for a bunch of different language pairs. The **token** is the sequence of words in the input, and the **sequence of states** are the sequence of states for the model. Formal: Formal: Formal: Formal",
        "directory": "D17",
        "filename": "D17-1099.json"
    },
    {
        "casual_text": "A part-whole pattern shows that one thing is part of another thing. Take the example \"There's a valley on my mattress.\" Here, \"valley\" and \"mattress\" have a part-whole relationship, indicated by the word \"on.\" But in reality, \"valley\" isn't actually part of the mattress—it's more like an effect on the mattress. This is called a pseudo part-whole relation. For our purposes, though, we won't make a big deal out of the difference between this and a real part-whole relationship because they don't matter much for our feature mining task. In this case, \"noun 1 on noun 2\" is a helpful pattern that suggests noun 1 is part of noun 2. So if we know \"mattress\" is a class concept, we can guess that \"valley\" is a feature for \"mattress.\" There are lots of other phrases or sentences that show this kind of relationship, as mentioned in (Girju et al, 2006).\n\nBesides part-whole patterns, the \"no\" pattern is another important feature indicator, especially in opinion documents. We'll talk more about these patterns in Sections 3.2 and 3.3. \n\nNow, let's tackle the first issue: noise. With opinion words, part-whole patterns, and \"no\" patterns, we have three ways to identify features. But all of these are kind of vague, meaning they're not strict rules. This means we'll probably end up with some wrong features (also known as noise) when we use them. Getting rid of noise from our feature candidates is tricky. Instead of trying to prune out the noise directly, we're going to take a different approach: feature ranking.",
        "formal_text": "A part-whole pattern indicates one object is part of another object. For the previous example \"There is a valley on my mattress\", we can find that it contains a part-whole relation between \"valley\" and \"mattress\". \"valley\" belongs to \"mattress\", which is indicated by the preposition \"on\". Note that \"valley\" is not actually a part of mattress, but an effect on the mattress. It is called a pseudo part-whole relation. For simplicity, we will not distinguish it from an actual part-whole relation because for our feature mining task, they have little difference. In this case, \"noun 1 on noun 2 \" is a good indicative pattern which implies noun 1 is part of noun 2. So if we know \"mattress\" is a class concept, we can infer that \"valley\" is a feature for \"mattress\". There are many phrase or sentence patterns representing this type of semantic relation which was studied in (Girju et al, 2006). Beside part-whole patterns, \"no\" pattern is another important and specific feature indicator in opinion documents. We introduce these patterns in detail in Sections 3. 2 and 3. 3. Now let us deal with the first problem: noise. With opinion words, part-whole and \"no\" patterns, we have three feature indicators at hands, but all of them are ambiguous, which means that they are not hard rules. We will inevitably extract wrong features (also called noises) by using them. Pruning noises from feature candidates is a hard task. Instead, we propose a new angle for solving this problem: feature ranking.",
        "GPT2_formal_text": "_t * attention_mask_p = input_t * output_t; }  \n\n# We figure out the attention weights for the conversation model by taking the average of the hidden states from each cell. Formal: For each word x_t in the conversation, we get the attention weights T a_t for the attention matrix A, which we call A_t. Formal: Here, T_a_t is the attention weight for the attention matrix A, and A_t is the attention weight for the dialogue history. Formal: The final output is just the value of the attention weights. Formal: To get the full conversation history, we mix the attention weights from both the dialogue history and the conversation history. Formal: For each of the dialogue acts k_i, the attention weights T a_k_i are calculated using the dialogue history encoder (which is just an attention-based neural network). Formal: After that, we use a linear layer with ReLU as the activation function to get the dialogue acts. Formal: Finally, we use the attention weights to compute the dialogue acts. Formal: Lastly, we combine all the dialogue acts into one sequence to get the full sequence of dialogue acts. Formal: Basically, we're adding up the attention weights for the conversation history, dialogue history, and the dialogue history encoder to get the final dialogue act sequence. Formal: Formal: The dialogue acts are made up of a mix of attention weights that we've already figured out for the dialogue history and dialogue history encoders. Formal: Formal: Using this sequence, we get the full sequence of dialogue acts. Formal: Formal: The whole process is shown in Figure 2. Formal: The dialogue acts can be different lengths. Formal: For the full sequence of dialogue acts, we start by combining the attention weights for each sentence and the dialogue history encoder to create the dialogue acts. Formal: Formal: Finally, we use a linear layer with ReLU as the activation function to get the dialogue acts. Formal: Formal: Formal: The dialogue acts are fed into the dialogue encoder, which is an attention-based neural network. Formal: Formal: Formal: After that, we use a linear layer with ReLU as the activation function to get the dialogue acts. Formal: Formal: Formal: The whole process is shown in Figure 2. Formal: Formal",
        "directory": "C10",
        "filename": "C10-2167.json"
    },
    {
        "casual_text": "After combining the question with both image-level and object-level features (check out Section 4.3 for more details), we add these features to something called the \"counter feature,\" which was introduced in the paper. This counter feature helps the model keep track of how many objects there are. Then, we use a simple two-layer neural network to pick the answer from a list of options. The list is made up of all the correct answers from the training data that showed up more than 8 times. To figure out the best answer, we calculate something called \"logits\" using this equation:",
        "formal_text": "Given the final joint representation of question with image level and object level features (c. f. Section 4. 3), we augment both of these features to the counter feature (c f ) proposed in. The counter feature helps the model to count the objects. Finally, we employ a two-layer perceptron to predict the answer from a fixed set of candidate answers. It is predetermined from all of the correct answers in the training set that appear more than 8 times. To this end, the logits can be computed by the following equation:",
        "GPT2_formal_text": "= {query_ids, key_ids} |= p_query_ids_i, where i is the i-th row in the source or target dataset. Formal: When you do an IF-statement, the statement gets filled with boolean values (0 or 1) based on the feature values from p_feature_i. In this paper, we use a version of MERT that works with a set of boolean features. Formal: The parser checks the input tokens (x_t) using a bunch of features (f_features) and picks the best one using a function g_best. Formal: Once the parser finds the best possible answer for each input token x_t, it checks the vocabulary (v_t) using a function g_v_t. Then, it sorts the options using a function g_sort. Formal: Here's how the algorithm works step by step, where we use the best answer from the list of options to guess the title. Formal: The final result is the title, which is made by putting all the other possible title options together. Formal: Formal: This method works great for all kinds of machine translation (MT), whether it's in English, Chinese, or Japanese. Formal: We also figured out a way to automatically generate the title by combining the best guess with a term frequency table and a part-of-speech (POS) tagger. Formal: We also have a way to automatically generate the title by combining the best guess with a term frequency table and a POS tagger. Formal: Formal: Another approach uses an ORM (Machine Optimization) to make sure the output from the parser is the best possible. Formal: This method uses an ORM (Machine Optimization) to make sure the output from the parser is the best possible. Formal: Formal: We also have a method to automatically generate the title by combining the best guess with a term frequency table and a POS tagger. Formal: Formal: Formal: The data we use is actually a mix of real MT outputs and the best guesses made by a neural network. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "We suggested using space to represent the meanings of words. We came up with a way to learn how to do this in tile space. We also showed that our methods work well in an information retrieval system.",
        "formal_text": "We proposed to represent the meanings of words in space. We gave a learning mechanism for tile space. We proved tile effectiveness of the proposed methods in an information retrieval system.",
        "GPT2_formal_text": "ed, update_attention, and update_attention_modes are the parameters that describe the attention distribution for the current token, token i, and the tokens that come before it. The hidden states from these tokens are passed along as input to the logistic regression model. Formal: The graph is shown in Figure 1. We start by figuring out the attention weights for each token. Then, we mix the output from the regular unigram model with these attention weights to create the attention matrix. \n\nWe do the same thing for the previous token and the one before it. After that, we update the attention vector by using the updated weights. This process is repeated for each token in the sequence, just like in Equation 2. Formal: When you apply this attention, the model is updated to reflect the new attention distribution. Formal: For the last token, the attention is updated using the updated weights. Formal: This model creates a graph that's part of a bigger graph. It uses a graph algorithm to link tokens together using attention. Formal: The result of applying this graph attention is a new graph. It's like the last token and the next two tokens in the sequence being linked together using attention. Formal: In our model, we use a graph to keep track of attention across the sequence. Formal: Formal: We use some linear algebra operations to build the graph and the attention weights, and then we update the model's parameters. Formal: We use a graph-based algorithm to gather information from the tokens. Formal: The nodes in the graph represent the tokens in the sequence. Each node is a token, and it has a label (x_i) that tells us which token belongs to that node. Formal: The graph uses two types of edges: edges that link nodes to their parent nodes (v_i) and edges that link nodes to their child nodes (v_k). Formal: There are three types of edges: edges that link nodes to their child nodes (v_i), edges that link nodes to their parent nodes (v_k), and edges that link nodes to their child nodes (v_k). Formal: We use a non-linearity to figure out how the nodes and edges are connected. Formal: Formal: Here, v_k is a vector of attention weights that the model learns to encode the attention distribution for the token. Formal: The updates to the model's parameters are based",
        "directory": "C86",
        "filename": "C86-1089.json"
    },
    {
        "casual_text": "We use model checking to compare the FOL structure and the FOL formula for inference, and we do this with NLTK while also optimizing things (check out Section 3.4 for more details). Basically, we're working with the FOL formula and the FOL structure and making some assumptions.",
        "formal_text": "We apply model checking between the FOL structure and the FOL formula for inference using NLTK with optimization (see Section 3. 4). Under the FOL formula and the FOL structure, we assume",
        "GPT2_formal_text": "= { u1, u2, ..., uN } | P1, P2, ..., Pn }  \n\nNow, let's break down the steps involved in creating a knowledge graph, kind of like how the process described in (Shen et al., 2020) works. Formal: When training a model to predict the type of relationship, the goal is to minimize the cross-entropy loss. Formal: Let’s say the i-th relation r_i is in the graph G, and the relation type r_i is in the set A. Formal: In the embedding layer, each token s_i gets a vector representation called e_s_i. Formal: The embedding layer has two main parts: the token embedding e_s_i and the relation embedding e_t_i. Formal: The embedding layer works by first creating a representation of the token s_i. Formal: Then, the relation embedding e_t_i is calculated using the relation vector e_s_i. Formal: Finally, the cross-entropy loss is calculated using the embedding layer to minimize the cross-entropy loss. Formal: Formal: The embedding layer is built using the embedding function f(e_t_i), where e_t_i is the embedding of the token s_i, and f(e_s_i) is the cross-entropy loss for the relation r_i. Formal: The final embedding layer has two parts: the token embedding e_s_i and the relation embedding e_t_i. Formal: The cross-entropy loss is calculated using the embedding layer to minimize the cross-entropy loss. Formal: Formal: Formal: The embedding layer also uses the relation embedding e_t_i to get the embedding representation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-srw.8.json"
    },
    {
        "casual_text": "To make sure a fragment shows up in two parse trees, its labels have to be exactly the same, including all those categories, tags, and features we talked about earlier. The \"h 1 binarization\" thing means that fragments can include parts of bigger chunks; like, a string of kids from a big family of pieces. Figure 1 has an example of a parse tree, but it’s not binarized to keep things simple. The non-terminal labels have a syntactic category (in red) and a function tag (in green). The part-of-speech tags also have extra details (in black) in square brackets. Some labels have extra details that got passed down, and they’re marked with a colon at the beginning.",
        "formal_text": "For a fragment to be attested in a pair of parse trees, its labels need to match exactly, including the aforementioned categories, tags, and features. The h \" 1 binarization implies that fragments may contain partial constituents; i. e. , a contiguous sequence of children from an n-ary constituent. Figure 1 shows an example parse tree; for brevity, this tree is rendered without binarization. The non-terminal labels consist of a syntactic category (shown in red), followed by a function tag (green). The part-of-speech tags additionally have morphological features (black) in square brackets. Some labels contain percolated morphological features, prefixed by a colon.",
        "GPT2_formal_text": "_t and attention_mask_t are the max layer representations for the input and output, respectively. When using the CNN model (from Kim, 2014), the first and last hidden layers are randomly initialized with a dimension of 128 and 512, respectively. Formal: We also tried using word embedding with a dimension of 0 to get a better idea of how the model reacts to changes in context. The results are shown in Figure 1. The model's performance keeps improving over time for different values of the word embedding. The best results come when the word embedding is set to 1, which matches the 1-dimensional word embedding we used in the first experiment. Formal: We checked how the model performed using standard metrics like precision, recall, and F1 score. To do this, we ran the experiments with different embedding sizes: 0 (which is the word embedding size in the first experiment), 1 (which is the word embedding size in the second experiment), and 2 (which is the word embedding size in the third experiment). For every word embedding size, we averaged the results from all three experiments. Formal: We ran the same experiments with different word embedding sizes, but using a specific term embedding size, which we used in the first experiment. This setup lets us compare performance with other models that use different embedding sizes. Formal: We also included the F1 score for the best model and the best average F1 score for all the models. Formal: Finally, we ran the same experiments with different embedding sizes, but this time we used the word embedding size (which is 1 in the first experiment) and the average F1 score. Formal: We included the results for the model using the word embedding size (1 in the first experiment) and the average F1 score (1 in the second experiment). Formal: We also included the results for the model using the word embedding size (1 in the first experiment) and the average F1 score (1 in the third experiment). Formal: For each test, we calculated the mean and standard deviation. Formal: We used the same setup for each test as the second experiment. Formal: The results for the model using the word embedding size (1 in the first experiment) and the average F1 score (1 in the third experiment) are shown in Table 1. Formal: The best result for the dataset with the word embedding size (1 in the first experiment",
        "directory": "E17",
        "filename": "E17-1115.json"
    },
    {
        "casual_text": "For each phrase in the training file, grab its count (C(f)). Then, go through all the unique phrases.",
        "formal_text": "foreach phrase f in training file do extract C(f ); /* phrase count */ foreach unique phrase",
        "GPT2_formal_text": ". Formal: If all the words in the query are titles, the words in the title will match up with all the words in the query. Also, we use a character-based NER model to figure out the part of speech (POS) of each word in the query. Formal: To figure out the POS of a word in the query, we use the BPE tagging method. The BPE tagging method (from Loper and Bird, 2001) is a semi-supervised approach that uses a mix of alignments and POS tags. The results for this method are shown in Table 1. Formal: For the query-sentence pair, the matching links are made by combining all the POS tags in the query and the answer sentence. To make sure the answers are consistent, we use the same setup as before. Formal: Formal: To get the POS tag for a word in the query, we use the BPE tagging method. Formal: Lastly, we calculate the probability p(y|x) for the word in the query, given the sentence x. Formal: To check if the word x is in the sentence, we look at all the words in the sentence x, which gives us the probability p(y|x). Formal: Formal: We use the BPE tagging method to figure out the POS of a word in the query. Formal: For the query-sentence pair, the matching links are made by combining all the POS tags in the query and the answer sentence. Formal: To make sure the answers are consistent, we use the same setup as before. Formal: Finally, we calculate the probability p(y|x) for the word in the query, given the sentence x. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D10",
        "filename": "D10-1064.json"
    },
    {
        "casual_text": "Neural networks have been doing really well in various AI areas lately (LeCun et al., 2015), and because they’re so good at modeling stuff, a lot of researchers have been diving into creating new recommendation algorithms using Deep Learning. You can check out some examples like Barkan and Koenigstein (2016), He et al. (2017), Hidasi et al. (2015), and Covington et al. (2016).",
        "formal_text": "Due to the recent success of neural networks in multiple AI domains (LeCun et al. , 2015) and their superior modeling capacity, a number of research efforts have explored new recommendation algorithms based on Deep Learning (see, e. g. , Barkan and Koenigstein, 2016; He et al. , 2017; Hidasi et al. , 2015; Covington et al. , 2016).",
        "GPT2_formal_text": ". The QaP loss, as mentioned in equation (1), is calculated using the goal-level attention mechanism. After the adversarial training, the model is adjusted to handle the reverse direction of the conditional distribution. Formal: Another way to tweak the QaP loss is to tweak the latent variables that were trained using discriminative training. Formal: In this paper, we tried out a different approach. We split the original latent variables into two groups: one group was used for training discriminative models, and the other for training generative models. Then, we trained the model using the discriminative group's latent variables and also the generative model's latent variables. The main difference between these two groups is the loss function. Formal: We used a standard L2-regularized objective function to train the discriminative model. The model was trained on a pretrained BERT-base model, which was fine-tuned with the QaP loss from equation (1). Formal: The generative model was also fine-tuned using the loss function we mentioned in equation (1). Table 5 shows the average results from our experiments for all three datasets. For the QA datasets, we looked at the average accuracy across all three tasks, while for the HCD datasets, we used the average accuracy across all three tasks, as reported in Table 2. Formal: We also did some experiments with the log loss. For the QA dataset, we tested the log loss for single-task and multi-task learning, with or without using a latent variable approach. For the HCD dataset, we tested the log loss for both single-task and multi-task learning, with or without using a latent variable approach. Formal: We also tried out an ensemble approach for the QA dataset, where we trained a single model for all three tasks at the same time. For the HCD dataset, we also tested the ensemble approach for all three tasks. Formal: Table 6 shows the average performance from our experiments for all three datasets. Formal: We also did some experiments with the latent variable approach. For the QA dataset, we trained the model with a latent variable approach using equation (1), and for the HCD dataset, we trained the model with a latent variable approach using equation (2). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "ecnlp",
        "filename": "2020.ecnlp-1.8.json"
    },
    {
        "casual_text": "At each time step t, we figure out the value using a max-pooling thingy across different window size filters. In simpler terms, q c t = max(q l 1, c t, q l 2, c t, . . . , q l L, c t ). The final output from the CNN layer can be written as {q c t } T t=1. Taking inspiration from how well this has worked in other NLP tasks (like Luong et al., 2015; Yue-Hei Ng et al., 2015), we stack a bunch of Bi-LSTM (Hochreiter and Schmidhuber, 1997) layers to get a better understanding of the whole question's meaning. The first LSTM layer takes in the convoluted representation of the question, which is {q c t } T t=1. Then, q r t = Bi-LSTM(q r t−1, q c t ).",
        "formal_text": "t at a time step t is computed by the max-pooling operation over different window size filters. Mathematically, q c t = max(q l 1, c t, q l 2, c t, . . . , q l L, c t ). The final representation computed by CNN layer can be denoted as {q c t } T t=1. Inspired from the success in other NLP tasks(Luong et al. , 2015; Yue-Hei Ng et al. , 2015), we employ stacking of multiple Bi-LSTM(Hochreiter and Schmidhuber, 1997) layers to capture the semantic representation of an entire question. The input to the first layer of LSTM is the convoluted representation of the question {q c t } T t=1. q r t = Bi-LSTM(q r t−1, q c t )(3)",
        "GPT2_formal_text": "= 1; t = t'1; s = s'1; h = h'1; c = c'1; h2 = h'2; h3 = h'3; c2 = c'2; h3 = h'3; c3 = c'3; ...; n = n'1; ...; n' = n'1; } Formal: The hidden state h_t for the word w_t is calculated using this formula: Formal: Alright, let's break this down in a simpler way. We're using a CNN to pull out a bunch of features from the text. Here's what we've got: Formal: We have a matrix called V_t, which has a bunch of numbers in it. Each number is a vector. The first column in V_t is called the \"conditional\" vector, and the second column is the \"contrastive\" vector. The whole thing is a bit like a recursive neural network. \n\nNow, let's say we have a sentence like x = x1...xn, where n is the length of the sentence. The first row in V_t, which is labeled w_1, represents the \"conditional\" vector. The second row, w_2, is the \"contrastive\" vector. The third row, w_3, is the \"contiguous\" vector. Formal: Here, w_t = h_t1; h_t2; h_t3; ...; n' = n'1; ...; n' = n'1; } Formal: We're using a convolutional neural network to get the output features. Formal: The output features are vectors with n numbers. Formal: For each word, we use a linear transformation called δ_t to create a word embedding. Formal: We also use another linear transformation called δ_c to create a character embedding. Formal: Lastly, we use a linear layer with softmax to combine the input word embeddings. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "Since the model's accuracy is pretty high, you don't see many mistakes in the output. It's also clear that the chunk model can still predict the right form even when it's shortened to just one character, like on line 5.",
        "formal_text": "Since the model accuracy is rather high, the errors are not very common in the output. We can also see clearly that the chunk model is able to predict the right form even when form is reduced to one character, as on line 5.",
        "GPT2_formal_text": "ed = (x, y, h) A represents the attention values for an entity mention y in the context c. Formal: The attention weights are figured out using an LSTM-based network. Formal: Here's the formula for the attention weights for an entity mention: Formal: You can find the full list of attention weights and their parameters in the appendix. Formal: You can find the attention parameters in the appendix too. Formal: To find the attention weights for a mention m in the context c, the attention weights are the ones that cover m as well as the ones that don't. Formal: And for the entity mention x_i in the context c, the attention weights are the ones that cover x_i as well as the ones that don't. Formal: Lastly, the entity mention y_i is represented by the attention weights that cover its mention m and its context context c. Formal: The attention weights for entity m in context c are {x_i, y_i } = {x_i, y_i } and {x_i, y_i }, respectively. Formal: Here, γ is a special number that decides how much weight each entity mention gets in the attention matrix. Formal: Formal: In Figure 1, you can see an example of how to use attention weights to help with entity mention detection in a knowledge graph. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-5519.json"
    },
    {
        "casual_text": "In this add-on, we’re going to break down how we collected our data (Section A.1), explain the nitty-gritty of our algorithm (Section A.2), and dive into the results of our alignment process (Section A.3).",
        "formal_text": "In this supplementary, we describe the details of our data collection process ( § A. 1), experimental details of our algorithm ( §A. 2) and provide analysis of our alignment outputs ( §A. 3).",
        "GPT2_formal_text": ". Each time you send an update, you'll have to pay attention to the most recent chunk of the training data that most closely matches what you're proposing. Formal: In this setup, we've already figured out the reward functions for the most recent embeddings of the input tokens. But now, let's get back to the main point. Formal: The main idea behind these methods is to make sure that the token representations we get match up with the ones predicted by the teacher model. Formal: This idea is different from the one from Dyer et al. (2016), who trained their model using the teacher's predictions without considering the embeddings of the student's vectors. Instead, they focused on the most recent chunk of the training data. Formal: Another way to handle this is to just focus on the outputs from the student model's output layer. So, for the output layer, we'd get the embeddings from the output layer. Formal: This approach is different from the one from Dyer et al. (2016), who trained their model on the teacher's predictions without considering the embeddings of the student's vectors. Formal: This works because we only need to deal with the most recent embeddings, which are usually just the teacher's predictions. Formal: But here's the catch: in real-world situations, the output layer can sometimes produce output vectors that don't match the predicted ones. This makes the whole process super uncertain and can lead to bad predictions. Formal: This is a big deal for problems like image captioning, where the output layer can't predict the input. Formal: Formal: So, to fix this, we're introducing a new way of doing things called LMLM, which is based on this idea of using the most recent embeddings from the student model. Formal: LMLM is a method that uses an MLP to figure out the hidden patterns of the student's predictions and then uses that to make predictions. Formal: Formal: Formal: Basically, we take the student's output vector, which we get from the output layer, and use it to predict the output vectors of the input tokens. Formal: This is kind of like the process described by Kendall et al. (2014). Formal: Formal: Lastly, we use a logistic regression (LR) to predict the embeddings of the input tokens. Formal: The whole process is shown in figure 3.",
        "directory": "acl",
        "filename": "2020.acl-main.440.json"
    },
    {
        "casual_text": "First off, we checked out single task models to make sure our setups could hold their own against the best out there. We tested on dependency parsing (EWT), GLUE classification tasks, and machine translation (WMT14 DE-EN (Bojar et al., 2014), IWSLT15 EN-VI (Cettolo et al., 2014)) using mBERT for our embeddings. Check out Table 2 for our results on the test sets compared to other studies. For all the UD tasks, we did a bit better, but for the GLUE tasks, we consistently scored lower than the benchmarks. This is probably because of how we fine-tuned things, since the actual implementations are pretty similar. The biggest dips were in the machine translation tasks, which suggests that we might need to tweak our fine-tuning and pre-processing for these specific tasks.",
        "formal_text": "As a starting point, we evaluate single task models to ensure our implementations are competitive with the state-of-the-art. We report scores on dependency parsing (EWT), the GLUE classification tasks, and machine translation (WMT14 DE-EN (Bojar et al. , 2014), IWSLT15 EN-VI (Cettolo et al. , 2014) ) using mBERT as our embeddings. 10 Table 2 reports our results on the test sets compared to previous work. For all UD tasks, we score slightly higher, whereas for GLUE tasks we score consistently lower compared to the references. This is mostly due to differences in fine-tuning strategies, as implementations themselves are highly similar. Scores on the machine translation tasks show the largest drops, indicating that task-specific finetuning and pre-processing might be necessary.",
        "GPT2_formal_text": "ed_max:\n\nIf the output isn't masked, the attention distribution is the same as the input attention distribution. Let's say it's a random walk. The step size is the same as the length of the input. This setup is pretty straightforward and works really well for those cases where a big batch size is needed. Formal: So, our model has three main parts, which we call M, R, and T. The equation is basically: M_0 = G_0 ×d_0 + B_0, R = (1 - α + β) * d_0 + (1 - α + β) * b_0. Formal: The equation shows that M and R both equal G_0 ×d_0 + B_0, and T is just a sum of d_0 and b_0. Formal: Lastly, the equation shows that T is the same as G_0. Formal: We use this setup to figure out the attention. Formal: Using the attention from M and R, we can figure out the hidden state h_t and the attention weight, α_t, that go with it. Formal: From this, we can infer the hidden state h_t and the attention weight, α_t. Formal: The final result is the probability distribution for the target token. Formal: Finally, we use this probability distribution to handle the token attention. Formal: For the sequence-level task, we'll use the full model's parameters, including the attention parameters. Formal: Formal: For the sequence-level task, we'll use the full model's parameters, including the attention parameters, for both the encoder and decoder. Formal: Formal: This setup works really well for the task of identifying sequence tokens and the results show that it's the best fit for both the encoder and the decoder. Formal: Formal: We're using the full model's parameters, including the attention parameters, for both the encoder and the decoder. Formal: Formal: For the sequence-level task, we're using the full model's parameters, including the attention parameters, for both the encoder and the decoder. Formal: Formal: Formal: Lastly, we're using the full model's parameters, including the attention parameters, for both the encoder and the decoder. Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eacl",
        "filename": "2021.eacl-demos.22.json"
    },
    {
        "casual_text": "Alright, so let's break this down in simpler terms. We've got:\n\n- `l` which is just the number of the layer we're looking at.\n- `N_i` represents the neighbors of a specific entity `e_i`.\n- `σ` is the activation function we use.\n- `norm(•)` is a way to normalize things.\n- `V(l)` and `b(l)` are the parameters (kind of like settings) for the `l`-th layer.\n\nNow, after we process each entity `e_i` through all the layers of the GCN (Graph Convolutional Network), we take all the different representations we got and smash them together into one big representation for each entity.",
        "formal_text": "where l is the layer index, N i is the neighbouring entities of entity e i, and σ is the activation function, norm(•) is a normalization function, and V (l), b (l) are the parameters in the l-th layer. The representations of each entity e i obtained in all GCN layers are concatenated into a single representation:",
        "GPT2_formal_text": ". * ) * ) * (1, 2) Formal: The results for the verbs in the right context are in italics, and for the verbs in the left context, they're underlined. Formal: The model also creates a three-dimensional vector for each word, which we'll call W(v_i, x_i). This vector is a mix of the words' vector representations, and it's calculated using a thing called cross-entropy loss. Formal: Basically, this means the model tries to minimize something called the cross-entropy loss. Formal: You can check out the parameters and parameters for the model in Table 1. Formal: For example, in the MSCOCO dataset, the context vector is represented by the words in the sentence, which we call W(v_i, x_i). Formal: For each word, the contextual vector is figured out by using cross-entropy loss on the word embedding, which we call W(x_i, w). Formal: Here, w is the word embedding, and v_i, x_i are the vectors that are related to the word. Formal: The cross-entropy loss is calculated using a formula that looks like this: Formal: Let's say w is the word embedding, and v_i, x_i are vectors that are related to the word. Formal: So, the cross-entropy loss for the model is just a function that works with these vectors. Formal: The model is trained using two parts: Formal: The first part uses the context vector W(v_i, x_i) to figure out the probability distribution for the target word x_i. Formal: The second part is based on the context vector W(x_i, w) to guess the probability distribution for the target context. Formal: If the target context has the same word as the source context, the loss for the model will be the same. Formal: Formal: This loss is then applied to the words in the target context, so the predicted probability of the target word x_i is calculated as Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.270.json"
    },
    {
        "casual_text": "The sentence hypothesis selection module just looks at the final translation results from different systems, including the one from the glass-box combination. For every source sentence, it picks the \"best\" translation by using some feature functions to decide.",
        "formal_text": "The sentence hypothesis selection module only takes the final translation outputs from individual systems, including the output from the glass-box combination. For each input source sentence, it selects the \"optimal\" system output based on certain feature functions.",
        "GPT2_formal_text": "ed is the number of hidden units that get frozen, while attention_attention is the size of the attention mechanism. The final layer (layer l) of the sequence is then used to handle the correlation between the attention values of the past and future states. Figure 2(a) shows a step-by-step process for encoding a document using multi-head attention. Each time, the model learns the next sequence by calculating the dot product of the last hidden state and the current history vector h(t). The model is trained using this process. The time it takes to train the model is O(|H|k), where k is the number of heads and H(t) is the sequence length, both of which are numbers between 1 and the length of the sentence. This training process is done for 10 epochs, which gives us a batch size of |D| = (|D| + |H|k). Formal: Each time we encode a document, we consider the context of the document to be the feature vector H(t). This means we pick the most similar context and the current history vector h(t) to encode. Formal: For the output feature, we use the hidden state of the last head. Formal: We adjust the parameter λ by a set value λ1 and λ2 to match the parameter λ. Formal: We also use the output feature vector h(t) with a weight τ = 1 to represent the feature output. Formal: Finally, we add the parameters λ1, λ2, and λ3 to get the final result, which we use to update the model. Formal: The model is trained using the hidden state of the last head and the output feature vector h(t). Formal: Finally, we adjust the parameter λ by a set value λ1 and λ2 to match the parameter λ. Formal: Formal: Finally, we use the output feature vector h(t) with a weight τ = 1 to represent the feature output. Formal: Formal: Finally, we add the parameters λ1, λ2, and λ3 to get the final result, which we use to update the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D07",
        "filename": "D07-1029.json"
    },
    {
        "casual_text": "Using word-level details, like word n-grams and word length, just feels right in a word-based segmenter. On the other hand, character-level details, such as character n-grams, make more sense in a character-based segmenter. Sun (2010) did a deep dive into comparing these two methods, looking at both the theory and real-world results. Word-level info is better at showing how words connect in context, while character-level info shines when it comes to breaking down words and understanding their structure.",
        "formal_text": "It is more natural to use word-level information, such as word n-grams and word length, in a word-based segmenter; while it is more natural to use character-level information, such as character ngrams, in a character-based segmenter. Sun (2010) gives a detailed comparison of the two approaches from both the theoretical and empirical perspectives. Word-level information has greater representational power in terms of contextual dependency, while character-level information is better at morphological analysis in terms of word internal structures.",
        "GPT2_formal_text": "_t = w_t; ...). This model learns a relation vector h_t that can be used to figure out the sentiment of the query, R_q. Formal: The main goal of deep neural networks (DNNs) is to learn a hidden state h_t that represents the input, which is basically the output from the token-level feedforward neural network (FFNN). Formal: We use a neural network (NN) that creates a representation for the query, R_q, using the arguments from the facts. Formal: Let's say the value of R_q is 0. Formal: The hidden state h_t for the token-level feedforward neural network (FFNN) gets passed to a linear layer to create a vector representation for the argument, R_a_t. This vector is then sent to a softmax classifier to decide if the argument is positive or negative. Formal: The neural network learns the relation vector h_a_t using the facts, and then it uses that learned vector to predict the sentiment of the query, R_q. Formal: Lastly, the output from the linear layer is used to create a new word embedding for the query, which is h_q. Formal: To get the representation for the word y_t, we use the learned vector h_t for the word, y_t. Formal: The whole process is laid out in Algorithm 1. Formal: We tested our method on three popular datasets and found that our model works really well, performing better than other methods on the three datasets. Formal: We also created a new model called the entity-based Bi-LSTM (Bliutani et al., 2017) to predict the sentiment for a specific query q. Formal: We set the learning rate to 0.001 to make it easier to train the model. Formal: The BLEU score (Papineni et al., 2002) is the most commonly used metric for evaluating how well neural networks perform. Formal: We calculated the BLEU score for the human evaluation with n = 1, 2, 4, 8, 16, 32, 64, 128, 256, and 512. Formal: The BLEU score between 0 and 1 is 0.89. Formal: The BLEU score between 1 and 2 is 0.94. Formal: The BLEU score between 2 and 4 is 0.",
        "directory": "D13",
        "filename": "D13-1119.json"
    },
    {
        "casual_text": "To make things simpler and less resource-intensive, we suggest picking important representations one by one to create angles, which introduces the hyperparameters k1 and k2. We tried out different values for k1 and k2 by using token-level and sample-level triplet-wise relationships to guide the student model. To cut down on the options we had to consider, we just made k1 equal to k2. We plotted the accuracy for different values of k1 and k2, and you can see that in Figure 2. \n\nFor token-level tasks, we noticed that bumping up k1 and k2 helps boost accuracy when they're small. But once k1 and k2 hit 20 or more, the accuracy starts to fluctuate. So, we decided to go with k1 = k2 = 20 for token-level angle calculations. \n\nOn the other hand, for sample-level features using triplet-wise relationships, the accuracy just keeps going up as we increase k1 and k2. So, we just set k1 and k2 to match the batch size.",
        "formal_text": "To ease the computation and memory complexity, we propose to sequentially select important representations to form angles, leading to the hyperparameters k 1 and k 2. We test different choices of k 1 and k 2 by adopting token-level and sample-level triplet-wise relations to teach the student respectively. To reduce the search space, we simply set k 1 = k 2. We draw the accuracy curve for different choices of k 1, k 2, as shown in Fig. 2. For tokenlevel objectives, we find that increasing k 1, k 2 improves the accuracy when they are small and when k 1, k 2 ≥ 20, the curves begin to vibrate. Therefore we choose k 1 = k 2 = 20 for token-level angle calculation. While for the triplet-wise relations of sample-level features, we observe that the accuracy increases monotonically with k 1, k 2. Therefore we just set k 1, k 2 as the batch size.",
        "GPT2_formal_text": "_value is basically the attention mask value, which we get by averaging the output values from all the attention heads in the model. Formal: We're using the forward seq2seq model for this. Formal: To train the model, we train it using the cross-entropy loss function to get a fair comparison. Formal: To handle the less common cases, we use a CKY approximation, which helps to find the best value for this attention. Formal: We train the model with a batch size of 32 for up to 4 epochs. Formal: When decoding, we use beam search with a beam width of 4 for both directions. Formal: We train the model for 100 epochs. Formal: For the decoding part, we use a beam width of 4. Formal: For our test set, we set the beam width to 2. Formal: Formal: For the WMT14 dataset, we train for 50 epochs. Formal: For the WMT15 dataset, we train for 100 epochs. Formal: For the WMT16 dataset, we train for 200 epochs. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.71.json"
    },
    {
        "casual_text": "(2) Connecting the arguments in different speeches that disagree with each other.",
        "formal_text": "(2) establishing counterargument relations between such arguments found in different speeches;",
        "GPT2_formal_text": "= {}; k = i; for(k = 1; k <= n; k++ ) { for(k = 1; k <= n; k++ ) { for(k = 1; k <= n; k++ ) { arg = arg_from_source(k); } } }\n\nBasically, if we have a document D_s, we represent it using a graph G_s. For a given source token s_i, the graph G_s is a directed acyclic graph, with the nodes (s_i) being the documents in the source document D_s. The connections (e_i, e_j) between these nodes are shown as edges.\n\nIt's worth mentioning that, in the method described by Shukla et al. (2019), the feature function for the edge between an entity mention e_i and a sentence mention s_i is written as f(e_i, e_j) = (e_i, s_i). We think this function is better because it lets us represent the document's relation structure better. Formal: Even though these inputs get passed through the graph G_s to create the final output, the actual relation information is handled differently. Formal: In this paper, we assume that the graph G_s is already connected to other parts of the graph, like edges that link to the source document. Formal: For any relationship f between a source word x and a target word y, we also have a relation r between two mentions r_i and r_j. Formal: In the graph G_s, the edge between a source mention e_i and a target mention s_i is represented as a directed edge, which is defined as e_i = (e_i, s_i). We also assume that x and y are already in the graph G_s. Formal: To pass the input inputs to the graph G_s, we use a graph convolutional network (GCN), which is a type of neural network. Formal: To figure out the relationship between a source word x and a target word y, we also need a source relation r. Formal: For instance, in Figure 1, the graph G_s is built by combining a GCN and a convolutional network. Formal: We write the graph G_s as a directed acyclic graph, where the nodes (s_s) are",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "Slang is just a casual, offbeat part of language that often pops up in specific settings or follows trends in society (Dumas and Lighter, 1978). It helps people feel connected to a group (González, 1998; Bembe and Beukes, 2007; Carter, 2011) or even to a particular generation (Citera et al., 2020; Earl, 1972; Barbieri, 2008). Mattiello (2005) points out how slang adds new words to the language and follows its own unique way of forming words. Based on this, Kulkarni and Wang (2018) came up with a data-driven model to mimic how slang words are created, as Mattiello described. Some people have talked about how slang words tend to be short-lived (González, 1998; Carter, 2011), but this idea hasn’t really been tested using computational methods before.",
        "formal_text": "Slang is an informal, unconventional part of the language, often used in connection to a certain setting or societal trend (Dumas and Lighter, 1978). It can reflect and establish a sense of belonging to a group (González, 1998; Bembe and Beukes, 2007; Carter, 2011) or to a generation (Citera et al. , 2020; Earl, 1972; Barbieri, 2008). Mattiello (2005) highlights the role slang plays in enriching the language with neologisms, and claims that it follows unique word formation processes. Inspired by this, Kulkarni and Wang (2018) propose a data-driven model for emulating the generation process of slang words that Mattiello (2005) describes. Others have described the ephemerality of slang words (González, 1998; Carter, 2011), although this property has not been previously verified by computational approaches.",
        "GPT2_formal_text": "/output/sentence/sentence. The output will be a vector containing the attention values. Formal: • BERT-base-cased-fga - This is the base model trained on a mix of CNN and transformer layers, with a Gaussian bias. It has the same setup as BERT-base. Formal: • BERT-base-cased-en - This is the base model trained on an encoder-decoder setup. Formal: • BERT-base-cased-fran - This is the base model trained on a forward seq2seq model with a 1-1 feedforward setup. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.101.json"
    },
    {
        "casual_text": "Share the outcomes of the project at conferences, scientific meetups, and industry events. This will make a big difference not just in the countries involved but also in other places.",
        "formal_text": "• Disseminate the results of the Action through conferences, scientific and industrial gatherings, which will have substantial impact in the participating countries and beyond.",
        "GPT2_formal_text": "\" and \"users_ids\" are calculated using a straightforward linear transformation, which is part of the GLSA algorithm. The results of this linear transformation are then sent to a classifier that was trained on the training set, which helps predict how the model will perform on the test set. Formal: Basically, if a user isn’t in the training set, the model uses a simple function to figure out how important their task is. If the task is important, the model figures out a score for it, kind of like a \"summary\" for that task. Formal: When we’re generating a sentence, the model is like a black box, meaning we don’t know what it’s trying to do. So, we’re starting with a model that’s only partly built—it’s missing some pieces. Formal: The model’s job is to generate a summary that includes all the key pieces that make up the task. Formal: If the model isn’t doing a great job, we can use the steps we’ve figured out in this paper to fix it. Formal: Once we’ve gotten the model to generate the summary, we can use that summary to figure out how likely the model is to produce the task-specific summary. Formal: To make sure we don’t accidentally mess up the model by picking the wrong summary, we create a special label called \"the_reference\" that tells us which summary we’re aiming for. Formal: The training of the model is done by tweaking a few parameters. Formal: The user’s task, t, is represented by a pair of vectors, x_t and y_t, where x_t is the training data and y_t is the test data. Formal: Formal: The parameters θ, θ', ..., θ¹ are kind of like the learned weights for this model, and the big question is: how do we find the best θ, θ', ..., θ¹ for our model? Formal: We use the trainable parameters θ, θ', ..., θ¹ to calculate the training loss and minimize the cross-entropy loss, which basically means we want to minimize the cross-entropy loss. Formal: This approach, called log loss (from Lafferty et al., 2001), is pretty straightforward and works",
        "directory": "eamt",
        "filename": "2022.eamt-1.63.json"
    },
    {
        "casual_text": "Some categories have way fewer questions than others—like astronomy, which only has 331 questions. So, we’re just focusing on literature and history, since they make up over 40% of the whole set. That gives us 21,041 history questions and 22,956 literature questions to work with.",
        "formal_text": "Because some categories contain substantially fewer questions than others (e. g. , astronomy has only 331 questions), we consider only literature and history questions, as these two categories account for more than 40% of the corpus. This leaves us with 21, 041 history questions and 22, 956 literature questions.",
        "GPT2_formal_text": "_encoder. It uses a linear layer to handle the multilingual attention mapping and a linear layer with ReLU activation to create the contextual representation. Formal: We used the supervised baseline approach in the experiments because the hidden representation for the target language is already set. Formal: We tested our model on two crosslingual RE tasks: SemEval-2010 Task 4 and SemEval-2011 Task 2. The results are in Table 2. The model with the bigram language model (MTM) performed the best. This is because it uses a hierarchical context-aware model and gets a better embedding in the target language. However, the bigram model doesn’t perform as well as MTM and MTG. Also, the bigram model’s performance is pretty much on par with MTM, except for the fact that it can’t handle extra-linguistic details. Formal: We also tried different sequence generation models, like Fin-Fin and Fin-Gram. The results for Fin-Gram are in Table 2. Formal: Fin-Gram is a pretty straightforward model that doesn’t need a clear grammar or any fancy techniques. Formal: For SemEval-2010, we compared the sequence generation model with MTM and MTG to two other models: RNNG and GloVe. We also looked at the results from SemEval-2011 Task 2. Formal: RNNG is a deep neural network that can generate embeddings for word embeddings and sentence embeddings, which is what our model is based on. Formal: We tested Fin-Gram on two crosslingual RE tasks: SemEval-2010 Task 4 and SemEval-2011 Task 2. The results are in Table 2. Formal: We tested RNNG and GloVe on two crosslingual RE tasks: SemEval-2010 Task 4 and SemEval-2011 Task 2. We also included results from SemEval-2011 Task 1. Formal: We tested Fin-Gram on two crosslingual RE tasks: SemEval-2010 Task 4 and SemEval-2011 Task 2. Formal: We tested Fin-Gram on two crosslingual RE tasks: SemEval-2010 Task 4 and SemEval-2011 Task 2. Formal: We included results from SemEval-2011 Task 1. Formal: The results for Fin-G",
        "directory": "D14",
        "filename": "D14-1070.json"
    },
    {
        "casual_text": "Basically, we think our study is a cool move toward combining computational social science and derivational morphology. In the next phase, we plan to tweak our approach even more to handle MFEP better.",
        "formal_text": "Overall, we see our study as an exciting step in the direction of bringing together computational social science and derivational morphology. In future work, we intend to further fine-tune our methodological apparatus for tackling MFEP.",
        "GPT2_formal_text": ", attn_ids, and attn_topic_ids. Since the human contributors weren't familiar with the algorithm, they didn't know which dataset to pick and they didn't agree on the topic labels, so they couldn't give a good opinion. The results were all over the place, ranging from kind of positive (like S1) to negative (like S2). Formal: Lately, there's been a lot of buzz about using deep learning models for stuff like summarizing text or finding information (like the work by Collobert et al., 2011; Pang et al., 2014; Zhou et al., 2014; Guo et al., 2014). The most popular deep learning model is RNN, which has been pretty successful in summarization and has a ton of examples to learn from. Formal: Our RNN model is basically a simpler version of RNN, where we only keep the words that show up in the text. Formal: The RNN model has three parts that work together: the input sentence, the words in it (like words w and z), and the hidden state h, which is basically a vector that tells us how likely each word is to appear in the next sentence. Formal: We focus on the word w i in the context of the current word z i, which gives us the predicted next word y i. Formal: If the prediction is right, we use the hidden state to update the output vector. Formal: After the last update, the output is the vector that represents the word (w). Formal: To make sure we don't overfit, we use dropout with a probability of 0.1. Formal: The total loss we calculate is the sum of losses from the last two steps, and we do this using a gradient descent method. Formal: Since the model only looks at the word position, the loss here can be expressed as L w = log p (w i |z i ) = 1 − log p (z i |w i ), where p (w i |z i ) = 1 − log p (z i |w i ) (1 − log p (z i |w i )). Formal: Formal: This formula is pretty straightforward and can be written in a straightforward way. Formal: We make sure to use the sample embedding (e) from the task's dataset D t and the log probability of the next word's embedding (f). Formal: The model's",
        "directory": "acl",
        "filename": "2020.acl-main.649.json"
    },
    {
        "casual_text": "So, N + (0, 1) and N − (0, 1) are just the positive and negative parts of the normal distribution, but cut off at 0.",
        "formal_text": "Where N + (0, 1) and N − (0, 1) is the positive and negative truncated normal distribution, truncated at 0, respectively.",
        "GPT2_formal_text": "ed( self ): attn = attn[ self.position] self.masked_attn = self.masked_attn[ self.position] self.masked_attn_int = self.masked_attn_int[ self.position] self.masked_attn_shape = self.masked_attn_shape[ self.position] self.masked_attn_len = self.masked_attn_len[ self.position] self.masked_attn = None self.masked_attn_shape = None self.masked_attn_len = None self.masked_attn = None self.attn_pos = None self.attn_len = None self.attn = None self.attn_pos = None self.attn_len = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn = None self.attn",
        "directory": "D19",
        "filename": "D19-1661.json"
    },
    {
        "casual_text": "But, super high scores might not tell the whole story. There’s some debate about whether these probing tasks actually check if the model truly understands the structure of language or if the results are just being interpreted correctly (Hewitt and Liang, 2019; Zhang and Bowman, 2018; Voita and Titov, 2020; Pimentel et al., 2020b). To figure this out, the next part will look at different probing methods used with language models and how we judge how well a probe works.",
        "formal_text": "However, high performance may have confounding factors; there is uncertainty on whether the probing tasks properly test if representations actually encode linguistic structure and on how to interpret the results of probes (Hewitt and Liang, 2019; Zhang and Bowman, 2018; Voita and Titov, 2020; Pimentel et al. , 2020b). Toward that end, the following section reviews several probing approaches in the context of language models, and the evaluation criteria used to determine the proficiency of a probe.",
        "GPT2_formal_text": "= {'s1', 's2', 's3', 's4', 's5', 's6', 's7', 's8', 's9'}, \n\nFigure 1: The basic setup for a multi-class classification task. It uses both the image features and the model's predictions for the labels. Formal: For each class, we create a dataset of images using the gold labels. For the negative class, we create a dataset of negatives using the gold labels. Formal: Each image has a bunch of labels, and we use a multi-label classification model. Formal: We also add extra labels to the images to make the dataset bigger. Formal: We use a modified version of the CoNLL09 dataset to train the model, which is shown in Fig. 2. Formal: In our experiments, we used the top-k negative samples as negative labels. Formal: The negative label dataset is pretty similar to the gold label dataset in size. Formal: When we use the negative class dataset as negative labels, the gold label dataset ends up being a bigger dataset than the negative one. Formal: The negative labels are used to create a negative dataset. Formal: After training, we calculate the cross-entropy loss for the classification task. Formal: For multi-class classification, we use the gold labels. Formal: We train the model using cross-entropy loss to calculate the cross-entropy loss for the classification task. Formal: We also include a linear layer to deal with the non-linear relationship between the image and label features. Formal: For multi-class classification, we use the gold labels. Formal: Finally, we calculate the cross-entropy loss for the classification task. Formal: We train the model using cross-entropy loss to calculate the cross-entropy loss for the classification task. Formal: Formal: We also add a linear layer to deal with the non-linear relationship between the image and label features. Formal: Lastly, we calculate the cross-entropy loss for the classification task. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "bigscience",
        "filename": "2022.bigscience-1.11.json"
    },
    {
        "casual_text": "Event argument detection is super important for event extraction. It’s kind of like semantic role labeling (SRL), where the goal is to figure out which words or phrases fit into the roles of an event. But with event arguments, things can get a bit more complicated—they might not just be within the same sentence. Sometimes, they can be in different parts of the document, like when an argument is implied or hidden somewhere else. Take a look at Figure 1 for an example: in the purchase event triggered by the word \"bought,\" the money argument is actually found in the sentence before.",
        "formal_text": "Event argument detection is a key component in the task of event extraction. It resembles semantic role labeling (SRL) in that the main target is to find argument spans to fill the roles of event frames. However, event arguments can go beyond sentence boundaries: there can be non-local or implicit arguments at the document level. Figure 1 shows such an example: for the purchase event, which is triggered by the word \"bought\", its money argument appears in the previous sentence.",
        "GPT2_formal_text": "( s = {s1, ..., sT}) = sT to sT T−1. The score is normalized by the softmax function. The weight matrix for the output layer has two main parts: the embedding vector w_i, which is the i-th word embedding, and the hidden representation h_i, which is the i-th hidden representation. For each pair (s1, ..., sT), the output layer has a hidden dimension of d_m, and each element has an embedding dimension of d_n. Formal: We've figured out the attention weights for the sentence and token embedding layer using the BERT model. Formal: We also get the attention weights for the word and token embedding layers. The token embedding layer has an embedding dimension of d_s = d_s + d_m, and the word embedding layer has an embedding dimension of d_n = d_n + d_m. Formal: For the POS embedding layer, the embedding dimension is d_p = d_p + d_n, and the hidden representation for that layer is d_h = d_h + d_n. Formal: Finally, we calculate the cross-attention weights for the sentence and token embedding layers. Formal: To do this, we create a vector for each word and token embedding pair. This vector has a dimension of d_v = d_v + d_n, and the hidden representation for this vector is d_h = d_h + d_n. Formal: To match the attention weight matrix, we add a length-normalized version of the attention weight matrix. Formal: We also calculate the cross-attention weights for the POS embedding layer. Formal: To train our model, we first use the BERT model's transformer architecture to process the input data. We set the transformer's parameters to be the parameters of the BERT model's output layer. Formal: To train the model using the word embedding layer, we first train the base model using the tokens in the training data. We then train the phrase-level model using the tokens in the training data and the tokens in the test data. The final model has a hidden dimension of d_s = d_s + d_m, and the word embedding layer has an embedding dimension of d_n = d_n + d_m",
        "directory": "acl",
        "filename": "2020.acl-main.667.json"
    },
    {
        "casual_text": "These findings suggest that mined coreference chains are a pretty handy resource and offer insights that work well alongside other techniques. While adding coreference-based embeddings does boost performance in antonym classification, the experiments also highlight that relying solely on coreference-based embeddings usually doesn’t work as well as sticking with text-based embeddings. This makes sense when you think about it—the amount of training data for the word embeddings is different in both cases. Coreference chains only cover a tiny fraction of the word-word relationships that the word2vec skip-gram model gets when it’s applied to raw text. If both methods had similar amounts of training data, we’d probably see similar results.",
        "formal_text": "These results show that mined coreference chains are a useful resource and provide information that is complementary to other methods. Even though adding coreference-based embeddings improves performance on antonym classification, the experiments also show that using only coreference-based embeddings is almost always worse than using only text-based embeddings. This is not surprising given that the amount of training data for the word embeddings is different in the two cases. Coreference chains provide only a small subset of the word-word relations that are given to the word2vec skip-gram model when applied to raw text. If the sizes of the training data sets were similar in the two cases, we would expect performance to be comparable.",
        "GPT2_formal_text": ") = (num_sentences, len(sent_tokens))  \nval = sigmoid(cosine(vw))  \nreturn sigmoid(vw) + 1 * tanh(sum(x) - sum(x[i]) + 1)  \n}  \n\nIn the second step, we do this process over and over again. Formal: We train a model that understands grammar using this setup. Formal: The model learns to predict a label for each possible outcome. Formal: We use the model to create a training dataset by picking the right examples for the labels. Formal: We use this dataset to tweak the model and train it further using a data augmentation method. Formal: In the third step, we use the augmented dataset to train the model again using the augmentation method. Formal: After each training round, we update the model's parameters based on the augmented dataset. Formal: The model then uses this updated model to predict a new label for a sentence. Formal: Finally, we use the new predicted label to update the parameters of the model. Formal: In this part, we introduce a new approach called deterministic phrase-based model distillation (DPMD). The idea is to train the model using a dataset that’s already been trained on a similar topic. Formal: For the final step, we use the outputs of the model that’s been trained so far to guide the training of the new model. Formal: We train this new model using the augmented dataset. Formal: Finally, we use the new model to generate a training dataset. Formal: We fine-tune the model using this updated dataset. Formal: We also explain how to train the model using this new dataset. Formal: For the final step, we use the outputs of the model that’s been trained so far to guide the training of the new model. Formal: We train this new model using the augmented dataset. Formal: Formal: Formal: We also explain how to train the model using this new dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1151.json"
    },
    {
        "casual_text": "We're planning to dive deeper into feature engineering for instance-based SRL. Specifically, we want to look into ways to automatically pick out the best features, especially when it comes to composite features. We're also thinking about testing how well things like word clusters or word embeddings work in instance-based SRL.",
        "formal_text": "We intend to focus more specifically on feature engineering for instance-based SRL. In particular, we plan to explore automatic feature selection methods especially in the context of composite features. We also plan to evaluate generalization features such as word clusters or word embeddings in the context of instance-based SRL.",
        "GPT2_formal_text": ". Formal: (1) Formal: We can figure out the attention weights using the accuracy of the prediction. Formal: We do this by calculating the accuracy for the first prediction in the regression, and then we use that to adjust the attention weights. Formal: We use this approach to figure out the attention weights for the regression loss. Formal: (2) Formal: We take the average of the attention weights from the first prediction in the regression, as mentioned in Equation 2. Formal: Basically, to calculate the attention weights for the regression loss, we first calculate the accuracy for the first prediction, which we call 'A_1_i'. Then, we adjust these weights by multiplying them by the normalized accuracy of the first prediction, which we call 'A_1'. Formal: In this setup, we calculate the attention weights using the first prediction in the regression. We refer to this as the first prediction weight, and we use the result to adjust the attention weights. Formal: The final attention weight for the regression loss is calculated using this same method. Formal: For training, we have two things to consider: the model's loss function and the input for the target language model. Formal: (3) Formal: We add more knowledge to the model using cross-lingual language modeling. Formal: Formal: We train the model using the target language model using the regular supervised learning method. Formal: (4) Formal: In this setup, the model is trained on the target language model without any knowledge about the source language, and it's tested on the source language model. Formal: Formal: For more specifics, you can check out the Supplementary Material. Formal: (5) Formal: We use the target language model as a second language model for the same task. Formal: Formal: (6) Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1058.json"
    },
    {
        "casual_text": "HPSG's type system includes something called parametric types, like the one shown in Figure 1 from (PS94). Unlike regular types and features, we don't really understand how powerful parametric types are. In fact, they've never been properly formalized in a way that could be used for HPSG parsing, so we couldn't even compare them to other types. This paper talks about a formalization of parametric types, based on the typed attribute-value logic from (Car92). This logic is special because it has a strict idea of what's \"appropriate\"—it tells us which features an object of a certain type can have and what types those features can take. It also says that every feature structure must have the right values for all the features that match its type.\n\nBefore, people thought that every parameter in a subtype had to be in all its supertypes and vice versa. This made it impossible to use something like Figure 1, because if _1_ was a list(X) and A_ was parametric, then everything else would have to be too. But this paper changes that (check out Section 2). It gets rid of that rule by saying there should be a simple most general type (which (Car92)'s logic already requires). This general type is then used during type-checking and figuring out new parameters.",
        "formal_text": "HPSG's type system also includes parametric types, e. g. , Figure 1, from (PS94). In contrast to the relative expressive potential of normal typing and features, the expressive potential of parametric types is not at all understood. In fact, parametric types have never been formalized in a feature logic or in a manner general enough to capture their use in HPSG parsing so that a comparison could even be drawn. This paper summarizes such a formalization, 1 based on the typed attribute-value logic of (Car92). This logic is distinguished by its strong interpretation of appropriateness, a set of conditions that tell us which features an object of a given type can have, and which types a feature's value can have. Its interpretation, total well-typedness, says that every feature structure nmst have an appropriate value for all and only the appropriate features of its type. Previous approaches have required that every parameter of a subtype should be a parameter of all of its supertypes, and vice versa; thus, it would not be 1The full version of this paper presents a denotational semantics of the logic described here. possible to encode Figure 1 because _1_ __ list(X), and if A_ were parametric, then all other types would be. 2 The present one eliminates this restriction (Section 2) by requiring the existence of a simple most general type (which (Car92)'s logic requires anyway), which is then used during type-checking and inferencing to interpret new parameters.",
        "GPT2_formal_text": "= { l_i } | l_i is in [0, i+1] = {l_i+1, . . . , l_i+m}. Formal: Here, δ(•) is a non-linear function that works with N input dimensions. These parameters are learned as the model trains. Formal: We also use a new lookup table to handle the graph attention and update the output attention features. This lookup table is calculated using Equation 7. Formal: The embedding of the entity pair is based on the embedding of the path connecting them. Formal: A sentence is considered a full utterance if it has at least one entity mention and one path mention. Formal: For now, we’re skipping sentences with fewer than n entities. For each entity mention p i, we calculate the embedding for the path connecting p i to the entity mention h i. Formal: To see how well we’re doing, we check the performance of our model against three human-written datasets. Formal: We use the standard Stanford CoreNLP toolkit (Manning et al., 2014) to evaluate our model. Formal: For the human evaluations, we use the BERT model (Devlin et al., 2018). Formal: We use the same evaluation metrics as the publicly available models. Formal: The results of the evaluation are shown in Tables 7 and 8. Formal: Here, v_i represents the main entity mention. Formal: We also calculate the weight for the embedding of the path mention v_i by summing up the embedding values for the entity mention v_i for each path mention. Formal: The results of the evaluation are shown in Tables 7 and 8. Formal: We use the same evaluation metrics as the publicly available models. Formal: We evaluate the model by calculating the accuracy (acc) and the F1 score (F1) for the top-1 result. Formal: Formal: We also calculate the weight for the embedding of the path mention v_i by summing up the embedding values for the entity mention v_i for each path mention. Formal: The results of the evaluation are shown in Tables 7 and 8. Formal: We calculate the weight for the embedding of the path mention v_i by summing up the embedding values for the entity mention v_i for each path mention. Formal: Formal:",
        "directory": "C98",
        "filename": "C98-2164.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. In Table 3, we tested how well a Semantic-aware Model works when combined with BERT. The results show that this combo performs the best. This tells us that BERT and semantic dependency graphs can work together nicely for solving the Chinese zero pronoun problem.\n\nBut when we compare the Semantic-aware Model without BERT to the Baseline Model with BERT, we see that BERT plays a bigger role in improving performance than the semantic dependency graph. Plus, BERT boosts the Baseline Model more than it does the Semantic-aware Model. This suggests that BERT might already be capturing some of the semantic information that the dependency graph would provide.\n\nLooking ahead, we think it would be cool to combine BERT with not just the semantic dependency graph, but also semantic role labeling. This could make BERT even better at handling Chinese zero pronouns in the future.",
        "formal_text": "Semantic-aware Model with BERT 57. 9 Table 3: Integration with BERT According to the experimental results in the Table 3, we can see that the performance of the Semanticaware Model with BERT is the best. This shows that BERT information and semantic dependency graph information have certain complementarity in the Chinese zero pronoun resolution task. But by comparing the performance of the Semantic-aware Model without BERT and Baseline model with BERT, We can see that the BERT information contributes more to the Chinese zero pronoun resolution task than the semantic dependency graph information. In addition, we can also see that BERT information improves the Baseline Model more than the Semantic-aware Model. This shows that the BERT model may encode part of the semantic information of the semantic dependency graph. Based on the above analysis, we hope that in the future research, we can further integrate the semantic dependency graph and even the information of semantic role labeling on the basis of the BERT model, so as to further enhance the ability of the BERT model in the Chinese zero pronoun resolution task.",
        "GPT2_formal_text": ". The regular model, called teacher_f, is a multi-head attention network that focuses on the teacher's attention, while the generator model, g, is a decoder that processes the data in the same way as the teacher. Formal: In this project, we're looking at how to automatically figure out the gender of a person from their tweets. First, we'll explain what the main tasks are, and then we'll show how we can automatically find tweets with specific gender labels. Formal: We're using a pre-trained BERT-Base model with a 5-layer transformer setup (like the one from Devlin et al., 2019) to handle the text. Formal: We also tried out a Bi-LSTM model for this, which we call \"GenBi.\" This model has three layers of Bi-LSTM cells, each with a hidden layer and a ReLU activation function. This is based on the work by Sanh et al. (2019). Formal: We've shared the code for these models, including the model-specific hyperparameters, and the code for the model evaluation. Formal: The model evaluation tool uses something called the \"num_eval\" feature. Formal: We've also included a script to automatically check the quality of the tweets and run the validation process with the validation set. Formal: We'll use the model evaluation script we talked about to evaluate the tweets. Formal: The script we used to automatically check the quality is also available. Formal: Lastly, we're sharing the results from three tweets from three different individuals. Formal: Lastly, we're sharing the results from three tweets from three different individuals. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "ccl",
        "filename": "2020.ccl-1.77.json"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms.\n\nN is just the number of examples we use to train the model. yᵢ and ŷᵢ are the actual and predicted labels for the i-th instance, respectively. Θ represents all the parameters that the model can adjust during training. λ is the value that controls how much we care about L₂-regularization, which helps prevent overfitting.\n\nNow, for the experimental setup part, we'll set up everything to test how well the model performs.",
        "formal_text": "where N is the training data size. y i andŷ i respectively represent the ground-truth and estimated label distribution of instance i. Θ denotes all trainable parameters of the model, λ represents the coefficient of L 2 -regularization. 4 Experimental Setup",
        "GPT2_formal_text": "= {...}  \n\nBasically, if the model is trained on a dataset where words are tokens, it should automatically figure out the label for any word that shows up in the text. Formal: For a given sequence of words y i, the label id x i for the next word in the sequence is calculated using a formula like this: Formal: If the model isn’t trained on a dataset with a fixed number of tokens, it should figure out the label for any word that’s next to the current token. Formal: In this paper, we’re focusing on the first and last tokens of a sequence, and we’re assuming that the model has already figured out the label for the token in the previous token. Formal: This setup is all about creating a quick and efficient method for detecting taggers. For example, we can figure out the label for any token in a dataset with just one token in between. Formal: But, this method has its limitations. When dealing with a sequence with a fixed number of tokens, we still need to predict the label for every word in the sequence. This means that in real-world situations, a label might not be available just after the token we’re looking at. Formal: To deal with this issue, we’re suggesting a new method called Pause-and-Discount (PDD) training, which starts with a fixed number of tokens and gradually adjusts the label distribution. Formal: To make sure the model can learn the label for the token in the last token in a sequence, we’re planning to add an early-out (EOS) mechanism. Formal: We’re also thinking about adding a method that makes the model pay attention to the order of words in a sequence. This could help the model decide which token is likely to be the right one. Formal: In our experiments, we used a model that’s based on a BERT-based deep learning framework (like the one by Devlin et al., 2019). This model was pre-trained using a sequence of word embeddings called W w. Formal: This model has a set of internal parameters that help it figure out the label for each token in a sequence. Formal: These parameters are updated using a method called autoregressive learning. Formal: In this part, we’re introducing our own model called Q-Dimensional Wave-GANIC,",
        "directory": "acl",
        "filename": "2022.acl-long.124.json"
    },
    {
        "casual_text": "Stepwise regression isn't a good fit for this kind of analysis because there just aren't enough language pairs to work with in each setup (for example, PanLex only has 14 pairs for each source-to-target language selection). This small number makes it hard to get reliable results, which is what stepwise regression needs. So, we decided to go with a standard multiple linear regression model instead. In this model, we use the isomorphism measure that has the strongest individual correlation along with some other linguistic measures. Just like with stepwise regression, we’ll share the overall correlation coefficient, r.",
        "formal_text": "Stepwise regression analysis is not suitable for the selection analysis due to the limited number of language pairs in each language selection setup (e. g. , PanLex offers 14 language pairs for each source-or target-language selection analysis). These conditions impede the statistical significance power of the tests which stepwise regression requires. We therefore opt for a standard multiple linear regression model instead; the regressors include the isomorphism measure with the highest individual correlation combined with the linguistic measures. Similarly to the stepwise analysis, we report the unified correlation coefficient, r.",
        "GPT2_formal_text": ". This operation, based on the previous one, grabs the hidden representation h_i, which is a sequence of numbers that represents the attention weights for all the tokens in the current sentence. Formal: Let's call the token representation h_i. We also need to figure out the attention weights. Formal: For each token representation h_i, we calculate the attention weights for it and the next token. Then, we use those attention weights to update the attention for the final token. Formal: For the fully connected layer, we take the hidden representation h_i and multiply it by the multi-head attention weights. We also apply the dot product between the hidden and output representations. Formal: Lastly, we pick the most important token from the selected list. Formal: Formal: Our framework is all about tackling the multimodal sequential text generation problem and using three main approaches to achieve this goal. Formal: Formal: We use the same setup for the two-step generation process. Formal: Formal: Basically, in each step, we generate a sequence of tokens based on the output of the previous step. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.186.json"
    },
    {
        "casual_text": "So, when people try to assign keyphrases without repeating themselves, they usually look back at what they've already said. Inspired by that, we created a \"target side review context set\" that keeps track of the context of the phrases we've generated so far. By using this context along with an attention mechanism, we can better predict the next phrase. We call this the \"review mechanism.\"",
        "formal_text": "Considering human behavior on assigning keyphrases that review previous phrases to avoid duplicate assignment, we construct a target side review context set which contains contextual information of generated phases. The target context with an attention mechanism can make use of contextual information of generated phrases to help predict the next phrase, which we call the review mechanism.",
        "GPT2_formal_text": "= {}, sentence_ids = {}, from = [], to = [ ], from_idx = [], from_idx_props = [], ..., from_idx_props_props = [], ..., in_ids = [], out_ids = [], a = [], a_props = [], ..., a_props_props = [], ..., a_props_props_idx = [], ..., a_props_props_idx_props = [], ..., a_props_props_idx_props_idx = [], ..., a_props_props_idx_props_idx_props = [], ..., a_props_props_idx_props_props = [], ..., a_props_props_idx_props_idx = [], ..., a_props_props_idx_props_props_props = [], ..., a_props_props_idx_props_idx_props = [], ..., a_props_props_idx_props_idx = [], ..., a_props_props_idx_props_idx_props = [], ..., a_props_props_idx_props_idx_props_props = [], ..., a_props_props_idx_props_idx_props_props = [], ..., a_props_props_idx_props_idx_props_idx = [], ..., a_props_props_idx_props_idx_props = [], ..., a_props_props_idx_props_idx = [], ..., a_props_props_idx_props_idx_props_props = [], ..., a_props_props_idx_props_idx_props_idx = [], ..., a_props_props_idx_props_idx_props_idx_props = [], ..., a",
        "directory": "D18",
        "filename": "D18-1439.json"
    },
    {
        "casual_text": "Smart To-Do: Two-Step Process\n\nAlright, so here's how we handle creating To-Do items in two steps. In the first step, we...",
        "formal_text": "Smart To-Do: Two Stage GenerationIn this section, we describe our two-stage approach to generate To-Do items. In the first stage, we",
        "GPT2_formal_text": ", g_mask, and b_mask as vectors. The input, which is a sequence of numbers, is turned into a vector using a linear projection called h_input, which is calculated by adding up all the weights from the attention layers. Formal: Formal: Instead of using the standard sentence embedding matrix M, we add a binary hidden state, h_b_i, that's 1 or 0. This hidden state helps estimate the probability p(i|h_b_i). Formal: The input is an n-gram vector x = (x1, ..., xn). We take the embedding of the input sentence x and its corresponding output y from the sentence encoder and use them as the input for the sentence decoder. Formal: We create a loss function like this: Formal: We use the maximum-likelihood estimation for the loss. Formal: We also calculate a cross-entropy loss by adding up the cross-entropy losses for all the embedding vectors, which gives us the cross-entropy loss. Formal: Following the approach from Yang et al. (2019), we use cross-entropy loss for an undirected GCN. Formal: We train the GCN using the input X = (x1, ..., xn). Formal: The output is the cross-entropy loss for the sentence. Formal: The method we use for training the GCN involves two main steps: Formal: We use cross-entropy loss to train the GCN. Formal: We also calculate a cross-entropy loss for the sentence. Formal: Formal: We calculate cross-entropy loss for the sentence. Formal: We train the GCN using the input x = (x1, ..., xn). Formal: Finally, the output is the cross-entropy loss for the sentence. Formal: Formal: We train the GCN using the input x = (x1, ..., xn). Formal: Formal: We also calculate a cross-entropy loss for the sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.767.json"
    },
    {
        "casual_text": "We've tested the translation checker on eight different texts: two novels, two software manuals translated by hand, three software manuals translated using a translation memory tool, and one set of dialogue fragments that was machine-translated. The novels aren't super interesting for checking consistency, mainly because they’re way less repetitive than the manuals, and also because consistency isn’t always a big deal in literary translations. For more details, check out the section \"Ange relationer mellan tabeller\" in Chapter 7, \"Grunder för tabeller.\" SOURCE 1:",
        "formal_text": "We have run the translation checker on eight different texts, two novels, two manually translated software manuals, three software manuals translated with the aid of a translation memory tool, and one MT-translated collection of dialogue fragments. The two novels are not really interesting applications for consistency checking, partly because they are not at all as repetitious as the manuals and partly because consistency is not something necessarily aimed for in literary translation. Mer information finns i avsnittet \"Ange relationer mellan tabeller\" i kapitel 7, \"Grunder för tabeller\". SOURCE 1:",
        "GPT2_formal_text": "* (int) + (int) + (int) + (int) + (int) + (int) + (int) = (int) + (int) + (int) + (int) + (int) + (int) + (int) + (int) + (int) = (int) + (int) + (int) + (int) + (int) + (int) + (int) = (int) + (int) + (int) + (int) + (int) + (int) = (int) + (int) + (int) + (int) + (int) + (int) = (int) + (int) + (int) + (int) + (int) = (int) + (int) + (int) + (int) + (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = = (int) = = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (int) = (",
        "directory": "amta",
        "filename": "1996.amta-1.16.json"
    },
    {
        "casual_text": "This part takes in a bunch of vector representations, each one matching up with a single token. These vectors are labeled as d t, and they're in a space with a certain dimension, l. Think of it like each token has its own little vector. For instance, this input could come from the co-attention layer we talked about in Section 3. The way this layer works is...",
        "formal_text": "This layer takes input as a sequence of vector representations corresponding to individual tokens, d t ∈ R l, where l is the given vector dimension. For example, such input can be the output of the co-attention layer in Section 3. The operation of this layer is defined as",
        "GPT2_formal_text": "ed = [att_masked | att_op] s = [s1, ..., sI, ..., sm], where I and j can be 1, 2, ..., m and j are the number of word embeddings in the input sentence and the token embedding. Formal: For the discriminative model, we used the same setup as Barhom et al. (2019). Formal: We trained the decoder using an ensemble of the encoder, the classifier, and the discriminator. Formal: If the encoder and classifier work well together, adding the discriminator can help reduce the differences between the outputs from the different models. In the discriminative model, we set δ to 0.5. Formal: We set the window size of the decoder to 128 and the learning rate to 0.01. Formal: For the classification part, we added a logistic classifier to the encoder. Formal: The decoder has a hidden size of 256. Formal: We made the decoding matrix D_enc to have the same size as the input embedding. Formal: We trained the classifier and the discriminator together using beam search. Formal: We trained the decoder using the adversarial training method. Formal: We trained the classifier and the discriminator together using beam search. Formal: We trained the classifier and the discriminator together using beam search. Formal: We trained the decoder using the adversarial training method. Formal: We trained the classifier and the discriminator together using beam search. Formal: We trained the classifier and the discriminator together using beam search. Formal: Formal: We trained the classifier and the discriminator together using beam search. Formal: We trained the classifier and the discriminator together using beam search. Formal: Formal: We trained the classifier and the discriminator together using beam search. Formal: Formal: We trained the classifier and the discriminator together using beam search. Formal: Formal: We trained the classifier and the discriminator together using beam search. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1237.json"
    },
    {
        "casual_text": "Parameter settings. One cool thing about HAPS is that you don’t need to know how many segments you want beforehand. Instead, you just set how much you care about each level. HAPS is pretty flexible with these preferences, so this general setting is a handy way to adjust the level of detail in your segmentation without having to pick the exact number of segments for each part of the tree. In our case, we kept things simple by setting the preferences evenly across the board, but you could definitely add more specific details if you wanted to.",
        "formal_text": "Parameter settings. An important advantage of HAPS is that it does not require the number of segments in advance. Instead, the user needs to set the preference values for each level. However, HAPS is fairly resistant to changes in preferences and this generic parameter is a convenient knob for fine-tuning the desired granularity of segmentation, as opposed to specifying the exact number of segments at each level of the tree. In this work we set preferences uniformly, but it is possible to incorporate additional knowledge through more discriminative settings.",
        "GPT2_formal_text": "= {int_tokens} from model_tgt. This is done using a linear transformation, which basically means we take the word embedding and turn it into a 1D vector. To make sure this is correct, we use a softmax function to calculate the score between the original word embedding (w_i) and the transformed version (w_t). Formal: Okay, let's break this down in a simpler way:\n\nFirst, we have:\n- u_t_i: 0\n- u_t_j: 1\n- w_i: 0\n- w_j: 1\n- h_t_i: 0\n- h_t_j: 1\n- t_i: 0\n- t_j: 1\n\nBasically, it's a bunch of equations involving u_t_i, u_t_j, and w_i, w_j, and h_t_i. Formal: \n\nNow, for the sum of all the tokens:\n- Formal: We're looking at the sum of all the tokens, no matter what, and averaging them out, like: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C14",
        "filename": "C14-1005.json"
    },
    {
        "casual_text": "Vocabulary growth models help us see how words change over time in a specific area by looking at the relationship between different types of words and how often they're used. Macau, being a mix of different languages and cultures, can show us what's important in society based on how people use words. But no one has really looked at how Macau's vocabulary has changed over time—until now.\n\nThis paper is the first to create a big collection of texts from Macau Chinese over time. We used three different models to see how the vocabulary changed in these texts and picked the best one, the Heaps model, to dig deeper. It turns out that the way words change in Macau is closely tied to what's trending in the news, government policies, and everyday life.\n\nTo make sure our method works, we also tested it on texts without any time information, and it still gave us good results. This is the first time anyone has studied how Macau's vocabulary has evolved using such a big collection of texts over time. This research helps us understand Macau's language life better and how it's developed over the years.",
        "formal_text": "Vocabulary growth models can reflect the diachronic change of vocabulary in a certain field by fitting the quantitative relationship between word types and tokens. As a place of multi-language and multi-cultural integration, Macau's vocabulary use can reflect the focus of society, but there is no research on Macau's diachronic vocabulary growth. This paper constructed a diachronic corpus of Macau Chinese for the first time, used three vocabulary growth models to fit the vocabulary changes in the corpus, and selected the Heaps model with the best effect to further analyze the relationship between vocabulary change and newspaper content. The results reflect that the changing trend of Macau vocabulary is closely related to hot news, Macau's policy guidelines and people's livelihood. This research also uses the out-of-order text after removing the text timing information to verify the effectiveness of the method. This is the first study to investigate the evolution of Macau vocabulary based on a large-scale diachronic corpus, which is of great significance for the in-depth understanding of the development of Macau's language life.",
        "GPT2_formal_text": "ed=1 token. This token can then be used to generate the query. Formal: Alright, let's break this down in a simpler way.\n\nFirst off, we take the query and run it through a 3-layer feedforward neural network. This network then grabs the hidden representation of the entity pair (e_i, e_j) and turns it into something called an embedding vector. In our case, this vector is a 1 in size. After that, we combine all the embeddings of the entity pairs, E_i, E_j, to get the query representation.\n\nIn the end, the query is made up of the embeddings of all the entity pairs. Formal: The query gets turned into an embedding vector using a feedforward neural network. The embedding vector for the entity pair is then used to generate the query. Formal: Finally, we combine all the embeddings of the entity pairs to get the query representation. Formal: We use a feedforward neural network to turn each word in the query into a vector. The embedding for the entity pair is then used to generate the query. Formal: The whole thing is encoded using a bidirectional GRU. Formal: In this setup, we're using a bidirectional GRU to encode the query, which helps improve the retrieval performance. Formal: The query vector is used to generate the query. Formal: Finally, we combine all the embeddings of the entity pairs to get the query representation. Formal: This whole process is repeated for each entity pair to create the query. Formal: The query vector is used to generate the query. Formal: Finally, we combine all the embeddings of the entity pairs to get the query representation. Formal: This whole process is repeated for each entity pair to create the query. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "ccl",
        "filename": "2021.ccl-1.33.json"
    },
    {
        "casual_text": "In Row 5, we focus only on subtask B, but we work with a set of ten related questions, using their connections to help us out. This gives us a small boost in all the measures we're tracking. Even more importantly, this approach is key to getting better results with the joint models.",
        "formal_text": "Row 5 is a special case where we only consider subtask B, but we do the learning and the inference over the set of ten related questions, exploiting their relations. This yields a slight increase in all measures; more importantly, it is crucial for obtaining better results with the joint models.",
        "GPT2_formal_text": "= [ [ arg1, arg2, ..., argn ], ..., [ argn ] ] ] Formal: Basically, this formula simplifies to F(•), but there are some issues with it. First off, we have to keep track of all the things we need to do during this process. For instance, if we have an arg_i and some conditions, like arg_i = arg1 + arg2, then this formula becomes: Formal: We can expand this idea by adding some extra conditions to it. For instance, if arg_i = arg1 + arg2, then we can write it like this: Formal: We also want to add a third condition for any arg_i, which we can write as a step-by-step process. Formal: So, the final formula looks like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1452.json"
    },
    {
        "casual_text": "Let's take a look at the noun phrases with conjunctions in the last three rows of Table 5. The way conjunctions work has been a big topic of debate in syntax for a long time (Johannessen, 1998, among others). Our model shows that when dealing with single nouns connected by conjunctions, it might pick either the first noun (8) or the last one (9). But when it comes to conjunctions involving multiple noun phrases (not just single words), the model always chooses the conjunction itself as the main element. These strategies have all been discussed separately in linguistics, and since our model uses all of them, it seems to share the same confusion that linguists have been dealing with.",
        "formal_text": "We analyze the case where the noun phrase contains a conjunction in the last three rows of Table 5. The syntax of conjunction is a long-standing source of controversy in syntactic analysis (Johannessen, 1998, inter alia). Our model suggests that several representational strategies are used, when coordinating single nouns, both the first noun (8) and the last noun (9) may be selected. However, in the case of conjunctions of multiple noun phrases (as opposed to multiple single-word nouns), the model consistently picks the conjunction as the head. All of these representational strategies have been argued for individually on linguistic grounds, and since we see all of them present, RNNGs face the same confusion that linguists do.",
        "GPT2_formal_text": "ed_partial, and the random walks are treated as an attention mechanism. Formal: In this setup, the model tries to predict the hidden state and the next word from the source text at the same time. Formal: The original ATN model, which uses attention mechanisms, doesn't perform well when it comes to generating text because it only uses one attention head, which doesn't have enough info to do its job. To fix this, we added an extra attention head that considers the surrounding words to handle the context, giving it more context to help it learn better. Formal: In this setup, the attention mechanism works by focusing on the words around the current token, like the one right after the one it was trained on. Formal: In this version, the model looks for the word with the highest probability and turns it into a vector using the context vector. Formal: We added a GRU to the input of the GAT model, which helps it understand the meaning of the words around the current token. Formal: The final output is a probability distribution based on the hidden states, and the weighted sum of those hidden states helps the model learn better. Formal: We also ran experiments with different setups, including the one we talked about earlier. Formal: We used a single-layer Transformer encoder as the decoder, which was trained with self-attention. Formal: We experimented with different settings for the first transformer layer and the GCN layer. Formal: The final output has three types of features: the attention, the sentence embedding, and the label embedding. Formal: The attention network calculates a weighted average of the attention weights for the source, token, and target tokens to figure out the hidden states, which helps the model learn better. Formal: The label embedding gives a representation of the label, which is what the target model predicts. Formal: To handle the fact that tokens with the same label might share some structure, we added a label embedding layer after the token embedding layer. Formal: In this case, the output is a vector where the label is the word with the highest probability, and the word with the highest probability is the one right after the token. Formal: The final output is a probability distribution based on the hidden states, and the weighted sum of those hidden states helps the model learn better. Formal: We also added a GRU to the input of the GAT model, which helps it understand the meaning",
        "directory": "E17",
        "filename": "E17-1117.json"
    },
    {
        "casual_text": "A lot of the current ways we represent word meanings rely on the distributional hypothesis. This idea, which has been around since the 1950s (thanks, Harris!), suggests that words that show up in similar contexts tend to have similar meanings. Researchers like Turney and Pantel, and more recently Clark, have built on this concept. These representations, often called embeddings, can capture how people naturally think about word similarities and connections. They’ve been super useful in all kinds of NLP tasks, like figuring out word meanings across languages (Mikolov et al., 2013b), analyzing emotions in text (Socher et al., 2013), and even identifying important names in text (Turian et al., 2010; Guo et al., 2014).",
        "formal_text": "Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al. , 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al. , 2013b), sentiment analysis (Socher et al. , 2013) and named entity recognition (Turian et al. , 2010; Guo et al. , 2014).",
        "GPT2_formal_text": ", class_ids, entity_ids, and event_ids, respectively. Formal: The input vector is made up of (u j, v j ) ∈ R D×D, where i and j represent the i-th and j-th elements from the input matrix, respectively. Formal: Similarly, the output vector is just a matrix with dimensions |D|×|D|. Formal: Using this approach, we can figure out the hierarchical structure of the entity relations, as shown in Fig. 2. Basically, for each entity_i, a few possible relations are possible. For each relation r_i from a set R, we grab the relevant entity_i, a_j, and a_j from all the possible relations and label them as r, r_i, r_i, r_i, r, r_i, r, r_i, r, r_i, r, r_i, r, r_i, r, r_i, r, r_i, r, r_i, r, r_i, r, r_i, and r_j. Formal: We use the RNNG approach to predict how an entity_i should be classified, like in Fig. 2. Formal: To keep things simple, we use a single GAN with a fixed size, which we call the GCN. Formal: To get the hierarchical structure for each entity_i, we first create a GCN and then update its state using the updated state vector for the entity_i. Formal: Finally, the final representation of the entity_i is a mix of the original representation and the updated representation. Formal: Formal: For the entity relation classification task, we first grab the relevant entity_i, a_j, and a_j from all the possible relation_r_i, r, and r_i, r, respectively. We then label the relation as r_i. Formal: Finally, we use the GAN to generate a prediction for each possible relation, which we call the relation prediction task. Formal: Formal: This gives us a final representation for the entity_i, a_j, and a_j. Formal: Formal: Formal: For instance, the prediction for r_1 is {r_1, r_2, ..., r_n}. Formal: Formal: Formal: Form",
        "directory": "D15",
        "filename": "D15-1242.json"
    },
    {
        "casual_text": "Social media data is usually pretty messy since it’s user-generated. Different researchers have used all kinds of methods to clean this data before feeding it into a machine learning algorithm. We found that the steps they choose can change the size of the data, making it harder to compare results fairly between studies. Table 2 shows some examples of papers using three popular hate speech datasets and the different preprocessing techniques they used, which messes up the ability to compare them properly.\n\nSome studies use different train-test splits, like 70:30 or 80:20, while others do a train-test-validation split, such as 70:15:15, 60:20:20, or 80:10:10. Some even go for 10-fold or 5-fold cross-validation. All these different setups make it impossible to compare studies fairly unless every researcher reruns all the studies they want to compare, which is not only impractical but also super expensive.\n\nSo, we’re pointing out what makes a dataset a good benchmark—something everyone can use to compare their results fairly.",
        "formal_text": "Train-Test Splits Social media data is often very noisy since it is a user-generated data. Different researchers have employed varying steps to clean the data in preparation for an ML algorithm. We show that these choice of steps can affect the data size, therefore obstructing an objective comparison between studies even more. Table 2 shows a few papers using three commonly used hate speech datasets and the preprocessing applied which leads to variations that negatively affect a fair comparison. Some of the existing studies select different train-test splits such as 70: 30 or 80: 20, some do a train-test-validation split of 70: 15: 15 or 60: 20: 20 or 80: 10: 10 while some do a 10-fold or 5-fold cross validation. This varying setting means that fair comparison amongst studies is not possible except if every researcher reruns all existing studies they wish to compare with. This is both impractical and costly. Here, we highlight factors that qualifies a dataset to be considered as benchmark.",
        "GPT2_formal_text": "ing_text_1 = 1; BERT_base = base; BERT_text = text; BERT_text_2 = text2; BERT_text_3 = text3; T = token; X = [x1, ..., xN], y = [y1, ..., yL], and T = [y1, ..., yD]. Formal: The RNNLM, which is basically a unidirectional neural network, has four basic parts. Formal: The three output layers in the RNNLM are set up like this: Formal: The word-level attention mechanism is what gives the lower layers of the RNNLM their job. It uses a two-level attention mechanism. The output layer for the word-level attention is also shaped like a 2x2 square and has two hidden layers. Formal: The LSTM layer is used to generate the next sequence of tokens. Formal: The RNNLM's output layer is the smallest unit that combines the input of the three lower layers. Formal: The final hidden layer of the RNNLM is shaped like a 3x3 square and has two hidden layers. Formal: We use a linear layer with a window size of 4 to figure out the hidden representation of the output word. Formal: We also throw in a dropout layer (Srivastava et al., 2014) to prevent overfitting. This layer is trained using cross-entropy loss to prevent the model from making the same mistakes over and over. Formal: Finally, we use a ReLU activation function to add some noise to the outputs. Formal: Formal: The model takes the word-level representations and turns them into a multi-dimensional vector by using an attention mechanism. Formal: Formal: The final output layer of the RNNLM is shaped like a 3x3 square and has two hidden layers. Formal: The entire model is trained using cross-entropy loss to prevent the model from making the same mistakes over and over. Formal: Formal: In Appendix A, we show how to build the model and how to evaluate it. Formal: The final hidden layer of the RNNLM is shaped like a 3x3 square and has two hidden layers. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "alw",
        "filename": "2020.alw-1.18.json"
    },
    {
        "casual_text": "For the Farsi-to-Arabic translation project, we started by testing the impact of adding reordered Farsi sentences along with their Arabic translations to the original training data, as explained in Section 3.6. Table 4 shows how this new system compares to the baseline, which was trained on the original, unreordered data. About 52% of the sentences were reordered using parse-based rules—we skipped POS-based rules for this. This change bumped up the training data size from 289K to 439K. For the translation part, we used a run-based penalty model for this experiment. The results were pretty good: we saw a solid improvement over the baseline, with a 1.2% boost in BLEU scores and a 0.6% improvement in WER. \n\nFunny enough, when we tried the same thing for Farsi-to-English translation, we didn’t notice much of an improvement. We think the main reason for the better results in the Farsi-to-Arabic case is because of better word alignment. In Farsi, the verb usually comes at the end of the sentence, but in Arabic, it’s often at the beginning. Due to how the alignment model works, this can cause issues. By seeing the same verb in both positions, the alignment algorithm might be able to figure it out better.",
        "formal_text": "For the Farsi-to-Arabic translation task, we first show the effect of adding reordered Farsi training sentences with their target language counterparts to the original bilingual training corpus as described in Section 3. 6. Table 4 compares the baseline system trained on the unreordered corpus with this system. About 52% of the sentences were actually reordered with parse-based rules; we did not use POSbased rules for reordering. Thus, the training corpus size increased from 289K to 439K. In translation, we used the run-based penalty model for this experiment. The improvement in MT error measures over the baseline is substantial: 1. 2% absolute in BLEU and 0. 6% absolute in WER. Interestingly, we did not observe any notable improvement when performing the same experiment for Farsi-to-English translation. We speculate that the main reason for the improvement here is better word alignment quality: because of its sentence-final position, the Farsi verb is often not aligned to its Arabic counterpart in the sentence-initial position due to alignment model restrictions 1. Having seen the same verb in two positions, the iterative alignment algorithm may be able to align it correctly.",
        "GPT2_formal_text": "\" : [], collection = [], date = NULL, ...], ..., ..., [], c = NULL, x = NULL, y = NULL, ..., [], m = NULL, r = NULL, ..., [], u = NULL, v = NULL, w = NULL, e = NULL, c = NULL, y = NULL, ..., [], c = NULL, x = NULL, y = NULL, ..., [], m = NULL, r = NULL, ..., [], c = NULL, x = NULL, y = NULL, ..., [], m = NULL, r = NULL, ..., [], c = NULL, x = NULL, y = NULL, ..., [], m = NULL, r = NULL, ..., [], ...\n\nIn the setup, the entity's representation is represented as a directed acyclic graph, and the related relationships between the entities are shown as directed directed edges. The entity mentions are represented by the word embedding vectors z, y, and z (or z and y), and their corresponding relation embeddings are also represented by the embeddings of the entities.\n\nSpecifically, the entities are represented as (x, y) matrices, and the relation embeddings are the (x, y) tensors. The entity mentions are represented as (x, y) tensors, and the relation embeddings are the (x, y) tensors. The entity mentions are vectors that represent the word embeddings and the relation embeddings, respectively.\n\nIn this paper, we use the RNN model as the backbone for the model. It has four main parts: the entity encoding, the relation encoder, the relation decoder, and the entity mention encoder. Formal: The entities are represented as (x, y) matrices, and the relation embeddings are the (x, y) tensors. The entity mentions are represented as (x, y) tensors, and the relation embeddings are the (x, y) tensors. Formal: The entities are represented as (x, y) matrices, and the relation embeddings are the (x, y) tensors. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "2010.amta-papers.29.json"
    },
    {
        "casual_text": "The other group has rules to help tell the difference between good and bad stuff. For example, if the answer has words like \"disappointed\" or \"terrible,\" we mark it as negative. E5 is a good example of this kind.",
        "formal_text": "The other group contains the guidelines which aim to distinguish the categories of positive and negative, i. e. , (e) If the answer text contains sentimental expressions to question like \"disappointed\", \"terrible\", and so on, we annotate it as negative. E5 is an example of this type.",
        "GPT2_formal_text": "ed | | | | --- | | C is the hidden layer. | | | | | A_r is a softmax layer that helps the model learn how to keep the attention for each of the R(0) tokens. Formal: So, for each token, the decoder decides if it should get attention from the sentence-level attention or the word-level attention. If it does the attention is given to the token's representation, and if not, it's given to the word's representation. Formal: Then, the decoder gets an input token x and a context vector v, which are both in R(0) by the previous step. Formal: But unlike regular attention mechanisms, this model skips the global context vector and just focuses on the attention mask for the token x. Formal: The decoder can also decide if it should use word-level attention or attention from the sentence-level attention. This is because word-level attention is way more effective than sentence-level attention. Formal: By looking at the representation v_i, the decoder can figure out the attention mask for the token x. Formal: Here's how it works: Formal: The input token x is built using the input vector v_i. Formal: Then, the decoder decides if it should use the word-level attention or the sentence-level attention. Formal: The output token is then calculated by combining the attention mask for the token x and the previous attention mask for the word. Formal: Finally, the decoder decides if it should use the sentence-level attention or the word-level attention. Formal: Formal: So, the whole process is a mix of these three steps: Formal: This setup can be represented using a simple attention system. Formal: The attention network A_r is a hidden layer with dimensions [h_i, h_j] that helps the model learn how to pay attention to the tokens. Formal: The network uses a cross-attention mechanism with weights π_r and a cross-modal mechanism Φ_r to figure out how to handle different types of relationships between tokens and words. Formal: The model's parameters are adjusted using a gradient descent method, with a learning rate of 0.001 and a mini-batch size of 256. Formal: To compare the model with other models that use attention or skip the global context vector, we ran some tests on the dev set using",
        "directory": "D18",
        "filename": "D18-1401.json"
    },
    {
        "casual_text": "Lately, a bunch of NLP studies have been diving into lie detection by gathering datasets and using computer models to spot lies (Hirschberg et al., 2005; Pérez-Rosas et al., 2014; Peskov et al., 2020). However, most of these studies don’t really consider the traditional methods and findings in lie detection, and there’s hardly any follow-up research. This makes it tricky to figure out which datasets are actually good for training models.\n\nTo bridge the gap between machine learning and lie detection research in psychology and linguistics, this study is all about analyzing verbal leakage cues. For the sake of simplicity, we’ll just call them leakage cues from now on. We’re looking at how these cues work in terms of how the data is collected and how well the models perform. We’ve got seven lie detection datasets to play with, and we’re analyzing them using word categories from LIWC2015 (Pennebaker et al., 2015).\n\nThrough this research, we’re trying to answer three main questions:\n1. How does the way we collect data affect strong leakage cues?\n2. What’s the deal with the number and type of strong leakage cues in lie detection tasks?\n3. Do strong leakage cues actually help make the models more reliable?\n\nWe’re hoping that by answering these questions, we can figure out how to build and pick the right datasets for lie detection tasks.",
        "formal_text": "Many NLP studies have recently collected lie detection datasets and detected lies using computational models (Hirschberg et al. , 2005; Pérez-Rosas et al. , 2014; Peskov et al. , 2020); most of these ignore traditional lie detection methods and findings, and have no follow-up studies, making it difficult to know which datasets are suitable for model training. To use machine learning approaches together with lie detection research in psychology and linguistics, and to seek a way to evaluate and select proper datasets, this study focuses on analyzing verbal leakage cues within; leakage cues hereafter indicate verbal leakage cues. We study leakage cues in terms of the data collection method and model performance. Seven lie detection datasets are adopted for experiments. We analyze these datasets using word categories defined in LIWC2015 (Pennebaker et al. , 2015). Through this study, we aim to answer three questions: (1) How do data collection methods affect strong leakage cues? (2) What is the role of the quantity and the category of strong leakage cues in lie detection task? (3) Do strong leakage cues contribute to model validity? We expect these answers to help in the construction and selection of appropriate datasets for lie detection tasks.",
        "GPT2_formal_text": ", which are all the sentences that mention the current query. We use the same attention layer that the original model uses to figure out the topic representations. Formal: There are two main ways to represent topic distributions using the distributional vector model: the first is called the topic-specific vector model, and the second is the topic-specific distributional vector model. Formal: We use the distributional vector model with k = 1, 2, and 3 dimensions to represent the k different topics, where each topic is made up of k different sentences. Formal: The idea behind the topic-specific vector model is to focus on the words that are directly linked to the topics in a sentence. Formal: We use a k-dimensional vector called V_K to represent the topics of a specific sentence. This vector is part of the negative log-likelihood, which helps us calculate the topic distribution. Formal: Here, k is the number of topics in the sentence, and d is the number of sentences in the article. Formal: The two topics we're looking at are \"dogs\" and \"cats.\" Formal: A vector called V_c can also be used to represent a specific topic, like \"cats\" in the example we just talked about. Formal: If we don't know the topic, we use a topic-specific vector called V_K to figure it out. Formal: Formal: To figure out the topic-specific vector, we first calculate the correlation between the topic and the sentence using Equation 2. Formal: Formal: For the k-dimensional vector V_K, we look at the topic-specific vectors V_k and V_k to find the topic-specific vector. Formal: Formal: For the k-dimensional vector V_k, we look at the topic-specific vectors V_k and V_k to find the k-dimensional vector. Formal: In the example we just saw, we start with the topic vectors V_k and V_k as the topic vectors. Formal: Formal: Formal: The final vector V_k is then calculated using Equation 3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.370.json"
    },
    {
        "casual_text": "In our approach, the POS predictor adds an extra linear layer after the decoder to create the target sentence, which you can see in Figure 3. Once we're done training, we only use that POS predicting linear layer for making predictions, which helps us get better results for POS sequence prediction.",
        "formal_text": "In our method, the POS predictor uses an extra linear layer after the decoder to generate the target sentence, as shown in Figure 3. After training, we only need the POS predicting linear layer for inference, thus enjoying the better performance for the POS sequence prediction.",
        "GPT2_formal_text": "_u and attention_mask_v. For instance, if we're dealing with a mask with u = 0, we add this sequence to the sequence we call {0, 1, ..., 0, 1} to create the token sequence: {0, 1, ..., 0, 1}. The same idea applies to the word embedding: the 0th token is shaped like (0, 0, ..., 0, 1). Formal: The CRF layer, as described by Chiu and Nichols in 2016, uses a linear layer with a batch size of 16, a max sentence length of 64, a window size of 16, and an inner product kernel with a width of 2. Formal: The encoder takes the sequence of word embeddings H_a and turns it into a hidden representation. Formal: To get a representation for a word, we use the input from the CRF layer. Formal: The CRF layer uses a linear layer with a batch size of 16, a max sentence length of 64, and an inner product kernel with a width of 2. Formal: The encoder takes the word embedding (H_a) and turns it into a hidden representation. Formal: The CRF layer uses a linear layer with a batch size of 16, a max sentence length of 64, and an inner product kernel with a width of 2. Formal: The decoder combines the input from the CRF layer and the output from the CRF layer using a window size of 16. Formal: Formal: The decoder combines the input from the CRF layer and the output from the CRF layer using a window size of 16. Formal: The decoder combines the input from the CRF layer and the output from the CRF layer using a window size of 16. Formal: The CRF layer uses a linear layer with a batch size of 16, a max sentence length of 64, and an inner product kernel with a width of 2. Formal: The CRF layer uses a linear layer with a batch size of 16, a max sentence length of 64, and an inner product kernel with a width of 2. Formal: The CRF layer uses a linear layer with a batch size of 16, a max sentence length of 64, and an inner product kernel with a width of 2. Formal: The CRF layer uses a linear layer with a batch size of 16, a max sentence length of 64, and an inner product kernel",
        "directory": "acl",
        "filename": "2021.acl-long.467.json"
    },
    {
        "casual_text": "Sequence labeling models have been a big deal in basic NLP tasks like POS tagging, chunking, and named entity recognition (NER). Back in the day, people used statistical methods like Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with manually designed features and task-specific resources. But with the rise of deep learning, neural models have been crushing it on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are now encoded using distributed representations (Mikolov et al., 2013), and sentence-level features are learned automatically during end-to-end training.\n\nA lot of the current top-performing neural sequence labeling models use word-level Long Short-Term Memory (LSTM) structures to handle global sequence information and a CRF layer to deal with dependencies between labels (Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). On the other hand, Convolutional Neural Networks (CNN) (LeCun et al., 1989) have also been used because they can do parallel computing, making training and decoding faster and more efficient.",
        "formal_text": "Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al. , 2014; Luo et al. , 2015 ) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al. , 2015; Lample et al. , 2016; Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al. , 2013 ) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels Lample et al. , 2016; Ma and Hovy, 2016; Peters et al. , 2017). As an alternative, Convolution Neural Network (CNN) (LeCun et al. , 1989) has also been used for its ability of parallel computing, leading to an efficient training and decoding process.",
        "GPT2_formal_text": "_t = arg max idx, i, j) = (C(i, j) - C(i, j)) for (i, j) in seq(1:N) Pairs (C(i, j)) where C(i, j) is part of the set C(i, j). Formal: Basically, for a pair (s1, s2), the correct label is the one that comes first, and the wrong one is the one that comes last. Formal: We need a way to measure how much the output changes depending on which sentence or phrase we're looking at, and that's what λ is for. Formal: λ is the ratio of how well the model predicts the correct label to how well it predicts the incorrect one. We use an approach that mixes the usual methods for classifying labels in text, like the sigmoid function, with some tweaks. Formal: λ tells us the probability of the correct label, and α and β are the weights that go with it. We figure out how to represent the model's predictions by using a mapping called λ_t to a vector called ŷ_t. Formal: We also need a way to estimate how likely each result is based on the input sentence or phrase. Formal: In our experiments, we try out different ways to estimate these probabilities using Bayesian models. Formal: We calculate these probabilities using three different Bayesian models: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C18",
        "filename": "C18-1327.json"
    },
    {
        "casual_text": "CDSP methods use different types of encoders: one for handling utterances and another for managing MRs (meaning representations). Utterance encoders create neural representations for both the current and past utterances, whereas MR encoders focus on building neural representations using historical MRs.",
        "formal_text": "Context-aware Encoders Encoders of CDSP methods differentiate between utterance encoders and MR encoders. Utterance encoders construct neural representations for both current and historical utterances, while MR encoders build neural representations based on on historical MRs.",
        "GPT2_formal_text": "_dmg = dmg_mask_dmg_dec = dmg_mask_dec_dec. # The final output is the predicted distribution q, which has a probability p(z|d) = 1/k. Formal: The similarity between the original question and the question generated using the question generator q g is calculated using this equation: Formal: The main goal is to get a high similarity score using the Q-sentence generator, but we also want to make sure the generated question is similar to the original query q. Formal: To get a high similarity score, we use different methods to measure similarity between query and question. Formal: Basically, we look at the cosine similarity between the query and question, and then adjust the similarity score by dividing it by the cosine similarity between the question and the query. Formal: We also include some unnormalized similarity scores using the cosine similarity between the query and query generator q, which we calculate using the cosine similarity between the query and query generator q. Formal: The score is just the sum of these unnormalized scores. Formal: We use a logistic regression model to calculate the similarity between the query and query generator q. Formal: We set the number of epochs to 5 to prevent overfitting, which makes the model not perform as well. Formal: For comparison, we also calculate the sentence similarity by combining the cosine similarity between the query and query generator q. Formal: We do this using the cosine similarity between the query and query generator q, which we calculate using the cosine similarity between the query and query generator q. Formal: The similarity score is the sum of all these scores. Formal: We use the cosine similarity between the query and query generator q to compute the similarity score, which we calculate using the cosine similarity between the query and query generator q. Formal: We also consider the hyper-parameter λ_K, which is a set number between 0 and 1, to control how much we pay attention to hyper-parameters. Formal: We use the cosine similarity between the query and query generator q to compute the similarity score, which we calculate using the cosine similarity between the query and query generator q. Formal: We also consider the hyper-parameter λ_K, a set number between 0 and 1, to control how much we pay attention to hyper-parameters. Formal: We use the cos",
        "directory": "coling",
        "filename": "2020.coling-main.226.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. The findings are all laid out in Table 2. Applying INLP has a mixed effect on how well the main task performs: it boosts BOW by 1.9%, but brings BWV down by 5.1% and BERT by 5.51%. \n\nNow, GAP TPR, RMSg, which measures how close the true positive rates are for male and female classifiers, takes a noticeable hit. It shows that the rates are getting closer on average: \n- In BOW, it drops from 0.203 to 0.124, a 38.91% decrease.\n- In BWV, it goes from 0.184 to 0.089, a 51.6% drop.\n- In BERT, it falls from 0.184 to 0.095, a 48.36% decrease.\n\nWe also looked at the correlation between GAP TPR for each profession and the percentage of women in those professions. Here's what we found:\n- In BOW, the correlation went from 0.894 before INLP to 0.670 after, a 33.4% decrease.\n- In BWV, it dropped from 0.896 to 0.425, a 52.5% decrease.\n- In BERT, it fell from 0.883 to 0.470, a 46.7% decrease (check Figure 4b for more details).\n\nInterestingly, De-Arteaga et al. (2019) found a correlation of 0.71 for BWV when they used a \"scrubbed\" version of the biographies, meaning they removed all pronouns and names. INLP does way better than that baseline, even while keeping all the gender markers intact. \n\nSo, the big question is: How does trying to make things fair affect how much importance the classifier gives to different words in the biographies? That's what we're analyzing here.",
        "formal_text": "The results are summarized in Table 2. INLP moderately changes main-task accuracy, with a 1. 9% increase in BOW, a 5. 1% decrease in performance in BWV and a 5. 51% decrease in BERT. GAP T P R, RM S g is significantly decreased, indicating that on average, the true positive rate of the classifiers for male and female become closer: in BOW representation, from 0. 203 to 0. 124 (a 38. 91% decrease); in BWV, from 0. 184 to 0. 089 (a 51. 6% decrease); and in BERT, from 0. 184 to 0. 095 (a 48. 36% decrease). We measure the correlation between GAP T P R y, f emale for each profession y, and the percentage of biographies of women in that profession. In BOW representation, the correlation decreases from 0. 894 prior to INLP to 0. 670 after it (a 33. 4% decrease). In BWV representation, the correlation decreases from 0. 896 prior to INLP to 0. 425 after it (a 52. 5% decrease). In BERT representation, the correlation decreases from 0. 883 prior to INLP to 0. 470 following it (a 46. 7% decreases; Figure 4b ). De-Arteaga et al. (2019) report a correlation of 0. 71 for BWV representations when using a \"scrubbed\" version of the biographies, with all pronouns and names removed. INLP significantly outperforms this baseline, while maintaining all explicit gender markers in the input. Analysis. How does imposing fairness influence the importance the logistic classifier attribute to different words in the biography?",
        "GPT2_formal_text": "= (s, t) for i from 1 to T. for i from 1 to T: n_t = {s, t}, where the only thing that matters is the type of entity. Formal: The F1 score here shows how many of the correct matches are correct. Formal: We look at the top F1 score (F1) for a whole sentence, ignoring any repeats. We also give the average of the F1 scores for the top 5 sentences. Table 4 shows the results for each of these steps for the same entity pair. Formal: To see how well we're doing with our multi-label methods, we check the F1 score for the top 1, 2, and 3 F1 scores for each entity pair. Formal: The F1 score here just tells us how many of the correct matches are correct. Formal: We also check how well we're doing with the average F1 score. Formal: We also check how well our multi-label methods are doing with the average F1 score. Formal: The results from our experiments are in Table 4. It shows the F1 scores for different entity pairs and how well the multi-label methods match up with the human judgments. Formal: Lastly, the results from our experiments are in Table 5. Formal: Formal: The results from our experiments are in Table 4. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.647.json"
    },
    {
        "casual_text": "Alright, so here's the deal: If both C1 and C2 can be learned in a structured way (PAC-learnable), and h01 and h02 are kinda helpful in predicting f1 and f2 (weakly useful predictors), plus χ is on point with D, f1, f2, h01, and h02, and has a discrimination score that’s high enough—like at least 4(|Y| − 1)² for 0/1 loss or ≥ 4|V|²(|Y| − 1)² for Hamming loss—and h01 and h02 aren’t related to each other (uncorrelated), then C1 and C2 can also be learned with two-sided hints.",
        "formal_text": "Corollary 1. Suppose C 1 and C 2 are both PAClearnable in the structured setting, h 0 1 and h 0 2 are weakly useful predictors of f 1 and f 2, and χ is correct with respect to D, f 1, f 2, h 0 1 and h 0 2, and has discrimination ≥ 4(|Y| − 1) 2 (for 0/1 loss) or ≥ 4 |V | 2 (|Y | − 1) 2 (for Hamming loss), and that h 0 1 and h 0 2 are uncorrelated. Then C 1 and C 2 are also PAC-learnable with two-sided hints.",
        "GPT2_formal_text": ". The word embeddings are then fed into a fully connected layer to get the final representation. The results show that our model gives a similar representation as the previous best model, but with a smaller BERT embedding. \n\nTo dig deeper, we ran a few extra experiments. For each token, we calculated the cross-entropy loss. This gives us the average cross-entropy performance for each token. We also looked at the cosine similarity between the embeddings of the token and the whole sentence. This shows how important the token is. For example, if a sentence has a total of N tokens, the embedding for the token is either: Formal: If the token is not \"apple,\" it's 0.0, otherwise, it's 1. Formal: If the token is \"apple,\" it's 0.0, otherwise, it's 1. Formal: If the token is \"gold,\" it's 0.0, otherwise, it's 1. Formal: If the token is \"apple,\" it's 0.0, otherwise, it's 1. Formal: If the token is \"gold,\" it's 0.0, otherwise, it's 1. Formal: If the token is \"apple,\" it's 0.0, otherwise, it's 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "Limitations and Future Plans. Our paper mainly focuses on unsupervised summarization because it's super important for situations where we don't have a lot of data. One thing we haven't done yet is get solid results for supervised summarization, even though our model might work there too. The reason for this is that past studies on supervised summarization don't clearly categorize summaries by length (Yang et al., 2020), which makes comparing results tricky and not really fair (Schumann et al., 2020). This issue is also pointed out by Su et al. (2021), who noticed that the same model can score a few ROUGE points differently depending on the length of the summary it generates. Despite this, we did compare our work with Su et al. (2021) under our conditions and showed that our NAUS model comes out on top when the comparison is fair. For future work, we plan to dive into supervised summarization once we set up a proper and rigorous testing environment, but that's something we'll tackle in another paper.",
        "formal_text": "Limitation and Future Work. Our paper focuses on unsupervised summarization due to the importance of low-data applications. One limitation is that we have not obtained rigorous empirical results for supervised summarization, where the developed model may also work. This is because previous supervised summarization studies lack explicit categorization of summary lengths (Yang et al. , 2020; , making comparisons unfair and problematic (Schumann et al. , 2020). Such an observation is also evidenced by Su et al. (2021), where the same model may differ by a few ROUGE points when generating summaries of different lengths. Nevertheless, we have compared with Su et al. (2021) in our setting and show the superiority of the NAUS under fair comparison. We plan to explore supervised summarization in future work after we establish a rigorous experimental setup, which is beyond the scope of this paper.",
        "GPT2_formal_text": "= {}, class_ids = {}, and references = {}. Formal: To get the final representation, we use the softmax function to make sure the representations are close to each other. Formal: Here, we'll explain how we combine the context vectors and the word representations to get the final representation for the question. Formal: The final representation for the question q_t, which is in R^m, is made up of two parts: the query vector for the question (q_t) and the embedding for the evidence span (e_t). Formal: A contextual embedding is basically a way of representing the whole document D. Formal: For each word w_i in the question text, we create a contextual embedding by using its embedding, d_w_i, as the context vector. Formal: We want to give a little boost to the model's performance by using information from the entity mention at the start. Formal: This helps the model use the context from the evidence span. Formal: We also create an attention layer to focus on the words that are most relevant to the evidence span, and we use an input of e_t × d_w_i to get the final representation for the query. Formal: The entity mention at the start of the document gets turned into a contextual embedding by combining its embedding, d_w_i, with the context vector of the evidence span. Formal: In our approach, we treat the query and evidence span as a single vector and calculate the embedding for the query using the context from the evidence span. Formal: Our approach is a bit different from other methods like a Laplace distribution, which uses a distortion model to make sure the vectors are close to each other. Formal: We don't need to update the model's parameters after every step because we don't need to apply any specific loss function to it. Formal: To make sure the training distribution is accurate, we set the discount factor λ_d to 0.01 and adjust the weighting function σ to ensure the weights are independent. Formal: We use a dropout rate of 0.5 with a probability of 0.3 to prevent overfitting. Formal: Lastly, we set a hyperparameter α to 1.0 to prevent overfitting. Formal: Figure 1 shows an example of how our model (with an attention mechanism) generates the final question text. Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.545.json"
    },
    {
        "casual_text": "In this paper, we’ve added a straightforward alignment algorithm to Anymalign to help us figure out where it’s currently falling short. These new alignments make Anymalign’s phrase tables much better, enough to get top-notch results. Along the way, we also came up with a quick and easy method to calculate ITG alignments using any word-level association scores.",
        "formal_text": "In this paper, we complement Anymalign with a simple alignment algorithm, so as to better understand its current limitations. The resulting alignments improve Anymalign's phrase tables to a point where they can be used to obtain state-of-the art results. In passing, we also propose a computationally cheap way to compute ITG alignments based on arbitrary word level association scores.",
        "GPT2_formal_text": "ed_text_cnt = 1 / (sizeof(D_CNTL) - 1)/2; // And for L_CNTL, we add this extra part: \n\nAlso, in the case of word2vec models, we figure out the cross-entropy loss for the whole model by following this formula: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2012.eamt-1.62.json"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way:\n\nSo, we're dealing with some kind of system or process here. There are different parts labeled with codes like *c12, *c13, and so on. These parts seem to be interacting with each other in a specific order.\n\nFor example, there's something called OBJ2 that's connected to *c12 and *c13. Then, there's another part called FAf~ that's linked to *el3 and *el40BL. It looks like FAf~ is doing something with these parts, maybe controlling or processing them.\n\nNext, there's FA¢~ which is working with *c14, *ell, and *oont. It seems like FA¢~ is handling these parts and might be outputting something (maybe \"nil\" is the result here).\n\nThen, there's FAOJP' which is connected to *clO, *ell, and (*gf *cont). This part seems to be doing something with these connections and then passing it on to *gf.\n\nFinally, there's a part labeled *i0 and *10 that's linked to ~-VP\". This part is working with *cI0, *ell, *cont, and *outpxcomp. It seems like it's processing these parts and then passing them to NP (*el0 *ell *ontpnp).\n\nAt the end, there's a <- lET that's connected to *el0, *cii, and *ontpdet. This part is working with N (*outpdet *outpnp), which seems to be the final output or result of the whole process.\n\nIn short, this is a bunch of interconnected parts doing different tasks, and they all seem to be working together to produce some kind of output.",
        "formal_text": "Ifun£tional FA(~ (*¢12 *c13 OBJ2 ~ *~) controll FAf~=P (*el3 *el40BL ~ *~) FA¢~\" (*¢14 *ell *oont xcem ~ nil) l i~iAst~ FAOJP' (*clO *ell (*gf *cont) *gf ). *i0) *10) ~-VP\" (*¢I0 *ell *cont *outpxcomp) NP (*el0 *ell *ontpnp) <- lET (*el0 *¢ii *ontpdet) N (*outpdet *outpnp)",
        "GPT2_formal_text": "= {'L' : {'e1' : [], 'e2' : [], 'e3' : [], 'e4' : [], 'e5' : [], 'e6' : [], 'e7' : [], 'e8' : [], 'e9' : [], 'e10' : [], 'e11' : [], 'e12' : [], 'e13' : [], 'e14' : [], 'e15' : [], 'e16' : [], 'e17' : [], 'e18' : [], 'e19' : [], 'e20' : [], 'e21' : [], 'e22' : [], 'e23' : [], 'e24' : [], 'e25' : [], 'e26' : [], 'e27' : [], 'e28' : [], 'e29' : [], 'e30' : [], 'e31' : [], 'e32' : [], 'e33' : [], 'e34' : [], 'e35' : [], 'e36' : [], 'e37' : [], 'e38' : [], 'e39' : [], 'e40' : [], 'e41' : [], 'e42' : [], 'e43' : [], 'e44' : [], 'e45' : [], 'e46' : [], 'e47' : [], 'e48' : [], 'e49' : [], 'e50' : [], 'e51' : [], 'e52' : [], 'e53' : [], 'e54' : [], 'e55' : [], 'e56' : [], 'e57' : [], 'e58' : [], 'e59' : [], 'e60' : [], 'e61' : [], 'e62' : [], 'e63' : [], 'e64' : [], 'e65' : [], 'e66' : [], 'e67' : [], 'e68' : [], 'e69' : [], 'e70' : [], 'e71' : [], 'e72' : [], '",
        "directory": "E83",
        "filename": "E83-1009.json"
    },
    {
        "casual_text": "To optimize this objective function while considering the constraints, we need to pick vectors a and b in a way that makes a_i * b_j high when C_xy_ij is also high. Basically, each non-zero entry in the crosscovariance matrix limits the options for the projection directions we can choose. This usually isn't a big deal if the training data is clean, but that's pretty rare, especially with high-dimensional data like text documents. Plus, natural languages are inherently ambiguous, so there's a good chance any document will have some noisy words. Every time a noisy word pops up, it adds to the covariance matrix, making it denser and harder to find the right projection directions.",
        "formal_text": "Maximizing this objective function, under the constraints, involves a careful selection of the vectors a and b such that a i b j is high whenever C xy ij is high. So, every non-zero entry of the crosscovariance matrix restricts the choice of the projection directions. While this may not be a severe problem when the training data is clean, but this is very uncommon especially in the case of high dimensional data like text documents. Moreover, the inherent ambiguity of natural languages increases the chances of seeing a noisy word in any document. Every occurrence of a noisy word will have a non-zero contribution towards the covariance matrix making it dense, which in turn prevents the selection of appropriate projection directions.",
        "GPT2_formal_text": "ed_softmax, which is basically the softmax function applied to the word embeddings.\n\nTo get the embeddings for a word w, we take the word embeddings of its closest neighbors w_c and w_d. Formal: To get the embeddings for a word w_i, we use a model called M_i where i is from 1 to d_d. We apply the softmax function to these embeddings. Formal: Let’s call the word embedding of a specific mention m_i from i=1 to d_d. Formal: If a mention m_i is in the lexicon, we can calculate the embedding of m_i using the context vectors from the last layer of a neural network called M_i, which we’ll call c_m_i. We then combine the embeddings of m_i with their context vectors c_m_i. Formal: Lastly, to predict the attention value for a mention m_i, we use the same approach as before. Formal: We start by pulling out the context vectors from the last layer of M_i, and then we combine those with the embeddings of m_i to get the final attention representation for m_i. Formal: The attention vector for a mention m_i is calculated by adding up the attention vectors for all its neighbors. Formal: We feed these attention vectors into a neural network, which gives us the attention representation for m_i. Formal: Lastly, we calculate the attention value for m_i by summing up the attention vectors for all its neighbors. Formal: Formal: Formal: This process helps us figure out the attention weights for m_i. Formal: Figure 3 shows a diagram of how we calculate the attention weights for a mention m_i using the graph convolutional neural network (GCN). Formal: To get the attention weights for a mention m_i, we use the GCN’s GCN convolutional network. Formal: The final attention weight vector for m_i is calculated by combining the attention weights for all its neighbors. Formal: Figure 4 shows a diagram of the graph convolutional neural network (GCN’s GCN convolutional network). Formal: We start by getting the context vectors from the last layer of M_i, and then we combine them with the embed",
        "directory": "D11",
        "filename": "D11-1086.json"
    },
    {
        "casual_text": "We add e_t in Eq. 8 to handle cases where N_g(t) is empty, making it possible to compute the context representation. Think of it as a way to smooth things out when calculating context. After that, we figure out the distance between the head-relation context of t and the orthogonal transform representation of a triple (h, r, t), like this.",
        "formal_text": "We use e t in Eq. 8 to make the computation of context representation possible when N g(t) is empty. This can be viewed as a kind of additive smoothing for context representation computation. Then, we compute the distance of the headrelation context of t and the corresponding orthogonal transform based representation of a triple (h, r, t) as follow.",
        "GPT2_formal_text": "is a list of id numbers, like {1, 2, 3, …, m}. The order of these lists is strictly limited, and the order of the nodes in the dependency parse tree, like D(s, t), can only be determined by considering the order of the input sentence, which is the union of all the possible sequences s_i from i=1 to m. Formal: In a strict grammar, it's not possible to have a strict left-to-right order. To deal with this, we use a left-to-right parser, like LTL, which is built right into the original LFG parser. Formal: This parser has a left-to-right, left-recursive algorithm that uses a left context pointer, c_l, along with a right context pointer, c_r. The input for the parser looks like this: Formal: For each sentence s, the left context c_l is a pair of input strings, c_l1 and c_l2. The right context c_r is also a pair, c_r1 and c_r2. The left and right context pointers in this parser are passed on as first-order functions through LTL's recursive left and right passives, respectively. Formal: The left context pointer c_l1 is passed to the left-recursive algorithm, which then uses that context pointer to create the left-to-right, left-recursive parse tree for s. Formal: The right context pointer c_r1 is passed to the right-recursive algorithm, which then uses that context pointer to create the right-to-left, right-recursive parse tree for s. Formal: Finally, the output from the left-to-right algorithm, c_l, is a left-to-right, left-recursive parse tree for s. Formal: Here, c_l_i is the jth word in the sentence s, and c_r_i is the jth word in the sentence s. Formal: Formal: Formal: We tested this system using the German-English translation system from the CoNLL-2009 shared task, which is available online. Formal: Formal: We didn't include the error type code in the results because it's not relevant to our approach. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.241.json"
    },
    {
        "casual_text": "Our approach falls into the third category of studies, but it’s a bit different because we combine two types of features in a unique way. In our method, internal component features are more important. We’ll show that a model using just these features can work really well on its own. The other type of features is like a helpful extra that can slightly improve the results for certain words. \n\nWe connect the two models by giving each part-of-speech (POS) guess from the first model a credibility score. If a result has a lower credibility score, we flag it and run it through a second step that uses global context to reconsider it.",
        "formal_text": "Our method is among the third type of studies, but is different from the rest in the scheme of combining the two types of features together. In our method, internal component features play a more important role. We will prove that a model based on this type of features alone can perform very well. The other type of features acts as a useful supplement and can improve the results of some words in a certain degree. The two models are linked together by assigning a credibility score for each POS guessing result generated by the initial model. The results with a relatively lower credibility score are identified and put through reconsideration by a method based on global context information.",
        "GPT2_formal_text": ", self, g, h, t. The c is just the attention mechanism, and x is a matrix with dimensions n × d. Formal: To figure out the attention weights, we use the softmax function. We assume that the attention weights are independent and can be calculated from word embeddings. Formal: In Section 5.1, we show that the attention weights are super sensitive to how important the words are. Formal: But in real life, the attention weights can be learned pretty easily, and the attention can act like a regularizer. So, we can make sure the attention weights don't get too soft. To make this happen, we use a distortion model, which we'll explain next. Formal: We use some fancy math to make sure the attention weights don't get too far off. Formal: To make sure the attention weights don't get too soft, we make the distortion model stronger by adding a feature called the distortion type. Formal: We use a threshold ρ to make sure the attention weights aren't too soft, but we don't bother with the distortion type. Formal: If ρ is 0.0, the attention weights are soft, and if ρ is 1.0, they're hard. Formal: After that, we take the output from the distortion model, which we call the input, and use it to update the attention weights. Formal: Finally, we add the output of the attention mechanism, which we call the output t. Formal: The final output is the output t, which is 1 if the model is doing well, and 0 if it's not doing well. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C08",
        "filename": "C08-1089.json"
    },
    {
        "casual_text": "Chinese abbreviations are made by picking out key characters from the full name. Like with \"北京大学\" (Peking University), the abbreviation is \"北大,\" which comes from the first and third characters. You can think of it as a sequence labeling problem. Check out Table 1 for an example: the abbreviation \"北大\" from the full form \"北京大学\" (Peking University).",
        "formal_text": "Chinese abbreviations are generated by selecting representative characters from the full forms. For example, the abbreviation of \"北京大学\" (Peking University) is \"北大\" which is generated by selecting the first and third characters, see TABLE 1. This can be tackled from the sequence labeling point of view. Table 1: The abbreviation \"北大\" of the full form \"北京大学\" (Peking University)",
        "GPT2_formal_text": ") for each entity. The retrieval module takes all the data it finds and turns it into a bunch of event mentions, which it sends to the next parts of the system. Formal: Let’s use the same entity mention j u as in Figure 1 to represent the event e i. The entity mention e i is linked to an event e j, and the event e j is linked to an entity mention e j. For instance, the entity mention \"Serbia\" in Figure 1 is linked to the event \"Serbia-Croatian War,\" which is linked to the entity mention \"Serbia.\" Similarly, \"Serbia-Croatian War\" is linked to the event \"Serbia-War.\" This setup means that all these entity mentions share a common event coreference link. Formal: So, for an entity mention e i and its event e j, the connection between them is like this: Formal: For the entity mention e i, the connection is like this: Formal: For the event e j, the connection is like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1147.json"
    },
    {
        "casual_text": "For instance, in the top graph in Figure 5, there's an edge connecting a vowel to a Hebrew character with a really high weight. But when you look at the bottom graph, which shows the results after discriminative training, that same edge now has a weight of zero.",
        "formal_text": "We can see for example, in the top graph presented in Figure 5 an edge matching a vowel to a Hebrew character with a high weight, the bottom graph showing the results of the discriminative training process show that this edge is associated with a zero weight score.",
        "GPT2_formal_text": "ed. \n\nTable 5: Our updated baseline system, which uses a masked language model that uses the attention mask mechanism, along with our proposed model. Formal: This setup is based on the input sequence representation (x_i) with some changed tokens. In this setup, the first word of each sentence gets turned into a hidden vector (h_i). Formal: The model we're proposing takes the input sentence x_i and mixes it with the previous hidden state, h_1, h_2, and so on, up to h_T. Then, it combines h_T with the attention mask H and the output vector from the linear layer. This combined vector is then used to predict the next word in the sequence, x_i_n. Formal: The Bi-LSTM (from Liu et al., 2015) and LSTM (from Hochreiter and Schmidhuber, 1997) are pretty popular in NLP. But the attention mechanism, which was introduced by Bahdanau et al. (2014), gives us a cool new way to handle sequence modeling. Formal: Unlike the traditional sequence modeling methods, this new approach works well for real-time chat systems that handle long-distance exchanges. Formal: We’re using the model we’ve developed to predict the next word in a conversational text. Formal: This model is pretty straightforward and can be easily customized for specific chats. Formal: It’s not super accurate, but it’s faster than the traditional model and can handle a lot more data. Formal: We also use this model to analyze and analyze the dialogue context, like the domain-specific ones we talked about earlier. Formal: Another example is the Bi-LSTM-based model (from Liu et al., 2015), which was adapted for the NER task. Formal: The Bi-LSTM model is pretty popular in NLP, but it’s not as good at generating natural-sounding responses. Formal: So, we’re using the same model to generate the responses to get a better understanding of the conversation context. Formal: Using this model for generating responses helps us understand the context better. Formal: The response to the current question can either be the whole question or just part of the current conversation. Formal: Formal: We’re hoping our work will encourage more research in improving this model. Formal:",
        "directory": "D08",
        "filename": "D08-1037.json"
    },
    {
        "casual_text": "Also, we check out how our method stacks up against CTC beam search (Graves et al., 2006). Usually, a non-autoregressive model trained with CTC can be decoded in two ways: either greedily or using beam search. Greedy decoding just picks the most likely token at each step, like w*i = argmax wi P(wi|x), and then squeezes those tokens into a sentence using Γ.",
        "formal_text": "In addition, we compare our approach with CTC beam search (Graves et al. , 2006). 6 Typically, a CTC-trained non-autoregressive model can be decoded either greedily or by beam search. The greedy decoding finds the most probable token at each step, i. e. , w * i = argmax w i P (w i |x), and reduces the tokens to a sentence by Γ",
        "GPT2_formal_text": ", you can figure out the top k results for each query by running these searches. If you want to learn more about how to do this, check out (Zhang et al., 2019). Formal: The goal of this task is to find the answer to a question q based on a specific query q. The main issue here is that there are way too many possible options (up to the size of M^d) to handle. Formal: A lot of earlier work focused on doing the classification part manually (like Barzilay and Lapata, 2008; Anderson et al., 2015; Madnani et al., 2017; Liu et al., 2018b). But since QA datasets are way bigger than QA datasets for data augmentation (which is like a smaller test set), we thought it would be better to use the model we got from the task to help with the classification part. This way, the model can learn from the whole dataset and use that knowledge to make the final call. Formal: We’ve already talked about how the latent variable y t can help out with the classification part, but we’re now going to focus on how to use the latent variable y t to help with the question-answering part. Formal: In this part, we’ll explain how we create a latent variable y t that can help us learn the answer to a question q. Formal: In this part, we’ll lay out the theoretical stuff we’re using and how we set up the task. Formal: We’re using the multi-layer perceptron (MLP) model (Mikolov et al., 2013) for the training process. The L2-norm kernel is a standard choice for learning the parameters. For prediction, we’re using an autoencoder (from Sutskever et al., 2014) with a hidden state size of 512, with a softmax activation. Formal: Here’s how the equation is set up: Formal: The parameters we’re dealing with are {1, 2, ..., m}, and they’re all learned as part of the training process. Formal: We’re using linear programming (LP) to handle the classification part. Formal: The input features for this classification part are {q t } (which are the labels for the question). Formal: The loss function L is calculated using the softmax",
        "directory": "acl",
        "filename": "2022.acl-long.545.json"
    },
    {
        "casual_text": "In Table 3, we’ve got the results from our pairwise alignment algorithm, comparing it to some baseline methods using 200 text-text recipe pairs from Common Crawl that were aligned by humans. Unlike the text-video alignments, we noticed that the uniform alignment baseline didn’t do better than the textual similarity baselines. This seems to be because the different ways text-text recipe pairs are reordered make alignment a bit trickier. \n\nWhen it comes to the textual similarity baselines, RoBERTa came out on top, just like in the text-video alignment. We think this is because text recipes often use similar vocabulary, which makes it easier to spot matching words between two sets of instructions. On the other hand, video narrators usually use more casual language than the folks writing text recipes, so it’s harder to align things based on word similarities.\n\nSomething cool we found is that both BM25 and RoBERTa have higher recall than our best HMM+IBM1 model, but they fall short in precision. This means retrieval models are great at finding more alignments, even if they’re not always super precise. Our unsupervised HMM+IBM1 model, though, still crushed all the baselines when it came to the F1 score (p < 0.001). \n\nWhen we broke down the HMM+IBM1 model, we saw that using all the words to learn alignments gave us the best results.",
        "formal_text": "On text-text alignments Table 3 shows results of our pairwise alignment algorithm compared with baselines on 200 human-aligned text-text recipe pairs from Common Crawl. Unlike text-video alignments, we find that the uniform alignment baseline does not outperform textual similarity baselines, suggesting that the different re-orderings between text-text recipe pairs makes alignment more challenging. Within textual similarity baselines, similar to text-video alignment, RoBERTa outperforms all others. We believe this is because text recipes tend to share similar vocabulary, making it easier to find similar words between two textual instructions. Video narrators tend to use more colloquial language than the authors of text recipes, making it more difficult to learn alignments using word similarities. Interestingly, both BM25 and RoBERTa get higher recall than our best HMM+IBM1 model but they lose out on precision. This suggests that retrieval models are good for identifying more alignments, albeit with lower precision. Our unsupervised HMM+IBM1 model again significantly outperforms (p < 0. 001) all baselines on F1 score. Under ablations of the HMM+IBM1 model, we again find that using all words to learn alignments performs best.",
        "GPT2_formal_text": ". This approach helps the model pick the right response. The first row in Figure 1 shows the results of the encoder block for the example shown in Figure 1. The c-test results for this example are 0.75, 0.58, 0.51, and 0.55. Formal: To get a clearer picture, we use a stratified random sampling method to see how different response categories stack up. For each response category, we randomly select 5 response candidates from the list of options available and toss out the ones that don’t meet our quality requirement (Q). Formal: In this paper, we mainly look at how well the model can generate responses with a specific query in a specific area. To make things easier, we use a simple online method that makes the model generate the same response in multiple areas. Formal: The results from the online method, which we call the query-specific response generation method, are listed in Table 1. Formal: We calculate the query-specific answer importance for each query by averaging the response candidates generated by the online method. The average results for this method are shown in Figure 1. The query-specific answer importance is calculated by averaging the results for each query. Formal: Lastly, to compare the average results with the best performance, we average the answer importance scores from the last 20 queries. Formal: This method is pretty similar to the online method but also uses some extra factors, like the query, the words in the query, the training data, and the answer relevance score from the last 20 queries. Formal: Lastly, we get the query-specific answer importance scores for each query by averaging the results from the last 20 queries. Formal: We also calculate the query-specific answer importance scores for each query by averaging the results from the last 20 queries. Formal: The results from this method are shown in Table 1. Formal: The answer relevance score from the last 20 queries is also calculated by averaging the results for each query. Formal: Finally, we get the answer relevance score for each query by averaging the results from the last 20 queries. Formal: Finally, we get the answer relevance score for each query by averaging the results from the last 20 queries. Formal: The final answer relevance score is calculated by averaging the scores from the last 20 queries. Formal: Finally, we get the answer relevance score for each query by averaging the scores from the last 20 queries. Formal: The answer relevance score from the last",
        "directory": "acl",
        "filename": "2020.acl-main.440.json"
    },
    {
        "casual_text": "• Hansards (200 sentences): This is a set of sentences that aren't related to the main topic. They're just random ones picked from a standard test set of Canadian Hansards.",
        "formal_text": "• Hansards (200 sentences): out-of-domain set consisting of randomly selected sentences from the standard test set of Canadian Hansards.",
        "GPT2_formal_text": "(in_ids), extract_ids (in_ext), and conj_ids (in_conj). The reason we used this approach is because S2ORC doesn't have enough training data to handle the words in the input sentences. Our method, though, gives us a solid understanding of the text by creating a detailed, organized representation of the whole corpus. Plus, this knowledge helps us quickly figure out the most relevant examples for any given query. Formal: S2ORC has a lot of useful information that can be used to train models, so we decided to create a knowledge base to put together all this useful info. Formal: To make sure the information in S2ORC is as accurate as possible, we added some extra details to each entity mention to make sure we're comparing every mention correctly. Formal: We ran some experiments to see how the features and features based on the subject matter (S2ORC subject) affect things. Formal: Following what other researchers have done, we used two different methods to measure how accurate our representations are. Formal: We ran some experiments to see how the features and features based on the subject matter (S2ORC subject) affect things. Formal: We used two different methods to measure how accurate our representations are. Formal: We also did some experiments to see how the outputs from the S2ORC subject-based models affect how accurate the representations are. Formal: We ran some experiments to see how the outputs from the S2ORC subject-based models affect how accurate the representations are. Formal: Formal: We also did some experiments to see how the outputs from the S2ORC subject-based models affect how accurate the representations are. Formal: Formal: Formal: Table 1 shows the results for our baselines on the S2ORC corpora. Formal: Table 2 shows the results for our best model, which is the S2ORC subject-based model, on the NIST TAC-2009 dataset. Formal: Table 3 shows the results for our best model, which is the S2ORC subject-based model, on the NIST TAC-2009 dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "amta",
        "filename": "2012.amta-commercial.11.json"
    },
    {
        "casual_text": "In this part, we’re looking into how each layer figures out what’s “local.” First, we took a look at how the window size changes as we move through the layers. After that, we dug into the behavior of the first word embedding layer, which turned out to be a bit different from the rest.",
        "formal_text": "In this section, we try to answer how does each layer learn the localness. We first investigated how the window size varies across layers. Then we checked the specific behavior of the first word embedding layer, which is inconsistent with the trend of other layers.",
        "GPT2_formal_text": ") // k_s = attention_mask(y * x)  \n\nFor the first model, we run the LSTM model on the first sentence in the source document. We treat the output of the LSTM as the context. Formal: We keep updating the attention matrix for the source document. This is kind of like how attention works in the ResNet-152 model. Formal: After each sentence in the source document, we grab all the sentence vectors from the same document as the one before it. So, if we have a sentence x, we can write it as a matrix of size W × n, where n is the number of sentences in the source document. Formal: The attention matrix for the source document, which we call α_s, is calculated using this formula: Formal: After each sentence in the source document, we grab the sentence vectors from the same document as the one before it. Formal: Since we're dealing with fixed-length vectors, we first create a max-pooling operation with the attention matrix α_s to get the final attention vector for the source document. Formal: The final attention vector for the source document, which we call f_s_i, is calculated by adding up all the attention vectors from the source document. Formal: For a given source document, we can figure out the probability that a sentence y_i is generated by the model M by looking at the attention matrix α_s_i. Formal: The input sentence x, along with the model M, is turned into a fixed-length vector called x_t. Formal: We combine x_t with the model's output vector, called x_t_i, to get the attention vector for the source document, which we call α_s_i. Formal: Finally, we use the attention vector α_s_i to calculate the attention score for the source document, which we call a_s_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1475.json"
    },
    {
        "casual_text": "As more and more video content gets uploaded online every day, learning stuff from videos, especially for tasks like \"How to,\" has become a big part of our daily lives and work. But watching long videos can be super time-consuming. Right now, there are two main ways tech is trying to make videos easier to handle: video summarization, which cuts long videos into shorter ones, and (dense) video captioning, which creates a text summary of the important parts in the video. For really long videos, dense video event captioning breaks things down into smaller, detailed captions for all the key events. This helps people quickly scan through the video and opens up cool possibilities like creating video chapters or making it easier to search within a video.",
        "formal_text": "With the increase of video data uploaded online every day, the acquisition of knowledge from videos especially for Howto tasks is indispensable for people's daily life and work. However, watching a whole long video is time-consuming. Existing technologies focus on two main research directions to compact video information: video summarization to trim long videos to short ones and (dense) video captioning to generate a textual description of the key events in the video. Typically for long untrimmed videos, dense video event captioning generates fine-grained captions for all events to facilitate users quickly skimming the video content and enables various applications e. g. video chaptering and search inside a video. *",
        "GPT2_formal_text": "= {h_i, ..., h_i+1} i=1 to L s=1. Formal: The S-rule we just talked about can also be applied to a more complicated situation, like the one in Table 1. We’re assuming that the answer to question qi is fully covered by the question. This means that every possible answer, including qi, gets a probability. Formal: If an answer is missing any info, we toss it out. This is kind of like how the answer to question qi is made, where the missing info can be turned into an \"answer\" using the S-rule. We figure out a specific number of possible answers to get the total number of possible answers, N s, i=1 to M. This is the total number of possible questions we’re looking at in each round of the search. Formal: For each of these answers, we create a probability distribution p(q|q i ), using the S-rule we just talked about. Formal: The same idea applies here. Formal: Instead of just looking at the question and the answer, we also calculate the probabilities for each possible relation r i in the answer, based on the S-rule we just used. Formal: Finally, we add the probability distribution for each relation r i in the answer to get the final probability, P(r i |q i ). Formal: Formal: Also, instead of using this probability distribution, we could use something like a bootstrap resampling method, which can estimate the probability for each relation r i in the answer. Formal: Formal: We’re also planning to tweak this idea to work better with the QA model. Formal: In Section 4.2, we’ll see that we can improve the quality of the answer by tweaking how we handle the questions and the answers. Formal: After that, we can use the new probability distribution to estimate the probabilities for different relations. Formal: We’ll also talk about how we can use the information from the answer to figure out the answer to a new question q. Formal: The QA model can be seen as a sequence of matching words that should be in the answer. Formal: For instance, the question \"What is the capital of the United States?\" should match the answer \"Washington, D. C.\" Formal: Formal: We’ll also give a quick overview",
        "directory": "acl",
        "filename": "2021.acl-long.156.json"
    },
    {
        "casual_text": "Even though this approach might seem like it’s not following the usual rules of compositionality, it’s actually pretty straightforward to create a function that combines the meanings of smaller parts to give you the meanings of all these sentences. We can just set it up with different rules for different cases: for example, \"departs~from/\" means \"connect,\" \"departs/from/on\" means \"dday,\" and so on. Hirst tweaks the definition of compositionality to say that \"the meaning of a whole is a systematic meaning of the parts\" (from his work, page 27, with our emphasis added), but he doesn’t really explain what \"systematic\" means.",
        "formal_text": "Although this semantics does look like noncompositional, it is easy to create a function that produces the meanings of all these sentences from the meanings of its parts --we can simply define such a function by cases: the meaning of departs~from/ is connect, the meaning of departs/from/on is dday, and so on. Hirst therefore changes the definition of compositionality to \"the meaning of a whole is a Lystematic meaning of the parts\" (op. cit. p. 27. ; tile emphasis is ours), but without defining the meaning of the word \"systematic. \"",
        "GPT2_formal_text": "_t = tanh(1-attn(z)) at the same time. Formal: • **AttnAttnAttn**: This is the attention matrix for the word embedding matrix z. Formal: • **AttnAttnAttn**: This is the attention matrix for the word embedding matrix z. Formal: Formal: The attention for the non-chatty word x, which comes from the word embedding matrix z, is calculated using the formula below. Formal: Formal: Here, `<j>` is the length of the word embedding matrix z, `<c>` is the width of the window that can be changed during training, ``(x, z)` is the number of character embeddings that belong to the word embedding matrix, and `<n>` is the number of non-chatty words. Formal: The attention for a non-chatty word x, which comes from the word embedding matrix z, is calculated by multiplying two matrices, `<j>w` and `<c>w`, and then using a linear transformation. The output of this process is a matrix called `AttnAttnAttn`, which is the attention matrix for the word embedding matrix z. Formal: Lastly, the attention for the non-chatty word x, which comes from the word embedding matrix z, is calculated using the formula below. Formal: The attention for a non-chatty word x, which comes from the word embedding matrix z, is calculated by multiplying two matrices, `<j>w` and `<c>w`, and then using a linear transformation. The output of this process is a matrix called `AttnAttnAttn`, which is the attention matrix for the word embedding matrix z. Formal: Finally, the attention for the non-chatty word x, which comes from the word embedding matrix z, is calculated using the formula below. Formal: Formal: Formal: Lastly, the attention for the non-chatty word x, which comes from the word embedding matrix z, is calculated using the formula below. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C92",
        "filename": "C92-1042.json"
    },
    {
        "casual_text": "We’ve noticed that some parts of morphology don’t really fit well with the transformation-based approach we’re using here. Infixation and templatic morphology are good examples. Even agglutinative systems, which seem like they’d work with transformation rules because they add stuff to the ends of words, can get tricky when you start dealing with more complex morphological processes. For example, in Turkish, the suffixes -lar and -ler seem like they’d need two separate transformation rules, but they’re actually just one morpheme that changes depending on vowel harmony. \n\nThis isn’t a huge deal for breaking words into their parts (morphological segmentation) because -lar and -ler show up a lot and we can still recognize them as separate suffixes. But to really cover all the different ways languages work, we’re going to need a more solid way of representing these processes. We’ll save that for later research, though.",
        "formal_text": "We note, however, that certain morphological phenomena do not readily yield to the transformationbased approach here. Infixation and templatic morphology are obvious examples. Even agglutinative systems, which at first glance appear suitable for transformation rules that operate at word edges, may still prove problematic when more global morphological processes are at play. For instance, the Turkish suffixes -lar and -ler will fall under two distinct transformational rules but are in fact one morpheme that is realized differently due to vowel harmony. This problem does not pose insurmountable problems for the purpose of morphological segmentation since both -lar and -ler are relatively frequent and can be identified as genuine (and distinct) suffixes, but clearly a more robust representation of morphological processes will be necessary to account for the full range of languages. We leave this problem for future research.",
        "GPT2_formal_text": ", source_ids, and labels, which are the relevant parts of each sentence. Formal: Next, the model calculates the relevance score, which is just the average of all the relevance scores it gets. Formal: The final result is the normalized relevance score, which we call r. Formal: In this part, we explain how the model calculates relevance scores. Formal: The relevance scores are calculated using the following method: Formal: First, the model picks out the most important words from the original sentences. These words are then used to calculate the relevance scores, which help adjust the importance of the chosen words. Formal: After that, we calculate the relevance scores by using the weighted average of the source and target nodes. Formal: So, it's like combining the importance scores from both the source and target nodes to figure out the relevance scores for each word. Formal: In the second step, we use the relevance scores to calculate the score for each sentence. Formal: This method helps us understand how important each sentence is for the summarization model. Formal: Formal: Lastly, we use the target node to calculate the relevance score for each target mention. Formal: This is like getting the relevance score for each mention by using the target node. Formal: We explain how we calculate the relevance scores for the target and source nodes in Section 4. Formal: The relevance scores for the source and target nodes are calculated using equation (4). Formal: Finally, the relevance scores for the source and target nodes are calculated using equation (5). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C18",
        "filename": "C18-1005.json"
    },
    {
        "casual_text": "We're using the 460-hour LibriTTS clean sets (Zen et al., 2019) for training our text-to-speech (TTS) model. This dataset is a multispeaker collection of English speech from audiobooks on the LibriVox project. We've trimmed the audio waveforms following the ESPnet recipe (Watanabe et al., 2018). To check the Word Error Rate (WER), we're using the open-source ASR model wav2vec 2.0 CTC 6. For evaluating how natural the synthetic speech sounds, we're relying on the NISQA-TTS model (Mittag and Möller, 2020), which is also open-source.",
        "formal_text": "Dataset and Evaluation Metrics We use the 460-hours LibriTTS clean sets (Zen et al. , 2019), a multispeaker corpus of read English speech from the audiobooks of the LibriVox project, as TTS training dataset. We trim the waveform as ESPnet recipe (Watanabe et al. , 2018). The WER is evaluated by using the open-source ASR model wav2vec 2. 0 CTC 6. The naturalness of synthetic speech is estimated by using the open-source TTS naturalness prediction model NISQA-TTS 7 (Mittag and Möller, 2020).",
        "GPT2_formal_text": "ed = { 0.1, 0.5, 0.8, 0.9 } ctx.\n\nFor the CKY-based methods (like those by Lin et al., 2015; Shen et al., 2017; Huang et al., 2017; and Krizhevsky and Lenci, 2016), they combine the attention vector for the last word with the attention vector for the previous word, as shown in equation (8). Formal: For instance, if you have the sentence \"John plopped down Mary's head,\" the attention for the word \"Mary\" (maj) and the previous word (mi) is made up of a = [maj + mmi]. Formal: Formal: Formal: We also added attention weights for the pre-trained model. These weights were learned using the example in equation (6) and the entire training set, which has 52K sentences. Formal: Formal: Formal: We decided to use word2vec because it lets us use word embeddings that are really good at handling context. Formal: Formal: Formal: We trained a fully-connected layer to get word embeddings for the words in the sentence, and then we used the embeddings for the pre-trained model to figure out attention weights. Formal: Formal: To measure how much the attention weights are contributing to the prediction, we calculated the BERTScore (from Devlin et al., 2019) for each word embedding. Formal: Formal: Formal: Formal: We calculated the BERTScore using a hidden layer size of 200. Formal: Formal: Formal: Formal: The attention weights are calculated using the method by Shen et al. (2017). Formal: Formal: Formal: Formal: We also used the best-fit linear-chain CRF layer (from Liu et al., 2018) and trained it using the algorithm from Shen et al. (2017). Formal: Formal: Formal: Formal: Formal: Formal: We combined the outputs from the CRF layer and the model's output using a hidden layer size of 200. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.393.json"
    },
    {
        "casual_text": "Okay, let's break this down in simpler terms:\n\nFirst, \"D i, j\" just means there's a connection between words \"w i\" and \"w j\" in the sentence's dependency tree. \"Sim(•)\" is how we measure the similarity between words. If there's no similarity, we just set \"Sim(•)\" to 0. \n\n\"ξ i, j\" is a tweak factor that deals with how the sentiment (like happy or sad) of an image part doesn't match the sentiment of a word in the text. \"ω(w i )\" is the emotional weight of word \"w i,\" which we get from a thing called SenticNet. If the word isn't in SenticNet, we just set its weight to 0. \"|•|\" is just the absolute value, like turning -5 into 5.\n\n\"a j\" and \"o j\" are the attribute and object of a box in the image, kind of like describing what's inside the box.\n\nNow, inspired by some smart people (Kipf and Welling), we made a graph that connects different types of info (like text and images) without any direction, so it's like A i, j = A j, i. Each part of the graph also has a loop to itself, so A i, i = 1.\n\nThe reason we made this graph (see Equations 7 and 9) is two-fold:\n1) Sometimes, sarcasm in text can be spread across multiple words, like \"wonderful weather\" when it's actually bad. So, we added connections based on the sentence's structure to help understand these situations better.\n2) We also made a special factor, \"κ i, j,\" that adjusts how much we care about opposite sentiments. \"γ\" is a number we can change to control how much we focus on these opposite sentiments, and we usually set it to be more than 1.",
        "formal_text": "Where D i, j indicates that there is a relation between w i and w j in the dependency tree of the sentence. Sim(•) represents the computation of word similarity 2. We set Sim(•) = 0 if the return value is N one. ξ i, j is a modulating factor refers to the sentiment relation (sentiment incongruity) between an image region and a text token. ω(w i ) ∈ [−1, 1] represents the affective weight of word w i retrieved from SenticNet (Cambria et al. , 2020). We set ω(w i ) = 0 if w i cannot be found in SenticNet. |•| represents absolute value calculation. a j and o j respectively denote the attribute and the object of the bounding box j. Inspired by Kipf and Welling (2017), we construct the cross-modal graph as an undirected graph, A i, j = A j, i, and set a self-loop for each node, A i, i = 1. The intention of the cross-modal graph construction (Equations 7 and 9) is that: 1) As in the examples shown in Figure 1, the sarcastic information of text-modality may be expressed by multiple words, such as \"wonderful weather\". Therefore, we incorporate the syntax-aware relations over the dependency tree of the sentence into the cross-modal graph to advance the learning of the contextual dependencies 3. 2) We devise a coefficient κ i, j, which is associated with the affective weights, to modulate the influence of contrary sentiment relations. Here, γ > 1 is a tuned hyper-parameter to regulate the bias of inconsistent sentiment relations.",
        "GPT2_formal_text": "is a list of words that are part of the reference collection and aren't part of the candidate collection. It uses the tfIDf score, which stands for the total average TFIDF score of each word in the reference set. In our setup, this score is always set to 0.3 for the candidate set and 0.5 for the reference set. We also add two more factors to make sure the candidate and reference collections match up correctly. Formal: The reference, candidate, and reference set are all linked together using graph structures. These graphs are made using SVD (which is a method from Och and Ney in 2003) to create a directed acyclic graph. Since there are a lot of possible translations for a given word, we use a modified version of the reciprocal of the length of the reference word, which we call w r. Formal: For the reference, candidate, and reference set to match, the embedding for w r should be the same as the embedding for w r. Formal: This means the embedding for the reference, candidate, and reference set should be the same as the embedding for w r. Basically, we're looking for the embedding that gives us the highest probability. Formal: For the candidate set and reference set to be in synchronic relationship, w r should be either the same as w r or very close to w r. Formal: The candidate set and reference set should be in synchronic relationship, w r should be the same as w r, and w r should be very close to w r. Formal: We set w r to 0.3 and 0.5. Formal: The embedding for w r should be 0.3 or 0.5. Formal: We also tweak the parameters α_r and β_r to adjust the importance of each relation r in the reference set. Formal: This tweak ensures that w r and w r don't overlap much. Formal: We use a dynamic programming algorithm to find the best set of parameters. Formal: We train the model using the training data. Formal: We train the model using the training data. Formal: We train the model using the training data. Formal: We train the model using the training data. Formal: We train the model using the training data. Formal: We train the model using the training data. Formal: We train the model using the training data. Formal: We train",
        "directory": "acl",
        "filename": "2022.acl-long.124.json"
    },
    {
        "casual_text": "First, let’s quickly go over Procrustes Analysis (PA), which is a method to find the best orthogonal mapping between two languages. Then, we’ll explain our approach, called Multi-Pairwise Procrustes Analysis (MPPA), which builds on PA and works for multiple languages. \n\nImagine we have d-dimensional word embedding data from two languages, along with a dictionary that pairs up corresponding words. Mikolov et al. (2013) found that there’s a strong linear relationship between the vector spaces of two languages, and surprisingly, a simple linear mapping works just as well as a more complicated nonlinear one. Xing et al. (2015) took this a step further and showed that making sure the linear mappings are orthogonal (like rotation matrices) helps prevent overfitting and improves results. \n\nTo find this orthogonal mapping, T, we can minimize this cost function:",
        "formal_text": "We first briefly review Procrustes Analysis (PA), a procedure to find the best orthogonal mapping between two languages. We then describe our approach, Multi-Pairwise Procrustes Analysis (MPPA), which extends PA to the multilingual case. Assume we are given d-dimensional word embedding data from two languages along with a dictionary consisting of pairs of corresponding words. Mikolov et al. (2013) showed that there is a strong linear correlation between the vector spaces of two languages and that learning a complex nonlinear neural mapping does not yield better results than with a linear mapping. Xing et al. (2015) further showed that enforcing the linear mappings to be orthogonal matrices reduces overfitting and improves performance. We can learn the orthogonal mapping T by minimizing the following cost function:",
        "GPT2_formal_text": "ed_X (d). Formal: We measure how well the translation is doing by checking the accuracy at the token level (called CEILING). We calculate the score for each word in the source language (SL) using the average of the scores for each token in the target language (TL). For each word in the target language, we calculate the score using the average of the scores for its SL neighbors. Formal: We also use the attention score at the token level, which is the average of the attention scores from the source and target tokens. Formal: To make sure the model isn't overfitting, we add a dropout layer before the main network, which helps reduce the effect of the source and target tokens. Formal: The final loss function for the model looks like this: Formal: We train the model for up to 5000 iterations with a batch size of 32. Formal: We also check how the model performs with different learning rate settings. Formal: After training, we update the model's parameters with the cross-entropy loss function. Formal: The loss function is calculated based on a warm-up period of 25 epochs. Formal: We use a linear layer at the token level, with a dropout rate of 0.3, and we set the weight for each token to be between 0.1 and 0.3. Formal: We also set the hidden state size to 300. Formal: For more details, check out the Appendix. Formal: The model learns a maximum of 10 tokens. Formal: The model was trained on a Tesla P100 GPU using a batch size of 32. Formal: For the validation set, we calculate the validation loss using cross-entropy loss, and we set the dropout rate to 0.1. Formal: We also adjust the model's parameters during training, based on the validation loss. Formal: For the cross-entropy loss function, we use a linear layer at the token level, with a dropout rate of 0.3. Formal: The size of the hidden state is 300. Formal: We train the model on a Tesla P100 GPU for up to 5000 iterations with a batch size of 32. Formal: We also check how the model performs with different learning rate settings. Formal: Formal: To get a better understanding of how the model performs, we use a four-fold cross-validation with an average of 10,000 iterations. Formal",
        "directory": "D19",
        "filename": "D19-1363.json"
    },
    {
        "casual_text": "To make things sound more natural and varied, we have real people manually turn graph queries into regular questions. We’ve got two ways of doing this, which you can check out in Figure 5. Each query gets sent to a few different people to rewrite it in their own words. Plus, we use different names or ways of saying things for the same entity, which we get from FACC1, to mix it up even more. We’ve got a list of common ways to say each entity, along with how often they’re used. For instance, for \"UnitedStatesOfAmerica,\" you’ve got options like \"us\" (108M times), \"united states\" (44M), \"usa\" (22M), and so on. Lastly, we automatically convert these graph queries into SPARQL queries to find the answers.",
        "formal_text": "In order to ensure naturalness and diversity, we employ human annotators to manually convert graph queries into natural language questions. We manage to provide two levels of paraphrasing (Figure 5). Each query is sent to multiple annotators for sentence-level paraphrasing. In addition, we use different lexical forms of an entity mined from FACC1 for entity-level paraphrasing. We provide a ranked list of common lexical forms and the corresponding frequency for each topic entity. For example, the lexical form list for UnitedStatesOfAmerica is \"us\" (108M), \"united states\" (44M), \"usa\" (22M), etc. Finally, graph queries are automatically translated into SPARQL queries to collect answers.",
        "GPT2_formal_text": "= arg_ids; } t, r = t; e, f = f; }\n\nThe final solution is a list of all possible correct answers, sorted from the most likely to the least. Formal: To calculate the total score for the answer span, we just add up the scores of all the candidates that are right next to it. Formal: Using the method we talked about earlier, we end up with a list of all possible answers for a given question q_i. Formal: We use a probabilistic model to figure out the overall score for the span, which we write as α_t = exp(r_t * s_t * q_i) * θ. Formal: This score is then adjusted based on a specific knowledge label L_t. Formal: Finally, we average all the possible answers from the span using a function f_t, which adjusts the score based on a probability distribution p_t. Formal: In this project, we tested how well our model works on five different datasets. Formal: To see how well our model performs on new questions, we tested it on a dataset where questions have already been answered. Formal: To make the model better, we could use knowledge from different sources (like a knowledge base) to help the model make better decisions. Formal: The last step is to try out different knowledge distillation methods to see how they help improve the model. Formal: Formal: After training, our model uses this updated knowledge to predict the final answer. Formal: Formal: We also tried our model on three datasets where the questions have been answered already. Formal: Formal: We used the knowledge from these datasets to predict the answer. Formal: Formal: To see how much the model relies on the prior knowledge, we compared it to a model that uses a large prior knowledge base. Formal: Formal: We also ran experiments to see how well the knowledge distillation helps us improve the model. Formal: Formal: Formal: Our model does pretty well on two datasets that have multiple correct answers, but it struggles with datasets that only have one correct answer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1054.json"
    },
    {
        "casual_text": "We’ve added our stuff to our Lee-BERT framework, and here’s the gist of it:",
        "formal_text": "Our contributions are integrated into our Lee-BERT framework, which can be summarized as follows:",
        "GPT2_formal_text": "ed is the data augmentation part that adds a residual connection between the input and the output. The loss is calculated as l_ij = (1 - tanh(1 - word_t)^ij + theta(1 - word_s)), where theta(1 - word_s) is the average of how often words appear together, and tanh(1 - word_t) is the average of the word's temperature. The loss function is optimized using the cross-entropy loss function. Formal: Here, δ_ij is the loss function that adjusts the attention parameters based on the prediction made by the K-layer perceptron. Formal: You can see how the embeddings are defined in Figure 2. We use a log-linear model to make the embeddings, which gives us a loss of L_P = 0.1. For the non-contextual word embeddings, we stick with the default model (a ReLU activation function) and set the dimension to 50, which is what's used in the SVM-based semantic role labeler (SRL). Formal: We also use a log-linear model for the context embeddings, which gives us a loss of L_C = 0.1. Formal: For the token embeddings, we use a log-linear model (a BiLSTM) to create them. We set the dimension to 50 and the weight vector to 50, following the default settings for BERT. Formal: We use a log-linear model for the token embeddings, and the input word embeddings are also generated as a binary feature vector using a linear layer. Formal: We apply the same loss function as before for the contextual word embeddings, using the same L_C loss. Formal: We take the embeddings we got from the word embeddings and use them to create contextual embeddings for the entity embeddings. Formal: The loss function is optimized using the cross-entropy loss function. Formal: We compare the performance of the two models to a setup where we replace the latent representations in the embeddings with a fully-connected layer, which is a popular approach for token embeddings. Formal: We use a linear model to create the contextual embeddings, and the input word embeddings are also generated as a binary feature vector using a linear layer. Formal: We train the model",
        "directory": "acl",
        "filename": "2021.acl-long.231.json"
    },
    {
        "casual_text": "On Wikipedia, \"category\" basically means tags or topics that group together things related to the same subject. For example, Yao Ming is in the category \"Olympic basketball players of China,\" which tells you what he's about, not what kind of thing he is. Categories can be helpful for figuring out what something is if you don't already know.",
        "formal_text": "• Category refers, in Wikipedia, actually to tags/topics that group entities on similar subjects. For instance, Yao Ming in Wikipedia has category Olympic basketball players of China, which is the topic of the entity instead of its type. Category can be a useful information to infer missing type information.",
        "GPT2_formal_text": "= {}, title = t, title_text = t, title_summary = t, topic = t, and topic_text = t. Formal: We've got a bunch of training examples for the query and topic, which we call s1_t = {s1_t1, ..., s1_tn}, and s2_t = {s2_t1, ..., s2_tn}. Formal: When we're training, we get a sentence S = {s1_s1, ..., s1_sn} by randomly picking a topic K from the set K. Formal: At test time, we grab all the training examples from the query and topic, which we call h_q = {h_q1, ..., h_qn}, and the test set H = {h_q1, ..., h_qn}. Formal: Here, the numbers in the denominator (1 − m) represent how many training examples we get for each topic, starting from zero. Formal: We calculate the mean average precision (MAP) and mean reciprocal rank (MRR) for the selected topic K and the query Q, respectively. Formal: We also calculate a recall (R) for the selected topic K and the query Q, and we calculate a precision (P) and recall (R) for the selected topic K and the query Q. Formal: Finally, we average the precision and recall to get the overall result. Formal: We average the precision and recall for each selected topic K and query q, but for the selected topic K and query q, we only calculate the mean average precision (MAP) and mean reciprocal rank (MRR) to get the overall result. Formal: Formal: And we also average the precision and recall for each selected topic K and query q, but for the selected topic K and query q, we only calculate the mean average precision (MAP) and mean reciprocal rank (MRR) to get the overall result. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-1502.json"
    },
    {
        "casual_text": "If we don't consider the LM score, we can calculate everything exactly. We can sort the different translation options for each hyperedge based on their costs. For hypernodes without any predecessors (basically, just words), we can generate these in a straightforward, step-by-step manner. This setup lets us, with the right approach, generate all the options for each hypernode in order of increasing cost.",
        "formal_text": "If no LM score is taken into account, this computation can be carried out in an exact way. The different translation alternatives for one hyperedge can be sorted according to their costs. The derivations in the hypernodes with no predecessors (purely lexical rules), can thus be generated in a monotonic way. This allows, with a proper combination strategy, to generate the derivations in every hypernode in a cost-increasing order.",
        "GPT2_formal_text": "ed (src) and attention_masked (mk) are trained separately using the data split in Table 2. Formal: The attention maps are then fed into an MLP layer that decides which entity spans are relevant, using softmax. Formal: The fine-tuning process involves two main steps: (1) fine-tuning the encoder with the info from the training data, and (2) using cross-entropy loss on the validation set to minimize the cross-entropy between the new prediction and the old one. Formal: Check out the results for the cross-entropy loss in Table 3. Formal: Also, for the model without attention, the cross-entropy loss works out better than the attention loss. This tells us that the attention mechanism helps the model pick up on the important context, which helps improve its performance. Formal: Formal: In Figure 2, you can see the cross-entropy loss for the base model, and for the better model, it's the cross-entropy loss for the cross-entropy loss-pretrained model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2009.eamt-1.33.json"
    },
    {
        "casual_text": "It'll be a basic search tool for regular users, but behind the scenes, it uses fancy semantic annotations to find audiovisual files more accurately and smartly.",
        "formal_text": "• it will serve as simple search interface for general users, taking advantage of the rich semantic annotation -behind the scenes-for more precise and intelligent retrieval of audiovisual files",
        "GPT2_formal_text": ") {  // Return the full path from the visited node to the current node in the stack.    return ( visit_index(c) < 0 || visit_index(c, p) < 0); }      // The graph traversal algorithm that turns the stack into the attention matrix         public static GraphTraverse(int i) {  // Set the graph context for the visited node.      // Check if there's a node history that can be used to update the current node.     if (node_history_i == node_history) {    return (node_history);    }      // Update the current node's context.      // Set the graph context for the visited node.        return (node_context);       }      // The graph interaction algorithm that combines the graph context and the visited node's context      public static GraphInteraction(int i) {  // Set the graph context for the visited node.    // Check if there's a node history that can be used to update the current node.     if (node_history_i == node_history) {    return (node_history);    }        // Update the current node's context.        return (node_context);      }      // The graph interaction algorithm that combines the graph context and the visited node's context.        public static GraphInteraction(int i) {   // Set the graph context for the visited node.    // Check if there's a node history that can be used to update the current node.      if (node_history_i == node_history) {     return (node_history);     }          }         }\n\nIn the end, the updated graph context for the visited node x i is:",
        "directory": "E09",
        "filename": "E09-2014.json"
    },
    {
        "casual_text": "Alright, let's break down what Table 7 is showing us about the comment generation dataset. The table has a few key stats:\n\n1. The total number of samples.\n2. The average number of subtokens in the code.\n3. The percentage of samples where the code has fewer than 100, 150, or 200 subtokens.\n4. The average number of subtokens in the comments.\n5. The percentage of samples where the comments have fewer than 20, 30, or 50 subtokens.\n\nFigure 5 gives us a visual representation of how the number of subtokens in the code (on the x-axis) and the number of subtokens in the comments (on the y-axis) are distributed.",
        "formal_text": "Comment generation. Table 7 shows the statistics of the comment generation dataset. The rows, from top to bottom, are: the number of samples; the average number of subtokens in code; the percentage of samples whose number of subtokens in the code is less than 100, 150, 200; the average number of subtokens in comments; the percentage of samples whose number of subtokens in the comment is less than 20, 30, 50. Figure 5 visualizes the distributions of the number of subtokens in code (x-axis) and the number of subtokens in comments (y-axis).",
        "GPT2_formal_text": "_t, which is the total number of attention masks in the layer before the activation function. \n\nAfter the attention-layer layer, the output of the text segmentation layer gets fed into a BiLSTM layer. This BiLSTM model is then used to create a final image representation called h_t. After that, the feature representation for the whole image is calculated by combining h_t with the image features. Formal: We use the Image-to-Text Model (VIM) (Hessel et al., 2015) with LSTM-based attention to figure out the attention mask distribution. Formal: Following Wang et al. (2017), we add an attention module at the output layer to get the masked token representation. Formal: We look at the context window around the masked token and calculate the max value for the word embedding vector, which is v_t. We do this for all the words in the sequence. Formal: Using a logistic regression model, we calculate the probability of the masked word, p_mask_t, based on the context words and the word embedding vector. Formal: Then, we use an ensemble method to predict the masked token distribution p_mask_t, which helps us predict the image features. Formal: Finally, we use a Maximum Entropy classifier to predict the image features, which helps us predict the image attributes. Formal: Finally, we use a Maximum Entropy classifier to predict the masked token distribution p_mask_t, which helps us predict the image features. Formal: We calculate the mask feature representation p_mask_t for all the words in the sequence. Formal: For the regularization term, we set a value of 0.1 using a linear kernel. Formal: We also set the regularization parameter, p_trunc, to 0.1. Formal: For the max pooling term, we use a linear kernel. Formal: Lastly, we add a linear kernel to the convolutional layer to get the final image representation. Formal: Formal: We pick the output as the image feature representation, h_t. Formal: Formal: We also add a linear kernel to the convolutional layer to get the final image feature representation, h_t. Formal: Formal: We pick the output as the image feature representation, h_t. Formal: Formal: We also add a linear kernel to the",
        "directory": "acl",
        "filename": "2022.acl-long.339.json"
    },
    {
        "casual_text": "When you run two NLP systems on the same data, you’d expect their results to follow certain rules or patterns. This is like a built-in assumption we have about how they should work. We’re suggesting a method where we use this idea to make one of the systems perform way better. The main concept is to focus on improving it only where the outputs match those expected patterns.",
        "formal_text": "When two NLP systems are run on the same data, we expect certain constraints to hold between their outputs. This is a form of prior knowledge. We propose a self-training framework that uses such information to significantly boost the performance of one of the systems. The key idea is to perform self-training only on outputs that obey the constraints.",
        "GPT2_formal_text": ", a_t; a_s, where a_t is the title of the document. To create a summary, we start by using the full embedding of the title. Formal: • Not all sentences in the text are available. For instance, in Figure 1, the sentence \"John loves Mary\" is missing. Formal: • Unlike what you might expect, people's opinions on a specific piece of content (like an article) are usually pretty diverse. Formal: • Since the whole text is already tagged with the document, the task is to figure out the most important pieces of information, like the title, that can help us understand what the rest of the text is about. Formal: • These are the key pieces of info (like the title) that the user is looking for. Formal: • The words in the title are usually more important than the words in the whole document. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "In this paper, we’re suggesting three different ways to automatically break words into syllables using a bunch of text data. The first method is rule-based, the second one uses sequence labeling, and the third one combines the rule-based approach with sequence labeling.",
        "formal_text": "In this paper, we propose three different corpus-based automatic syllabification methods; (i) Rule-based approach, (ii) Sequence labelling approach and (iii) Hybrid of rule-based and sequence labelling approaches.",
        "GPT2_formal_text": "_tgt = [0, 1] Formal: Let's use b = [0, 1] as the input for the biaffine attention mechanism. Formal: Alright, so here's what's going on with these two attention methods: Formal: • Masked Attention: This one focuses on the parts of the input that need attention, while the other one pays attention to the whole input. Formal: Formal: We also tested the pipeline model and the one that uses attention. The results are in Table 4. The pipeline model did better than the attention-based model in every case. This shows that when we can, we can use the guidance from the neural network to figure out the attention patterns in the input and make it work well. Formal: For the dataset (RTE 2005), we used the transformer model (Vaswani et al., 2017) as the base model. We tweaked the transformer model a bit by adding a hidden layer with a linear kernel, and we trained it using the same training set that was used for the relation extraction task. The results are in Table 5. The training set used a random sample of sentences to create the relation vectors for RTE 2005. Formal: In the test set, we only trained the transformer model on sentences that were marked as relation mentions (sent2rel). This means we didn't use any sentence pairs that weren't direct mentions of entities. We also tested the model on sentences that were marked as \"not\" relation mentions (np_nme). Formal: Formal: We found that the pipeline model does a better job of capturing the attention patterns of sentences. It did better than the attention-based model for both the RTE 2005 and RTE 2006 datasets. Formal: Formal: We also used the attention-based model for the two tasks we're looking at: entity detection (ED) and relation extraction (RE). The results are in Table 6. The pipeline model did better than the attention-based model, which shows that its ability to learn attention patterns from sentences is pretty useful. Formal: Formal: For ED, the results are in Table 7. Formal: Formal: For RE, the results are in Table 8. Formal: Formal: The pipeline model didn't do as well as the attention-based model, but it still performed pretty well for the two tasks. Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1034.json"
    },
    {
        "casual_text": "The usual ways to combine features in neural networks are adding them together, sticking them side by side (concatenation), or projecting them into a new space. These methods all happen after the features are processed. But for tasks like reading comprehension, where we need to think more interactively, we’d rather combine features earlier on. This way, each part can use what the others have figured out. \n\nFor the QCP model, as we talked about earlier, we only use [CLS] to represent the overall meaning (holistic semantic encoding) of the QCP, not the whole input sequence. There are two reasons for this. First, we believe that focusing on the big picture helps with the interaction between questions and paragraphs. Second, we want to steer clear of the attention deconcentration problem we mentioned before.",
        "formal_text": "The feature fusion methods commonly used in neural networks are addition, concatenation, and projection. All of these modalities are post-fusion mechanisms. However, in order to perform more interactive reasoning in reading comprehension tasks, we want to do feature fusion as early as possible. Therefore, these modalities can benefit from the results of the other modalities. In particular, for the QCP model, as we mentioned above, we use only [CLS] as the holistic semantic encoding of h QCP, but not the encoding of the whole input sequence. There are two reasons for this. First, we think that holistic semantics has essential benefits for the interaction of questions and paragraphs. Second, we need to avoid the effects of the attention deconcentration problem we addressed before.",
        "GPT2_formal_text": ". Without any data, the system should be able to detect all possible possible pairs of people, especially since they might not all be the same. So, the first thing to do is figure out a way to handle this. Formal: But, we think this idea could actually work really well in real life. Formal: So, instead of just checking every possible pair of people (like in Equation 1), we could focus on the ones that actually match. For each pair, we look at all the possible combinations of names and places where they could go (like their last names, first names, middle names, or last names). Then, we narrow it down by narrowing it down to the one that's closest to the actual names and the places they could appear in. Formal: We tested this on the BBC's News quiz show, where we had around 20 people answer questions about their own lives, either confirming or questioning something. We had them label the closest matches in each category, focusing only on the ones that were actually people. Formal: The results? We got a precision of 0.69 and a recall of 0.64 for this setup. Formal: We also checked how much each person's response changed based on their specific label. Formal: The bigger the label, the bigger the change in the response. In the example shown in Figure 1, the response only changes when someone says they're \"not allowed\" to go to the destination. Formal: For the final round of testing, we used the same setup but with a different set of labels. Formal: The results? We got a precision of 0.39 and a recall of 0.40. Formal: We ran all these tests 10 times with different random seeds, and the average result came out to 0.84. Formal: Oh, and we also included results for the WIKI approach, which is a simpler version of this basic approach. Formal: The WIKI approach is a quick way to look up all the possible pairs of people. Formal: Formal: Basically, a person's response to a query is like a list of all the things they could be. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.241.json"
    },
    {
        "casual_text": "We're using a single Bi-LSTM layer for words, with hidden dimensions set to 128 for CONLL and 256 for ONTONOTES. Both models have a character embedding size of 25, and the forward and backward character LSTMs have hidden dimensions of 50. To avoid overfitting, we add a dropout mask (thanks, Srivastava et al., 2014) with a probability of 0.5 on the input and output vectors of the Bi-LSTM layer. For both datasets, we set the capitalization embeddings to 25 dimensions and trained the models for up to 50 epochs.",
        "formal_text": "Our system uses a single Bi-LSTM layer at the word level whose hidden dimensions are set to 128 and 256 for CONLL and ONTONOTES respectively. For both models, the character embedding size was set to 25, and the hidden dimension of the forward and backward character LSTMs are set to 50. To mitigate overfitting, we apply a dropout mask (Srivastava et al. , 2014) with a probability of 0. 5 on the input and output vectors of the Bi-LSTM layer. For both datasets, we set the dimension of capitalization embeddings to 25 and trained the models up to 50 epochs.",
        "GPT2_formal_text": "(D) = (1 - tanh(W)) / tanh(W)  \n\nFigure 2 shows the regularization parameters for the attention weights. Formal: The attention weights are basically the mean of two things: the attention weights for a specific mention and the attention weights for all the mentions combined. Formal: When we're talking about a mention and its context, we calculate the attention weights for the specific mention, using the context. Formal: Lastly, we add a tanh function to the attention weights, which helps them focus on the important parts of the text. Formal: For instance, if the mention \"Adolf Hitler\" is in the context of \"world leaders\" and includes a word like \"NSS,\" the attention weights for this mention would be (1 - tanh(NSS)) / tanh(NSS) = (1 - tanh(NSS)) / (1 - tanh(NSS)) Formal: This whole process is called attention propagation, and it's pretty similar to the attention-based attention in the attention mechanism. Formal: Let's say t represents the sentence-level attention weights for the mention. Formal: To find the attention weights for the mention, we use a method called attention-based multi-view learning (MVL). Formal: Using MVL, we can calculate the attention weights for any mention. Formal: Finally, the attention weights for the mention are the mean of these attention weights. Formal: Formal: Alright, let's break this down in a simpler way. Formal: Here, we're looking at a mention and its context, and we calculate the attention weights for both the mention and the context. Formal: So, the attention weights for the mention are basically the mean of the attention weights for the context. Formal: Formal: The process of learning attention weights is basically a way of learning how important the mention is. Formal: Formal: The goal is to find the attention weights that maximize these importance scores. Formal: Formal: Formal: Formal: Finally, we add a tanh function to these weights to focus on the important parts. Formal: Formal: Finally, we combine these attention weights to get the final attention weights for the mention. Formal: Formal: The final attention weights for the mention are the mean of these attention weights. Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C18",
        "filename": "C18-1161.json"
    },
    {
        "casual_text": "If a language limits how its possible LGTs can be structured, LGTs become way more efficient and simpler to work with. The issue we talked about earlier doesn't happen with constituent grammars and phrase structure rules because these rules show hierarchy just by the names they give to the parts.",
        "formal_text": "If a language constrains the structures of its possible LGT's, LGT's become computationally much more economical devices. The problem discussed above does not arise with constituent grammars and phrase structure rules because these rules indicate hierarchy implicitly through the naming of the constituents.",
        "GPT2_formal_text": "= {\n\nA_i = {A_1, ..., A_N},\n\nB_i = {B_1, ..., B_N},\n\nC_i = {C_1, ..., C_N},\n\nD_i = {D_1, ..., D_N},\n\n}\n\nThis setup lets us focus on finding relationships between entities that share a certain type. For instance, we can only use the relations A_i for entities that are e_s, f_s, g_s, h_s, or i_s. It also means that we can't get the same relation from different entities. This restriction can be handled by adding rules for different types of relationships.\n\nThis setup also makes it easier to handle situations where we need to handle different types of relationships between entities. For instance, instead of trying to find e_s, f_s, g_s, and h_s relationships separately, we can use a single set of rules to handle them all together.\n\nThe whole setup can be seen as an optimization problem where we look at the final value of a feature vector F_i, which is equal to the sum of the probabilities P_f(e_s) and P_g(e_s), plus the sum of P_i(f_s) and P_i(g) for the specific type of relationship we're interested in.\n\nWe use something called the EM algorithm to solve this optimization problem. In our case, the \"subset\" operation is a bit more complex, but it's described in a more general way in (He et al., 2007). We'll walk you through the algorithm in this section. Formal: To optimize this problem, we use the AdaDelta algorithm (He et al., 2007), which builds on the EM algorithm. Formal: For each pair of entities A_i and B_i, we calculate the eigenvalue decomposition for each pair of features. Formal: We use a concatenation of these eigenvectors, which we call e_c, to calculate the value of p_f(e_i). Formal: Finally, we use a linear transformation, like a linear sigmoid, to get the eigenvalue decomposition of p_c, which we call e_c. Formal: Formal: Finally, we can use the Euler process to estimate",
        "directory": "C88",
        "filename": "C88-1056.json"
    },
    {
        "casual_text": "Okay, so EWD is the array where English words are stored after looking them up in the dictionary, and ENG is where they're stored during the analysis. The first five spots in the VOC table hold the vowels U, O, I, E, and A. \n\nNow, if the first letter of the noun we're looking at, which is stored in EWD(1, N), starts with a consonant, then in statement 1476, K will be zero, and in statement 1477, K-1 will be negative. If the first letter is U, K-1 will be zero, and if it's any other vowel, K-1 will be positive. Basically, for any noun that doesn't start with U, statement 1477 decides whether to use \"a\" or \"an\". \n\nBut if the noun starts with U, the choice depends on the third letter. Statement 1910 checks that: if the third letter is a vowel, it picks \"a\", and if it's a consonant, it picks \"an\". \n\nOf course, this system still needs some tweaking for cases where the word starts with \"un\" followed by a vowel. \n\nNow, about machine-word parts: \n4.1. CSW0 (which stands for \"compose word\").",
        "formal_text": "EWD is the array where the English words were brought after the dIB~ionary lookup and ENG where they are brought d~ring the analysis. The first five locations of the table VOC contain the vowels U, O, I, E A. If the first letter of the noun in question, stored in EWD(1, N~, starts with a consonant, K in statement 1476 will be zero, and K-1 in statement 1477 will be negative. If the first letter is U, K-1 =0, and if the first letter is any other vowel t K-1 will be positive. That means, that for any first letter of the noun, except U, statement 1477 decides whether \"a\" or \"an\" will be chosen. In the case of U the choice depends on the third letter of the noun, found in statement 1910. \"A\" will be chosen if the third letter is a vowel, and \"an\" will be chosen if it is a consonant. This routine will, of course, still have to be refined for the case when the prefix \"un\" is followed by a vowel. 4. Machine-word parts 4. 1. CSW0 (compose word)",
        "GPT2_formal_text": "ed. 1 In this paper, we explain our model for handling multiple goals at once by using both global and local graph structures to keep track of the information. Formal: The LOCAL GRAPH lets you have graph structures for all the entities and relationships in a sentence, which helps the model focus on the important details while also giving it some leeway to handle the messy parts. Formal: Sure! Here's a more casual version of that sentence: Formal: To figure out how well the model is doing on its own, we run it through three different kinds of evaluations: Formal: In these tests, we measure how well the model can reconstruct the source spans and how well it can predict the target spans. Formal: We also look at how well the model can handle edges between the source and target spans, both for the source and target entities. Formal: We also check how well the model can predict relationships between the source and target entities. Formal: Lastly, we compare the model's accuracy to the best single model out there. Formal: Formal: So, in short, the LOCAL GRAPH is all about combining different types of knowledge—like the ones from different sources—and using that to make better predictions. Formal: Formal: The main goal here is to learn how to do this all by yourself. Formal: Formal: To figure out how well the model is doing on its own, we run it through three different types of evaluations: Formal: In these tests, we measure how well the model can reconstruct the source spans and how well it can predict the target spans. Formal: We also look at how well the model can handle edges between the source and target spans, both for the source and target entities. Formal: Lastly, we compare the model's accuracy to the best single model out there. Formal: Formal: We take the results from these evaluations and compare them to the best single model in a supervised way. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C67",
        "filename": "C67-1017.json"
    },
    {
        "casual_text": "The question will be represented by the controller's state at t = 1. To generate the answer, we'll use a softmax function on a fixed answer vocabulary. This approach makes sense for factoid and list questions since the possible values for a given variable are usually known. For Yes/No and Indefinite knowledge questions, we add {Yes, No, Maybe} to the output options. Following the approach in (Weston et al., 2014), a list-task answer is treated as a single item in the answer set, similar to how count questions are handled. \n\nAn alternative could be swapping the softmax activation function at the MemN2N output for a logistic one and using categorical cross-entropy loss. However, this would require cross-validating a decision threshold to pick the right answers, which is a bit of a hassle. For count questions, the numbers found in the training set are included in the vocabulary.",
        "formal_text": "question will be encoded as the controller state at t = 1. The answer will be produced using a softmax operation over the answer vocabulary that is supposed fixed. We consider this hypothesis valid in the case of factoid and list questions because the set of value for a given variable is often considered known. In the cases of Yes/No and Indefinite knowledge question, {Yes, No, Maybe} are added to the output vocabulary. Following (Weston et al. , 2014), a list-task answer will be considered as a single element in the answer set and the count question. A possible alternative would be to change the activation function used at the output of the MemN2N from softmax activation function to a logistic one and to use a categorical cross entropy loss. A drawback of such alternative would be the necessity of cross-validating a decision threshold in order to select a eligible answers. Concerning the individual numbers for the count question set, the numbers founded on the training set are added into the vocabulary.",
        "GPT2_formal_text": "' array. The input and output are separated by '|' and include the length of the text, including the position where the input and output overlapped. Formal: You can either calculate the probability for the output based on the input (like in Equation (7)) or tweak the latent space model to focus on the output instead of the input (like in Equation (6)), both of which are called softmax-based models. Formal: The sum of all the probability values for a single token e_i is calculated like this: Formal: A matrix A that has n rows and n columns is trained to predict the probability of token e_i when it's paired with some context c_i. Formal: Formal: Here, \"c_i\" is the context vector for token e_i, and we're using a specific word embedding, which we'll call \"y_i\" for that token. Formal: If \"c_i\" is 0, the model just predicts the probability of token e_i without considering the context. Formal: Formal: We take the output vectors from the model and turn them into probabilities for the tokens. Formal: We can think of this whole process of training the latent space model as a multi-label classification problem. Formal: Formal: In this case, the context vector is the average of the word embeddings of all the tokens in the input sentence. Formal: We're using a linear layer with a ReLU activation function (from Srivastava et al., 2014) to get the latent space model. Formal: The final hidden state from the linear layer is what we use to predict the probability of token e_i. Formal: The whole training process is explained in Algorithm 1. Formal: Formal: We also tried a different approach where we treat the input sentence as a \"selection task\" where we focus on the output tokens instead of the input. Formal: Formal: This works because the input and output are often similar, which makes it easier to predict the output in the same way. Formal: We tested this on three different text classification tasks, and the results were pretty good. Formal: Formal: We also came up with an adaptive learning method that can adapt to different tasks, which helps us better understand how the latent space model is performing. Formal: Formal: Formal: The adaptation part is based on the conditional probability that the",
        "directory": "E17",
        "filename": "E17-1029.json"
    },
    {
        "casual_text": "In this paper, we're looking at how to model zero pronouns using related text. To do this, we’ve added a cool new self-attentive mechanism that helps our model pay attention to different parts of the text, which is super helpful for understanding zero anaphors. Plus, we use an attention-based method to handle candidate modeling, so our model can focus on the most important bits of the mentions. All of this makes our approach really effective for resolving zero pronouns.",
        "formal_text": "In this paper, we focus on exploring an effective way of modeling zero pronoun by using the associated texts. More specifically, we integrate a novel self-attentive mechanism, which provides our model an ability to focus on multi-aspects text, benefiting the encodings of zero anaphoras. In addition, by employing an attention-based technique for modeling candidates, our model learns to encode more informative parts of the mentions. All these bring advantages to the resolution of zero pronouns.",
        "GPT2_formal_text": ", word_mask, and hidden_pos. These are the final word representations for the word x i, which we call h i. Formal: This part talks about the idea of expanding a graph G to include the surrounding parts of the graph. The main idea is to create a new graph G* that includes all the nodes in G, along with the edges between them. Formal: So, for the graph G* created by the GCN, it looks like this: Formal: Basically, the final graph G* is the result of adding up the tokens in G and the edge weights for all the nodes in G. Formal: The goal here is to figure out how relevant each token in G is to the edge weights of the nodes in G. Formal: The final representation h i, given the graph G* and the sentence x i, is just a number that shows how relevant x i is to the sentence x i. Formal: The relevance scores come from a formula that uses a bunch of regularization parameters. Formal: We picked these regularization parameters based on how well they work in practice. Formal: We tried out different values for these parameters and found that using a value of -0.5 works best. Formal: For the Gaussian kernel, we set the kernel dimension to 256 and ran it with the Adam optimizer. Formal: We also tried out different kernels with different dimensions, like 1, 2, and 4. Formal: Formal: For the recursive kernel, we set the kernel dimension to 256 and used a polynomial kernel. Formal: We tested different kernels with different parameter values and found that the 1-dimensional kernel is the best choice for our experiments. Formal: We noticed that the most helpful kernel for our model was the polynomial kernel. Formal: The numbers we got using a linear kernel are shown in Table 2. Formal: We also got some results from the shared task, which you can find in Appendix A. Formal: Formal: We averaged the results across all the tasks to get a final value for the relevance scores. Formal: Formal: The results are in Table 3. Formal: Formal: Formal: The results are in Table 4. Formal: Formal: Formal: The results are in Table 5. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C18",
        "filename": "C18-1002.json"
    },
    {
        "casual_text": "First off, we’ll talk about the dataset and how we’re measuring success. Then, we’ll go over the settings we used for our experiments. After that, we’ll compare how our method stacks up against other approaches, both feature-based and neural-based ones. Oh, and we also did a case study showing that our SHTCNN is pretty good at pulling out better semantic features.",
        "formal_text": "In this section, we first introduce the dataset and evaluation metrics. Then we list our experimental parameter settings. Afterwards, we compare the performance of our method with feature-based and selected neural-based methods. Besides, case study shows our SHTCNN is an effective method to extract better semantic features.",
        "GPT2_formal_text": "= p_i + 1; // All these are the same except for the x_i, which is set to null. Formal: Formal: Using a max pooling layer with a matrix-size of 1, we get the final representation p_i for the text. Formal: Finally, we use a cross-attention mechanism to help the model recognize and identify different types of text, like titles, subtitles, and even full articles. Formal: The feature structure we get from our classification module is shown in Figure 1. Formal: We used the cross-attention mechanism with RNN to get the attention for each token. To do this, we just calculate the attention weight matrix A for the token x_i. We do this for each token to maximize the attention weight vector. Formal: In the second step, we find the output representation p_t for each t_i. Formal: Our system is built on the work of Lu et al. (2016). It uses three main components: attention, cross-attention, and a latent variable c_t. We ran our experiments on the CIFAR-10 dataset, and the results are in Table 1. Formal: We tested our system on the IWSLT14 data and the CIFAR-10 dataset. Our results show that our method works well. Formal: The results of our experiments are in Table 2. Formal: Our system performs better than the current top systems on the CIFAR-10 dataset. Formal: Formal: We also ran some experiments on the CoNLL09 dataset and the IWSLT08 dataset. We made the best training data by picking the top-k most relevant data. Formal: Formal: Finally, we included the results from these experiments on the IWSLT09 dataset in Table 3. Formal: Formal: Formal: Table 3 has the results from the experiments on IWSLT09. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-5533.json"
    },
    {
        "casual_text": "Alright, let's start by looking at how the model spreads out its predictions across all the different possible sequences.",
        "formal_text": "First, consider the sequence-level distribution specified by the model over all possible sequences",
        "GPT2_formal_text": "ed and attention_attention_masked_copy. If the target image is marked as negative, the attention goes to the next closest image, which is the target image minus one. If the target image is labeled as positive, the attention is shared equally among the previous and next image, which is the target image plus one. The final result is a mix of the original and the negative versions of the image. Formal: Another approach is to go with the shortest dependency path, which is just the shortest path between the source and target. We'll call this method MINVLM. Formal: Another option is to use the shortest path from the source to the target, just like the hyper-parameters. We'll call this method SVMVLM. Formal: For the symmetric model, we use the log likelihood from the source-side. For the non-symmetric model, we use the log likelihood from the target-side. Formal: We'll call the model we're proposing the symmetric model. Formal: We'll also refer to the log likelihood from the source-side of the target-side. Formal: Formal: We'll have two models, H_S and H_T, in our setup. We'll train them using the source and target representations. Formal: The posterior probability of an image at position i will be a combination of the log likelihood of the target image at that position and the log likelihood of the input image, plus the log likelihood of the source image. Formal: Our goal is to minimize the log likelihood of the target image at position i, plus the log likelihood of the input image, plus the log likelihood of the source image. Formal: We'll call the model we're proposing the non-symmetric model. Formal: Formal: For the pseudo-target image, we'll use the log likelihood of the source image at position i. Formal: For the pseudo-target image, we'll use the log likelihood of the target image at position i. Formal: Formal: For the posterior probability of the target image, we'll use the log likelihood of the source image at position i. Formal: Formal: Lastly, to maximize the log likelihood of the target image, we'll use the log likelihood of the source image at position i. Formal: Formal: Formal: We'll call the model we're proposing the pseudo-target model. Formal: Formal: Formal",
        "directory": "D16",
        "filename": "D16-1139.json"
    },
    {
        "casual_text": "1. A way to think about how we come up with a conclusion based on some starting ideas.  \n2. Two different but helpful methods that help us figure out what the conclusion should be, using the information we already have.  \n3. Real-world proof that focusing on what we're aiming for when making a conclusion is really important.",
        "formal_text": "1. A conceptual model of the task of generating an argument's conclusion from its premises. 2. Two complementary approaches that infer a conclusion's target from premises effectively. 3. Empirical evidence for the importance of modeling targets in conclusion generation.",
        "GPT2_formal_text": "= {entity_id, entity_type, entity_value}  \n# entity_type is the type of the entity, and entity_value is the value. Formal: A user's purchase history can be broken down by their item type and the price they paid. Formal: A query can be broken down by the users query and the items they bought. Formal: We use the APT method for tracking interactions and picking the right items for a given query. Basically, we use the APT score (using the F1 score) to pick the best items for a user's query. Formal: In our experiments, we set the APT score to 0.2 to avoid the problem of having a ton of candidates. But for each item type, we figure out the best values for the values of the components using the evaluation set from the user's query. Formal: For each query, we calculate the relevance score (using the relevance score formula from Section 5.1) for the items we can find. Formal: For a query, the search results can be broken down by the queries and the items. Formal: We create two types of relevance scores for each query. Formal: The query relevance score (using the query relevance formula from Section 5.1) is the highest value for the query and the items we find. Formal: The item relevance score (using the item relevance formula from Section 5.1) is the highest value for the items we find. Formal: For our experiments, we set the relevance scores to 0.2 because we don't want to have tons of candidates in our results. Formal: To find the best match, we look at both the query and the item relevance scores. Formal: Finally, we pick the highest match from the two scores to find the best match. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.399.json"
    },
    {
        "casual_text": "Detecting clickbait automatically is super important for websites to clean up their content and make things better for users. The usual way people have been doing this is by using specific features they’ve picked out manually to describe webpages. For example, Chen and his team in 2015 came up with a way to describe news articles using things like how the language sounds (like suspenseful words or numbers being overused), how the sentences are structured (like forward references), and even how the images look (like where they’re placed or if they seem emotional). They also looked at how people interact with the news, like how long they read it, if they share it, or if they comment. They used models like Naive Bayes and SVM to figure out if something was clickbait based on these features.\n\nAnother group, Biyani and friends in 2016, took a different approach. They focused on features from the webpage’s content, like patterns in the words (n-grams), how positive or negative the tone is, the parts of speech used, and how many numbers are there. They also checked how similar the title was to the first few sentences of the article using TF-IDF. Plus, they considered other stuff like how casual the title sounds, if it uses forward references, and even the URL as extra clues.",
        "formal_text": "Automatic detection of clickbaits is important for online platforms to purify their web content and improve user experience. Traditional clickbait detection methods usually rely on handcrafted features to build representations of webpages (Chen et al. , 2015; Biyani et al. , 2016; Potthast et al. , 2016; Chakraborty et al. , 2016; Bourgonje et al. , 2017; Cao et al. , 2017; Indurthi and Oota, 2017; Geçkil et al. , 2018). For example, Chen et al. (2015) proposed to represent news articles with semantic features (e. g. , unresolved pronouns, affective words, suspenseful language and overuse numerals), syntax features (e. g. , forward reference and reverse narrative) and image features (e. g. , image placement and emotional content). In addition, they incorporate users' behaviors on news, like reading time, sharing and commenting, to enhance news representation. They use various classification models like Naive Bayes and SVM to identify clickbaits based on the news and user behavior features. Biyani et al. (2016) proposed to represent webpages using content features like n-gram features extracted from title and body, sentiment polarity features, part-of-speech features and numerals features. They also incorporate the similarities between the TF-IDF features of title and the first 5 sentences in the body. Besides, they consider the informality of title, the use of forward reference, and the URL of webpage as complementary information.",
        "GPT2_formal_text": ". It's similar to the layer attention in ReLU, but it works in a different way. The input gets transformed into a sequence that's a mix of multiple input vectors, so the hidden state stays the same. The input vector h_i is just the identity matrix. Formal: Using a linear transformation and the vector representations of the keys and values, we can figure out the attention weights Φ_k and Φ_k_p. Formal: The model creates attention weights by doing a ReLU on the hidden states of the input. Formal: Since the attention is linear, the model can create a probability distribution for each token based on the hidden state of the input. Formal: In this paper, we focus on optimizing the attention weights by minimizing the cross-entropy between the training data and the predicted target, treating it as the objective function. Formal: For our experiments, we're using the CoNLL-2009 dataset, which has a bunch of English articles from the New York Times. Formal: We use a linear transformation and the vector representations of the keys and values to figure out the attention weights Φ_k and Φ_k_p. Formal: The model creates attention weights by doing a ReLU on the hidden states of the input. Formal: We use a linear transformation and the vector representations of the keys and values to estimate the attention weights Φ_k and Φ_k_p. Formal: We combine the cross-entropy between the training data and the predicted target into the objective function. Formal: We combine the cross-entropy between the training data and the predicted target into the objective function. Formal: For the next part, we check if the model can adapt to different domains by training on the target domain and then testing it on the target domain. Formal: Formal: Finally, we train the model using the target domain data and test it on the target domain data. Formal: Formal: We train the model using the target domain data and test it on the target domain data. Formal: Formal: We train the model using the target domain data and test it on the target domain data. Formal: Formal: We train the model using the target domain data and test it on the target domain data. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "ccl",
        "filename": "2020.ccl-1.106.json"
    },
    {
        "casual_text": "So, we've got two sets of vectors: l and r, each with n elements. For each pair of corresponding vectors ⃗h_il and ⃗h_ir, we combine them to get a final feature vector ⃗h_if. The way we do this is by using the formula: ⃗h_if = tanh(W_l ⃗h_il + W_r ⃗h_ir + ⃗b).",
        "formal_text": "(1: n) l, h (1: n) r, respectively. Each ⃗ h i l is combined with its corresponding ⃗ h i r for final feature vector ⃗ h i f: ⃗ h i f = tanh(W l ⃗ h i l + W r ⃗ h i r + ⃗ b)",
        "GPT2_formal_text": "= {h t } s t = {h t1, . . . , h tK }, where h t1, . . . , h tK are the input sequences in the source sequence s, which are labeled as [h t, h t1, . . . , h tK ]. The entity embedding layer takes the output of the entity embedding layer and turns it into a vector that represents the entity pair. Finally, the relation embedding layer combines all these entity embeddings to create the relation vector for the input sentence. Formal: Once we've figured out the context and entity embeddings, we plug them into the BiLSTM to figure out the relation vector for the sentence. Formal: This process takes the source text x t and the target text y t and turns them into an embedding space where each word is a vector in R d. Formal: Finally, we create a relation vector R d by doing a convolution with these two vectors. Formal: Basically, for the first step, we start with the input x t. Then, we break down the source text x t into smaller pieces. Finally, we use an entity embedding layer to get the output of the entity embedding layer. Formal: We use a BiLSTM model that's trained on the input x t to figure out the context and entity embeddings. We then use a max-pooling layer to combine these three embeddings. Formal: Finally, we apply a max-pooling layer on the triple (h t1, . . . , h tK) to get the relation vector R d. Formal: The BiLSTM model then creates the relation vector R d by using these three embeddings as inputs. Formal: The last step is a max-pooling layer on the triple (h t1, . . . , h tK) to get the relation vector R d. Formal: The final vector we get is the sentence representation x t. Formal: In this paper, we train the model using a cross-entropy loss function. Formal: But for more advanced models, we can add a simple linear layer, which could be the basic max-pooling layer or some other loss function. Formal: We've noticed that the regular max-pooling layer might not be the best choice for our model because the model doesn't have a lot of parameters, which makes it hard to handle",
        "directory": "D16",
        "filename": "D16-1070.json"
    },
    {
        "casual_text": "To check how well we can spot mistakes in captions (like extra or missing info), we picked a random bunch of data and went through it by hand to find the real extra stuff (called E C) and the actual missing bits (called O C) in each caption. We then used a method based on the average co-2 https: //github. com/tylin/coco-caption sine similarity to compare the errors our system found (a C i⊥ &g i⊥) with the actual errors. Figure 3 shows two examples of this process. \n\nIn the examples, red highlights (like \"on a table\") mark extra details that shouldn’t be in the caption, while green highlights (like \"mashed potatoes\") show things that are missing from the caption but are in the image and the reference caption. We noticed that the errors our system found are pretty similar to the real ones in both examples (with a similarity score of 0.65 or higher). This tells us that our method can effectively catch both extra and missing information in image captions.",
        "formal_text": "In order to measure the effectiveness of error identification (i. e. , extraness and omission), we randomly sampled a subset of data, and manually identify the actual extraness (i. e. , E C ) and true omission (i. e. , O C ) of each candidate caption. We conduct validation based on the average co-2 https: //github. com/tylin/coco-caption sine similarity between the machine-identified error (i. e. , a C i⊥ &g i⊥ ) and true error description. Figure 3 provides two illustrative examples of the validation process. Phrases highlighted in red (e. g. , \"on a table\") are extra information (more text in the candidate caption than in the ground truth). Meanwhile, phrases in green (e. g. , \"mashed potatoes\") are missing from the candidate description, but occur in the image and the reference caption. We observe that machine-identified errors are highly similar to the true error information in both cases (≥ 0. 65). This result suggests that our method can capture extraness and omission from an image caption.",
        "GPT2_formal_text": "_token embeddings. These are then combined to create the representation for the whole caption. Formal: Alright, let's break this down in a simpler way. First, we calculate the attention weights for each mention. Then, we calculate the attention weights for the entire caption. \n\nNow, let's talk about how we handle stuff like hyperparameter sharing, word embeddings, and the fact that we're working with images.\n\nLet's break it down with some math:\n\nWe've got three vectors, let's call them u_t, v_t, and b_t, for each mention i in the caption. These vectors are made by combining the embeddings of the mention and its context. \n\nNow, for each word w_i, we take the attention weight vector u_t and average it out across all the mentions in the caption. \n\nSimilarly, for each mention e_i, we average the attention weights v_t and b_t. \n\nFinally, we get the final attention vector for the whole caption. Formal: Basically, we're multiplying the vectors we get for each mention and the caption together, adding them together, and then averaging them all out. Formal: So, basically, we're multiplying all the attention weights for each mention and the caption together, adding them together, and then averaging them all out. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-1156.json"
    },
    {
        "casual_text": "KWPSI looks pretty similar to the traditional SI, at least when it comes to its overall appearance.",
        "formal_text": "The general appearance of KWPSI is close enough to the appearance of SI of traditional type.",
        "GPT2_formal_text": "(F) i, j=1. Formal: So, the model is trained using one way of doing things: either it predicts x, or it predicts y. Formal: We also tried a version with a hidden layer of σ size, which makes the hidden state distribution more uniform. This setup has the same problem as the original model, but it works just as well. Formal: Using the hidden layer setup, the model learns to predict y based on x. Formal: The model gets a prediction for each word in the input. Formal: We set the word embedding dimensions to 300. Formal: The model learns from the last 2-3 epochs of data. Formal: Training is done using the Adam optimizer (props to Kingma and Ba, 2014) with a learning rate of 0.002. Formal: The model is trained using the approach we just talked about, but now we're doing it with the hidden layer setup. Formal: We're using the same learning rate settings as before, but we're using a batch size of 64. Formal: We're checking the performance using the L2 loss. Formal: We're doing a total of 6 rounds of testing. Formal: We're testing the model on 4 datasets with different feature distributions. Formal: We're using the same learning rate settings as before, but we're using a batch size of 64. Formal: We're also training on the same datasets. Formal: We're testing the model on 4 datasets with different feature distributions. Formal: We're training the model on the same dataset with different feature distributions. Formal: We're testing the model on the same dataset with different feature distributions. Formal: We're testing the model on the same dataset with different feature distributions. Formal: We're testing the model on the same dataset with different feature distributions. Formal: We're training the model on the same dataset with different feature distributions. Formal: Formal: We're testing the model on the same dataset with different feature distributions. Formal: We're training the model on the same dataset with different feature distributions. Formal: We're testing the model on the same dataset with different feature distributions. Formal: Formal: We're testing the model on the same dataset with different feature distributions. Formal: We're training the model on the same dataset with different feature distributions. Formal: Formal: We're testing the model on the same dataset",
        "directory": "A83",
        "filename": "A83-1023.json"
    },
    {
        "casual_text": "First, let's talk about how the GTAHS attaches language-specific function words to a main word. This can cause some overlap between word alignments and the way sentences are structured because many of these function words are already connected to a main word within a part of the sentence (like how the English word \"the\" is part of a noun phrase, or NP). \n\nBut here's a bigger problem: the GTAHS can create fake ambiguities. We know that words can have multiple meanings when translating, like how the English word \"bank\" can mean different things and each meaning has a different Chinese word. But the GTAHS adds extra confusion that, in our opinion, could mess up Machine Translation (MT) if it tries to use these as translation rules. \n\nTake this example: the Chinese word 苹果 is linked to six different English phrases (the connected parts are underlined):",
        "formal_text": "First note that, by attaching language-particular function words to a host, the GTAHS creates redundancies between word alignments and syntactic structures since many of these function words have already been associated with a host within a constituent in the parse tree (e. g. , the English determiner the is placed inside the projection of its host, namely an NP). A more serious issue is that the GTAHS creates spurious ambiguities. Lexical ambiguity is inevitable in translation. For instance, the English noun bank has more than one lexical meaning and each of the meanings corresponds to a different Chinese word. That fact aside, the GTAHS creates spurious ambiguities, which, in our view, would be harmful to Machine Translation (MT) if extracted as translation rules. Consider the following example, where the Chinese noun 苹果 is aligned to six English strings (aligned elements are underlined):",
        "GPT2_formal_text": "ed = b - [CLS] + [CLS] + attention_masked_1 = (a - [CLS] + [CLS]) b - [CLS] + [CLS] + [CLS] + attention_masked_2 = (b - [CLS] + [CLS] + [CLS]) s - [CLS] + [CLS] + [CLS] + [CLS] + [CLS] - [CLS] + [CLS] + [CLS] + [CLS] = s - [CLS] + [CLS] + [CLS] + [CLS] + [CLS] - [CLS] + [CLS] + [CLS] + [CLS] Formal: The training data for the attention mechanism includes a pair of slot and intent labels. Formal: In a dialogue state transition, the slot intent vector is calculated based on the current intent label. It's calculated by taking the log-likelihood of the slot intent vector for slot i, written as log(i). Formal: The current intent label is represented as an integer from 0 to 1. Formal: The slot intent vector is also an integer from 0 to 1. Formal: To get the attention vector, we use the attention mechanism on the slot intent vector. Formal: We can use a simple linear transformation to map the slot intent vector to the intent label. Formal: After applying the attention mechanism, the slot intent vector gets converted into a numerical form. Formal: We can use a linear transformation to map the slot intent vector to the intent label. Formal: Formal: We can also use a linear transformation to map the intent label to the slot intent vector. Formal: Formal: Formal: Using a linear transformation, the slot intent vector can be converted back into a numerical form. Formal: Formal: Formal: Finally, the slot intent vector can be converted back into a numerical form. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C14",
        "filename": "C14-1143.json"
    },
    {
        "casual_text": "We highlight how crucial it is to calculate the CHAIR metric for both models and human-generated text. We then use this metric to do a deeper, more qualitative analysis to really get a handle on what's going on with the results.",
        "formal_text": "• We show the importance of computing the CHAIR metric on models' and humans' text, and use this metric to guide a qualitative analysis to better understand the results.",
        "GPT2_formal_text": "(g_i) = {q_i, i_a, ..., i_M}. Formal: We're suggesting a new way to represent these relationships in a structured way, using something called a graph attention network. This setup lets us handle these relationship types more flexibly and gives us better results. Formal: First, we'll quickly go over what graph attention is all about. Then, we'll give a quick overview of how to set up the graph network in this new method. Formal: To get the graph attention for a relationship r_j, we start by running some initial calculations on the graph G. Formal: For a specific pair (r_j, r_i), we calculate the graph attention for r_j using the following formula: Formal: Each node in this graph, except for the node that has the relation type r_j, gets an input like σ(r_j). Formal: The graph attention for the relation r_j is calculated using this formula: Formal: Here, σ(r_j) is the average attention score, and λ is a parameter we can adjust. Formal: Based on this, we pick the graph node with the highest attention score. Formal: To get the graph attention for the i-th relationship r_i, we do the following calculation: Formal: First, we calculate the graph attention for the node r_i using the formula: Formal: Then, the graph attention for the relation r_i is calculated using this formula: Formal: Finally, we pick the graph node with the highest attention score. Formal: Formal: The process of picking the graph node with the highest attention score is shown in Figure 2. Formal: Once we've chosen the graph node with the highest attention score, we calculate the graph attention for the relation r_i using the formula: Formal: The graph attention for the relation r_i is calculated using this formula: Formal: The graph attention for the relation r_i is calculated using this formula: Formal: Finally, we pick the graph node with the highest attention score. Formal: Formal: The process of picking the graph node with the highest attention score is shown in Figure 2. Formal: Formal: The process of choosing the graph node with the highest attention score is shown in Figure 2. Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-srw.11.json"
    },
    {
        "casual_text": "Deep reinforcement learning (RL) has been used in a bunch of cool stuff, like computer games, text-based games, and controlling robots (Mnih et al., 2015; Narasimhan et al., 2015; Kimura, 2018). But there’s a catch: these methods need a ton of practice runs to figure out the best moves, and even then, the final strategy is super hard for humans to understand. Why? Because all the smart decisions are locked away in a deep neural network, which is kind of like a black box—you know it works, but you can’t see how.\n\nThis becomes a big deal when someone wants to use RL for real-world problems and actually check if the rules make sense. If the rules were clear and easy to tweak, people could adjust them or set limits as needed. Representing the rules in a symbolic or logical way could help with both understanding and faster training, but traditional training methods struggle to learn these logical rules effectively.",
        "formal_text": "Deep reinforcement learning (RL) has been successfully applied to many applications, such as computer games, text-based games, and robot control applications (Mnih et al. , 2015; Narasimhan et al. , 2015; Kimura, 2018; . However, these methods require many training trials for converging to the optimal action policy, and the trained action policy is not understandable for human operators. This is because, although the training results are sufficient, the policy is stored in a black-box deep neural network. These issues become critical problems when the human operator wants to solve a real-world problem and verify the trained rules. If the trained rules are understandable and modifiable, the human operator can control them and design an action restriction. While using a symbolic (logical) format as representation for stored rules is suitable for achieving interpretability and quick training, it is difficult to train the logical rules with a traditional training approach.",
        "GPT2_formal_text": "ed. [8th AMTA conference in Hawaii, happening from October 21-25, 2008] Formal: To help others, we’ve put together the latest version of this dataset, which is under an MIT license. It’s part of the VLN project from Lu et al. (2009). Formal: We’ve got five text samples in this paper. One is a fresh attempt at summarizing data, another is a double-blind comparison of summaries made by different people, and the last one is an experiment where we compare summarization methods using a multi-sentence dataset. Formal: It’s super important to use all the possible ways to summarize data. This means we need some way to rank or measure these possible summaries. But even though we’re dealing with a big dataset of text, the ranking methods we’re looking at are pretty basic and don’t really take advantage of the huge amount of information in the dataset. Formal: A straightforward way to make ranking better is to use more advanced methods that can pull out information from the whole dataset. This would give us a more complete, annotated dataset. Formal: But we don’t have that kind of annotated dataset. Formal: One way to try to get around this is to train a search algorithm that can find the best match in the whole dataset. Formal: Another option is to start by picking a smaller part of the dataset to test how well it works. Formal: The VLN project (Lu et al., 2009) has a bunch of existing methods for finding matches in text. We picked the CNN model (Wang and Yang, 2009) because it’s the one that’s been the most successful so far. Formal: The CNN model was originally trained on a huge dataset called the Universal Sentence Encoder (USE) corpus. Formal: We combined the USE corpus with a part of the SpeechT5 dataset (Klein and Manning, 2007). Formal: Finally, we used the bert-base-cased embeddings from the SciBERT package (Berger et al., 2019). Formal: The results for this evaluation are in Figure 1. Formal: The CNN model scored the highest overall. It took the highest number of steps to create the initial summary for the first test. Formal: The bert-base-cased embeddings worked better than the CNN",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.283.json"
    },
    {
        "casual_text": "We looked at how different paraphrasing methods create diverse paraphrases and shared the results for two key datasets in Table 3. (We’ve put all the other datasets in Appendix B because there wasn’t enough space here.) For each method and dataset, we calculated some metrics using unlabeled sentences and their paraphrases. \n\nNow, when it comes to measuring how diverse the paraphrases are, the usual BLEU score used in Neural Machine Translation isn’t great for this purpose (Bawden et al., 2020). So, we went with the bi-gram diversity (dist-2) metric suggested by Ippolito et al. (2019). This metric counts how many unique 2-grams there are and divides that by the total number of tokens. We also checked the average similarity within each set of sentences using the Universal Sentence Encoder as a separate way to compare sentences.\n\nThe results showed that paraphrases made with back-translation were way too similar to each other, meaning high sentence similarity but low bi-gram diversity. On the flip side, DBS created more diverse sentences with less similarity. And guess what? Our masking strategies made this even better, boosting the diversity even more. The diversity we measured here was closely linked to how well the intent detection task performed on average, as shown in Table 4.",
        "formal_text": "We evaluate the diversity of paraphrases for each method, and report results for two representative datasets in Table 3 (due to space limitations, the report for all datasets is given in appendix B). For each paraphrasing method and each dataset, metrics are computed over unlabeled sentences and their paraphrases. To assess the diversity of paraphrases generated by the different methods, the popular BLEU metric in Neural Machine Translation is a poor choice (Bawden et al. , 2020). We use the bi-gram diversity (dist-2) metric as proposed by (Ippolito et al. , 2019), which computes the number of distinct 2-grams divided by the total amount of tokens. We also report the average similarity (denoted use) within each sentence set, using the Universal Sentence Encoder as an independent sentence encoder. Results show that paraphrases obtained with back-translation are too close to each other, resulting in a high sentence similarity and low bi-gram diversity. On the other hand, DBS generates more diverse sentences with a lower similarity. Our masking strategies strengthen this effect and yield even more diversity. The measured diversity strongly correlates with the average accuracy of the intent detection task (Table 4 ).",
        "GPT2_formal_text": "[j] = {p_i, a_i}  \nFor each pair (e_i, e_j) in the set {p_1, a_1}, we get a group of features {f_s, f_t} from a set {f_1, a_1} that includes the token id {s_i, t_i} for e_i. This group is what we call the \"class feature set.\" Formal: We check out how well these class features work and see how much they help or hurt our model. Formal: We use two metrics to evaluate how well these class features work: precision and recall. Formal: Precision tells us if the class feature sets the right class for a given query. We use something called \"negative-augmented F1\" (NAF) to measure this. We also look at the F1 score on the validation set to see if the class feature sets the right class. Formal: We set the recall to 0.8 for the class features, and the precision to 0.1 for the class weights. Formal: We look at three different ways to measure how good these features are. Formal: Precision, Recall, and F1 score are all calculated for each query in the same batch. Formal: We calculate these scores for both the NAF and the F1 scores for each query. Formal: We use the q_t, c_t, and f_s query parameters to calculate precision, recall, and the F1 score. Formal: Finally, we use the queries that have the best F1 scores to train our model. Formal: We look at three different ways to measure how good these features are. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.191.json"
    },
    {
        "casual_text": "We wanted to see how well BERT can predict transitive IS-A relationships, so we tested it on a bunch of cases. Imagine we have three word senses: A, B, and C, where A is a B, and B is a C. We checked how often BERT could correctly predict that A is a C, if it already got A is a B and B is a C right. We looked at 666 of these transitive triples and put the results in Table 2.\n\nIn the table, p(AB) shows the percentage of times BERT correctly predicted A is a B out of the 666 pairs. Same goes for p(BC) and p(AC). p(AC|AB, BC) tells us how often BERT got A is a C right, given that it already predicted A is a B and B is a C correctly. The table shows that when BERT says A is a B and B is a C, it’s right about A is a C 82.4% of the time.\n\nBut since it doesn’t always get A is a C right, even when it’s right about the other two, it seems like BERT isn’t always making logically consistent predictions.",
        "formal_text": "We quantify BERT's prediction ability for transitivity by measuring how often BERT makes logically consistent predictions for IS-A relations. Specifically, suppose word senses (A, B, C) form the following transitive IS-A relations: A is-a B is-a C. We measure how often BERT correctly predicts the IS-A relation (A, C) given that it correctly predicts (A, B) and (B, C). Table 2 shows the accuracy scores for the 666 transitive 3-tuples. In the table, p(AB) denotes the percentage of cor-rectly predicted (A, B) in the 666 (A, B) pairs. Similar definitions apply to p(BC) and p(AC). p(AC|AB, BC) denotes the percentage of correctly predicted (A, C), given that (A, B) and (B, C) are correctly predicted. The conditional probability in Table 2 indicates that when BERT predicts that A is-a B and B is-a C, it correctly predicts that A is-a C 82. 4% of the time. That A is-a C is not always predicted correctly (given that BERT correctly predicts A is-a B and B is-a C) suggests that BERT lacks the ability to make logically consistent predictions.",
        "GPT2_formal_text": "ed: Compute the attention-based attention distribution for the target entity. Formal: To figure out the attention-based attention, we use a Bi-LSTM to focus on the output from the attention mechanism at each time step. Formal: We also use the discriminator to measure how well the attention from the target entity is spread out. Formal: After taking the attention-based representations (h_t and h_s) from the target entity's decoder and the encoder's decoder, we average them to get the attention-based representations for the entire sentence. We keep doing this for each sentence in the test set. Formal: To get the attention-based representations, we use a Bi-LSTM to focus on the output from the attention mechanism at each time step. Formal: The attention-based representations for a sentence are calculated based on the attention weights for the entities and the type of the sentence (like an utterance or a question). We add up the attention weights to get the attention-based representation for the whole sentence. Formal: To calculate the attention weights for the entities, we combine the attention weights for each entity and type combination. Formal: The entity weights for the correct answer are calculated based on the attention weights for the correct answer entity and the correct answer type combination. Formal: For the correct answer entity, the attention weights for the correct answer entity, the correct answer type combination, and the correct answer itself are all multiplied by the entity weights. The entity weights for the correct answer, the correct answer type combination, and the correct answer itself are all added together. Formal: For the correct answer type combination, the entity weights for the correct answer type combination, the correct answer type combination, and the correct answer itself are all multiplied by the entity weights. Formal: Formal: We also update the entity weights for the entities, the correct answer type combination, and the correct answer itself to focus on the correct answer entity's output. Formal: We use a Bi-LSTM to calculate the attention-based representations for the entities. Formal: To figure out the attention weights for the correct answer entity, we combine the attention weights for the correct answer entity, the correct answer type combination, and the correct answer itself to get the attention-based representations for the whole sentence. Formal: Formal: We also update the entity weights for the correct answer entity, the correct answer type combination, and the correct answer itself to focus",
        "directory": "acl",
        "filename": "2022.acl-short.11.json"
    },
    {
        "casual_text": "This tool creates a short, clear, and useful summary of a scientific paper, covering the main points. You can either ask it to focus on a specific question or just get a general overview. Scientific papers can be tricky—they're usually long, organized in different ways, and often use different language in different parts, like the introduction versus the experiment section. To make sure we give each part the attention it deserves, we decided to make a separate summary for each section. This way, we’re summarizing smaller, more focused pieces of text, and it’s easier for you to follow along since you can see the paper’s structure. Finally, all these section summaries are put together to give you a complete summary of the whole paper.",
        "formal_text": "This module generates a concise, coherent, informative summary for a given scientific paper that covers the main content conveyed in the text. The summary can either be focused around a query, or query agnostic (a generic summary) 10. Scientific papers are complex: they are long, structured, cover various subjects and the language may be quite different between sections, e. g. , the introduction is quite different than the experiments section. To ensure our summarizer assigns sufficient attention to each of these aspects we have opted to generate a standalone summary for each section. This way we summarize a shorter, more focused text, and the users can navigate more easily as they are given the structure of the paper. Each of these section-based summaries are eventually composed together into one paper summary.",
        "GPT2_formal_text": "= {}, where each id is either [i, j], where i is a word, or {i, j, k}, where j is a word and k is just 1 or more. Formal: For each candidate sentence, we'll get the top 20 translation pairs from the candidate's translation model, which we call {ŷ 1, ŷ 2, . . . , ŷ T }. Formal: After that, we'll figure out the score for the best translation, which we'll call {ŷ 1, ŷ 2, . . . , ŷ T }. Formal: After all that, we'll use the scores from the candidates to narrow it down to a few candidates, {ŷ 1, ŷ 2, . . . , ŷ T }. Formal: The candidate that gets the highest score from the models is the one we pick as the final translation. Formal: The candidate that's ranked highest is the one we choose. Formal: You can check out the full source code for all these methods here: https://github.com/zhengyuanfeng/T5 Formal: We've also added a parameter called \"attachment_loss\" to control how much weight we give to attachment scores. Formal: Check out Figure 2 for a quick look at the attachment-based system we've built. Formal: In this paper, we're mainly focusing on how to score translations, but we're also checking out how to do word-based similarity comparisons and comparing the results to existing methods. Formal: We're building a grammar-based machine translation system, but we're not stopping there. Formal: To make sure the translation models we use are accurate, we're also using these models to build a phrase-based translation model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-3036.json"
    },
    {
        "casual_text": "We came up with a way to clean up short social media posts using paraphrasing. We collected a bunch of tweets and their cleaned-up versions from parallel data on microblogs, all using machine translation techniques. After that, we created two models to learn how to normalize these posts—one works on phrases and the other on individual characters. Finally, we made a decoder that uses both models together during the process. The fact that our method improved several machine translation systems shows it actually works.",
        "formal_text": "We introduced a data-driven approach to microblog normalization based on paraphrasing. We build a corpora of tweets and their normalizations using parallel corpora from microblogs using MT techniques. Then, we build two models that learn generalizations of the normalization process, one the phrase level and on the character level. Then, we build a decoder that combines both models during decoding. Improvements on multiple MT systems support the validity of our method.",
        "GPT2_formal_text": "ing. It was originally introduced for image classification, but it can be used for other NLP tasks too. Formal: In this paper, we're introducing a new attention mechanism that lets us control how much attention each token gets. In our experiments, we ran experiments with attention masking weights that are between 0 and 1. We also added a second linear layer, which is a standard part of traditional attention mechanisms, to handle the attention from the last hidden state. In each part of the model, we looked at how the attention is distributed for the token and the whole input sequence. We compared our results with some baseline methods and found that the attention from the last hidden state has a positive effect, which suggests that the second linear layer helps in handling the negative effects of the first layer. Formal: We trained our model using a dataset called MSCOCO, which has images with different levels of detail, like 1, 2, and 3. We also used the same dataset for testing our approach. Formal: We trained our model using a dataset called MSCOCO, which has images with different levels of detail, like 1, 2, and 3. Formal: Formal: We trained our model using MSCOCO, which has images with different levels of detail, like 1, 2, and 3. We also used the same dataset for testing our approach. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D13",
        "filename": "D13-1008.json"
    },
    {
        "casual_text": "We tell our annotators to always connect slot fillers to the EXPERIMENT mention that’s closest in the sentence. If an experiment description is spread across multiple clauses, we use the \"same exp\" relation to link the two EXPERIMENTs together. For experiments done on the same cell but with slightly different conditions, we use \"exp variation\" to link them. The \"exp variation\" link can also connect two frame-evoking elements that talk about measurements on different materials or cells but under the same conditions. These elements often suggest a comparison, like \"increase\" or \"reach from...to...\".",
        "formal_text": "We instruct our annotators to always link slot fillers to the syntactically closest EXPERIMENT mention. If the description of an experiment spans more than one clause, we link the two relevant EXPERIMENTs using the relation same exp. We use exp variation to link experiments done on the same cell, but with slightly different operating conditions. The link type exp variation can also relate two frameevoking elements that refer to two measurements performed on different materials/cells, but in the same experimental conditions. In this case, the frame-evoking elements usually convey an idea of comparison, e. g. , \"increase\" or \"reach from. . . to. \"",
        "GPT2_formal_text": "_seq = list(t_i, b_i)  \n\nThis is basically the same as (Qi, Ai) = (Qi, Ai) X. Formal: To make it clear that we're dealing with the same kind of type of distribution (like x), we add the term *v* to the set of label sequences. Formal: To get the label sequence *y*, we use the data from the last layer and the gate g. Formal: The loss function (λ) is basically the sum of all the losses from each step. Formal: Each node has a label sequence *y* that has the form q, a1, ..., aj. For each node, we have a representation *v* that has the form q, a1, ..., aj, with the corresponding label sequence *y*. Formal: We use the grid-style attention mechanism with the hierarchical attention to figure out the final label sequence *y*. Formal: This process (or model) is called a Bi-LSTM. Formal: Let's quickly go over the key parts of the Bi-LSTM model (called the latent variables). Formal: The hidden state vector for the input node *i, *j* is represented using this formula: Formal: The output vector is created by taking the attention from the outgoing edges from the last layer. Formal: In the output layer, we calculate a hidden vector for each word *w_i* and each entity *e_i* in the input sentence. Formal: Here, we consider a sequence of entities *e_i* as a whole. Formal: Finally, the main part of the Bi-LSTM model is a multi-layer perceptron that processes the input sentence and assigns weights to the entities. Formal: For each entity *e_i* in the input sentence, we calculate the hidden state vector *h_i* by taking the attention from the outgoing edges from the last layer. Formal: The entity representation *h_i* is calculated as: Formal: We're using a Bi-LSTM (or GRU) with a learned maximum-likelihood objective function to figure out the label sequence *y*. Formal: Finally, we update the output vector by combining the hidden state vector *h_i* with the weight vector *W_i* from the last layer. Formal: We can",
        "directory": "acl",
        "filename": "2020.acl-main.116.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way.\n\nOne way to train models is by using extra bits of info about the article, like who wrote it, when it was made, and where it came from. You add this info as extra tokens at the start of the article before training the model. These tokens give the model more context about the article, helping it understand how the extra info connects to the main content. After training, you can control the model by giving it the same kind of extra info that you want.\n\nThe first model that does this is called GROVER. It can make a news article based on details like the headline, author, and date. The tricky part is, GROVER can create really convincing fake news that's harder for people to spot than fake news written by humans. This could be a big problem.\n\nAnother model, called CTRL, works similarly but uses control codes found naturally in the text, like the URL of a news article. These codes help the model adjust the style, content, and even how it performs specific tasks, like answering questions or translating text. For example, it can switch between writing about sports or politics, or even between different sources like FOX Sports or CNN Sports.",
        "formal_text": "Training with control tokens: The first way is to leverage meta-information about the article such as its author, date of creation, source domain and prepend this information as additional token(s) to the input sequence, before training the TGM. These tokens act as additional context for the article, allowing the TGM to learn the relation between the meta-information and the original article. Once trained, the TGM model can be controlled by prompting with the meta-information of users' interest. The first controllable TGM proposed is the GROVER model, which can generate a news article given the meta-information of the news article (such as headline, author, and date). The GROVER model can create trustworthy fake news that is harder for humans to identify than human written fake news and can thus pose a significant threat. Similar to the GROVER model, the CTRL model provides explicit control of particular aspects of the generated text by exploiting naturally occurring control codes (e. g. , the URL for a news article) to condition the text (e. g. , news article body). These control codes govern style (e. g. , sports vs. politics, FOX sports vs. CNN sports), content (e. g. , Wikipedia vs. books), and task-specific behavior (e. g. , question answering vs. machine translation).",
        "GPT2_formal_text": "ed_vectors. This approach takes the token representation vector for a sentence and assigns a weight to its hidden representations, while leaving out the context vector. This helps the model focus on the important parts of the sentence, which is important for both understanding the words and making the semantic connections between them better. Formal: For the current setup, we're using the BERT model (Devlin et al., 2019), which is a transformer-based model that's been trained on a huge amount of text. Formal: We also tested our method on a smaller dataset called the CoNLL F1 dataset, which has around 9,000 sentences. Formal: When we train the model, it gets a specific training goal, which we call θ, which we train using the target-side token embeddings. Formal: Here's how we do it: we add up the hidden states from the token embedding layer in the model to calculate the log loss. The log loss is calculated using a stochastic gradient descent algorithm, kind of like a hill-climbing approach. Formal: We train our model with both the source and target sentences we have. If we train it on just one sentence (let's call it S), the log loss is calculated as: Formal: For the token embeddings, we use them to update the representations in the hidden layers of the model. Formal: Check out Figure 2 for an example of how we use token embeddings to update the attention. Formal: We train the model using our method, and after a warmup, we test it on the test set to see how well it can perform. Formal: For the final test, we do another round of training using the target-side tokens and the updated representations. Formal: Formal: We use the cross-entropy loss for the classification task, which we learned with the original BERT model. Formal: Lastly, we use the cross-entropy loss for the classification task, which we learned with the original BERT model. Formal: In the examples from Table 1, we show the linear and log loss functions for the classification task. Formal: Formal: We can directly apply our approach to other tasks, like text classification, sentiment analysis, and text mining. Formal: Formal: For future work, we could explore combining the token embeddings with the token embeddings in the hidden layers of the model, like in the Transformer model",
        "directory": "coling",
        "filename": "2020.coling-main.208.json"
    },
    {
        "casual_text": "Figure 4 shows how skipping chunks of Transformer layers affects task performance. The Y-axis and X-axis represent the first and last layers being skipped, and there's no additional fine-tuning. Generally, performance goes down when more layers are skipped, but skipping just one layer usually doesn't hurt much, except for the very first layer. Like the results we saw earlier, skipping some of the higher layers doesn't seem to mess things up too much.",
        "formal_text": "Figure 4: Layer Experiments: Task performance when skipping contiguous spans of Transformer layers, with the Y-axis and X-axis indicating the first and last (inclusive) skipped layers, with no further fine-tuning. Performance tends to drop as more layers are skipped, but in many cases skipping any single layer makes little to no impact to performance, except for the first layer. Consistent with results above, many of the higher layers can be skipped with minimal impact to performance.",
        "GPT2_formal_text": "and position_ids from the text are just the same as the query and article, so the output from the CRF layer gets combined with the sentence encoder (that's the square). Formal: Basically, our model uses the same parameters as LMLM and can be seen as the base model with some tweaks (check out Section 2 for more details). Formal: In this setup, we're just swapping out the input embeddings for the LSTM vectors. This setup helps with cross-modal understanding. Formal: For embedding, we're using the Spider embedding (Kipf and Welling, 2018), which was introduced by Welling et al. in 2017. Formal: We're using the word2vec tokenizer by Mikolov et al. (2013) for tokenization and stemming. We're also sticking to the same preprocessing steps as Mikolov et al. (2013) to clean up the text. Formal: We're using FastText embeddings (Mikolov et al., 2013) for embeddings. Formal: We're using a max-pooling method with a window size of 4 to process the text. Formal: Following the approach from (Zhang et al., 2015), we're preprocessing the input text by removing stopwords and fixing punctuation. Formal: We're using FastSpeech 2 (Och and Ney, 2014) to generate text. Formal: Formal: In the process outlined in Algorithm 1, we start by creating an initial representation of the sentence. Formal: Using the token representation from the embedding layer, we use a bidirectional LSTM (BiLSTM) to process the text. Formal: The text is processed based on the input token embeddings and the predicted position embeddings. Formal: We predict the next token's embedding based on the predicted position embeddings. Formal: We then turn the output from the BiLSTM into an embedding matrix, which we pass through a fully connected layer to create the text representation. Formal: Finally, the output from the fully connected layer is fed into a linear layer to get the final representation. Formal: Formal: We're testing our model on a bunch of NER tasks to see how well it performs. Formal: Formal: We're also including the performance of a baseline model that's already been trained and tested on the NER task",
        "directory": "blackboxnlp",
        "filename": "2021.blackboxnlp-1.42.json"
    },
    {
        "casual_text": "This subroutine lets you switch \"O\"-bits to \"1\"-bits and the other way around. It works kind of like this: J = LBIT(WORD, N). The result stored in J will be \"1\" if a \"0\" was changed to \"1\", and \"0\" if a \"1\" was changed to \"0\".",
        "formal_text": "This subroutine permits the conversion of \"O\"-bits into \"1\"-bits and vice versa. Its format is similar to that o--~IT, namely J = LBIT(WORD, N) ~he result stored at J will be \"1\" when \"0\" has been replaced by \"1\" and \"0\" when \"1\" has been replaced by \"0\".",
        "GPT2_formal_text": "ed(attn, [attn, attn], [mask, mask])\n\nThis setup creates two attention mechanisms, one for each attention head: one focuses on the words in the input text, and the other on the words in the output text. The function mask is basically the inverse of attention_masked. It tells you the position of the word in the text that gets the most attention. For example, in Figure 1, the word \"BLOCK\" gets the most attention in the input text. But it doesn't get the same attention in the output text.\n\nAlso, the masked word (m) gets a fixed representation and is kept as a vector, but the full word (w) gets a different vector. This difference is called a \"differentiable vector,\" which is different from the normal vector (µ) used in traditional attention systems. Formal: If we set a certain threshold λ, it helps us figure out if the attention vector is differentiable. If it is, we say it's differentiable, and if it's not, we say it's not. Formal: In this project, we're dealing with differentiable attention here. Formal: We're using a distance-based attention mechanism to balance these two vectors. Formal: We're using two differentiable vectors, which we call σ_j and σ_c. Formal: The output from the text generation module is a mix of both σ_j and σ_c, where σ_j is a vector of attention, and σ_c is a vector of control functions. Formal: Each attention head is trained to focus on a specific attention vector that doesn't change based on the context. Formal: So, the attention vector is learned to focus on the [mask] and [mask] tokens, but it doesn't change based on the context. Formal: The output from the text generation module is a mix of these two attention vectors, with σ_j being the attention vector for the [mask] and [mask] tokens, and σ_c being the attention vector for the [mask] and [mask] tokens. Formal: We're using a distance-based attention mechanism to balance these two vectors. Formal: We're using two differentiable vectors, σ_j and σ_c, to balance the attention vectors. Formal: The output from the text generation module is a mix of these two attention vectors,",
        "directory": "C67",
        "filename": "C67-1017.json"
    },
    {
        "casual_text": "We’ve talked about the LMG framework, how it works in practice, and what we know about its complexity, especially for a specific subset of cases. Take Example 2.9, for instance—it shows how LMG can neatly explain movement patterns. The complexity result in section 3.5 is mostly there to give us an idea of how tough it is to work with LMG compared to regular context-free grammars. Just to be clear, though, the result we’re discussing here only applies to non-combinatorial LMGs, so it doesn’t cover things like the grammar in Example 2.9 as it’s presented.",
        "formal_text": "We have presented the LMG formalism, examples of its application, and a complexity result for a constrained subclass of the formalism. Example 2. 9 shows that an LMG can give an elegant account of movement phenomena. The complexity result 3. 5 is primarily intended to give an indication of how the recognition problem for LMG relates to that for arbitrary context free grammars. It should be noted that the result in this paper only applies to non-combinatorial LMGs, excluding for instance the grammar of example 2. 9 as presented here.",
        "GPT2_formal_text": "= { ..., {. . . , \" \" }, ...} α = ∆ {. . . , \" \" }, N = {. . . , \" \" }, T = {. . . , \" \" }, o = {. . . , \" \" }, r = {. . . , \" \" }, a = {. . . , \" \" }, t = {. . . , \" \" }, s = {. . . , \" \" }, o_r = {. . . , \" \" }, n = {. . . , \" \" }, t_r = {. . . , \" \" }, s_r = {. . . , \" \" }, m = {. . . , \" \" }, t_m = {. . . , \" \" }, s_m = {. . . , \" \" }, a_m = {. . . , \" \" }, s_m = {. . . , \" \" }, m_r = {. . . , \" \" }, t_m_r = {. . . , \" \" }, s_m_r = {. . . , \" \" }, t_m_r = {. . . , \" \" }, m_r_r = {. . . , \" \" }, t_m_r = {. . . , \" \" }, n_r = {. . . , \" \" }, t_r = {. . . , \" \" }, s_r = {. . . , \" \" }, m_r_r = {. . . , \" \" }, n_r_r = {. . . , \" \" }, m_r_r = {. . . , \" \" }, t_r_r = {. . . , \" \" }, s_r_r = {. . . , \" \" }, m_r_r = {. . . , \" \" }, t_r_r = {. . . , \" \" }, m_r_r = {. . . , \" \" }, n_r_r = {. . . , \" \" }, s_r_r = {. . . , \" \" }, m_r_r = {. . . , \" \" }, t_r_r = {. . . , \" \" }, s_r_r = {. . . , \" \" }, t_r_r = {. . . . ,",
        "directory": "E95",
        "filename": "E95-1013.json"
    },
    {
        "casual_text": "Don’t share any personal info. Stick to your role, meaning talk like the user or the assistant.",
        "formal_text": "• Do not share any of your personal information. Always be \"in your character\", i. e. , speak as the user or the assistant.",
        "GPT2_formal_text": "_t, and b_t_mask_t. They're calculated using the same formula as the pre-trained model. Formal: For this analysis, we're using the results from the GLUE benchmark (thanks, Wang et al., 2018) to compare our method with the best model out there. Formal: We're using the standard evaluation metrics, like F1 score, standard deviation, and accuracy. Formal: We've also set the batch size to 2,000, the learning rate to 5e-5, and the temperature to 20. Formal: We're using the training split from the official GLUE site for our experiments. Formal: We're using the same hyperparameters for our experiments as in section 3.1. Formal: We're also reporting results for the Amazon Web Services (AWS) APIs. Formal: We're checking out how our method performs on two different datasets: Yelp Review-MultiRank and the Yelp-MultiRank dataset (from Liu et al., 2017b). Formal: In section 3.2, we talk about what sets our approach apart from the other methods. Formal: We'll provide the code and model for this project. Formal: We'll also share the results for the existing and new methods. Formal: We're checking out how our method performs on two datasets: Yelp Review-MultiRank and the Yelp-MultiRank dataset (from Liu et al., 2017b). Formal: We'll provide the code and model for this project. Formal: We'll also share the results for the existing and new methods. Formal: We'll also share the results for the existing and new methods. Formal: Formal: We'll also share the results for the existing and new methods. Formal: We'll also share the results for the existing and new methods. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "coling",
        "filename": "2020.coling-main.463.json"
    },
    {
        "casual_text": "But maybe the coolest thing about Europe right now is how many companies are popping up that focus on software localization. These guys are getting really good at using tools and machine translation systems, like Logos, Metal, and XL8. To help everyone share ideas and set some standards, the Localisation Industry Standards Association was created back in 1990. They even put out a newsletter called the LISA Forum and made a CD-Rom with all kinds of info about products, standards, and methods called the LISA Showcase. Ireland is a big player in this space—since 1994, they’ve had their own Software Localisation Group, which hosts conferences and workshops. They’ve also just launched a Localisation Resources Center with help from the Irish government and the EU.",
        "formal_text": "However, perhaps the most distinctive feature of the European scene is the growth of companies providing software localisation. These services are acquiring considerable experience in the use of translation aids and MT systems (e. g. Logos, Metal and XL8). As a forum for the interchange of experience and the establishment of standards the Localisation Industry Standards Association was set up in 1990; the association publishes a newsletter (LISA Forum) and has produces a CD-Rom directory of products, standards and methods (LISA Showcase). A major centre for localisation is Ireland, which since 1994 has its own Software Localisation Group, holding conferences and workshops and recently setting up a Localisation Resources Center (with support from the Irish government and EU. )",
        "GPT2_formal_text": "= {entity_id, entity_type} from the query q. Formal: We set the probability of each token (let's call it p) to equal the sum of its embedding probabilities, p_i = q_i_i_1, ..., p_i_i_n. Formal: Using context vectors to enhance multi-modal chatbots, we turn each chat into a sequence. First, we grab the chat history (like a set of messages) from the image. Then, we create two types of context embeddings for each token: the first type focuses on the image's context, and the second type uses the context from the last message. We feed these context vectors into a GRU-based model that processes them in a sequence. This helps create a personalized response for each chat. Formal: We tweak the model to match the input by setting it up to generate an embedding for each token in the dialog. To do this, we use a conditional random field (CRF) on the generated embeddings, with a fixed feature size δ. Formal: Here, δ is a non-linear function that we tweak during training. We set δ to 0.2 and train the model with the cross-entropy loss function ln(δ). Formal: We take the embedding vectors p_i from the last message and apply the model's CRF to create a new embedding for each token in the dialog. Finally, we use this new embedding to generate a new response for the current token in the dialog. Formal: Lastly, we use the new contextual embeddings p_i to generate the response for each token in the dialog. Formal: We create embeddings for each token by using the CRF on the generated embeddings p_i from the last message. Formal: We use the generated embeddings to create the responses for each token in the dialog. Formal: After training, we apply a simple linear transformation to the outputs of the model to get the final input embeddings for the current token in the dialog. Formal: We feed the generated embeddings p_i from the last message into a GRU-based model that processes them in a sequence. This helps create a personalized response for each dialogue. Formal: We tweak the model to match the input by setting it up to generate an embedding for each token in the dialog. Formal:",
        "directory": "amta",
        "filename": "1996.amta-1.20.json"
    },
    {
        "casual_text": "Since the term \"syntax\" alone isn't specific enough, they added a part that really limits how the parser builds structures. This part, which is based on Lexical Phonology, sets rules for the order in which affixes and stems can be combined. As a result, it limits what kind of stems an affix or another stem can connect to, or how complex that stem can be. This helps avoid mistakes like assigning the wrong word class or incorrectly splitting up words, and can even lead to rejecting structures that don’t work.",
        "formal_text": "Because the word syntax as such is not restrictive enough, it was supplemented with a component which heavily restrains the parser in building structures. This component, which is inspired by Lexical Phonology, imposes an ordering on the attachment of affixes and stems. Consequently, it restricts the type or the complexity of the stem that an affix or other stem may attach to. Rejection of structures can result in avoiding incorrect word class assignment and rejection of incorrect segmentations.",
        "GPT2_formal_text": ".\n\nIn our experiments, we're looking at how the mask factor affects the masked entity prediction accuracy. The goal is to see if the updated embedding model can capture the contextual knowledge needed to correctly identify the masked entity. To do this, we tweak the input representations for the masked entity prediction by using the attention mechanism we talked about earlier. This method ensures that the new embedding model keeps the knowledge from the original model, and it's super efficient.\n\nFor the zero-shot scenario, we focus on embedding the entities using a CNN with 100 dimensions. After that, we throw in a linear layer to predict the mask. In this setup, the embedding vector of the entity ends up being a bunch of random, non-numerical values. Formal: Instead of using a CNN with 100 dimensions to predict the mask, we use a linear layer to predict the mask. Formal: We keep the embedding vector of the entity as a static, non-numerical value.\n\nWe tested this setup on two datasets: Europarl (EP) and the TEST dataset (TEST). The results are in Table 2. As you can see, with this updated embedding model, the model performs way better than the original one. Formal: We ran the cross-entropy loss on the TAT-2009 dataset to see how the updated embedding model would perform. The results are in Table 2, showing the F1 score for both the original and our updated embedding model. Formal: The cross-entropy loss is calculated using the equation: Formal: We used the full dataset for evaluation, which has the original embedding model, and the updated embedding model. Formal: We ran a regression model using the full dataset for evaluation. Formal: We used the original embedding model and the updated embedding model, along with the TAT-2009 dataset, to evaluate the model. Formal: Our model uses a linear transformation to predict the mask. Formal: For the zero-shot scenario, we used the test dataset to train the model and then tested it on the full dataset. Formal: We used the full dataset to train the model and then tested it on the full dataset. Formal: We used the full dataset to train the model and then tested it on the full dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E93",
        "filename": "E93-1023.json"
    },
    {
        "casual_text": "To create the candidate generation function, we start by aligning each dataset using GIZA++. Then, we build a bilingual lexicon with probabilities based on maximum likelihood (P mle ) from the aligned data. After cleaning up the lexicon by removing less frequent and less significant entries, we keep the top 200 translations for each word in the source language, ranked by P mle (t|s) • P mle (s|t). These translations are considered the candidate word pairs. \n\nWe also use these word alignments to train the BNN models. Each alignment link is treated as a training example, and we don’t do anything special for words that don’t align or for cases where one word aligns with multiple words.",
        "formal_text": "To prepare the candidate generation function, each dataset is first word-aligned with GIZA++, then a bilingual lexicon with maximum-likelihood probabilities (P mle ) is built from the symmetrized alignment. After some frequency and significance pruning, 7 the top 200 translations sorted by P mle (t|s) • P mle (s|t) are kept as candidate word translations for each source word in the vocabulary. Word alignments are also used to train the BNN models: each alignment link constitutes a training sample, with no special treatment of unaligned words and 1-to-many alignments.",
        "GPT2_formal_text": "= {..., ..., {1, ..., n} } (where {1, ..., n} are the input features). Formal: To make sure the model doesn't make mistakes by mistakenly treating features like a high-level feature, we use a dynamic threshold α. This threshold is based on a validation loss to check how well the model can predict the target class labels. Formal: The algorithm can be written as... Formal: Basically, this algorithm splits the training set into K smaller parts, each with a size of k. Formal: For each K-part, it looks at all the features from the input data that are related to that part and decides if they should be included or not. Formal: We're mainly using the beam search method from (Wang et al., 2019b), which has been proven to work well for supervised learning in NER (Kim, 2014). Formal: The main goal is to maximize the log-likelihood, which is basically the log-likelihood ratio. Formal: The first part of the algorithm is trying to maximize the log-likelihood of each part, which is like trying to maximize the chance that a feature in a part will be correct. Formal: In our case, this probability is calculated using a positive log-likelihood, and it's based on the training data. Formal: The goal of this part of the algorithm is to maximize the log-likelihood of the entire training set. Formal: We only keep the parts of the training set where the log-likelihood of the whole training set is greater than this threshold α. Formal: We use this threshold α to decide if a feature in a part is included or not. Formal: Formal: Formal: For a given input, we make a bunch of guesses using different parameters, and the model then decides if a feature should be included or not. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1175.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\n1. **τ, α, g, σ |= X: Y: P** means that if τ (α(Y )) is in σ(P ) and α(X) * ∧ α(Y ), then it's true.\n\n2. **τ, α, g, σ |= ARG S (X, a)** means there's an i in S where α(X) • i is in D(τ ) and τ (α(X) • i) equals g(a).\n\n3. **τ, α, g, σ |= ARG S (X, Y )** means there's an i in S where α(X) • i is in D(τ ) and α(X) • i equals α(Y ).\n\n4. **τ, α, g, σ |= X * Y** means α(X) * α(Y ) is true.\n\n5. **τ, α, g, σ |= X =/ = Y** means α(X) is not equal to α(Y ).\n\n6. **τ, α, g, σ |= v 1 =/ = v 2** means g(v 1 ) is not equal to g(v 2 ).\n\n7. **τ, α, g, σ |= P Q** means σ(P ) is a subset of σ(Q).\n\nIn simpler terms, these are rules that explain how different things relate to each other based on certain conditions.",
        "formal_text": "τ, α, g, σ |= X: Y: P iff τ (α(Y )) ∈ σ(P ) and α(X) * ∧ α(Y ) τ, α, g, σ |= ARG S (X, a) iff exists i ∈ S s. t. α(X) • i ∈ D(τ ) and τ (α(X) • i) = g(a) τ, α, g, σ |= ARG S (X, Y ) iff exists i ∈ S s. t. α(X) • i ∈ D(τ ), α(X) • i = α(Y ) τ, α, g, σ |= X * Y iff α(X) * α(Y ) τ, α, g, σ |= X =/ = Y iff α(X) =/ = α(Y ) τ, α, g, σ |= v 1 =/ = v 2 iff g(v 1 ) =/ = g(v 2 ) τ, α, g, σ |= P Q iff σ(P ) ⊆ σ(Q)",
        "GPT2_formal_text": ". Formal: The input of the model is a sequence of words w_i. At each spot t, the model grabs the word from the input sequence that matches up with the word from the target sequence. Formal: The words in the target sequence are all the words that come after the one we're looking at. Formal: The weights for the position predictor are calculated based on the predictions from the sentence generator's attention heads. Formal: We use a beam size of 5 to create a context vector and calculate the attention weights. Formal: For training the model, we use a linear projection of the target sequence. Formal: If the target sequence is longer than n, we set the number of target words to n. Formal: We use the Adam optimizer (from Kingma and Ba, 2014) to train the model. Formal: For training, we use this sequence-level attention and set the hidden layer dimension to 200. Formal: Formal: We take the best model we trained using stochastic gradient descent (SGD) and update it with the target sequence. Formal: We take the model from the teacher (S) and tweak it using the target sequence (T). Formal: Formal: We then use this updated model to generate a single output using different sequence lengths (n). Formal: Formal: We repeat this process for all sentences in the target sequence and also for the generated sentences in the teacher. Formal: We train the model with different random seeds and set the output dimension to 200. Formal: Formal: We also use the student model (S) to generate a single output using different random seeds and set the output dimension to 200. Formal: Formal: The student model is trained using the target sequence (T). Formal: Formal: We use the student model to generate a single output with different random seeds and set the output dimension to 200. Formal: Formal: The output is a sequence of characters. Formal: Formal: The output is a sequence of characters. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E09",
        "filename": "E09-1052.json"
    },
    {
        "casual_text": "So, after generating possible words, each one gets checked in a dictionary to see if it actually exists. If it does, it might be a correct spelling. But this method has a couple of issues. First, it only works for certain types of spelling checks based on \"edit distance.\" Second, it can take a lot of time if the edit distance is more than 2 and there are a lot of possible characters, like in Unicode for many Asian languages.\n\nHulden (2009) came up with a Finite-State-Automata (FSA) algorithm to speed things up by finding similar words quickly, even if they're misspelled. Other people have used FSA too, but these methods are still just approximations for finding the closest match.\n\nMore recently, de Amorim and Zampieri (2013) suggested another way to cut down on the number of checks needed by using something called \"anomalous pattern initialization\" and \"partition around medoids.\"",
        "formal_text": "Then each of such generated words is checked in the dictionary for existence, and if it is found in the dictionary, it becomes a potentially correct spelling. There are two shortcomings of this approach. First, such search space reduction works only for edit distance-based measures. Second, this approach too leads to high time complexity when the edit distance threshold is greater than 2 and the possible characters are large. Large character set is real for Unicode characters used in may Asian languages. Hulden (2009) proposes a Finite-State-Automata (FSA) algorithm for fast approximate string matching to find similarity between a dictionary word and a misspelled word. There have been other approaches as well using FSA, but such FSA-based approaches are approximate methods for finding closest matching word to a misspelled word. Another more recent approach to reduce the average number of distance computations is based on anomalous pattern initialization and partition around medoids (de Amorim and Zampieri, 2013).",
        "GPT2_formal_text": ") = (int)(DIST(w_h)), (int)(DIST(w_e)) = (int)(DIST(w_f)), (int)(DIST(w_b)) = (int)(DIST(w_a)), (int)(DIST(w_e)) = (int)(DIST(w_f)), and (int)(DIST(w_b)) = (int)(DIST(w_a))) Formal: This is a bit more complicated, but basically, if you have a pair of input sentences (w_i, w_j) and a keyphrase (h_k, h_l), the probability of getting h_k (i) when you know h_l (j) is equal to the probability of h_k (i) given h_l (j). Formal: Here, (1 − h_k) is the difference between the two, and σ(i, j) is the sum of σ(i) for each i from 1 to n. Formal: Since w_i and w_j are in the same sentence, we can assume that σ(i, j) is the same for both. Formal: From this, we can estimate the value of σ(i, j) using the distribution of the predicate variables in the DST. Formal: The formula σ(i, j) = (1 − σ(i)) + σ(j) basically shows how much the value of σ(i, j) depends on the probability of σ(j) when we know σ(i) and σ(j). Formal: This is like saying σ(i, j) is the same as the probability of σ(j) when we know σ(i) and σ(j). Formal: Basically, σ(i, j) is the same as the probability of σ(j) when we know σ(i) and σ(j). Formal: We use a conditional random field (CRF) (from Chiang, 2001) to handle these calculations. Formal: Here, w_i and w_j are the words in the input sentence, and σ(w_i) is the difference between σ(w_j) and σ(w_i). Formal: The output can be either a list",
        "directory": "E17",
        "filename": "E17-2027.json"
    },
    {
        "casual_text": "When we train models on just one dataset and then test them on other datasets, they don't perform as well, which isn't surprising. But, our method does way better at handling different datasets compared to the standard approach by Bugert et al. (2020a), even when we only train on one dataset.",
        "formal_text": "For models trained on the train split from a single corpus, we see significant performance loss when evaluated on test splits from other corpora as is expected. However, we see vastly improved generalizability with our approach when trained on a single corpus compared to the baseline set by Bugert et al. (2020a).",
        "GPT2_formal_text": "ed, and the rest are the response embeddings. Formal: We're doing two main things here. First, we're gathering sentences with entity pairs, which we call {e1, e2} from the sentence with entities e1 and e2. Then, we're also using the same number of training sentences to train our model. Formal: Another approach is using the same model but with a different mix of entity pairs, {e1, e2} from the sentence with entities e1 and e2. Formal: The first approach is to use the same model as before, but this time with a different mix of entity pairs, {e1, e2} from the sentence with entities e1 and e2. Formal: The second approach is to use the same model but with a different mix of entity pairs, {e1, e2} from the sentence with entities e1 and e2. Formal: Lastly, you can train a single model to handle all three scenarios. Formal: Another approach is to use the same model but with a different mix of entity pairs, {e1, e2} from the sentence with entities e1 and e2. Formal: In the third scenario, you train a single model to handle all three scenarios. Formal: One more thing: Formal: Basically, we're optimizing two things here: Formal: To get the best parameters, we pick the λ value based on how the model performs during training, especially when dealing with a really big dataset. Formal: Then, we do the same thing as in the second scenario, but this time we pick the λ value based on how well the model performs during training, especially when dealing with a really small dataset. Formal: This is all laid out in Equations 5 and 6. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.106.json"
    },
    {
        "casual_text": "- METEOR is a way to measure how well your translation matches the reference, kind of like a mix of precision and recall, but it puts more emphasis on recall. (Banerjee and Lavie, 2005)\n- TER (Translation Edit Rate) is like a score that tells you how many changes you’d need to make to your translation to match the reference sentence exactly. (Snover et al., 2006)\n- TERP is an upgraded version of TER. It’s smarter because it can do things like swap out phrases, simplify words, and even check for synonyms to make the score more accurate. (Snover et al., 2009)\n- TERPA is a specific version of TERP that’s been tweaked to work really well for measuring how good a translation is, especially when it comes to making sure the meaning is clear.",
        "formal_text": "• METEOR, which measures the harmonic mean of unigram precision and recall, with a higher weight for recall (Banerjee and Lavie, 2005) • TER (Translation Edit Rate), a measure of the number of edits required to transform a hypothesis sentence into the reference sentence (Snover et al. , 2006) • TERP, an augmented version of TER which performs phrasal substitutions, stemming, and checks for synonyms, among other improvements (Snover et al. , 2009) • TERPA, an instantiation of TERP with edit weights optimized for correlation with adequacy in MT evaluations",
        "GPT2_formal_text": ", and the length of the output string to match the input string. Formal: To make sure we're comparing things fairly, we're using the usual methods for comparing data sets. We're using the ROUGE metric (Lin, 2004) to check how well everything matches up. Formal: We calculate the average ROUGE score for all the samples we test. This helps us figure out the best hyperparameter settings for the cross-entropy loss. Formal: We set the hyperparameter λ to 0.1, which is the highest value of this hyperparameter that gave us the best ROUGE score on the validation set. Formal: We use the standard ROUGE-2 evaluation method to evaluate all the samples. Formal: To see how different loss functions affect things, we run the same experiments again with different loss functions, but with the same input data and the same training data. In each case, we check the ROUGE score using the WMT2016 dataset, but this time with the cross-entropy loss instead of the standard cross-entropy loss. Formal: We repeat the evaluation process for all the samples in the test set, but this time, we use the cross-entropy loss instead of the standard cross-entropy loss. Formal: Finally, we pick the model that performs best on the validation set for each language pair. Formal: We calculate the average ROUGE score for all the samples we test. This helps us figure out the best hyperparameter settings for the cross-entropy loss. Formal: To find the best hyperparameters for the cross-entropy loss, we run the same evaluation on the WMT2016 dataset. Formal: We use the standard ROUGE-2 evaluation method to evaluate all the samples. Formal: We run the same evaluation for all the samples in the test set, but this time, we use the cross-entropy loss instead of the standard cross-entropy loss. Formal: We pick the model that performs best on the validation set for each language pair. Formal: We calculate the average ROUGE score for all the samples we test. This helps us figure out the best hyperparameters for the cross-entropy loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D10",
        "filename": "D10-1055.json"
    },
    {
        "casual_text": "Once the text-image matching model is trained and the sentences are simplified, we look at each text-image pair (T i, I j ) in our task. If the score s(T i, I j ) is higher than a certain threshold T match, we consider them a match. We picked the threshold as the average matching score for positive text-image pairs in Flickr30K. But, in theory, tweaking this threshold could make the matching better for our specific task.",
        "formal_text": "After the text-image matching model is trained and the sentences are simplified, for each textimage pair (T i, I j ) in our task, we can identify the matched pairs if the score s(T i, I j ) is greater than a threshold T match. We set the threshold as the average matching score for the positive text-image pair in Flickr30K, although the matching performance for our task could in principle be improved by adjusting this parameter.",
        "GPT2_formal_text": "ed, and the attention matrix is in R n ×d. Formal: If the current word has a relation r, and the next word has a relation r' that's not an n-gram, then the previous word's attention matrix is basically the same as the last word's. Formal: The SVM classifier (from Hinton et al., 2015) is trained using just one pre-trained model, but it's better than the naive Bayes classifier, which only uses one training instance for each language. Formal: In this case, the student model is the embedding layer for the embedding layer in the teacher model. Formal: This is a typical situation where the student model doesn't have a fixed vocabulary, so the embedding layer has to learn from all the text that the teacher has already translated. Formal: We just use the embedding layer from the teacher model as the output for the student. Formal: The current word gets its representations (or embeddings) from a pre-trained word embedding model, which we call G w. The student's embedding layer is trained using the same model as the student, G w. Formal: The input embedding embeddings for a word are calculated like this: Formal: For each word w in the vocabulary V w, Formal: Formal: For the word w in the vocabulary V w, Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1114.json"
    },
    {
        "casual_text": "• The question pattern prediction score, which is the score given by either the retrieval-based method or the generation-based method;",
        "formal_text": "• question pattern prediction score, which is the prediction score by either retrieval-based approach or generation-based approach;",
        "GPT2_formal_text": "= [ {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', }, {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', }, {'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', } ] } Formal: The way we create sentences is a bit different from the way people usually use tools like Moses, GIZA++, or PTB. Formal: We’ve got two basic tools for dealing with sentence structure: Formal: In this part, we’ll explain our version of Moses’s sentence compressor, which is the part that pulls out info from the input. Formal: Here’s the info you’ll get from a sentence with just N words. Formal: We’ve come up with a straightforward way to pull out features for a sentence, but there’s no clear way to combine them. Formal: We can use Moses’s compressor to get the feature set for the whole sentence. Formal: Formal: We’ve got a basic compressor that works with a simple set of features, which we’ll call \"s\". Formal: Formal: Formal: Formal: This simple compressor will get rid of any sentences where the first word is either the beginning or end of the sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1090.json"
    },
    {
        "casual_text": "To save time and avoid asking people the same question over and over, we assume that if we get an answer for a specific part of a sentence and one of its possible translations, we can use that same answer for all other possible translations of that part. So, for each sentence, we create a little database with entries like <source part, target part, judgment>. The judgment can be YES, NO, or NOT SURE. This way, we don’t have to keep asking about the same source part with different target options.",
        "formal_text": "The first assumption we make to minimize the number of human judgments, is that once we have a judgment for a source-candidate substring pair, that same judgment can be used across all candidates for this source sentence. In other words, we build a database for each source sentence, which consists of <source substring, target substring, judgment> entries. For a given source substring, multiple entries exist, each with a different target candidate substring. The judgment field is one of YES, NO, and NOT SURE.",
        "GPT2_formal_text": "ed: This is the hidden layer where the attention mechanism is applied.\n\nTable 1: The attention weights for the cosine similarity between an entity mention and its related entity mention, using the formula for the average cosine similarity between mentions, where C(i, j) is the cosine similarity between the mention and the entity mention. The score for a positive entity mention is 0 if it matches the mention and 1 if it doesn't. Formal: For instance, in Figure 1, the entity mention \"New York Times\" is a key part of the mention \"Wall Street Journal.\" To make it easier to find the entity mention, we use a method called label attention. So, if we have a mention like \"New York Times,\" the attention for the mention \"New York Times\" is calculated like this: Formal: We figure out the attention weights for the entities mentioned in the mention span using the formula from Equation 7. Formal: We use a lookup table for the entity mention to figure out the attention weights. Formal: We run the attention weight calculations on the whole mention span using a special lookup table. Formal: Here, |C| is the cosine similarity between the mention and the entity mention, and 0 means the mention is positive. Formal: To find the vector for the entity mention, we use a naïve Bayes classifier that we build using a BiLSTM on the mention. Formal: We take the average of the attention weights for the entity mention in the span, but we don't bother with the non-entity mentions because they don't contribute much to the final representation. Formal: The final representation for the entity mention is the vector for its mention. Formal: We use a vector-based attention mechanism that gives different weights to different nodes in the vector representation, and we adjust the weights using a simple linear transformation. Formal: We use a non-linear function called negative sampling to make sure the vector representation we get matches up well with the entity mention embedding representation. Formal: We look for the nearest neighbors of the mention mention in the embedding space and use them to figure out the vector for the mention. Formal: To learn the vector for the entity mention, we use a Conditional Random Field (CRF) with a negative sampling method. Formal: The CRF function is learned using a multi-layer perceptron (MLP) on the embedding space. Formal: The vector for the entity mention is a",
        "directory": "D09",
        "filename": "D09-1006.json"
    },
    {
        "casual_text": "The next word distribution might have multiple peaks. But using a bunch of softmaxes makes things way more complicated because we have to calculate the dot product between each facet and every word in our vocabulary. Based on our analysis, we thought of a way to simplify this: we can divide all the words in the vocabulary into different groups and use different facets for each group. For instance, if we put words like \"queen,\" \"man,\" \"woman,\" and \"king\" into one group and the rest of the words into another, we won't end up with weird combinations like \"queenking\" being the same as \"womanman\" in either group. With this approach, each word is only in one group, so we only need to calculate one dot product per word. The only extra work we have to do is setting up the extra linear projections for the facets.",
        "formal_text": "The next word distribution could have many modes. However, using many softmaxes significantly increases our computational burden because we need to compute the dot product between each facet and all the word embeddings in our vocabulary. Inspired by our analysis, we propose to split all the words in the vocabulary into multiple partitions 4 and use different facets for different partitions. For example, if we can put any word from {queen, man, woman, king} into one partition and the rest of the words into another partition, we no longer have queenking = womanman in either of the partitions. In this method, each word only belongs to one partition, so we only need to compute one dot product for each word. Thus, the extra computational cost only comes from the extra linear projections for preparing the facets.",
        "GPT2_formal_text": ". Formal: For each word w i, we figure out the actual text that matches it. Formal: For the target language X, we figure out the target word that matches it. Formal: We’re using an 8-layer BERT (base) model, which was pre-trained on text data (Devlin et al., 2018). The model is trained on a mix of sources like Amazon reviews and Wikipedia articles. Formal: We compare our work to the latest methods. Specifically, we look at the proposed method (Xi et al., 2019) and the ones that use a language model (like Yu et al., 2019; Miwa and Bansal, 2020; and Miwa and Bansal, 2021) to see how much better they are at translating from English to the target language. Formal: The main goal of this paper is to help improve translation quality by using NMT models and better understanding the target language. We also want to push the idea that using multiple NMT models together can lead to better translation quality. Formal: We tested our model against three top-notch methods: the BERT-based method (Xi et al., 2019), the Transformer-based method (Yu et al., 2019), and the new method we came up with. Formal: To see how well our model performs, we used three popular NMT models: BERT (base), GPT-2 (gpt2), and T5 (t5). Formal: Our model got a score of 0.971 in the evaluation task. Formal: For the evaluation task, we look at how well our model does in both the target and source languages. Formal: For each pair (X, Y), we check how often they show up together. Formal: We also add up the counts for all the words in the sentences. Formal: Formal: The most common word is \"home,\" and the least common word is \"house.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.554.json"
    },
    {
        "casual_text": "The University Bookstore stays open until 4:30 PM. I'm not sure if they sell floppy disks, though. But they do have a bunch of other stuff that's useful for school, so it might be worth checking out.",
        "formal_text": "\"The University Bookstore is open until 4: 30 PM. But I don't know whether it sells floppy disks. However it does sell many other items of an educational nature, so it is perhaps a good place to try. \"",
        "GPT2_formal_text": "kernel, which is calculated using the input representation, as shown in Equation 5. Formal: So, to sum up the matrix A from D: Formal: This way of calculating attention is super efficient. It can be done in parallel, so we don't have to worry about the time it takes to process all those connections. Formal: Since the hidden state h D t for the relation r is in R t, we can figure out the attention matrix A t by taking the output from that r-th layer. Formal: Using this formula, we can get the attention matrix A t by doing a softmax (∆t) operation on A t. Formal: So, we end up with this big matrix A t = {A t 1, . . . , A t m } t=1, . . . , t=m, where each element in A t is a vector representing a specific relation. Formal: In our experiments, we tried out different attention methods and found that SVM (a popular choice) worked the best. Formal: After that, we calculated the attention matrix A t to get A t = {A t 1, . . . , A t m } t=1, . . . , t=m, which we then fed into a classifier. Formal: The results were super good—our model not only outperformed the others, but it also beat the top models by a big margin. Formal: For future work, we plan to add hierarchical attention to handle hierarchical relationships. Formal: In the future, we plan to explore a few more types of relationships and how they work together. Formal: In the next section, we'll show that our model is also really good at handling structured data. Formal: Here's how it works: First, we create an n-dimensional vector for each relation r, but we only keep the most important ones. We also add some special attention weights to the vectors to focus on the important parts. Formal: The first part of the process is pretty straightforward: we first make a vector for each relation r by using the output from the r-th layer. Then, we use the relation vectors we made to calculate the attention weights, which we call A t = {A t 1, . . . , A t m } t=1, . . . , t=m. Formal: After that, we adjust the attention weights based on the relation vectors we made and get",
        "directory": "C86",
        "filename": "C86-1006.json"
    },
    {
        "casual_text": "Step 4: Check out the telops in TV images. Figure 5 has a bunch of different info that telops show during TV news (thanks to Watanabe in '96 for pointing that out). Watanabe also came up with a way to analyze the meaning behind these telops, and it worked pretty well—92% accurate. We’re using that same method to figure out what each telop is actually saying.",
        "formal_text": "Step 4 Analyze telops in TV images. Figure 5 shows several kinds of information which are explained by telops in TV Newscasts (Watanabe 96). In (Watanabe 96), a method of semantic analysis of telops was proposed and the correct recognition of the method was 92 %. We use this method and obtain the semantic interpretation of each telop.",
        "GPT2_formal_text": "= { ] } model_ids = { } model_types = { } model_parse = { } model_stat = { } model_train = { } model_test = { } results = { } model_time = { } model_rerun = { } model_model = { } model_topic = { } model_review = { } model_image = { } model_summary = { } model_video = { } model_image = { } model_score = { } model_log = { } model_det = { } model_mask = { } model_label = { } model_log = { } model_label = { } model_score = { } model_det = { } model_mask = { } model_label = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_det = { } model_mask = { } model_score = { } model_score = { }\n\nTable 1: Our model representations and evaluation results",
        "directory": "C98",
        "filename": "C98-2220.json"
    },
    {
        "casual_text": "Right now, we're using a basic beam search to find a sequence of parts that covers the entire input. These sequences can have gaps, meaning they don't have to be connected, but they can't overlap. The algorithm likes shorter sequences. To figure out the length of a sequence, we add up the lengths of the parts in it, but we give some parts more weight than others.\n\nWe tried two ways to assign weights and lengths to these parts. In the first method, each part gets a length of 1, but we multiply that by a factor based on how \"good\" the part is. Sequences can be extended by adding gaps that cover one input token at a time, and these gaps get a weight of 3. Parts created by rules that have <OUTPUT> on the left side are considered the best and get a weight of 1. Other parts can also be added to the sequence and get a weight of 1.5.\n\nFor example, if we have a sequence with an <OUTPUT> part covering tokens 1 to 3, a gap covering tokens 4 and 5, and a <Vl> part covering tokens 6 to 10, the total length would be 1 (for the <OUTPUT> part) + 6 (for the gap) + 1.5 (for the <Vl> part) = 8.5. This algorithm really prefers sequences that are connected and calculates lengths based on the number of parts in the sequence, not their actual length.",
        "formal_text": "Currently, a simple best-first beam search through the chart is used to find a sequence (path) of constituents spanning the whole input. Paths are allowed to have gaps, i. e. , they do not have to be contiguous, although we do not allow for overlapping constituents. The algorithm prefers shorter paths. The length of a path is computed as a weighted sum of the lengths of constituents in the path. We experimented with two different ways of assigning weights and lengths to constituents. In the first method, each constituent was assigned the length of 1 that was weighted by a factor depending of the \"quality\" of the constituent. Paths can be extended by a gap spanning one input token at a time. Such a gap is weighted with the factor of 3. Constituents that are created by rules with the nonterminal <OUTPUT> on their LHS are assumed to be of the highest quality and they are weighted with the factor of 1. All remaining constituents can also be added to the path and are weighted with the factor of 1. 5. So a path consisting of an <OUTPUT> constituent spanning input tokens 1 to 3, a gap spanning input tokens 4 and 5, and a <Vl> constituent spanning input tokens 6 to 10 would receive the length of 1 + 6 + 1. 5 = 8. 5. This algorithm shows a strong preference for contiguons paths and assigns lengths depending on the number of constituents in the path, ignoring their length.",
        "GPT2_formal_text": ", attention_ratio, and attention_weight. Formal: This is basically the formula we just talked about, where α and β are constants we set to 0.2 and 0.5, respectively. Formal: We've got a matrix A_i that's the size of N, and we can write it as A_i = |B|^T. We also have a vector v_i that's the size of R^T. Formal: To understand what this all means, we can look at it in a simpler way. Formal: The negative logarithm of the logarithm of the product between the input and output vectors is calculated using this formula. Formal: For each input token, we calculate the logarithm using this formula: Formal: We calculate the negative logarithm for each token using this formula. Formal: We calculate the attention between the input and output vectors. Formal: We calculate the attention between the input and output vectors. Formal: To get a token representation h_i, we use this formula: Formal: The logarithm for the product between the input and output vectors is calculated using this formula: Formal: To get the attention vector h_i, we use this formula: Formal: We calculate the attention vector for the token. Formal: The logarithm for the product between the input and output vectors is calculated using this formula: Formal: To get the attention vector for the token, we use this formula: Formal: Formal: To get the attention vector for the token, we use this formula: Formal: Formal: Formal: To get the attention vector for the token, we use this formula: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "A00",
        "filename": "A00-3002.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. First, we start by setting 'a' to 0.5. Then, we tweak the topic number K, changing it from 2 to 20, but we do it in steps of 2. If you check out Figure 7 (1) and Figure 7 (2), you'll see that the ROUGE score hits its highest point when K is around 12. \n\nNext, we lock K at 12 and adjust the value of 'a' from 0 to 1, stepping it up by 0.1 each time. When 'a' is set to 0, the model basically becomes a one-layer graph ranking thing, ignoring any topic clustering info. Looking at Figure 7 (3) and Figure 7 (4), the ROUGE scores peak around 0.6 and then start to drop after that. \n\nSo, for our test dataset, we go with K set to 12 and 'a' set to 0.6.",
        "formal_text": "First, a is set to 0. 5, then we change topic number K from 2 to 20 at the interval of 2. The ROUGE score reaches their peaks when the topic number is around 12, as shown in Figure 7 (1) and Figure 7 (2). Then we fix the number of K to 12 and change the value of parameter a from 0 to 1 with the interval of 0. 1. When the value of a is set to 0, the model degenerates into a one-layer graph ranking algorithm where topic clustering information is neglected. As we can see from Figure 7 (3) and Figure 7 (4), the ROUGE scores reach their peaks around 0. 6 and then drop afterwards. Thus, the topic number is set to 12 and a is set to 0. 6 in the test dataset.",
        "GPT2_formal_text": "= { ... } i_s = { ... } i_s = [ ... ] n_s = [ ... ] n_s = [ ... ] } n_t = [ ... ] n_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] λ_t = [ ... ] } Formal: We're using a sub-sampling method called Expectation-Maximization (EM) for both the training and test data. Formal: The training data includes features (X, Y), which can be either real or simulated, like images or sounds. The test data is made up of sentences (S) that don't have any of these real or simulated features. The goal is to minimize the loss function L(S) defined in Equation 1. Formal: When we're dealing with a specific entity, the loss L(X, Y) is calculated based on the uncertainty of the true label Y, given X and the rest of the data. Formal: We're using the base-cased Expectation-Maximization (EM) algorithm, which was introduced by Gentzen back in 1960. Formal: Following the approach by Furtmann and Koehn in 2003, we're tweaking the EM algorithm by adding a bit of randomness to the training data. Formal: For the main experiment, we're using the Stanford NER Tagger (Xu et al., 2003), which was developed by Fries et al. in 2003. Formal: The final input includes a start token y_i, a stop token y_j, a sample of N sentences from a pre-made training dataset, and a confidence score for the correct label. Formal: We're working with two datasets: the Yelp-EL dataset (from Xu et al., 2019) and the Yelp-WEB dataset (from Fu et al., 2019). Formal: The full training and test sets have 1024 sentences each. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C14",
        "filename": "C14-1113.json"
    },
    {
        "casual_text": "We built a website kinda like Wikipedia, called the GMB Explorer, where people can check out the Groningen Meaning Bank. It does three main things: lets you browse and search through the documents, shows you the different layers of annotations, and lets you fix any mistakes in those annotations. We'll go over these features in a bit.",
        "formal_text": "We developed a wiki-like Web interface, called the GMB Explorer, that provides users access to the Groningen Meaning Bank. It fulfills three main functions: navigation and search through the documents, visualization of the different levels of annotation, and manual correction of the annotations. We will discuss these functions below.",
        "GPT2_formal_text": "_t and the rest of the input's [CLS] tokens are combined into one big representation called h_att. Formal: We also make sure that the last hidden state of h_att is the same as the one in h_vec. The final attention vector h_att gets fed into the next-best decoder output h_vec, which is basically the output from the last layer of the encoder. Formal: So, h_vec is a representation of the input text, and h_att is a representation of the output from the last decoder layer. Formal: Next, we can use attention weights to get the final hidden representation for the attention in h_att. Formal: We figure out the attention weights for the input text using three different methods. Formal: Here, we first use a bidirectional LSTM (like the one from Bahdanau et al., 2014) to predict the attention weight for the word embeddings in h_att. After that, we calculate the attention weight for the character embeddings in h_vec by using the same bidirectional LSTM as before. Formal: After that, we use a self-attentive attention mechanism to combine the attention weights from h_att and h_vec. Formal: Finally, we calculate the attention weights for the character and word embeddings in h_vec by feeding the attention weights from h_att and h_vec together. Formal: In short, we just combine the attention weights for the input text, character embeddings, and the output from the last decoder layer to get the final attention representation for the attention in h_att. Formal: Lastly, we calculate the attention weights for the character and word embeddings in h_vec by feeding the attention weights from h_att and h_vec together. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E12",
        "filename": "E12-2019.json"
    },
    {
        "casual_text": "When we plug this IF into the mapper, it spits out the FS you see in Figure 7. Basically, it's like saying \"for whom=us\" and \"room-spec=room\" with a quantity of 4.",
        "formal_text": "['for-whom=', map_value(we, pronoun, ‫, )ﳓﻦ‬ 'room-spec=', map_value(room, noun, '‫', )'ﻏﺮﻓﺔ‬quantity=', 4] ] When this IF is fed into the mapper, the FS shown in Figure 7 is produced.",
        "GPT2_formal_text": ", we can figure out the embeddings of the target entity. Formal: We can use the embedding for the target entity as the value vector. The entity embedding can be found by checking the frequency of each token across the whole dataset. Formal: To get the embeddings of the entity tokens, we calculate them using the entity embedding. Formal: Using the token embeddings, we find the inner product between the word embeddings for the entity and the target entity. This inner product gives us the embeddings of the target entity token. Formal: For the greedy algorithm, the embeddings are generated from the E input, using the inner product between the token embeddings and the target entity embeddings. The E input is basically the same as the input for the greedy algorithm, but this time, the token embeddings are used as the input. Formal: The greedy algorithm can be used for all entity pairs, but the beam search algorithm is specifically designed for the greedy algorithm. Formal: The greedy algorithm can be used for any language pair, but the beam search algorithm is specifically tailored to use for the greedy algorithm. Formal: Formal: The greedy algorithm is usually faster than the beam search algorithm when working with datasets with varying sizes. Formal: Since the greedy algorithm is pretty slow and memory-intensive, it's usually better to use a different algorithm instead. Formal: Formal: Since the greedy algorithm is fast and memory-efficient, we mainly need to look at the geometric details when calculating the embeddings. Formal: Formal: We can also use the finite-state method (like the one suggested by Chiang in 1995) for the greedy algorithm. Formal: The whole process of calculating the embeddings ends up taking O(|V|^|D|) time. Formal: We can tweak the greedy algorithm by adding a hidden variable called h, which represents the embedding for the target entity token in the target language. Formal: The outer product for the source language is O(|V|^|D|). Formal: Formal: We can also add a hidden variable called h, which represents the embedding for the target entity token in the source language. Formal: Formal: We can also add a hidden variable called h, which represents the embedding for the target entity token in the source language. Formal: Formal: Formal: Formal: Formal",
        "directory": "bcs",
        "filename": "2006.bcs-1.12.json"
    },
    {
        "casual_text": "With RoBERTa and for TroFi and VUA Verbs, we notice some crazy jumps in performance in those final layers.",
        "formal_text": "For RoBERTa and in the case of TroFi and VUA Verbs, we see exceptional increases in the last layers.",
        "GPT2_formal_text": ", a bit of self-attention, a residual connection between the current word and the previous word, and a dimension of 200. After that, we use a linear layer to add the contextualized embeddings to our input. Formal: To create the representation for the question, we use a CNN that has an input size of 200 and a window size of 3, as shown in Figure 2. We then pass this through a fully connected layer and a softmax activation layer to get the final representation for the question. Formal: The current input to the question encoder is represented as q_i, the current output from the question decoder is... Formal: The question decoder, which is the same as the input layer, has a hidden size of 200, an embedding size of 200, and a window size of 3. We pass the output from the question decoder through an 80-dimensional fully connected layer and a softmax activation layer, and finally, we get the final question representation q_i. Formal: Finally, the question representation q_i is calculated using an attention network with the hidden size of 200 and the embedding size of 200. Formal: Our model does a great job of capturing the context of the question, which makes it easy to extract key information for answering. Formal: Check out Figure 1 for more details. Formal: For the question (q), the embedding size is 200. The token representation (y_i) is created using a CNN with an input size of 200 and a window size of 3. Formal: We'll save the exact details of the model and dataset for future work. Formal: We trained our model using the Adam optimizer (from Kingma and Ba, 2014) with a learning rate of 0.0001. Formal: We also used the Skip-Gram method (Kong et al., 2015) to learn the embedding size for the question. Formal: To make the question-answer pair better, we tweaked it by using a Transformer model (Vaswani et al., 2017) with a learning rate of 0.1. We trained the model with the Adam optimizer (from Kingma and Ba, 2014) with a learning rate of 0.0001. Formal: Finally, we tested our model on the popular image captioning dataset, Flickr30k (thanks to He et al., 2016). Formal: Formal: We trained our model on the CNN/DailyMail dataset",
        "directory": "acl",
        "filename": "2022.acl-long.144.json"
    },
    {
        "casual_text": "So, to effectively debug models with human input, it's crucial to understand these dimensions (we call them features). We use an explanation method to get this understanding. There are various ways to explain predictions from text classifiers—like natural language explanations, rules (Ribeiro et al., 2018), extracted rationales (Lei et al., 2016), and attribution scores (Lertvittayakumjorn and Toni, 2019). Some methods, like LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee, 2017), work with any model and don't need access to the model's inner workings. On the other hand, methods like DeepLIFT (Shrikumar et al., 2017) and LRP (layer-wise relevance propagation) (Bach et al., 2015; Arras et al., 2016) dive into the model's architecture and parameters to generate explanations. In our work, we use LRP to explain the learned features, not just the predictions, to help humans understand the model's behavior and make better debugging decisions.",
        "formal_text": "Therefore, understanding the roles of these dimensions (we refer to them as features) is a prerequisite for effective human-in-the-loop model debugging, and we exploit an explanation method to gain such an understanding. Explaining predictions from text classifiers -Several methods have been devised to generate explanations supporting classifications in many forms, such as natural language texts ), rules (Ribeiro et al. , 2018, extracted rationales (Lei et al. , 2016), and attribution scores (Lertvittayakumjorn and Toni, 2019). Some explanation methods, such as LIME (Ribeiro et al. , 2016) and SHAP (Lundberg and Lee, 2017), are model-agnostic and do not require access to model parameters. Other methods access the model architectures and parameters to generate the explanations, such as DeepLIFT (Shrikumar et al. , 2017) and LRP (layer-wise relevance propagation) (Bach et al. , 2015; Arras et al. , 2016). In this work, we use LRP to explain not the predictions but the learned features so as to expose the model behavior to humans and enable informed model debugging.",
        "GPT2_formal_text": "= [t, e, h](t, e, h)  \n| φ(x)  \n| φ(y)  \n| φ(z)  \n| φ(a, x)  \n| φ(b, y)  \n| φ(c, y)  \n| φ(a, z)  \n| φ(b, z)  \n| φ(a, a) Formal: The model takes the entity mentions (t, e, h) and predicts what the other entity mentions (t, e, h) might be. Formal: You can see how the model does by checking out the logits (r, c, y) and their average (r, c, y|x, y), or by seeing the \"accuracy\" (r, c, y) and \"lower bound\" (r, c, y|x, y) for the logits and their averages. Formal: Let’s say L(x) is the number of mentions in the document, and C(z) is the total number of mentions in the whole collection. Formal: Based on these two results, you can figure out the model's performance using this equation: Formal: So, for the entity mentions t, e, h, the model predicts their logits r, c, y, which then gives us the lower bound, R(c, y|x, y). Formal: This model is designed to tell apart mentions from entities that are the same. Formal: To understand how the model makes these decisions, we look at how it predicts the logits for different entity types, like Person, Organization, Location, and Other. Formal: Here’s what we’ve done: Formal: We’ve developed a new model called Multi-ResNet-152 to tackle the tricky entity linking task. Formal: We’ve tested it on two popular datasets for entity linking, like Yelp, FB15K, and MS COCO. Formal: We’ve found that our model works better than other methods for this task. Formal: We’ve shown that our model can really boost performance by using some smart data selection techniques, including some knowledge distillation techniques, to make the model more reliable. Formal: We’ve also shared some results for",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.24.json"
    },
    {
        "casual_text": "We create meaning representations for hypothesis sentences using the CCG derivation tree and Neo-Davidsonian Event Semantics (as described by Parsons in 1990). To get these meaning representations (which are basically FOL formulas) from hypothesis sentences, we use a tool called ccg2lambda. This tool was developed by Mineshima and others in 2015 and later improved by Martínez-Gómez and team in 2016. It works with CCG and λ-calculus to do the job.",
        "formal_text": "We construct meaning representations of hypothesis sentences based on the CCG derivation tree and Neo-Davidsonian Event Semantics (Parsons, 1990). ccg2lambda (Mineshima et al. , 2015; Martínez-Gómez et al. , 2016 ) is used to obtain meaning representations (FOL formulas) of hypothesis sentences based on CCG and λ-calculus.",
        "GPT2_formal_text": "= {0.1, 0.2, 0.3, 0.4} Formal: Using sparse linear models for L2T, we found that the final results aren't really affected by the number of hidden units. Instead, the model can learn just fine from the overall structure of the data. Formal: In this paper, we take a closer look at the theoretical side of modeling attention for text generation. We take a closer look at the attention models and show that they actually help improve how well the model performs. Formal: Another method for generating text is to use gradient descent (Dyer et al., 2016) to maximize the likelihood of the generated text. Formal: In this paper, we tried out three different approaches for generating text: the baseline method from (Dyer et al., 2016), the proposed one, and our own attention-based model. Formal: We compared our approach to (Dyer et al., 2016), (Kamath and Hovy, 2017), and (Hu et al., 2017), and we also looked at how much better the model performed compared to the standard model and the regular attention model. Formal: We also looked at the quality of the generated text in terms of fluency and how smooth it was. Formal: We also did a study where we compared our attention-based model to another one that doesn't consider hidden units. Formal: Our work is similar to (Hu et al., 2017) because we use the same attention model and regular attention model to generate text. Formal: We compared our results to the attention-based model from (Dyer et al., 2016), which was also trained using a bidirectional LSTM for text generation. Formal: Formal: Unlike (Dyer et al., 2016), we didn't use pre-trained word embeddings for text generation. Formal: We also compared our model to a random baseline, which doesn't use any attention at all. Formal: We also compared our model to the attention-based model from (Kamath and Hovy, 2017), which uses an attention-based bi-directional LSTM for text generation. Formal: Formal: Formal: Lastly, we looked at how different attention models affect text generation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-srw.8.json"
    },
    {
        "casual_text": "Translators typically rely on commercial tools for their work, including both translation and post-editing tasks. According to the 2018 Language Industry Survey 1, which was put together by EUATC, Elia, FIT Europe, GALA, and LINDWeb, SDL Trados 2 was the most popular tool, used by over half of the market. It was followed by MemoQ, Memsource, Wordfast, and Across. \n\nBut here’s the thing: these post-editing tools have their limitations. Since they’re proprietary, they’re not super flexible or easy to tweak. Plus, they don’t usually offer much data on the translator’s activity, which could be useful for studying how much effort goes into post-editing. \n\nOn the flip side, there are open-source CAT tools like OmegaT that have been modified for data collection purposes. Moran and others explored this in a 2014 study.",
        "formal_text": "Professional translators usually use commercial products to translate and post-edit. In the 2018 Language Industry Survey 1 conducted by EUATC, Elia, FIT Europe, GALA and LINDWeb, SDL Trados 2 was the most used product with more than half of the market quota, followed by MemoQ, 3 Memsource, 4 Wordfast, 5 and Across. 6 However, these existing post-editing environments have a restricted availability and flexibility. As proprietary tools, they are difficult to modify and do not usually provide translator activity data that may be used to study post-editing effort. However, other open-source computer-assisted translation (CAT) environments such as OmegaT, 7 have been modified and used for data collection (Moran et al. , 2014).",
        "GPT2_formal_text": ", which are used to keep track of the different aspects of the input document. For a specific input token y_i, the chance of picking a token w_i as the answer is calculated like this: Formal: The model creates a bunch of possible sequences, which include one that has the answer, and so on, up to the longest one (which is the longest token, which we'll call o_i). Formal: Let’s say µ_i is the chance of the i-th token coming from the input. The probability of the longer sequence (µ_i + 1) is calculated like this: Formal: The \"LSTM + Self Attention\" setup lets the model focus on the \"whole paragraph\" and the \"context at the beginning.\" Formal: To make sure we’re comparing our results fairly with earlier work, we’ve made all the settings and hyperparameters the same as what was used in that paper by Rush et al. (2019). Formal: Formal: The model calculates the Kullback-Leibler (KL) divergence using a log-likelihood approach. Formal: We use Adam (from Kingma and Ba, 2014) for training and dropout (from Srivastava et al., 2014) as the dropout rate. Formal: We train the model using stochastic gradient descent (SGD) and use Adam (from Kingma and Ba, 2014) to minimize the cross-entropy loss. Formal: We run the model for 50 epochs. The epochs are set to 10,000 for training, and the learning rate is 0.1 for both training and validation. Formal: We pick the best model based on its perplexity and how well it performs on the validation set. Formal: For the hyperparameters, we follow what was done in (Krawczyk et al., 2017) and calculate them using the development set. Formal: We set the dropout rate to 0.1 for both training and validation, and the learning rate to 0.0001 for both. Formal: Formal: For the parameters α, β, and γ, we calculate the Kullback-Leibler divergence using the development set. Formal: Formal: We train the model for 50 epochs. The epochs are set to 10,000 for training, and the learning rate is 0.1 for both. Formal:",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "We need to create some solid computational tools that can take these source sentences, even if they're super unclear, and turn them into clear descriptions.",
        "formal_text": "Effective computational frameworks shoud be developed for producing such descriptions from source sentences which might be quite ambiguous.",
        "GPT2_formal_text": "ed/attn_masked is a transformation matrix that works with a text encoder, like a BiLSTM (Hochreiter and Schmidhuber, 1997). It's a matrix with dimensions n by d, where d is the size of the text. We set d to 200. Formal: In Section 5, we use the same setup for training as in the original paper. Formal: In Section 6, we tweak the model by adding three extra features: (1) word alignment, (2) word-to-word LSTM, and (3) word-to-word RNN. Formal: For our NMT, we use the usual setup. Formal: In Section 7, we explore different ways to handle the issue of overfitting. Formal: In Section 8, we look into the best setup for the attention mechanism and the transformer model. Formal: In Section 9, we introduce a new idea: using a single-layer transformer. Formal: In Section 10, we share the results of our experiments and talk about the future directions. Formal: We also share the results of some tests on the Gigawords dataset. Formal: The full text of this paper is available online: http://citeseerx.ist.psu.edu/downloads/termgeometry.html Formal: If you're curious about our approach, check out Appendix B. Formal: We've also shared the code for our experiments on the Gigawords dataset, which is the same size as the original dataset. Formal: Formal: We’ve also included a comparison of our method and other models, which you can find here: http://citeseerx.ist.psu.edu/tg/ Formal: We’ve also shared the results of our experiments on the Gigawords dataset, which is the same size as the original dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C86",
        "filename": "C86-1155.json"
    },
    {
        "casual_text": "Using counterfactual reasoning in NLP usually involves defining a relationship between x and x, and then generating x based on that relationship. This has led to different counterfactual generators being designed for various tasks, each focusing on specific subsets of x that are relevant to the job. For instance, human annotators create counterfactuals like \"It is great for kids\" to help with model training and evaluation.",
        "formal_text": "Applications of counterfactual reasoning to NLP generally specify the relationship x )x, and then createx according to the relationship. As a result, prior work has tailored counterfactual generators for different applications, only collecting subsets ofx that are useful for the specific task. For example, to support model training and evaluation, human annotators create counterfactuals It is great for kids.",
        "GPT2_formal_text": "= {q1, q2, ..., qn}  \n\nThat's basically it!  \n\nIn this project, we're looking to figure out how the accuracy and diversity of URNs affect things. We're checking two main things:  \n\n1) Does the model's performance change when we tweak its parameters?  \n2) Does the diversity of the generated text change when we tweak its text generation model?\n\nWe're comparing these two things using accuracy, precision, recall, and F1 scores. For the first question, we're looking at the F1 score on the test set itself, without any changes to the model's parameters. We're focusing on \"Natural Questions\" because, as mentioned in (Gulordava et al., 2017), the model tends to generate more natural-sounding questions than other types.  \n\nFor the second question, we're checking the F1 score on the test set as the response, after any changes to the model's parameters. This is because the model tends to give more varied responses in test sets compared to in the training set, where it might just spit out the same sentence.  \n\nThe first question is way harder than the second, so we're focusing on the first one for now. Formal: To figure out how the diversity of the generated text changes when we tweak the text generation model, we're calculating the average length of the generated text when we apply the model's parameters (let's call this \"δ\"). Formal: The best result for diversity is when the model's parameters match up with the text's diversity. Formal: The worst result for diversity is when the model's parameters don't match the text's diversity. Formal: But if the text has a different topic, the model's parameters might be more in sync with the text's diversity. Formal: We're not trying to create a perfect score for diversity here. Formal: Instead, we just want to see how much the model's parameters are affected by the text's diversity. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "Language models (LMs) try to figure out how likely a sequence of symbols {x t } T t=0 is, by looking at the joint probability.",
        "formal_text": "Language models (LMs) estimate the likelihood of a symbol sequence {x t } T t=0, based on the joint probability,",
        "GPT2_formal_text": ". Similarly, the residual connections get updated using the softmax function. The weights for the Sigmoid function in the log-linear model are optimized using the Sigmoid function, and we use the Adam optimizer (from Kingma and Ba, 2014) with a learning rate of 10^-4 to fine-tune the model. Formal: There are a few ways to measure how well the model is doing, but we’re sticking with the standard multilingual BLEU score (from Papineni et al., 2002) since it’s the most widely accepted metric for evaluating different NLP tasks. Formal: Instead of just looking at the f1 score, we also check the precision at which the reference sentence is correctly identified. In our project, we’re using the reference translation from the development set as the training data. Formal: Following what Brown et al. (1993) did, we also include the standard F-score (from Brunner and Vanderwende, 2006) in the evaluation. Formal: Since our system has a lot of extra parameters that aren’t directly visible in the text, we just use the average of the accuracy from both the system’s and reference sentences. Formal: In our experiments, we noticed that the average F-score didn’t really show a clear difference between systems. Formal: We ran the experiments on the WMT17 development set, using the version of the WMT17 test set that was available at the time. We picked WMT17 because it includes a lot of test sentences for each system, plus it’s been suggested that the WMT17 test set has way more data than the development set (like about 5 times more) for English, German, and Chinese (check out Figure 1 for more). This suggests that our system might not be performing as well because the system’s training data is too limited to actually be useful in real-world applications. Formal: The results from this experiment are in Table 6. Formal: We calculated the precision for the reference translations, which we’ll call \"reference.\" Formal: The results from this experiment are in Table 7. Formal: The results from this experiment are in Table 8. Formal: Table 9 shows the results for the development set, with the reference sentences highlighted in bold. Formal: Table 10 shows the results for the test set, with the reference sentences highlighted in red. Formal",
        "directory": "D18",
        "filename": "D18-1150.json"
    },
    {
        "casual_text": "Like always, when we're dealing with limited resources, the improvements we see on the test and dev sets don't really match up. We can clearly see better results on the test set with the BTEC stuff, but when we apply SBR, the score on the dev set actually goes down.",
        "formal_text": "As usual, for the tasks with scarce resources the improvements on the test and dev sets are not coherent. While a clear improvement of test results can be observed in the BTEC results, the development set score degrades when SBR is applied.",
        "GPT2_formal_text": "= {}, which in turn have a set of (attribute_tag, value_tag) tuples. Formal: The input for slot filling is basically a mix of the slot type and its corresponding intent. Formal: Unlike earlier approaches, our method uses a different way to combine and group input slots into different intent representations. Formal: Plus, by using the bidirectional Long Short-Term Memory (LSTM) neural network (LSTM) for slot filling, it makes the whole process more efficient, resulting in better performance. Formal: The LSTM encoder takes a sequence of slot slots, turns it into a sequence of hidden representations, and then uses a GRU layer to create these representations. Formal: Let’s say the input sequence for intent label i is labeled as {0, 1, ..., N}. The encoder then calculates the probability P_S(slot_type, intent_type) by doing something similar, but for slot_type, intent_type, and their respective input slot values. Formal: Finally, the final output is a probability distribution over these slot types, with the highest probability given to the current intent token. Formal: To get a better idea of how well the LSTM encoder is doing, we add up the probability of all possible intent sequences given the input sequence. Formal: The LSTM encoder calculates the probability P_S(slot_type, intent_type) by doing something similar, but for slot_type, intent_type, and their respective input slot values. Formal: The final output is a probability distribution over these slot types, with the highest probability given to the current intent token. Formal: Formal: Formal: So, the final result is the probability of the output happening given the input sequence and all the possible intent sequences. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2009.eamt-1.27.json"
    },
    {
        "casual_text": "To check how well the models are doing, we calculate the cosine similarity between the model's output and every possible ground truth vector. During training, we have 3701 word vectors to compare with, and during testing, we have 4284 (which includes the 3701 from training plus 583 new ones). \n\nWith these word vectors, we figure out the Recall@k (R@k), which tells us how often the ground truth vector is among the top k most similar vectors to the model's output. If the ground truth is the most similar to the model's output, that counts for R@1. Similarly, if the ground truth is in the top 5 (or top 10) most similar words to the output, it counts for R@5 (or R@10).",
        "formal_text": "To evaluate the models, the cosine similarity between a model's output and every possible ground truth vector representation is computed. The set of competing word vectors, therefore, consists of 3701 word embeddings during training, and of 4284 (3701 training + 583 testing) vectors during testing. Given these competing word embeddings, Recall@k (R@k) is computed as the proportion of times that the set of top k word embeddings which are closest to the model's output also includes the ground truth vector representation. If the ground truth is most similar to the output vector of a model, then this contributes to R@1. Furthermore, a word contributes to R@5 (R@10), if the corresponding ground truth word embedding is within the top 5 (top 10) most similar words to the output vector.",
        "GPT2_formal_text": "is basically a mix of the input attention vectors and a linear layer that adjusts them. After that, we use two types of loss functions to help the model understand the context better. Formal: For each token, we calculate a probability for each input label and the label embedding y. Formal: The sequence embedding for the input word w is created by combining the sequence embedding for w and the word embedding for the token x. The learnable parameters θ, γ, and η are learned from the data. Formal: The values of θ, γ, and η are learned from the data. Formal: The parameters θ, γ, and η are learned from the training data. Formal: We randomly pick the embedding for the token x using a uniform distribution p(x|w). Formal: We randomly pick the embedding for the token x using a uniform distribution p(x|w). Formal: We randomly pick the embedding for the token x using a uniform distribution p(x|w). Formal: We randomly pick the embedding for the token x using a uniform distribution p(x|w). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-srw.3.json"
    },
    {
        "casual_text": "This part looks at the stuff that impacts how well PLM works for automatically assigning ICD codes.",
        "formal_text": "This section provides analysis on factors that affect PLM's performance on automatic ICD coding.",
        "GPT2_formal_text": "= {entity_id} form = {item_type} {template} [, //][, //] Formal: Using the relation extraction method, a model can create a query query that has a bunch of possible words, and a sentence sentence with these words, where each word is just one of the ones in the query. Formal: The final result from the relation extraction process, which includes the entity type and the sentence, looks like this: Formal: For the first round, we use the relation extraction process to pull out the keyphrases for the first entity in the input. Then, we use the relation extraction process to figure out the pair (e1, e2) of sentences where e1 and e2 both appear. Formal: We handle this whole process in the second round. Formal: Using the same set of relation extraction results, a model can create query queries with even more words. Formal: Check out Figure 3 for an example of a query query with both the entity and its keyphrases. Formal: We create the query by combining the two documents we retrieved earlier with the query query to get the final query. Formal: We're using the top 20 keyphrases that were extracted by the first round of relation extraction. Formal: From the top 10 results, we pick the top 10 sentences that have at least 10 words. Formal: Using these top 10 sentences, the model can generate the query query with a bunch of words. Formal: From the top 20 keyphrases, we pick the top 20 sentences that have at least 10 words. Formal: Formal: From the top 10 results, we pick the top 10 sentences that have at least 10 words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "clinicalnlp",
        "filename": "2022.clinicalnlp-1.2.json"
    },
    {
        "casual_text": "Alright, let’s break down how we measure the connection between two sentences using word embeddings. There are three main methods for doing this:\n\n1. **Greedy Metric**: This one matches words in both sentences based on how similar their embeddings are (using cosine similarity). It then averages all these similarity scores to get a final number. This method was introduced by Rus and Lintean in 2012.\n\n2. **Average Metric**: Instead of matching words one by one, this method just averages all the word embeddings in each sentence to create a single vector for each sentence. Then, it checks how similar these two vectors are using cosine similarity. Mitchell and Lapata came up with this idea in 2008.\n\n3. **Extreme Metric**: This one is a bit different. It looks at all the word embeddings in a sentence and picks out the most extreme values (like the highest or lowest ones). Then, it uses these extreme values to create a vector for each sentence and compares them using cosine similarity. Forgues and his team introduced this method in 2014.\n\nSo, these are the three ways we can measure how semantically connected two sentences are using word embeddings.",
        "formal_text": "Relevance Metrics: The following three word embedding based metrics 3 are used to compute the semantic relevance of two utterances. The Greedy metric is to greedily match words in two given utterances based on the cosine similarities of their embeddings, and to average the obtained scores (Rus and Lintean, 2012). Alternatively, an utterance representation can be obtained by averaging the embeddings of all the words in that utterance, of which the cosine similarity gives the Average metric (Mitchell and Lapata, 2008). In addition, one can also achieve an utterance representation by taking the largest extreme values among the embedding vectors of all the words it contains, before computing the cosine similarities between utterance vectors, which yields the Extreme metric (Forgues et al. , 2014).",
        "GPT2_formal_text": "ing_type\" to control which parts of the text are being masked. This means the model can focus only on specific parts of the text. Formal: The final result is a list of embeddings called h_f, which includes the output from the pre-trained transformers h and h_t. Formal: Lastly, we get a score for how well the model is doing by checking its accuracy (A_acc). Formal: The model is trained using the Adam optimizer (from Kingma and Ba, 2015) with a learning rate of 0.001, a regularization parameter α, and a batch size of 32. Formal: The setup of the model is shown in Figure 2. Formal: The model is trained using the Adam optimizer (from Kingma and Ba, 2015) with a learning rate of 0.001, a regularization parameter α, and a batch size of 32. Formal: The model is trained using the Adam optimizer with a learning rate of 0.001, a regularization parameter α, and a batch size of 32. Formal: Formal: We set the input embeddings E_i to 1 and the output embeddings E_t to the same length. Formal: The model is trained using the Adam optimizer with a learning rate of 0.001, a regularization parameter α, and a batch size of 32. Formal: The model is trained using the Adam optimizer with a learning rate of 0.001, a regularization parameter α, and a batch size of 32. Formal: Formal: We set the input embeddings E_i to 1 and the output embeddings E_t to the same length. Formal: The model is trained using the Adam optimizer with a learning rate of 0.001, a regularization parameter α, and a batch size of 32. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D17",
        "filename": "D17-1065.json"
    },
    {
        "casual_text": "Basically, the decision is always made using the formula in Eq. 1. The difference between models comes down to how they represent things and the weight vectors they use.",
        "formal_text": "In all cases, the decision is formulated as in Eq. 1, where different models differ by the representations and weight vectors used.",
        "GPT2_formal_text": "= { ... } t = {t1, ..., tk, ..., tkt} ... t = {t1, ..., tkt+1, ..., tkt+m}. Formal: We set up a two-level attention system to figure out the attention weights, which we call the latent vector representation of the s-th label. The hidden vector representation, called h s = h 0, is basically the sum of the hidden vector representations of the s-th token in the token set T. Formal: The head-tail attention mechanism works by using attention weights to create a personalized representation of the current sentence. The head-tail attention mechanism is a type of attention mechanism, and we can think of it as a self-attention mechanism. Formal: We use a hidden vector representation of the k-th token from the query to the head, as a way to capture the attention. For each query, we first calculate the attention weights h k, which are similar to the hidden vector representations of the k-th token, using Equation 4. Then, we add these weights to the head-tail attention mechanism to get the final attention weight representation, h k. Formal: The model calculates the attention weights based on how likely it is that a token should show up at position k. Formal: The final representation h k is made by combining the two attention weights, h k, with a window size of k. Formal: We use the approximate attention weight for the head-tail attention mechanism, h 0, to calculate the attention weights for the query. Formal: To get the attention weights for a token, we calculate the estimated attention weights, h 0, using Equation 5. Formal: We do the same for the head-tail attention mechanism to calculate the attention weights for the query and token. Formal: Finally, we use the expected attention weights, h k, to calculate the expected attention weights for the head-tail attention mechanism, h k. Formal: We also use an attention mechanism to get the attention weights for the query and token. Formal: We then add these estimated attention weights, h k, to the head-tail attention mechanism to get the final attention weight representation, h k. Formal: Finally, the attention weights h k are used to update the hidden vectors, h 0, to keep the head-tail attention mechanism focused on the k-th token. Formal: Here, m is the size of",
        "directory": "D08",
        "filename": "D08-1037.json"
    },
    {
        "casual_text": "In this paper, we looked at how SABLE, a user-friendly system for building translation dictionaries, works for finding domain-specific word translations using small, specialized text collections. When tested on a really small corpus (around 400,000 words), the system did pretty well. It can suggest possible single-word translations by automatically removing common words. After filtering those out, the system gets a precision of up to 89% for domain-specific terms, with a recall that’s estimated to be between 30-40%, though this is a very cautious estimate.",
        "formal_text": "In this paper, we have investigated the application of SABLE, a turn-key translation lexicon construction system for non-technical users, to the problem of identifying domain-specific word translations given domain-specific corpora of limited size. Evaluated on a very small (400, 000 word) corpus, the system shows real promise as a method of processing small domain-specific corpora in order to propose candidate single-word translations: once likely general usage terms are automatically filtered out, the system obtains precision up to 89% at levels of recall very conservatively estimated in the range of 30-40% on domain-specific terms.",
        "GPT2_formal_text": "ed: A normalization method that figures out the attention value for each token. This value is usually used to adjust the model's parameters. Formal: 2. Residual Variational Autoencoder (rva) (Zhang et al., 2018): This method, which was introduced by Zhang et al. in 2018, creates a residual connection between the input and output at each step. The thing is, the way it calculates the attention is kind of messy because it’s a bit random and often depends on the position of the token. So, we took inspiration from how recurrent neural networks work (like the one by RNNG, which is a recurrent neural network), and we came up with a more efficient way to estimate the attention. We call this new approach residual-variational autoencoder (rva). Formal: 3. Residual Attentive Attention (rva) (Zhang et al., 2018): This version of the residual neural network is more like a regular attention model. It uses a residual connection to create a residual context. Formal: 4. ReLU (Lin et al., 2014) Formal: 5. Random-Context Attention (rva) (Zhang et al., 2018): This method is the one we used for the supervised tests. We also did some experiments to see how different attention techniques affect the model’s performance. Formal: 6. Over-Attention (rva) (Zhang et al., 2018): This method uses a random-context mechanism to handle the outputs. Formal: 7. Attention-based Baselines Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "A97",
        "filename": "A97-1050.json"
    },
    {
        "casual_text": "We started working on classifying words to help model a morpho-syntactic alternation in Nêhiyawêwin verbs. Inspired by Arppe et al. (2008), we came up with a hypothesis that both the semantic classes of the verbs and their noun arguments could play a role in this alternation. Because of time limits, we looked into ways to automatically classify both verbs and nouns in Nêhiyawêwin. While our main goal is still to use statistical modeling, semantic or thematic classifications can be super helpful for language learners and revitalization efforts, especially in online dictionaries. Instead of just looking up English translations, you could see all the words related to a specific theme, which is way more useful.",
        "formal_text": "We begun word classification in preparation for modelling a morpho-syntactic alternation in Nêhiyawêwin verbs. One hypothesis we developed for this alternation, based on Arppe et al. (2008), is that the semantic classes of the verbs themselves as well as their nominal arguments would inform the verbal alternation. Due to constraints of time, we investigated methods to automatically classify both verbs and nouns in Nêhiyawêwin. Although statistical modelling remains the immediate motivator for the authors, semantic/thematic classifications have a wide range of benefits for language learners and revitalization, particularly in online lexicographic resources, where one may want to view all words to do with a theme, rather than simply finding translations of single English words.",
        "GPT2_formal_text": "= {[CLS], [SEP]}, where [CLS] is a vector in R^m. To calculate the ROUGE score, the model uses the Kmeans algorithm, where each point is a cluster in R^m. Formal: In the classifier, the hidden state h_i is calculated using this formula. Formal: In this paper, we used RoBERTa's word embeddings to get the embeddings for the query and the answer. For RoBERTa, we set the number of layers to 3. We trained it for 100 epochs using the Adam optimizer (from Kingma and Ba, 2014) with an initial learning rate of 0.0001. We also used a batch size of 32, and the batch size for the teacher model is 1. We applied a dropout of 0.2. Formal: The final embedding for the query and the answer is generated using an MLP. In this MLP, the word embedding vector is split into 300 dimensions. Formal: For the query, we picked a random topic from a window of 300 and trained it for 100 epochs using the Adam optimizer. After that, we checked how well it did by checking the F1 score. Formal: We used two approaches to pick the topics for our queries: top-k and top-p. The top-k topic selection method is different from top-p because the user has to decide which topic to choose for the query. The top-p topic selection method is similar to top-k because the user can choose more than one topic. Formal: To get the embeddings for the query and the answer, we used LSTMs. For the first LSTM, we used CNNs (Chang et al., 2015) with hidden states that are 300 dimensions. We ran two CNNs with a batch size of 64 and added dropout with a 0.2 probability. After that, we trained the LSTM for 100 epochs using the Adam optimizer. Formal: We used a transformer model as the teacher model to generate the embeddings for the query and the answer. To make sure the model did a good job, we trained it for 500 epochs using the Adam optimizer. Formal: We used the usual metrics for evaluating the classifier: precision (P), recall (R), and F1 score (F). Formal: To check how well the classifier was doing,",
        "directory": "americasnlp",
        "filename": "2021.americasnlp-1.12.json"
    },
    {
        "casual_text": "Here are some examples of titles generated using rejection sampling. In Figure 7, you'll see titles that were picked and ones that got rejected. As you can tell, the rejected titles usually aren't as good.",
        "formal_text": "Examples of generated titles. We present examples of titles that from rejection sampling that are selected and that were rejected in sampling in Figure 7. As shown, rejected titles tend to be of lower quality.",
        "GPT2_formal_text": "ed is equal to the average of the attention weights in the input, but this isn't the same as the attention scores that the model learns during training. Since our model focuses on sentence-level retrieval, we take advantage of this extra information. We use the model's internal state as the sentence representation. Then, we calculate the attention score by summing up the attention weights for all the tokens in the sentence. Formal: Once we get the query and the history from Equation 1, we use an attention-based attention system to pick out the important bits of text and pass them on to the next steps in the process. Formal: For each document, we calculate the attention weights for the text tokens in the query and the history tokens of the document. To make sure we're not mixing up the query and history, we use a beam search decoder (from Du et al., 2016) to rank the generated responses. This decoder picks the one with the highest attention score for the query and history, then sends it through the model to get the final answer. Formal: To avoid messing up the original response by mixing it with the wrong query, we use a loss function that takes into account the query and the history. Formal: Following what other studies have done (like Junczys-Dowmunt et al., 2017), we take the predicted query and the query's history and treat them as the query and history combined. Formal: To make sure the model doesn't use the wrong query and the wrong history, we use a loss function that keeps all the query and history information in one neat vector. Formal: For each document, we calculate the attention weights for the text tokens in the query and the history tokens. Formal: To make sure we're not mixing up the query and history, we use a loss function that keeps all the query and history information in one neat vector. Formal: To make sure the model doesn't use the wrong query and the wrong history, we use a loss function that keeps all the query and history information in one neat vector. Formal: To make sure the model doesn't use the wrong query and the wrong history, we use a loss function that keeps all the query and history information in one neat vector. Formal: To make sure the model doesn't use the wrong query and the wrong history, we use a loss function that keeps all the query and history information in one neat vector. Formal: Formal: For each document, we calculate",
        "directory": "acl",
        "filename": "2020.acl-main.66.json"
    },
    {
        "casual_text": "c. GloVe: We use GloVe embeddings (Pennington et al., 2014) that we trained on a collection of 3 million words from a mix of recipe texts and video transcriptions. For any given instruction, we take the GloVe embeddings (Pennington et al., 2014) of the nouns and verbs and average them to get a single vector representing the instruction. When comparing two instructions, we measure how similar they are by looking at the cosine similarity between their vectors.",
        "formal_text": "c. GloVe: We train GloVe embeddings (Pennington et al. , 2014) on an in-domain corpus of 3 million words put together by combining text recipes and video transcriptions. Given an instruction, we average the GloVe embeddings (Pennington et al. , 2014) of nouns and verbs 12 to obtain its embedding vector. Given two instructions, we define their embedding similarity as the cosine similarity of their embedding vectors.",
        "GPT2_formal_text": "= {[CLS] } for each pair in the training set. This setup helps ensure that every time we need to process a new source token, we only have to calculate the source token attention vector. Formal: We use the objective function to maximize this embedding vector. Formal: After running the training for a given batch size, we use this optimized embedding vector as the output for the next batch. Formal: Unlike the usual method, we can't just stick with the same settings for all the models during training. To avoid losing too much ground to models that do the exact same thing, we adjust the settings for each model individually. We're focusing on these settings: Formal: Here, λ is a number we adjust during training, and we pick the best λ based on the validation set's performance. We call the model's performance \"best\" if its perplexity is under λ. For instance, if we're training on the validation set with λ = 0.5, we're happy with the best model's performance if it's below λ. Formal: We also use a hyperparameter called the \"weight\" to control how much we penalize the model's decisions. Basically, if a model's decision is different from the original input's decision, the weight is set to 1. Formal: For the basic model that predicts the target sequence, we train it using the same setup as the original model. Formal: In this case, we're using the same training batch size as the original model. Formal: For the regular model that predicts the source sequence, we train it with different batch sizes. Formal: After training, the model can be queried to calculate the target sequence attention. Formal: We don't use any data augmentation during training. Formal: For the multi-model approach, we train three models. Formal: Here, T is the number of models, and λ is the highest value we can optimize. Formal: The fourth model calculates the target sequence attention for the source token. Formal: Formal: We feed the source sentence through three different transformer layers. Formal: The third transformer layer is responsible for figuring out the hidden representation of the source token. Formal: Formal: The fourth transformer layer calculates the target sequence attention for the target token. Formal: Formal: The last transformer layer calculates the target sequence attention for the target token. Formal: Formal: The final transformer",
        "directory": "acl",
        "filename": "2020.acl-main.440.json"
    },
    {
        "casual_text": "We tried this method: first, we ranked all the candidates using a similarity function and picked the top 3. Then, we only kept those 3 if they were within the last 4 sentences. Without worrying about their semantic class, this bumped up the precision to 41% (it was 30% if we limited the distance earlier, or 39% without limiting it). If we added a filter based on semantic classes (so only keeping the top 3 if they matched the right class and were in the last 4 sentences), we got a much better precision of 53%, with a decent recall of 57.8%. Compared to the similarity-list approach, we got way better precision than other methods with similar recall (like the one with the 100 most similar items, which has 44% precision, or the one with 50 items and two-way matching, which has 46% precision).",
        "formal_text": "The approach we tried was as follows: we rank all candidates using the similarity function, and keep only the 3 top-rated candidates. From these 3 toprated candidates, we keep only those within the last 4 sentences. Without filtering by semantic class, this improves the precision to 41% (from 30% for limiting the distance beforehand, or 39% without limiting the distance). Adding filtering based on semantic classes to this (only keeping those from the 3 top-rated candidates which have a compatible semantic class and are within the last 4 sentences), we get a much better precision of 53%, with a recall that can still be seen as good (57. 8%). In comparison with the similarity-list-based approach, we get a much better precision than we would get for methods with comparable recall (the version with the 100 most similar items has 44% precision, the version with 50 most similar items and matching both ways has 46% precision).",
        "GPT2_formal_text": ". In contrast, the ones from the previous layer are pretty much the same as the ones in the prior layer. Formal: We're using a Bi-LSTM decoder for this task, which was introduced by Hochreiter and Schmidhuber back in 1997. The input for the Bi-LSTM is a sequence of words, y = (w1, w2, ..., wn). The output from the Bi-LSTM is a vector x = (x1, x2, ..., xn). Formal: Also, if we have a specific reference word wn, which is the last word in the input, we can use a different Bi-LSTM to get the vector representation for that word, y = (y1, y2, ..., yn). Formal: In this part, we’ll talk about the BERT model we’re using to create the representations. Formal: We’re using the Stanford NER dataset (Krishna et al., 2014) to train our model. Formal: To check how well our model’s results match up with the reference word, we’re using the F1 score on the test set. Formal: The BERT model we’re using is built on an embedding layer that’s 300 dimensions and has 6 layers. For each layer, it has an input size of d, a hidden size of h, and a dimension of d. Formal: The baseline model we’re comparing is a 4-layer Bi-LSTM. Formal: For each layer, we train a bi-LSTM and calculate the attention score. Formal: We also check how similar the generated vectors are to the reference vectors using cosine similarity (Schwenk et al., 2015). Formal: We’ve set the batch size to 64 and the number of epochs to 5 for our experiments. Formal: Formal: We calculate the F1 score for each layer in a Bi-LSTM using the test set as our reference. Formal: We use the bi-LSTM model to create the representations for each word in the input. Formal: Formal: Formal: We’re using the unsupervised model training method by Imamura et al. (2016) to train our model. Formal: Formal: Formal: Formal: Formal:",
        "directory": "D07",
        "filename": "D07-1052.json"
    },
    {
        "casual_text": "Hey, just a heads-up: we're using the same dataset for both training the discourse parser (MEGA-DT) and evaluating sentiment (Yelp'13). To keep things fair, we're only using the training part of the dataset to train the discourse parser. This means the development and test documents haven't been seen at all during the training process.",
        "formal_text": "Please note that since we use the same base-corpus for training the discourse parser (MEGA-DT) and predicting sentiment for the final evaluation (Yelp'13), we restrict the data used to train the discourse parser to the training-portion of the corpus. This way we ensure that development-and test-documents are unseen during the whole training process.",
        "GPT2_formal_text": "_a = [ ..., ..., ..., ... ] α, where α is the negative log likelihood of the label y. Formal: We calculate the label attention weights like this: Formal: The distributional vectors σ_ij are calculated by dividing the attention weight in the output layer by the log likelihood of the input vector x_ij. Formal: Formal: For instance, if we have a vector w_i that includes the token w1, ..., wm, the attention weight for w_i in the output layer is calculated as the sum of the attention weights for all the tokens in the input vector x_ij. Formal: Formal: For example, if we have a vector w_i that includes the token w1, ..., wm, the attention weight for w_i in the output layer is the sum of the attention weights for all the tokens in the input vector x_ij. Formal: Formal: We take the first token, w_i, and use it as the latent representation for the first word in the input. Formal: Formal: The attention weights for the first word are calculated using Equation 6. Formal: Formal: We add a linear layer after the output layer to predict the label y_i. Formal: Formal: After adding the linear layer, we predict the label y_i using the last hidden state. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.16.json"
    },
    {
        "casual_text": "The reference translation is the smoothest overall. Makes sense since it's written by humans. On average, the reference translation scored the same in terms of adequacy compared to the others. Our reference was collected from the web, so it has some issues, as mentioned in section 3.3. One big problem with the reference translation is that it focuses on the whole document, which makes it less adequate for sentence-by-sentence translation. This is especially noticeable in conversational texts, where the reference was translated based on the entire session (like a talk or vlog). Here's an example:\n\nSource: \"-Nope, they're shutting us down.\"\nRef: \"-Tidak, misi ditunda.\"\nOurs: \"-Tidak, mereka menutup kita.\"\nGoogle Translate: \"-Tidak, mereka menutup kita.\"",
        "formal_text": "The reference translation is the most fluent across every domain. This result is expected, as the reference is written by humans. Reference translation's adequacy scored equally on average, compared to the rest. Our reference is crawled; therefore, it contains several issues, as mentioned in section 3. 3. . One main problem in reference translation is that they are translated with document level in mind, therefore reducing adequacy as encapsulated sentence-based translation. This is especially true in conversational, where the reference was translated from the whole session (i. e. , talk, or vlog). One example can be seen below: Source \"-Nope, they're shutting us down. \" Ref \"-Tidak, misi ditunda. \" Ours \"-Tidak, mereka menutup kita\". Google Translate \"-Tidak, mereka menutup kita. \"",
        "GPT2_formal_text": ". Formal: If a word is in the dictionary, we use the word embedding to figure out its type. Formal: At the end of the sentence, we add the token embedding for the word's token type, and we do this to generate the final representation. Formal: To make sure we're generating the right token types, we use a loss function that adjusts the probability P(w|x) for the generated tokens. Formal: The generator learns using a part of the data, and then it adjusts its loss to make sure it gets it right. Formal: So, the whole process is like this: Formal: The loss function is calculated using a batch of tokens. Formal: We use two types of loss functions: Formal: When we generate a token, we calculate the loss L_n for the next token. Formal: We add up the losses for the tokens generated so far. Formal: Finally, we add the token embedding L_n to get the final representation of the token. Formal: The loss function is adjusted based on the batch size, and we do this using a second loss function L_2, which is adjusted by the size of the training data. Formal: The generator starts with a fixed word embedding, let's call it \"y.\" We can also use a vocabulary from the same dataset, or we can calculate the loss L_w for each word. Formal: We use two loss functions: L_w_w and L_w_w_i. Formal: Formal: We adjust the loss to fit the parameters of the generator. Formal: To learn a new loss function, we start with a fixed word embedding, let's call it \"x.\" Formal: We use L_w_w to learn the parameters, which we then use to adjust the loss. Formal: The generator learns using a part of the data, and then it adjusts its loss to make sure it gets it right. Formal: To train the generator, we use a second batch of tokens. Formal: The loss function is calculated for each word in this second batch. Formal: We add up the losses for the tokens generated so far. Formal: Finally, we add the token embedding L_i to get the final representation of the token. Formal: Formal: Formal: Finally, we add the token embedding L_i to get the final representation of the token",
        "directory": "bucc",
        "filename": "2020.bucc-1.6.json"
    },
    {
        "casual_text": "Sometimes, language models (LMs) can get confused and think \"and\" signals a similar relationship between words, but it might go beyond just the word you're looking at. For instance, if you type \"The human heart not only makes heart sounds and,\" the LM might predict \"muscle\" as one of the top suggestions, and then follow up with \"movements.\" So, it's not just matching \"sounds\" but also the whole phrase \"heart sounds\" with something like \"muscle movements.\" We didn't really focus on this in our current work, but we think making sure the predictions match the target word in terms of grammar (like being the same part of speech or having the same number) could help fix this. Plus, this opens up a cool possibility for moving beyond just single words and dealing with whole phrases instead.",
        "formal_text": "Multi word phrases substitutes Sometime the LM does interpret the and as a trigger for a symmetric relation, but on a chunk extending beyond the target word. For example, when presented with the query The human heart not only makes heart sounds and, the forward LM predicted in its top twenty suggestions the word muscle, followed by a next-word prediction of movements. That is, the symmetry extends beyond \"sounds\" to the phrase \"heart sounds\" which could be substitutes by \"muscle movements\". We didn't specifically address this in the current work, but note that restricting the prediction to agree with the target word on part-of-speech and plurality may help in mitigating this. Furthermore, this suggests an exciting direction for moving from single words towards handling of multi-word units.",
        "GPT2_formal_text": "= { idx } (in) ∈ { 0, 1 } for i in (0, 1). Formal: A chunk of data is said to be \"private\" if it doesn't have any annotations for it, or \"public\" if it does have annotations. Formal: Just so you know, the total number of observations, labeled as n_o, in a dataset D, is also called the \"private\" or \"public\" number. Formal: • L_o is the total number of observations in a dataset D, plus one for each unique label in D. Formal: The \"private\" number is different for each dataset D. Formal: The \"public\" number is different for each dataset D. Formal: Formal: The \"private\" number is different for each dataset D. Formal: The \"public\" number is different for each dataset D. Formal: Formal: A model can be labeled with more than one \"private\" number. Formal: Formal: If there are more than two \"private\" numbers, the model is labeled with the last one. Formal: Formal: Formal: The \"private\" number is different for each dataset D. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1523.json"
    },
    {
        "casual_text": "In Table 3, it looks like basic emotions (like joy, anger, or sadness) usually perform better than complex emotions (like positive, neutral, or negative). But in Table 1, the amount of data for basic emotions is often smaller than for complex ones. This suggests that the difference in performance is probably due to the emotional content of the labels, not the size of the data. For example, the complex emotion 'negative' (which includes things like 'hate' and 'anxious') is way more varied than the basic emotion 'sad', and that variety makes it harder to detect. \n\nAlso, even though 'sad' and 'angry' are both basic emotions and have similar amounts of data, it seems way easier to spot 'sad' than 'angry'. Maybe this is because 'angry' can be caused by all sorts of different things, and it's harder to figure out and use that information. So, for emotion classification, it's important to have an encoder that can pull out the event-related info about what caused the emotion from the text. \n\nTable 4 shows how different emotion cause detection models perform, with \"Sequence\" listing the order of words each model uses. In Table 4, JMECause does better than the best pipeline model (LSTM) by 0.8% in terms of F-scores.",
        "formal_text": "In Table 3, the performance of a basic emotion (i. e. , joy, angry or sad) is often better than the one of a complex emotion (i. e. , positive, neutral or negative). However, in Table 1, the data size of a basic emotion is often smaller than the one of a complex emotion. This indicates that difference in performance is likely linked to differences in the emotional contents of labels rather than differences in data sizes. E. g. the complex emotion 'negative' (i. e. a collection of complex emotions with negativity, such as 'hate', 'anxious', and so on) is more diverse than the basic emotion 'sad', and this diversity in emotional contents brings more challenges to the detection of this complex emotion. Furthermore, even if both 'sad' and 'angry' are basic emotions and have similar data sizes in our experimental data, it seems much easier to detect 'sad' instances than to detect 'angry' instances. This is maybe because 'angry' is caused by more various events and it is more difficult to capture and utilize those cause events. Thus, it is necessary for the emotion classification to have an encoder which can extract the eventbased information of emotion cause from texts. Table 4 shows the performances of different emotion cause detection models, where \"Sequence\" lists the sequences of input words used by each model. In Table 4, JMECause outperforms the best pipeline model (LSTM) by 0. 8% in F-scores.",
        "GPT2_formal_text": "_tensor layer. This layer is calculated by multiplying the layer's output with the attention weights, and then it's fed into a fully connected layer. Formal: To make sure the model can handle different types of situations, we also use a separate hidden layer with attention weights to keep track of the keyphrases the model is currently focusing on. Formal: To calculate the attention weights, we use the sigmoid function on a scalar called z_i. This function helps us understand the probability distribution across all possible keyphrases. Formal: To get the word embeddings for a group of words (called a word embedding set), we use the word2vec library. Formal: For each word i in the set W, the vector representation (i_w) from the word2vec library is calculated as: Formal: For a single word (w_i), the vectors for its neighbors in the embedding set are: Formal: For a pair of words (w_i1 and w_i2), the vectors are: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1066.json"
    },
    {
        "casual_text": "Lexical Semantics, Ambiguity, and Plausible Assignments. The accuracy numbers we’re getting from our methods are kind of \"pessimistic\" in a way, meaning they should be seen as lower limits. When we dig into the mistakes our models make, we find that a lot of the frame assignments they come up with are actually pretty reasonable, even if they don’t match the exact labels in the leave-one-out test. Take the LU \"guerrilla,\" for example. In FrameNet, it’s assigned to the frame PEOPLE BY VOCATION. Our mixed model suggests the two most similar frames are MILITARY and TERRORISM, which still make sense, right? Same thing with the LU \"caravan.\" Our model says the closest frame is VEHICLE, but in FrameNet, it’s only linked to the frame BUILDINGS. This happens because FrameNet doesn’t cover everything—some LUs aren’t fully annotated and only show up in a few of their possible frames. So, the actual accuracy of our models is probably higher than what the numbers suggest.",
        "formal_text": "Lexical Semantics, Ambiguity and Plausible Assignments. The overall accuracies achieved by our methods are \"pessimistic\", in the sense that they should be intended as lower-bounds. Indeed, a qualitative analysis of erroneous predictions reveals that in many cases the frame assignments produced by the models are semantically plausible, even if they are considered incorrect in the leave-one-out test. Consider for example the LU guerrilla, assigned in FrameNet to the frame PEOPLE BY VOCATION. Our mixed model proposes as two most similar frames MILITARY and TERRORISM, which could still be considered plausible assignment. The same holds for the LU caravan, for which the most similar frame is VEHICLE, while in FrameNet the LU is assigned only to the frame BUILDINGS. These cases are due to the low FrameNet coverage, i. e LUs are not fully annotated and they appear only in a subset of their potential frames. The real accuracy of our models is therefore expected to be higher.",
        "GPT2_formal_text": "(t) = [t_i] for i = 1 to n-1. Formal: To make sure we cover all the different types of relationships, we make sure each type of relation has a unique id, which we write as r_i = [r_i1, ..., r_iL]. For example, for the relation \"parent of,\" we write it as r_p = [r_p1, ..., r_pL], where p1 is the parent and pL are the kids. Formal: We start by using the main template (T_s) to pick the most likely type of relation and then go through the sub-template (T_s1...T_sL) to pick the sub-type that matches best. Formal: To find the best possible sub-type, we look at the words in both the main and sub-template templates and see how likely each word is to be the right answer. Formal: Since the main template is kind of like a dictionary, we can also use the sub-template to get the best possible type for the relation. Formal: The main template and the sub-template together give us the overall prediction for the relation. Formal: The result of the main template and the sub-template is the predicted relation type. Formal: After that, we calculate the best possible type for the relation. Formal: For each relation type r_i, the best possible type of relation t_i is T_s(r_i) = T_s1...T_sL, where s1, s2, ..., sL are the words in the main template and the sub-template that describe the relation, respectively. Formal: We use the sub-template to get the best possible type for the relation. Formal: Finally, we combine the final predicted relation type (T_s) with the predicted sub-type (T_s1...T_sL) and use the output from that combined expression to get the prediction for the relation type. Formal: Formal: We calculate the overall prediction for the relation type r_i using the predicted sub-type (T_s) and the sub-template (T_s1...T_sL). Formal: Finally, we calculate the best possible type for the relation type r_i using the predicted main template (T_s). Formal: Formal: Finally, we",
        "directory": "D08",
        "filename": "D08-1048.json"
    },
    {
        "casual_text": "Microsoft's statistical MT engine has two types of decoders: one that uses a parser to create dependency treelets, which is helpful for translating between languages with different word orders. Then there's a simpler string-based decoder that doesn't need any fancy linguistic info to work. This one's great for quickly training on language pairs when you don't have a parser handy.",
        "formal_text": "Microsoft's statistical MT engine uses two different decoders: A syntactically informed tree based decoder, which uses a parser building dependency treeletsbetter for translating between languages with different word orders. And a string-based decoder that needs no linguistic information to work. Good for fast training of language pairs without a parser.",
        "GPT2_formal_text": "_x is like a special version of attention_mask_y, which is also called attention_mask_x. Formal: The task is to figure out if a tweet has an opinion tweet (like \"I really dislike that restaurant\"), if it's neutral, and if it has a neutral tweet at all. Formal: For the objective function, we use the log loss (which is L s, n) to calculate the loss. The gradient is set to 0.3, and the weight decay is 0.5. Formal: We're using the fastText library (from Bojanowski et al., 2017) for both our NMT models. We start by preprocessing the tweets by turning them into text, skipping punctuation. We also turn off punctuation to prevent any issues with noisy input. \n\nFor the context-aware model, we add the word \"angry\" to each tweet. We do this to make the tweet seem more varied, which helps the model figure out the topic better. Lastly, we use the ROCStories dataset (Hodosh et al., 2014) to train our NMT model. This dataset has both the original and negative tweets along with their generated versions, so we can train the model on both types of data together. Formal: Here's what each formula in the algorithm does. The first part is about extracting the sentiment polarity. Formal: The bias_tweets formula helps the model figure out the sentiment sentiment distribution by looking at the tweets. Formal: The attention_tweets formula helps the model understand the context by checking the tweet. Formal: The sentiment_score formula compares the sentiment polarity and the sentiment score. Formal: The attention_score formula compares the sentiment polarity and the attention score. Formal: The weight_tweets formula determines the importance of the attention in the model. Formal: The weight_tweets formula determines the importance of the attention in the model. Formal: The loss_tweets formula decreases the model's loss to make the model more stable. Formal: The loss_tweets formula decreases the model's loss to make the model more stable. Formal: The attention_score formula decreases the model's attention score to make the model more informative. Formal: Formal: The bias_tweets formula keeps the model's attention focused on the tweet. Formal: Formal: The bias_tweets formula keeps the",
        "directory": "amta",
        "filename": "2010.amta-commercial.10.json"
    },
    {
        "casual_text": "The weights in the logistic regression model, as shown in Table 1, help us predict how the classifier will behave with new data. When we look at the features, specifically the one about matching syntactic paths and the one that checks if the first mention is in the subject position, we notice both have positive weights. However, the first feature (syntactic path match) is stronger than the second (subject position), which tells us that parallel roles seem to have a bigger effect than just the subject position of the antecedent. Based on this, we can guess that the Subject Assignment Strategy is at play, but it’s paired with the Parallel Function Strategy. And if the Parallel Function Strategy comes into play, it seems to have a stronger pull and might even override the Subject Assignment Strategy.",
        "formal_text": "The weights of the logistic regression model in Table 1 predict the preferences the classifier will show on experimental data. Looking at the feature of syntactic path match and the feature that checks if the first mention is in the subject position, we see that both features have a positive weight; but we can also see that the first is stronger than the second, suggesting that parallel roles are of a greater impact than the subject position of the antecedent. From this data we can hypothesize that the Subject Assignment Strategy exists alongside the Parallel Function Strategy, and that the Parallel Function Strategy, if applicable, has a stronger influence that can overrule the Subject Assignment Strategy.",
        "GPT2_formal_text": "= {'q': q, 'a': a, 'i': i, 'r': r} Formal: The target sentences get a score of 1. This score tells us how good the translation is. Formal: For both Viterbi and Batch, we use a single-layer perceptron to figure out the score for the whole sentence. Formal: We came up with three ways to pick the most likely translation from all possible options. Formal: We tweaked the BPE model to use a hierarchical attention mechanism, where the size of the hierarchical attention grows with the probability p_h(w|x). The weighted average of the scores from each layer is what we call the \"rank\" of the sentence. Formal: This method of scoring sentences with multiple target sentences has been used before, like in studies by Kudo and Okumura (2017) and Niwa et al. (2018). Formal: This method lets us add in a bunch of different reference translations, like reference 1, reference 2, and so on, until we reach the maximum number of possible translations. Formal: We trained the model using 10^-7 probability to make it work better. Formal: We added this extra hyperparameter to adjust the weight of the query. Formal: We also added a penalty term to help the model learn better. Formal: We built the model to fit a specific setup, specifically the K-Means algorithm. Formal: We tested how it did by doing classification tasks in the same area. Formal: We've included the results for different hyperparameters, the results of which are in Table 1. Formal: For Viterbi and Batch, we calculated the average of the scores from each layer. Formal: We trained the model using 10^-7 probability to maximize performance. Formal: We found that using this hyperparameter helps the model learn better. Formal: We tweaked the BPE model to use a hierarchical attention mechanism, where the size of the hierarchical attention grows with the probability p_h(w|x). Formal: We used the top-k predictions from the model to calculate the rank of the sentence. Formal: We trained the model using 10^-7 probability to maximize performance. Formal: We tweaked the BPE model to use a hierarchical attention mechanism, where the size of the hierarchical attention grows with the probability p_h(w|x). Formal: We used the top-",
        "directory": "E17",
        "filename": "E17-4006.json"
    },
    {
        "casual_text": "There are a few ways to normalize matrix D1. One common method is called Sinkhorn balancing, which was introduced by Sinkhorn in 1964. This technique turns a square matrix with non-negative elements into a doubly stochastic matrix, meaning both the rows and columns add up to 1.\n\nSinkhorn balancing works through an iterative process. At each step, it calculates the sums of the rows and columns, then uses those sums to rescale the matrix. To balance matrix A, each iteration involves two updates: A gets multiplied by WR and then by WC. WR is a diagonal matrix with the inverse of the row sums of A, and WC is similar but with the inverse of the column sums.\n\nThis algorithm converges linearly, so it might take a lot of iterations to get the job done. You can tweak it to normalize the rows and columns using any norm you like. From what we've seen, normalizing D1 using the Euclidean norm works pretty well in real-world applications.",
        "formal_text": "There are several possibilities for normalizing the matrix D 1. Sinkhorn balancing (Sinkhorn 1964 ) is a popular technique for creating a doubly stochastic matrix (rows and columns all sum to 1) from a square matrix of nonnegative elements. Sinkhorn balancing is an iterative algorithm in which, at each step, the row and column sums are computed and then subsequently used to rescale the matrix. For balancing the matrix A, each iteration consists of two updates A W R A A AW C where W R is a diagonal matrix containing the inverse of row sums of A, and W C is a diagonal matrix containing the inverse of column sums of A. This algorithm exhibits linear convergence, so many iterations may be needed. The algorithm may be adapted for normalizing the row and column vectors according to any norm. Our experience has shown that normalizing D 1 with respect to the Euclidean norm works well in practice.",
        "GPT2_formal_text": "ed.lstm, and if the model still has the same hidden state, it'll just update the hidden state. Formal: You can find the code for this over at https://github.com/matthewdolan/Attention-Masked. Formal: For each word in the input sentence, we pick one word at a time, called w_i. Then, we take each word in the output sentence and compare it to the chosen word. If w_i is the chosen word, we say the system correctly identified the word. Formal: We use a set of templates to guide the attention structure, which you can check out in the next section. Formal: We use the MT system to predict the attention vector for the i-th token in the input sentence. Formal: Finally, we update the hidden state of the model by summing up all the attention vectors from the last hidden state. Formal: We tweak the attention distribution vector so it focuses on the most important token in the input sentence. Formal: We add a linear layer between the input sentence and the output word, using the word embeddings from the last layer. Formal: We train the model using these parameters: Formal: We do the full pre-training using the MT-pretrained model (check out the last section for more details). Formal: We pick a learning rate of 0.1 using the Adam optimizer (props to Kingma and Ba, 2015). Formal: We use a linear layer with a window size of 2. Formal: We apply a linear layer to update the hidden state of the model. Formal: Finally, we optimize the model using the gradient descent algorithm. Formal: We test our method on the standard NER task, which has a lot of vocabulary and lots of noise. Formal: Our approach can be used in other NLP tasks too. Formal: We tested our approach on the CoNLL-2014 NER task and the IWSLT 2014 CoNLL-2014 NER task. Formal: We're sharing our code, data, and results for everyone to use. Formal: For more details on the data, analysis, and experiments, check out Appendix A. Formal: For comparison, we looked at two basic models: linear embeddings and GCN models. Formal: We combined the generated tokens into a fixed-length vector using word2vec with 300 dimensions. Formal:",
        "directory": "C08",
        "filename": "C08-1007.json"
    },
    {
        "casual_text": "We're using an NVIDIA GeForce RTX 2080 to train the smaller versions of GPT-2 and BERT base, and a GeForce RTX 8000 for training the medium GPT-2. For BERT large, we're using Tesla M40 GPUs. Since we're starting with pre-trained models, we can wrap up training each one in about two weeks. It takes just one GPU for GPT-2 Small, BERT base, and GPT-2 Medium, but we need four GPUs for training BERT large.",
        "formal_text": "We use NVIDIA GeForce RTX 2080 for training GPT-2 Small and BERT base, GeForce RTX 8000 for training GPT-2 Medium, Tesla M40 for training BERT large. Since we start from the pretrained LM, we can finish training each LM within 2 weeks using 1 GPU for GPT-2 Small, BERT base, and GPT-2 Medium, and using 4 GPUs for training BERT large.",
        "GPT2_formal_text": "= (chat_id, question, slot_type) response_ids = (response, question, slot_type) for each pair in dialog. To make the whole process smoother, we add an attenuation factor called β to control how much the model learns from the entire conversation. Formal: To make sure we get the best possible (and most accurate) results, we set the α, β values to 0.9 and 0.5, respectively. Formal: We ran a bunch of tests to check how well our method works. Formal: We picked a few questions to work with: Formal: For each pair of responses, we calculated a weighted average score for the response and the question using the Dialogue act dataset (see Section 2.2). Formal: We then averaged these scores across the entire dialogue. Formal: We also calculated the average score for each pair of responses to see how well they were connected. Formal: For each pair of answers, we calculated the mean reciprocal rank correlation (MRR) for the correct answer and the correct question, using the Dialogue act dataset (see Section 2.2). Formal: Then, we averaged these rankings across the entire dialogue. Formal: Finally, we calculated the correlation between each answer and each question, using the Dialogue act dataset (see Section 2.2). Formal: The results for each dialog state are shown in Figure 3. For the first two dialogues, we had to pick a response because no question had an answer. Formal: For each pair of responses, we calculated the average score for the response and the question using the Dialogue act dataset (see Section 2.2). Formal: We also averaged these scores across the entire dialogue. Formal: For each pair of answers, we calculated the mean reciprocal rank correlation (MRR) for the correct answer and the correct question, using the Dialogue act dataset (see Section 2.2). Formal: We also averaged these rankings across the entire dialogue. Formal: The results for each dialog state are shown in Figure 3. Formal: We tested our method against some popular datasets like DialoGPT and DialogueQA. Formal: We ran three experiments to see how well our method worked on different datasets. Formal: We compared the results from the human evaluation (in Section 5.1) with the results from the experiments with the DialogueQA dataset (in Section 5.2). Formal: Lastly, we compared the results from the human evaluation",
        "directory": "acl",
        "filename": "2022.acl-long.554.json"
    },
    {
        "casual_text": "So, basically, DATR theories can work as a kind of dictionary for a PATR system. In this setup, the words (or lexemes) are like DATR nodes, and when you ask about them, you get back these feature structures. In a dictionary set up like this (like in example 6), you can see three types of nodes:\n\n1. The main ones, like FROG, which are the actual words.\n2. Nodes like SYNTAX or NV, which are like the attributes in the PATR system.\n3. Nodes like NOUN or LEX/CAL, which are more like categories or types.\n\nThe words get their general info from these type nodes, but the specific details about each word are directly linked to the main nodes.",
        "formal_text": "It has been shown above that DATR theories can serve as a lexicon for a PAT R system where the lexemes are represented as DATR nodes and the returned values of queries are the corresponding feature structures. In a lexicon which is formulated as in (6), aparl; from the lexical nodes (i. e. nodes like FROG which define lexemes) two other kinds of nodes can be distinguished: nodes like SYNTAX or NV, which correspond to PATR attributes, and nodes like NOUN or LEX/CAL, which represent a kind of type information (see Pollard/Sag, 1987). The lexemes inherit this information through reference to the type nodes, while the lexeme-specific information is as~ciated direct. ly with the lexical nodes.",
        "GPT2_formal_text": "_x. It's calculated like this: Formal: We've got this objective function we want to minimize, and here's how we do it: Formal: We use this equation to figure out the cross-entropy loss. Formal: We build a model that can predict how well an input token will be masked. This model is trained using a dataset of masked words. Formal: We use a set of input features to create a prediction for each token. Formal: We find the log-probability for the masked word, which we call the z_m loss. Formal: We update the model by using a cross-entropy loss. Formal: We combine the input features with the prediction z_m loss and also add the masked word's prediction to it. Formal: After that, we update the model by adding the original input features plus the predicted mask x. Formal: Finally, we pick the tag sequence t with the highest probability from the output set. Formal: We put all this together in a formula that looks like this: Formal: Here's a quick example of how we calculate the cross-entropy loss and the prediction z_m loss for a sentence made up of tokens and their labels. Formal: Formal: Formal: To make sure we get the best cross-entropy loss, we use a grid search to find the best tag sequence. Formal: Formal: We start by randomly picking a sequence of words. Then, we calculate the cross-entropy loss for each token and label pair. Formal: Formal: We train the model using a dataset of masked words and a masked word prediction using a grid search. Formal: Formal: Formal: Formal: Finally, we pick the label sequence t with the highest probability from the output set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E91",
        "filename": "E91-1024.json"
    },
    {
        "casual_text": "Vocabulary is something we need to keep an eye on when working with test data. TSNI, P does this by limiting both the size and the scope of the vocabulary. It tries to steer clear of words that are tricky to categorize or have multiple meanings, unless the test specifically wants to check how people handle that kind of ambiguity.",
        "formal_text": "Vocabulary is an aspect of the test data that needs to be controlled. TSNI, P achieves this by restricting the vocabulary in size as well as in domain. Categorially and semantically ambiguous words are avoided where possible and only included when ambiguity is explicitly tested for.",
        "GPT2_formal_text": ") = {(u, v) in (u, v) | u == v}. Formal: You can define the weight vector for a specific token, y_i, by adding up the expected counts for the whole sentence, x_i. Formal: For the first loss, δ_i, we set a threshold, λ_i, that we can tweak during training. Formal: We also add a step in between the prediction and the actual search to create a smoothed negative log-likelihood loss. Formal: We use Adam (from Kingma and Ba, 2014) to optimize the loss, specifically the worst-case scenario, and set the learning rate to 0.0001 for the search and 1.0 for the actual search. Formal: We're using the lwtf2 package, version 1.7.1, to convert the input text into word embeddings, and then we convert that back to text to train our model. Formal: The token and span embeddings are calculated using the recommended method from the WMT 2016 training dataset (thanks to Zanzotto et al., 2016). Formal: To get the final predicted output, we do a 1-to-1 comparison of the input text. Formal: For the training corpus, we set the token embedding dimensions to 8, and we assign a span embedding dimension of 64. Formal: We train the model using the development set, which includes the test set, the development set, and the test set. Formal: We train the model with a batch size of 32. Formal: We train the model with a learning rate of 0.001 for the final prediction. Formal: We apply the softmax function to get the predicted sequence length, which we also use to calculate the segment embedding dimension. Formal: Our final model, called M_DIST, is a mix of the original model M_DIST and M_DIST+1, where each dimension has a weight of 1. Formal: We also calculate the predicted sequence length by testing the model with a batch size of 32. Formal: For the loss functions, we use lwtf2 to convert the input text into word embeddings, and then we convert that back to text to train our model. Formal: Finally, we calculate the masked token and span embeddings by using the recommended method from the WMT 2016 training dataset (thanks to Zanzotto et",
        "directory": "C96",
        "filename": "C96-2120.json"
    },
    {
        "casual_text": "Okay, so we have this thing where w = w_1, ..., w_W. At a specific spot in the text, let's call it position i, the element d_i in d = d_1, ..., d_W tells us which document has the word w_i in it. And then there's this vector z = z_1, ..., z_W that kind of does something similar.",
        "formal_text": "w = w 1, • • •, w W. At corpus position i, the element d i in d = d 1, • • •, d W designates the document containing observed word w i. Similarly, the vector z = z 1, • • •, z W defines",
        "GPT2_formal_text": ", attention_op, and attention_size. Formal: We use the standard LSTM model setup with a hidden layer size of 768 and an embedding size of 512. The model was trained using the Adam optimizer with a learning rate of 0.001. Formal: We can compare the results using LSTM to the ones using neural networks, which are shown in Figure 2a. The graph also gives a quick summary of how well the model does on the CoNLL-2009 dataset. Formal: Using the decoder. We start by encoding the input text using the encoder from Section 2. Then, we use this encoded text to fill in the tokens in the target text. Formal: The decoder is trained using the joint model from Section 2. Formal: Here's how we handle tokenization and prediction for the input sentence: Formal: We use the standard LSTM model with a hidden layer size of 768 and an embedding size of 512. The model was trained with the Adam optimizer with a learning rate of 0.001. Formal: We compare the results using LSTM to the ones using neural networks, which are shown in Figure 2b. The graph also gives a quick summary of how well the model does on the CoNLL-2009 dataset. Formal: We can use the decoder to tokenize and predict for the input sentence. Formal: Here's how we handle tokenization and prediction for the input sentence. Formal: We use the standard LSTM model with a hidden layer size of 768 and an embedding size of 512. The model was trained with the Adam optimizer with a learning rate of 0.001. Formal: We compare the results using LSTM to the ones using neural networks, which are shown in Figure 2c. The graph also gives a quick summary of how well the model does on the CoNLL-2009 dataset. Formal: Finally, we can use the decoder to tokenize and predict for the input sentence. Formal: Here's how we handle tokenization and prediction for the input sentence. Formal: We use the standard LSTM model with a hidden layer size of 768 and an embedding size of 512. The model was trained with the Adam optimizer with a learning rate of 0.001. Formal: We compare the results using LSTM to the ones using neural networks, which are shown in Figure 2d. Formal: Lastly, we can",
        "directory": "C12",
        "filename": "C12-1182.json"
    },
    {
        "casual_text": "Coecke and his team (2010) used category theory to give distributional models of meaning a way to combine things, making sure that each grammatical step has a matching mathematical operation. Basically, if you have a sentence s = w1 w2 • • • wn, there's a linear map f that takes the context vectors of the individual words and combines them into a single vector representing the whole sentence.",
        "formal_text": "Using the abstract framework of category theory, Coecke et al. (2010) equip the distributional models of meaning with compositionality in a way that every grammatical reduction is in one-to-one correspondence with a linear map defining mathematical manipulations between vector spaces. In other words, given a sentence s = w 1 w 2 • • • w n there exists a syntax-driven linear map f from the context vectors of the individual words to a vector for the whole sentence:",
        "GPT2_formal_text": ") as a key to help the model figure out the right answer. Formal: To train the classifier using each hidden state h_i from the encoder, we calculate the cross-entropy loss for the whole dataset. Formal: The classifier is trained by pulling out the best features from the retrieved data, which is all the data that doesn't have any of the features we're looking for. Formal: The loss for this multi-class classification thing is calculated by dividing the correct label y_i by the total number of labels in the dataset. Formal: We also want to use the embeddings for both the loss and the feature extractor. In our experiments, we set the embedding dimension to 20. Formal: We use a logistic function to make sure the classifier's predictions are valid. Formal: We combine the classifier's predictions with the classifier's feature extractor to get the final label. Formal: Here's how we train the classifier: Formal: We train the classifier using a discriminative learning approach. We create cross-entropy losses for the target classes and for the classifier itself. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach. Formal: We train the classifier using a discriminative learning approach",
        "directory": "C12",
        "filename": "C12-2054.json"
    },
    {
        "casual_text": "Okay, so for each item i in T, where i is between 1 and the total number of items in T:\n\n1. Set up the sentiment classifier SC (0) using the data from Ω ij, and make sure j is randomized from 1 to the total number of items in Ω i, with i still being between 1 and the total number in T.",
        "formal_text": "s i in T, 1 ≤ i ≤ |T | 2: Initialize sentiment classifier SC (0) based on cf e(Ω ij ), randomize j ∈ [1, |Ω i |], 1 ≤ i ≤ |T | 3:",
        "GPT2_formal_text": "ing to the actual masking vector and the inner product, which we call h_i. Formal: For each word, we look at the tokens nearby, p_t_i and p_t_i+1, and we calculate the local attention scores for each token by averaging the log-likelihoods for these tokens. Formal: The attention weights for the whole sentence are calculated by averaging the attention weights for each token in the sentence. Formal: We use a linear layer to encode the sentence. We also calculate the cosine similarity between the hidden representations of the token in the input and the token in the output. Formal: The hidden states for the token are represented as h_i = h_i + b_i. Formal: The input and output representations are combined using a nonlinear function. Formal: We use the input (x_i) and output (y_i) as input. Formal: If the input is 0, we start by calculating the representation for the token. Formal: If the input is 1, the token's representation is calculated using the entire input. Formal: We then calculate the representation for the entire input. Formal: Finally, the representation for the whole input is calculated using the representation for the token. Formal: This process is repeated for each token in the sentence. Formal: We also calculate the cosine similarity between the hidden representations of the input and the token. Formal: We use the input's representation as input. Formal: If the input is 0, the input's representation is calculated using the input's representation. Formal: If the input is 1, the input's representation is calculated using the input's representation. Formal: We then calculate the representation for the entire input. Formal: Finally, the representation for the entire input is calculated using the representation for the token. Formal: We then calculate the representation for the whole input. Formal: Lastly, the representation for the whole input is calculated using the representation for the token. Formal: This process is repeated for each token in the sentence. Formal: If the input is 0, the input's representation is calculated using the input's representation. Formal: If the input is 1, the input's representation is calculated using the input's representation. Formal: If the input is 2, the input's representation is calculated using the input's representation. Formal: If the input is 3, the input's representation is calculated using",
        "directory": "D14",
        "filename": "D14-1054.json"
    },
    {
        "casual_text": "We'll start by looking at the frames that have been used most recently and also the ones linked to noun phrases that refer back to something earlier in the sentence.",
        "formal_text": "Those frames most recently used and those which a~e associated with anaphoric noun phrases within the sentence are examined first.",
        "GPT2_formal_text": "= {'FirstName', 'LastName', 'Address', 'City', 'State', 'ZipCode', 'Postcode', 'Phone', 'E-Mail', 'City', 'State', 'ZipCode', 'Postcode', 'Postcode2', 'Postcode3', 'City', 'State', 'ZipCode', 'Postcode2', 'Postcode3', 'Postcode4', 'City', 'State', 'ZipCode', 'Postcode2', 'Postcode3', 'Postcode4'}};\n\nAdditionally, we add another row, called `[Entity]`, that holds the entity type. This is just a list of text labels like `[Entity]`, where each label is a name like `Person`, `Animal`, or `Product`. Formal: We use a list of text labels to represent each entity type. This list is made up of the text labels for each entity type. Formal: We combine these text labels using a linear transformation, with `r` as the size of this list. Formal: The entity type information is pulled out by looking at each entity type in the text. Formal: We use a linear transformation, which takes into account the size of the list and the size of each entity type. Formal: Formal: Lastly, we calculate the attention weights by using a logistic regression classifier. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C82",
        "filename": "C82-1011.json"
    },
    {
        "casual_text": "A lot of research about evaluating machine translation (MT) focuses on identifying and categorizing MT mistakes. For example, studies like those by Vilar et al. (2006), Farrús et al. (2010), Stymne and Ahrenberg (2012), Lommel et al. (2014), and Klubička et al. (2018) have done this. However, not many papers look at how people actually perceive these errors, and none of them really nail down what exactly makes a translation good or bad.\n\nKirchhoff et al. (2014) took a different approach by using conjoint analysis to figure out what users prefer when it comes to MT errors. They started by labeling the errors in MT translations and then showed different versions with various types of errors to people who evaluated them. The evaluators were asked to pick the MT output they liked best and explain why.\n\nOne interesting finding was that the frequency of certain error types didn’t match up with what users preferred. For instance, word order errors were the least liked, even though they happened less often. After that came word sense errors (like ambiguity), then morphological errors (which were the most common). On the other hand, errors with function words were the least annoying to people.",
        "formal_text": "Lot of research on MT evaluation deals with classification and analysis of MT errors, for example (Vilar et al. , 2006; Farrús et al. , 2010; Stymne and Ahrenberg, 2012; Lommel et al. , 2014; Klubička et al. , 2018). Few papers deal with human perception of these errors, but neither of them defines precisely which criterion is the translation quality based on. Kirchhoff et al. (2014) uses conjoint analysis to investigate user preferences for error types of SMT systems. First, the errors in MT outputs were annotated, and then MT outputs with different error types were given to the crowd evaluators. They were asked to choose the MT output which they like best and to give the reason for their preference. One of the findings is that the frequencies of error types are not related to the user preferences. The most dispreferred error type was word order error, although it was the least frequent one. It was followed by word sense errors (ambiguity), then morphological errors (most frequent ones), whereas errors in function words were the most tolerable.",
        "GPT2_formal_text": "ed_x. Formal: In this paper, we use a list of character n-grams that are either marked as 1 or 2 to represent a span. Formal: We create a sentence for each span by picking a set of n-grams from the vocabulary of the first word in the input sentence. Formal: We make a list of character n-grams for each span to represent the span. Formal: For each input sentence x, we start with a sentence with the n-grams from the vocabulary V of the first word in it. Formal: We grab all the character n-grams from the first word of the input sentence. Formal: We then use an attention mechanism (like the one in Bahdanau et al., 2014) to focus on the most important n-grams, adding them to the list of character n-grams for the span. Formal: We then calculate the normalized similarity between the n-grams in the input sentence and the n-grams in the span. Formal: We then check how similar the two n-grams are using a similarity measure, which is the average similarity between the representation vectors for the input and the span. Formal: We do the same thing for each input sentence x to get the normalized similarity between the n-grams in the input and the n-grams in the span. Formal: Finally, we pick the n-grams that show the best match to represent the span. Formal: We do the same for each input sentence x to get the normalized similarity between the n-grams in the input and the n-grams in the span. Formal: Finally, we pick the n-grams that show the best match to represent the span. Formal: We do the same for each input sentence x to get the normalized similarity between the n-grams in the input and the n-grams in the span. Formal: We do the same for each input sentence x to get the normalized similarity between the n-grams in the input and the n-grams in the span. Formal: We do the same for each input sentence x to get the normalized similarity between the n-grams in the input and the n-grams in the span. Formal: Formal: Formal: We do the same for each input sentence x to get the normalized similarity between the n-grams in the input and the n-",
        "directory": "conll",
        "filename": "2020.conll-1.19.json"
    },
    {
        "casual_text": "The model's performance really depends on the set of permutations used during training. We don’t know exactly how B&L came up with their permutations, but we’re guessing they were generated completely randomly.",
        "formal_text": "The quality of the model learned depends on the set of permutations used in training. We are not aware of how B&L's permutations were generated, but we assume they are generated in a perfectly random fashion.",
        "GPT2_formal_text": "= { h_i } n. For each i from 1 to m, we grab the top 50 sentences that mention both the entity and its category (like \"Madden\" or \"NBA Champions\"). Formal: The i-th sentence in the corpus is linked to the category of the entity in that sentence. We use a standard sentence-level BiLSTM setup with three hidden layers (H_t, H_a, and H_b) to process all the data. Formal: We're using the latest pre-trained transformer model (like the one by Vaswani et al., 2017) and the Adam optimizer (from Kingma and Ba, 2015) to optimize the parameters of the model. Formal: We’ve tweaked the pre-trained transformer model (Vaswani et al., 2017) and the Adam optimizer (Kingma and Ba, 2015) so that they can learn both entity and category embeddings, working together. We call this updated model the VAG-NET. We train it using a mixture model, following the method by Chen et al. (2019). Formal: Lastly, we figure out the likelihood of the sequence of labels y, which can be written like this: Formal: The probability of y is calculated by multiplying a sigmoid function with a uniform distribution. Formal: We feed the model the sequence of labels to get the probability, p(y|x). Formal: Using a weighted sum (we call this the w-weight), the model can make better use of the labels during training. Formal: In Appendix A, we show that our model works better than the original Bi-LSTM-based model. Formal: The weight w can be adjusted based on the specific task you’re working on. Formal: We set the batch size to 16. Formal: Training for 15 epochs will take around 300,000 steps. Formal: For each epoch, we update the model by using a Monte-Carlo simulation and then pick the model with the highest probability. Formal: We use the train set to build the model’s parameters, as shown in Table 1. Formal: We randomly initialize the label embeddings for each sentence in the training set. Formal: The embeddings for each label are randomly initialized as follows: Formal: W_e, W_e', W_a', W_b', and w are randomly initialized.",
        "directory": "E12",
        "filename": "E12-1032.json"
    },
    {
        "casual_text": "We need to understand x_i. So, we suggest replacing 3. This goal is a bit different from the minimum risk training method used by Li and Eisner (2009), and it's a difference that matters. In both approaches, θ* aims to minimize risk or expected loss. However, the expectation is calculated with respect to different distributions: in Li and Eisner (2009), it's based on the conditional distribution p(y | x), whereas in our case (1), it's based on the joint distribution p(x, y).",
        "formal_text": "requires to know x i. So we propose to replace 3 This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. In both cases, θ * minimizes risk or expected loss, but the expectation is w. r. t. different distributions: the expectation in Li and Eisner (2009) is under the conditional distribution p(y | x), while the expectation in (1) is under the joint distribution p(x, y).",
        "GPT2_formal_text": "_len(d_i) = [1; len(d_i)] let D_k(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_c(n) = [1; n] let D_k(n) = [1; n] let D_d(n) = [1; n] let D_k(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_k(n) = [1; n] let D_d(n) = [1; n] let D_k(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_k(n) = [1; n] let D_d(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_k(n) = [1; n] let D_d(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_k(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_k(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_c(n) = [1; n] let D_d(n) = [1; n] let D_k(n) = [1; n] let D_d(n) = [1; n] let",
        "directory": "D11",
        "filename": "D11-1085.json"
    },
    {
        "casual_text": "We're sharing the macro-averaged F1 scores for pronoun translation, which we got using a simplified version of AutoPRF (a method by Hardmeier and Federico from 2010). Basically, for each sentence in our test set, we look at the pronouns in the system's translation and compare them to the ones in the reference translation. We adjust the counts based on what's in the reference, and then use those numbers to calculate precision, recall, and F1 scores.",
        "formal_text": "Evaluation. We report the macro-averaged F1 scores of the pronoun translation based on a simplified version of AutoPRF (Hardmeier and Federico, 2010). For each sentence in the testset, the counts of the pronouns in the system translation are clipped based on the pronouns in the reference translation; these counts are then used to compute the precision, recall and F1 scores.",
        "GPT2_formal_text": "= { SELECTOR (1, {'p_i'}, NULL, {'f_i'}), SELECTOR (2, {'p_i'}, NULL, {'f_i'}), ... }, which we can figure out by running the model on the training set and checking the results. Formal: This whole setup is laid out in Algorithm 1. Formal: We figure out the final values for the output variables using a method called \"hardmax,\" which is explained in Algorithm 2. Formal: For each label y that's not aligned with any training examples, we create a new representation called y_i, which is the cross-entropy between y and y_i. Formal: We update the labeled data x_i using a method called \"softmax.\" Formal: We also add a bias term to make the model work better by minimizing the cross-entropy between the predicted and actual labels, respectively. Formal: Lastly, we use a linear regression model to predict the label y. Formal: Formal: We use the current output variable x_i and the current output variable y to predict the label y_i. Formal: We also update the labeled data x_i using a method called \"softmax.\" Formal: We use the current output variable x_i and the current output variable y to predict the label y_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.177.json"
    },
    {
        "casual_text": "• Users. This tab lets the project manager create accounts for people and assign them to specific tasks. Each person will only see the tasks they’ve been assigned to. Users can’t see other people’s annotations unless they’re in \"revision mode,\" which shows an existing annotation for them to review.",
        "formal_text": "• Users. In this tab the project manager can create accounts for users and assign them to different tasks. Each user will see only the task(s) s/he has been assigned to. Users do not see other users' annotations unless they are working in \"revision mode\", where an existing annotation is presented for revision.",
        "GPT2_formal_text": "ed(u) is just a special number we use to turn each input into a kind of code (which is the main thing). Formal: • BERT (Devlin et al., 2018) is a BERT model that uses an attention mechanism, specifically from Sutskever et al. (2014). Formal: • ELMo (Peters et al., 2018) is a vector-based model that was introduced by Peters et al. (2018) and adds a linear layer. Formal: • ELMo (Peters et al., 2018) also has a linear layer, but it’s set up differently. It uses some non-linear stuff to work with a more complicated data structure. Formal: • LSTM (Hochreiter and Schmidhuber, 1997) is a neural network designed for text data that uses a bidirectional LSTM network. Formal: • FastText (Bojanowski et al., 2018) is a 3-gram language model that’s been trained on a big collection of text and is way faster than other methods. Formal: • FastText (Bojanowski et al., 2018) is also trained on a ton of text, so it’s a lot faster than other methods. Formal: Formal: • BERT-base (Devlin et al., 2018) is another BERT model trained on a ton of data, and it’s pretty close to FastText. Formal: • FastBERT (Lewis et al., 2019) is the FastBERT that uses two layers of self-attention. Formal: • FastBERT-base (Lewis et al., 2019) is the FastBERT-base that uses three layers of self-attention. Formal: • FastBERT-fine (Lewis et al., 2019) is the FastBERT-fine that uses four layers of self-attention. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C14",
        "filename": "C14-2026.json"
    },
    {
        "casual_text": "For ACSA, we make templates by hand. These templates have one spot for the category and another for the sentiment type (like positive or negative). We have a set of category words, A = {a1, ..., a|C|}, where |C| is the number of category types (for example, a1 could be \"price\"). We also have a set of polarity words, P = {p1, ..., p|L|}, where |L| is the number of polarity types (like p1 being \"positive\"). We use these words to create templates like \"The sentiment polarity of price is positive\". The basic template is \"The sentiment polarity of ai is pk\". So, for any category ai, we can make a bunch of these templates.",
        "formal_text": "For ACSA, we manually create templates containing one slot for the given_category and another slot for the polarity_type label. We set a category word set A = {a 1, . . . , a |C| }, |C| is the category type size (e. g. , a i =\"price\") and polarity type word set P = {p 1, . . . , p |L| }, |L| is the polarity type size (e. g. , p k =\"positive\"), and use words to define templates T a i, p k (e. g. \"The sentiment polarity of price is positive\"). The template T is \"The sentiment polarity of a i is p k \". For a given category a i, we can obtain a list of templates",
        "GPT2_formal_text": ", items, and edges. Formal: For each entity e, the goal is to figure out the type of attribute s_p by analyzing the pair (e_p, e_t). Formal: We use a convolutional neural network (CNN) to grab features from both the input and output vectors at the same time. Formal: We’ve added a linear layer on top of the convolutional layer, which helps the model focus on the important features. \n\nNow, let’s look at how well each model does by looking at the micro-averaged F1 scores, which we’ll call F1 for short. The dataset we’re using is called dataset 3. Formal: We’re using these scores to measure how well the model handles different entity types and types of relationships. Formal: The model’s performance is a good example of how using things like entity types and relationships can help improve how well the model works. Formal: We’re also sharing the average F1 scores for both supervised and unsupervised models. Formal: The unsupervised model does really well, scoring high on both F1 and KB scores. Formal: On the other hand, the supervised model shows a clear improvement over the unsupervised baseline. Formal: Formal: We also share the average F1 scores for the supervised model and the unsupervised one. Formal: Formal: Lastly, we calculate the cross-entropy loss for the unsupervised model to see how it’s doing overall. Formal: Formal: Formal: We’ve also included the raw F1 scores from the F1 model in Table 6, which you can see in Table 7. Formal: We also include the unsupervised baseline F1 score for comparison. Formal: Formal: Formal: Lastly, we calculate the cross-entropy loss for the supervised model to see how it’s doing overall. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.361.json"
    },
    {
        "casual_text": "Okay, let me break this down in simpler terms:\n\nx, y, and s IRF( ) are just the weight matrix, bias vector, and squashing function for the IRF layer at specific time steps x and y. Meanwhile, g 0x, y (τ ) = τ is just saying that τ is equal to itself. \n\nNow, w IRF(1) and b IRF(1) are the initial weight and bias vectors that are applied globally for the first layer of the IRF. These vectors are used to transform the scalar τ, and each of them is adjusted based on its own random effects. W just refers to the whole thing.",
        "formal_text": "x, y, and s IRF( ) are respectively the th IRF layer's weight matrix at predictor timestep x and response timestep y, bias vector at time x, y, and squashing function, and g 0x, y (τ ) = τ. w IRF(1), b IRF(1) are respectively globally applied initial weight and bias vectors for the first layer of the IRF, which transforms scalar τ, each of which is shifted by its corresponding random effects. W",
        "GPT2_formal_text": ", which is basically a matrix of size d_t, where the i-th row represents the attention weight vector for the i-th word in the input sequence. The whole setup is laid out in Figure 1. Formal: We start by creating a BERT-based model for the task. This model works with a sequence of tokens as input. The input for the BERT layer in this model is a sentence token x, and the output is the attention vector for that token. Formal: For each sentence s in our dataset D_s, we create an attention vector for the sentence, which we call h_s. Formal: We create two different attention vectors, x_t and x_s, for the token x_t and the word word w_t. Formal: These attention vectors, h_t and h_s, are then passed through a fully connected layer to get the final representation of the sentence, which we call h_s. Formal: Finally, we normalize this representation using a linear transformation, and this gives us a final hidden representation of the whole sentence, which we call h_t. Formal: Formal: Finally, we normalize this hidden representation using a linear transformation. Formal: Here, |z_s| represents the length of the sequence of tokens we're working with. Formal: The model's parameters for the masked language modeling task are adjusted based on these representations. Formal: Formal: The overall model architecture is shown in Figure 2. Formal: In the training phase, we start by adding a linear layer to the model's parameters, λ, to match the dimension of the vocabulary V. Formal: Then, we train the model using a softmax function on the token representations h_t and h_s to get the final attention weights. Formal: Finally, we normalize the final representations using a linear transformation, and this gives us the final attention weights for the sentence, which we call h_t. Formal: Formal: We use a batch size of 3 for training, and we update the model's parameters each time we make a prediction. Formal: Formal: To check how well our model is doing, we use the validation loss. Formal: Formal: We use a batch size of 3 for training, and we update the model's parameters each time we make a prediction. Formal: Formal: Formal: Finally, we normalize the final representations using",
        "directory": "acl",
        "filename": "2021.acl-long.288.json"
    },
    {
        "casual_text": "You can limit your search in different ways, like choosing a specific area to look in. And when you get the results, you can focus on certain parts of the information by picking specific fields from EURODICAUTOM.",
        "formal_text": "Queries can be restricted in a number of ways (e. g. by indicating domain). Output can be restricted by selecting specific EURODICAUTOM fields:",
        "GPT2_formal_text": ", key_ids, and values. The way we pick these keywords is guided by a learning rate algorithm that adjusts the weights based on the graph structure. Formal: Since we don’t have any predefined rules, the main idea behind this paper is to figure out how to guess the model’s parameters from the training data. To do this, we’re going to use some methods that could come in handy later. Formal: We’ll start with the main idea behind the Gibbs sampler we’re talking about, which is to keep the parameters of the model in check. Then, we’ll tweak it a bit to handle how many parameters are in the model. Formal: We’ll call the parameters of a model θ. Formal: From now on, the matrices A and B will be denoted as A, B, and θ. Formal: Next, we’ll calculate the minimum term (or gradient) for the model. Formal: If the loss function is defined like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "1998.eamt-1.2.json"
    },
    {
        "casual_text": "We also tried out some cutting-edge transformer-based seq2seq models, like Bert-SumExtAbs (Liu and Lapata, 2019) and BART (Lewis et al., 2019). Bert-SumExtAbs needs the encoder to be fine-tuned and the decoder to be trained from scratch, while BART just fine-tunes both the encoder and decoder. We only used AMI data for training and fine-tuning, and the results from these models are shown in the bottom two rows of Table 6. \n\nEven though our hier2hier t-learn model only needs the decoder to be fine-tuned and uses hierarchical attention, the more complex semi-supervised training of both the encoder and decoder in BART, along with its much larger size (100x), gives better performance. But, if you're working with limited memory, like on some mobile devices, our model might be a better choice. \n\nAlso, even though Bert-SumExtAbs has a pre-trained encoder, training a big decoder from scratch with just a small AMI dataset led to overfitting, which resulted in lower scores.",
        "formal_text": "We also experimented with state of the art transformer-based seq2seq models, e. g. , Bert-SumExtAbs (Liu and Lapata, 2019) and BART (Lewis et al. , 2019). BertSumExtAbs requires finetuning of the encoder and a de novo training of decoder while both encoder and decoder of BART are only fine-tuned. We use only AMI data for the de novo training and fine-tuning purpose, and the bottom two rows in Table 6 show the results from these models. 1 Although our hier2hier t-learn also only requires fine-tuning of the decoder and hierarchical attention, a highly-sophisticated semisupervised training of both the encoder and decoder of BART and larger model size (100x) yields better performance. However, for applications that have limited memory, as on some mobile devices, our model may be more desirable. Furthermore, despite a pre-trained encoder of BertSumExtAbs, a de novo training of a large size decoder with a tiny AMI data lead to over-fitting, and therefore, lower scores.",
        "GPT2_formal_text": "_ed_to = (0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0) Formal: This is the same as b_s minus the sum of the dot products of the w_i's for each i from 1 to N, plus the sum of the dot products of the w_i's for each j from 1 to J, plus the sum of the dot products of the w_i's for each k from 1 to K. Formal: This is the same as b_s minus the sum of the dot products of the w_i's for each i from 1 to N, plus the sum of the dot products of the w_i's for each j from 1 to K. Formal: This is the same as b_s minus the sum of the dot products of the w_i's for each j from 1 to N, plus the sum of the dot products of the w_i's for each k from 1 to K. Formal: This is the same as b_s minus the sum of the dot products of the w_i's for each j from 1 to N, plus the sum of the dot products of the w_i's for each k from 1 to K. Formal: This is the same as b_s minus the sum of the dot products of the w_i's for each k from 1 to K. Formal: This is the same as b_s minus the sum of the dot products of the w_i's for each i from 1 to N, plus the sum of the dot products of the w_i's for each j from 1 to K, plus the sum of the dot products of the w_i's for each k from 1 to K. Formal: This is the same as b_s minus the sum of the dot products of the w_i's for each j from 1 to N, plus the sum of the dot products of the w_i's for each k from 1 to K. Formal: This is the same as b_s minus the sum of the dot products of the w_i's for each j from",
        "directory": "adaptnlp",
        "filename": "2021.adaptnlp-1.24.json"
    },
    {
        "casual_text": "Basically, we used three different methods (MUSE, VecMap, and XLM-R) to train cross-lingual models. For MUSE and VecMap, we got the cross-lingual mapping from monolingual embeddings, like we mentioned earlier. This gave us thirteen models for each of those two methods. On the other hand, we only made one model using XLM-R for the third approach, which we’ve already explained. We take the last layer of the XLM-R model to create representations for each token.",
        "formal_text": "To put it more concisely, we trained cross-lingual models using three different methodologies (MUSE, VecMap and XLM-R) where the cross-lingual mapping obtained for MUSE and VecMap were generated via the monolingual embeddings, as described above. We obtained thirteen models using each of these two methods. A single cross-lingual model was, however, trained using XLM-R and used for the third cross-lingual approach whose training methodology has been described above. We utilize the last layer from the XLM-R model to generate representations for each token.",
        "GPT2_formal_text": "= { 'name1', 'name2', 'name3', ..., 'nameT' }  \n\nFor the sake of comparison, we've also included the results from the model shown in Figure 1. Formal: To check how well the model is doing, we look at the F1 score, which we calculate using the n-best list. Formal: Figure 2 shows a simple example of how our model performs for the same input, but with different random seeds. Formal: (1) If it's true that T equals T, then the model should do better than the one with the same seed. Formal: For any given seed, the model should perform at least as well as the one with the same seed. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.119.json"
    },
    {
        "casual_text": "Step 3 is all about checking if the marker from the prediction tree matches up with the elementary tree for \"open.\" To make parsing PLTAG easier and more efficient, Demberg and his team (2013) came up with the idea of \"fringes.\" Fringes help us understand that when building a tree step by step, you can only connect a prefix tree to an elementary tree at certain spots. \n\nFor example, in Figure 3, the prefix tree has two places where you can make a substitution, one for B and one for C. But if you try to substitute into B, you get a valid new prefix tree. If you substitute into C, you end up with the tree shown in Figure 3b, which isn't a valid prefix tree—it’s like trying to build the tree in a way that doesn’t work step by step.",
        "formal_text": "Step 3 involves the verification of the marker introduced by the prediction tree against the elementary tree for open. In order to efficiently parse PLTAG, Demberg et al. (2013) introduce the concept of fringes. Fringes capture the fact that in an incremental derivation, a prefix tree can only be combined with an elementary tree at a limited set of nodes. For instance, the prefix tree in Figure 3 has two substitution nodes, for B and C. However, only substitution into B leads to a valid new prefix tree; if we substitute into C, we obtain the tree in Figure 3b, which is not a valid prefix tree (i. e. , it represents a non-incremental derivation).",
        "GPT2_formal_text": "_tensor: (mask_tensor, h, g, t) = [mask_tensor(x_i) for x_i in x_train, mask_tensor(y_i) for y_i in y_train] [shape(tensor)] Formal: We use a linear layer to map the output into a normalized vector. Formal: After training, the attention is updated with the updated values, so the attention weights can be easily calculated. Formal: For the first step, we use cross-entropy loss to get the updated attention weights. Formal: To figure out the response vector, we use a one-layer bidirectional RNN model. Formal: We train a one-layer bidirectional RNN with a feature embedding layer. Formal: The response vector, h, is calculated using the hidden layer's outputs. Formal: For the second step, we use two-layer bidirectional RNN with L1-regularization. Formal: We train a two-layer bidirectional RNN with a feature embedding layer. Formal: The response vector, h_i, is learned from the feed-forward network. Formal: The model tries to find the best model based on the loss and the features we have. Formal: After using these two-layer models, the output vector, h_s, is trained using the cross-entropy loss, and we fine-tune the model using the standard cross-entropy loss. Formal: In the final step, we use two-layer bidirectional RNN with L1-regularization to create the response vector h_t. Formal: Formal: Finally, we combine the hidden layers' outputs to get the final response vector h_t. Formal: This whole process is repeated for all possible responses. Formal: A lot of researchers have looked into how to train neural networks for natural language understanding (NLU) using this method. For instance, (Yin et al., 2019b) introduced an RNN model to predict the probability of a response by looking at the context. On the other hand, (Zhou et al., 2020) used a GRU-based model to capture both the meaning and the context in an RNN. Formal: Another approach is to learn the latent representation h by representing the response vector using a neural network. Formal: In this approach, the latent representation h",
        "directory": "D14",
        "filename": "D14-1036.json"
    },
    {
        "casual_text": "Hyp-Negation. This idea pops up when the hypothesis has a word like \"no\" in it. Basically, if this concept is around, it strongly suggests that an NLI model will predict the answer to be \"contradiction,\" no matter what else is going on in the NLI stuff (Gururangan et al., 2018).",
        "formal_text": "Hyp-Negation. This concept is realized in instances containing negation words (e. g. , 'no') in the hypothesis. The presence of this concept is highly indicative of an NLI model's prediction to be the contradiction class regardless of other NLI semantics (Gururangan et al. , 2018).",
        "GPT2_formal_text": "= []. To keep things simple, we'll use a single id for both the entity mention and the sentence mention, which we'll call i_s_i. The main thing we're focusing on here is picking the right sentence for the entity mention, which we'll call e_e. Formal: These two equations are related, but they're different ways of saying the same thing. Basically, the total loss for both the entity mention and the sentence mention is defined as: Formal: Formal: L_s is basically the same as the total loss for entity mention, but it's set to a bigger value to account for the extra info in the sentence mention. Formal: To pick the best sentence for the entity mention, we pick the one that has the smallest loss. Formal: The cost is calculated by subtracting the sum from E_e_s(e_e) for all possible sentence options. Formal: The loss for entity mention can be broken down into two parts: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.120.json"
    },
    {
        "casual_text": "Alright, so here's the deal: with the zero-one loss (01), the risk R(g) is calculated using π + F N F N +T P + π − F P T N +F P. During training, we use something called \"sig,\" which is basically a smoother version of this formula that works better for backpropagation. In real-world training, we take the average of these losses and optimize them using methods like batched gradient descent or something similar.",
        "formal_text": "Notice that under the zero-one loss ( 01 ), the risk R(g) refers to π + F N F N +T P + π − F P T N +F P. When training, we use sig which can be regarded as a soft approximation of this formulation for backpropagation. In practice, the expectations are expressed as the average of losses and optimized in batched gradient-descent or similar methods.",
        "GPT2_formal_text": ". This step is super important in the task because it helps the model learn how to make decisions by considering the information it gets from other parts of the graph, like the neighbors and the graph itself. Figure 1 has two examples of how to use this for relation extraction, and you can find more details in the Appendix. Formal: Since the graph is super flexible and can be built in different ways, we can figure out how important different features are by using the confidence scores from different layers. Formal: We’ve also figured out the training goals and weights for the graph. Formal: There are also regularization parameters that help the model learn better and better when dealing with different types of data. Formal: We’re using the beam-search algorithm to get the final answer. Formal: We’ve got a head-start on this task because we’re only focusing on the immediate neighborhood of the entity. Formal: To tackle the problem of not having enough examples to train a good model, we’re training our model with a maximum surprisal approach. Formal: This means we’re trying to make the model as confident as possible by minimizing the maximum surprisal. Formal: If the model doesn’t perform well, we ditch it and try again with a different training approach. Formal: We’ve made a dataset of real text sentences by sampling from the real part of the graph. Formal: We’ve also created a dataset of real text sentences using the set-theoretic distribution. Formal: Lastly, we’ve trained a model using this dataset to get the answer. Formal: This model is the only one that’s been trained on the whole dataset. Formal: We’re also sharing the results of a test where we’re checking how well the model can perform. Formal: Since the graph is super connected, the model can get really good results by looking at the connections between the neighbors and the edges. Formal: Even though it’s not super efficient, it’s still easy to compute and is really fast because the number of nodes is pretty small. Formal: We’ve also calculated the accuracy and perplexity for the query, which we’ll explain in the next section. Formal: To make the model learn faster, we’re using the average attention mechanism and the fast-growing attention mechanism. Formal: We",
        "directory": "eacl",
        "filename": "2021.eacl-main.47.json"
    },
    {
        "casual_text": "We looked at three different setups for our system. The first one, called iSRL, uses all semantic roles for each PLTAG lexicon entry, runs the PLTAG parser (IRPA), and uses both classifiers to handle identification and disambiguation, just like we explained in Section 4. \n\nThe second setup, Majority-Baseline, skips the classifiers and deals with argument identification and role disambiguation in a different way. For identification, we used some heuristics based on Lang and Lapata's work (2014), which rely on gold syntactic dependency info from CoNLL input. For disambiguation, we just picked the most common role based on the gold standard dependency relation label for that specific argument. Keep in mind, these dependencies were made looking at the whole sentence, not step by step.\n\nWe used MaltParser, a top-notch shift-reduce dependency parser, to get labeled syntactic dependencies, following Nivre et al. (2007). Based on Beuck et al. (2011), we tweaked the parser to give us intermediate output after each word by showing the current state of the dependency graph before each shift. We trained MaltParser with the arc-eager algorithm (which worked better than the other options available) on the CoNLL dataset, and it got an 89.66% labeled dependency accuracy on section 23.",
        "formal_text": "We evaluated three configurations of our system. The first configuration (iSRL) uses all semantic roles for each PLTAG lexicon entry, applies the PLTAG parser, IRPA, and both classifiers to perform identification and disambiguation, as described in Section 4. The second one (Majority-Baseline), solves the problem of argument identification and role disambiguation without the classifiers. For the former we employ a set of heuristics according to Lang and Lapata (2014), that rely on gold syntactic dependency information, sourced from CoNLL input. For the latter, we choose the most frequent role given the gold standard dependency relation label for the particular argument. Note that dependencies have been produced in view of the whole sentence and not incrementally. Nivre et al. (2007) to provide labeled syntactic dependencies MaltParser is a state-of-the-art shift-reduce dependency parser which uses an incremental algorithm. Following Beuck et al. (2011), we modified the parser to provide intermediate output at each word by emitting the current state of the dependency graph before each shift step. We trained Malt-Parser using the arc-eager algorithm (which outperformed the other parsing algorithms available with MaltParser) on the CoNLL dataset, achieving a labeled dependency accuracy of 89. 66% on section 23.",
        "GPT2_formal_text": "= set(prn_1, prn_2, ..., prn_k) for k in set(P) # prn_l = {(prn_l, prn_m)} and prn_r = {(prn_r, prn_l)} for l in set(P). The result is a list of tokens {(w_1, w_2, ..., w_k)} where each token is the start and end points of a sentence. Formal: We can break down the whole process of training the model into two steps: Formal: In each step, we adjust the hidden state based on the context it’s already been trained on and the examples it’s seen so far. Formal: Then, we go through each example one by one and figure out the hidden state using the whole training set. Formal: To keep things simple, we’ll call the model we end up with the \"base model.\" Formal: The final hidden state gets fed into a linear model that predicts the next word in the sentence. Formal: The model we’ll talk about later, which uses the latent vector g_s, gets trained using both the latent vector g_s and the context vector c, using the same loss function L. Formal: Formal: The final hidden state we get can then be used to adjust the hidden state of the base model (we call it the \"base model\" or \"base\"). Formal: Formal: The extra loss term L_r is calculated using the same loss function as the original loss L, but it’s also adjusted for the value of the latent vector, g_s. Formal: To check how well the model is doing, we measure its perplexity using the Jensen-Shannon divergence, which is calculated using the log probability of the correct token. Formal: This Jensen-Shannon divergence is figured out by looking at the log probability of the correct token in the training set. Formal: To measure the perplexity of the model, we check the log probability of the correct token in the training set. Formal: We also take the log probability of the correct token in the training set. Formal: Lastly, we calculate the perplexity of the model using the Jensen-Shannon divergence. Formal: Formal: Formal: The final hidden state gets fed into a linear model that predicts the next word in",
        "directory": "D14",
        "filename": "D14-1036.json"
    },
    {
        "casual_text": "Okay, so here's a real-life example taken from a study by Mgrquez and Padr6 back in 1997. There are two taggers being compared: T1 and T2. \n\nT1 only uses bigram information, and it gets a performance score of 0.9135, which is like 96.86% accuracy overall when dealing with ambiguous words. \n\nT2, on the other hand, uses trigrams plus some automatically learned context rules, and it scores a bit better with an accuracy of 0.9282, or 97.39% overall.\n\nBoth of these taggers were tested on a specific corpus (let's call it wsa) where the estimated error rate is about 0.03 for both T1 and T2. The average number of possible tags for each ambiguous word in this corpus is 2.5 tags per word. \n\nThe error rate for the wsa corpus is calculated across all the words in it.",
        "formal_text": "The following real example h~ been extracted from (Mgrquez and Padr6, 1997): The tagger T1 uses only bigranl information and has an observed performance on ambiguous words K1 = 0. 9135 (96. 86% overall). The tagger T2 uses trigrams and automatically acquired context constraints and has an accuracy of K2 = 0. 9282 (97. 39% overall). Both taggers have been evaluated on a corl)us (wsa) with an estilnated error rate 5 C1 =C2 =0. 03. The average ambiguity ratio of the ambiguous words in tile corpus is a = 2. 5 tags/word. SThe (wsa) corpus error rate is estimated over all words.",
        "GPT2_formal_text": "and attention_pool_mask, which are parameters that have been trained to handle the multi-hop problem. In Section 4.1, we’ll explain how we set up these parameters to make the most of the hidden states from the pre-trained models. Formal: We’re also looking into a different way to handle the multi-hop problem and see how adding more data affects how well it works. Formal: We’re using a version of the multi-hop problem from the Universal Proposition Bank (UPB) (Manning et al., 2014). The idea is that if you have two sentences with n sentences, the job of the multi-hop problem is to figure out the best possible sequence of states (like the one in Figure 1) by looking at the hidden states from each sentence. The UPB has this multi-hop problem for n + 1 sentences and it’s been looked at before (like in (Mihalcea and Tarau, 2004) and (Bordes et al., 2011)). So, we’re using a version of this problem for our experiment as a starting point. Formal: Here’s a quick rundown of the steps we’ve followed for training our proposed method. Formal: We’ve started by using a few basic baselines (which you can find at the bottom of the Figure 1) to get some training data. After that, we’ve fine-tuned our model on a much bigger dataset called Wikipedia-Multi-hop. Formal: Next, we’ve made a special dataset called MultiHop-Test (MHP) for testing our approach. The goal of MHP is to figure out the best possible sequence of states for a single sentence, which can be tackled using any decoding method. Formal: We’ve built a system to handle the multi-hop problem using a multi-task learning setup. Formal: We’ve designed a straightforward search algorithm that can be used to search through the entire dataset, using a single, fixed approach. Formal: We’ve built a model using a simple attention mechanism and a multi-task learning approach that can handle the multi-hop problem. Formal: We’ve also developed a version of this system that can handle the multi-hop problem, which we call MultiHop-NER. Formal: The search algorithm we’re proposing is simple and efficient, allowing us to search through",
        "directory": "C98",
        "filename": "C98-2159.json"
    },
    {
        "casual_text": "To figure out if PLMs actually have generalizable metaphorical knowledge, we test them in situations where the test data and training data come from different sources. We look at how well they can transfer knowledge between languages and across different datasets as two main ways to check this. We'll explain each of these in the next sections.",
        "formal_text": "To see if PLMs encode generalizable metaphorical knowledge, we evaluate them in settings where testing and training data are in different distributions. We explore transferability analysis across languages and datasets as two sources of distribution. We explain each in the following sections.",
        "GPT2_formal_text": "(x) = {x_1, ..., x_m, ...}  \n\nThe main idea behind the self-attention mechanism is to take the embedding vector from an input sentence x and focus on the words and parts of speech in the input sentence that match up with the context, including the beginning and end of each word. It does this by focusing on these two parts of speech separately. Formal: The self-attention mechanism is set up to pay attention to the main parts of speech and the context of the input sentence. Formal: We can use this self-attention mechanism to figure out the \"contextual embeddings\" of the input sentence. This will help the model focus on the most important parts of the sentence. Formal: Let's say x_i is the input sentence, and we have a set of embeddings {x_i, e_i, C}. We can define the representation of the input sentence as: Formal: This representation is created by using a multi-layer self-attention network, which looks like this: Formal: The input sentence x_i is represented using the input embeddings x_i. This representation helps the model learn how to represent the input sentence. Formal: To make the self-attention mechanism work for this representation, we can add an input bias to it. Formal: Here, e_i is the input embedding for the i-th word in the input sentence. The bias is set to 0.5 for the i-th word and 1.0 for the i-th word in the input sentence. Formal: After the self-attention mechanism finishes its operation, we can figure out the representation of the input sentence, which we can write as: Formal: The representation of the input sentence x_i is then used to form a new representation called h_i. Formal: This new representation is then passed through a fully connected layer to get the final representation h_i. Formal: Finally, the final representation of the input sentence is turned into a vector, which we can write as: Formal: Formal: The final output vector h_i is then used to calculate the hidden state of the self-attention mechanism. Formal: Formal: The output vector h_i is then passed through a fully connected layer to get the final representation h_i. Formal: Formal: Finally, the output vector h_",
        "directory": "acl",
        "filename": "2022.acl-long.144.json"
    },
    {
        "casual_text": "There's been a lot of buzz around crosslingual transfer learning for SRL lately. A bunch of researchers have been diving into this topic, like Padó and Lapata (2009), van der Plas et al. (2011), Kozhevnikov and Titov (2013), Tiedemann (2015), Zhao et al. (2018), Chen et al. (2019), Aminian et al. (2019), and Fei et al. (2020). Most of the work so far has been split between two main approaches: annotation projection and model transfer.",
        "formal_text": "There has been a great deal of interest in crosslingual transfer learning for SRL (Padó and Lapata, 2009; van der Plas et al. , 2011; Kozhevnikov and Titov, 2013; Tiedemann, 2015; Zhao et al. , 2018; Chen et al. , 2019; Aminian et al. , 2019; Fei et al. , 2020). The majority of previous work has focused on two types of approaches, namely annotation projection and model transfer.",
        "GPT2_formal_text": "_tgt. It's pretty straightforward to check if the input is valid. To make sure we're dealing with the right token type, we'll compare the result from the HMGCN with the one from the CRF layer. Formal: We use these GCNs to figure out the probabilities for our clustering models. We set the number of GCNs to six and calculate the log probabilities for each event type, like \"used_drugs\" for a positive label and \"used_medicines\" for a negative label. Formal: To make the summarization process more efficient, we'll tweak the clustering model by using different baselines instead of just the most common one. Formal: We look at the three main types of event types: positive, negative, and neutral. To figure out the event types, we use the event type distribution shown in Figure 1. This way, we get a chance to see all the event types from the training data. Formal: We use the GCN embedding matrix to represent the event types. Finally, we calculate the probabilities for each event type. Formal: The GCN embedding matrix, φ_enc, helps us pick out the most important event types from the training data. Formal: We also calculate the probabilities for each event type individually, like \"used_drugs\" for a positive event and \"used_medicines\" for a negative event. Formal: Finally, we calculate the probabilities for each event type individually, like \"used_drugs\" for a positive event and \"used_medicines\" for a negative event. Formal: The final probabilities for each event type are calculated using the parameters from the GCN embedding matrix. Formal: Our experiments show that our GCN model works really well. It can actually create a higher-quality summary compared to other methods, which gives us more insight into the event types. Formal: Our experiments on the CNNDM dataset show that our GCN model can work with different types of data. Formal: We created a single GCN with six GCNs, which is enough to create a full GCN model for summarization. Formal: We also included a cross-validation setup to see how well our GCN model can extract event types. Formal: Lastly, we tested different baselines on the CNNDM dataset. Formal: The results from the cross-validation setup are in Table 3. Formal: Here are the GCN",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.319.json"
    },
    {
        "casual_text": "The rank of (A + Ã) won't be more than d_k. If you've got a bunch of vectors in a linear space, any combination of those vectors should still be in the same space. So, the rows we made up for A are part of LN(T). That means there's an Ã that proves the whole thing about A being unidentifiable.",
        "formal_text": ", the rank of (A +Ã) is not more than d k. For a set of vectors lying in a linear space, a vector formed by their linear combination should also lie in the same space. Thus, the artificially constructed rows of A belongs to LN(T). Therefore, there exist anÃ that establishes the proposition which claims the unidentifiability of A.",
        "GPT2_formal_text": "(x, y) = c(x, y) for each pair of text and image pairs. The final embedding is d_T. For each sentence, we create two embeddings, H_s and H_o, using the embeddings from the sentence itself. We also add a third embedding, H_e, which is calculated based on the length of the sentence, d_s. Formal: So, it turns out that adding the third embedding helps the model focus on the important parts of the sentence, which we think is what we're aiming for. Formal: To make sure we're comparing things fairly, we used the same training set for all the models, so we don't need to worry about any differences in how we sample. Formal: For each example from E_i, we can calculate the scores for the model using these equations. Formal: Since the model's predictions are always consistent (uniform), we can just plug these scores into the model's output. We'll call this model \"LM\" for short. Formal: Here, d_e is the final embedding, and d_s is the embedding for the next word. Formal: If we're dealing with a linear model, we'll just add up the scores for all the words, and we'll call this whole setup \"LM+linear.\" Formal: For the word embeddings, we pick the embedding that gives the highest score. Formal: Finally, we calculate the conditional probability P(l) for the whole sentence, P(x, y) = P(x, y)^P(x|y) for each pair of words x and y. Formal: We call the model \"LM+cond\" for this setup. Formal: Formal: We use the SVM model (from Lafferty et al., 2001) to predict the conditional probability P(l) for the whole sentence. Formal: This same model can also be used to predict the conditional probability P(x, y) for each pair of words x and y. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.94.json"
    },
    {
        "casual_text": "Alright, so here's the deal with error analysis. The main issues come from not properly adding the right information to the system. Check out Figure 6, which shows two examples of how our system handles things. In Figure 6a, the system managed to make a connection because the link between \"spouse\" and \"husband\" was pretty strong (score of 0.747). That's why it added the \"HUSBAND\" function during the knowledge injection. But in Figure 6b, it didn't add the \"WIN\" function because the connection between \"award\" and \"win\" wasn't strong enough (score of 0.336). \n\nAlso, even though we made the original model checking program faster, some tests still took way too long to finish. Table 6 shows the average and longest times (in seconds) for checking the model with and without the optimization.",
        "formal_text": "Error Analysis Error analysis shows that main errors are caused by the failure of knowledge injection. Figure 6 shows two premise-hypothesis pairs, one for which our system was able to perform inference and one for which it was not. In Figure 6a, the function HUSBAND was added to the model in the knowledge injection process because the relatedness score between spouse and husband was high (0. 747). On the other hand, in Figure 6b, the function WIN was not added to the model because the relatedness score between award and win was low (0. 336). In addition, even though we improved the speed of the original model checking program, several test cases still ran out of time. Table 6: Average and maximum run time (seconds) for model checking with and without optimization.",
        "GPT2_formal_text": "_c. This is a space where all the attention mask values get combined together. The final attention vector is always a mix of the hidden representations of the query and the answer. The final vector for this whole thing is called the attention mask. To get the attention mask vector for a query v_q, we calculate the dot product between the query and the context vectors, as shown in Equation 2. Formal: We look at all the pairs of entities in a document d and calculate the attention vector for each pair. We use the same way to compute the attention mask for each entity pair too. Formal: The query, context, and attention vector are all combined into a single vector. Formal: We use the dot product of the query vector and the context vector to figure out the attention mask. Formal: The final attention vector for a query v_q is just the combination of the query vector, the context vector, and the attention mask vector. Formal: To get the attention mask for a query v_q, we calculate the dot product between the query and the context vectors. This gives us the attention mask for v_q. Formal: For the words in the query, we calculate the attention vector using the words in the query. We also use the word embeddings from the CNN to get the embeddings for the words in the query. Formal: We take the embeddings for the words in the query and combine them with the words in the context to get the query embedding. Formal: We then use the attention mask to combine the query embeddings with the context embeddings to get the query context embedding. Formal: Lastly, we use a linear transformation to get the hidden representation for the word embedding v_q. Formal: We calculate the attention mask for each word in the query. We use the word embedding v_q to do this. Formal: We use the words in the context to get the embeddings for the words in the query. Formal: Finally, we use the attention mask to combine the query embeddings with the context embeddings to get the query context embedding. Formal: Formal: Lastly, we use a linear transformation to get the hidden representation for the word embedding v_q. Formal: Formal: Lastly, we use a linear transformation to get the hidden representation for the word embedding. Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-srw.8.json"
    },
    {
        "casual_text": "Here, θ represents a fixed version of θ. This training method doesn't add any extra parameters to the model. The only extra cost for supervised learning is running a training pass and a generation pass for each unlabeled sentence. As the main goal, we train using a combination of the supervised loss from Equation 1 and the unsupervised loss from Equation 3, with some weights applied to balance them.",
        "formal_text": "whereθ denotes a fixed copy of θ. This training process does not introduce additional model parameters. The entire additional training cost to supervised learning is a training pass and a generation pass for each unlabeled sentence. As the overall objective, we train a weighted sum of the supervised loss in Equation 1and the unsupervised loss in Equation 3:",
        "GPT2_formal_text": "\" is a list of pairs of embedding vectors (x_i, x_j) that represent the i-th and j-th tokens, respectively. These vectors are treated as query arguments for the attention mechanism. Formal: The model for the query (q_i) is written as a multi-layer GRU with a hidden size of d_l and a hidden state size of d_t. On the other hand, the model for the relation (r_i) is a multi-layer GRU with a hidden state size of d_l + d_t and a hidden state size of d_t. Formal: The model for the question (q_i) is a GRU with an LSTM hidden state size of d_l + d_t and a hidden state size of d_t. Formal: Lastly, the model for the answer (r_i) is a multi-layer GRU with an LSTM hidden state size of d_l + d_t and a hidden state size of d_t. Formal: The input to the attention mechanism is a set of query and relation vectors, each having a size of d_l + d_t, and a hidden state size of d_t. Formal: The input to the attention mechanism is a set of query and relation vectors, each having a size of d_l + d_t, and a hidden state size of d_t. Formal: Formal: The input to the attention mechanism is a set of query and relation vectors, each having a size of d_l + d_t, and a hidden state size of d_t. Formal: Formal: Formal: The attention mechanism's weights are learned through a cross-entropy loss function, which is explained in Section 3. Formal: The weights for the attention mechanism are learned through a cross-entropy loss function, which is explained in Section 3. Formal: Formal: The weights for the attention mechanism are learned through a cross-entropy loss function, which is explained in Section 3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.321.json"
    },
    {
        "casual_text": "NMT is kind of the big thing right now, but not much has been done specifically for translating between Bengali and English. Dandapat and Lewis (2018) worked on creating a general-purpose NMT model for Bengali-English using sentences from comparable corpora. They dealt with the lack of training examples by using data augmentation and back-translation (Sennrich et al., 2016). Hasan et al. (2019) and Mumin et al. (2019a) also showed that even with limited parallel data available online, NMT improved translations for the Bengali-English pair.",
        "formal_text": "Although NMT is currently being hailed as the state-of-the-art, very few works have been done on NMT for the Bengali-English pair. Dandapat and Lewis (2018) trained a deployable general domain NMT model for Bengali-English using sentences aligned from comparable corpora. They combated the inadequacy of training examples by data augmentation using back-translation (Sennrich et al. , 2016). Hasan et al. (2019); Mumin et al. (2019a) also showed with limited parallel data available on the web that NMT provided improved translation for Bengali-English pair.",
        "GPT2_formal_text": "* f = g_vec_d;\n\n};\n\n# define USE_PACK ( i ) USE_PACK(i)\n\n# define USE_RESTART ( i ) USE_RESTART(i)\n\n# define USE_SILVER ( i ) USE_SILVER(i)\n\n# define USE_SILVER_TEST ( i ) USE_SILVER_TEST(i)\n\n# define USE_TEST ( i ) USE_TEST(i)\n\n# define USE_TEST_TEST ( i ) USE_TEST_TEST(i)\n\n# define USE_TEST_RESTART ( i ) USE_TEST_RESTART(i)\n\n# define USE_TEST_RESTART_TEST ( i ) USE_TEST_RESTART_TEST(i)\n\n# define USE_TEST_RESTART_TEST_TEST ( i ) USE_TEST_RESTART_TEST_TEST(i)\n\n# define USE_TEST_RESTART_TEST_TEST_TEST ( i ) USE_TEST_RESTART_TEST_TEST(i)\n\n# define USE_TEST_RESTART_TEST_TEST ( i ) USE_TEST_RESTART_TEST_TEST(i)\n\n# define USE_TEST_RESTART_TEST_TEST_TEST ( i ) USE_TEST_RESTART_TEST_TEST(i)\n\n# define USE_TEST_RESTART_TEST_TEST ( i ) USE_TEST_RESTART_TEST_TEST(i)\n\n# define USE_TEST_RESTART_TEST_TEST ( i ) USE_TEST_RESTART_TEST_TEST(i)\n\n# define USE_TEST_RESTART_TEST_TEST_TEST ( i ) USE_TEST_RESTART_TEST_TEST(i)\n\n# define USE_TEST_RESTART_TEST_TEST ( i ) USE_TEST_RESTART_TEST_TEST(i)\n\n#",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.207.json"
    },
    {
        "casual_text": "For phrase-level attacks, we wanted to see if tweaking a part of a sentence (the source subtree) could mess with the prediction for another part (the target subtree). Check out Figure 2 for an example. We tested two scenarios: one where the source and target subtrees are at least one word apart (k ≥ 1), and another where they just can't overlap (k ≥ 0). \n\nIn the k ≥ 0 case, we found 1420 sentences in the test set that fit the bill. For k ≥ 1, there were 1340 valid examples we could use for these attacks. The test set has a total of 2416 sentences, by the way. \n\nOh, and all the subtrees we looked at had between 4 and 12 words. For each source-target pair, we could change up to 3 words in the source subtree. Sometimes, just swapping one or two words was enough to create an adversarial example for some sentences.",
        "formal_text": "For the phrase-level attacks, we aim to study whether changes in a source subtree can alter the prediction on another target subtree (see an illustration in Figure 2 ). We tried two different settings: one asks for the source and target subtrees to be separated by at least one word (k ≥ 1), and another only requires those two subtrees do not overlap with each other (k ≥ 0). In the case of k ≥ 0, we can find 1420 sentence examples from the test set, while for k ≥ 1, there are 1340 valid examples that can be used to deliver phrase-level attacks (there are 2416 sentences in total in the PTB test set). Note that all the subtrees should contain 4 to 12 words. For each source-target pair, we allow to modify the source subtree up to 3 words. For some sentences, their adversarial examples can be generated by replacing just one or two words.",
        "GPT2_formal_text": "(V, E) = {v_i, e_i}  \n\nwhere e_i is the query e_i, and V is the vocabulary. Formal: This approach works well for processing any kind of text. But when it comes to text with non-standard words, like words that don't have a special sound to tell them apart, the algorithm might not be able to figure out the difference between the queries and the words. Formal: In this project, we look at how non-standard words affect models for detecting these words and think we can use something called a disfluency model to help the model better identify these words. Formal: For disfluency detection, we start with a bunch of unlabeled text and train a model using this data. Formal: The goal here is to create a model that can recognize disfluencies. Formal: For the disfluency detection task, we train a model using the text before and after the disfluency, so the model can better understand the context. Formal: We also use a disfluency model to help the model predict the next word. Formal: In this part, we explain how we train the model using disfluent text, with the goal of making it better at identifying disfluencies. Formal: We use a disfluent model to help the model predict the next word. Formal: Formal: We also use a disfluent model to help the model predict the next word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.590.json"
    },
    {
        "casual_text": "The vector s, which is part of the set R S, holds overall, population-wide estimates for the parameters that describe how we predict things. In this study, we assume that these predictions follow a normal (bell-shaped) distribution. So, s includes the average prediction (µ, also known as the intercept) and the spread or variability of those predictions (σ², which is the variance).",
        "formal_text": "Vector s ∈ R S contains global, population-level estimates of the parameters of the predictive distribution. Under the univariate normal predictive distribution assumed in this study, s contains the predictive mean (µ, i. e. the intercept) and variance (σ 2 ):",
        "GPT2_formal_text": ". The model is trained using a pretrained Transformer model (like the one Vaswani et al. introduced in 2017) with a softmax layer, and we use the Adam optimizer (from Kingma and Ba, 2014) to minimize the cross-entropy loss. Formal: Here’s a more casual version of that text:\n\nWhen it comes to figuring out how to translate something, we look at two main things: the source sentence and the target sentence. Both of these are represented by n-gram vectors, which we feed into the network through attention. Since the training data we have includes all possible source-target pairs, the attention-based model helps improve its ability to pick up on the important stuff. Formal: Formal: We use a Softmax layer to calculate the cross-entropy loss. We set it to 0.1 and set the dropout rate to 0.3. Also, to match the learning rate schedule for the Transformer model, we use the standard hyperparameter β. We train our model for 20 epochs, starting with a learning rate of 0.1 and a decay factor λ. Formal: We trained the model for 1 epoch, using the learning rate β = 0.1 and a decay factor λ = 0.3. Formal: We also use the Adam optimizer (from Kingma and Ba, 2014) to minimize the cross-entropy loss. For training, we use the stochastic gradient descent algorithm (from Kingma and Ba, 2014) with a learning rate of 0.0001. The model is trained for 20 epochs, starting with a learning rate of 0.1 and a decay factor λ = 0.3. Formal: Formal: Our final model is trained to predict the target language's word type, which we use as the target input. Formal: Formal: Formal: We consider two tokens as two different words if they’re next to each other in the target vocabulary. Formal: Formal: In this setup, each token has a hidden representation h x t (i, j) = {h t (i), h t (j) } = {h t (i), h t (j) } × d t (i, j). Formal: Formal: Here’s the formula that breaks it down: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.288.json"
    },
    {
        "casual_text": "In the second experiment, we looked at multilingual word translation across six European languages: English, German, French, Spanish, Italian, and Portuguese (Lample et al., 2018). We compared our MPPA method to MAT+MPSR (Chen and Cardie, 2018). Since MAT+MPSR is an unsupervised approach, we swapped out the MPSR part with our MPPA algorithm to make it MAT+MPPA for a fair comparison. We ran 5 refinement epochs after the MAT step, which is the default setting in the MAT+MPSR source code. The MPPA training phase is about 10 times faster than the MPSR equivalent, though both methods have hyperparameters that need tuning. We also tested UMH (Alaux et al., 2019) on this benchmark. The precision@1 results are in Table 2. MPPA performed similarly to UMH, and MPSR did slightly better. One thing to note is that the MPSR mapping matrices weren’t exactly orthogonal. They had a smaller mean-square error (MSE) on the training data compared to our method, which was restricted to be orthogonal. This suggests that the orthogonality constraint, especially when combined with transitivity constraints in multilingual settings, might be too strict.",
        "formal_text": "The second experiment involved multilingual word translation in six European languages: English, German, French, Spanish, Italian and Por-3564 tuguese (Lample et al. , 2018). We compared MPPA to MAT+MPSR (Chen and Cardie, 2018). MAT+MPSR is an unsupervised method, so for a fair comparison we replaced the MPSR algorithm with our MPPA algorithm, thus obtaining MAT+MPPA. We ran 5 refinement epochs, after the MAT step, as the default option in MAT+MPSR source code 3. MPPA training phase is 10 times faster than MPSR equivalent phase, which also have hyper-parameters that needed to be tuned. UMH (Alaux et al. , 2019), was also evaluated on this benchmark. Table 2 shows precision@1 results. MPPA was comparable to UMH and MPSR performed slightly better. Note that the MPSR mapping matrices were not exactly orthogonal. They indeed achieved smaller mean-square error (2) on the training data than our solution, which was restricted to be orthogonal. This suggests that the orthogonality constraint, especially in the multilingual case where it is combined with transitivity constraints, can be too restrictive.",
        "GPT2_formal_text": "= {}, ...\n\ntemplate:\n\n```\n\npython\n\ndef check_response(response_id):\n\nresponse_id = response_id + 1\n\nreturn response_id\n\n```\n\nBasically, this script checks if the response is relevant to the current query. It does this by calculating the relevance score. If it's not, it suggests a different response. Formal: To make sure our model doesn't get confused by different responses from different users, we set up some rules: \n\n1) If the response is about a single user, it has to be about their exact query. \n2) If the response is about multiple users, we set a limit—specifically, it can't be more than two users. \n3) If the response is about a query or a series of queries, we limit it to a maximum of three queries or a maximum of ten queries. \n\nTo show how these rules work, we ran some tests using a small dataset of 100,000 responses. Figure 1 shows the average relevance scores for each dataset, with the number of queries and queries per query highlighted. Formal: In our system, the relevance score is calculated using the average relevance scores from users. Formal: For each user, we calculate the relevance scores for their responses, which are shown in Table 1. Formal: For each query, we calculate the relevance scores for each user's response, which are shown in Table 2. Formal: For a single query, the relevance scores for each user's response are calculated using the average relevance scores from their responses. Formal: For each query, the relevance scores for each user's response are calculated using the average relevance scores from their responses. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-1363.json"
    },
    {
        "casual_text": "Since tables made from plain text are pretty basic, they don’t have a lot of fancy features or options.",
        "formal_text": "Because of the simplicity of table construction methods in free text, the expressive capability is limited.",
        "GPT2_formal_text": "ed_c = ( |t_i|, |c_i|, • • •) t_i * c_i. This setup treats the attention as a type-based attention mechanism, which is basically the same as the context-based attention in earlier research (Hessel et al., 2020a). It uses an attention-based feed-forward neural network to figure out the attention vector for each token, and it does this using a position-wise max pooling operation. The attention vector, H_t, is calculated as H(t_i, c_i, |t_i|), where |t_i| is the size of the attention vector. Formal: In this paper, we take a look at how to handle paraphrasing in multi-turn conversations by using a pretrained model for summarization. We focus on the relationship between the dialogue acts and the generated responses, as well as between the conversation history and the responses, to better understand how dialogue acts interact with the response generation process. We tested our approach on two different datasets, Question Answering (QA) and Quora, and the results showed that our model works well and is really versatile. Formal: In this section, we dive into some details about the dataset and the data structure, as well as the supervised models we used. Formal: We also talk about the training of our model and the validation set we used. Formal: To make it easier to compare different models and datasets, we broke our model into a pre-trained model and an unsupervised one, using the GPT-2 model (Radford et al., 2018). Formal: For the test set, we trained the model on a single-turn QA dataset with 1 million examples, as mentioned in section 3.2. Formal: We also tested the model on a QA dataset with 1 million examples and added more examples to both datasets. Formal: Lastly, we added a validation set to check how well it performed in this new task. Formal: Formal: In the training process, we tweaked the model parameters based on the validation set results. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C00",
        "filename": "C00-1025.json"
    },
    {
        "casual_text": "So, basically, if we say: F is the forest made up of all the syntactic trees (T i ) from the considered lexical entries, plus the output (the type we're trying to get), then the generation problem (check out figure 4) is about finding a matching M for the atomic formulas in F.",
        "formal_text": "Thus, if we define: F the forest made of the syntactic trees (T i ) of all the considered lexical entries plus the output (the type we want to derive), the generation problem (see figure 4) is to find a matching M of atomic formulas of F such that:",
        "GPT2_formal_text": ") = [u(k), y(k), v(k)] (8) Formal: We don't need to tweak the model's parameters directly to make it perform as well as other methods. Formal: For each training round, we tweak the model's parameters by adding or removing the parameters from the model's own embedding space. This helps us measure how well the model is doing. Formal: The embedding of a word w_i is the average of the embedding vectors of its surrounding words. Formal: We're trying to find the best w_i by minimizing the cross-entropy loss. Formal: The cross-entropy loss is a way to measure how uncertain a model is. Formal: We evaluate the model using the cross-entropy loss. Formal: We calculate the cross-entropy loss for each training round using a development set. Formal: We train the model using a beam search algorithm. Formal: For each training round, we check if the cross-entropy loss is better than the current best method. Formal: We calculate the cross-entropy loss for each training round using a development set. Formal: We also calculate the cross-entropy loss for each training round. Formal: Formal: The cross-entropy loss is a way to measure how uncertain a model is. Formal: The cross-entropy loss is a way to measure how uncertain a model is. Formal: Formal: We calculate the cross-entropy loss for each training round using a development set. Formal: We also calculate the cross-entropy loss for each training round. Formal: Lastly, we calculate the cross-entropy loss for each training round. Formal: Finally, we calculate the cross-entropy loss for each training round. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C00",
        "filename": "C00-2091.json"
    },
    {
        "casual_text": "(2) Cool thing: we noticed that some of the tricky annotation issues mentioned in the PDTB (by Webber et al., 2019) are popping up in our own data too. One of those tricky bits is figuring out the difference between Comparison. Contrast and Comparison. Concession, specifically when dealing with \"Despite what\" and \"What is contrasted with.\" In our manually checked data sample, we found 3 cases where QADiscourse and PDTB annotations got mixed up.",
        "formal_text": "(2) Interestingly, we observe that some annotation decision difficulties described in the PDTB (Webber et al. , 2019) are also mirrored in our collected data. One of those arising ambiguities is the difference between Comparison. Contrast and Comparison. Concession, in our case Despite what and What is contrasted with. In the manually analyzed data sample, 3 such confusions were found between the QADiscourse and the PDTB annotations.",
        "GPT2_formal_text": "ed is a special type of input token that gets turned into a special vector called attention_masked. This vector is used to make sure the attention weights are spread out evenly across the different parts of the input. Basically, the embedding vector for this token gets turned into a fixed-length vector, which we call attention_masked, based on the token embedding. Formal: So, x_t, y_t, w_t, and a_t are the input, output, and attention vectors for a specific time period t in the past, all from a specific time interval A. Formal: Equation (4) is a straightforward way to think about attention. Formal: Another way to think about attention is the attention matrix M. Formal: The attention matrix M is just the sum of all the attention weights, like H_t = [H_t1, ..., H_tM], from a specific time period t in the past, all from a specific time interval A. Formal: Formal: In our experiments, we use two types of attention: sequence and span attention, which we explain in the next sections. Formal: Formal: We look at two types of attention: sequence and span attention, which we use in the next sections. Formal: Also, we calculate the attention weight using the formula shown in Equation (2). Formal: Figure 1 shows how we calculate the attention weight. Formal: We also calculate the attention vector, H_t, using the same formula. Formal: Formal: Finally, we use the attention matrix M to figure out the attention weight. Formal: Formal: We use the attention matrix M to figure out the attention weight. Formal: Figure 2 shows the attention matrix. Formal: We calculate the attention vector, H_t, using the formula shown in Equation (2). Formal: Formal: Finally, we use the attention matrix M to figure out the attention weight. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "In subfigure (c), M-CAN+2R o gives us the best attention weights. The attention for the aspect \"food\" is pretty much at right angles to the attention for \"service.\" The \"food\" aspect focuses on the first half of the sentence, while \"service\" zeroes in on the second half. Also, the words \"outstanding\" and \"tops\" get the most attention in their respective aspects.",
        "formal_text": "As shown in subfigure (c), M-CAN+2R o generates the best attention weights. The attention weights of the aspect food are almost orthogonal to the weights of service. The aspect food concentrates on the first part of the sentence while service on the second part. Meanwhile, the key opinion words \"outstanding\" and \"tops\" get highest attention weights in the corresponding aspects.",
        "GPT2_formal_text": ", which are basically the actual entity embeddings. This is a basic approach that doesn't really account for the way the document is structured. Formal: For the entity embedding layer, we use the vocabulary from Wikipedia (thanks to Mikolov et al., 2013) and the predicted entity embeddings from a transformer-based model, which was trained on the MNLI dataset (thanks to Devlin et al., 2019). Formal: We start by randomly picking some entity embeddings from a uniform distribution called D, then use them to figure out the expected labels for each sentence. Formal: We also use a Bi-LSTM model (from Hochreiter and Schmidhuber, 1997) to handle the context of the document. Formal: For the first set of embeddings, we use the ones from the last layer of the Bi-LSTM model. To make things easier, we use a pre-trained BERT model (Devlin et al., 2019) to get the embeddings. Formal: For the second set of embeddings, we use the ones from the last layer of the BERT model. After that, we apply a linear transformation to the encoder outputs of these last-layer embeddings to create the final entity embedding. Formal: Finally, we apply a linear transformation to the encoder outputs of the last-layer embeddings to get the entity embedding vector. Formal: Formal: We apply a linear transformation to the entity embedding vector, which gives us the predicted entity embeddings, e_e. Formal: We set the hidden size of the entity embedding vector h_e to 4 to get an embedding with a length of 4. Formal: We also set the dropout rate λ to 0.2 to prevent overfitting, and the final learning rate λmax to 0.00002. Formal: The BERT model (Devlin et al., 2019) is trained using the same loss function as the encoder. Formal: We train the BERT model using the same loss function as the encoder, and the final learning rate λmax is also trained with λ. Formal: The final weights of the Bi-LSTM model are w(d_c) and w(d_d), which are the cross-entropy losses. Formal: For the Bi-LSTM model, we calculate the cosine",
        "directory": "D19",
        "filename": "D19-1467.json"
    },
    {
        "casual_text": "The only reason mistakes happen is because of issues with the annotations or problems in how we handle pronouns. In the next section, we'll compare the results we get from the extraction process using semantic annotations to the ones we get when we only use syntactic annotations.",
        "formal_text": "Mistakes can arise only because of the annotation errors and errors in the anaphora resolution procedure. 3 The comparison of the results obtained by the extraction procedure based on the semantic annotation with the results of the procedure based on the syntactic annotation only is provided in the next section.",
        "GPT2_formal_text": "is calculated using this formula:\n\nx = A(x_t) * T_E_u * T_B * A_t + A(x_i) * T_B * A_i + η. Formal: We've found that mixing the word embeddings can make text representations even better. For example, if you add the character embedding L_k, the character embedding L_k should only focus on the most important stuff, like the k-th character in the sequence. We tweaked this idea by using the character embedding weights M_k and also adding a normalized word embedding vector q_k. The normalized vector q_k can be seen as a way to bring in some context, not just focusing on the k-th character. Formal: The vector q_k acts as a kind of \"context vector\" for each word, helping to keep the attention aligned with the sequence. Formal: In our experiments, we looked at how well the text representations we created could predict the words the model would pick up. We compared the text representations to the word embedding vector q_k and found that using a text embedding with a special character-specific token (like k) that helps pick up the right words, q_k, works better than a plain text embedding. Formal: We tested how well the updated attention works with different word embedding vectors, using the word embedding vector q_k, which is normalized using cosine similarity. Formal: We also checked how the model learns to focus on the most important characters, L_k, by training on a smaller set of data. Formal: Lastly, we ran a human evaluation to check how well our method works for text. Formal: To get a better understanding of how our model picks up on important words, we also did a human evaluation. Formal: This evaluation included five people who rated the attention on a scale from 1 to 5, with 1 being bad and 5 being good. Formal: The results are in Table 2. Formal: We noticed that the model can sometimes pick up on specific characters that are almost impossible to predict, like the character x_t. Formal: But in some cases, it doesn't even know how to spell words, like x_i. Formal: In Figure 5, you can see the average accuracy for all the characters. Formal: Averaged across all three levels, the model does better",
        "directory": "D09",
        "filename": "D09-1144.json"
    },
    {
        "casual_text": "Looking at the context sentences manually, we noticed that a big reason for missing full evidence is the way literary language can be vague. Authors often avoid repeating stuff or directly mentioning character names, so it’s super important to catch accurate paraphrases and figure out coreference. We think commonsense knowledge is really key to making BookQA better. While testing our system, we kept finding situations where the model messed up because it missed important details that weren’t spelled out. Some common examples we saw were: \n\ni) Character relationships that were obvious to readers but never actually explained (like, “Who did Mark’s best friend marry?”); \nii) A character’s feelings about something (like, “Who was mad about the school’s policy?”); \niii) The order of events (like, “Who did Marriat talk to after the big fight?”). \n\nAdding commonsense knowledge to a QA system is still a tricky problem in general, and that includes BookQA.",
        "formal_text": "Manual inspection of context sentences indicated that a common reason for the absence of full evidence is the inherent vagueness of literary language. Repeated expressions or direct references to character names are often avoided by authors, thus requiring very accurate paraphrase detection and coreference resolution. We believe that commonsense knowledge is particularly crucial for improving BookQA. When exploring the output of our system, we repeatedly found cases where the model failed to arrive at the correct answer due to key information being left implicit. Common examples we identified were: i) character relationships which were clear to the reader, but never explicitly described (e. g. , \"Who did Mark's best friend marry? \" ); ii) the attitude of a character towards an event or situation (e. g. , \"Who was angry at the school's policy? \" ); iii) the relative succession of events (e. g. , \"Who did Marriat talk to after the big fight? \" ). The injection of commonsense knowledge into a QA system is an open problem for general and, consequently, BookQA.",
        "GPT2_formal_text": "(s, f) is the final identifier for the source token f. Formal: Since the model is linear, it doesn't really consider the structure of the input sequence. Formal: Most of the time, a neural network learns to approximate the input sequence by using the most common subsequence from the target text. To keep the model from getting stuck in local minima, we suggest using a regularization term on top of the input-output matrix E. Formal: To make sure we can create the highest-scoring subsequence that matches the target sentence, we add a constant to the input-output matrix. Formal: We also add a term to the matrix W to decide the order of the output tokens. Formal: We use L1 regularization to balance the input-output and output-token distributions. Formal: The model can generate outputs with the same length as the input if the log probability P_i for the input is greater than or equal to P_i for the output. Formal: For generating the output, we use the canonical form of the input, which is written as x i = (e i, f i ), where e i is the source token and f i is the target token. The model uses a Bi-directional GRU (BiGRU) to create the output. Formal: We optimize the model using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1/n, a regularization term of 1e-5, and a batch size of 32. Formal: We train the model using a validation set of 4 examples, but we don't train it with the full validation set. Formal: We measure the performance by the F1 score, which is calculated as f 1 = (1 − P_i). Formal: We also calculate the cost using the Sigmoid function (Sigmoid), which is a logarithm. Formal: We use gradient clipping to get the final score. Formal: We train the model for 100 epochs, using a batch size of 32. Formal: We train the model using a validation set of 4 examples, but we don't train it with the full validation set. Formal: We calculate the cost using the Sigmoid function (Sigmoid), which is a logarithm. Formal: The model uses a Canonical Formal: When we optimize the model using the learning rate and the validation set, the number",
        "directory": "D19",
        "filename": "D19-5811.json"
    },
    {
        "casual_text": "Alright, let's dive deeper into this. Instead of looking at languages as a whole, we're zooming in on individual nouns. The big question here is whether there's any rhyme or reason to the mistakes we see. Are they just random, or do certain types of nouns give us more trouble than others?",
        "formal_text": "In this section, we perform a finer-grained analysis: focus not on languages, but on individual nouns. Our main question is if there are any patterns in the distribution of errors. Is it random or are certain classes of nouns systematically more difficult to predict than others?",
        "GPT2_formal_text": ") == [MASK] and [ELEMENT] respectively. These are different from the non-linear updates we saw in Figure 1, and they also work with the length of the input sequence being less than d. Formal: For a given context c, we figure out the attention weights that go with it. Formal: The model takes the context vector c and turns it into a bunch of vectors with different numbers of elements, which we call wc. Formal: Each time we predict a word w, we look at the input sequence x and calculate the attention weights for the words in the prediction. Formal: We get a weight vector for each word in the prediction, which we call a_wt. Then, we multiply this vector with the output vector from the last hidden layer. Formal: Finally, we use these attention weights to get the prediction for the next word in the sequence. Formal: Here's what each formula looks like: Formal: We use a context vector c and a vector with weights to predict the next word in the sequence. Formal: We do this for each word w in the input sequence x. Formal: We add the current hidden layer's vector to the vector we got for the previous word, and also add the attention weights for the words in the prediction. Formal: Finally, we use this to predict the next word in the sequence. Formal: We add the vector of the last hidden layer's vector to the vector we got for the previous word, and also add the attention weights for the words in the prediction. Formal: The model updates the parameters by adding the weights for each word in the prediction, and then uses this to predict the next word in the sequence. Formal: Finally, we use this to predict the next word in the sequence. Formal: We use a context vector c, a vector with weights, and a vector with attention weights to predict the next word in the sequence. Formal: Finally, we use this to predict the next word in the sequence. Formal: We use a vector of the last hidden layer's vector and a vector with attention weights to predict the next word in the sequence. Formal: We use a context vector c, a vector with weights, and a vector with attention weights to predict the next word in the sequence. Formal: We use a context vector c, a vector with weights, and a vector with attention weights to predict the next word in the sequence. Formal: We",
        "directory": "conll",
        "filename": "2020.conll-1.20.json"
    },
    {
        "casual_text": "Basically, we use a special thing called \"[AM]\" to show the AM query, which is like a way to find all the arguments, like A a = {arg a k }.",
        "formal_text": "Formally, we use a special token \"[AM]\" to represent the AM query q am, which aims to identify all the arguments A a = {arg a k }",
        "GPT2_formal_text": "= (], [], []); E_t = E_p - 1; while (t = e) do { E_t = (1 - E_t) * e_t; } while (t = e) E_t = E_p + E_t;\n\nBasically, the idea is that using L_t as the teacher for creating a response works better than the simpler approach of just using E_t. The P_t part is there to help improve the response by using a specific sub-language vector. Formal: We added a CRF layer with a softmax function to the model to deal with the long-tail problem. Formal: We also tried out different ways to change the weights for the CRF layer and the softmax function. Formal: We tested the model with different initializations, like the one in Figure 1. Formal: We used a simple rule to pick the best weights for the CRF layer and the softmax function. Formal: After each round, we calculated the average value for each sub-phrase. Formal: We looked at how the model did when using different initializations and different sub-phrases. Formal: We also checked out how the model performed when starting with different initializations and different sub-phrases. Formal: We ran the model on four different datasets, each with different numbers of training steps and different sub-phrases. The results are in Table 4. Formal: We compared the models using the best setup and different sub-phrases. Formal: We also ran the model on three datasets with the same number of training steps and different sub-phrases. Formal: Finally, we calculated the average value for each sub-phrase. Formal: We used the top-1 and top-5 scores in Table 3 for our evaluation. Formal: The results are in Table 4, and they show the performance for the best setup and different sub-phrases. Formal: Formal: We also ran the model on three datasets with the same number of training steps and different sub-phrases. Formal: Formal: Lastly, we calculated the average value for each sub-phrase. Formal: We used the top-1 and top-5 scores in Table 3 for our evaluation. Formal: Formal: Finally, we calculated the average value for each sub-phrase. Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-short.4.json"
    },
    {
        "casual_text": "A typical way to add semantic meaning to language models is to check how similar a word is to its context and then tweak the probabilities from an n-gram model based on that. This helps the n-gram model, which usually focuses on short-term connections, also consider longer-term, more meaningful relationships. A lot of earlier research followed this idea (like Bellegarda in 2000, Coccaro and Jurafsky in 1998, and Wandmacher and Antoine in 2007), often using LSA to handle the semantic parts for individual words. Some researchers (Coccaro and Jurafsky, Wandmacher and Antoine) used the idea of a \"vector centroid\" to represent the context, while others (Bellegarda, Deng and Khundanpur) went with a \"pseudodocument\" approach, which comes from the relationship between documents and words in LSA. All of them calculate the probability of a word based on its context using cosine similarity, but they need to adjust it to get proper probability values. Gildea and Hofmann (1999) took a different route, using pLSA to create representations that already have a clear probabilistic meaning. This makes it easier to directly calculate the probability of a word based on its context without needing extra adjustments.",
        "formal_text": "A common approach to embedding semantic representations within language modeling is to measure the semantic similarity between an upcoming word and its history and use it to modify the probabilities from an n-gram model. In this way, the n-gram's sensitivity to short-range dependencies is enriched with information about longer-range semantic coherence. Much of previous work has taken this approach (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007), whilst relying on LSA to provide semantic representations for individual words. Some authors (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007) use the geometric notion of a vector centroid to construct representations of history, whereas others (Bellegarda, 2000; Deng and Khundanpur, 2003) use the idea of a \"pseudodocument\", which is derived from the algebraic relation between documents and words assumed within LSA. They all derive P(w i |h i ), the probability of an upcoming word given its history, from the cosine similarity measure which must be somehow normalized in order to yield well-formed probability estimates. The approach of Gildea and Hofmann (1999) overcomes this difficulty by using representations constructed with pLSA, which have a direct probabilistic interpretation. As a result, the probability of an upcoming word given the history can be derived naturally and directly, avoiding the need for ad-hoc transformations.",
        "GPT2_formal_text": "_y_t+1, and if p_t_i equals y, the prediction is made based on the attention mask. Formal: The variance of y_t, based on the attention mask, is calculated as a ratio of the total variance, calculated as τ_y_t * y_t + τ_p_t * y_t. Formal: (Equation 4) Formal: Here, τ_y_t is the total variation across the whole dataset, while τ_p_t is the variation between the query and the actual answer. The α_t is just the weight for the attention mechanism. Formal: Following a similar approach to how attention weights are assigned in attention-based sequence-to-sequence models, we use a two-layer perceptron to figure out the final prediction for the query q_t_i, based on the correct answer y_t_i. Formal: Equation 5 explains how the attention weight τ_y_t, τ_p_t, and α_t interact with each other. Formal: For the query query, we use a teacher model to create a query-specific attention vector w_t, which helps the model learn how to answer each query question better. Formal: Formal: We create a query-specific attention vector for the keyphrase key_k, like this: Formal: We take the attention vector τ_y_t, τ_p_t, and α_t and multiply them by w_t_k to get the query-specific attention vector for the keyphrase key_k. Formal: Formal: Then, we mix all the query-specific attention vectors together to get the final query-specific attention vector w_t_k. Formal: Formal: To train the model, we take the average of the outputs from three different attention heads, like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D09",
        "filename": "D09-1045.json"
    },
    {
        "casual_text": "After that, one of the authors went through the explanations the experts gave and put them together.",
        "formal_text": "Following this step, one of the authors read the explanations provided by the experts and formed",
        "GPT2_formal_text": "= {q_v_i, q_v_m}  \nu_v_i, u_v_m = (r_v_i * r_v_m) / (r_v_i + 1)  \nu_v_m = (r_v_i * r_v_m + 1) / (r_v_m + 1)  \n}\n\nBasically, it's a way to combine the vector representations of v_i and v_m, using something called a matrix. Formal: The chance of something happening is calculated by looking at all the possible ways to do it. Formal: We think the reranking model is way better than the older neural machine translation (NMT) model, and it works really well too. Formal: We also see big improvements in reranking models compared to NMT models in the MIMIC-III dataset. Formal: This shows that the reranking model is better than the old NMT model. Formal: The MT model struggles because it has to use a ton of different patterns to match words correctly, but the reranking model can skip all that and still make the right match. Formal: This also shows that the reranking model is better than the old NMT model. Formal: We're trying to figure out how to use the reranking model to improve how well the word segmentation process works. Formal: This is like solving a puzzle with lots of pieces, where we want to figure out how to combine these pieces to get the best possible result. Formal: Formal: We're not trying to build a complete reranking model for every possible way to rephrase words. Formal: Instead, we're focusing on the best reranking model we can build using the neural machine translation model. Formal: Also, we're not trying to create a general reranking model that can handle any situation. Formal: The improvement we've seen in MIMIC-III is a good sign that our approach is better than the old NMT model. Formal: We tested the reranking model in the MIMIC-III dataset and found that it works just as well as the older NMT model. Formal: We're not trying to build a complete reranking model for every possible way to rephrase words. Formal: Instead, we're focusing on the best reranking model we can build using the neural machine translation model",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "So, φ is the Gram-Schmidt process (check out Section 3.3 for more info) applied to the square matrix M r (i). The result, φ(M r (i)), is an orthogonal matrix that comes from M r (i). And ẽ t is basically the combination of all the sub-vectors ẽ t (i) from Equation 1, like, you know, putting them all together.",
        "formal_text": "where φ is the Gram Schmidt process (see details in Section 3. 3) applied to square matrix M r (i). The output transform φ(M r (i)) is an orthogonal matrix derived from M r (i). ẽ t is the concatenation of all sub-vectorẽ t (i) from Eq. 1, e. g. ,",
        "GPT2_formal_text": "_t * hidden_t ) = conv_attn(attn, b, hidden_t, d_t ); }\n\nBasically, this is saying that the hidden representation of b is calculated using the convolution operation, and d_t is the Dense layer. The whole thing is based on the max operation, which is a matrix with dimensions n×d_t. Formal: The convolutional layer of a bidirectional LSTM can be thought of as a way to grab information from both directions. Formal: The convolutional layer of a bidirectional LSTM can be seen as a way to grab information from both directions. Formal: In this part, we’re talking about the different types of attention and how they work. Formal: For a specific bidirectional LSTM, the attention is calculated using two things: a (hidden) vector h_t and a (d_t) vector d_t. The d_t vector is used to make sure the attention is focused on the right target. Formal: We’re also introducing two new attention mechanisms. Formal: For a specific bidirectional LSTM, the attention is calculated using two things: a (hidden) vector h_t and a (d_t) vector d_t. The d_t vector is used to make sure the attention is focused on the right target. Formal: The final attention vector is calculated using a (hidden) vector h_t and a (d_t) vector d_t, which gives us the final representation h_t. Formal: Finally, the output h_t from the bidirectional LSTM is what we use to calculate the attention vector for the whole sequence. Formal: There are a couple of issues with this approach. First, the attention weights are only calculated for the current token, which doesn’t help the model learn the attention for all the tokens. Second, we’re also assuming that the target embedding has already been learned. Formal: We’ve come up with a new way to calculate attention weights for bidirectional LSTMs. Formal: We’ve developed a new way to calculate attention weights for bidirectional LSTMs. Formal: The hidden vectors of the tokens we’re interested in are calculated using two different vectors: the attention vector h_t and the attention vector h_t.",
        "directory": "acl",
        "filename": "2020.acl-main.241.json"
    },
    {
        "casual_text": "In this paper, we take a closer look at how one of the cool new things in SEAL—using character-level methods to spot potential regular structures, or wrappers, in web pages—affects performance. Back in the day, some early systems for analyzing web pages, like WIEN (Kushmerick et al., 1997) and DIPRE (Brin, 1998), did work at the character level, but more recent methods for set expansion have mostly focused on tokenized or parsed free text (Carlson et al., 2009; Talukdar et al., 2006; Snow et al., 2006; Pantel and Pennacchiotti, 2006) or used tricks to take advantage of HTML structures that often contain lists and tables (Nadeau et al., 2006; Etzioni et al., 2005).",
        "formal_text": "In this paper, we explore the impact on performance of one of the innovations in SEAL, specifically, the use of character-level techniques to detect candidate regular structures, or wrappers, in web pages. Although some early systems for web-page analysis induce rules at character-level (e. g. , such as WIEN (Kushmerick et al. , 1997) and DIPRE (Brin, 1998) ), most recent approaches for set expansion have used either tokenized and/or parsed free-text (Carlson et al. , 2009; Talukdar et al. , 2006; Snow et al. , 2006; Pantel and Pennacchiotti, 2006), or have incorporated heuristics for exploiting HTML structures that are likely to encode lists and tables (Nadeau et al. , 2006; Etzioni et al. , 2005).",
        "GPT2_formal_text": "to create better representations of entities in the query and document sets. This setup can be used for both training and testing. Figure 2 shows how well each method for predicting query and document entities works on its own. To keep things simple, we'll just talk about the results for the top 10 entities in Table 4 and the top 100 in Table 5. Formal: Let's start by looking at how well different entity recognition models (ER) perform on the test set. Formal: We'll break down the results for the top 10, top 100, and top 200 entities in Table 4 and Table 5, respectively. Formal: The results from the top 10 entities are shown in Table 4. Figure 3 shows the results for the top 20 entities in Table 5. Formal: The results for the top 20 entities in Table 5. Formal: The results for the top 50 entities in Table 5. Formal: The results for the top 100 entities in Table 5. Formal: The results for the top 200 entities in Table 5. Formal: The results for the top 300 entities in Table 5. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1156.json"
    },
    {
        "casual_text": "Okay, so here's how we handle the recipe rewrite task in a straightforward way: we just look up the dish online and find a version that matches the dietary restriction. Starting with the original recipe, we figure out what dish it’s for, then search through all the recipes we have (training, dev, and test) to find one for the same dish that fits the dietary requirement.",
        "formal_text": "Retrieval: We imitate a simple approach to the recipe rewrite task: searching the web for a version of the dish that obeys the given dietary constraint. Given a source recipe, we determine the dish to which this recipe belongs and retrieve a recipe for the same dish that fits the dietary constraint from the combined pool of train, dev, and test recipes.",
        "GPT2_formal_text": "ed, while the other focuses on the masked entity. Formal: To make sure the attention layer and the output layer don't mess with each other, we pick the best k words from the target embedding to create the output. Formal: This setup ensures we don't have any overlapping words or anything that could change the prediction. We tested a few different versions of this, but the one that worked best for our experiments was the one with a batch size of 1, which we call \"d.\" Formal: In the experiments we did, we used a batch size of 1 to set up the output layers. Formal: We also tried using a multi-head attention model with two heads (which we call \"m\" in this paper) that were trained on the target and the source. This setup got us the best results in our tests. Formal: We ran the experiments 5 times with different random seeds, picking the one that gave us the best F1 score. The results are shown in Figure 3. The results for different K values of K are: Formal: For the final setup, which uses the source embedding, we set the batch size to 1. Formal: The results for different K values of K are: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.526.json"
    },
    {
        "casual_text": "Learning to rank has been a hot topic in the information retrieval field lately. There are different methods like pointwise, pairwise, and listwise approaches that have been studied a lot (Liu, 2009). Some of these methods have been used for document summarization, such as SVR (Ouyang et al., 2007), classification SVM (Wang et al., 2007), and RankNet (Svore et al., 2007). However, no one has really compared these ranking algorithms in a systematic way. As far as we know, this is the first time a listwise learning-to-rank algorithm called ListNet (Cao et al., 2007) is being used for document summarization in this paper. Plus, pairwise and listwise learning-to-rank algorithms have never been applied to concept ranking for extractive summarization before.",
        "formal_text": "Learning to rank attracts much attention in the information retrieval community recently. Pointwise, pairwise and listwise learning-torank approaches have been extensively studied (Liu, 2009). Some of those have been applied to document summarization, such as SVR (Ouyang et al. , 2007), classification SVM (Wang et al. , 2007), and RankNet (Svore et al. , 2007). Again, there is no work to systematically compare these ranking algorithms. To the best of our knowledge, this is the first time that a listwise learning-to-rank algorithm, ListNet (Cao et al. , 2007), is adapted to document summarization in this paper. Moreover, pairwise and listwise learning-to-rank algorithms have never been used to perform concept ranking for extractive summarization.",
        "GPT2_formal_text": ", which are basically vector representations of the types of entities we're looking at. For instance, in the instance shown in Figure 1, the input to the mBERT model would be {x 1, x 2, ..., x n }, where each x i, j is part of a group where the label y i belongs to, and we're dealing with the idea that entities with label y i probably aren't entities at all. Formal: The embeddings for the entities in a sentence are represented by the entity embeddings, E e i. Formal: Here, e i is the embedding for entity i, and e i+1, e i+2, ..., e i+n represent the embeddings for entities j, which are parts of the sentence. Formal: For the entity embeddings, we use a GCN to make sure the vector representations match up with the semantic representations. Formal: We check if the embeddings are actually capturing the entity types or if they're just repeating words. If they're capturing the entities, we say the embedding is capturing the entity types. If it's just repeating words, we say it's not capturing the entities. Formal: We show how to train the entity embeddings using the same neural network setup as in Equation 4. Formal: Here, e i is the embedding for entity i, and e i+1, e i+2, ..., e i+n represent the embeddings for entities j, which are parts of the sentence. Formal: We use the entity embedding to figure out the semantic representations for the entity types in the sentence. We do this by using the entity embedding to find the embeddings for the entities in the sentence. Formal: Here, e i is the embedding for entity i, and e i+1, e i+2, ..., e i+n represent the embeddings for entities j, which are parts of the sentence. Formal: We're using the entity embedding to find the embeddings for the entity types in the sentence. Formal: We're using the entity embedding to find the embeddings for the entity types in the sentence. Formal: Here, e i is the embedding for entity i, and e i+1, e i+2, ..., e i+n represent the embeddings for entity j, which are parts of the sentence. Formal",
        "directory": "C10",
        "filename": "C10-2060.json"
    },
    {
        "casual_text": "Nie et al. (2020) came up with a thing called SA-NER that uses some fancy methods to deal with data sparsity issues. Basically, they grab extra semantic info from a huge collection of text and then use two cool tools: an attentive semantic augmentation module and a gate module. These tools help them organize and combine all that extra info.",
        "formal_text": "• Nie et al. (2020) (SA-NER), which utilize semantic enhancement methods to reduce the negative impact of data sparsity problems. Specifically, the method obtains the augmented semantic information from a largescale corpus, and proposes an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively.",
        "GPT2_formal_text": "ing. To make sure we keep the attention attention and mask mechanisms intact, we need to handle the variable β to keep the attention part's input and output intact. Formal: • Typed Attention: Instead of using the full attention mechanism, we use a single-mode typed attention thingy. Formal: Formal: We also throw in some linear algebra stuff like L(λ_t) and L(λ_q) to help make the model more understandable. Formal: This helps keep the model accurate and trustworthy. Formal: For the comparison, we’re using the same setup as the original paper. Formal: Formal: We’re sticking with the same hyperparameters as their original paper. We’re skipping the final layer of M and skipping the softmax function. Formal: Formal: The results? Check out Table 1 for the results of our experiments with different attention mechanisms. Formal: To measure how well the model is doing, we look at precision and recall. Formal: To see how many keyphrases we’ve identified, we compare the number of keyphrases to the total number of sentences. Formal: We report precision, recall, and the F1 score for the top N keyphrases. Formal: We also report how many tokens were extracted for each keyphrase. Formal: To figure out how important keyphrases are for our model, we take the average of the DPM scores for the top N keyphrases. Formal: Formal: We also look at how many tokens were extracted for each keyphrase. Formal: Formal: Lastly, we’re reporting the DPM score for each keyphrase. Formal: Formal: For the optimization part, we’re using stochastic gradient descent (SGD) with a learning rate of 5e-5 and a learning rate decay of 2e-5. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.383.json"
    },
    {
        "casual_text": "Argumentation Mining (AM) is all about spotting the parts of arguments, like claims and premises, and figuring out how they support or attack each other to show how arguments are structured. This has been done in a bunch of areas recently, like legal documents (Mochales Palau and Ieven, 2009), news articles (Deng and Wiebe, 2015; Sardianos et al., 2015), and user-generated content (Wachsmuth et al., 2014; Habernal and Gurevych, 2015). The goal is to automatically find arguments in messy text by sorting out what's argumentative and what's not, and then pulling out the key components and how they connect.\n\nLately, there's been more interest in making tools that use AM to help people with their arguments, especially for students who need feedback on their writing (Song et al., 2014; Stab and Gurevych, 2014a, b; Wambsganss et al., 2020b). These tools give personalized tips on how to improve their argumentation. But, using this tech in schools hasn't really taken off yet (Stab and Gurevych, 2017b; Lawrence and Reed, 2019; Rosé et al., 2008), mostly because there aren't many datasets with student-written texts that have been labeled for argumentation (Lawrence and Reed, 2019; Wambsganss et al., 2020c).",
        "formal_text": "Argumentation Mining AM aims to identify argument components in the form of claims and premises, along with support and attack relationships that model the discourse structure of arguments. In recent years, this has been done for several domains, including legal texts (Mochales Palau and Ieven, 2009), newswire articles (Deng and Wiebe, 2015; Sardianos et al. , 2015), or usergenerated content (Wachsmuth et al. , 2014; Habernal and Gurevych, 2015). The objective is to automatically identify arguments in unstructured textual documents based on the classification of argumentative and non-argumentative text units and the extraction of argument components and their relations. Recently, researchers have built increasing interest in adaptive argumentation support tools based on AM (Song et al. , 2014; Stab and Gurevych, 2014a, b; Wambsganss et al. , 2020b), offering argumentative writing support to students by providing individual feedback about the argumentation discourse. However, utilizing this technology in a pedagogical scenario for educational purposes lacks a wider-scale adoption (Stab and Gurevych, 2017b; Lawrence and Reed, 2019; Rosé et al. , 2008), as argumentation-annotated corpora with student-written texts are rather rare (Lawrence and Reed, 2019; Wambsganss et al. , 2020c).",
        "GPT2_formal_text": ", and the average score for the whole collection. Formal: We're suggesting an updated version of the method we talked about earlier that uses two big collections of text. Formal: The updated method uses the titles and authors of the papers as its key points and figures out the coreference links between them. Formal: We also came up with a way to check how well these coreference links work. Formal: We trained a classifier to predict the probability of a title being the antecedent of a document. The training data is all about how titles are tagged with mentions of authors. The classifier uses two things to check: Formal: In our experiments, we only looked at the text from the title and authors. Formal: We tested our model on two big datasets, both from the biomedical field. We also tried it on the first dataset from the book field. Formal: We did this to see how well our model works on different types of text. Formal: There's also a paper evaluation benchmark, called XSUM, that we used to test how well our model performs. Formal: We also used the validation data for evaluation. Formal: When we tested our model on the validation set, we used the same evaluation metrics as other studies. Formal: We got 981 correct matches for the titles and authors, and 532 for the abstracts. Formal: When we compared our results to earlier studies, we focused on the titles and abstracts. Formal: We did this because the validation set was bigger than what we used for our other experiments. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.599.json"
    },
    {
        "casual_text": "Alright, so this just gives us the axiom links. We still need to figure out the word order so that none of the axiom links cross. Turns out, this can be done in less than quadratic time—it’s basically a bracketing problem.",
        "formal_text": "Actually, this only gives us the axiom links. It still requires to compute the word order to have no crossing axiom link. This can be done from the axiom links easier than quadratic time (it is a bracketing problem).",
        "GPT2_formal_text": "_type_cased and attention_mask_type_ld. Formal: This part talks about the models and their structure. Formal: Since the model uses softmax to make predictions, the first layer is set up to maximize the log likelihood. Formal: We can see that the LSTM layers in the model use different types of attention. We’ll show a few examples to explain each attention type. Formal: Let’s break down the attention weights for the target query x and the keys and values in the query x. Formal: For each word in the target word's vocabulary, there are K different attention weights, which we’ll call w_t1, ..., w_tK. We’ll call the attention weight that picks the best key as the \"Attn\" layer. Formal: Also, there are K different attention weights for each word, and they’re all in the same order, so we’ll call them w_t1, ..., w_tK. Formal: The key attention weight for the word x is calculated by multiplying the attention weights for the words that come before and after it. Formal: The query attention weight for the word x is calculated by multiplying the attention weights for the words that come after it. Formal: Finally, there’s another attention weight that decides how much weight to give to the word x in a query. Formal: We’ll also talk about the probabilities of each attention weight for the target word, and the final attention weights for the words that come before and after it. Formal: To help you understand, we’ll give a quick summary of the attention weights for a query word x. Formal: Basically, for a query word x, the attention weights for the words that come before and after it are different. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C00",
        "filename": "C00-2091.json"
    },
    {
        "casual_text": "The language stuff we're talking about here comes from basic grammar rules that are pretty common and not just specific to Hindi.",
        "formal_text": "The linguistic features exploited here are drawn from a range of well-understood grammatical features and are not peculiar to Hindi alone.",
        "GPT2_formal_text": ", ctx_type, and p_type. Formal: For each image, the caption gives a simple yes or no answer. It also includes the size of the image, which is u. Formal: These two parts (Section 4.1) are used to figure out the caption's length. Formal: We use a straightforward yes or no question (the length part) to figure out if the image has enough words. Formal: First, we grab all the words from the image that have the word \"yes\" in them. Then, we match those words to the English words in the image caption. Formal: Next, we look up each word in the image captions in the list of words {w1, w2, ..., wn} in the vocabulary. If a word fits into this group of words, it gets a 0 for the length part and a 1 for the question part. Formal: The last step is to predict the image caption. Formal: After that, we use some rules to combine these parts into one caption. Formal: Formal: We start by creating a new list of English words for each image caption, based on the model we just trained. Formal: Then, we pick a bunch of possible caption words from this new list, and we adjust the question to make sure the captions sound natural and easy to understand. Formal: After that, we use a translation model to translate these captions into French. Formal: Finally, we match these French captions to the original English captions using the methods we talked about earlier. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C12",
        "filename": "C12-1152.json"
    },
    {
        "casual_text": "Alright, let's break down the process shown in Figure 1, which outlines our Graph2Tree framework. First off, Graph2Tree starts by using a BiLSTM to encode the MWP text input. At the same time, it builds two graphs: the Quantity Cell Graph and the Quantity Comparison Graph. The BiLSTM gives us word-level representations, which we use as the nodes for these graphs.\n\nNext, these node representations, along with the two graphs, are fed into a graph transformer. This transformer has a modified multiGCN component that focuses on learning the graph representation based on the Quantity Cell Graph and Quantity Comparison Graph. This tweak helps the model understand the relationships between quantities and their numerical qualities, making the final graph representation richer.\n\nAfter that, pooling is applied to combine all the nodes into a single graph embedding vector, which is the output of the graph transformer. Finally, this graph representation, along with the updated node representations, is passed to a tree-structure decoder. The decoder then works to figure out the final solution expression tree.",
        "formal_text": "3 Methodology Figure 1 shows our proposed Graph2Tree framework. Graph2Tree first encodes the MWP text input using BiLSTM and simultaneously constructs Quantity Cell Graph and Quantity Comparison Graph. The output of BiLSTM, word-level representations, are used as node representations. Together with the two constructed graphs, the node representations are input into a graph transformer to learn a graph representation of the MWP. The multiGCN component of the graph transformer is modified to learn the graph representation based on the Quantity Cell Graph and Quantity Comparison Graph. This enriches the final graph representation with quantities' relationship information and numerical qualities. Pooling is used to aggregate all nodes into a pool-based graph embedding vector as the graph transformer's output. Finally, the output graph representation and the updated node representations are used as input to a tree-structure decoder to infer the final solution expression tree.",
        "GPT2_formal_text": "_tensor_cubed. The rest of the model is trained using a log-linear setup. The goal of the multi-task learning is to maximize the log-likelihood of the predictions. Formal: The c in our sequence labeling model is made up of three parts: one for the source sequence (s), one for the target sequence (t), and one for the relation context (c). Here, s is the source sequence, t is the target sequence, and c is the relation context. Formal: We use a special format called a document vector (like the one in Figure 1) to keep track of the source and target sequences, along with the context. Formal: The input and output from the pre-trained language model (PLM) are passed through the attention module to create the context vector. We also use a softmax function to get the likelihood of the target sequence s t. Formal: Since the relation context is important for both the source and target sequences, we combine it with the source representation c t. Formal: The final representations for the source and target sequences are combined into one feature vector using the cross-attention mechanism. Formal: Once we have all the source and target sequences, we use the cross-attention mechanism to combine them into a single representation, which we then send to the PLM to predict the relation class. Formal: The final representation of the prediction is a matrix M t, which is made up of the source and target representations. Formal: Finally, we calculate the cross-attention function f and the softmax function g, which give us the probability and the log-likelihood for each relation class. Formal: Formal: This approach is similar to the one used in a previous study by Li et al. (2015a). Formal: Figure 1 gives an example of how we represent the source and target sequences. Formal: We start by training the PLM using different target and source sequences to get the context vector C t (s t, t ). Formal: The whole process for the PLM starts with a sequence labeling model, which can be either SVM or CNN. Formal: The label sequence encoder takes the source and target sequences from the previous step (S e = s s t, t ). Formal: After that, we use the cross-attention mechanism to figure out the relation contexts for the target sequence s t. Formal: Finally, the final representations for the",
        "directory": "acl",
        "filename": "2020.acl-main.362.json"
    },
    {
        "casual_text": "You can find the MELD dataset here: https://github.com/SenticNet/MELD, and the IEMOCAP dataset here: https://sail.usc.edu/iemocap/iemocap_release.htm. Basically, A is a matrix in R w×dq, where w stands for the total number of window segments.",
        "formal_text": "https: //github. com/ SenticNet/MELD, https: //sail. usc. edu/ iemocap/iemocap_release. htm i. e. , A ∈ R w×dq where w represents total window segments.",
        "GPT2_formal_text": "= n_labels; h_labels = h_labels + 1; n_outputs = n_outputs; n_labels_seg = n_labels; n_labels = 1; output_ids = output_ids + 1; output_labels = output_labels + 1;\n\nThe more output labels there are, the bigger the training set needs to be to meet the predicted value for the label with the highest score. We measure how well the model performs using accuracy, which we calculate using the ROUGE-1-2 metrics from the validation set. We also report a micro-average performance, using the micro-average F1 score, to get a clearer picture of the model's performance. Formal: Here, `labels` is the number of labels in the training set, `labels_seg` is the number of labels in the validation set, `labels_seg_len` is the number of labels in the segments with the highest score, and `labels_seg` is the number of labels in the segments with the lowest score. Formal: To calculate the performance of the model, we first calculate the ROUGE-1-2 performance for each segment `seg` (which is the number of labels in that segment) using the validation set. Then, we check if `labels_seg` is greater than `labels_seg_len` and if `labels_seg` is less than `labels_seg_len`. If `labels_seg` is greater than `labels_seg_len`, we say the segment is correct, otherwise, it's wrong. Formal: Following what Shen et al. (2019) did, we calculate the precision, recall, and F1 score for each segment `seg` using the validation set. Formal: The model was trained using an R-GA loss (from Shen et al., 2019) with a learning rate of 0.001 and a batch size of 64. Formal: We tested the model with different loss functions and different batch sizes. The model was trained with an R-GA loss with a learning rate of 0.001 and a batch size of 64. Formal: We trained the model with an R-GA loss with a learning rate of 0.001 and a batch size of 64. Formal: We trained the model with an R",
        "directory": "acl",
        "filename": "2020.acl-main.402.json"
    },
    {
        "casual_text": "Check out the appendix for the PPLM details. And yeah, the CTRL details are in there too. For the implementation, we're using the one from this GitHub link: https://github.com/gooppe/transformer-summarization.",
        "formal_text": "See appendix for PPLM implementation details. 11 See appendix for CTRL implementation details. 12 We use the implementation at https: //github. com/gooppe/transformer-summarization.",
        "GPT2_formal_text": "is a small fixed-size value that decides how much of the input's context is being ignored. It's calculated using the inner product. Formal: Here, |s₁| represents the number of token embeddings, and the inner product is just a way to show how much the token embedding is contributing to the prediction. Formal: To get a better understanding of how tokens contribute, we ran some experiments with different attention methods. Turns out, the best results come when you set the attention weights to -2, which gives a big boost to how well the model performs. The only exception is when using the fully-connected layer for the masked language model. Formal: After combining the hidden layer and the output layer, the model's output gets combined with the current token embedding to create a new embedding for the masked token. This new embedding is then sent through a non-linear transformation to get its representation. The \"filter\" (which is a hyperparameter we picked) decides how much of the input's hidden state is used for prediction. For more details, check out the paper by Sun et al. (2019a). Formal: The final output embedding for the masked token is then combined with the current token embedding, and this combined representation is sent through a linear transformation to get its representation. The \"filter\" (a hyperparameter we picked) decides how much of the input's hidden state is used for prediction. Formal: The final output embedding for the masked token is then combined with the current token embedding, and this combined representation is sent through a linear transformation to get its representation. Formal: Lastly, the probability distribution of the output is calculated using the inner product. Formal: Formal: We ran experiments on a model trained on the MNLI dataset. The results are in Table 2. Formal: We tested our model with different attention methods and found that using the fully-connected layer with a layer size of 1 gave the best results. This layer is also used in SVM by Dyer et al. (2019), which performs really well on the dataset. Formal: In our experiments, we did a word-level error analysis and found that the F1 score was pretty similar to SVM and SVM-ATT. Formal: We also did some experiments on a CNN model that was trained on the Common Crawl corpus, and the results are in Table 3. Formal: We're planning to work on a dataset",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.526.json"
    },
    {
        "casual_text": "When figuring out vocabulary sizes and word stats, we usually clean up the text by making everything lowercase and getting rid of any characters that aren't letters or numbers.",
        "formal_text": "When computing vocabulary sizes and word statistics, text transcripts are normalized by lower-casing all the alphabets and removing characters that are neither alphabets nor digits.",
        "GPT2_formal_text": ", and first_name, last_name, and age_group all have indices of 0.\n\nThe correlation is calculated using Pearson's correlation coefficient. We're using Spearman's ρ to check for significant differences between groups. We ran all the experiments 10 times to get the average result. Formal: In Table 5, we show how well each method did, using the F1 score as a measure. Formal: In Table 4, we show how well the full model did, using the F1 score as a measure. Formal: The results for the best model are in Table 5. Formal: If we're looking at the same prediction, the full model usually gets the prediction right. But if we're changing predictions, like changing the age group, the model tends to make the old prediction more likely, as shown in Table 4. Formal: For the first prediction, the full model makes the prediction right, but for the second one, the model makes the prediction wrong. The correlation is 0.39, which is pretty low. Formal: The accuracy for the first prediction is better than the second one, which is better than the first prediction and the second prediction. Formal: There are some really high correlations—like +0.9 for the first prediction, +0.83 for the second prediction, and +0.85 for the third prediction. These high correlations suggest that the method we're looking at can really vary how accurate its predictions are. Formal: Using these high correlations, we can make a solid prediction about the age, based on the full model. Formal: The accuracy for the first prediction is better than the second prediction, and the accuracy for the third prediction is better than the second prediction. The correlation is 0.33, which is pretty high. Formal: The third prediction is really good, and the second prediction is okay too. Formal: The correlation for the first prediction is high, but the second prediction is okay too. Formal: The third prediction is good, but the second prediction is okay too. Formal: The correlation for the third prediction is high, but the second prediction is okay too. Formal: The third prediction is good, but the second prediction is okay too. Formal: The correlation for the third prediction is high, but the second prediction is okay too. Formal: The third prediction is good, but the second prediction is okay too. Formal: The correlation for the third prediction is high, but the",
        "directory": "acl",
        "filename": "2021.acl-long.411.json"
    },
    {
        "casual_text": "When you’ve got the annotated document ready, it’s pretty straightforward to use it to figure out p(t a |t b ). The document should have all the info you need, even if someone forgot to add some tags. Here’s the formula we use to estimate p(t a |t b ) based on how often tags and words show up together:",
        "formal_text": "When the content of the annotated document is available, using it for estimating p(t a |t b ) is a natural thought. The content is expected to be complete and information-rich whether or not the user has omitted any tags. We use the following formula to estimate p(t a |t b ) by tag-word cooccurrences:",
        "GPT2_formal_text": "ed = [masked_subreddits, class_embeddings] label_embedding = [embedding, token_embeddings] For the LSTM model, we'll be using a basic version that just uses word2vec embeddings and a softmax activation function. It's pretty straightforward and works well. We'll call this model the feedforward neural network with attention. Formal: The loss function we're talking about is basically the log loss function, l_t. This loss helps us figure out the chance that a token t = s_t. Formal: The attention matrix A_t_i is what we use for the i-th token, s_i. It's a matrix with dimensions n×d and it's built from the output from the sub-tokens, x_i. Formal: After we've got this matrix A_t_i, we can combine it with the attention mask, A_t_i_1, to get the attention mask for the i-th token, s_i_1. Formal: Lastly, we calculate the attention score, A_t_i, by combining the scores from the sub-tokens. Formal: Formal: Equation (2) is basically the same as the usual attention matrix A_t_i. Formal: Formal: Basically, we're looking for the token with the highest attention score, which we call the attention mask for that token, s_i_1. Formal: Formal: We can see how the tokens are grouped together using the attention score A_t_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C10",
        "filename": "C10-1114.json"
    },
    {
        "casual_text": "• Data Sets: As mentioned in Section 3, the annotated question-answer pairs are from three different areas. For each area, we divided the data into a training set (80% of each category) and a test set (20% of each category). On top of that, we kept 10% of the training set aside as development data to tweak the parameters.",
        "formal_text": "• Data Sets: As introduced in Section 3, the annotated QA text pairs cover three different domains. In each domain, we randomly split the data into a training set (80% in each category) and a test set (20% in each category). In addition, we set aside 10% from the training set as the development data for parameters tuning.",
        "GPT2_formal_text": "_t) counts the unique IDs from both the source and target texts for a particular mention t. We measure how similar the mentions are by checking the cosine similarity between their embeddings. To get the mention embedding vector, we calculate it like this: Formal: Basically, this is a way of representing the mention as a single number (like 0, 1, ..., n). Formal: To calculate the embedding for a mention m, we first use a BiLSTM to convert the mention's embedding into its numerical form, giving it a numerical value. Then, we calculate the cosine similarity between these m's embeddings. Formal: We use a fancy model called the multi-head attention mechanism (thanks to Vaswani et al., 2017) to figure out the attention weights for the source and target mentions. Formal: To handle the cross-entropy loss, we calculate the cosine similarity between the mention embedding and the target embedding using a linear transformation. Formal: From what we've seen in practice, it's a good idea to handle the cross-entropy loss separately for the source and target mentions. Formal: If the source mention is longer than the target, the cross-entropy loss isn't as important, so we focus more on the target embedding to make sure the loss is balanced. Formal: The model is trained using a maximum likelihood estimation method (like the one from Collobert et al., 2011) and then tested on the validation set. Formal: For instance, if the validation set has 100 mentions, the model gets a score of 0.83. Formal: In the zero-shot setup (where we don't have any target instances), we adjust the loss function a bit by adding up the cross-entropy loss for each mention. Formal: We use the cosine similarity between the mention embedding and the target embedding to calculate the similarity vector for a specific mention, m. Formal: Formal: For the example in Table 5, we train the model using a batch size of 32 and a batch size of 4 for the source and target domains. Formal: Formal: The MLE loss is calculated using the cross-entropy loss for each mention m, considering both the source and target domains. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1401.json"
    },
    {
        "casual_text": "We picked Japanese literature because the researchers in that area really wanted a better system. The usual keyword-based systems just didn’t cut it for literature stuff.",
        "formal_text": "One of the reasons we chose the field of Japanese literature was that there was strong demmld from th. e researchers in that field to make a useful system; systems based on conventional key words did riot work welt for the field of literature.",
        "GPT2_formal_text": "_x) = { src_x = [src_x], attn_x = [attn_x], //[attn_x], attn_mask_y = [attn_mask], attn_mask_z = [attn_mask], attn_mask_m = [attn_mask], attn_mask_w = [attn_mask], attn_mask_h = [attn_mask], attn_mask_t = [attn_mask], attn_mask_g = [attn_mask], attn_mask_b = [attn_mask], attn_mask_u = [attn_mask], attn_mask_v = [attn_mask], attn_mask_l = [attn_mask], attn_mask_r = [attn_mask], attn_mask_g = [attn_mask], attn_mask_b = [attn_mask], attn_mask_u = [attn_mask], attn_mask_v = [attn_mask], attn_mask_l = [attn_mask], attn_mask_r = [attn_mask], attn_mask_g = [attn_mask], attn_mask_b = [attn_mask], attn_mask_u = [attn_mask], attn_mask_v = [attn_mask], attn_mask_l = [attn_mask], attn_mask_r = [attn_mask], attn_mask_g = [attn_mask], attn_mask_b = [attn_mask], attn_mask_u = [attn_mask], attn_mask_v = [attn_mask], attn_mask_l = [attn_mask], attn_mask_r = [attn_mask], attn_mask_g = [attn_mask], attn_mask_b = [attn_mask], attn_mask_u = [attn_mask], attn_mask_v = [attn_mask], attn_mask_l = [attn_mask], attn_mask_r = [attn_mask], attn_mask_g = [attn_mask",
        "directory": "C86",
        "filename": "C86-1089.json"
    },
    {
        "casual_text": "We're using β to draw a comparison to inside probability—well, more specifically, the Viterbi approximation of inside probability. This is because we're focusing on maximizing instead of summing up all the possible parses.",
        "formal_text": "The use of β is intended to suggest an analogy to inside probability-or more precisely, the Viterbi approximation to inside probability (since we are maximizing rather than summing over parses).",
        "GPT2_formal_text": "= {t_i, t_j, ..., t_n, t_n+1}  \n\nThe pairwise relevance scores for a pair are:  \nr_i: If the input is a pronoun, then r_i is 1.  \nr_j: If the input is a pronoun, then r_j is 0.  \nr_p: If the input is a proper noun, then r_p is 1.  \nr_s: If the input is a proper noun, then r_s is 0.  \nr_c: If the input is a proper noun, then r_c is 1.  \nr_v: If the input is a proper noun, then r_v is 0.  \n\nAs mentioned earlier, we don't handle cases where the first word in a sentence is a pronoun. This makes the model's performance drop. But, if the input is a pronoun, we can still calculate relevance scores for the words after the pronoun. Formal: So, the relevance scores for a pair are basically the accuracy (recall) of the model that creates the pair. Formal: Since this accuracy is calculated using the two-level relevance scores (r_i and r_j), the accuracy is directly related to the logarithm of the correlation between the two scores. Formal: This is a bit more complicated than just using the average of all the relevance scores for a pair. Formal: The function θ_ij is basically the inverse of the logarithm of the correlation between the relevance scores for a pair. Formal: Finally, the correlation between the relevance scores for a pair can be written as: Formal: So, the relevance scores for a pair are basically the accuracy (recall) of the model that creates the pair. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1105.json"
    },
    {
        "casual_text": "Classification systems, whether they're basic logistic regression or fancy neural networks, usually predict the likelihood of different classes and pick the one with the highest probability. We then check how well these predictions match the actual labels (called ground-truth labels) on new, unseen data to see how good the model is. But sometimes, we need to pay close attention to how confident the model is in its predictions, not just whether it got the right answer. For example, if the model's confidence levels are accurate, it can help us figure out if a tool predicting someone's likelihood of reoffending is fair (Chouldechova, 2017) or decide the best number of labels for medical diagnoses (Kavuluru et al., 2015). Guo et al. (2017) pointed out that even if a model is really good at getting the right class, it doesn't always mean it's good at estimating how sure it is about that prediction.\n\nTo fix models that aren't great at showing their confidence, we use calibration methods (like the ones from Zadrozny and Elkan, 2001; Platt et al., 1999; Guo et al., 2017; Kumar et al., 2019). These methods tweak the probabilities the model gives after it's been trained. They basically retrain the model a bit on a separate validation set to get a better idea of how confident it should be, and then apply that to new test data. The problem is, this can mess up the consistency of the model's predictions across different data sets. Since the data is split into fixed parts, the calibration doesn't really adapt to how well the model is doing, making it kind of rigid and not very flexible.",
        "formal_text": "Classification systems, from simple logistic regression to complex neural network, typically predict posterior probabilities over classes and decide the final class with the maximum probability. The model's performance is then evaluated by how accurate the predicted classes are with respect to outof-sample, ground-truth labels. In some cases, however, the quality of posterior estimates themselves must be carefully considered as such estimates are often interpreted as a measure of confidence in the final prediction. For instance, a well-predicted posterior can help assess the fairness of a recidivism prediction instrument (Chouldechova, 2017) or select the optimal number of labels in a diagnosis code prediction (Kavuluru et al. , 2015). Guo et al. (2017) showed that a model with high classification accuracy does not guarantee good posterior estimation quality. In order to correct the poorly calibrated posterior probability, existing calibration methods (Zadrozny and Elkan, 2001; Platt et al. , 1999; Guo et al. , 2017; Kumar et al. , 2019) generally rescale the posterior distribution predicted from the classifier after training. Such post-processing calibration methods re-learn an appropriate distribution from a held-out validation set and then apply it to an unseen test set, causing a severe discrepancy in distributions across the data splits. The fixed split of the data sets makes the post-calibration very limited and static with respect to the classifier's performance.",
        "GPT2_formal_text": "is like a switch that helps with the second part. We'll go into more detail about how this works in the next section. Formal: We combine the hidden state h_t from the start of the LSTM with the hidden state h_s from the last layer, which we get by using the PDP bidirectional matching function. This gives us a combined hidden state for the whole sequence, which we'll call h_t. Formal: We also use a similar function to calculate h_s, but we just pass in h_t instead of h_s. Formal: Finally, we take the normalized sum of all the attention weights (A_a_t * A_a_s) for each token. Formal: So, by the end of the token sequence, we've got an attention matrix A_t that's the sum of two things: h_s (which is from the last layer) and h_t (which is from the start). Formal: So, the final attention matrix A_t for the entire sequence is... Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.242.json"
    },
    {
        "casual_text": "It's pretty cool to see which tech and online stuff people are into. In the UK, Twitter and Facebook are big deals, but in India, Orkut used to be the go-to. Over in Singapore, blogging platforms like Wordpress are super popular.",
        "formal_text": "It is also interesting to compare what technologies and web services people use. Twitter and Facebook are popular in the UK whereas Orkut is more popular in India. Blogging services like Wordpress are popular in Singapore.",
        "GPT2_formal_text": "= { u1, u2, ..., un }; to get all the pairs of input inputs. In the rest of this section, we'll talk about how we used this approach to figure out the exact values of the RNN parameters. Formal: To make it easier to compare results, we didn't include a significance test on the whole dataset. We did, however, do an extra one on the test set for each parameter to see how well it was doing. Formal: The first step in calculating the loss is to find the best value for the objective function, which is the vector with the highest cross-entropy loss. Formal: For the positive class (which is the positive version of the binary class we talked about in Section 3.2), we calculate the loss for each positive input. Then, we pick the best one by maximizing the cross-entropy loss, which is the negative part. Formal: We measure how well the model is doing using the MLE loss. Formal: Check out Figure 1 for an example of how to calculate the loss for the positive class, using the input x = {x1, ..., xn}. Formal: For the negative class (which is the negative version of the binary class we talked about in Section 3.2), we calculate the loss for each negative input. Then, we pick the best one by maximizing the cross-entropy loss, which is the negative part. Formal: For the positive and negative classes, we figure out the loss for each positive input. Formal: Formal: The loss function we use here is a logistic-gradient optimization, with a training time of O(|•|) for the positive class and O(|•|) for the negative class. Formal: For more info, you can find the original code here: https://github.com/yoojinhwa/SVM Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D09",
        "filename": "D09-1146.json"
    },
    {
        "casual_text": "Looking at Table 5, the NB classification model throws in some random hashtags that don't really fit. On the other hand, LDA, being a generative model, leans towards suggesting broader hashtags like \"Information News\", \"mobile phone\", or \"Technology leaders\". But it misses out on more specific ones like \"WWDC\" or \"MAC OS Lion\". The IBM1 method also suggests some hashtags that have nothing to do with the topic, like \"2012 Jinshan Inc cloud computing\" or \"2012 spring and summer men's week\", just because they both have \"2012\" in them. In contrast, TSTM does a better job by suggesting specific hashtags that actually relate to the tweet's topic.",
        "formal_text": "From Table 5, we observe that classification model NB suggests some unrelated hashtags. While LDA, as generative models, tends to suggest general hashtags, such as \"Information News\", \"mobile phone\" and \"Technology leaders\", and fail to generate the specific hashtags \"WWDC\", \"MAC OS Lion\". IBM1 method will suggest some topic-unrelated hashtags. For instance, \"2012 Jinshan Inc cloud computing\" and \"2012 spring and summer men's week\" are triggered by the word \"2012\". On the contrary, TSTM succeeds to suggest specific hashtags, and most of them are topic-related to the tweet.",
        "GPT2_formal_text": "_tensor. It's super important to have this mask to keep the attention from getting messed up by noise. Formal: In a lot of cases, using this mask can help boost the performance of neural networks, especially when it comes to text classification. Formal: The length of the text token itself doesn't really matter much in regular situations, but it's still helpful if we're trying to decide how long the summary should be. Formal: Let's say the input token is \"the.\" Formal: The attention vector that's being passed through the layer is called \"o_t(t).\" The attention vector for the token itself, \"the,\" is \"o_t(t).\" Formal: The hidden representation of the token \"the\" at the end of the sentence is calculated by adding up the attention vector and the hidden representation of the token at the start of the sentence. Formal: The attention at the end of the sentence gets calculated by combining these three vectors. Formal: We're using a formula that looks like this: Formal: The input token is \"the.\" Formal: The hidden representation of the token \"the\" is \"o_t(t).\" Formal: The attention vector for the token itself, \"the,\" is \"o_t(t).\" Formal: The attention vector for the token itself, \"the,\" is \"o_t(t).\" Formal: Finally, the hidden representation of the token at the end of the sentence is calculated by combining the attention vector and the hidden representation of the token at the start of the sentence. Formal: The input token is \"the.\" Formal: The hidden representation of the token \"the\" is \"o_t(t).\" Formal: The attention vector for the token itself, \"the,\" is \"o_t(t).\" Formal: The attention vector for the token itself, \"the,\" is \"o_t(t).\" Formal: Finally, the hidden representation of the token at the end of the sentence is calculated by combining the attention vector and the hidden representation of the token at the start of the sentence. Formal: We're using a formula that looks like this: Formal: The input token is \"the.\" Formal: The hidden representation of the token \"the\" is \"o_t(t).\" Formal: The attention vector for the token itself, \"the,\" is \"o_t(t).\" Formal: The attention vector for the",
        "directory": "C12",
        "filename": "C12-2027.json"
    },
    {
        "casual_text": "We need to use both the grammar stuff and the meaning details from DICT for this process.",
        "formal_text": "This process must employ both syntactic features and semantic information from DICT.",
        "GPT2_formal_text": "= list(tokens[i]) label_ids = list(tokens[i]) return {h(tokens), e(tokens)} end def update(tokens, label_ids, view_ids): for i in range(1, tokens+1): if i is not equal to label_ids: self.add_view(tokens[i]) elif i is not equal to label_ids: self.add_view(label_ids[i]) elif i is not equal to view_ids: self.add_view(view_ids[i]) elif i is not equal to view_ids: self.add_view(view_ids[i]) end end def update(tokens, label_ids, view_ids): for i in range(1, tokens+1): if i is not equal to label_ids: self.add_view(tokens[i]) elif i is not equal to label_ids: self.add_view(label_ids[i]) elif i is not equal to view_ids: self.add_view(view_ids[i]) end end def update(tokens, label_ids, view_ids): for i in range(1, tokens+1): if i is not equal to label_ids: self.add_view(tokens[i]) elif i is not equal to label_ids: self.add_view(label_ids[i]) elif i is not equal to view_ids: self.add_view(view_ids[i]) end end\n\nThis function updates the tokens in the data using a linear transformation, using a set of labels. Formal: For each token i in the set C, the function updates the data using this transformation. Formal: We also need to add an extra step to handle the return value of the view_ids we add. Formal: Formal: Here, w_r is a vector representing the label r for the view_ids we add. Formal: We also need to add an extra step to handle the return value of the view_ids we add. Formal: The function updates the tokens in the data by adding one token at a time, starting with the token with the most recent view_ids. Formal: We also add an extra step to handle the",
        "directory": "C67",
        "filename": "C67-1032.json"
    },
    {
        "casual_text": "The accuracy difference between the test set and dev set could be because we're only using a part of ConceptNet. This subset was picked based on the vocabulary from the training and development data. But the test data might have words that aren’t in this subset. So, there might be hardly any or even no connections for the test data in the chosen subset. That’s why the accuracy for the test data in the model without Q is pretty similar to the accuracy for the dev data in the model without P A Rel and QA Rel.",
        "formal_text": "The reason for difference in accuracy of test set and dev set might be due to the fact that we are using a subset of ConceptNet. The subset was selected based on the vocabulary of training data and development data. The vocabulary of test data might not be in the selected subset of ConceptNet. There might be few or even no edges for the test data in the selected subset. Thus the accuracy of test data for model w/o Q is pretty close to accuracy of dev data for model w/o P A Rel andQA Rel.",
        "GPT2_formal_text": "ing is a function that helps us deal with the problem of dealing with long input sequences, like text. Formal: In this paper, we introduce a new Transformer-based model for handling multi-hop text that can handle different types of data while being efficient. Formal: We explain the structure of our multi-hop text model, which is based on a graph of attention. We break down how the model learns attention using two types of gradient methods: one using the gradient from the last-layer attention mechanism and another using gradient descent. We demonstrate that our model can achieve top-notch results on a bunch of text-related tasks. We tested our model on three natural language processing (NLP) benchmarks—BERT-base, TAC-2008, and TAC-2009—and it consistently did better than other top-performing systems in the field. Our multi-hop model has two main parts. First, we use graph attention to connect different parts of the text. Second, we create text representations for each hop using a process that sorts the text into tokens and then builds a graph. Formal: We suggest a way to learn text representations for a hop using graph attention and then use graph convolutional networks to get the final representation for the hop. This approach is based on the idea of using a multi-hop graph to learn representations for a single hop. Formal: We suggest a way to learn text representations for a hop using graph attention and then use graph convolutional networks to get the final representation for the hop. We also explain how to get the final representation for a hop by adding up the attention weights for different hops. Formal: We suggest a way to learn text representations for a hop using graph attention and then use graph convolutional networks to get the final representation for the hop. We also explain how to get the final representation for a hop by adding up the attention weights for different hops. Formal: We introduce a new multi-hop text model that can handle multi-hop text effectively. We use graph attention and graph convolutional networks to learn the final representations for the hop. Formal: We introduce a new multi-hop text model that can handle multi-hop text effectively. We use graph attention and graph convolutional networks to learn the final representations for the hop. Formal: We introduce a new multi-hop text model that can handle multi-hop text effectively. We use graph attention and graph convolutional networks to learn the final representations for the hop",
        "directory": "D19",
        "filename": "D19-6008.json"
    },
    {
        "casual_text": "We owe a big thanks to the program committee and the secondary reviewers they brought in for some papers. With so many submissions this time, their workload was way bigger than usual. We’re super grateful for their help, especially since most of them managed to get everything done on time (though we did have to give a few gentle reminders after the deadline). We hope the reviewers’ feedback was helpful and constructive for everyone—whether it was to boost your chances for a future submission or to polish your paper if it got accepted. We know the timeline was tight, so thanks to all the authors for getting your final versions in on time.",
        "formal_text": "We are in debt with the members of the programme committee and with the secondary reviewers they appointed for some of their papers. As the number of papers received was unusually high, they had an unusually large workload: we especially thank them for their invaluable help, which most of them completed on time (yes, we had to twist some arms after the deadline). We hope that the reviewers' comments were useful and constructive and helped all authors: for those whose papers weren't accepted, by increasing their chance in a later submission somewhere else; for those whose papers got in, to improve their manuscripts. We know we didn't give them a lot of time to do so, and we thank authors for sending their camera-ready versions on time.",
        "GPT2_formal_text": "= { (e_i, f_i, e_j, f_j, e_k, e_l), ... }. Formal: For an entity e_i, the thing it's connected to (e_j) is also its parent entity (e_k). Formal: The first part of this setup is to find the closest child relation (e_i, e_j) in the set R, so we can do local inference and use the parent relation as a guide. For instance, we can figure out the nearest child relation (e_i, e_j) using the one from the set R. Formal: Unlike what we did with the parent relation, we use the child relation to guide us. Let's say e_i is the closest child relation to e_j in the set R. Now, we can use the parent relation to help us find the closest child relation. Formal: Using the parent relation as a guide helps us find the nearest child relation (e_i, e_j). Formal: Another thing we do is to find the closest child relation for an entity e_i. In this case, e_i is the parent relation, so we can use the parent relation to guide us. Formal: We use the child relation to guide us. Formal: We use the child relation to guide us. Formal: In this paper, we show that with just a few basic features, we can spot, even with just a tiny number of features, the closest child relation for a given entity e_i. Formal: The relationship type of e_i is the same as the type of e_j. Formal: For the main part, we figure out the best child relation for a given entity e_i. Formal: We use the best child relation for a given entity e_i. Formal: Formal: We use the best child relation for a given entity e_i. Formal: Formal: The parent relation is a bit more flexible because it can have two different types. Formal: Formal: We use the parent relation to guide us. Formal: We use the parent relation to guide us. Formal: Formal: The child relation is a bit more flexible because it can have two different types. Formal: Formal: Formal: We use the child relation to guide us. Formal: We use the child relation to guide us. Formal",
        "directory": "eamt",
        "filename": "2011.eamt-1.0.json"
    },
    {
        "casual_text": "In real-world use, we went with a bigram model because it was quick to implement and performed pretty well in our tests.",
        "formal_text": "2 In practice, we use a bigram model, which was fast enough to scale and worked sufficiently well in experiments.",
        "GPT2_formal_text": ". Then, there's an adversarial classifier that uses a hard negative log-likelihood loss function to adjust how much attention is given to each token. Formal: Alright, let's break this down in simpler terms. \n\nA sentence has at least two parts: an input and a result. The input is the summary of a conversation, and the result is the summary of a document. The reason we pick a topic for both is to make sure our model can make an informed choice. So, the model can't just grab the summary of the conversation or document without considering the other stuff. \n\nIn this project, we're using the idea that the summary should focus on the important stuff and avoid repeating the same info. To do this, we use a \"bag-of-words\" approach to find the best topic for the summaries. This is super important because it means the model doesn't have to spend as much time on selecting the topics. \n\nIn simpler terms, the model grabs the key sentences and words from the source document and picks the best topic for the summary. To make it work, we use a method called minimum error rate training (Och, 2003). This helps us keep the model focused on the important stuff while still getting the job done. Formal: Let's look at the examples in Table 1 again. The input is '1' and the output is '2', which is the result of a conversation with a chatbot. The '1' in the example means the conversation was conducted. Formal: The input is '1' and the output is '2', which is the result of a conversation with a chatbot. Formal: The input is '1' and the output is '2', which is the result of a conversation with a chatbot. Formal: The input is '1' and the output is '2', which is the result of a conversation with a chatbot. Formal: The input is '1' and the output is '2', which is the result of a conversation with a chatbot. Formal: The input is '1' and the output is '2', which is the result of a conversation with a chatbot. Formal: The input is '1' and the output is '2', which is the result of a conversation with a chatbot. Formal: The input is '1' and the output is '2', which is the result of a conversation with a chatbot.",
        "directory": "D19",
        "filename": "D19-1432.json"
    },
    {
        "casual_text": "Let's break this down in simpler terms. First, let's call the previous memory graph G x−1 and the updated one Gx j. The R-GCN layer updates each entity's hidden state by looking at the hidden states of its neighbors, but it does this separately for each type of connection. After that, it adds up all these updates across different types and then runs the result through a GELU activation function (Hendrycks and Gimpel, 2016). To calculate the hidden state of an entity e j in the next layer (l + 1), we use a residual connection (He et al., 2016) to keep the original information of the entity, not just its neighbors, and then we apply layer normalization.",
        "formal_text": "denote the graph connectivity of the previous user memory graph G x−1 and N r, Gx j denote the updated graph. The R-GCN layer updates the hidden states of each entity with the incoming messages in the form of their neighbors' hidden states type-by-type. Then R-GCN sums over all types before passing through the GELU activation (Hendrycks and Gimpel, 2016). The hidden state of entity e j in the (l + 1)-th layer is computed via a residual connection (He et al. , 2016 ) (to keep the original entity information instead of just neighbors' information) and layer normalization.",
        "GPT2_formal_text": ". The sentence is stored using a subword tokenizer, which does tokenization, as explained in section 5.1. The best model is picked based on how it performs on the validation set (check out Fig. 2a). This model is trained to learn both the context and the output. For example, if the training data is X = (s1, t1), and the validation set is Y = (s2, t2), then this model is trained to: Formal: 1. Find the best translation for the current input sentence x that has the highest score from the candidate set y_t. Formal: This is done by minimizing the difference between the score of the best translation y_t and the score of the best translation y_1. Formal: We train this model using the gradient from the cross-entropy loss function, which is shown in Eq. 3. Formal: To minimize the cross-entropy loss, we add an extra loss term called δ_θ. Formal: The whole process of training is shown in Fig. 2a. Formal: We also train the model to find the best translation y_t using the target language model M. Formal: Finally, the best translation y_t is chosen by multiplying all the words in the target sentence with the best translation y_1. Formal: We train the model M using the target language model M. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.463.json"
    },
    {
        "casual_text": "Imitation Learning is all about learning skills by watching demos. It's shown some cool potential in areas like fixing the exposure bias issue in structured prediction (Zhang et al., 2019b), helping non-autoregressive translation models by sharing knowledge (Gu et al., 2018; Wei et al., 2019), and even teaching dialogue systems how to reward themselves (Li et al., 2019b). In our project, we have a regular dialogue model acting as a student, trying to copy a scenario-based dialogue model, not just in the final output but also in the middle layers.",
        "formal_text": "Imitation Learning Imitation learning, acquiring skills from observing demonstrations, has proven to be promising in structured prediction, such as alleviating the exposure bias problem Zhang et al. , 2019b), transferring knowledge to guide non-autoregressive translation model (Gu et al. , 2018; Wei et al. , 2019), and automatically learning the reward of the dialogue system (Li et al. , 2019b). In our work, the conventional dialogue model as a student mimics the scenariobased dialogue model on both the output layer and intermediate layers.",
        "GPT2_formal_text": "_trees (B). There's also a discriminative version of the attention_mask_trees with the same setup (D). Formal: We start by picking a value λ = 0.5 from the softmax pooling layer, which gives us the final attention weights for the mention types. Formal: We use the BERT architecture (Devlin et al., 2019) for the attention layers. For the model layers, we fine-tune BERT on the same data and try out different values like 50, 50/100, 100, and 150. Formal: For the BiLSTM part, we fine-tune BiLSTM on the whole dataset D. The decoder has two layers, where each layer has a hidden dimension of 100 and an output dimension of 200. Formal: We tested the model with 100 hidden units and 100 hidden states to see how each hyperparameter affects the performance. You can find more info in Appendix D. Formal: We used the Code-Switching dataset to train the model. Formal: We ran the model for 10 epochs on the test set and picked the best model based on the validation loss. Formal: To get a summary for each mention, we first calculate the mean and standard deviation for all the mentions in D using the Adam optimizer (Kingma and Ba, 2014). Formal: Then, we calculate the score for the mention and its top-N neighbors using an FST (for token classification). Formal: The score is calculated based on how well the model identifies the top-N neighbors of the mention. Formal: Following the approach from (Zhou et al., 2020), we use the attention mechanism to capture the mention information. Formal: Following the approach from (Zhou et al., 2020), we use the attention mechanism to capture the mention information. Formal: We compare the validation loss of the model with the validation loss of the mention detection model. Formal: We trained the model for 10 epochs on the test set and picked the best model based on the validation loss. Formal: We compared the validation loss of the model with the validation loss of the mention detection model. Formal: We trained the model for 10 epochs on the test set and picked the best model based on the validation loss. Formal: We trained the model for 10 epochs on the test set and picked the best model based on the validation loss. Formal: We trained the model",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.534.json"
    },
    {
        "casual_text": "The N-of-AD. I algorithm, which you can find explained in section 6, is a function that helps figure out the main noun, with a certain property P, that an adjective is talking about. It's written in a special language used for defining lexicons and is also built into the lexicon system as a handy tool. When rules use this function, they can make the translation of an adjective depend on the noun phrase it modifies having specific traits. Oh, and by the way, the N-of-ADJ function can also find the noun when the adjective is used as a pronoun-like modifier or when it's in the VP predicative form.",
        "formal_text": "The algorithm N-of-AD. I, given in detail in section 6 below, characterizes a parameterized function which identifies the head noun argument, with property P, of a given adjective. This algorithm is written in the lexicon specification language and is also included as a utility function in the lexicon system. Rules which employ this function can render the translation of an adjective dependent upon the modified noun phrase having specific attributes. It should also be noted that the function N-of-ADJ identifies the noun when the adjective acts as a pronominal modifier as well as when it appears in the VP predicative form.",
        "GPT2_formal_text": "= [ { \"srl\" : [ { \"id\" : \"srl1\" }, { \"id\" : \"srl2\" }, ], \"created_utc\" : [ { \"id\" : \"createdutc\" }, ], \"summary\" : [ { \"id\" : \"summary\" }, ], ], }, }\n\nFor the utterance-level features, we’re using the [CLS] token. In the QA part, we’re also using the [SEP] token. Formal: We’ve got a bunch of translation pairs from different systems and have them lined up in order. Formal: Basically, the relation vectors have a max length of 1 and are normalized, but they don’t overlap. Formal: Then, for each pair of translation vectors, we find the best translation, using a function f(r, c) that takes in two things: the relation vectors and the context vectors. Formal: Next, we calculate the best translation for the given context and relation vectors, using a function f(r, c) that takes in two things: the relation vectors and the context vectors. Formal: Finally, we update the embeddings and the embedding space to get the final translation pair embedding. Formal: To get the embeddings, we use the methods from Mikolov et al. (2013b). Formal: We then find the best translation pair embedding f(r, c) using a non-linear function g(r, c) that takes in two things: the relation vectors and the context vectors. Formal: To get the embeddings for the current relation vector r_i and the current context vector c_i, we use a non-linear function g(r, c) that takes in two things: the relation vectors and the context vectors. Formal: Finally, the embeddings for the two relation vectors and the context vectors are combined to get the final embedding. Formal: The embeddings and the embedding space are then normalized, so the embeddings match the embedding spaces of the embedding spaces of the translation pairs. Formal: Finally, we update the embeddings and the embedding space to get the final translation embedding. Formal: Formal: This whole process is repeated for the remaining translation pairs. Formal: Finally, we do the same for the rest of the pairs, and we",
        "directory": "C88",
        "filename": "C88-1042.json"
    },
    {
        "casual_text": "We made these tables by running Anymalign for the same amount of time in every setup. That's why bigger length parameter values result in smaller tables—check out the specifics in (Lardilleux et al., 2011b).",
        "formal_text": "These tables were produced by running Anymalign for an identical amount of time in all configurations, which explains why larger values of the length parameter lead to smaller tables-see details in(Lardilleux et al. , 2011b).",
        "GPT2_formal_text": "_tgt = [l_i, t_i] → [T, {l_t}]. Formal: The lookup table has the title and an abstract token. Just a heads-up: if you're using the same entity for both the title and the abstract, you can't use the entity lookup table. Formal: For the entity lookup table, you need to find the title and an entity token. This table is created by combining the title and a set of entities. Each entity token in the title and the entities is represented by its ID. Formal: After each entity token, you have two entity IDs. The first one is the entity ID that the title belongs to, and the second one is the entity ID that the abstract belongs to. Formal: The output from the entity lookup table is a sequence of vectors, T_e, where T_e is the entity ID, and T_e is the title token's ID. Formal: For the entity and abstract lookup table, you can use any sentence-level transformer model, like BERT or GRAPH-CNN. Formal: For the retrieval module, you'll need a pre-trained model (like SciBERT, GPT2, or BERT) that can generate the title and the abstract. Formal: We're using the pre-trained BERT model called BERT-base-uncased for our experiments. Formal: For the sentence-level transformer model, we're using the pre-trained SciBERT model called SciBERT-base-uncased. Formal: We're using the SciBERT model and the pre-trained BERT model to generate the title and the abstract. Formal: The entity lookup table and the retrieval module are both trained with the SciBERT model. Formal: For the entity lookup table, we'll start with the title and the entities, and then we'll use the retrieval module. Formal: For the entity and abstract lookup tables, we're using the pre-trained BERT model called BERT-base-uncased. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2012.eamt-1.62.json"
    },
    {
        "casual_text": "Here, π represents the chance that the community will switch emotions between two time periods, t and t + 1. The letter b stands for the total number of times the emotion state changes across all the time intervals from 1 to T.",
        "formal_text": "where π is the probability the community will change the emotion state in two consecutive time intervals t and t + 1. b denotes the number of emotion state changes in the whole time intervals [1, T ].",
        "GPT2_formal_text": "_t) = (t, *) end for i from 1 to T do send(t, (t, T_i) ) end end return {(t, *) | s_i = s_i - 1, *} end\n\nThe last part is about figuring out how to mix these features together to create a summary. Formal: In Section 5.2, they talk about the general idea behind summarization. They use some basic models, like a bi-level attention network, to map the input features into a hidden state and then send that to the model's output. Formal: In Figure 4, you can see the final output for the query \"Tom Hanks\". Formal: For the output layer (which is the one that gets the summary), they use a tanh linear layer with a hidden dimension of 2.5. Formal: The final output layer has a hidden dimension of 2.5, which is more focused on the core of the query and a learning rate of 0.1. Formal: Lastly, in Figure 5, you can see the final summary for the query \"I'm Very Tired\". Formal: Here, we're using a bi-level attention network that's learned from the query. Formal: Finally, we calculate the attention weights for each token and combine them to get the final representation. Formal: Here's the equation for the bi-level attention network: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D14",
        "filename": "D14-1123.json"
    },
    {
        "casual_text": "SST-2, also known as the Stanford Sentiment Treebank, has around 70,000 sentences tagged as either positive or negative (thanks to Socher et al. in 2013). We took the pre-trained RoBERTa base model and tweaked it for this sentiment classification task. As a result, we got an accuracy of 94.5 on the development set.",
        "formal_text": "SST-2 The Stanford Sentiment Treebank (Socher et al. , 2013) contains 70k sentences labeled with positive or negative sentiment. We fine-tune the pre-trained RoBERTa base to the classification task and achieve an accuracy of 94. 5 on the dev set.",
        "GPT2_formal_text": "ed(d) is just a sub-layer of the multi-head attention layer. \n\n2. Next, we turn each sentence into a vector using the attention mechanism. We do this by calculating the attention weights for each word in the sentence. The sentence attention weights are calculated using the formula H_a = E_a(i) × W_a(i) = [W_a(i) * a_i(i-1)] / [a_i(i-1) * W_a(i)]. Formal: Formal: The attention weights are calculated using the formula H_a = E_a(i) × W_a(i) = [W_a(i) * a_i(i-1)] / [a_i(i-1) * W_a(i)]. Formal: Formal: We use two different attention mechanisms: one with a specific attention mechanism and another that helps us with entity detection. Formal: After using these attention weights, the final representation for the sentence is a matrix. Formal: We train the model using the task loss function L_t. Formal: Formal: We calculate the loss using the objective function η_t. Formal: Formal: We use the sentence classification task L_t and the entity detection task L_t for training. Formal: Formal: In this paper, we talk about a model that uses multiple attention mechanisms and learns to encode sentences using the attention mechanism. Formal: Formal: We tested this model on the CoNLL-2009 dataset and the results show that it works really well. Formal: Formal: There are some other methods for generating text by combining different techniques, like the ones by Yin and his team, which were published in 2014. Formal: Formal: We also tried out a bunch of methods for generating text from the multi-head attention mechanism. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-srw.16.json"
    },
    {
        "casual_text": "Lemmatization is all about figuring out the base form (lemma) of a word from its inflected form. For example, turning \"atmest\" into \"atmen.\" This process is pretty straightforward for languages with simple grammar, like English, but it gets way more complicated with languages like Finnish. Essentially, it's like doing the opposite of what happens when you add endings to words to change their form. This idea has been talked about in a few studies (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where they look at how you can generate an inflected word by starting with the base form and adding the right ending.",
        "formal_text": "Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor languages like English, but much harder for languages like Finnish. The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al. , 2014; Nicolai et al. , 2015; Faruqui et al. , 2016), where an inflected form is generated from a lemma plus an inflection tag.",
        "GPT2_formal_text": "= [ ] for i from 1 to n + 1, where i goes from 1 to the number of documents we have. For training, we set the word embedding dimension to 300 and the hidden state dimension to 300. The model is trained for 100 epochs with a learning rate of 1e-4. Formal: Using this dictionary, we create a rough version of the input document. Then, we use a statistical classifier to figure out the probability P(q | d). This probability is based on how likely the document is to have a specific question q and a specific answer d. Formal: Following the approach by Wang et al. (2019), we combine the input and output vectors using linear transformation to create a combined representation. We calculate the probability P(q | d) using the output from the classifier. Formal: We figure out the probability P(d | q) by using a statistical classifier. Formal: We use equation (2) to estimate the probability P(d | q). Formal: The accuracy is the percentage of questions where we can correctly identify the correct answer. Formal: We calculate the accuracy using the labels from the model. Formal: We calculate the accuracy using the labels from the model. Formal: We use equation (3) to estimate the probability P(d | q). Formal: We calculate the accuracy using the labels. Formal: We use equation (4) to estimate the probability P(d | q). Formal: We use equation (5) to estimate the probability P(d | q). Formal: We use equation (6) to estimate the probability P(d | q). Formal: We use equation (7) to estimate the probability P(d | q). Formal: We use equation (8) to estimate the probability P(d | q). Formal: We use equation (9) to estimate the probability P(d | q). Formal: We use equation (10) to estimate the probability P(d | q). Formal: We use equation (11) to estimate the probability P(d | q). Formal: We use equation (12) to estimate the probability P(d | q). Formal: We use equation (13) to estimate the probability P(d | q). Formal: We use equation (14) to estimate the probability P(d | q). Formal: We use equation (15) to estimate the probability P(d |",
        "directory": "C16",
        "filename": "C16-1160.json"
    },
    {
        "casual_text": "Collapsed sampling tends to take a long time to get to the right answer. This is a big issue for us, especially since we start with a segmentation provided by a morphological analyzer. The analyzer works by splitting text based on fixed rules, so the initial segmentation it gives is pretty consistent. However, this means that errors, especially with unknown words, happen in a predictable way. Basically, we start off close to the best possible solution, but not quite there. The problem is that the collapsed Gibbs sampler can get stuck in this not-quite-right starting point. That's why we often just randomly pick the initial segmentation instead. Sentence-based block sampling can also get stuck if it starts with a consistent segmentation (Liang et al., 2010).",
        "formal_text": "Collapsed sampling is known for slow convergence. This property is especially problematic in our settings where the initial segmentation is given by a morphological analyzer. Since the analyzer deterministically segments text using pre-defined parameters, the resultant segmentation is fairly consistent. Segmentation errors involving unknown words also occur in a regular way. Intuitively, we start with a local optimum although it is not too distant from the global optimum. The collapsed Gibbs sampler is easily entrapped by this local optimum. For this reason, the initial segmentation is usually chosen at random. Sentence-based block sampling is also susceptible to consistent initialization (Liang et al. , 2010).",
        "GPT2_formal_text": "ed. This is a fancy way of saying it uses the attention mask. The attention mask is set to one, which helps separate the input from the output, making it easier to figure out and understand. We're using a bunch of pre-trained word embedding models, like GloVe (Pennington et al., 2014), BERT (Devlin et al., 2019), and GloVe-Large (Pennington et al., 2019), along with some transformer layers, just like in the setup in Figure 1. Formal: We're using a beam size of 16, and we train for 5 epochs. Formal: The cross-entropy loss function for the cross-modal dialogue control task, as shown in Equation 6, is calculated like this: Formal: To make things easier, we’ve come up with a new optimization goal called cross-modal dialogue control loss. It’s based on a bigram cross-entropy loss, which is super helpful for handling complex topics. You can check out all the details in Appendix A. Formal: We’ve added a few extra features to the cross-modal attention model. Formal: We’ve tweaked the structure of the LSTM-based attention model so it’s better at capturing the context of the current modality in the conversation. Formal: We’ve tested our approach on three popular datasets to see how it performs. Formal: Our setup is different from the usual attention-based dialogue control models. The main goal here is to pay attention to all the different modalities in the conversation, so the model can really learn how to pick the right one for the user. Formal: We’ve made it possible for users to adjust the size of the special tokens for their modalities without needing to specify them. Formal: We’ve also used a fancy self-attention model that’s been trained on a lot of dialogue data, and it’s been super helpful in understanding how the conversation flows. Formal: We’ve included some extra details about the specific dataset we used, like the training, development, and testing splits. Formal: Our model takes the combined dialogue history and the multimodal history into account. Formal: To train our model, we use an ensemble of models using self-attention. Formal: We’ve come up with a new optimization goal for cross-modal dialogue",
        "directory": "D11",
        "filename": "D11-1056.json"
    },
    {
        "casual_text": "2. Updating an entity-event database: Besides D (test), let’s say we also have access to a historical database of killings E (train) and a collection of news articles D (train) from before the time T. This kind of setup is pretty common in real-world scenarios and is what this paper is mainly about. It lets us use distantly supervised learning methods, which is pretty cool.  \n\nThe task itself is really important for society, but the NLP research community might also want a more scientific reason to care. We think police fatalities are a great test case for event extraction research. Fatalities are a specific type of event with clear meanings, so they avoid some of the trickier issues in this field (Hovy et al., 2013). Plus, this task ties into a lot of existing research on building knowledge bases (like Craven et al. (1998)).  \n\nLastly, we believe that NLP should, whenever possible, work on applications that matter to the public. Previous research showed that news text is valuable for this problem, but using computational methods could make it way less labor-intensive than doing everything manually.",
        "formal_text": "2. Updating an entity-event database: In addition to D (test), assume access to both a historical database of killings E (train) and a historical news corpus D (train) for events that occurred before T. This setting often occurs in practice, and is the focus of this paper; it allows for the use of distantly supervised learn-ing methods. 2 The task itself has important social value, but the NLP research community may be interested in a scientific justification as well. We propose that police fatalities are a useful test case for event extraction research. Fatalities are a well defined type of event with clear semantics for coreference, avoiding some of the more complex issues in this area (Hovy et al. , 2013). The task also builds on a considerable information extraction literature on knowledge base population (e. g. Craven et al. (1998) ). Finally, we posit that the field of natural language processing should, when possible, advance applications of important public interest. Previous work established the value of textual news for this problem, but computational methods could alleviate the scale of manual labor needed to use it.",
        "GPT2_formal_text": "\" function in the classifier to adjust the reward for each class, aiming for a balanced score between the i-th and j-th classes. Formal: We use the cross-entropy loss to train the classifier, which is shown in equation 4. Formal: In this paper, we use a single RNN for classification, but for the experiments in section 5, we swap it out for two separate RNNs for each of the three labels. Formal: The classification task is broken down into three parts, and each part is represented as a sequence of hidden vectors. Formal: In each part, there are n columns with M possible labels, each labeled with one of the N labels. Each label comes from a mix of the labels for the first n columns and the predictions of the classifier for the last n columns. Formal: For each part, the goal is to find a vector that maximizes the cross-entropy loss, which is calculated using the i-th and j-th classes. Formal: The cross-entropy loss for a single RNN is calculated using the cross-entropy loss for two separate RNNs, and then it gets adjusted by the classifier, as shown in equation 4. Formal: For example, let's say we have two RNNs: Formal: In the original paper, they used a beam size of 3 for the task, but for more realistic tasks, they usually go with a beam size of 2 for better performance. Formal: Using a beam size of 1 for the main task, and a constant number of output units (like 1 for the main task) for the auxiliary tasks, the beam size for the main task becomes 2 in the auxiliary tasks. Formal: For the original paper, they tested different beam sizes like 1, 2, 3, and 4. Formal: The weight matrix for the main task in the auxiliary tasks is calculated as Formal: In this paper, we use a standard two-layer feed-forward neural network, and the weight matrix for the main task in the auxiliary tasks is: Formal: In the original paper, they used a beam size of 3 for the task, but for more realistic tasks, they usually go with a beam size of 2 for better performance. Formal: For the original paper, they tested different beam sizes like 1, 2, 3, and 4. Formal: The weight matrix for the main task in the auxiliary tasks is calculated as Formal",
        "directory": "D17",
        "filename": "D17-1163.json"
    },
    {
        "casual_text": "There are two situations where COs (Content Objects) might change, but only one of them is actually a big deal. The other one is pretty straightforward and not really an issue. The easy case happens when a resource gets updated, like when it has more annotations or fixes some errors, but the old version stays available. In this situation, the long-term archive just creates a new version of the CO, and the old one stays as it is, still accessible.\n\nNow, there’s a bit of a conflict here. On one hand, users might find it useful to know there’s a new version of a CO. On the other hand, updating the metadata to reflect this new version would mess with the whole idea of long-term archival, which is supposed to keep things stable. We think the second point is way more important: there’s no need to link to the new version in the long-term archive or change the metadata. \n\nThat said, it’s totally fine, from a long-term archival standpoint, to link from the new version to the old ones—as long as the old versions are archived first, which usually happens. To make things easier for users, the archive’s presentation layer can flip these links around without actually changing the metadata itself.",
        "formal_text": "There are two cases in which COs change, only one of which constitutes a veritable challenge. The other one can be seen as unproblematic. The unproblematic case arises when a resource is published in a new version, e. g. containing more annotations, but also correcting mistakes that will stay accessible in the previous version. The long-term archive will in this case simply issue a new version of the CO and the old version stays intact and accessible. There is a certain conflict of interests here: On the one hand, it may be interesting to users to see that there is a new version of an CO; on the other hand, integrating this information into the archive would most evidently be possible changing the metadata, a measure which evidently goes against the general guarantees of long-term archival. We suggest that in this case the latter point far outweighs the former: It is not necessary to point to the new version in the long-term archive and make changes to metadata. However, it is by perfectly admissible from an LTA perspective to point to old versions from the new ones -as long as the latter are archived after the former have been, as is normally the case. To improve usability, the presentation layer of the archive can invert these links without integrating them into the metadata proper.",
        "GPT2_formal_text": "_t [t h] in f, which is a vector of entity embedding dimensions. Formal: The entity embedding matrix for h i, t i, is created by averaging the entity embedding vectors from the attention heads of all the entities in f. Formal: For the Bi-LSTM-CRF model, we use the combination of the Bi-LSTM layer with the CRF layer. For the Bi-LSTM model, the attention weights are learned by combining the individual attention heads of the entity pairs. Formal: This means that for each entity pair in the dataset, we look at how the entity embedding dimensions are distributed. The size of the embedding dimension (d) tells us how many entity pairs we're dealing with. Formal: We also add a 1 to the embedding dimension of each entity pair, as well as the embedding dimension of the entity pair itself. Formal: As for the Bi-LSTM-CRF model, it just keeps the same entity embedding dimension (d) and calculates the attention weights for the entity pair. Formal: Using this approach, the model learns to calculate the attention weights for the entity pairs in the dataset. Formal: Formal: The final entity embedding vector (d) is calculated using the equation: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "cmlc",
        "filename": "2020.cmlc-1.1.json"
    },
    {
        "casual_text": "Okay, so here's the deal with how we define an \"effective question.\" It all revolves around this thing called the reference set, which we call RS. For every question q t, we figure out what RS is. \n\nAt the very beginning, before any conversation starts, RS(q 0 ) includes every single object in the image. Basically, it’s a list of all the objects that are in the dataset and were given to the Oracle model. But here’s the thing: the human Oracles didn’t get this list. \n\nNow, as the conversation goes on, at each turn t, RS(q t ) is made up of the objects that were in RS(q t−1 ) and have the same answer A for question q t as the one for the referent r. All these answers A are calculated using the Oracle we have, which is 79% accurate on the test set. So, in short:",
        "formal_text": "Our definition of effective question is based on the set of candidate objects: the reference set RS. We compute RS for each question q t. The reference set before the dialogue starts, RS(q 0 ), contains all the objects in the image. That is, it contains the list of objects annotated in the dataset and given to the Oracle model. Human Oracles did not have access to this list. At each dialogue turn t, RS(q t ) is the set of objects in RS(q t−1 ) such that the answer A to q t on those objects is the same than the answer to q t on the referent r. All answers A are computed using the Oracle proposed in whose accuracy on the test set is 79%. Formally:",
        "GPT2_formal_text": "= [t1, t2, ..., tk], and the answer_ids = [a1, a2, ..., an]. Formal: We look at the tokens in the response and the question to figure out the attributes for the response. Formal: As mentioned in Section 2, the output encoding p, y_i is usually a fixed-length vector. But since the response might include extra words or sentence-level details that don’t fit into the usual fixed-length vectors, we can use a data-weighted encoding to make the response more readable. Formal: We use a representation layer called r_s to represent the response. We also use a response layer called r_r, which is also a fixed-length vector, and we feed these into a logistic classifier. This setup gives us a probability distribution p_s(r_s, a_r) for each response. In this experiment, we’re only using the response layer with a fixed-length vector, so p_s(r_s, a_r) = 0. The objective function we use is simply the log probability of the response. Formal: With the output embeddings from the response layer, the classifier spits out a predicted class label y_t. Formal: The classifier spits out a predicted class label y_t for the response. Formal: The classifier spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal: The response layer spits out a predicted class label y_t for the response. Formal",
        "directory": "alvr",
        "filename": "2020.alvr-1.4.json"
    },
    {
        "casual_text": "Like other pre-training models (like UNILM, BERT, and XLM), we also use the multi-layer Transformer (Vaswani et al., 2017) as the main tool to encode sentences and get contextual representations for each word in a sentence. The multi-layer Transformer works by:",
        "formal_text": "Sentence Encoder As previous pre-training models (UNILM, BERT, and XLM), we also apply the multi-layer Transformer (Vaswani et al. , 2017) as the basic sentence encoder for obtaining the contextual representations h i for each token in the sentence s. The output of multi-layer Transformer is computed via:",
        "GPT2_formal_text": "_tiling(u) = sum from k=1 to K of h_t(u) * tanh(u) * (1 − f_t(u)) / (1 + f_t(u)). Formal: But in real life, the process of figuring out the best attention weight matrix W and its normalization matrix U tends to favor words with high probabilities. Formal: To deal with this, we came up with a method called a dynamic attention (DAD) to deal with the non-linear relationship between the attention weights, q_t and u_t, during the decoding process. Specifically, DAD tweaks the attention weights based on the embedding h_t. Formal: Here, c_t is the input embedding for the word embedding y_t. The latent vector h_t is calculated by combining the hidden state h_t1 + h_t2 from the last layer with the global activation function g_t. Formal: We use a linear transformation layer that is shaped like a kernel to add noise to the embedding. Formal: After the DAD process, we calculate the attention weights q_t using the input embedding y_t. Formal: For the next step, we use beam search to find the best attention weights and normalization vector u_t. Formal: Finally, we use the softmax function to update the embedding h_t and calculate the attention weights q_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eacl",
        "filename": "2021.eacl-main.251.json"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. W represents the n-gram, which is just a fancy way of saying a sequence of words. The function f(.) counts how many times something happens, and f_match(.) looks at the biggest number of n-grams that show up in both the summary you made (let's call it C) and the real, expert-made summaries (we'll call those S). \n\nNow, if your summary C has n words and the gold summary S has u sentences, ROUGE-L is a way to measure how well your summary matches up with the real one.",
        "formal_text": "Where W is the n-gram, f (. ) is the count function, f match (. ) is the maximum number of n-grams cooccurring in the generated summary and in a set of gold summaries. For a candidate summary C with n words and a gold summary S with u sentences, ROUGE-L is defined as follows:",
        "GPT2_formal_text": "= [], weight = e, entity_ids = [], parent_ids = [], position = e, parent_type = [], parent_type_mask = [], parent_type_mask_src = [], parent_type_mask_dest = [], parent_type_mask_src_in = [], parent_type_mask_dest_in = [], parent_type_mask_src_out = [], parent_type_mask_src_out_in = [], parent_type_mask_out_out = [], parent_type_mask_src_out_out_in = [], parent_type_mask_src_out_out_out_out = [], parent_type_mask_out_out_out = [], parent_type_mask_src_out_out_out = [], parent_type_mask_src_out = [], parent_type_mask = [], parent_type = [], parent = [], parent_type_type = [], parent_type_type_mask = [], parent_type_type_mask_src = [], parent_type_type_mask_src_in = [], parent_type_type_mask_src_in = [], parent_type_type_mask_src_out = [], parent_type_type_mask_src_out_in = [], parent_type_type_mask_src_out_out = [], parent = [], parent_type = [], parent_type_type_mask_src_out_out = [], parent = [], parent_type_type_mask_src = [], parent = [], parent_type_type_type_mask_src_in = [], parent = [], parent_type = [], parent_type_type_type_mask_src_out = [], parent = [], parent = [], parent_type_type = [], parent = [], parent = [], parent_type_type_type_mask = [], parent = [], parent_type = [], parent = [], parent_type_type_type_mask_src = [], parent = [], parent = [], parent = [], parent = [], parent_type = [], parent = [], parent = [], parent_type = [], parent = [], parent = [",
        "directory": "D15",
        "filename": "D15-1045.json"
    },
    {
        "casual_text": "Paraphrasing has been around for a while and has a bunch of uses in different areas of natural language processing, like summarizing text (Cao et al., 2016), understanding sentences (Berant and Liang, 2014), and answering questions (Yu et al., 2018). Back in the day, people mostly used rule-based methods or statistical machine translation systems to create paraphrases (McKeown, 1980; Meteer and Shaked, 1988; Bannard and Callison-Burch, 2005).",
        "formal_text": "Paraphrase generation has been a long-standing task that has several applications on downstream NLP tasks including text summarization (Cao et al. , 2016), semantic parsing (Berant and Liang, 2014), and question answering (Yu et al. , 2018). Early works on paraphrase generation mostly rely on rule-based or statistical machine translation systems (McKeown, 1980; Meteer and Shaked, 1988; Bannard and Callison-Burch, 2005).",
        "GPT2_formal_text": "ed_final_image, attention_masked_in_order, and attention_masked_in_reverse_image) to grab attention for a particular instance from the set of input vectors in the order they appear in the input sequence. After that, we can use a fully-connected layer to figure out the final representation for the token in the target sequence. Formal: We use the pre-trained BERT model for token classification to predict the sequence of tokens (and their corresponding attention weights) in the target sequence. Formal: The token label attention, p_t, is calculated like this: Formal: For each token t_i, we look at the token's attention vector p_t, which is a d_s embedding. The attention vector for t_i is what we use to calculate the label attention for that token. Formal: Once we have the token label attention vector p_t, we add it to the original input vector h_t, which we learned during training. Formal: Finally, we combine the attention from both the token label attention vectors and the original input vector h_t to get the final attention representation for the token. Formal: We test our model against some of the best models out there, and the results are in Table 4. From the results, we can see that BERT does a good job with model comparison because it uses the main parts of the original input and the output to learn a good representation for the target sequence. This helps it focus on the important parts and give a good representation for the target sequence. Formal: To check how well our model can handle different target sequence lengths, we measure its performance using the average attention values (pre-trained BERT's average attention). Formal: We also tried using a version of our model that didn't use the BERT pre-training model. This version is trained only on the target sequence using a supervised approach. Formal: We tested this approach on the CNN/DailyMail dataset (Sun et al., 2019) using the same validation set as the original model. Formal: We call our model a multi-task sequence labeling (MTL) model. Formal: For every task, we trained our model for 30 epochs using the Adam optimizer with a learning rate of 1e-5 for the batch size, and then fine-tuned it for another 30 epochs using the same learning rate. Formal: Our approach is based on the Transformer model",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.417.json"
    },
    {
        "casual_text": "But when we throw the LM score into the mix, things get a bit more complicated. The total cost of a derivation isn't just the sum of the costs from the previous hypernodes and the grammar rule anymore. Now, we need to tack on the cost from the language model, which is calculated based on the parts of the target language we're dealing with. This LM score is called a \"combination cost\" because it impacts how we combine hypernodes. \n\nThe problem is, this score is kind of a pain to calculate—it depends on everything involved in the combination (the predecessor hypernodes and the translation rule). And because of that, the sorting strategy we talked about earlier can't guarantee that we'll always generate derivations in a nice, neat, monotonic order.",
        "formal_text": "When including the LM score, however, the situation is different. The costs of the derivation are no longer simply the sum of the corresponding derivations in the predecessor hypernodes plus the cost of the grammar rule. Now we have to add the cost of the language model computed on the associated target language parts. The language model score is called a combination cost, as it is a cost that affects the combination of hypernodes. This score is costly to compute and dependent on all elements participating in the combination (predecessor hypernodes and translation rule). The effect of this is that the sorting strategy referred to above cannot longer guarantee that the generation of the derivations in an hypernode will proceed in a monotonic order.",
        "GPT2_formal_text": "= (D_i)  \n\nFor the graph token representation, we go with the embedding from the top-k edge in the graph. Formal: For the embedding, we use the top-k neighbors of a particular token in the sentence. Formal: To create the embedding for the i-th word, we first turn the word into its base form (basically, turning it into the sentence's basic structure). Then, we match up the vectors from the two embedding matrices to create a representation of the word as a whole. Formal: To predict the token's vector representation, we go with the embedding of the token's top k neighbors. Formal: We use a logistic regression model to figure out the probability that a token belongs to the i-th class. Formal: The semantic similarity score is calculated based on the token's embedding vector. Formal: We use the BERT-score for the token's embedding. Formal: We combine the feature vectors of the tokens to create the contextual features of the tokens. Formal: We train our model using a batch size of 32,000 tokens. Formal: We also use two types of semantic similarity scores: one that looks at the sentence's syntactic structure and another that looks at the source and target text. Formal: Since all the token embeddings are normalized to a common space, the scores from these similarity scores are normalized too. Formal: Finally, we calculate the semantic similarity scores for the tokens using the same formula as in Section 2. Formal: Here's the formula for the semantic similarity score: Formal: Here's the formula for the BERT-score: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2009.eamt-1.33.json"
    },
    {
        "casual_text": "Machine reading comprehension (MRC) is all about finding the right answers in a passage based on a specific question (Devlin et al., 2019; Wen et al., 2021). In recent years, turning NLP tasks into MRC tasks has become pretty popular. For example, things like dependency parsing (Gan et al., 2021), relation extraction (Levy et al., 2017), named entity recognition (Li et al., 2020), and sentiment analysis (Chen et al., 2021; Mao et al., 2021) have all been approached this way. Unlike those earlier studies, we’re using an MRC framework to dig into the complex argumentative connections between two super-long documents.",
        "formal_text": "Machine reading comprehension (MRC) aims to extract answer spans from a passage according to a given query Devlin et al. , 2019; Wen et al. , 2021). Formulating NLP tasks as MRC tasks has been a rising trend in recent years, such as dependency parsing (Gan et al. , 2021), relation extraction (Levy et al. , 2017), named entity recognition (Li et al. , 2020), sentiment analysis (Chen et al. , 2021; Mao et al. , 2021). Unlike previous studies above, we employ a MRC framework to analyze the complex argumentative relations between two documents with excessively long length.",
        "GPT2_formal_text": "_x,0. This helps the model figure out the hidden state for the token x. Formal: To figure out the hidden state for the input x, we use a graph convolutional neural network (GCN) called GRU(x, x). Formal: We also add a residual connection to the attention representation h_i. To keep things simple, we'll call the attention representation h_i x instead of h_i. Formal: In our experiments, we set h_i to the size of the smallest possible hidden state for the token x. Formal: In this setup, the model picks the token with the highest probability. Formal: Another option is to use the log-likelihood for this: Formal: We're using the LogRegNN library to train our model. Formal: We train the model using a text corpus as a development set and then use it to predict the next word in the source text. Formal: We train the model using a corpus and then use it to predict the next word in the source text. Formal: We train the model with the corpus, use it to predict the next word, and then use that prediction to predict the target word. Formal: We train the model using the corpus and then use it to predict the next word in the source text. Formal: We train the model using the corpus, use it to predict the next word in the source text, and then use that prediction to predict the target word. Formal: We train the model using the corpus, use it to predict the next word in the source text, and then use that prediction to predict the target word. Formal: We train the model using the corpus, use it to predict the next word in the source text, and then use that prediction to predict the target word. Formal: We train the model using the corpus, use it to predict the next word in the source text, and then use that prediction to predict the target word. Formal: We train the model using the corpus, use it to predict the next word in the source text, and then use that prediction to predict the target word. Formal: We train the model using the corpus, use it to predict the next word in the source text, and then use that prediction to predict the target word. Formal: We train the model using the corpus, use it to predict the next word in the source text, and then use that prediction to predict the target word.",
        "directory": "acl",
        "filename": "2022.acl-short.4.json"
    },
    {
        "casual_text": "But this feature doesn't really help us pick between a doctor and a surgeon, because surgeons still study medicine too. If we run the experiment again, but this time focus on surgeons, the top result switches to \"patients died.\" This bigram seems to be a better way to tell the difference between those two groups in the BIOS model we trained.",
        "formal_text": "However, this is not a particularly useful feature to favor physician rather than surgeon, since surgeons also entail medicine studies; when we repeat the experiment in contrast to surgeon, the top highlight changes to \"patients died\", indicating that this bigram is a better differentiator for those classes in the trained BIOS model.",
        "GPT2_formal_text": ", k; Encoder Attr K = (A × R) × ct, where ct is a non-linear function that maps the embedding vectors of the input example to the hidden state vector. Formal: When it comes to the embedding vectors of the response, the word embedding vector comes from the hidden state vector, while the response vector is calculated using the non-linear function. Formal: But for the loss functions, the embedding vector is picked based on how often the input example shows up in the test set. Formal: Using the non-linear function, the embedding vectors can be adjusted using a smoothing technique called Emotion-Loss Sampling (EMSS) (Saha et al., 2018). Formal: To make the embedding vectors better, ESS boosts the probability of the best hidden state vector by looking at how often the input example appears in the test set. Formal: To adjust the loss functions, we use some stochastic adjustments based on the training data (check out Figure 1 for more details). Formal: For example, the loss function would be adjusted like this: Formal: Formal: For instance, the loss function would be adjusted like this: Formal: Formal: Formal: For instance, the loss function would be adjusted like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.120.json"
    },
    {
        "casual_text": "Pronouns are kind of a big deal in sentences because they help writers spice up their writing with a bigger vocabulary and make more complex sentences. Basically, pronouns are like stand-ins in the text—they can take the place of a subject or an object, show ownership, point to places, or even refer back to people or things mentioned earlier. If you just ignore pronouns and don’t replace them with something useful, you might miss out on important stuff, like grammar or meaning. When we read, our brain automatically figures out what the pronoun is pointing to, so we understand the full picture. In NLP (Natural Language Processing), they do something similar with anaphora resolution. So, our plan is to use this info that would normally get lost and put it to work for AKE (Automated Knowledge Extraction).",
        "formal_text": "Moreover, pronouns have a relevant role in the sentences since they allow the author to enrich his writing using a richer vocabulary, composing more complex sentences, and so on. Pronouns are parts of the text which typically have the function of a substitute: depending on the case, they can replace a subject or an object, they can indicate possession, places, or refer back to people or things previously mentioned. Given these premises, disregarding all pronouns without replacing them with a valuable substitute could lead to a loss of a syntactical and/or semantical information. In fact, during the reading process we are able to decode the information conveyed by pronouns because we automatically replace them with the entity they refer to. In NLP a similar process is performed by anaphora resolution, thus our idea is to use this information, which would be otherwise lost, for AKE.",
        "GPT2_formal_text": "(x, y) is a matrix that shows the attention value for each token x. After that, we use a softmax function to figure out the probability p(x, y). Finally, we use the cross-entropy loss to normalize the probabilities. Formal: We created a new dataset called COSQA with a ton of examples, and the top model is the one from Wang et al. (2019). Formal: We picked this dataset because it has way more examples to train on than the usual benchmark. Formal: To test how well our model works, we used it to find the best answers in questions. Formal: To figure out the best answer, we look at the answer and the question together. Formal: When we're trying to find the best answer, we first calculate the hidden state H. This hidden state H is a combination of the hidden states from the question and the answer. Formal: We then use a softmax function to get the probability P(h(y) | x) for the best answer y. Formal: Finally, we use the cross-entropy loss to normalize the probability P(h(y) | x). Formal: The results from our COSQA model are in Table 1. Formal: We also tried out a few other datasets, like 'XiaoBi', 'QQP', and 'SQuAD'. Formal: We only included the results for question-answer pairs where both questions and answer were in the dataset. Formal: For our experiments, we used a batch size of 16 and set the learning rate to 0.5. Formal: We picked the dataset COSQA for our experiments because it has way more examples. Formal: Table 1 shows the results of the COSQA model when we trained it on COSQA and then tested it on COSQA. Formal: The results from the COSQA model are in Table 2. Formal: We trained the COSQA model for 5 epochs and tested it on the training set to get the best answer. Formal: Formal: We picked this dataset for our experiments because it has way more examples to train on. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1077.json"
    },
    {
        "casual_text": "今年9月，好几家外国石油公司和哈萨克斯坦的国家石油公司签了一大堆“世纪合同”。这些合同预计在未来40年内能带来7000亿美元的巨额利润。",
        "formal_text": "(8) 今年 This year ９月 September ，, 多 many 家 CL 外国 foreign 石油 oil 公司 company 与 with 哈 Kazakstan 国家 national 石油 oil 公司 company 签署 sign 了 LE 一揽子 a series of \"世纪 \"century 合同\" contract\" ，, 这些 these 合同 contract 将 will 在 in 今后 future ４０ 40 年 years 内 within 产生 generate ７０００亿 700-billion 美元 dollar 的 DE 巨额 enormous 利润 profit 。 \"",
        "GPT2_formal_text": "= { 1, 2, 3, 4, 5 }  \n2. For each query, we check the most common matching tokens in the cluster. Basically, for each query, we look at all the matching tokens in the cluster and rank them based on that. Formal: The NLP model we get, called tp_n(T), is based on the embedding layer from the BiLSTM model. Formal: This setup gives us a representation tp_n(T) of T. Formal: Here, T_j, T_k, and tp_n(T) are the top 5, 10, and 20 embeddings for each query, respectively. Formal: This representation is then fed into the PLM. Formal: In the PLM, the output y_t is just the embedding for the query. Formal: Here, T_j, T_k, and tp_n(T) represent the top 5, 10, and 20 embeddings for each query, respectively. Formal: Finally, the final output is the embedding for the t-th token. Formal: The final output is the embedding for the t-th token. Formal: Formal: Finally, the final output is the embedding for the t-th token. Formal: Finally, the final output is the embedding for the t-th token. Formal: Formal: Formal: Finally, the final output is the embedding for the t-th token. Formal: Formal: Finally, the final output is the embedding for the t-th token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C10",
        "filename": "C10-2156.json"
    },
    {
        "casual_text": "The Tanglish tweets mix both Tamil and English words. We changed the Tamil words into English using a Tamil to English mapping corpus. We used the NLTK library in Python to clean up the data. For any classification task, pre-processing is super important because it helps the classifier work better. We did some cleaning steps like removing stop words, lemmatization, and getting rid of special characters. For example, after cleaning, the tweet \"@Bala sundar ayyo sorry. . . antha line ah clarify pannama vittutu irukan[: drowsy]ok na solran( en appavum indha grant work ku vanthurukkaru, neenga en appava paakala pola. . . . en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaru[: yeah][: yeah] ' chiiiii karumam podinnngggg. . . asingama vaila vanthurum. . . . \" becomes \"bala sundar ayyo sorry antha line ah clarify pannama vittutu irukandrowsyok na solran en appavum indha grant work ku vanthurukkaruneenga en appava paakala pola en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaruyeahyeah chi karumam poding asingama vaila vanthurum. \"",
        "formal_text": "The code-mixed Tanglish tweets contain both Tamil and English words and phrases. So, we have converted the Tamil words into English terms by using Tamil to English Mapping Corpus. We have utilized NLTK(Natural Language Tool Kit) package in python to clean the data. For any classification task, the pre-processing steps are important which helps to improve the classifier performance. We have performed some of the cleaning steps like stop word removal, lemmatization and removed the special characters. For example, after pre-processing, the tweet \"@Bala sundar ayyo sorry. . . antha line ah clarify pannama vittutu irukan[: drowsy]ok na solran( en appavum indha grant work ku vanthurukkaru, neenga en appava paakala pola. . . . en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaru[: yeah][: yeah] ' chiiiii karumam podinnngggg. . . asingama vaila vanthurum. . . . \" is converted into \"bala sundar ayyo sorry antha line ah clarify pannama vittutu irukandrowsyok na solran en appavum indha grant work ku vanthurukkaruneenga en appava paakala pola en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaruyeahyeah chi karumam poding asingama vaila vanthurum. \"",
        "GPT2_formal_text": "is calculated using a zero-mean Gaussian error distribution. Formal: We focus on how to set up the teacher model (like in equation (1) above) while keeping the student model (like in equation (2) below) as flexible as possible. Formal: We’ll dive into more specifics about this in the next section. Formal: To make sure our model can handle different tasks, we’ll train it on the same examples as the teacher. But if the examples aren’t quite as similar to the teacher, we might train on new ones that are closely related to the examples in the training data. This approach doesn’t really limit the number of examples we use for training. We’ll explain how we do this in the next section. Formal: In this setup, we can use any part of the original data—the original examples (y) and any new ones we generate (ŷ). Basically, we’re just picking the data with the biggest variation when creating the new ones. We’ll get into how we choose this data in the next section. Formal: To give our model a head start, we’ll add some regularization to the loss function. Formal: To keep the model from becoming too focused on one task, we’ll try out a few different choices for this regularization. Formal: To make sure the student model is flexible, we’ll train it on a smaller set of examples while keeping the teacher model’s dimensions the same. Formal: To keep the model from getting stuck on one specific task, we’ll use something called constrained decoding to try out different decoding methods. Formal: Finally, to find the best parameters, we’ll tune the model’s parameters using both the original data and the new ones we generate. Formal: We’ll break this down further in the next sections. Formal: In the next section, we’ll also explain our way of picking the examples for training and how we use constrained decoding to improve the student model. Formal: We’ll dive into more specifics about how we choose the data in the next sections. Formal: In the next section, we’ll also explain our way of choosing the examples for training and how we use constrained decoding to improve the student model. Formal: We’ll also talk about a few ways to tweak the student model to get",
        "directory": "dravidianlangtech",
        "filename": "2021.dravidianlangtech-1.53.json"
    },
    {
        "casual_text": "We looked at eight different ways to check how well two texts match up, specifically comparing the system summary to the reference summary. BERTScore (BScore) checks for soft overlap by looking at the contextual BERT embeddings of tokens in both texts (Zhang et al., 2020). MoverScore (MScore) uses a distance measure on contextualized BERT and ELMo word embeddings (Zhao et al., 2019). Sentence Mover Similarity (SMS) finds the minimum distance matching between texts based on sentence embeddings (Clark et al., 2019). Word Mover Similarity (WMS) measures similarity by finding the minimum distance matching between texts represented as bags of word embeddings (Kusner et al., 2015). JS divergence (JS-2) measures the Jensen-Shannon divergence between the bigram distributions of the two texts (Lin et al., 2006). ROUGE-1 and ROUGE-2 measure the overlap of unigrams and bigrams, respectively (Lin, 2004). ROUGE-L looks at the overlap of the longest common subsequence between the two texts (Lin, 2004). We used the recall version of all these metrics (since the Pyramid method for human evaluations is based on recall) except for MScore, which doesn't have a specific recall version.",
        "formal_text": "We examine eight metrics that measure the agreement between two texts, in our case, between the system summary and reference summary. BERTScore (BScore) measures soft overlap between contextual BERT embeddings of tokens between the two texts 4 (Zhang et al. , 2020). MoverScore (MScore) applies a distance measure to contextualized BERT and ELMo word embeddings 5 (Zhao et al. , 2019). Sentence Mover Similarity (SMS) applies minimum distance matching between text based on sentence embeddings (Clark et al. , 2019). Word Mover Similarity (WMS) measures similarity using minimum distance matching between texts which are represented as a bag of word embeddings 6 (Kusner et al. , 2015). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text's bigram distributions 7 (Lin et al. , 2006). ROUGE-1 and ROUGE-2 measure overlap of unigrams and bigrams respectively 8 (Lin, 2004). ROUGE-L measures overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics (since the Pyramid method of human evaluations is inherently recall based) except MScore which has no specific recall variant.",
        "GPT2_formal_text": "= [ \"w/o_id\", \"w/o_name\", \"w/o_uri\", ] Formal: (I1) This is basically the feature representation for the query (like the entity mention in sentence 1). Formal: (I2) This is the feature representation for the keyphrase (like the entity mention in sentence 2). Formal: (L1) This is the feature representation for the mention (like the entity mention in sentence 3). Formal: (L2) This is the feature representation for the query (like the entity mention in sentence 4). Formal: (L3) This is the feature representation for the mention (like the entity mention in sentence 5). Formal: (L4) This is the feature representation for the query (like the entity mention in sentence 6). Formal: (L5) This is the feature representation for the mention (like the entity mention in sentence 7). Formal: (L6) This is the feature representation for the query (like the entity mention in sentence 8). Formal: Formal: (L7) This is the feature representation for the mention (like the entity mention in sentence 9). Formal: (L8) This is the feature representation for the query (like the entity mention in sentence 10). Formal: Formal: (L9) This is the feature representation for the mention (like the entity mention in sentence 11). Formal: Formal: (L10) This is the feature representation for the query (like the entity mention in sentence 12). Formal: Formal: (L11) This is the feature representation for the mention (like the entity mention in sentence 13). Formal: Formal: Formal: (L12) This is the feature representation for the query (like the entity mention in sentence 14). Formal: Formal: Formal: (L13) This is the feature representation for the mention (like the entity mention in sentence 15). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.751.json"
    },
    {
        "casual_text": "This paper talks about experiments done on tweet datasets in five languages: English, French, Spanish, German, and Italian. The basic idea is pretty straightforward: instead of sticking with a single-language model, we went with a multilingual one. We trained it on a big collection of English tweets, the original non-English tweets, and their automatic translations. We picked the XLM-RoBERTa model, which is a multilingual transformer model from Lample and Conneau (2019), and used a data-augmentation trick with machine translation. We looked into how pre-training with English data and data-augmentation affected things. We also compared how our multilingual models performed against their single-language versions for French (Martin et al., 2020) and English (Liu et al., 2019) and found some cool improvements.",
        "formal_text": "This paper presents the experiments carried out on several datasets of tweets in five different languages: English, French, Spanish, German and Italian. The general idea is pretty simple: instead of using a monolingual model, we chose to use a multilingual model that we can train over a large dataset of English tweets, over the original non-English tweets and over their automatic translations. We chose the multilingual transformer model XLM-RoBERTa from (Lample and Conneau, 2019) with a dataaugmentation technique using machine translation. We investigated the effects of pre-training with English data and data-augmentation. We also compared performances of multilingual models against their monolingual French (Martin et al. , 2020) and English (Liu et al. , 2019) counterparts and found interesting improvements.",
        "GPT2_formal_text": "ed layer, which gives us a hidden vector that represents the attention weight for each word in the document. Similarly, the word embeddings for the words in the paragraph are set to d_p, which is just the size of the word embeddings for the paragraph. Formal: We're assuming that the context vectors in this setup are just the word embeddings of the paragraphs. Formal: The goal here is to figure out a score vector for each pair of paragraph vectors, x_p and x_p, using the context vectors from both paragraphs. This score vector will then act as the average for all the paragraphs. Formal: For this task, we randomly choose up to n paragraphs from the document using a random sampling method, following the method by Hinton et al. (2014). Formal: After that, we add the averaged scores from the two paragraphs to our initial attention matrix. Formal: Our final attention vector, which is a multi-dimensional tensor, will be a mix of the original attention matrix, which we call A_i, plus a new attention score vector for each sentence s_i. Formal: Formal: To get the final attention vector, we use a linear transformation, which is a bit different from what we saw in Figure 1. Formal: The function f(h(x_p, x_p)) equals 1 if the representation h(x_p, x_p) is the sum of the embeddings of the word embeddings for the sentence x_p. If the embeddings are different, f(h(x_p, x_p)) is -1. Formal: Also, in the second sentence, x_p has a vector of attention weight vector r_i, and r_i is the average of the attention weights for the sentence s_i. Formal: Formal: For any two paragraphs p i and p j, f(h(p i, p j)) will equal 1 if h(p i, p j) equals the sum of h(p i, p j) for each i from 1 to n. Formal: Formal: To keep things simple, we'll just use the average weight for each attention vector for each sentence s_i as the main value. Formal: The weights for the paragraphs and sentences will be adjusted based on the representation h(p i, p j). Formal: Finally, the last hidden representation h(s",
        "directory": "coling",
        "filename": "2020.coling-main.23.json"
    },
    {
        "casual_text": "Looks like we gotta normalize those regression labels, otherwise the performance might not be as good as it could be.",
        "formal_text": "We find that the regression labels need to be normalized, or the performance may be sub-optimal.",
        "GPT2_formal_text": ": {id: 1, tag: 2, ext: 3, parent: 4 }  \n\nThe hash table is created by checking every possible sequence of tags. Formal: Each tag is made by combining the words that start with that tag. Formal: We're using the same method for calculating the log-likelihood, so that's how we calculate the probabilities. Formal: The similarity between two tags is basically the chance that word i and word j show up together in the same text. Formal: The parameters σ t and σ r are set up based on the dictionary and are used to calculate the probability for a specific tag, P t, given a set of words w t. Formal: We use the cosine similarity between two tags to measure how similar they are. Formal: We take the max-pooling of the cosine vectors, which gives us the probability distribution P t for a specific tag, P r. Formal: The probability of a word in the target language is calculated using the cosine similarity between the words in the source language, P s, and the words in the target language, P t, using the same parameter σ r, which was already calculated for the task. Formal: The probabilities can be broken down into probabilities for different parts of the translation process. Formal: The probabilities for the whole translation can be broken down by their individual parts, and the probabilities for each individual word can be broken down by its individual words, p i, p j. Formal: When we're calculating the probabilities, we consider the whole input string, which is basically a mix of words. Formal: We use the standard translation model, which works with any word in a sentence. Formal: Since we're dealing with a structured approach, we use a regression model to predict the probabilities. Formal: The regression model is basically a logistic regression model that looks at the whole input sentence. Formal: We use the logarithm function to get the probabilities. Formal: We calculate the probabilities by looking at the logarithm of the cosine similarity between the source and target language word embeddings. Formal: We set the weights for the parameters σ t and σ r, and for each word embedding, we calculate the expected probabilities for the different parts of the translation. Formal: To make sure we don't overfit, we set the weights for the parameters θ to zero to avoid overfitting",
        "directory": "acl",
        "filename": "2020.acl-main.267.json"
    },
    {
        "casual_text": "After round 22, we'll pick a smaller group of data, called D*, from the main training set D. This D* will be what the model uses for the next round of training. To choose D*, we can use different methods, like weighting (Liang et al., 2016; Zhou et al., 2020), sampling (Zhou et al., 2021), or batching (Yong Jae Lee and Grauman, 2011).",
        "formal_text": "end for 22: end procedure a subset D * of the training set D, which will be used by the model in the next round of training. The selection function SF can utilize different approaches, e. g. , weighting (Liang et al. , 2016; Zhou et al. , 2020), sampling (Zhou et al. , 2021) or batching (Yong Jae Lee and Grauman, 2011).",
        "GPT2_formal_text": "ed for each word in the input sequence, and then we use the inverse attention mechanism to estimate the attention. Formal: We calculate the attention scores for each word in the input sequence by doing a max-pooling operation on the attention weights, and we keep the embeddings of the words in the input sequence fixed. Formal: Let's say we have a sequence of size m, where each word w i is part of a set called X. In this sequence, each word w i in X is basically the average of the attention weights that go with it. Formal: The equation is X = (A × B) × (A × C) × D, where \"A\" and \"B\" represent the attention weights, and \"C\" and \"D\" are the hidden representations of the words in the input sequence. So, it's like taking the average of the two attention weights, A × B × C, for each word in the input sequence. Formal: Once we have the attention vectors for each word in the input sequence, we use them to predict the probability p(w i |X). Formal: To predict the probability, we just pick the embedding for the word w i, and then we calculate the average of the attention weights for the entire input sequence. Formal: Formal: Now that we know the probability p(w i |X), we can use the word embedding to predict the probability of each word in the sequence. Formal: This is how we calculate the probability for a word in the input sequence. Formal: Formal: The result is the probability p(w i |X) for the word w i, given the input sequence X. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "alta",
        "filename": "2021.alta-1.3.json"
    },
    {
        "casual_text": "So, how do we use this system to deal with the issue of making clauses and texts make sense together? The first thing we did was to categorize the different ways that semantic coherence works and then fit them into our system. Basically, semantic coherence happens through things like:\n\n1. Proforms\n1.1.",
        "formal_text": "How do we apply this system to the problem of semantic coherence in clauses and texts? The first practical step consisted in olasslfylng the forms of semantic coherence and integrating them into our system. Accordingly, semantic coherence is expressed via 1. Proforms 1. 1.",
        "GPT2_formal_text": ", and is_embedding(c_i, p). Formal: We can use the cross-entropy loss to compare different models. Formal: This method will help us understand the model's parameters better by looking at how often each type of information shows up in the model's output. Formal: For our task, we use the BERT model, which was pre-trained on the Wiki dataset (thanks to Artetxe et al., 2017). It’s trained on a huge pile of text, with around 100 billion tokens. Formal: For example, in Figure 2, the embedding for [CLS] is [CLS], and the embedding for [SEP] is [SEP]. Formal: We’ve got three main types of information to look at: (1) the word embedding, (2) the position embedding, and (3) the character embedding. Formal: The word embedding, which we call [w], is the most common type of information in a sentence. It’s usually made by two words, like \"cat\" and \"lion.\" Formal: The position embedding, which we call [p], is created by a character, like \"cat\" or \"lion.\" Formal: And the character embedding, which we call [c], is created by a character, like \"lion\" or \"cat.\" Formal: So, the embedding for [CLS] is [CLS], the embedding for [SEP] is [SEP], and the embedding for [C] is [C]. Formal: To get the word embedding for a sentence, we use this formula: Formal: The word embedding is written as [w], and the embedding for a word is written as [w]. Formal: The word embedding is made by two words, \"cat\" and \"lion.\" Formal: The position embedding is made by a character, like \"lion\" or \"cat.\" Formal: And the character embedding is made by a character, like \"lion\" or \"cat.\" Formal: So, the embedding for [CLS] is [CLS], the embedding for [SEP] is [SEP], and the embedding for [C] is [C]. Formal: These three types of information help us understand how words are represented. Formal: Each type",
        "directory": "C82",
        "filename": "C82-2041.json"
    },
    {
        "casual_text": "Here, we'll talk about the different datasets we used for our experiments, the steps we took to get the data ready, and how we added extra info like automatically generated group labels and word forms.",
        "formal_text": "In the following, we present the different corpora used in our experiments, the preprocessing performed prior to experimentation and the enrichment of the data with automatically derived cluster labels and lemma information.",
        "GPT2_formal_text": "ed, attention_type_masked, and attention_type_flat. Formal: We're suggesting a new model called the multi-head attention network, which can handle multiple attention heads in a bidirectional setup. Formal: To figure out which attention heads are focusing on different parts of the input, we use three types of attributes: Formal: Here, we'll look at how different attention heads behave on the same piece of data. Formal: We're thinking about combining the multi-head attention with a multi-attention model. Formal: We'll also talk about the scalability issue that comes from the multi-attention model. Formal: We'll go into more detail about how multi-attention works and what it means for different tasks. Formal: Lastly, we'll throw in an attention system that uses a linear projection. Formal: This will help us understand how the attention coefficients work in the multi-attention network. Formal: We'll see if the linear projection model can capture the overall attention distribution in the multi-attention network. Formal: Finally, we'll check how well our multi-attention works on a bigger task. Formal: Our model can figure out the attention coefficients for multiple heads at once, and we'll show how well it works with different attention heads. Formal: Our model is flexible enough to work with different attention networks. Formal: We'll share the results for different attention heads in the multi-attention network. Formal: For more info on multi-attention, check out Table 6. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C12",
        "filename": "C12-2088.json"
    },
    {
        "casual_text": "Alright, let's break down how we built the groundtruth dataset. Here's what we did:\n\n1. **Xtrain**: We started with 10,000 resumes, each represented as a 9054-dimension vector. These vectors are basically one-hot encoded versions of the resumes. The 9054 dimensions cover all the words in all the resumes after cleaning them up and getting rid of common stop words.\n\n2. **Ytrain**: Next, we created 10,000 output vectors, each with 50 dimensions. To do this, we picked a random job description and used the same method we talked about in the white-box approach (check out Section 5.1 for details). This method helped us identify the 50 most important words from the job description. \n\nNow, we assumed the recruitment algorithm uses something called USE embedding, but the attacker could use other text embedding methods or even just go with the most common words. So, in reality, the attacker doesn't need to know exactly how the recruitment algorithm works. We chose 50 words to match our white-box attack setup and to give us plenty of options. But, this number can be adjusted depending on the attack.\n\nOnce we had the important words for the job description, we added each of these words to every resume in our training set. Then, we checked with the black-box algorithm to see if adding these words improved the resume's position. If it did, we marked that word and resume combination as a \"1\" in Ytrain; if not, it was a \"0\". So, the output labels are one-hot encoded vectors.",
        "formal_text": "Creating the groundtruth dataset. We employed these steps: (1) Xtrain: 10, 000 records of 9054-dimension vectors, each vector is a resume that is coded by one-hot format. The 9054dimension vector represents all the words (after cleaning and removing stop words) in all the resumes. (2) Ytrain: 10, 000 records of 50-dimension vectors. For creating the output vectors, we selected a random job description, and employed the same technique proposed in white-box approach (Section 5. 1) and identified the 50 most influential words in the job description. We assumed the recruitment algorithm uses the USE embedding, however the attacker can try this approach with other text embedding algorithms, or even choose the most frequent words. Therefore, in practice, the adversary does not need to know about the recruitment algorithm. We used 50 words to be consistent with our white-box attack and have enough choices of words. However, this is a parameter that can be defined during the time of attack. After identifying the influential words for the target job description, the adversary adds each of the words to each of the resumes in the training set, and then queries the black-box algorithm asking if the position of resume is improved. If it is improved then the value for that word and that resume in Ytrain would be 1, otherwise it would be 0. Therefore, the output label is an encoded vector by one-hot format.",
        "GPT2_formal_text": "\" with the word's embedding, which is kind of like what we talked about in Section 2. Formal: We figure that if a model can predict the next word based on the context it's already seen, it should do a great job predicting the next word too. Formal: We think that if we have a model that can make the most of the context, we should also expect it to do a great job predicting the next word. Formal: To show this idea, let's look at the task of classifying words in a sentence. Imagine you have a sentence like \"Mary presented Paul with a gift.\" The context could be \"Mary presented Paul with a gift\" or \"Mary presented Paul with a gift.\" We think that if the model can predict the next word based on these context clues, it should do a great job in classifying the next word too. Formal: The issue is that there are so many possible words to choose from. That's why we came up with a model called \"word-level attention.\" This model uses a hidden layer, which is like a toolkit, to pick the right word from the options. We tested it and found that this model works really well for identifying words in a sentence. Formal: We think that if we had a model that could learn the next word from the context, it should also be able to predict the next word in a sentence. Formal: We ran experiments on both datasets and found that our model works really well. Formal: Our model picks the right word from the options based on the context, which is a big deal. Formal: It's been tested and shown to be really effective, which means our model has some cool potential. Formal: Formal: The experiments we did show that our model is pretty effective. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "blackboxnlp",
        "filename": "2021.blackboxnlp-1.36.json"
    },
    {
        "casual_text": "Assumption 5: There's no limit on what graph constants you can assign to a specific word.",
        "formal_text": "Assumption 5. There are no constraints imposed on which graph constants can be assigned to a particular word.",
        "GPT2_formal_text": "ed_seg (a_s, a_t). Formal: Using attention on the whole input sequence helps reduce the differences between how the reference and its response are structured. In the section we talk about the structure of the reference response, we'll explain how we can find the best answer by using the information from the surrounding sentences and its overall structure. Formal: The model we’re suggesting is all about finding the best answer by using the structure of the reference response. It works by looking at the sequence of attention scores, along with the sentence representations. The model takes the reference response as input and creates something called an encoding vector. This vector is made up of attention scores and a position embedding, where each score is like a direction and the embedding is a size. Formal: For a specific example, let’s look at a reference response, like \"This is a good cat because it has a great personality.\" The model could choose the response that has the highest attention score, which is the one that’s closest to the reference’s input. Formal: In this paper, we use a model that works with a specific input. This input can be a question, a paragraph, a sentence, or even an entire passage. In this case, we’re only focusing on the response, so it’s just the input we’re working with. Formal: We tweak the model by making sure the embedding for a word (i) is similar to the embedding for all the words in the passage (p). This means the response is already designed with these words in mind. Formal: To handle the difference in attention scores between the reference and the response, we create something called attention masks, which we call a_mask. This mask is like a special way to pick the answer with the highest attention score. Formal: Formal: So, for each input sequence x, we create a response y that has the highest attention score, calculated using a_mask. Formal: We use something called attention weights to handle this attention embedding. Formal: For the model we’re proposing, we use a sentence-level encoder to create a representation vector, which is a bit like the input vector we used before. Formal: We’re introducing something called attention layers to pull out the most important parts of the response from the input. Formal: We also add some extra components to handle the differences in attention",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "• P(D): The domain priors try to figure out how much useful data the learning system is picking up. We could do this using smaller parts of the text (phrase-level), but we'd rather use whole sentences (sentence-level): 6",
        "formal_text": "• P (D): the domain priors aim at modeling the percentage of relevant data that the learning framework induces. It can be estimated via phrase-level parameters but here we prefer sentence-level parameters: 6",
        "GPT2_formal_text": "= {'max_selections'} m = [], w = [], b = [], n = [], w2 = [], m2 = [], N = [], C = [], s = [], B = [], i = 1, j = 1, T = [], T2 = [], y = [], a = [], c = [], t = [], i2 = 1, j2 = 1, t2 = [], b = [], j = 1, G = [], q = [], u = [], M = [], s = [], k = [], t1 = [], t2 = [], t3 = [], s = [], t = [], t3 = [], s = [], t = [], s1 = [], s2 = [], s3 = [], s1 = [], s2 = [], s3 = [], e = [], w = [], w2 = [], a = [], c = [], b = [], y = [], a = [], q = [], u = [], n = [], W = [], x = [], y = [], w1 = [], w2 = [], m = [], j = 1, r = [], t = [], e = [], T = [], a = [], i = 1, j = 1, R = [], b = [], n = [], r = [], G = [], q = [], U = [], m = [], C = [], h = [], r2 = [], e2 = [], h2 = [], r3 = [], g = [], m2 = [], C2 = [], q = [], h = [], r = [], e2 = [], h2 = [], R2 = [], q = [], t = [], c = [], t2 = [], t3 = [], s = [], t = [], s1 = [], s3 = [], p = [], s = [], g = [], r = [], g = [], G2 = [], q = [], m = [], m2 = [], m3 = [], p = [], s = [], t = [], p = [], s = [], t2 = [], s3 =",
        "directory": "D14",
        "filename": "D14-1062.json"
    },
    {
        "casual_text": "We checked how well the human experts agreed on whether something was acceptable, just like we did for figuring out if it was a spelling variant. Turns out, spelling variants are way trickier for people to judge compared to other types of answers. The agreement score was only .60 for spelling variants, while it was .83 for everything else (check out Table 3). Even with regular training, clear guidelines, and lots of pre-testing, human raters still had some inconsistencies, which matches what Buck (2001) found.",
        "formal_text": "We compute the inter-annotator agreement of the human experts for the acceptability decision on the same subset as for the annotation if something is a spelling variant. We observe that spelling variants are substantially harder for humans to judge than other answer variants, with a  value of. 60 for spelling variants as opposed to. 83 for all other items (see Table 3 ). Such scoring inconsistencies by human raters despite regular training, annotation guidelines and thorough pre-testing are in line with Buck (2001).",
        "GPT2_formal_text": "_type set to 0.9 for both the CONCAT and PAS generation models, while the original input text length is set to 4. Formal: For CONCAT, we use this method to learn the attention parameters for both the regularizer and the softmax layer. Formal: After that, we can combine these learned attention weights with the actual correct inputs to get the final representation for the text. Formal: For PAS generation, we set the length to 80 and use a linear setup with a non-linear function at the output. Formal: Lastly, we use an attention mechanism to figure out the attention parameters for the text. Formal: Formal: (2) We run this process a total of 4 times for each of the 100 documents. Formal: Formal: (3) The final representation for the text is the average of the attention weights from the four runs. Formal: (4) In our tests, we set the learning rate to 0.0001. Formal: Formal: (5) We run this process a total of 4 times for each of the 100 documents. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "bea",
        "filename": "2022.bea-1.22.json"
    },
    {
        "casual_text": "- It works seamlessly with MULTITERM, so you can look up terminology as you go.  \n- Plus, it comes with extra features like concordance and coverage analysis.",
        "formal_text": "• it is fully integrated with MULTITERM so that terminology can be consulted on the fly; • it provides a number of additional features such as concordance and coverage analysis.",
        "GPT2_formal_text": "= [x i, j] ∈ R d. Formal: Basically, you can turn each query into a dependency graph by using a GAT-based transition network. Formal: You can also use a mix of different types of graphs to create dependency graphs. Formal: In this project, we're mainly focusing on creating graph embedding vectors. Formal: To make the graph embeddings more detailed, we create a convolutional neural network (CNN) that uses a window size of 200. This network has a hidden size of 1024 and an output dimension of 200. The input to the CNN is the dependency graph x i, which we get using a simple gradient descent approach. The output is a combination of the embeddings for x i, including the ones from the dependency trees. Formal: To compare our results with other methods, we use the FastText embedding (from Bojanowski et al., 2017) to turn each query into a vector. For our experiments, we use a single-layer Bi-LSTM as the neural network to calculate the embedding for each query. Formal: We test our graph embedding method on two popular datasets: SQuAD (Rajpurkar et al., 2016) and Yelp Review (Zhang et al., 2018). Formal: We also did some experiments on the CoNLL-2009 dataset. The results are in Table 4. Formal: For the CoNLL-2009 dataset, we use the same training and evaluation methods as in the previous work. Formal: We ran our experiments on a single-processor machine with an Intel Core i5 CPU running at 2.60GHz. Formal: We tested our method on the test set of the Yelp Review dataset. Formal: We ran our experiments on the test set of the Yelp Review dataset. Formal: We also ran our experiments on the test set of the CoNLL-2009 dataset. Formal: We ran our experiments on the test set of the Yelp Review dataset. Formal: We also ran our experiments on the test set of the CoNLL-2009 dataset. Formal: Formal: The graph embedding vector for x i is calculated using the maximum-likelihood approach. Formal: The graph embedding vector for x i is calculated using the maximum-likelihood approach. Formal: We used the toolkit from the WMT 2019 training workshop for our experiments. Formal: We used the toolkit from the W",
        "directory": "eamt",
        "filename": "1998.eamt-1.2.json"
    },
    {
        "casual_text": "Hey there,\n\nJust wanted to let you know that Prof. Jan Hajic from Charles University in the Czech Republic and Prof. Junichi Tsujii from Microsoft Research in China are the co-chairs of the COLING 2014 Program Committee. This was announced on July 8, 2014.\n\nCheers!",
        "formal_text": "Prof. Jan Hajic (Charles University, Czech Republic) Prof. Junichi Tsujii (Microsoft Research, China) COLING 2014 Program Committee Co-Chairs July 8, 2014",
        "GPT2_formal_text": "ed_model = attention_masked_model, # |model| = model_masked, # |dist| = dist, # |G| = g, # |Y| = y, # |T| = t, # |E| = e. Formal: In this setup, the sequence embedding is always a word embedding, but the hidden state is the result of an attention layer. This layer helps figure out how much attention to pay to each word based on its context. Formal: We add a dropout operation (like the one from Srivastava et al., 2014) to the hidden state. This helps prevent the model from getting too focused on certain words and making them less useful. In Figure 3, the main settings for the attention model are (1) a hidden state size of 200, (2) the dropout rate set to 0.2, and (3) the embedding dimension set to 200. Formal: The attention is applied to the beginning of the input word, which helps the model learn to focus more on the word's immediate context. Formal: The model learns to pay attention to the context of each word, which is super important when dealing with language with lots of words and lots of different meanings. Formal: We use a 300-dimensional vocabulary with 12 million words. The context embeddings are built using the word2vec model from Google (Mikolov et al., 2013). Formal: We create both the input and output embeddings by concatenating the embeddings from the base layer with their respective context embeddings. This makes up the word embeddings for the word at position |Y|. Formal: The model is trained for 10 epochs. Formal: Following the approach in Ghazvininejad et al. (2017), we train the model on a subset of the target vocabulary. For training, we use a sequence length of 2. Formal: We run the model for 10 epochs and use the Adam optimizer with a learning rate of 0.1. Formal: The final embeddings are trained using these embeddings. Formal: For training, we use a window size of 5. Formal: The model is initialized using the original training data. Formal: We apply dropout with a probability of 0.2. Formal: We also use a dropout layer to prevent the model from getting too focused on specific words. Formal",
        "directory": "C14",
        "filename": "C14-3000.json"
    },
    {
        "casual_text": "Check out Table 2 for the stats on transcribed and conceptually annotated data.",
        "formal_text": "Statistics on transcribed and conceptually annotated data are reported in Table 2.",
        "GPT2_formal_text": "= [r_idx], entity_ids = [e_idx], entities = [e_i, e_j], event_ids = [e_i, e_j], instances = [e_i, e_j], and the first part of the FST is a softmax function. Here, c_t is the count of instances in the training set. Figure 1 illustrates how we create a training set for an entity pair by using a dictionary entry e_i for the i-th word in the entity pair. Formal: During the training process, we predict the type of entity pair e_i for the i-th word in the entity pair. Specifically, we use a probability distribution P_t(e_i) = (1/m_t) for the predicted type and a temperature parameter α_t = 1/m_t for the temperature of the predicted type. Formal: To understand how different predictions for entity pairs affect the model's performance, we use a regression model to evaluate the model. This model uses a k-nearest neighbors (kNN) model with k parameters, and the weights θ for the predicted entity pairs are calculated using the cross-entropy loss function. Formal: To calculate the scores for each prediction, we calculate the average of the scores for all predictions. Formal: In the first part of the model, we train a classifier θ_t using the predicted type and a prediction distribution P_t(e_i) for the i-th word in the entity pair. The probability distribution P_t(e_i) ∈ R^k is calculated by using a cross-entropy loss function. Formal: The second part of the model uses an ensemble of classifiers θ_n, θ_n' ∈ R^m to predict the type. The probability distribution P_t(e_i) ∈ R^m ∉ R^m' is calculated by using a cross-entropy loss function. Formal: Finally, we use the predictions from the classes θ_n, θ_n' ∈ R^m' to predict the type. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1112.json"
    },
    {
        "casual_text": "In this part, we're using the type system from Groschwitz's 2019 work. In this system, types are represented as Directed Acyclic Graphs (DAGs), with the nodes being sources and the edges showing requests.",
        "formal_text": "Throughout this section we assume the type system of (Groschwitz, 2019), where types are formally defined as DAGs with sources as nodes, and requests being defined via the edges.",
        "GPT2_formal_text": "ed(word_i, word_j) = attention_masked(word_i) / sum from k=1 to K of attention_masked(word_i) for each i from 1 to K. Formal: The last layer of the attention layer predicts the next word based on the current hidden state. For instance, if you have a hidden state h_t, the last hidden state from the BiLSTM is h_t. Formal: The attention layer is trained to predict the next word based on the current hidden state. Formal: The final hidden state is a combination of the hidden states from the last layer and the hidden state from the BiLSTM. Formal: The final hidden state is a combination of the hidden states from the last layer and the hidden state from the BiLSTM. Formal: The attention layer is trained to predict the next word using the current hidden state. Formal: The attention layer is trained to predict the next word using the current hidden state. Formal: The final hidden state is a combination of the hidden states from the last layer and the hidden state from the BiLSTM. Formal: The attention layer is trained to predict the next word using the current hidden state. Formal: The final hidden state is a combination of the hidden states from the last layer and the hidden state from the BiLSTM. Formal: The attention layer is trained to predict the next word using the current hidden state. Formal: The final hidden state is a combination of the hidden states from the last layer and the hidden state from the BiLSTM. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "The second part is a regularization term that makes sure the inner product between the poisoning loss gradient and the fine-tuning loss gradient isn't negative. The λ is just a number that shows how strong this regularization is. We named this method \"Restricted Inner Product Poison Learning\" (RIPPLe). \n\nNow, in a situation where the environment changes, we don't actually know the true fine-tuning loss, so the attacker has to use a stand-in, like L FT, to guess what it might be. Later on, we'll show that even a rough guess—like using a dataset from a totally different area—can work well enough for the RIPPLe attack to do its thing.",
        "formal_text": "where the second term is a regularization term that encourages the inner product between the poisoning loss gradient and the fine tuning loss gradient to be non-negative and λ is a coefficient denoting the strength of the regularization. We call this method \"Restricted Inner Product Poison Learning\" (RIPPLe). 2 In the domain shift setting, the true fine tuning loss is unknown, so the attacker will have to resort to a surrogate lossL FT as an approximation of L FT. We will later show experimentally that even a crude approximation (e. g. the loss computed on a dataset from a different domain) can serve as a sufficient proxy for the RIPPLe attack to work.",
        "GPT2_formal_text": "(or a mix of them) can be written as r_i = e_i + r_j = e_i + ... + r_k. Formal: For the CCG parser, we’ve got a document-specific feedforward neural network (FFNN) setup using a convolutional neural network (CNN). We’ve also got a pairwise classification layer that connects the input features to their labels. The final classification layer in the CNN is trained using cross-entropy loss. Formal: For the DATR parser, we’ve got the document representation H and its parse tree T, which is made up of nodes called r_i. Each node in this tree is a part of a sentence (or, to be more precise, a set of syntactic phrases). Formal: To get the parsing output S, we train a fully connected neural network (FCN) using the parsed document representations H, along with the parse tree T. This FCN is trained using the submodularity of the input features. Formal: You can find more details about how to train the DATR parser and the DATR derivation language (which is a specific way of representing DATR) in (He et al., 2011). Formal: Finally, the FCRF is a deep neural network that’s really good at handling multi-label classification tasks. Formal: We’ve got the full details about our GCN-based models and how we implement them in section 4. Formal: When we use the DATR parser for real-world tasks, we run all our experiments using a single NVIDIA Tesla K80 GPU. Formal: Formal: We’ve got the cross-entropy loss term for the attention mechanism in Section 5. Formal: The fully connected layer in the CNN is in Section 6. Formal: We’ve got the full FCRF and DATR derivation language details in (He et al., 2011). Formal: Finally, the FCRF is a deep neural network that’s really good at handling multi-label classification tasks. Formal: We’ve got the cross-entropy loss term for the attention mechanism in Section 5. Formal: The fully connected layer in the CNN is in Section 6. Formal: We’ve got the full FCRF and DATR derivation language details in (He et al.,",
        "directory": "acl",
        "filename": "2020.acl-main.249.json"
    },
    {
        "casual_text": "Check out Table 2 for their results. Here's what stood out: the model without domain adaptation (Noadapt) is already pretty strong, mostly because we've got plenty of training data. But, if you just throw all the queries from different domains into one model (Union), it messes things up big time. This happens because the same query can be labeled differently depending on the domain and the context. Plus, the amount of training data varies a lot between domains. For instance, slots like app names in other domains get overshadowed by slots in the PLACES domain, like place names, since PLACES has the most slot types and is the second biggest in terms of dataset size. Lastly, the feature augmentation method from Daumé III (2009)—you can see the F1 scores for that and different LSTM model versions across seventeen personal assistant domains in Table 3.",
        "formal_text": "Their results are shown in Table 2. A few observations: the model without domain adaptation (Noadapt) is already very competitive because we have sufficient training data. However, simply training a single model with aggregated queries across all domains significantly degrades performance (Union). This is because in many cases the same query is labeled differently depending on the domain and the context. This is also because the amounts of training data are widely different across different domains. For example, slots (e. g. app name) in other domains are overwhelmed by slots in PLACES domain such as place name since our PLACES domain is the largest in terms of number of slot types and 2nd largest in terms of dataset size. Finally, the feature augmentation method of Daumé III (2009) Table 3: F1 scores for Daumé III (2009) and LSTM model variants across seventeen personal assistant domains.",
        "GPT2_formal_text": "ing, which was introduced by Luong et al. in 2015, is a type of attention mechanism designed to handle the sequential parsing problem. It works by combining a mix of attention and a non-linear layer (like an LSTM) to create a hidden representation for each word. This hidden representation is then used to figure out the prediction for the next word in the sequence. Formal: The model parameters are learned during the pretraining phase. Formal: The model parameters are learned during the pretraining phase. Basically, it's like the input and output are being shaped by the model. Formal: The model parameters are learned during the pretraining phase. Formal: Formal: Formal: Formal: The model parameters are learned during the pretraining phase. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1038.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\nP({e, f}, a) is like saying, \"Hey, we're looking at these things, e and f, and how they relate to a.\" It's calculated by multiplying a couple of other probabilities: PG(; p$) and P(a|{e, f}).\n\nNow, PM(e, f) is just a way of saying, \"How likely are e and f to happen together?\" And this is made up of two parts:\n- pø times θN(e, f), which is like saying, \"There's a chance pø that this normal thing θN(e, f) happens.\"\n- (1 - pø) times θJ(e, f), which is like saying, \"And if the normal thing doesn't happen, there's a chance (1 - pø) that this other thing θJ(e, f) happens instead.\"\n\nSo, it's all about figuring out how likely things are to happen together and how that connects to something else, a.",
        "formal_text": "P ({ e, f }, a) = P G (; p $ )P (a|{ e, f }) e, f P M ( e, f ) P M ( e, f ) = p ø θ N ( e, f ) + (1 − p ø )θ J ( e, f ).",
        "GPT2_formal_text": "= { ..., 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] Formal: We used the same setup for these methods as in (Li et al., 2021). To check how well they perform, we used the BERT-iBLEU score. Just like in (Li et al., 2021), we used the BERT model to check how well the similarity-based similarity selection works. The BERT model we used has an embedding dimension of 512. For training, we used a linear layer with a ReLU activation function. Formal: We ran each approach 10 times with different random seeds, and the results are in Table 5. Our model did better than all the other baselines in all the evaluation measures. Formal: For the basic model, we compared the performance of the baseline model, BERT-based models, and our model. We also included the best model for each metric and the scores for the baseline and our model. Formal: Since our model was trained on text data, the BERT model had to be pre-trained on a big dataset of Wikipedia articles. We trained a BERT model with an embedding dimension of 2048. We found that using a smaller embedding dimension worked better. Formal: To train the text segmentation model, we split the text into sentences and used BERT to find the best token representation. Formal: For the basic model, we used a single BERT model to find the best segmentation representation. Formal: The BERT model we used has an embedding dimension of 512. Formal: We trained a BERT model with an embedding dimension of 2048 on the text data. Formal: For the basic model, we compared the performance of the baseline model, BERT-based models, and our model. Formal: For the basic model, we compared the performance of the baseline model, BERT-based models, and our model. Formal: Formal: Table 6 shows the scores for the basic model and the BERT model. Formal: For the basic model, we trained a BERT model with an embedding dimension of 512 on the text data. Formal: For the",
        "directory": "D08",
        "filename": "D08-1033.json"
    },
    {
        "casual_text": "This feature is based on the method described in the paper by Fattah and Ren from 2008. We don’t usually include super short sentences in the final summary because they often don’t have much info. But longer Punjabi sentences? They can pack a lot of information. To calculate this feature, you just divide the number of words in a sentence by the word count of the longest sentence. The result will always be 1 or less.",
        "formal_text": "This feature is calculated as published in (Fattah & Ren, 2008). Very short sentences are avoided for including in final summary as often they contain less information. On the other hand lengthy Punjabi sentences might contain lot of information. This feature is calculated by dividing number of words in a sentence with word count of largest sentence. Its value will be always less than or equal to 1.",
        "GPT2_formal_text": "ed is a function that takes a single input token and turns it into a hidden vector. It then assigns a score to each token based on its attention weight. Formal: For any input token x i, the attention is calculated by taking the dot product of the attention vector x i and the hidden state vector h. This is done using a linear layer with a ReLU activation, which is the traditional approach. Formal: To get the hidden state vector h from the output layer of the CNN, we calculate it using the dot product of the attention vector x i and the output state vector h. Formal: Lastly, we combine all the hidden states from both the input and output layers using a linear layer with ReLU activation. Formal: In this part, we'll talk about our model architecture and how we use the attention mechanism to combine the text representations. Formal: In Section 4, we introduce our model using a bi-directional LSTM model with a CNN as the final output layer. The sentence embedding matrix, represented as H_s, is calculated by taking the dot product of the attention vector x i and the hidden state vector h. Formal: To get the hidden state vector h from the output layer of the CNN, we calculate it using the dot product of the attention vector x i and the output state vector h. Formal: Finally, we combine all the hidden states from both the input and output layers using a linear layer with ReLU activation. Formal: The encoder-decoder network is a bi-directional LSTM model that works with a sequence of tokens. It includes a text encoder and a position encoder that take into account the sequence of tokens. Formal: After the encoder is done generating the tokens, the position encoder grabs the hidden state vector h for each word in the sentence. Formal: Figure 1 shows the architecture of our model. Formal: For our model, we started with a convolutional neural network (CNN) as the encoder. To build the sentence embedding matrix, we first preprocess the text input by removing stopwords. Then, we add a linear layer to the CNN to handle the sequence of tokens. Formal: To handle the context of the sentence, we use an attention mechanism to combine the attention vector x i and the hidden state vector h. Formal: Here's how the model is set up: Formal: We use a bi-directional LSTM model with a CNN",
        "directory": "C12",
        "filename": "C12-3024.json"
    },
    {
        "casual_text": "For multi-label evaluation, let's say each edit \\( e_i \\) in the set \\( E \\) has a bunch of relevant categories, which we'll call \\( y_i \\) and it's part of the bigger set \\( C \\). Then, we also have the categories that our model predicts for that edit, which we'll call \\( h(e_i) \\).",
        "formal_text": "Multi-label Evaluation We denote the set of relevant categories for each edit e i ∈ E as y i ∈ C and the set of predicted categories as h(e i ).",
        "GPT2_formal_text": ", and the predicates are all set up correctly. Formal: Also, we’ve created a query parser that works with the NLTK text parser to figure out the answers to questions. The parser goes through all the training sentences in the training set and picks the best translation for each one. Formal: We’ve already checked and confirmed that the word-level alignments we got from the log-bilinear model are actually good for aligning text and speech. Formal: We’ve tweaked the parser to work with the TicoNMT system, and we’ve also added another model that uses the sentence-level alignments for translating speech. Formal: The log-bilinear model we’re suggesting also checks if the answer to a question is in the training data. It does this by looking at the translation probabilities from the lexical and syntactic models. Formal: We’ve tested it on ten different languages, and it consistently works better than the other models in a bunch of NLP tasks. Formal: You can grab the code and datasets for this project right here: http://github.com/tika-karimi/NLTK_Text_Speech_Translation. Formal: We’ve also shared our data, the results, and a statistical analysis of it. Formal: Finally, we’ve made a demo of our model. Formal: Check out Figure 2 for the whole setup of our method. Formal: We’re using a multilingual K-means clustering to group the training data. Formal: We’ve also included a scatterplot of the models we’ve trained in Figure 1. Formal: For the log-bilinear model, we’re using the same setup as the transformer model. Formal: We’ve also included the log-lexical model, which was trained on the English part of the training data (which was part of the MSRA corpus). Formal: The word-level alignment is based on the log-bilinear model. Formal: All the text, speech, and question translations in the training set are taken from the training set. Formal: For the TicoNMT system, we’re using the same setup as the TicoNMT system. Formal: We’ve also included the TicoNMT system’s stats, including how many",
        "directory": "D13",
        "filename": "D13-1055.json"
    },
    {
        "casual_text": "On average, 27.31% of compounds get translated wrong when looking at individual cases. But if we combine all five contexts, 61.62% of compounds have at least one incorrect translation. This shows that even a really good NMT model struggles with translating compounds, even though all the parts of these compounds are super common in the training data. We also noticed that PP compounds have a much higher error rate—37.72%—compared to the other two types, which are around 21.94% and 22.25%. We'll dive into this more in the next section.",
        "formal_text": "Instance-wise, 27. 31% compounds are translated incorrectly. However, when aggregating all 5 contexts, 61. 62% compounds suffer at least one incorrect translation. This suggests that a well-trained NMT model is not robust in translating compounds, though all atoms within them are highly frequent in the training set. We also observe that the error rate of PP compounds, 37. 72%, is much higher than the other two, 21. 94% and 22. 25%, which we will discuss in detail in the following section.",
        "GPT2_formal_text": ". In this setup, the max layer and the hidden state of the output layer are both set to the size of the input embedding. Formal: The word embedding for each word in the summary is turned into a fixed-size vector using a pre-trained word embedding model. Formal: We use a transformer model (like the one by Vaswani et al., 2017) to predict the vector for each word in the summary. Formal: We grab the cross-entropy loss, which is a way to measure how well the model's prediction is doing. Formal: Finally, we add some noise, called topic-specific random noise, to the word embedding vectors to make them less noisy. Formal: We also tweak the model's settings by training it using the source document and the test document, and then testing it on the test document. Formal: The model gets trained using cross-entropy loss (from Vaswani et al., 2017) and topic-specific random noise. Formal: Formal: To make the model's predictions more reliable, we use a hybrid classifier to get word-level and topic-level predictions from the document. Formal: The model's predictions are then combined and averaged using a cross-entropy loss. Formal: Check out Figure 1 for the whole setup of our model. Formal: To avoid too much variation in the predictions, we add a Gaussian noise term to the word embedding vectors. Formal: Our model is trained using cross-entropy loss, a hybrid classifier, and topic-specific random noise, all combined into one model. Formal: We use the cross-entropy loss, a hybrid classifier, and topic-specific random noise, all combined into one model. Formal: We use the cross-entropy loss, a hybrid classifier, and topic-specific random noise, all combined into one model. Formal: Formal: We use the cross-entropy loss, a hybrid classifier, and topic-specific random noise, all combined into one model. Formal: Formal: Formal: We use the cross-entropy loss, a hybrid classifier, and topic-specific random noise, all combined into one model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.368.json"
    },
    {
        "casual_text": "But, there aren’t many datasets out there that work well for creating adaptive argumentative writing systems for teaching purposes. As far as we know, there are only two datasets from the education field that use student-written texts and have been annotated to show argumentative structures. These are the ones by Stab and Gurevych (2017a) and Wambsganss et al. (2020c).",
        "formal_text": "However, corpora that are applicable for the design and development of adaptive argumentative writing systems in pedagogical scenarios are rather scarce. To the best of our knowledge, there are only two collections from the educational domain which are based on student-written texts and annotated for argumentative discourse structures (Stab and Gurevych, 2017a; Wambsganss et al. , 2020c).",
        "GPT2_formal_text": "= [n; i] p(i) = [t; i] (2) Formal: Basically, the sentence is made up of different words or parts, and each word has its own part-of-speech (POS) tag. This tagging helps with identifying the correct relationships between the words.  \n\n(2) Table 6 shows how the FST model did with the relationship extraction task. The FST model was trained with a set of general-purpose relationship patterns. For each pattern, it used a bunch of POS tags to figure out how related each pair of words in the sentence was.  \n\nSince these patterns were made by hand, they had a lot of noise. So, the model was trained to create these patterns at the same time it was translating the sentences. After that, it was used to make some sentence-level predictions by using a softmax function to create a probability distribution for the relationship patterns. Formal: The FST model was trained with a big dataset of sentence-level relationships, using a setup similar to what we used in Figure 1.  \n\nThe results are in Table 6. The FST model did really well, with an accuracy of 94.6%. This shows that the FST model is really good at pulling out useful relationship info from a big dataset. Formal: Overall, the FST model did a great job, with an accuracy of 94.6%. The accuracy was higher than the baseline model (0.82%). Formal: This shows that the FST model is really good at extracting relationship info from a big dataset. Formal: Compared to the baseline model, the FST model improved its accuracy by 0.61%. Formal: Overall, the FST model improved its accuracy by 0.63%. Formal: This is the first time this relationship extraction task has been used in a full-on machine translation setup. Formal: The FST model performed really well, with an accuracy of 94.6%. Formal: The FST model did a great job, with an accuracy of 94.6%. Formal: The FST model also did a good job, with an accuracy of 94.5%. Formal: The FST model also did a good job, with an accuracy of 94.4%. Formal: The FST model was trained with a dataset of sentence-level relationships, and it did a great job with the accuracy of 94.6%. Formal: The FST model",
        "directory": "acl",
        "filename": "2022.acl-long.599.json"
    },
    {
        "casual_text": "The work we're talking about here focuses on two main areas related to making things more robust. First, we're looking at using features from both syntactic and semantic processing that can handle changes in genre and language without breaking down. We're aiming to move beyond the simple \"bag of words\" approach used in older systems (like those by Bagga & Baldwin, 1998; Gooi & Allan, 2004; Pedersen et al., 2005) that didn't really try to dig into deeper semantic features, which are tricky to pull out. We're also trying to avoid relying too much on specific biographical details (like what Mann & Yarowsky, 2003, did) that you don't see often in the kind of documents search engines usually pull up—stuff like place of birth or family relationships.\n\nThe second part of our work involves applying these techniques to both English and Chinese news collections. Turns out, the methods work well for both languages, but when we look closely at the errors, we find some cool differences between the two.",
        "formal_text": "The specific contributions reported here fall into two general areas related to robustness. In the first, we explore the use of features extracted from syntactic and semantic processing at a level that is robust to changes in genre and language. In particular, we seek to go beyond the kind of bag of local words features employed in earlier systems (Bagga & Baldwin, 1998; Gooi & Allan, 2004; Pedersen et al. , 2005 ) that did not attempt to exploit deep semantic features that are difficult to extract, and to go beyond the kind of biographical information (Mann & Yarowsky, 2003) that is unlikely to occur with great frequency (such as place of birth, or family relationships) in many of the documents returned by typical search engines. The second contribution involves the application of these techniques to both English and Chinese news collections. As we'll see, the methods are effective with both, but error analyses reveal interesting differences between the two languages.",
        "GPT2_formal_text": "_t, and there are other attention masks like the one in Figure 3. The final attention mask we get is E_t = |H_t| × |H|. Formal: So, H_t is the hidden state of the first head, and H_b is the hidden state of the second head. Formal: In this paper, we’ve come up with a new approach to dealing with sentence segmentation and sentence boundary detection. We use a special kind of attention mask to focus on important bits of the sentence and its surrounding context. We tested this on two common datasets: SNLI (Bowman et al., 2014) and BERT-base (Devlin et al., 2019). Our method works really well and beats other methods on both datasets. We also looked into how well it works for Chinese sentences and found that it works better than the usual supervised sentence segmentation. Formal: We use two neural network models to handle sentence segmentation and sentence boundary detection at the same time. Formal: Our method is different from the usual supervised sentence segmentation approach because it doesn’t rely on any supervision. Formal: The first-order model we use to handle sentence segmentation and sentence boundary detection is trained using data from just one sentence. Formal: In this paper, we’ve introduced a new way to handle sentence segmentation and sentence boundary detection. Our method uses a special attention mask to focus on important bits of the sentence and its surrounding context. We tested this on two common datasets: SNLI (Bowman et al., 2014) and BERT-base (Devlin et al., 2019). Our method works really well and beats other methods on both datasets. Formal: Formal: Here, |H_t| represents the size of the hidden state for the first head, and |H_b| is the size of the hidden state for the second head. Formal: We made two versions of our model to test how well it works. One version uses the same word embedding embedding as in the original model, while the other uses a different word embedding that’s been pre-trained to work with the Chinese language. Formal: Formal: This work is under a Creative Commons Attribution 4.0 International License. Formal: Formal: Formal: Here, |H_t| is the size of the hidden state for the first head, and |H_b| is the size of the hidden state",
        "directory": "D07",
        "filename": "D07-1020.json"
    },
    {
        "casual_text": "This paper talks about tackling two main issues: polysemy (when a word has multiple meanings) and having a phrasal lexicon. Like Guthrie and Yuasa, we use a vector representation, meaning each article is represented as a vector. But here's the twist: instead of assigning each coordinate of the vector to every word in the article, we focus on a single word (specifically, a noun) whose meaning we’ve figured out. \n\nOur way of figuring out the meaning of a word is based on Niwa's method, which compares the similarity between two sentences—one that has a noun with multiple meanings and another that’s like a dictionary definition. To handle Walker's problem, after we’ve sorted out the meanings, we calculate how related the words are in terms of meaning and group the ones that are related together.",
        "formal_text": "The approach proposed in this paper focuses on these l)roblems, i. e. 1)olysemy and a phrasal lexicon. Like Guthrie and Yuasa's methods, our approach adopts a vector representation, i. e. every article is characterised by a vector. However~ while their ~pproaehes assign each (: oor(linate of a vector to each word in artMes, we use a word (noun) of wtfich sense is disambiguated. Our disambiguation method of word-senses is based on Niwa's method whMt use(l the similarit; y 1)etween two sentences, i. e. a sentevee which contains a polysenmus noun and a sevtenee of dictionarydefinition. In order to cope with Walker's l)roblem, for the results of disand)iguation technique, semantic relativeness of words are cMeulated, and semantically related words are grout)ed together.",
        "GPT2_formal_text": "= {title_1, title_2, ..., title_n, ..., title_n+1}, title_k = {title_1, title_2, ..., title_k+1}, title_s = {title_1, title_2, ..., title_k, title_s+1}, titles = {title_1, title_2, ..., title_k+1}, the_text = {title_1, title_2, ..., title_k}, title_s = {title_1, title_2, ..., title_k+1}, the_categories = {title_1, title_2, ..., title_k+1}, the_content = {title_1, title_2, ..., title_k}, title_s = {title_1, title_2, ..., title_k+1}, the_titles = {title_1, title_2, ..., title_k}, the_captions = {title_1, title_2, ..., title_k+1}, the_summary = {title_1, title_2, ..., title_k+1}, the_categories = {title_1, title_2, ..., title_k+1}, the_content = {title_1, title_2, ..., title_k+1}, title_s = {title_1, title_2, ..., title_k+1}, the_titles = {title_1, title_2, ..., title_k+1}, the_captions = {title_1, title_2, ..., title_k+1}, the_summary = {title_1, title_2, ..., title_k+1}, title_s = {title_1, title_2, ..., title_k+1}, the_titles = {title_1, title_2, ..., title_k+1}, the_titles = {title_1, title_2, ..., title_k+1}, the_titles = {title_1, title_2, ..., title_k+1}, title_s = {title_1, title_2, ..., title_k+1}, title_s = {title_1, title_2, ..., title_k+1},",
        "directory": "C96",
        "filename": "C96-1069.json"
    },
    {
        "casual_text": "Since the title words are already part of the nodes in X, we can just add another type of edge, like we did in section 3.2.4, to have three types of edges for the same pair of nodes: edges from X, R, and the title itself. For the edges from X, we use the proximity of the title words based on their positions to determine the edge weights. For the title edges, we use the context from the title to update the contextualized representations, kind of like what we did in equations (3, 9). The results are in Table 4.",
        "formal_text": "Since the title words are already represented as nodes in X, we can simply add another edge type, as similarly done in §3. 2. 4, to make three types of edges for the same node pair: edges from X, R, and the given title respectively. As in edges for X, we use position-based proximity between title words, for edge weights. Given the title edges, we use contexts gathered from the title, to update contextualized representations, similar to Eq (3, 9). The results are shown in Table 4.",
        "GPT2_formal_text": "ed_linear. This gets better by making the vector representation more sparse and less dense. So, we use the output from the first layer, which is based on the output from the last layer, as the embedding for the query and answer tokens. This method is different from the usual ones, which don't have a key-value setup, and it actually works better in real-world situations. Formal: We use this hybrid approach to tackle the scalability issue. Formal: • Equations (1), (2), and (3) are the same as equations (2), (3), and (4). Formal: We use the scalability method to estimate the probabilistic model, which is basically the next-step for the derivation from equation (2). Formal: We also use a scalability method to estimate the parameters of the neural network, specifically for the queries and answers, respectively. Formal: Lastly, we calculate the scalability of the model by finding the best solution. Formal: For our experiments, we trained different embedding functions to make the predicted sequence more sparse. Formal: For the queries, we use a three-layer neural network. Formal: The input for this neural network is a sentence, and each word is represented by a vector. Formal: Next, we calculate the score for each word in the input sentence by dividing its score by the average score of the words in its query. Formal: Finally, we pick the best solution by solving the equation from equation (3). Formal: Formal: This method is pretty similar to the variational Bayes method, which was introduced by Kipf and Welling in 2015. Formal: In our experiments, we used the same embedding functions to get word embeddings. Formal: We use the variational Bayes method to estimate the parameters of the neural network, specifically for the queries and answers, respectively. Formal: Finally, we calculate the scalability of the model by finding the best solution. Formal: Formal: Formal: For our experiments, we trained different embedding functions to make the predicted sequence more sparse. Formal: We use the variational Bayes method to estimate the parameters of the neural network, specifically for the queries and answers, respectively. Formal: Formal: Formal: For our experiments, we trained different embedding functions to get word embeddings. Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.209.json"
    },
    {
        "casual_text": "We're mainly focused on this problem: when you have a compound noun, how do you figure out which way it could be understood, from the most likely to the least likely? This problem can be thought of using something called unification. Unification is a neat way to describe how different pieces of information come together to make sense of things. It helps with figuring out the different meanings of words and structures, as well as how those meanings fit together.\n\nOne cool thing about unification is that it doesn't have a favorite order for putting things together. It doesn't automatically prefer one way over another. This is really useful when you're trying to decide which meaning of a compound noun is the most likely.\n\nWhen dealing with compound nouns, you have to figure out the different possible meanings (lexical ambiguity), how those meanings fit together in a sentence (structural ambiguity), and how everything adds up to make sense (semantic composition). It's not a great idea to focus too much on just one part of this process because everything is connected. For example, the way you pick a \"frame\" (like a mental outline of what's happening) can change how you understand the words (lexical ambiguity). Also, choosing a frame and deciding who is doing what (case/role binding) are part of the same process of making sense of the whole thing (semantic composition). And how you resolve structural ambiguity often depends on how you're putting the meanings together.",
        "formal_text": "We are primarily concerned here with the following p, : oblem: given a nominal compound, determine the ranking of its possible interpretations from most to least likely. The problem can be formulated in terms of unification. Unification-based formalisms provide an elegant means of describing the information structures used to construct interpretations. Lexical and structural ambiguity resolution, as well as semantic composition, are readily characterized as choices between alternative sequences of unification operations. A key feature of unification--especially important foJ: preference integration--is its neutrality with respect to control, i. c. , there is no inherent bias in the order of unifications, and thus, no bias as to which choices take precedence ovcr others. Although nominal compound interpretation involves lcxical and st t'uctural ambiguity resolution and semantic comp()sition, it is not a good idea to centralize control around any single isolated task, because there is too much interaction. For example, the frame selection problem affects lexical arnbiguity resolution (consider the special case where the frame selected is that signified by the lexical item). Likewise, frame selection and case/role binding are two aspects of the same semantic composition problem, and structural ambiguity resolution depends largely on preferences in semantic composition.",
        "GPT2_formal_text": ": {u1, ..., uj, ..., un}, which are the predicted entities. Formal: We also use an attention mechanism that turns each input token into a kind of \"representative vector.\" Formal: We figure out the attention weights for all the input entities by using something called an attention-based biLSTM with attention (Bahdanau et al., 2015). Formal: The encoder and decoder from the PEMT model are combined to create the final representation of the input entity, which we call e1. We take this representation and use it to train a classifier for the given entity type, which we call e2. After that, we use this classifier to predict the predicted entity type for the next input token, e3, and get a label for the type. Formal: This whole process of classifying entities is shown in Figure 1. Formal: We set the number of features to 100 in this experiment, which means we're dealing with 100 log-probabilities to represent the input entities. Formal: We take the predicted entity type and feed it into a classifier to figure out the chances of each entity type happening. Formal: After the classifier gives us the predicted entity type, we use its probability distribution to guess the predicted entity type for the next input token, e3. After that, we apply the predicted entity type to the current input token, e4, to predict the next entity type for the input. Formal: We repeat this process for all the input entities, e1 to ej. Formal: This process of classifying entities is shown in Figure 1. Formal: Finally, we use a linear layer with ReLU activation (Schumann and Schmidhuber, 1997) to combine the predicted entity type and the predicted entity type to predict the type of the next token, e5. Formal: We repeat this process for all the input entities, e1 to ej. Formal: To see how well the method we've come up with works, we test it on three datasets for identifying pathogens. Formal: We test it on three datasets for identifying pathogens. Formal: In the first dataset, we use the GCN architecture with two GCN layers, as suggested by Sun et al. Formal: In the second dataset, we use the PEMT model with an attention mechanism. Formal: We also run some experiments with different sequence lengths (from 1 to 100) to",
        "directory": "C90",
        "filename": "C90-2071.json"
    },
    {
        "casual_text": "LetsMT! uses Moses as its go-to language-independent SMT solution and plugs it into the LetsMT! online platform as a cloud-based service. The Moses toolkit has been tweaked to work smoothly in a fast-paced environment where training, updating, and interactive access are key. The Moses SMT training process is made up of several steps, each needing a different program to run. Within the LetsMT! setup, this whole process is simplified and can be automatically adjusted based on user inputs, like training corpora, language model data, dictionaries, and tuning sets.",
        "formal_text": "LetsMT! uses Moses as a language independent SMT solution and integrates it as a cloud-based service into the LetsMT! online platform. Moses toolkit has been adapted to fit into the rapid training, updating, and interactive access environment. The Moses SMT training pipeline involves a number of steps that each require a separate program to run. In the framework of LetsMT! this process is streamlined and made automatically configurable given a set of user-specified variables (training corpora, data for language model, dictionaries, tuning sets).",
        "GPT2_formal_text": "ed_pairs. The main thing is to find the attention mask, m, that makes the second vector, σ(u), as big as possible, based on the first vector, u. Formal: You start by using a linear transformation to turn each word into a vector that includes its n-gram vector, w_i, and the attention vector, m_i. Then, you go through the attention mask, m, to get the probability p(u|w_i), where u is the start word and w_i is the end word. Formal: We test out different attention mask combinations, like {masking, masking + position, position + position, position + position, position + word, word + word, word + word, word + word}, and calculate the losses using the validation set. Formal: To see how the model handles different types of attention, we add a simple linear transformation to the input vectors of the model. Formal: In each round of training, the model calculates the loss for all the tokens, using the validation set as a validation set. Formal: To check how well the model is doing, we run the model again, but this time using the validation set as a test set. Formal: Finally, we use the validation set to train the model again, but this time using the test set as a validation set. Formal: Formal: For the cross-lingual model, we use a multilingual BERT model (Devlin et al., 2019) that was trained on 4 languages. Formal: We train the model using cross-lingual BERT and test it on the validation set using multilingual BERT. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2012.eamt-1.32.json"
    },
    {
        "casual_text": "As shown in Figure 1, TTOS has three main parts: a teacher network focused on the knowledge base (T KB) that grabs entities from the KB, another teacher network (T DP) that learns dialogue patterns, and a student network (S) that aims to pull out accurate KB entities and create natural-sounding responses. All three networks have the same basic setup but are trained differently.",
        "formal_text": "As illustrated in Figure 1, TTOS consists of three networks: a KB-oriented teacher network (T KB ) that is specialized for retrieving entities from KB, a dialogue pattern-oriented teacher network (T DP ) that is specialized for learning the dialogue patterns, and a student network (S) that tries to extract accurate KB entities and generate human-like responses. The three networks share the same network structure but different training strategies.",
        "GPT2_formal_text": "= ( word_1, word_2, ..., word_n ) ∈ R d×d_t. \n\nThis approach is kind of like a greedy search, but instead of using the standard greedy search algorithm, it uses a simpler version that doesn’t need a ton of features. Formal: A simple way to estimate the score using the source context is to go through each source token once and then update the score using the same data. Formal: We also make sure the target word and word embedding are in the same subword token during training. This helps cut down on errors caused by words that have multiple meanings or words that don’t really fit together. Formal: To calculate the score for a single token x, we use the combination of the embedding vectors for the source and target words. We do this by averaging their probabilities. Formal: We estimate the score by running the target and source tokens through a fully connected layer. The process keeps going until the score reaches a steady state. Formal: Formal: We’re not diving deep into exactly how to break down the process of calculating the score. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.281.json"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. So, Φ y i, y i+1 is the chance of moving from one label (y i) to the next one (y i+1). Φ is like a grid (a matrix) that’s (k + 2) by (k + 2) big, where k is the total number of different labels we have. We add two extra labels, \"start\" and \"end,\" which are only used in this special layer (called CRF) to mark the beginning and end of a sequence.\n\nNormally, the CRF layer tries to find the most likely sequence of labels. But with this modified IOBES system, a sentence can have more than one correct label sequence, as you can see in Figure 1. So, we’ve tweaked the CRF layer to work a bit differently—we call it a \"fuzzy CRF model.\" Instead of just picking one sequence, it looks at all the possible sequences, including all the IOBES tags and the different entity types, and tries to maximize the total probability of all of them. We’ve written this goal mathematically in Equation 2.",
        "formal_text": "where, Φ y i, y i+1 is the transition probability from a label y i to its next label y i+1. Φ is a (k + 2) × (k + 2) matrix, where k is the number of distinct labels. Two additional labels start and end are used (only used in the CRF layer) to represent the beginning and end of a sequence, respectively. The conventional CRF layer maximizes the probability of the only valid label sequence. However, in the modified IOBES scheme, one sentence may have multiple valid label sequences, as shown in Fig. 1. Therefore, we extend the conventional CRF layer to a fuzzy CRF model. Instead, it maximizes the total probability of all possible label sequences by enumerating both the IOBES tags and all matched entity types. Mathematically, we define the optimization goal as Eq. 2.",
        "GPT2_formal_text": "_tokens = (tokens | [tokens, 0, 1])\n\nFor RNN-based models, here's what we're dealing with:\n\n- **src-tokens**: This is the collection of tokens that go into the summary sentence, and it's all in a fixed format.\n- **dst-tokens**: This is the collection of tokens that go into the summary sentence, but it's in a more flexible format.\n- **attn**: This is the attention weight for the RNN model.\n- **Attn(**|**src-tokens**)**: This tells you the attention weight for the RNN model.\n- **Attn_tokens**: This is the attention weight for the RNN model.\n- **Attn_tokens(**|**dst-tokens**)**: This tells you the attention weight for the RNN model.\n- **Attn_tokens(**|**attn**)**: This tells you the attention weight for the RNN model.\n\nFor example, if we have a source sentence like \"The most popular hotel chain in the United States,\" we'd break it down like this:\n- **src-tokens** = {tokens1, ..., tokens1, ..., tokensn}\n- **dst-tokens** = {tokens1, ..., tokensn1, ..., tokensnn+1}\n- **attn(**|**src-tokens**)** = {attn1, ..., attn2, ..., attnn+1}\n- **Attn_tokens(**|**dst-tokens**)** = {attn1, ..., attn2, ..., attnn+1}\n\nIf we don't know the attention weights, we just take the average of the inputs. Formal: Here, we're working with a dataset called **src-tokens**, which has 300,000 tokens. Formal: Here, we're using a dataset called **dst-tokens**, which has 25,000 tokens. Formal: We're starting with a model called **Attn(**|**src-tokens**)**",
        "directory": "D18",
        "filename": "D18-1230.json"
    },
    {
        "casual_text": "We're suggesting a method called co-training for Chinese word segmentation, which is useful when you don’t have much labeled data (data that’s already been segmented by humans) but you have a lot of unsegmented data in the same domain. Here’s how it works: we divide the features into two groups—character-level features and word-level features. Then, we create two separate segmenters: one that uses character-level features and another that uses word-level features, both trained on the small amount of labeled data we have. These two segmenters then help each other improve by working with the large amount of unsegmented data. Finally, we mix the character-level and word-level features together and use a more advanced segmenter to boost the performance of the co-training process.\n\nOur tests show that when we use just 10% of the data as labeled data and the remaining 90% as unsegmented data, co-training gives us a 20% performance boost compared to supervised training using all the labeled data in the SIGHAN 2005 PKU corpus. In the CU corpus, it gives us a 31% improvement.",
        "formal_text": "We propose the approach of co-training for Chinese word segmentation for the semi-supervised setting where there is only a limited amount of humansegmented data available, but there exists a relatively large amount of in-domain unsegmented data. We split the feature set into character-level features and word-level features, and then build a character-based segmenter with character-level features and a wordbased segmenter with word-level features, using the limited amount of available segmented data. These two segmenters then iteratively educate and improve each other by making use of the large amount of unsegmented data. Finally we combine the wordlevel and character-level features with an advanced segmenter to further improve the co-training performance. Our experiments show that using 10% data as segmented data and the other 90% data as unsegmented data, co-training reaches 20% performance improvement achieved by supervised training with all data in the SIGHAN 2005 PKU corpus and 31% in the CU corpus.",
        "GPT2_formal_text": ", which are basically all the sentences in a document that match the query and are the same length. Formal: We set K to the number of sentences in the target text. Formal: This approach assumes that a specific length will always be the same for both the source and target parts of the document. Formal: The real question isn't whether we can figure out the query, but when we can actually find the sentence that fits the query. Formal: To tackle this, we can look at the part of the query that doesn't match any sentence in the document. Formal: Since we already know where the query sentence is in the text, we can assume that the sentences in the document are grouped together by the query's length, which is just the total number of sentences in the whole document. Formal: For the query and the length of the source text, we can figure out the scores for all the source sentences using a fancy math thing called cosine similarity (shoutout to Mann and Thompson, 1995). Formal: For the length of the target text, we calculate the scores by using the length of the target text as a guide, since the query sentence might be longer than the source. Formal: Using these scores to rank the source sentences, we can get the best results for all the sentences. Formal: We tested this using a real-world translation system, and we found that our method works really well. Formal: The results from this system were used to train and test a newer version of BERT. Formal: The new BERT model is bigger, better, and faster, which made it better at handling multiple tasks. Formal: We also included some results from this model on the WMT 2019 test set. Formal: We also built a version of BERT called BERT-base for cross-lingual event extraction. Formal: BERT-base is a big, pre-trained model that was trained on a bunch of languages. Formal: For more info on the original BERT and its fancy training process, you can check out the paper by Devlin et al. Formal: Formal: Table 1 shows some test scores from our system with different query lengths. Formal: We tested how well BERT could do on three types of event extraction: cross-domain event extraction, event coreference, and event relation extraction. Formal: Formal: To see how BERT's size affects performance, we measured its performance on both cross",
        "directory": "D13",
        "filename": "D13-1119.json"
    },
    {
        "casual_text": "Alright, so when you round down, you get the index of the last singular value that's considered important. This number is basically seen as the effective number of dimensions, or the rank, of the matrix X. Now, if d is the number of dimensions in the space where X lives, and we think that the number of word vectors in X is usually way bigger than d, then this idea makes sense (and you can check out Roy and Vetterli's work from 2007 for more details).",
        "formal_text": ", rounded to the smaller integer, yields the index of the last singular value that is considered significant, and is interpreted as the effective dimensionality, or rank, of the matrix X. If d is the dimensionality of the embedding space X, and we assume that the number of word vectors in X is typically much larger than d, it then holds that (see Roy and Vetterli (2007)",
        "GPT2_formal_text": "_vec *, g *, and h *. These are like the usual attention vectors, but they're replaced by the output representations from the attention matrix. Then, we just add these outputs together. After that, we get the final representation for the whole image, which we call h_i. Formal: We just keep repeating this process as many times as needed. Formal: Basically, it's like finding the best possible output based on the input. The final result is the best of the options you can get from the attention. Formal: A lot of people have tried to build attention-based models for different NLP tasks, but no one has really figured out a good way to find the best way to arrange these attention vectors. Formal: Formal: So, we came up with a new way to do this: let's call the function g that generates these attention vectors h_0, h_1, ..., h_N. Formal: The function g(a_i, a_j) is calculated by combining the best possible output for the given image and text, like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.186.json"
    },
    {
        "casual_text": "Automatic metrics usually don’t do a great job of understanding the meaning of sentences or how they’re structured. They also struggle when there’s only one correct answer to compare against. In fact, these metrics don’t match up with how humans judge things in tasks like fixing grammar mistakes (Napoles et al., 2015) or generating dialogue (Liu et al., 2016). Since we’re working on a new task and haven’t really tested how these metrics perform in this context, we think it’s important to get human feedback and see if these metrics line up with what people think is good.",
        "formal_text": "Automatic metrics often fail to incorporate semantic meaning and sentence structure in evaluation as well as accurately capture performance when there is only one gold-standard reference; indeed, these metrics do not align with human judgment in other generation tasks like grammatical error correction (Napoles et al. , 2015) and dialogue generation (Liu et al. , 2016). Since automatic metrics have not yet been explored in the context of the new task we are proposing, we find it necessary to conduct human evaluation and study whether these metrics are consistent with human judgment.",
        "GPT2_formal_text": ", which are the user's personal preferences. Formal: To make the model more robust, we use a residual connection to handle the latent distribution, which helps us reduce the variation in the predictions. We do this by tweaking the latent codes to make sure the model can still make accurate predictions even when there are more than two possible answers. Formal: We turn the user's latent knowledge into a set of numbers, let's call them {q_t}, to represent the user's knowledge. The parameters in this set, like the maximum and average values, are based on the user's preferences. Formal: The model generates responses in a specific way and then checks how well it does by looking at the accuracy and how long it takes to process the responses. Formal: We use two types of loss functions to measure how well the model is doing. Formal: For each question, we have a bunch of input texts {y_t} and a set of output texts {x_t}, where the text x_t comes from a different version of the question q. We treat the question and the original texts as the model's input and output, respectively. Formal: To check how accurate the model is, we look at the following: Formal: We use a forward RNN for the response generation and then a backward RNN for the question generation. Formal: Here's how we train the forward RNN: Formal: We use three losses: Formal: For each training round, we pick a batch size and run it for N rounds, then calculate the mean squared error (MSE) of the generated responses. Formal: Each training round has N rounds of training, and the MSE is calculated for each round using the Mean Squared Error (MSE) formula. Formal: We average the results from each round for the whole model to get the final loss. Formal: Finally, we pick the best question and the model's output using a weighted average. Formal: Formal: For training the backward RNN, we use two losses. Formal: The loss L_t is calculated for each round. Formal: For training the forward RNN, we also use the loss L_t. Formal: For each training round, we select a batch size and run it for N rounds, then calculate the MSE for each round using the Mean Squared Error (MSE) formula. Formal: We average the results from each round for the whole model",
        "directory": "acl",
        "filename": "2020.acl-main.168.json"
    },
    {
        "casual_text": "The main thing we used for this experiment was the MULTEXT-EAST multilingual parallel and comparative corpus, which Dimitrova and others talked about in 1998. We took some random sentences from the corpus to train with, and the rest were for testing. The same sentences were picked for every language we worked with.",
        "formal_text": "The most important resource used in this experiment was the multilingual parallel and comparative corpus MULTEXT-EAST (Dimitrova et al. , 1998). Part of the corpus (randomly selected sentences) was used for training, the rest was used for testing purposes. The same sentences were selected for each language.",
        "GPT2_formal_text": "ed_to_1_in_modes, which looks at how the model handles words in different contexts. Formal: But, if you look at it another way, the final attention weight might be the same for both the source and target words. This issue can be dealt with by using a softmax function to figure out the final attention value. Formal: Here, w_s represents the context word embedding. P_c(w_s) and P_s(w_s) are the normalized probabilities for each word in the source and target vocabulary, respectively. Formal: We check the attention weights for each word in both the source and target words, and the outputs of the attention is what we want to predict. This is all laid out in Equation 7. Formal: We calculate the cross-entropy loss for both the source and target sides using the same method we used for the source side. Formal: We also calculate the cross-entropy loss for both the source and target sides using the same approach for the source side. Formal: We add the cross-entropy loss for the source side to the cross-entropy loss for the target side. This gives us: Formal: We also add the cross-entropy loss for the target side to the cross-entropy loss for the source side. This gives us: Formal: Finally, we adjust the weights of the attention vectors to match the weights of the source and target words. Formal: Formal: The cross-entropy loss function is a simple way to figure out the final attention weight for the source and target words. Formal: Formal: The cross-entropy loss function is a simple way to figure out the final attention weight for the source and target words. Formal: Formal: The cross-entropy loss function is a simple way to figure out the final attention weight for the source and target words. Formal: Formal: Formal: The cross-entropy loss function is a simple way to figure out the final attention weight for the source and target words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2010.eamt-1.24.json"
    },
    {
        "casual_text": "In this part, we’ll start by talking about the scenario-based dialogue model. Next, we’ll explain the imitation learning framework you can see in Figure 2. And lastly, we’ll go over the training objective.",
        "formal_text": "In this section, we first introduce the scenariobased dialogue model, then describe the imitation learning framework shown in Figure 2, and finally present the training objective.",
        "GPT2_formal_text": "= [i1, ..., j] include_ids = [i1, ..., j] with_ids = [i1, ..., j] if not include_ids: return False return True Formal: In this project, we’re introducing a new way to handle multi-label text classification by focusing on the relationship between two inputs. We’ve added a new part called relation_embedding to handle the structure of multi-label text data. This new approach can help create better representations for text by looking at how words connect in a graph. The main idea behind this is to use two main parts that help the model understand the relationship between the input and output. These parts include the input embedding and the output embedding. \n\nOur experiments show that our approach works really well, outperforming other approaches on the TAC-2009 dataset. Plus, it’s super efficient and doesn’t need a lot of computing power. Formal: To make use of the multi-label nature of text, we’ve come up with an approach called hierarchical multi-label text classification, which we’ll explain in more detail below. Formal: We’ve also developed a new relation embedding model that can create detailed representations for sentences that have more than two labels. We tested it on the TAC-2009 dataset and found it works really well. It can accurately learn the relationship between two inputs and their relationships. Formal: Lastly, we’re using the model to help train a classifier for multi-label text classification. Formal: To test how well our model works, we’ve put it up against other models on the TAC-2009 dataset. Formal: We’ve also proposed a new hierarchical multi-label text classifier. This new model can not only learn the relationship between two inputs but also create detailed representations for sentences with more than two labels. Formal: Our experiments show that our method is really effective. We also tested it on the TAC-2009 dataset. Formal: We’ve also developed a new relation embedding model that can generate detailed representations for sentences with more than two labels. We tested it on the TAC-2009 dataset and found it works really well. Formal: Lastly, we’ve developed a new relation embedding model that can create detailed representations for sentences with more than two labels. Formal: Formal: We’ve also developed a",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.534.json"
    },
    {
        "casual_text": "So, let's break this down in a simpler way. Each part of the text is talking about different models and how they perform on tasks like summarizing articles from CNN or DailyMail. Here's the info in a more casual tone:\n\n- **BART (Lewis et al., 2020)**: This model has an encoder-decoder setup with 12 layers each and a size of 1024. On the CNN/DailyMail task, it takes 2.4 seconds to process and gives a speedup of 7.7x compared to a baseline.\n\n- **DistilBART (Wolf et al.)**: This one is similar to BART but smaller—12 layers for the encoder and 6 for the decoder, still 1024 in size. It's a bit faster at 3.4 seconds and offers a speedup of 5.4x.\n\n- **ProphetNet (Qi et al., 2020)**: Another encoder-decoder model with 12 layers each and 1024 size. It takes 2.8 seconds and gives a speedup of 3.8x.\n\n- **T5 (Raffel et al., 2020)**: This model has an interesting setup—12 layers for the decoder and none for the encoder, with a size of 768. It takes 3.0 seconds and offers a speedup of 5.5x.\n\n- **UniLM (Dong et al., 2019)**: This one is an encoder-only model with 12 layers and a size of 768. It's the fastest at 1.7 seconds and gives a huge speedup of 9.6x.\n\nSo, basically, these models are all trying to summarize articles quickly, and each has its own way of doing it, with different speeds and efficiencies.",
        "formal_text": "Since each token needs to be read multiple times Model Architecture Task Baseline FastSeq Speedup encoder-decoder architecture BART (Lewis et al. , 2020) 12L-12L-1024 CNN/DailyMail 2. 4 18. 4 7. 7x DistilBART (Wolf et al. ) 12L-6L-1024 CNN/DailyMail 3. 4 18. 5 5. 4x ProphetNet (Qi et al. , 2020) 12L-12L-1024 CNN/DailyMail 2. 8 10. 7 3. 8x T5 (Raffel et al. , 2020) 12L (Radford et al. , 2019) 0L-12L-768 CNN/DailyMail 3. 0 16. 7 5. 5x encoder-only architecture UniLM (Dong et al. , 2019) 12L-0L-768 CNN/DailyMail 1. 7 16. 4 9. 6x",
        "GPT2_formal_text": "ed (B), which is basically the classifier's guess as to how important the attribute (x_attr) is in the text. Formal: We're using a Tensorflow model that was trained on a dataset called Constituency Treebank (CTB). This dataset has a total of 5,317,654 tokens, which is a lot of data to train a classifier. We're using a 40-dimensional character embedding for the training data, and we've set the dropout probability to 0.3. For the validation set, we randomly pick 15% of the tokens to be unlabeled. Formal: To test how well our model works, we do an ablation study, which is basically a random test. We take the predicted attribute value from the classifier and use it as the prompt to train a classifier. We also make sure the classifier's predictions match the actual attribute value by removing the predicted attribute value and a random span. Formal: For the dev set, we randomly pick 40% of the tokens from the training data to be unlabeled. Formal: We use the SVM classifier (Stolcke, 2002) with the default settings from the author. Formal: We train our model for 30 epochs and test it on the development set. Formal: The results are in Table 4, showing accuracy and macro-F1 score on test set D_dev. Formal: To check how well the model is performing, we train it for 10 epochs and test it on the dev set. Formal: We calculate macro-F1 score for 10 epochs and test it on the dev set. Formal: For both dev and test sets, we calculate the micro-F1 score for 10 epochs and test it on the dev set. Formal: The results are in Table 4, showing accuracy and macro-F1 score for 10 epochs and test set D_dev. Formal: We train our model for 5 epochs and test it on the dev set. Formal: The results are in Table 4, showing accuracy and macro-F1 score for 10 epochs and test set D_dev. Formal: Formal: Table 5 shows the results for the dev and test sets, with accuracy and macro-F1 scores for 10 epochs and test set D_dev. Formal: Formal: Table 5: Results for the dev and test sets, with accuracy and macro-F1",
        "directory": "acl",
        "filename": "2021.acl-demo.26.json"
    },
    {
        "casual_text": "Manish Shrivastava and his team came up with a system in 2008 that uses something called a Hidden Markov Model for Hindi. They used a stemmer as a kind of preprocessor to figure out the root of words. This system was built with 18 different parts of speech tags and managed to get an accuracy of 93.12%. \n\nThen, in 2011, Sanjeev Kumar Sharma and his group created a system using the same Hidden Markov Model to make the Punjabi Part of Speech tagger more accurate. They made a module that takes the output from an existing POS tagger and assigns the right tag to words that could have more than one tag. They tested it on a big chunk of text with 26,479 words and got an accuracy of 90.11%.",
        "formal_text": "Manish Shrivastava et al. , 2008 developed a system using Hidden Markov Model for Hindi. The System uses stemmer as a preprocessor to find the root of the words. The system was developed using 18 different pos tags and system achieved the accuracy of 93. 12%. Sanjeev Kumar Sharma et al. , 2011 developed a system using Hidden Markov Model to improve the accuracy of Punjabi Part of Speech tagger. A module has been developed that takes output of the existing POS tagger as input and assign the correct tag to the words having more than one tag. The system was evaluated over a corpus of 26, 479 words and system achieved the accuracy of 90. 11%.",
        "GPT2_formal_text": "= [y1, y2, ..., yn] label = y1:n label = y2:n label = yn:n from the same set of training examples. Here, y_i stands for the i-th mention, and α_i is a weight matrix that gives a general idea of how much a mention is connected to a specific mention. \n\nIn this setup, the mention and the mention's related mention are treated as instances in the same mention vector. The embedding for a mention and its related mention (called e_m_i) is calculated by multiplying e_m_i by a matrix M_m, which includes the mentions in the document. This M_m is a grid of d_m rows with m rows for the mentions and m columns for the related mentions. Formal: In the setup we're talking about, the embedding for a mention y_i is calculated by multiplying e_m_i by a matrix M_m. Formal: The embedding for a mention y_i is calculated using a matrix M_m. Formal: The embedding for a mention y_i is calculated using a matrix M_m. Formal: The embedding for a mention y_i is calculated using a matrix M_m. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C12",
        "filename": "C12-3021.json"
    },
    {
        "casual_text": "We found that names from minority groups show up way less often than names from majority groups in the training data for neural language models. Because of this, these names are less likely to be treated as single tokens, get less context, and end up with overfit representations in the models.",
        "formal_text": "We show that names predominantly belonging to members of minority social groups occur less frequently than majority group names in training corpora of neural language models, and that low frequency results in lower rates of single tokenization, less contextualization, and overfit representations in neural language models.",
        "GPT2_formal_text": "ed(x) = x) src = [src + [masked_to_1(x) + 1] for i from 1 to N]\n\nSo, this is basically the same as the Frobenius norm of the dataset, but with some tweaks. Formal: We're using two different versions of the same dataset: one called \"Masked to 1\" and the other called \"Masked to 0\". The \"Masked to 1\" version is kind of like the regular one, but it also includes the masked tokens. Formal: The \"Masked to 0\" dataset is just the regular one without any masks. Formal: Here, we're focusing on the task of figuring out if a masked token is active or not. Formal: We use some math to figure out the probability of a token being masked, p(t|s). Formal: p(t|s) is the probability of a token being masked. Formal: Finally, we add up the probabilities of all the tokens in the dataset. Formal: After that, we get the probability of the whole dataset being masked. Formal: Formal: To get the probability of a token, we start by picking the highest probability p(t|s) from all the tokens. Then, we calculate the probability of each token separately. Formal: We calculate the probability of a token using the following formula: Formal: Formal: Formal: Formal: Finally, we add up the probabilities of all the tokens in the dataset. Formal: After that, we calculate the probability of the whole dataset being masked. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.41.json"
    },
    {
        "casual_text": "Like earlier studies found, and like what we saw too, getting data for this kind of thing is pretty tough, especially when you’re trying to label whole speeches on a big scale. It’s a lot of work. To make it easier for people to study this, we changed the task a bit and set it up as a debate. We also made a special dataset for it. We gathered and are sharing over 3,600 debate speeches that are all labeled for this task.",
        "formal_text": "As previous studies have shown, and consistent with our own findings, obtaining data for such a task is difficult, especially considering that labeling at scale of full speeches is an arduous effort. To facilitate research of this problem, we recast the proposed general task in a defined debate setup and construct a corresponding benchmark data. We collected, and release as part of this work, more than 3, 600 debate speeches annotated for the proposed task.",
        "GPT2_formal_text": "). Since there are no mappings for every pair of entities, we use a Support Vector Machine (SVM) to figure out the joint distribution. This SVM uses a few features, like the Euclidean distance between the query and the possible entity pairs, the cosine similarity between the query and the possible entity pairs, and the cosine similarity between the answer and the possible entity pairs. The chance of a query slot being correct and a possible entity pair being correct is calculated like this: Formal: The probability of a query slot being correct and a possible entity pair being correct is calculated like this: Formal: The probability of a query slot being correct and a possible entity pair being correct is calculated like this: Formal: Formal: Here, e_i represents the i-th query slot, w_i is the i-th word in the query, and σ_i is a term that shows how important the query and entity pair are. Formal: We define the interaction between two entities as the probability of them interacting. Formal: Basically, the interaction between two entities is the probability of them interacting. Formal: Formal: This interaction probability is calculated like this: Formal: This interaction probability is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "The translation you got for that word is basically right, but it doesn't fit the situation you're using it in (it's the wrong meaning of the word).",
        "formal_text": "The obtained translation for the given word is in principle correct, but not in the given context (word sense error).",
        "GPT2_formal_text": ". We set the trainable state vector h_t to h(0), which means we get a single input x. Formal: In Equations (1) and (2), c(t) is just a constant, and this keeps the hidden state vectors from being too far apart. But, if we keep the hidden state vectors in a straight line and don’t change them much during the process, we get a more stable way of updating them. Formal: For a specific time t, the (i, j)th hidden state vector h_t for a specific word x_i is calculated as: Formal: The final hidden state vector h_t for a specific word x_i. Formal: The final hidden state vector h_t for a specific word x_i. Formal: The hidden state vector h_t for a specific word x_i. Formal: The hidden state vector h_t for a specific word x_i. Formal: The hidden state vector h_t for a specific word x_i. Formal: The hidden state vector h_t for a specific word x_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "conll",
        "filename": "2020.conll-1.19.json"
    },
    {
        "casual_text": "We picked the dataset from Tzioumis (2018) because it covers a lot of ground—about 85.6% of the U.S. population, according to Tzioumis. It’s also anonymized in an ethical way, has at least 30 observations for 91.2% of the names included, and uses first names based on self-identification, which Larson (2017) called the \"gold standard\" for figuring out demographics, especially when it comes to gender labeling. The dataset uses the same racial categories as the U.S. census for surnames, so it fits well with our cultural context. \n\nOne thing to note is that some names can belong to more than one racial or gender group, which could add some noise to our analysis. But, over 80% of the names we looked at have a self-identification rate of at least 70% with a single racial group in Tzioumis’ dataset. \n\nFrom the Social Security Administration (SSA) data, we found that 31.3% of the names in our study are only used by males, 38.6% are only used by females, and 30.1% are used by both. For names that appear in both genders, 88% have at least 70% of their occurrences tied to one gender. So, labeling a name based on the group it’s most common in should help us see which groups are most affected by our findings and also highlight the linguistic clues about race and gender that make first names a good way to study this stuff.",
        "formal_text": "We choose the dataset of Tzioumis (2018) because it has wide coverage (85. 6% of the U. S. population as estimated by Tzioumis); is ethically anonymized; is based on at least 30 observations for 91. 2% of included names; and contains first name data based on self-identification, considered a \"gold standard\" for demographic classification, as noted by Larson (2017) in the context of labeling data by gender. Tzioumis' dataset uses the same racial categories as U. S. census data for surnames, indicating that these categories correspond to our cultural context. That many names occur in more than one racial group or gender group is also a limitation, and may introduce noise into our analyses. However, more than 80% of names examined in our work have a self-identification rate of at least 70% with a single racial group in the Tzioumis dataset. Based on SSA data, 31. 3% of the names in our study have only male occurrences, 38. 6% of the names in our study have only female occurrences, and 30. 1% of names occur for both male and female individuals. 88. 0% of names with male and female occurrences have at least 70% of occurrences associated with one gender. Thus, assigning a label based on the group with the most occurrences is likely to capture both which groups are most affected by our findings, as well as the linguistic signals related to race and gender which make first names a useful proxy to this information.",
        "GPT2_formal_text": ", e1 = [], e2 = [], e3 = [], e4 = [], e5 = [], e6 = [], e7 = [], e8 = [], e9 = [], e10 = [], e11 = [], e12 = [], e13 = [], e14 = [], e15 = [], e16 = [], e17 = [], e18 = [], e19 = [], e20 = [], e21 = [], e22 = [], e23 = [], e24 = [], e25 = [], e26 = [], e27 = [], e28 = [], e29 = [], e30 = [], e31 = [], e32 = [], e33 = [], e34 = [], e35 = [], e36 = [], e37 = [], e38 = [], e39 = [], e40 = [], e41 = [], e42 = [], e43 = [], e44 = [], e45 = [], e46 = [], e47 = [], e48 = [], e49 = [], e50 = [], e51 = [], e52 = [], e53 = [], e54 = [], e55 = [], e56 = [], e57 = [], e58 = [], e59 = [], e60 = [], e61 = [], e62 = [], e63 = [], e64 = [], e65 = [], e66 = [], e67 = [], e68 = [], e69 = [], e70 = [], e71 = [], e72 = [], e73 = [], e74 = [], e75 = [], e76 = [], e77 = [], e78 = [], e79 = [], e80 = [], e81 = [], e82 = [], e83 = [], e84 = [], e85 = [], e86 = [], e87 = [], e88 = [], e89 = [], e90 = [], e91 = [], e92 = [], e93 = [], e94 = [], e95 = [], e96 = [], e97 = [], e98 = [], e99 = [], e100 = [], e101 = [], e102 = [], e",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.41.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way.\n\nBasically, Φ(d) is like a big table or matrix that shows how important different words are in a document (d). Each row in the table represents a different word, and each column represents a different document. The numbers in the table are calculated using something called \"tf-idf,\" which stands for \"term frequency-inverse document frequency.\" This helps figure out how important a word is in a specific document compared to all the other documents.\n\nSo, Φ(d) looks something like this:\n\n```\n[ tf-idf(b1) ]\n[    ...     ]\n[ tf-idf(bN) ]\n```\n\nAnd then, we take this table and multiply it by the tf-idf values for the specific document (d) we're looking at. This multiplication helps us focus on the words that are most important in that particular document.\n\nIn short, Φ(d) = X^T • tf-idf(d), where X^T is the table we talked about, and tf-idf(d) is the importance of words in document d.",
        "formal_text": "Φ(d) =     − −− → tf-idf(b1) T. . . − −− → tf-idf(bN ) T     − −− → tf-idf(d) = X T • − −− → tf-idf(d)",
        "GPT2_formal_text": "ed(x) is the hidden state representation for the x-th token, and h(w) is the hidden state representation for the w-th word. The loss function l is calculated by subtracting the attention vector representation from the original word embedding representation and then adding up the attention weights. Formal: We check how well our model performs using the teacher forcing accuracy. Formal: The model's performance is measured using the combined accuracy of the teacher forcing and the individual forcing scores for the token labels. Formal: For the experiment, we set the number of tokens to 100 and the attention size to 5. Formal: We set the cross-entropy loss to 0.01. Formal: To help the model learn better, we give it different rewards. Formal: For a reward, we look at the average attention weights for the token labels. For instance, if the token label is a [MASK], we would expect the token embedding representation to have a mean of 0 and a variance of 1. The reward for this token label is calculated like this: Formal: Formal: We also have an option to use a simple linear function to calculate the attention weights, so the parameters are shown as a scalar, like this: Formal: The loss function is calculated by subtracting the attention weights from the original embedding representation. Formal: The whole setup of the model is laid out in Figure 3. Formal: Our model does better than some other models when it comes to both multi-task and single-task performance, as shown in Figure 4. Formal: The model's performance is measured using the combined accuracy of the teacher forcing and the individual forcing scores for the token labels. Formal: Formal: The cross-entropy loss is 0.01. Formal: The reward for the token label is calculated like this: Formal: Formal: For the experiment, we set the number of tokens to 100 and the attention size to 5. Formal: Formal: We set the cross-entropy loss to 0.01. Formal: Formal: For the model's performance, we check the average attention weights for the token labels. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D13",
        "filename": "D13-1179.json"
    },
    {
        "casual_text": "Bleu seems to struggle with telling the difference between random translation variations, which makes you wonder if it really matches how humans judge translation quality in some situations. The more identical scores you get from different versions, the less likely it is that all of them would feel equally good to a human. This is more of a theoretical issue, since the examples are made up, but it does show that Bleu isn't exactly a super accurate way to measure translation quality. There are a few key reasons why Bleu isn't that great:",
        "formal_text": "Bleu's inability to distinguish between randomly generated variations in translation hints that it may not correlate with human judgments of translation quality in some cases. As the number of identically scored variants goes up, the likelihood that they would all be judged equally plausible goes down. This is a theoretical point, and while the variants are artificially constructed, it does highlight the fact that Bleu is quite a crude measurement of translation quality. A number of prominent factors contribute to Bleu's crudeness:",
        "GPT2_formal_text": "= {}, form_ids = {}, body_ids = {}, query_ids = {}, span_ids = {}, v_num = {}, v_count = {}, v_shape = {}, v_tgt = {}, v_for = {}, v_num = {}, v_count = {}, v_shape = {}, v_tgt = {}, v_for = {}, age_range = {}, form_type = {}, query_type = {}, span_type = {}, v_num = {}, v_count = {}, v_shape = {}, v_tgt = {}, v_for = {}, summary = {}, title = {}, body = {}, timestamp = {}, subreddit = {}, post = {}, timestamp = {}, body = {}, count = {}, v_shape = {}, v_tgt = {}, v_for = {}, arg = {}, link_type = {}, body = {}, count = {}, form_type = {}, span_type = {}, v_num = {}, v_count = {}, v_shape = {}, v_tgt = {}, v_for = {}, offset = {}, subreddit = {}, post = {}, subreddit = {}, post = {}, date = {}, body = {}, arg = {}, link_type = {}, body = {}, count = {}, v_shape = {}, v_tgt = {}, v_for = {}, title = {}, body = {}, offset = {},\n\nTraining with Finite State Transducers (FSTs) is way more efficient. Specifically, the evaluation metric we use is the FST's parameterized log-likelihood (PLP), which you can find in Algorithm 1. The goal is to minimize the function LM-W(1/2)t, where t is the transition number from the input sequence to the final output. This function is optimized using the FST model. Formal: In this part, we'll explain how we train the model with finite-state morphology and how to use that as the evaluation metric. Formal: As we train the FST model, we check the PLP log-likelihood for each symbol. Formal: The model learns from data by processing it, processing the input, and processing the output. Formal: We train the FST",
        "directory": "E06",
        "filename": "E06-1032.json"
    },
    {
        "casual_text": "After a product has been around for a while, companies often come up with different versions of it to keep their spot in the market. They do this to attract customers who have specific needs. A common example is \"light\" versions of food brands. Usually, these new versions have names that are similar to the original brand but with a little extra word to show what kind of variation it is, like \"mini babybel\" or \"philadelphia light.\" We went through and listed out 11 common prefixes or suffixes and checked how often they show up with different food items.",
        "formal_text": "Once a product has established itself on the market for a substantial amount of time, many companies introduce variants of their brand to further consolidate their market position. The purpose of this diversification is to appeal to customers with special needs. A typical variant of food brands are light products. In many cases, the names of variants consist of the name of the original brand with some prefix or suffix indicating the particular type of variant (e. g. mini babybel or philadelphia light). We manually compiled 11 affixes and check for each food item how often it is accompanied by one of them.",
        "GPT2_formal_text": "ed, and attention_hidden are just the inputs for those layers, which are basically the key-values that go with the input. Formal: To make sure we can actually measure how well each model performs, we’re using Krippendorff's alpha, which is like a confidence level for how well the model’s predictions match up with what humans think. This level is usually set at 0.7 for most machine learning tasks. Formal: Here’s how we’re doing it: \n\nWe’ve got a big dataset called CLUES-Real, which has around 20,000 questions from real people. For each question, we grab the top 10 results from CLUES-Real and then calculate the average score for each top-10 result. Formal: Basically, we’re averaging the scores of the top-10 results to get our final score. Formal: We also use a neural network called Conv-GRNN to get the prediction for each question. Formal: The results are broken down into individual tokens, which we then use to calculate the attention weights for the question’s words. Formal: The last part of the process is to figure out the probability for how likely the question is to be correct based on the top-10 results. Formal: We’re keeping the original image and the question as the same size, so we can calculate the probability for the correct answer using the question’s words. Formal: This gives us the answer. Formal: Formal: Finally, we use the predictions from the model to figure out the hidden variables. Formal: Formal: The results we get for each question are turned into probabilities to train the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C14",
        "filename": "C14-1216.json"
    },
    {
        "casual_text": "Figure 3 shows how our systems perform at different points, and Table 3 gives the results for our best setups on the ENNI development and test sets. Interestingly, it turns out that neither the feature set we use nor whether mazes are included really impacts the system's performance. This is quite different from Microsoft Word's grammar checker, which doesn't work well when mazes are part of the data. The system by Morley et al. (2013) can handle mazes, but it still doesn't perform as well as the one we're proposing.",
        "formal_text": "The plots in Figure 3 show how the performances of our systems at different operating points vary, while Table 3 shows the performance of our best system configurations on the ENNI development and test sets. Surprisingly, we see that neither the choice of feature set, nor the presence of mazes has much of an effect on system performance. This is in strong contrast to Microsoft Word's grammar check, which is minimally effective when mazes are included in the data. The Morley et al. (2013) system is robust to mazes, but still performs substantially worse than our proposed system.",
        "GPT2_formal_text": ", then this is what the model spits out: Formal: To figure out the document embedding, we pick the embedding from the last layer of the model that has the highest cosine similarity with the ground truth document embedding. Formal: Also, we keep track of how often each feature shows up in the training data. Formal: We do the same thing for the word embeddings, but instead of using the last hidden state, we use the last word. This means the model has to figure out the words in the text by reading the whole document. Formal: Basically, the language model for a document has to predict the embeddings for all the words in the document, which it does by trying to predict the last hidden state of the document's last hidden state. Formal: The language model for a document has to predict the embeddings for all the words in the document, which it does by trying to predict the last hidden state of the document's last hidden state. Formal: We take all the feature vectors from the embedding layer and the last hidden state of the last hidden state, then use a linear transformation to get the embeddings for the words in the document. Formal: Lastly, we calculate the cosine similarity between the embeddings of the document and the ground truth document. Formal: This is done by using the last hidden state of the last hidden state. Formal: The language model for a document has to predict the embeddings for all the words in the document, which it does by trying to predict the last hidden state of the document's last hidden state. Formal: We do the same for the word embeddings, but instead of using the last hidden state, we use the last word. This means the model has to predict the words in the text by reading the whole document. Formal: To get the embeddings for the words in the document, we use the last hidden state of the last hidden state. Formal: We calculate the cosine similarity between the embeddings of the document and the ground truth document, using the last hidden state of the last hidden state. Formal: Finally, we get the embeddings for the words in the document, using the last hidden state of the last hidden state. Formal: Formal: Formal: This is done by using the last hidden state of the last hidden state. Formal: Formal: Formal: Formal: Formal:",
        "directory": "D14",
        "filename": "D14-1106.json"
    },
    {
        "casual_text": "A QA sample has a question, a passage, and an answer span. Sometimes, there might be more than one part of the passage that matches the answer. To keep things simple, most baseline codes just use the first match they find for training. But, when you think about the context and the meaning of the question and answer, some parts of the passage are more likely to be the right answer than others.\n\nTo find the best possible answer span, we use something called a universal sentence encoder (Cer et al., 2018) to turn the question and the sentences in the passage into fixed-size vectors. We then pick the part of the passage that’s most similar to the question based on cosine similarity and call it the golden span.\n\nIn our tests, this method helped improve performance on some datasets, but on others, it actually made things worse.",
        "formal_text": "QA sample consists of a question, a passage, and an answer span. There could exist multiple answer spans because more than one phrase in the passage can be matched with the answer text. For simplicity, only the first occurrence of answer text is used for training in most of the baseline codes. However, considering context and semantic of the given question and answer, a certain phrase in the passage is more likely to be plausible answer span relevant to the question. In order to find the most plausible answer span, a question and sentences in the passage are encoded into fixed-size vectors with universal sentence encoder (Cer et al. , 2018). We choose the span in a sentence, which is the most similar to the question in terms of cosine similarity, as golden span. In our experiment, this approach boosts up the performance of some datasets but degrades the performance a lot in the other datasets.",
        "GPT2_formal_text": "_t and the activation function Θ for the generated text. Formal: So, we can check if the generation is good or not by comparing how close the generated text is to the reference. Formal: The generator θ G gives us the distribution of the tokens and how often they appear. Formal: The generator θ G takes the token distribution from the input sequence x and maps it into the latent space of the target language. The generated text x gets updated with the probability P θ (y|x) = 1/τ θ G (y), which gives us the token distribution P y = x i |y i. The updated distribution P θ G (y) is calculated by multiplying P θ (x i |y i ) by the negative log-probability (N p θ (y) − N p θ (x i ))) Formal: Also, we need to add the vector representations, y and x, and their corresponding probabilities, γ and λ. Formal: The final update to the text x is done by adding the original distribution P x to the updated distribution P θ G (y). Formal: We'll use these updated representations P x = x i |y i to represent the representation of the text y i. Formal: Formal: Formal: To make sure we're comparing the generated text to the reference properly, we'll use the GAT-based encoder-decoder model (from Sutskever et al., 2014) to create a special representation for the target language. Formal: You can check out the setup for GAT in Figure 1. Formal: To create the representation of the input text x, we use a BiLSTM (Hochreiter and Schmidhuber, 1997) with hidden dimensions of 100 and a hidden layer size of 5. Formal: We'll apply the GAT-based encoder-decoder model to this text x. Formal: The result from the GAT model is a vector representation, y = x i |y i, that we feed into a multi-layer perceptron (MLP) to predict the probability of the token y i. Formal: Formal: The final output from the MLP is the token probability distribution, P y = x i |y i, that we use to calculate the token distribution P y. Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-5826.json"
    },
    {
        "casual_text": "Most automatic summarization methods these days are extractive, meaning they just pull out sentences from the original text based on things like word patterns or sentence structure. They rank or score the sentences and then grab the top ones, with not much done to tweak them afterward (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). But this approach has its limits compared to abstraction, which can create new sentences that capture the main ideas better (Carenini and Cheung, 2008).",
        "formal_text": "Most automatic summarization approaches are extractive which leverage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al. , 2007; Wan et al. , 2007; Wang et al. , 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008).",
        "GPT2_formal_text": "= {(I, j), ..., (n, t)} and the sentence-level attention weights between I and j are labeled as I_I. The final attention representation for the input sentence is then calculated as: Formal: We combine the final representation from the two layers of the encoder and the decoder. Formal: For instance, if we have an input sentence s = {s1, ..., sT}, and we have a matching input representation v, it would look like this: Formal: Here, {s1, ..., sT} refers to the input sentences, and {v, I} is the attention weights for those input sentences. Formal: The final attention vector is then calculated as: Formal: Basically, the hidden layer's representation for the input sentence is written as the dot product of the final attention vector and the sequence representation vector. Formal: We also use a linear layer to handle the multi-head attention. Formal: We also make sure that the input representations for the matching input vectors are similar by using a uniform distribution for the input representations. Formal: We calculate the cross-entropy loss for the input sentence representation as: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D15",
        "filename": "D15-1219.json"
    },
    {
        "casual_text": "We've updated the neural L-PCFG model to include the idea of concreteness by adding a couple of concrete-related features. Figure 3b shows the source (captions) and target (semantic role labels) for the alignment method we talked about in Figure 3a. Words in parentheses are the stop words we took out from the original caption. We used SpaCy's default list of stop words for English (Honnibal et al., 2020). The labels are arranged with the first term being the predicted activity, followed by the entities involved in that activity (Yatskar et al., 2016). The alignment pairs are listed from the highest score to the lowest.",
        "formal_text": "We extend the neural L-PCFG model to incorporate the concept of concreteness by adding two concrete- Figure 3b shows the source (captions) and target (semantic role labels) for the alignment method of Figure 3a. Words in parentheses are the stop words removed from the original caption. We use the SpaCy default list of stop words for English (Honnibal et al. , 2020). The labels are ordered by the first term as the predicted activity followed by the set of the entities involved in the activity (Yatskar et al. , 2016). The alignment pairs are ordered from the highest to the lowest scores.",
        "GPT2_formal_text": "ed_text. Formal: In this setup, we use a multi-head attention mechanism (shoutout to Vaswani et al., 2017) to get a sentence representation H. Then, we feed H into a Bi-directional Long Short-Term Memory (Bi-LSTM) network, which gives us the hidden representation H. This is done by randomly combining the output from the attention heads. Formal: Here, we train the model using the test set, and if the test set is different from the training set, we just apply the adversarial loss to that difference. Formal: In short, for the loss loss, we set α to 0.1 and β to 0.2, while the loss α_ij is set to α_ij + β. For the adversarial loss, α is set to 0.1 and β to 0.2, while the loss α_ij is set to α_ij + β. In short, the loss loss is: Formal: The model calculates the cross-entropy loss as the negative of the adversarial loss, and the cross-entropy loss is: Formal: To keep things simple, we'll call the loss loss of the adversarial loss the adversarial loss. Formal: If we have N positive samples, the adversarial loss is: Formal: The loss loss of the adversarial loss is: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "conll",
        "filename": "2021.conll-1.2.json"
    },
    {
        "casual_text": "This project has a lot of room for expansion. Here's what we're thinking:\n\n(i) We want to look into more functions for τ to improve how we balance exploration and exploitation.\n\n(ii) We need to figure out better ways to assign nuclearity, especially since our evaluation showed too much N-N classification.\n\n(iii) We plan to test our approach on more sentiment datasets, like the one from Diao et al. (2014), and build even bigger treebanks.\n\n(iv) Our scalable solution could be expanded to predict discourse relations in addition to structure and nuclearity.\n\n(v) We’re thinking of combining our large-scale treebank with a neural discourse parser, like the one by Yu et al. (2018), to really make the most of data-driven discourse parsing.\n\n(vi) With the new MEGA-DT corpus, we want to revisit discourse-guided sentiment analysis to improve systems, especially for longer documents.\n\n(vii) Long-term, we’re interested in exploring other tasks like summarization, question answering, and machine translation for distant supervision of discourse. There’s a lot of annotated data available for these tasks (e.g., Nallapati et al. (2016); Cohan et al. (2018); Rajpurkar et al. (2016, 2018)).\n\nWe’re also doing a qualitative analysis of the generated discourse trees to see how they’re shaping up.",
        "formal_text": "This work can be extended in several ways: (i) We plan to investigate into further functions for τ to enhance the exploration-exploitation tradeoff. (ii) Additional strategies to assign nuclearity should be explored, considering the excessive N-Nclassification shown in our evaluation. (iii) We plan to apply our approach to more sentiment datasets (e. g. , Diao et al. (2014) ), creating even larger treebanks. (iv) Our new and scalable solution can be extended to also predict discourse relations besides structure and nuclearity. (v) We also plan to use a neural discourse parser (e. g. Yu et al. (2018) ) in combination with our large-scale treebank to fully leverage the potential of data-driven discourse parsing approaches. (vi) Taking advantage of the new MEGA-DT corpus, we want to revisit the potential of discourse-guided sentiment analysis, to enhance current systems, especially for long documents. (vii) Finally, more long term, we intend to explore other auxiliary tasks for distant supervision of discourse, like summarization, question answering and machine translation, for which plenty of annotated data exists (e. g. , Nallapati et al. (2016); Cohan et al. (2018); Rajpurkar et al. (2016 Rajpurkar et al. (, 2018 ). A Qualitative Analysis of Generated Discourse Trees",
        "GPT2_formal_text": "= {1, 2, ..., n}, y = {y1, y2, ..., yN} Formal: In this part, we'll talk about how we use the idea of causal constraints to work with distributional representations of entities. Formal: In this paper, we came up with a solution to the tricky problem of representing quantified modality. We showed that a basic N-gram model can effectively capture this quantified modality. The key to our approach is that we find the best combination of vectors that work best on the data, not just the most basic one. This is key to making the data-driven training approach work for our proposed metric. Formal: We also did some experiments to see how well our model works with different features. We created an NLI model that can handle quantified modality, but it can't directly handle the concept of context-free quantifiers. Formal: In this paper, we look at how it's possible to represent quantified modality, whether it's modal quantifiers or non-modal ones, in a new way that focuses on distributional properties. Formal: From the experiments, we found that our proposed representation is actually better than the current top methods. Formal: The main reasons our model works better are: Formal: For quantified modality, we use a straightforward, low-dimensional vector representation that's also easy to add to and edit, which we call the vector of modal quantifiers. Formal: We don't need to mess around with parametric type constraints. We just follow the kind of rules used in distributional semantics that work really well with our basic vector model. Formal: We're sharing our code for all the experiments we ran in this paper. Formal: We're not diving into the specifics of the language used to represent modality. Formal: We're also not directly dealing with the problem of quantified modality in NLP, which is a big part of what we're trying to address here. Formal: Basically, our approach is about using the idea of a modal quantifier to represent non-modal quantifiers. Formal: The model we talked about in this paper is a straightforward, low-dimensional vector model that can represent quantified modality. Formal: We think that if we can come up with a system that captures this kind of quantified modality, it might be a good starting point for more complex models that can also express quant",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.603.json"
    },
    {
        "casual_text": "Dense video event captioning (Krishna et al., 2017) and multi-modal video event captioning (Iashin and Rahtu, 2020b) are about creating a series of captions for all events in a video, whether it's just from the video itself (uni-modality) or from both the video and speech (multi-modality). Figure 1 shows an example of how tricky this task can be, especially when dealing with both visual and speech elements.\n\nFor the visual part, recognizing small or detailed objects can be really tough because of things like ambiguity, being hidden (occlusion), or changes in the object's state. In the example from Figure 1, the object \"dough\" is hidden in event 1, making it hard to spot just from the video. But if you look at the previous video frame, it's easier to recognize because it's clear and visible there.\n\nWhen it comes to speech, even though it provides useful semantic info (Shi et al., 2019; Iashin and Rahtu, 2020b), it also brings its own challenges. Speech can be informal, leading to issues like co-reference and ellipsis (where words are left out). In Figure 1, for event 3, the word \"dough\" is missing in the speech text, but it can still be figured out by looking at the context from other events, like event 1 in this case.\n\nIn short, both the immediate surrounding clips and the overall context between events are super important for creating captions that make sense and don't repeat the same stuff over and over.",
        "formal_text": "Dense video event captioning (Krishna et al. , 2017) and multi-modal video event captioning (Iashin and Rahtu, 2020b) aims to generate a sequence of captions for all events regarding to uni-modality (video) or multi-modality (video + speech) inputs. Figure 1 presents a showcase, which demonstrates the challenges of this task from both vision and speech text perspective. For vision understanding, the fine-grained objects are hard to recognize due to ambiguity, occlusion, or state change. In this case, the object \"dough\" is occluded in event 1 and is hard to recognize from the video. However, it can be recognized from the previous neighbor video frame with a clear appearance. From speech text perspective, although the speech text offers semantic concepts (Shi et al. , 2019; Iashin and Rahtu, 2020b), it brings another challenge of co-reference and ellipsis in speech text due to the informal utterance of oral speeches. In the case of Figure 1, the entity \"dough\" in event 3 is an ellipsis in the text. Nonetheless, it is capable of generating consistent objects \"dough\" in event 3 with the contextual information from other events such as event 1 in this example. To sum up, both local neighbor-clip and global inter-event contexts are important for event-level captioning to generate coherent and less duplication descriptions between events.",
        "GPT2_formal_text": "_tgt = attention_mask_tgt; Formal: All the word embedding embedding features are represented as d_word_embedding. Formal: Even though the latent variables are learned, they're still tied to the target model. This means they don't have direct access to the actual target words. Formal: So, the final hidden states from both the word and the embedding are passed through a single hidden layer and then combined. Formal: At the end, the word representation is turned into a tensor using a one-layer perceptron, which gives us the embedding vector. Formal: The final hidden state of the decoder's decoder is the embedding of the hidden state of the last hidden state of the decoder itself. Formal: The decoder's hidden state is calculated by the sum of the outputs from the outputs of the last hidden state of the last hidden state of the decoder itself. Formal: The model itself is built from three main parts: (1) the encoder, (2) the decoder, and (3) the hidden states of the last hidden states of the last decoder. Formal: The whole model is built using GANs, which are basically neural networks that have a hidden state dimension. Formal: The word and embedding embedding vectors are fed into the decoder to get the embedding vector. Formal: The decoder's hidden state, which is the embedding of the hidden state of the last hidden state of the last decoder, is calculated by the sum of the outputs from the outputs of the last hidden states of the last decoder. Formal: The word representations are combined using a multi-layer perceptron and fed into a fully connected layer to get the word representation vector. Formal: The decoder's hidden state, which is the embedding of the hidden state of the last hidden state of the last decoder, is calculated by the sum of the outputs from the outputs of the last hidden states of the last decoder. Formal: Formal: The final hidden state of the decoder's decoder is the embedding of the hidden state of the last hidden state of the last decoder. Formal: The decoder's hidden state is calculated by the sum of the outputs from the outputs of the last hidden states of the last decoder. Formal: The model itself is built using GANs, which are basically neural networks that have a hidden state",
        "directory": "acl",
        "filename": "2021.acl-long.156.json"
    },
    {
        "casual_text": "I labeled the synthetic and DBDC3 dialogues myself, using the ISO 24617-2 annotation scheme to mark the dialogue control functions. Since this project is all about finding common patterns in dialogue sequences, I mixed different types of conversations—like task-focused ones and casual chats—as well as different kinds of participants, such as human-to-human and human-to-machine interactions. Check out Figure 1 for the process I used to figure out dialogue patterns and create natural-flowing conversations for DMS.",
        "formal_text": "The synthetic dialogues and DBDC3 dialogues were hand labelled by the author with dialogue control functions following the ISO 24617 − 2 annotation scheme. Since, this work is aimed towards extracting generic sequences hence, we combined different domains (taks-driven and chit-chat) and participant types (human-human, human-machine). Figure 1: The workflow to obtain dialogue patterns for sequencing dialogues to build natural flows in DMS.",
        "GPT2_formal_text": ", and we'll use them later for the clustering process. Formal: So, p is the probability of some label happening based on the input sequence. Formal: We'll give each label an embedding, y_i, and a label-specific embedding, x_i, to help us label the remaining words. This setup helps us figure out how important each word is, based on the question. Formal: We'll combine these two embeddings to get the hidden representation for the token, h_t. Formal: Lastly, we'll use that h_t embedding for the question-answer pair to make the final prediction for the whole passage. Formal: Using the last hidden representation, h_t, we calculate the probability P_t_i = 1/|h_t|, and then use that to make the prediction. Formal: After doing all this, we get the predicted question and answer pair, y_i, and the correct answer, y_i. Formal: Using these, we can predict the question-answer pair, y_i, and the correct answer, y_i. Formal: Finally, we toss out the incorrect answers and get the correct answer, y_i. Formal: We use the attention mechanism to focus on important words in the question and answer, and then calculate the attention score for each word, h_t, to get the predicted label. Formal: Formal: Formal: In Appendix §A, we show how important words are in the generated question. Formal: Formal: The attention mechanism works based on the input, so each word in the question gets a kind of attention weight. Formal: Formal: Formal: Using the model we trained, the model predicts the label for the next question. Formal: Formal: Using these labels, we can also predict the next question. Formal: Formal: Formal: After using the model, we generate the question and answer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "codi",
        "filename": "2020.codi-1.2.json"
    },
    {
        "casual_text": "Alright, so when we have a new sentence, the system needs to understand it and update the conversation model (DM). There are some rules and methods, like plan inference rules [A1179] and constraint satisfaction [LA87, Car87], that help figure out what the sentence might be part of and suggest a sequence of actions. Plus, there are some tricks called forecasting heuristics [Car87, SidSl] that help prioritize these actions to make the conversation flow smoothly. For instance, the semantic representation of sentence (1) would look something like this:",
        "formal_text": "Given the senlalltic representation of a new utterance, the system mnst assimilate tim utterance and produce an updated dialogue model (DM). Plan inference rules [A1179] and constraint satisfaction [LA87, Car87] suggest chains of higher level actions that an utterance may in! part of, and foetlsing heuristics [Car87, SidSl] order these inference paths according to coherence. For exanlple, the semantic rel)resentation of (1) is:",
        "GPT2_formal_text": "_ttl: 0.9; o_ref: 0.7; b_ref: 0.3; N_ref: 0.4; s_ref: 0.1; h_ref: 0.3; f_ref: 0.1; d_ref: 0.7; w_ref: 0.5; a_ref: 0.5; q_ref: 0.4; h_ref: 0.5; s_ref: 0.1; h_ref: 0.5; f_ref: 0.1; d_ref: 0.3; w_ref: 0.5; a_ref: 0.5; q_ref: 0.4; h_ref: 0.5; s_ref: 0.1; h_ref: 0.5; f_ref: 0.1; d_ref: 0.5; w_ref: 0.5; a_ref: 0.5; q_ref: 0.4; h_ref: 0.5; s_ref: 0.1; h_ref: 0.5; f_ref: 0.1; d_ref: 0.7; w_ref: 0.5; a_ref: 0.5; q_ref: 0.4; h_ref: 0.5; s_ref: 0.1; h_ref: 0.5; f_ref: 0.1; d_ref: 0.3; w_ref: 0.5; a_ref: 0.5; q_ref: 0.4; h_ref: 0.5; s_ref: 0.1; h_ref: 0.5; f_ref: 0.1; d_ref: 0.7; w_ref: 0.5; a_ref: 0.5; q_ref: 0.4; h_ref: 0.5; s_ref: 0.1; h_ref: 0.5; f_ref: 0.1; d_ref: 0.3; w_ref: 0.5; a_ref: 0.5; q_ref: 0.4; h_ref: 0.5; s_ref: 0.1; h_ref: 0.5; f_ref: 0.1; d_ref: 0.5;",
        "directory": "C92",
        "filename": "C92-1049.json"
    },
    {
        "casual_text": "The hybrid change scores are still hanging out between the slang and non-slang groups, with an average of 0.621 ± 0.073. A permutation test shows there's a noticeable difference in how meaning changes between hybrid and slang words (p < 0.001) and also between hybrid and non-slang words (p < 0.05).",
        "formal_text": "tribution over hybrid change scores seem again to be centered between the slang and nonslang distributions, with mean 0. 621 ± 0. 073. According to a permutation text, there is a significant difference in semantic change both between hybrid and slang words (p < 0. 001) and between hybrid and nonslang words (p < 0. 05).",
        "GPT2_formal_text": ", which are basically sets of numbers that tell us who did what to whom. For the sake of keeping things simple, we'll just call these \"s_t.\" The first two columns in Table 1 show the results for each training example from the training set, which we call T_s. Formal: We've got a dataset D with pairs (x, y) made up of pairs (x, y). Each pair is the result of combining two things (x_i and y_i) using a specific method. Formal: To make things easier to understand, let's use two examples. In the first example, we can see that the \"Mortician\" is a person, while the \"Oracle\" is a tool. Formal: To understand how the model is being trained, we can check out the log probabilities from the logistic regression model. Formal: In this case, the model has already figured out the type of person (Mortician) and the tool (Oracle). Formal: Here, \"Mortician\" is the person, \"Oracle\" is the tool, and the domain of the relationships is set to be \"Mortician.\" Formal: For the second example, we can see that \"Mortician\" is a person, and the \"Patient\" is the tool. Formal: To get the final prediction for the two people, we just add up all the probabilities from the logistic regression model. Formal: This final prediction is basically the log probability of both the person and the tool combined together. Formal: So, the final prediction is the chance that the \"Patient\" and the \"Mortician\" pair are the correct answers. Formal: Basically, the probabilities are the expected numbers based on the model's predictions. Formal: Here, x_i and y_i are the inputs for the model. Formal: The logistic regression model has two main parts: Formal: The first part is the main model we're talking about here. Formal: The second part is the logistic regression model. Formal: Formal: The model learns how to combine the inputs and output stuff. Formal: The final output is the probability that the \"Patient\" and the \"Mortician\" pair are correct answers. Formal: Formal: We can also think of the logistic regression model as a kind of optimization problem, where we need to maximize the log probability of the correct answer for each",
        "directory": "acl",
        "filename": "2022.acl-long.101.json"
    },
    {
        "casual_text": "Hey, so there's this thing called \"Dipl. -Nath. Gisela Schlotter\" and it's about processing non-numeric data. Specifically, it talks about Part I: Subroutines (PI-17).",
        "formal_text": "Dipl. -Nath. Gisela Schlotter, Verarbeitung nichtnumerischer Daten Tell I. Unterprogramme (PI-17)",
        "GPT2_formal_text": "ed = 0.5; cond. cond_masked = 0.5; cond. xt = t; cond. ctx = ctx; cond. type = t; cond. trace = t; cond. buffer = b; cond. entity = e; cond. type = e; cond. trace = e; cond. buffer = g; cond. entity = g; cond. type = g; cond. trace = g; cond. buffer = h; cond. entity = h; cond. type = h; cond. trace = h; cond. buffer = i; cond. entity = i; cond. type = i; cond. trace = i; cond. buffer = k; cond. entity = k; cond. type = k; cond. trace = k; cond. buffer = l; cond. entity = l; cond. type = l; cond. trace = l; cond. buffer = m; cond. entity = m; cond. type = m; cond. trace = m; cond. buffer = n; cond. entity = n; cond. type = n; cond. trace = n; cond. buffer = p; cond. entity = p; cond. type = p; cond. trace = p; cond. buffer = q; cond. entity = q; cond. type = q; cond. trace = q; cond. buffer = r; cond. entity = r; cond. type = r; cond. trace = r; cond. buffer = s; cond. entity = s; cond. type = s; cond. trace = s; cond. buffer = t; cond. entity = t; cond. type = t; cond. trace = t; cond. buffer = v; cond. entity = v; cond. type = v; cond. trace = v; cond. buffer = w; cond. entity = w; cond. type = w; cond. trace = w; cond. buffer = x; cond. entity = x; cond. type = x; cond. trace = x; cond. buffer = y; cond. entity = y; cond. type = y; cond. trace = y; cond. buffer = z; cond. entity = z; cond. type = z; cond. trace = z; cond. buffer = a; cond. entity = a; cond. type = a; cond. trace = a; cond. buffer = b; cond. entity = b;",
        "directory": "C67",
        "filename": "C67-1017.json"
    },
    {
        "casual_text": "Alright, now we're going to take a closer look at these setups and show how they back up our second idea.",
        "formal_text": "Next we provide an analysis of these constructions which supports our second hypothesis.",
        "GPT2_formal_text": "is a list of unique query intent IDs, and it's in the form (query, intent). In this situation, the query id i can be any of the queries in the set C, but the intent id j can't be. The process for figuring out the query, intent, and slot values is shown in Fig. 2. \n\nWhen dealing with complex queries, the first thing we do is use a top-notch feature extractor to identify the key parts of the query and intent, which are the query and intent. These parts are then turned into three main types of features: slot features, slot arguments, and slot values. Formal: There are three main types of query-intent features: Formal: We grab slot representations from the query and intent using predefined predicates. Formal: We also use query-intent features to grab slot representations from the query and intent. Formal: Finally, we combine the query and intent embeddings to get the overall query representation, which is the query and intent combined. Formal: Since the query and intent are two separate things, we combine their features, but we don't use any special language to separate them. Formal: This way, we can use a classifier to figure out the proper queries and intents for a specific query. Formal: Once we have the queries and intents, we use something called a sequence-to-sequence model to encode the query and intent together. Formal: This is basically the same as the token-level encoder-decoder model mentioned in Section 2. Formal: We also add a special feature to the query to predict the slot values. Formal: Formal: To make sure we're comparing things fairly, we use a standard variational inference algorithm to train the model. Formal: This method works well because we don't need to figure out the exact probability distribution for the slot values. Formal: We also calculate the average accuracy using the development set to get a final score. Formal: Lastly, we calculate the F1 score for the whole dataset. Formal: Formal: Finally, we calculate the F1 score for the whole dataset. Formal: The results are in Table 3. Formal: The predicted slot values are also in Table 3. Formal: The results are in Table 3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E17",
        "filename": "E17-1115.json"
    },
    {
        "casual_text": "We take the samples that didn’t make it into the test sets and use them for training or tweaking models with noisy data. We use a regular expression to automatically break comments into sentences by spotting sentence endings. Then, we match up the source and target sentences. If this matching process doesn’t work (like when the source comment has a different number of sentences compared to the target comment after splitting), we just keep the whole comment without cutting it up. \n\nFor the training data, we don’t check the translations as carefully as we do for the test data. Lastly, here’s a quick breakdown of the samples and tokens for each language pair:\n\n- **en-fr**: 36,058 samples, 841k source tokens, 965k target tokens  \n- **fr-en**: 19,161 samples, 661k source tokens, 634k target tokens  \n- **en-ja**: 5,775 samples, 281k source tokens, 506k target tokens  \n- **ja-en**: 6,506 samples, 172k source tokens, 128k target tokens  \n\nWe also set aside around 900 samples for validation in each direction. You can check out the size of the data in Tables 1, 2, and 3 for the test, training, and validation sets, respectively. For tokenizing, we use the Moses tokenizer for English and French data, and Kytea for Japanese before counting the tokens in each dataset.",
        "formal_text": "We gather the samples that weren't selected for the test sets to be used for training or fine-tuning models on noisy data. We automatically split comments into sentences with a regular expression detecting sentence delimiters, and then align the source and target sentences. Should this alignment fail (i. e. the source comment contains a different number of sentences than the target comment after automatic splitting), we revert back to providing the whole comment without splitting. For the training data, we do not verify the correctness of translations as closely as for the test data. Finally, #samples #src tokens #trg tokens en-fr 36, 058 841k 965k fr-en 19, 161 661k 634k en-ja 5, 775 281k 506k ja-en 6, 506 172k 128k we isolate ≈ 900 samples in each direction to serve as validation data. Information about the size of the data can be found in Table 1, 2 and 3 for the test, training and validation sets respectively. We tokenize the English and French data with the Moses (Koehn et al. ) tokenizer and the Japanese data with Kytea before counting the number of tokens in each dataset.",
        "GPT2_formal_text": "ed(y_t), and then the attention weights are updated to reflect that. Formal: Our system doesn't need to know the domain, so it doesn't need a parallel corpus, which was the case before. Formal: In this setup, our approach has two main advantages. First, we can use the sentence-level, domain-specific stuff instead of having to go through a bunch of annotations and create multiple training data sets for each domain. Second, it allows us to skip the whole multi-label training process. Formal: We also think that since the teacher model is already doing its thing, we can skip the multi-label training part and focus on getting the model's predictions right on the test set instead. Formal: Here's how our framework works: (1) Our model uses three main parts: Formal: The student model takes in a sentence and a bunch of input bits, like the domain's vector and token representation. Formal: The student model then uses these inputs to figure out how to get the right answer. Formal: The student model uses the attention mechanism, which looks at both the current token and its previous context. Formal: Lastly, the student model uses a residual connection between the current token and the prediction it wants to make. Formal: The student model learns its representations using these representations. Formal: The student model's predictions are updated by the attention mechanism to reflect the domain's vector. Formal: Finally, the student model uses a residual connection to train. Formal: We tested how well our system works using different amounts of data: Formal: We used 50 examples for training, 15 for validation, and 5 for testing. Formal: We used the BERT-base model (from Devlin et al., 2019) and fine-tuned it on a test set of 10,000 examples. Formal: We got the results shown in Figure 1. Formal: Formal: Formal: We trained the student model for 5 epochs and then checked its performance after every 5 epochs. Formal: We also added a 300-dimensional continuous distribution layer with a non-linear function to the output vector to handle multi-label cases. Formal: The student model performs better after every 5 epochs. Formal: We trained the student model for 5 epochs and then checked its performance after every 5 epochs. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1050.json"
    },
    {
        "casual_text": "So, we've got τ = {a, b, c}, and ψ_n(•) and ψ_e(•) are the node and edge factors, right? And Z(•) is just this big normalization thing that makes everything work out. We're working with log-linear factors here, so keep that in mind.",
        "formal_text": "where τ = {a, b, c}, ψ n (•) and ψ e (•) are node and edge factors, respectively, and Z(•) is a global normalization constant. We use log-linear factors:",
        "GPT2_formal_text": "= { \"person_id\" : [ { \"first_name\" : \"John\", \"last_name\" : \"Smith\", \"age\" : 26 }, { \"first_name\" : \"Peter\", \"last_name\" : \"Brown\", \"age\" : 32 }, { \"first_name\" : \"Peter\", \"last_name\" : \"Brown\", \"age\" : 28 } ], \"age_ratio\" : [ { \"first_name\" : \"Peter\", \"last_name\" : \"Brown\", \"age\" : 25 }, { \"first_name\" : \"Peter\", \"last_name\" : \"Brown\", \"age\" : 31 }, { \"first_name\" : \"Peter\", \"last_name\" : \"Brown\", \"age\" : 28 } ] }\n\nThe \"person_id\" column represents the unique person names in a document. If we're using a document-specific representation, like the one in Figure 1, we'll just stick them together to get the final representation. Formal: A bunch of articles are grouped together under a specific topic and then broken down into individual sentences. Formal: In our example, the topic is \"the New York Times.\" Formal: The first sentence in the dataset is \"The New York Times,\" and the rest are articles from the same newspaper group. Formal: The ID field in the dataset includes the document ID, which is the ID of the article you're looking at. Formal: We also have the \"age_ratio\" column, which is like a histogram showing how often each group of articles appears in the dataset. Formal: We use a two-sided Spearman's ρ to measure how well our model does, which shows how good its predictions are. Formal: The results for the specific topic and the sample sentences we've chosen are shown in Figure 1. Formal: For each topic, we picked articles from the same group of newspapers to create a standard dataset that covers the whole topic. Formal: When we look at the article IDs, we see that \"person_id\" and \"age_ratio\" are pretty close to each other, meaning they’re similar and haven’t changed much over time. Formal: These two factors together make up the dataset we use for training and testing. Formal: We also use some clustering methods based on the concept of \"grouping.\" Formal: Our final dataset",
        "directory": "D18",
        "filename": "D18-1452.json"
    },
    {
        "casual_text": "Length-Control Inference. Summarization naturally involves controlling how long the output is, like when you need a short news headline for a mobile screen. Plus, Schumann and his team (2020) found that the main evaluation metric, ROUGE (Lin, 2004), is affected by how long the summary is—longer summaries usually get better ROUGE scores. So, it’s super important to keep the summary length in check for a fair comparison.",
        "formal_text": "Length-Control Inference. Controlling output length is the nature of the summarization task, for example, displaying a short news headline on a mobile device. Moreover, Schumann et al. (2020) show that the main evaluation metric ROUGE (Lin, 2004) is sensitive to the summary length, and longer summaries tend to achieve higher ROUGE scores. Thus, it is crucial to control the summary length for fair comparison.",
        "GPT2_formal_text": "_2 is the mask for the [CLS] token. h_c stands for the batch size. h_r is the number of samples in the sample-level attention network. A = {h_c, h_r} means the two tokens are kept in the same order as before. Formal: The input to the model is a simple string of tokens, like s_t = s_t1, s_t2, ..., s_tT. Formal: After the model does its thing, it sends a \"w_input\" vector to the decoder. Formal: The decoder is a sequence-to-sequence model that takes the token sequence s_t and turns it into a single representation. Formal: The input to the decoder is the word sequence [CLS] and a weight matrix V. Formal: The output from the decoder is a representation of the whole sentence. Formal: For each position p_t from 1 to T, the model generates a new sequence of tokens, s_t = s_t1, ..., s_tT. Formal: The model figures out the attention distribution p(y|x, v) for the [CLS] token y. Formal: Finally, the model sends this output to the decoder for its final output. Formal: The model's parameters (θ) are calculated using the cross-entropy loss function. Formal: The decoder is an end-to-end sequence model that takes the output from the model and turns it into a single representation. Formal: The input to the model is a token sequence and its attention weight matrix. Formal: The decoder is trained using a supervised learning approach, following the approach by Sun et al. (2019). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.545.json"
    },
    {
        "casual_text": "Alright, so when you're trying to figure out what's going on with someone's problem, it's a good idea to ask questions that help them explain things better. Open-ended questions work great for this because they let the person talk more freely and really get into what's bothering them.",
        "formal_text": "Question Asking for information related to the problem to help the help-seeker articulate the issues that they face. Open-ended questions are best,",
        "GPT2_formal_text": ", r = i_t, k_t, c = c_t)  \n\nBasically, it's saying that if one thing (the thing being measured) is missing from the pair of sentences, or if both sentences have the missing thing but in different ways, then the score is 0. Formal: We're dealing with pairs of things here, and they're tagged with each of these three types: true_positive, true_negative, and false_positive. Formal: We're using the same data as before, but instead of the usual NER pairs, we're using the sentence pairs in Table 1. Formal: The rest of the formula is the same as in Section 5. Formal: We're using the same token as before, and we're adding two special variables to count how many times each token appears. Formal: The rest of the formula is the same as in Section 5. Formal: The first thing we do is find the highest value in the sum of the first three scores. If we can't find that highest value, we just use the lowest score to update the system. Formal: This whole process happens over and over again, and we keep track of how often each token appears in the first two rounds. Formal: The final result is the total score. Formal: The second thing we do is set a threshold for the top value in the sum. We use a kind of rough estimate, based on how often the first token appears. Formal: The threshold is set using the eigenvalue decomposition. Formal: Finally, we calculate the average of all the scores to get the final result. Formal: We also check how similar the phrase vectors are between the current system and the one we tested. Formal: We measure how similar the phrase vectors are using the cosine similarity between the phrase vectors and the word vectors for the topic. Formal: We use the \"Syntactic Correlation Index\" (SCI) to calculate the similarity between the phrase vectors and the word vectors for the topic. Formal: Finally, we calculate the average of all the scores to get the final result. Formal: Formal: We also do some extra experiments to see how different model setups affect our results. Formal: We train our model by picking the best model from the top-k choices using the method from Section 3.3.3. We try out different values of k, from 1 to 100, and the results",
        "directory": "acl",
        "filename": "2021.acl-long.269.json"
    },
    {
        "casual_text": "Glosses are basically short definitions, and they're a big deal in plWordNet. They help users get what the network is about and make it easier for the editors to work efficiently.",
        "formal_text": "• Glosses are short definitions, a very important element of plWordNet. They help the user to understand the network, and plWordNet editors to work with high effectiveness.",
        "GPT2_formal_text": "ing = (1.0, 0.9) [posterior] [text] [image] [text] [image] [text] [image] [text] [image] [text] [image] [text] Formal: 2.0 - Vector Clustering Formal: 2.0 - GCN + Recurrent Neural Networks + Self-Attention + GCN Formal: 2.0 - GCN + Recurrent Neural Networks + Self-Attention + GCN + GCN Formal: 3.0 - Vector Clustering Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: 3.0 - GCN Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1213.json"
    },
    {
        "casual_text": "Yeah, there are some downsides too. For one, the whole thing is pretty small—around 500 million tokens in total. Plus, most languages only have one version of the New Testament, which is about 200,000 tokens each. The focus is pretty narrow, and there’s a lot of overlap with names and stuff. This can cause weird issues if you use random word combinations, because the model might just spit out a bunch of random names instead.",
        "formal_text": "Certainly, there are disadvantages as well, such as the limited size (roughly 500 million tokens in total, with most languages having only one translation of the New Testament each, with roughly 200 thousand tokens), the narrow domain and the high overlap of named entities. The latter can lead to some unexpected effects when using nonsensical language vectors, as the model will then generate a sequence of random names.",
        "GPT2_formal_text": "= {(x_i, y_i), ..., (y_j, x_i, y_j)}, where the numbers i, j can be from 1 to K. We just use the input slot tags and the [CLS] token from the model to get the predictions. Formal: We assume the dialogue is already set up in advance and doesn't change during the conversation. Formal: The variable `response_id` is a unique number assigned to each response in the response sequence. Formal: We also assign a value to each part of the response `h_i` by combining the values of its sub-sequence `h_i_1` and the sub-sequence `h_i_2`. Formal: For each response `h_i`, the responses are first turned into an embedding `h_i_t` using a self-attention mechanism (Bahdanau et al., 2015). Then, these responses are turned into a position embedding `h_i_r` using a position-weighted attention mechanism (Liu et al., 2016). Formal: Finally, we grab the global context vector `c_i` from the embeddings of all the responses in the response sequence and feed it into the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E17",
        "filename": "E17-2102.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nYou can find the data set over at http://homepages.inf.ed.ac.uk/gcong/qa/.",
        "formal_text": "http: //developer. yahoo. com/answers 7 The data set is available at http: //homepages. inf. ed. ac. uk/gcong/qa/",
        "GPT2_formal_text": "ed = {l_i, r_i, t} h_f_mask = {l_i, r_i, t} h_w_mask = {l_i, r_i, t} Formal: Here, the loss function is linear, and the parameters are scaled so that each one follows a specific power law. Formal: The loss function l_f_mask is calculated using this formula: Formal: Similarly, the l_r_mask and l_t_mask losses are also linear, but they're scaled so that the scaling factor β(•) equals 1. Formal: Here, α(•) is the scaling factor, and β(•) is the window size. Formal: The final log loss for the final hierarchical attention layer is calculated by adding up the scaling factors of all the layers. Formal: You can find all the nitty-gritty details of the NMT model in the paper by Wang et al. (2019). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C12",
        "filename": "C12-1193.json"
    },
    {
        "casual_text": "Hey, so here's a quick rundown of some #MH17 updates:\n\n- A pro-Russia separatist mentioned that the bodies found in the Ukraine field weren't fresh, implying they were already dead before the plane took off.\n- #MH17 is back in the news: \n  1. #Kolomoisky admits to being involved (link provided)\n  2. He also got $1.8 billion from Ukraine's bailout funds.\n- Pro-Russia and pro-Ukraine sides are still at it.\n- Russia is now saying that #MH17 was taken down by an air-to-air missile, but of course, it wasn't Russian-made. Classic.\n- And Moscow's latest claim is that a Ukrainian fighter jet shot down #MH17.",
        "formal_text": "RT @katehodal: Pro-Russia separatist says #MH17 bodies \"weren't fresh\" when found in Ukraine field, suggesting already dead b4takeoff m) RT @NinaByzantina: #MH17 redux: 1) #Kolomoisky admits involvement URL 2) gets $1. 8B of #Ukraine's bailout funds III Pro-U Pro-R n) #Russia again claiming that #MH17 was shot down by air-to-air missile, which of course wasn't russian-made. #LOL URL o) RT @20committee: New Moscow line is #MH17 was shot down by a Ukrainian fighter. With an",
        "GPT2_formal_text": "= [](/home/homepage/2009/articles/PU/index.html) Formal: In the case of SentimentTiling, the stack variables are split into two groups. Formal: For context-aware sentiment, the parts of the text that are related to the current question are picked out. But for context-agnostic sentiment, the specific content of the current question is left out. Formal: To make things simpler and more manageable, we only look at the parts of the text that are related to the current question. This way, we can avoid worrying about all the complicated relationships between the questions and the sentences in the text. Formal: Instead of just picking the most important words for a sentence in a specific context, we take into account the context of the whole passage. This helps us focus on the important parts and make the whole sentence more accurate. Formal: Each topic is represented using a simple vector, which is just the average of the vectors from all the topics that have the most words in their first-order words (the first n words). Formal: The context vectors for the question and the sentence are combined to create a single vector, which is the average of the vectors from all the questions. Formal: We use a linear linear model to calculate the probabilities. Formal: A simple way to get these probabilities is by adding up all the probabilities from the question and the sentence. Formal: Then, we get the probability of the question and the sentence by adding up all those probabilities. Formal: Formal: The model calculates these probabilities for each word in the text and assigns the probability to a topic vector. Formal: Since the model depends on the words in the text, it needs to be able to calculate probabilities for any word in the text. Formal: Formal: Since the model is based on the words in the text, it’s also important to calculate probabilities for any word in the text. Formal: Formal: When the probability is 0, the model doesn’t include any word in the text. Formal: Formal: We calculate the probabilities for each word in the text and assign the probability to the topic vector. Formal: Formal: Formal: Formal: After that, we calculate the probability for each word in the text and assign it to the topic vector. Formal: Formal: Formal: After that, we calculate the probability for each word in the text and assign it to the",
        "directory": "D19",
        "filename": "D19-5006.json"
    },
    {
        "casual_text": "To show that CoRA can handle long-tail relations really well, we did a test just on those long-tail relations. We used the same setup as before, where Hits@K (Macro) measures how well it performs on these long-tail relations. From Table 2, you can see we compared CoRA to other models and our own base models. It turns out that CoRA boosts performance on long-tail relations by a big margin and sets a new record. \n\nWhen we look at previous models (like PCNN+HATT/+KATT) that also use relation hierarchies, our relation-augmented attention (Base) without any hierarchy still does pretty well, even without fancy pre-trained graph embeddings like PCNN+KATT uses. If we compare our base model with selective attention (PCNN+ATT), the big difference in performance shows how our framework is better at dealing with both wrong labels and long-tail relations. \n\nLastly, as you can see in the last row of the table, if we take out the sent2rel attention we proposed, the performance drops a lot, which really highlights how important it is for handling long-tail relations.",
        "formal_text": "To prove the capability of CoRA in handling long-tail relations, we conduct an evaluation solely on longtail relations. Our evaluation setting is identical to, where Hits@K (Macro) is used to represent statistical performance on long-tail relations. As shown in Table 2, we compare CoRA with competitors and our base models. It is observed that, CoRA improves the performance on long-tail relations by a large margin and delivers a new state-of-the-art results. Compared to previous works (PCNN+HATT/+KATT) that also leverage the relation hierarchies, our relation-augmented attention (Base) without any hierarchy even gets competitive results, not to mention pre-trained graph embeddings used in PCNN+KATT. Further comparing our base model with selective attention (PCNN+ATT), the huge performance gap demonstrates the advantages of our framework in handling both wrong labeling and long-tail relations. Finally, as shown in the table's last row, removing the proposed sent2rel attention leads to significant decrease, which emphasizes its importance for long-tail relations.",
        "GPT2_formal_text": "is calculated as the average of the values for each word in the sentence, divided by the total number of words in the sentence. Formal: In this paper, we use the attention mechanism introduced by Bahdanau et al. (2014) to predict the next word in a sentence. Basically, in our model, the attention is calculated using the K-dimensional hidden state vector h tt, which is learned from the previous sentences in the source text. Formal: For the source text, we start by figuring out the hidden state vector for the word w i. Formal: The attention mechanism in the source model takes into account the context of the last word w i, which we got from the last hidden state vector h tt. Formal: The model also predicts the attention vector for the current word w i. The predictions are made using the latent variable h tt for the words in the current sentence, as mentioned in the main text. Formal: In the next section, we'll give a quick overview of the model and how it calculates the attention. Formal: The model is trained using a cross-entropy loss function. Formal: We tested our model using five different datasets, all from the News domain, as mentioned in Section 2. Formal: In this paper, we introduce a new approach called Multi-Head Attention Network (MHAN), which not only predicts the next word but also focuses on the context of the previous word. Formal: For the target sentences, we start by using the hidden state vector for the word w i. Formal: To estimate the attention vector, we calculate the average of the attention vectors for the whole sentence, as mentioned in Section 3.3. Formal: Finally, we apply the Multi-Head Attention Network to the source sentence, as shown in Figure 1. Formal: We tested our model on five different datasets, all from the News domain, as described in Sections 2.1 and 2.2. Formal: The results are in Table 1. The model performs really well on the English CNNDM dataset (from Lu et al., 2016), improving its performance by 2.5 points on the test set and 1.1 points on the development set. Formal: As we mentioned in Section 2.1, we also created a multi-task dataset with the News domain. Formal: In this dataset, we used a multi-head attention mechanism with a batch size of 5 and a window size of 5. Formal: We",
        "directory": "coling",
        "filename": "2020.coling-main.145.json"
    },
    {
        "casual_text": "The experiments show that getting rid of that noise helps our model work better.",
        "formal_text": "The experimental results show that removing these noise can improve the performance of our model.",
        "GPT2_formal_text": "(e) = id(e) + [1, 2, ..., m] + [1, 2, ..., n]  \n\nOutput: The final model has N(N²), where N is the number of layers. Formal: We can use this model to figure out the attention weights, which we'll call a_t = a_t_t + a_t_t_t_t. Formal: The encoder takes in the representation of the query and the entity embedding as input, like this: Formal: The input to the encoder includes the [CLS] token and the entity embedding. Formal: This model can work with either the [CLS] token or the entity embedding. Formal: We train it using the whole dataset. Formal: We take the decoder and adjust its parameters so it focuses on the entity embedding as input. Formal: The updated decoder looks like this: Formal: Then, we use this updated model to predict the answer for the query. Formal: Finally, we calculate the attention weights, a_t, based on this updated model and the query. Formal: Formal: For a more detailed explanation, check out section 3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "dravidianlangtech",
        "filename": "2021.dravidianlangtech-1.21.json"
    },
    {
        "casual_text": "Around 1% of BCCWJ is considered the \"core data,\" and this core data is manually annotated with extra linguistic details. The part of the core data that includes annotations for dependency structures, coordinate structures, coreference relations, and predicate argument structures is specifically called BCCWJ-DepParaPAS (Ueda et al., 2015; Maekawa et al., 2014). BCCWJ-DepParaPAS follows the annotation style of the NAIST Text Corpus (Iida et al., 2007b), where three key arguments—ga (nominative), wo (accusative), and ni (dative)—are marked at the SUW level.",
        "formal_text": "About one percent of BCCWJ is defined as the \"core data\", and the core data is manually annotated with richer linguistic information. The core data annotated with dependency structures, coordinate structures, coreference relations and predicate argument structures is particularly called BCCWJ-DepParaPAS (Ueda et al. , 2015; Maekawa et al. , 2014). BCCWJ-DepParaPAS adopts the annotation schema of NAIST Text Corpus (Iida et al. , 2007b) where three obligatory arguments, ga (nominative), wo (accusative) and ni (dative) are annotated at the SUW level.",
        "GPT2_formal_text": "ing(X, Y, W) = O. Formal: The LSTM layer works by predicting the next word based on the last hidden state it got from the encoder. Formal: The attention mechanism can focus on specific parts of the input (like the part that's been masked) or on all the words in the input. Formal: Even though there are some advanced models out there that focus on the context of the word being masked, we decided to go with a single-layer, fully-connected neural network instead. This is the one that gets the final output. Formal: The output is a binary representation that helps classify the input. Formal: Since we want the output to be general, we use a softmax function to normalize it. Formal: Figure 3 shows the model setup for the classifier. Formal: The output is just a list of tokens that the classifier thinks match the input. Formal: The classifier works by looking at the word's last hidden state to predict the next word. Formal: For instance, if we see \"left\" and \"restaurant,\" it's likely the next word will be \"restaurant.\" Formal: Formal: The input sentence X is turned into a fixed-size vector Y that's been randomly initialized. Formal: This initial Y is used as the query vector, which we'll use to predict the next word. Formal: The classifier's prediction can be broken down into multiple steps: Formal: The first step is using a deep neural network to predict the next word. Formal: In the second step, the classifier checks the query vector Y and picks the matching token. Formal: Lastly, in the third step, the classifier uses a linear layer to figure out the probability of the next word being the correct one. Formal: Formal: You can find more details about how we set up the classifier in Appendix B. Formal: In the last three steps, the classifier focuses on the masked part of the input to guess the next word. Formal: Finally, in the fourth step, the classifier combines both the hidden state and the query vector to predict the next word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1269.json"
    },
    {
        "casual_text": "For languages that aren’t as high-priority or when the source language isn’t English, and especially for apps where the quality expectations are a bit lower, we’re focusing on more budget-friendly solutions. Like we mentioned, Adobe’s started looking into the Moses open-source tool, and we’ve also been talking to some vendors to address the issues with those one-size-fits-all, free online services.",
        "formal_text": "For lower-tier languages, language pairs where the source is not English, and applications where the quality requirements are lower, we are developing solutions from the lower-cost categories. As described, Adobe has begun investigating the Moses open-source package, and has also been working with vendors to mitigate some of the drawbacks of vertically-customized, uncustomized, and free online services.",
        "GPT2_formal_text": ", key_values, and values_ids, we get a list of tuples where each tuple is a part of the sentence. Formal: For the T5 model, the parameters λ are set to 1e-3, while for the GPT-2 model, they're set to 1e-4. Formal: For BERT, we tweak the hidden state size to 256, but for GPT-2, we go with 300. Formal: The CNNs in the dependency parser are trained using the loss function l_CNN, which we picked based on the cross-entropy loss function. Formal: We use the 'best' word embeddings from both the source and target parts of the sentence to figure out the query vector. Then, we calculate the cross-entropy loss using the same cross-entropy loss function. Formal: We calculate the cross-entropy loss using the cross-entropy loss function. Formal: For the RNNG, we tweak the hidden state size to 256, while for GPT-2, we go with 300. Formal: For BERT, we use the loss function l_RNNG, but for GPT-2, we use the loss function l_TGRNNG. Formal: The word embeddings are chosen based on the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: The decoder outputs the final word representation. Formal: We train the model using the cross-entropy loss function. Formal: The decoder outputs the final word representation. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-entropy loss function. Formal: We train the model using the cross-ent",
        "directory": "amta",
        "filename": "2010.amta-commercial.5.json"
    },
    {
        "casual_text": "We can build the transition sequence that Theorem G.9 is asking for by using C LT L over and over on a starting configuration, let's call it c. Lemma G.15 says that when you use C LT L on a configuration, you either end up with a goal configuration or you add one more FINISH transition. Lemma G.14 also tells us there's a limit to how many times we can keep adding FINISH transitions in a valid sequence. Since C LT L only gives us valid sequences, we'll definitely hit a goal configuration after applying it a certain number of times.",
        "formal_text": "We can construct the transition sequence for which Theorem G. 9 asks by repeatedly applying C LT L to a given configuration c. Lemma G. 15 shows that applying C LT L to a configuration results either in a goal configuration or increases the number of FINISH transitions by one. Lemma G. 14 tells us that there is an upper bound on how many times we can increase the number of FINISH transitions in a valid transition sequence. Since C LT L returns only valid transition sequences, this means that we reach a goal configuration by finitely many applications of C LT L.",
        "GPT2_formal_text": "and response_mask are combined and then passed through an LSTM. The input for this whole process is the input and hidden representation of the image. Formal: We take a bunch of examples from our target domain, D_i, and feed them into a GRU block. The block then generates an output, y_t, that we use to update our attention. Formal: In a GRU block, we figure out the hidden state and bias vector of the GRU by looking at the input and the target domain. Formal: To train the GRU block, we use a bunch of examples from D_i. For each example, we pick a value between 0 and 1. Then, we run the GRU block using the chosen value and update the parameters. Formal: We use a transformer model (like the one Vaswani and team came up with in 2017) to train the GRU block. Formal: To check how well the GRU block is doing, we look at how similar the generated outputs are to the reference outputs and also how different the reference outputs are from the generated outputs. Formal: The parameters for the GRU block are based on the input, the target domain, and the dimensions of the input, which we calculate using equation (1). Formal: The GRU block has two input channels (h_1 and h_2) and two output channels (h_o and h_r) to handle the task. The goal of the task is to predict the target image y_t based on the output y_t, using the GRU block. Formal: To train the GRU block, we pick examples from D_i using the maximum-likelihood method, which is explained in (Zhang et al., 2017). Formal: We also add an output layer with three inputs: the source image X_s, the target image y_t, and the context image c_t. Formal: After training the GRU block, we use the training set to train the model on the target domain. Formal: Finally, we optimize the parameters using equation (2). Formal: We tested our method on three datasets and the results are in Table 3. Formal: The results show that the model's performance on D_i really depends on the dataset. Formal: Our approach works really well and beats the current top methods out there. Formal: To get the best results, we tweaked the parameters of the",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nFor text classification, it assigns a label to each piece of text by looking at the embedding of the first token. This first token is usually some kind of special marker, like a symbol or something.",
        "formal_text": "CLASSIFICATION For text classification, it predicts a label for every text instance by using the embedding of the first token, which is commonly a special token (e. g.",
        "GPT2_formal_text": "= { ..., [] }, label_ids = { ..., [] }, xt = { ..., [] } Formal: In the case of (1), we’ll give each word a label. But in the case of (2), where we’re using a phrase-based model, we’ll only get one label, which is \"1.\" Formal: For the first part of the analysis, we’ll look at the whole sentence. For the second part, we’ll only consider the parts that are actually in the sentence. Formal: In the first part, we’ll look at the whole sentence. We’ll then divide the word vector into parts. Formal: In the second part, we’ll only consider the parts that are in the sentence. Formal: We’ll take the word vector (w_i) and run it through a context-dependent softmax to get the probability distribution (p_ij). Formal: Formal: So, P(w_i|w_j) is calculated by multiplying P(w_i|w_j) with a probability distribution P(w_i|w_j). Formal: Lastly, we’ll use the model’s output (o_0) to calculate the next word vector (p_ij). Formal: We’ll use the model’s output (o_0) to calculate the next word vector (p_ij). Formal: In the first part, we’ll use the whole sentence. Formal: In the second part, we’ll only use the parts that are in the sentence. Formal: Formal: Finally, we’ll use the model’s output (o_0) to calculate the next word vector (p_ij). Formal: Formal: Formal: Finally, we’ll use the model’s output (o_0) to calculate the next word vector (p_ij). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-demos.22.json"
    },
    {
        "casual_text": "All parsing methods end up with the same number of nodes overall, but they just build those nodes at different times. Usually, left-branching ways of doing things get nodes done earlier compared to right-branching ones. But when it comes to right-adjunction, both left and right-branching methods hold off on making a lot of nodes until the right-adjunct is processed. That’s not true for revealing derivations, though—they’re made to handle right-adjuncts more flexibly and don’t delay like the others.",
        "formal_text": "All parsing strategies have the same total number of nodes, but only differ in the abstract timing of those nodes' construction. In general, leftbranching derivations construct nodes earlier than do the corresponding right-branching derivations. However, in the case of right-adjunction both leftand right-branching derivations delay construction of many nodes until the right-adjunct is consumed. This is not the case with the revealing derivations that are specifically designed to allow flexibility with right-adjuncts.",
        "GPT2_formal_text": "ed has the same context as the input. The final masked token is added to the input. Formal: The output from the source language model (SGM) is used to predict the next word. This is done by comparing the input to the output from the target language model (TGM). Formal: We use a pairwise max pooling method with the log loss function, which gives us a probability distribution (Pr(f, e)) over the entire context, c. Formal: We don't know the context. For instance, if we're looking at the input \"cat,\" the context is \"a cat,\" and the TGM might predict \"cat.\" Formal: This is similar to how LSTM-based models work. Formal: We do some checking to see if the context matches the input (C(e, f)) and if there are any words that aren't in the dictionary (no, N). If there are, we update the model to use them. For the \"cat\" example, if we know that the context is \"a cat,\" we might adjust the TGM to predict \"cat.\" Formal: We add a linear layer to the model, which helps us understand the input more clearly. Formal: We calculate the learned parameters using cross-entropy. Formal: We use a linear layer with cross-entropy to get the hidden parameters. Formal: We feed these parameters into a linear layer with cross-entropy, which gives us the parameters for the model. Formal: This is similar to LSTM-based models. Formal: We use a linear layer with cross-entropy to get the learned parameters. Formal: Finally, we combine the learned parameters with a linear layer with cross-entropy to get the parameters for the model. Formal: This approach helps us get a clearer understanding of the input. Formal: We check the accuracy of the predictions using standard metrics, like BLEU, METEOR, and ROUGE-L. Formal: We use cross-entropy to calculate the hidden parameters. Formal: We use the model's parameters to calculate the parameters for the model. Formal: We also do some extra testing to see how well the model can learn. Formal: Formal: We run a bunch of tests using different language models and models to see how well the model can learn. Formal: We keep running the model until we're sure we've seen enough examples",
        "directory": "cmcl",
        "filename": "2021.cmcl-1.3.json"
    },
    {
        "casual_text": "The main idea here is to guess what a person will buy next based on what they just bought. We use a pre-trained BERT model and tweak it a bit to work for this specific task. We treat the item they just bought as \"sentence A\" and the one we're trying to predict as \"sentence B.\" Both item names are combined and cut short if they’re too long, keeping the total under 128 tokens, with special markers like [CLS] and [SEP] included. \n\nFor the item they just bought, we find items they also bought in the same shopping session to use as positive examples. For negative examples, we just pick some random items. With these positive and negative sets, we calculate the cross-entropy loss to help the model learn to predict what comes next.",
        "formal_text": "The goal of this task is to predict the next item a user is going to purchase given the seed item he/she has just bought. We start with a pre-trained BERT base model, and fine-tune it for our next purchase prediction task. We feed seed item as sentence A, and target item as sentence B. Both item titles are concatenated and truncated to have at most 128 tokens, including one [CLS] and two [SEP] tokens. For a seed item, its positive items are generated by collecting items purchased within the same user session, and the negative ones are randomly sampled. Given the positive item set I p, and the negative item set I n, the cross-entropy loss for next purchase prediction may be computed as",
        "GPT2_formal_text": "] = {q_i, q_i+1, ..., q_i+k}. Formal: It’s pretty common to pick the smallest chunk of text that includes the input text, either from a single sentence or a whole collection of sentences. That’s why we use a beam-search to find the best chunk of text. Formal: We go through the sentence in a random order and pick the most relevant parts (like the stuff that’s most important) to create the summary. For each reference summary, we pick the segment that’s closest to the input text. This way, the summary’s length doesn’t depend on where the input text is in the sequence. Formal: We start by generating a random (but usually pretty good) summary for the target language (English) using the target-language model (check out section 2.1). For each reference summary, we pick a segment that’s closest to the input text and put it in the summary. This way, we keep the summary length the same for both the target-language model and the reference summaries. Formal: Then, we match the summary’s language to the reference one using the language model we generated. Formal: After that, we use the reference summary’s representation to generate the target language summary. Formal: The main thing we’re doing here is using the English reference summary (let’s call it x) to create the target language summary (x). Formal: We also use the target language model (x) to give the summary back to the language model (x). Formal: To train the model, we take the reference summary’s representation (x) and plug it into a linear model (shoutout to Meng and Clark, 2016). Formal: After that, we have to pick the most relevant segments from the source sentence (y) to create the summary’s representation. Formal: Finally, we put all the transformed representations together using the linear model and run it through a CRF layer. Formal: We picked the best output for the text length (λ) based on how much it helped improve the translation quality compared to the original reference summary. Formal: Our method can create reference summaries in a bunch of different languages. Formal: Also, we’ve made sure that the summary is length-related by using the length-sensitive CRF layer",
        "directory": "ecnlp",
        "filename": "2020.ecnlp-1.8.json"
    },
    {
        "casual_text": "We use a margin threshold to figure out if x and y are mutual translations. Research shows it works better than the usual cosine similarity for identifying correct translation pairs (Artetxe and Schwenk, 2019a).",
        "formal_text": "A margin threshold is applied to decide whether x and y are mutual translations or not. It has been shown to be more consistent than the standard cosine similarity in determining correct translation pairs (Artetxe and Schwenk, 2019a).",
        "GPT2_formal_text": "ed and attention_type were the main focus of the research. We looked at how these parts interact by doing numerical comparisons, which is basically how different methods perform in terms of classification accuracy. Just to clarify, the results we reported are the average of three runs, and they included the results from both the naive and greedy approaches.\n\nThe results from greedy attention-based models are in Figure 2. As you can see, greedy attention actually helps out more on the classification task than naive methods. This shows that the greedy approach has some cool benefits, especially when it comes to handling long inputs and the huge number of possible classes. However, greedy attention has a downside: it can't pick the best option based on the data. Formal: Here, E_t represents the number of nodes in the target domain. The code for this can be found at https://github.com/JadeLai/spam-detector-parsing. Formal: We use a classifier that takes in the input x and the target domain Y as input, along with some parameters θ. Formal: Here, T_t is the classifier's input to figure out the domain, and θ is the parameters of the classifier. The code for this can be found at https://github.com/Liu-ChenZhao/sentence-to-sentence-encoder. Formal: In this experiment, we're checking how well different models do with domain-specific information. Formal: Since the data distribution can vary a lot, we set up a simple framework for classifying text in the target domain. Formal: From the results, we see that the naïve approach struggles with dealing with the messy data in the target domain. Formal: For classifying text in the target domain, we use a Transformer encoder-decoder model (Vaswani et al., 2017) with a Bi-LSTM layer setup. Formal: The Transformer model was trained on a dataset with around 2.7 billion words. Formal: For the first run, we use a linear layer with 10 units, and for the second run, we use a linear layer with 100 units. Formal: The results from the greedy attention-based model are in Figure 3. Formal: We also tried a simpler approach using an LSTM layer. Formal: In this case, the LSTM layer was trained using a dataset with about 50 million words. Form",
        "directory": "bucc",
        "filename": "2020.bucc-1.6.json"
    },
    {
        "casual_text": "Alright, let's break this down in a more casual way.\n\nFor the Paraphrase Identification task, we used the PAWS-X dataset, which was introduced by Yang et al. in 2019. This dataset has around 24,000 human-translated pairs for evaluation and about 296,000 machine-translated pairs for training, covering 7 languages: English, Spanish, French, German, Japanese, Chinese, and Korean. However, we didn't use the Korean part because, during our initial experiments, we couldn't get the same results as in the original paper by Yang et al. We think there might have been an issue with the encoding when using the BERT multilingual model for Korean.\n\nMoving on to Sequence Tagging, we ran some experiments on Named Entity Recognition (NER) using the CoNLL 2002 and CoNLL 2003 datasets, which were created by Tjong Kim Sang in 2002 and Tjong Kim Sang and De Meulder in 2003, respectively. Following the approach of Rahimi et al. in 2019, we combined these two datasets into one, covering 4 languages: English, Spanish, German, and Dutch. The combined dataset has 51,821 sentences for training, 11,344 for validation, and 13,556 for testing. Each sentence is labeled with entities like Person, Location, Organization, and Miscellaneous.",
        "formal_text": "Sentence-Pairs Classification. We adopted the PAWS-X dataset (Yang et al. , 2019 ) for the Paraphrase Identification task. The dataset is composed of about 24, 000 human translated evaluation pairs and about 296, 000 machine translated training pairs over 7 languages: English, Spanish, French, German, Japanese, Chinese, Korean. We actually didn't used the Korean languages, as in preliminary experiment we were not able to reproduce the results of the (Yang et al. , 2019) paper. We suspect a problem in the encoding affected our results in this language with the bert multilingual model. Sequence Tagging. We reported experiments on Named Entity Recognition (NER) using the CoNLL 2002 (Tjong Kim Sang, 2002) and CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) datasets. We merged the two datasets as in Rahimi et al. (2019) to obtain a single dataset over 4 languages, i. e. , English, Spanish, German and Dutch. The dataset contains 51, 821/11, 344/13, 556 annotated sentences, respectively for train, validation and test. Each sentence has been annotated with respect to the following entities: Person, Location, Organization and Miscellaneous.",
        "GPT2_formal_text": "= (source, target) tuples = (source, target) tuples = (source, target) 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149 150 151 152 153 154 155 156 157 158 159 160 161 162 163 164 165 166 167 168 169 170 171 172 173 174 175 176 177 178 179 180 181 182 183 184 185 186 187 188 189 190 191 192 193 194 195 196 197 198 199 200 201 202 203 204 205 206 207 208 209 210 211 212 213 214 215 216 217 218 219 220 221 222 223 224 225 226 227 228 229 230 231 232 233 234 235 236 237 238 239 240 241 242 243 244 245 246 247 248 249 250 251 252 253 254 255 256 257 258 259 260 261 262 263 264 265 266 267 268 269 270 271 272 273 274 275 276 277 278 279 280 281 282 283 284 285 286 287 288 289 290 291 292 293 294 295 296 297 298 299 300 301 302 303 304 305 306 307 308 309 310 311 312 313 314 315 316 317 318 319 320 321 322 323 324 325 326 327 328 329 330 331 332 333 334 335 336 337 338 339 340 341 342 343 344 345 346 347 348 349 350 351 352 353 354 355 356 357 358 359 360 361 362 363 364 365 366 367 368 369 370 371 372 373 374 375 376 377 378 379 380 381 382 383 384 385 386 387 388 389 390 391 392 393 394 395 396 397 398 399 400 401 402 403 404 405 406 407 408 409 410 411 412 413 414 415 416 417 418 419 420 421 422 423 424 425 426 427 428 429 430 431 432 433 434 435 436 437 438 439 440 441 442 443 444 445 446 447 448 449 450 451 452 453 454 455 456 457 458 459 460 461 462 463 464",
        "directory": "acl",
        "filename": "2021.acl-short.106.json"
    },
    {
        "casual_text": "Unlike earlier features, lexical vectors are calculated offline and don’t get updated during training. In our experience, it’s helpful to apply a MinMax scaler that scales the values to a range of [-1, +1] for each LS vector we create. So, something like [. . , 0.095, . . , 0.20, . . , 0.76, . . ] gets transformed into [. . , -1, . . , -0.67, . . , 1, . . ].",
        "formal_text": "Contrarily to previous features, lexical vectors are computed offline and are not adjusted during training. We found useful in practice to apply a MinMax scaler in the range [−1, +1] to each LS vector we computed; thus, [. . , 0. 095, . . , 0. 20, . . , 0. 76, . . ] becomes [. . , −1, . . , −0. 67, . . , 1, . . ].",
        "GPT2_formal_text": "vector; L_vec = attention_vector; b_vec = bias_vector; W_vec = word_embedding; W_vec = W_vec + sigmoid_sum; b_vec = bias_vector; b_vec = b_vec + sigmoid_sum; }\n\nBasically, we're defining two types of attention: one is a simple one and the other is a fancy version. The simple attention gets some simple weights and uses something called sigmoid to smooth things out. The fancy attention, on the other hand, uses something called softmax to handle the more complex stuff. Formal: We think this setup works pretty well because it balances the benefits of both. We can also tweak it to handle other types of attention. Formal: Also, we tried using the linear transformation to create a 1D attention matrix, but it didn't really make things better. Formal: Another option is to just use a uniform prior, but this means we lose some useful info. So, we use a quadratic or linear function to make sure we get the best of both worlds. Formal: Another option is to just use a uniform prior, but this means we lose some useful info. So, we use a quadratic or linear function to make sure we get the best of both worlds. Formal: Even though both methods use the same weights, they use different matrices. In the first one, the weights are normalized by dividing them by the sum of a lot of elements. In the second one, they're normalized by the sum of a lot of elements. Formal: So, the more elements we have, the more the loss gets influenced by them. Formal: For each loss term h_z_v, we calculate something called the log probability of the word w_z_v happening based on the context c_v and the attention mask h_z_v. Formal: We use cosine similarity to measure how similar these two vectors are. Formal: To give more weight to the context, we do a little linear interpolation to get the attention vector. Formal: Formal: This gives us the attention vector we get from the linear interpolation, which we use to calculate the similarity between the vectors. Formal: Formal: Formal: Here, W_vec is the word embedding, and b_vec is the bias vector for the word embedding. Formal: Formal: Formal:",
        "directory": "C18",
        "filename": "C18-1161.json"
    },
    {
        "casual_text": "In this project, we're diving into differentiable natural logic models that blend natural logic with neural networks. The goal is to stick to the core of inference based on natural logic but also bring in subsymbolic vector representations and some neural network elements. Mixing the strengths of neural networks with natural logic comes with a few challenges. There are two main issues we need to tackle: 1) Figuring out how (and where) to use the power of neural networks within the natural logic framework, and; 2) Dealing with the lack of intermediate supervision when training smaller parts, which can cause problems like the spurious issue in end-to-end training (as mentioned by Guu et al., 2017, and Min et al., 2019).",
        "formal_text": "In this work we explore differentiable natural logic models that integrate natural logic with neural networks, with the aim to keep the backbone of inference based on the natural logic formalism, while introducing subsymbolic vector representations and neural components into the framework. Combining the advantages of neural networks with natural logic needs to take several basic problems into consideration. Two problems flow directly from this objective: 1) How (and where) to leverage the strength of neural networks in the natural logic formalism, and; 2) How to alleviate the issue of a lack of intermediate supervision for training sub-components, which may lead to the spurious problem (Guu et al. , 2017; Min et al. , 2019) in the end-to-end training.",
        "GPT2_formal_text": "= {'n_i', 'e_i', 'e_j', 'p_i', 'p_j', 't_i', 't_j'}, n_skips = 5, weight = 1e-5, context_size = 64, vocabulary = 30, epoch = 3.5, regularization_factor = 1e-5). Formal: We use LTF to handle the left-to-right decoding. The final output gets turned into a scalar called e_n. Formal: For the right-to-left decoding, we just use LTL to encode the document. Formal: In this setup, the decoder is written in LTF and is trained using cross-entropy loss. Formal: We train both LTF and LTL using cross-entropy loss for both the word and document levels. Formal: To deal with the issues mentioned in Section 2, we’ve come up with a simple fix called the \"hyper-parameter\" approach. Here’s how it works. Formal: We randomly pick the hyperparameters for both LTF and LTL using a Monte-Carlo distribution. Formal: To train the LTF decoder, we use an early stopping method based on the sampling rate φ (which is a function of the probability of an observed value). This stops the decoder from overfitting, especially when we’re dealing with low-frequency words and a vocabulary size of 30. Formal: To keep the LTF decoder from overfitting, we also use a hyperparameter called α. Formal: To fine-tune the LTF decoder, we tweak the training set by gradually reducing the gradient of the loss with respect to α. Formal: We train the LTF decoder using cross-entropy loss, which helps us get a better understanding of the parameters. Formal: For the LTL decoder, we have a flexible approach. Formal: To minimize the cross-entropy loss, we add a random bias λ to the prior distribution p_i for the LTL decoder, which helps us understand the parameters better. Formal: We use LTF and LTL for both decoding. Formal: For more details, check out Appendix A. Formal: We test our model on the Word2Vec and FastText datasets. Formal: For FastText, we pick a vocabulary size of 30 and",
        "directory": "coling",
        "filename": "2020.coling-main.101.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\n- If you add **x₁** to **C(s, t)**, you get **C(s)**.\n- If you add **x₂** to **C(s, t)**, you get **C(t)**.\n- If you add **x₄** to **C(s)**, you get **N**.\n- If you add **x₅** to **C(t)**, you also get **N**.\n- And if you add **x₂** and **x₃** together, you get **x₄**.\n\nSo, in short:\n- **C(s, t) + x₁ = C(s)**\n- **C(s, t) + x₂ = C(t)**\n- **C(s) + x₄ = N**\n- **C(t) + x₅ = N**\n- **x₂ + x₃ = x₄**",
        "formal_text": "C(s, t) + x 1 = C(s), C(s, t) + x 2 = C(t), C(s) + x 4 = N, C(t) + x 5 = N, and x 2 + x 3 = x 4. tt s C(s, t) x 1 C(s) s x 2 x 3 x 4 C(t) x 5 N",
        "GPT2_formal_text": "ed, attention_packed, and attention_relu features, respectively. This is the final setup for the new model, which we'll use in this paper. Formal: The training data for this model includes the same number of training samples as the original model. But instead of the original, we’ve got our own set of samples, which we call test sets. Formal: For the knowledge-based ASR system, we’ve got 4 different learning rates (p_train, p_val, p_valmax, p_num), a default vocabulary of 20,000 tokens, and a sentence length limit of 100. Formal: We trained the model using 1 million sentence pairs from Wikipedia, plus another 10 million pairs from the ASR corpus, totaling around 1.7 billion tokens. Formal: For the knowledge-based ASR system, we used the same settings as in the original paper. For the BLEU score, we used 300-dimensional scores for the word embeddings, the word vector, and the RNN parameters, which were trained on the same development set. Formal: Formal: The training was done using beam search with a beam size of 5. Formal: The test set has 1 million sentence pairs, and we averaged them out across all trials. Formal: The test set is a mix of sentences from Wikipedia. Formal: The test set was created using beam search with a beam size of 5, a length penalty of 0.25, and a max-margin of 0.25. Formal: We tested our new model on the development set using these parameters. Formal: We tested our new model on the test set using the parameters we learned from the training set. Formal: We also looked at the results from the sentence embeddings and the word vectors to see how they compare to those in the original paper. Formal: The performance here is basically a mix of the results from the word embeddings and the word vector. Formal: Formal: We ran 10,000,000 random tests, and the average of the results is shown in Figure 2. Formal: We also included the results from the BLEU score, which is shown in Figure 2b. Formal: The performance here is basically a mix of the results from the word embeddings and the word vector. Formal: Formal: Formal: We ran 10,000,000 random tests, and the average of the",
        "directory": "amta",
        "filename": "2012.amta-papers.28.json"
    },
    {
        "casual_text": "DATR was first introduced by Evans and Gazdar (1989a; 1989b) as a straightforward, nonmonotonic language for handling lexical inheritance hierarchies. Basically, a DATR hierarchy is set up using path-value pairs. This inheritance of values helps capture useful generalizations and cuts down on repeating the same information over and over. Plus, it has a simple default system that keeps descriptions neat and tidy, but still lets you note any exceptions to the inherited info in a pretty natural way.",
        "formal_text": "DATR was originally introduced by Evans and Gazdar (1989a; 1989b) as a simple, nonmonotonic language for representing lexical inheritance hierarchies. A DATR hierarchy is defined by means of path-value specifications. Inheritance of values permits appropriate generalizations to be captured and redundancy in the description of data to be avoided. A simple default mechanism provides for concise descriptions while allowing for particular exceptions to inherited information to be stated in a natural way.",
        "GPT2_formal_text": "= {[CLS], [E1], [E2], [E3], [E4], [E5]}; G_v = (v_1, v_2, ..., v_n);\n\nIn this setup, we use a greedy search to find the best translation between the source and target languages. We use the English and German source sentence pairs from the WMT'19 shared task (Och and Ney, 2003) as our training data, following the method from Wang et al. (2017). For each sentence pair, we create a linear projection and then add this linear projection to the target language sentence pairs. After that, we train a model using this multilingual cross-lingual transfer model. Formal: We use a cross-lingual transfer model to create an English and German sentence pair. The model is trained using the source-language training data and the German and English target sentence pairs. Formal: The English and German source sentences are combined into a big vector and fed into a linear layer. Then, this vector is used to train a linear layer on the combined data. Formal: After that, the final linear layer is applied to the English and German target sentence pairs. Formal: After adding the linear layer, the output of the linear layer is used to create the final English and German sentence pair. Formal: We train a multilingual model using the combined cross-lingual data from the WMT'19 shared task. The final input vector is used to create a linear layer. Formal: After training, the model combines the source and target language sentences. Formal: We also use the modified stochastic gradient descent to fine-tune the model. Formal: After fine-tuning, we use the target language sentences as the training data to train the model. Formal: The final output vector is used to create a linear layer on the combined cross-lingual data. Formal: After fine-tuning, we apply the linear layer to the source and target language sentences. Formal: Finally, we apply the modified stochastic gradient descent to fix any leakage bias in the linear layer. Formal: We also use the modified stochastic gradient descent to fine-tune the model. Formal: Formal: The final output vector is used to create a linear layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C96",
        "filename": "C96-2109.json"
    },
    {
        "casual_text": "\"The Extra Girl\" was made by Mack Sennett. b. It came after some earlier movies that were about the movie business. c. Plus, it kind of set the stage for later films that talked about Hollywood.",
        "formal_text": "[\"The Extra Girl\"] [was] produced by Mack Sennett. b. [\"The Extra Girl\"] followed earlier films about the film industry. c. [\"The Extra Girl\"] also paved the way for later films about Hollywood.",
        "GPT2_formal_text": "_i, o_t_i, c_t_i, and h_t_i are all vectors with i rows and j columns, respectively. Formal: So, R_y(y) is the sequence representation of y at layer i, R_z(x) is the sequence representation of x at layer i, and H(x) is the harmonic mean of the attention weights for each layer. Formal: We measure the relevance of the output at layer i to the question using a linear-time process. Formal: We take the cross-entropy loss across all the words in the question and average it out across all the words in the answer sentence. Formal: We use cosine similarity to compare the relevance of each word in the question and answer sentence. Formal: We just add up the cosine similarity scores for the words in the question and the answer. Formal: We use something called L1-norm for the cross-entropy loss. Formal: We use a linear-time process to compare the relevance of the output at each layer across all the words in the question. Formal: We use the L2 norm for the cosine similarity. Formal: We use the L2 norm to compare the relevance of the output at each layer across all the words in the answer sentence. Formal: The output representation at layer i is calculated using cross-entropy loss. Formal: We use the cosine similarity to compare the relevance of each word in the question and answer sentence. Formal: We use L1-norm for the cross-entropy loss. Formal: We use the L2 norm to compare the relevance of the output at each layer across all the words in the answer sentence. Formal: We use the L2 norm to compare the relevance of the output at each layer across all the words in the answer sentence. Formal: We use the L2 norm to compare the relevance of the output at each layer across all the words in the answer sentence. Formal: We use the L2 norm to compare the relevance of the output at each layer across all the words in the answer sentence. Formal: We use the L2 norm to compare the relevance of the output at each layer across all the words in the answer sentence. Formal: We use the L2 norm to compare the relevance of the output at each layer across all the words in the answer sentence. Formal: We use the L2 norm to compare",
        "directory": "C08",
        "filename": "C08-1043.json"
    },
    {
        "casual_text": "The correct classification is represented by y_i. The generator's loss function is the same as the one in the original paper, as shown in equation 2.",
        "formal_text": "where y i is the correct classification. The generator loss function is the same as the original as shown in equation 2.",
        "GPT2_formal_text": "= {x_i, y_i, z_i} model = T_W_model(x_i, y_i, z_i) end end end class_id(x_i, y_i, z_i) end end end #  \n\nThis method works because the model can't pick the wrong answer if it doesn't have any data to work with. Formal: We use a few rules to figure out the right answer. Formal: For each question, we need to assign a label to each possible answer. Formal: For each example, we first need to assign a label to the correct answer. Then, we look at all the possible answers and figure out their labels. Formal: For each answer, we need to assign a label to each possible label. Formal: Lastly, we check how similar each label is to the correct answer's label. Formal: Formal: For every question, we assign a label to each possible answer. Formal: For each example, we first assign a label to the correct answer. Formal: Finally, we check how similar each label is to the correct answer's label. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C18",
        "filename": "C18-1040.json"
    },
    {
        "casual_text": "In the world of combinatorial optimization, problem (4), which takes input B, is called the Linear Ordering Problem. This problem pops up in all sorts of real-world situations, like in economics, sociology, graph theory, drawing graphs, archaeology, and even when scheduling tasks (Grötschel et al., 1984). Researchers often test it out on real data using \"input-output\" matrices that show how resources move between different sectors of the economy (Schiavinotto and Stützle, 2004).",
        "formal_text": "In the combinatorial optimization literature, the maximization problem (4) (with input B) is known as the Linear Ordering Problem. It has numerous practical applications in fields including economics, sociology, graph theory, graph drawing, archaeology, and task scheduling (Grötschel et al. , 1984). Computational studies on real data have often used \"input-output\" matrices representing resource flows among economic sectors (Schiavinotto and Stützle, 2004).",
        "GPT2_formal_text": "ed_dummies and attention_masked_unigram_dummies. Formal: We've got two models: \n\n1. **RNNLM**: This model takes a sentence as input, turns it into a vector, and then uses a linear layer to predict the next word.\n\n2. **AR Transformer**: This one uses an attention mechanism to pull out a representation for each word.\n\nWe use these models to create a new dataset called **w** and a new task called **DT**. Formal: The dataset we create has 50,000 examples for each of the 50 languages, and we split them into training, validation, and testing sets. Formal: For the new task **DT**, we pick 10,000 examples from the dataset that are already labeled, and for the previous task **w**, we grab 10,000 examples from the training set. Formal: The dataset we use is **w**, with each example from the previous task labeled as **(w 1, . . . , w N )**.\n\nWe set a weight matrix **W_k** for each word in the source and target languages (like how the source and target words are related). This helps the model learn representations that match up with the target words. Formal: We also give the model **W_k** that's the same size as **W** but with a different weight for each word. Formal: We do this by subtracting the weights from each word in the target language. Formal: When we compare these two representations, we see that using **W** and **W** as the same size gives us a good result. Formal: We use this setup to train two models: \n\n1. **RNNLM**: This model takes a sentence as input, turns it into a vector, and then uses a linear layer to predict the next word.\n\n2. **AR Transformer**: This one uses an attention mechanism to pull out a representation for each word.\n\nWe test these models on the test set to see how well they perform. Formal: We created a new dataset called **w** by picking 10,000 examples from the target language and a similar amount of training data from the source language. Formal: The test set has 50,000 examples for each of the 50 languages, and we split them into training, validation, and testing sets. Formal: We also",
        "directory": "D09",
        "filename": "D09-1105.json"
    },
    {
        "casual_text": "We dug into the backgrounds of all 145 key employees by doing web searches and checking out public records or articles about them. To make sure we were looking at the right person, we added the word 'Enron' to our search terms. In the public records we found for each employee, we paid attention to any clues about their gender, like pronouns (he/him/his or she/her) or titles (Mr., Mrs., Ms., Miss). Since these folks were big shots at Enron during its bankruptcy, it wasn’t too hard to find info about them. For instance, the page we found on Kay Mann clearly shows she’s a woman. Using this method, we managed to figure out the gender of all 145 employees. One cool thing about doing it this way is that it gives us 100% confident gender assignments for everyone in our data set.",
        "formal_text": "We researched each of the 145 core employees using web search and found public records about them or articles referring to them. In order to make sure that the results are about the same person we want, we added the word 'enron' to the search queries. Within the public records returned for each core employee, we looked for instances in which they were being referred to either using a gender revealing pronoun (he/him/his vs. she/her) or using a gender revealing addressing form (Mr. vs. Mrs. /Ms. /Miss). Since these employees held top managerial positions within Enron at the time of bankruptcy, it was fairly easy to find public records or articles referring to them. For example, the page we found for Kay Mann clearly identifies her gender. 2 We were able to correctly determine the gender of each of the 145 core employees in this manner. A benefit of manually determining the gender of these core employees is that it ensures a high coverage of 100% confident gender assignments in the corpus.",
        "GPT2_formal_text": "= {t_i, t_j, ..., t_n, m_i, m_j} n_t = {t_i, t_j, ..., t_n, m_i, m_j} l_t = {l_i, l_j, ..., l_n, l_m} Formal: 2, 3, ..., n_i, l_n, m_i, m_j. Formal: Formal: t_i, t_j = t_i * (t_i + 1) + t_j * (t_j + 1) Formal: Formal: t_i, t_j = 1 + t_j * t_i Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1211.json"
    },
    {
        "casual_text": "Over those twelve years, more than 1,100 journalists and media workers were killed while doing their jobs. Some were targeted because someone didn’t like what they wrote or said, while others were in the wrong place at the wrong time. Table 5 shows an example sentence in Arabic Broadcast News, with the original text in Iraqi Colloquial Arabic (ICA) and Modern Standard Arabic (MSA). It also includes translations done using rule-based MT (LFG), statistical MT (SMT), and hybrid MT (HMT).",
        "formal_text": "During the twelve years which passed more than one thousand and one hundred journalists and media workers performing their duty, were killed because there was one who did not like what they wrote or what they said or because they were in a place and time they should not be present at. Table 5. Example sentence Arabic Broadcast News; Original in Iraqi Colloquial Arabic (ICA) and Modern Standard Arabic (after normalization; MSA) and the translations using rule-based MT (LFG), statistical MT (SMT) and hybrid MT (HMT).",
        "GPT2_formal_text": "ed: {type: 'text'}  \nRNNLM, introduced by Collobert et al. in 2011, is a top-notch transformer model that helps generate natural language responses in the field of natural language processing. It has three main parts: the transformer encoder, the transformer decoder, and the decoder's hidden layers. Formal: We trained the model using all the data points from the training set. Formal: We also tested it on a smaller dataset called 'BWVCS1' from the CoNLL-2013 shared task. Formal: The results show that our model outperforms the current best method for the CoNLL-2013 shared task. It also sets new records on the CNNDM task. Formal: Since the responses we generate are kind of messy, we used a separate classifier to label the generated text. Formal: Formal: For future work, we plan to tweak the transformer model to work better for text generation. Formal: We also made a quick tool to make generating responses faster. Formal: You can find the full details of the datasets and tasks in Table 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "amta",
        "filename": "2010.amta-papers.5.json"
    },
    {
        "casual_text": "The other stuff—like adverbs, where they go, tenses, helping verbs, active or passive voice, and negatives—gets stored in something called a \"history.\" This history has a section for each sentence that matches the T-expression we're looking at. When we look up the T-expression in our knowledge base, we link up its three parts and add the history H to it. So, you can think of the final entry in the knowledge base as a kind of \"condensed version\" of the sentence's grammar in English.",
        "formal_text": "The remaining parameters--adverbs and their position, tense, auxiliaries, voice, negation, etc. --are recorded in a represen. rational structure called history. The history has a page pertaining to each sentence which yields the given T-expression. When we index the T-expression in the knowledge base, we cross-reference its three components and attach the history H to it. One can thus think of the resulting entry in the knowledge base as a \"digested summary\" of the syntactic structure of English sentences.",
        "GPT2_formal_text": "and meta_ids, respectively. Formal: We grab entities from the target language using a cross-lingual approach, which helps us get the names of the entities. Formal: For each of the five target languages, we run a 10-fold cross-validation on the training data. The dataset has around 800k training examples and 4k for development. Formal: The training and development sets for the five target languages are called training_1, training_2, ..., training_k, and development_1, development_2, ..., development_k. We grab the names of the entities (or names of their types) from the target language's training set (i.e., training_1, training_2, ..., training_k) using cross-lingual approaches. Formal: We tweak the usual dataset selection method by trying out three different ways to make sure the training data is high-quality, differentiable, and not too hard. We pick the best one based on how well it performs on the development set and how much data we have to work with. Formal: For each dataset, we first check if the entity names in the training set (i.e., training_1, training_2, ..., training_k) match up with names in the target language's training set (i.e., training_1, training_2, ..., training_k). If they do, we use that matching name to train a model using the target language's training set (i.e., training_1, training_2, ..., training_k). If they don’t match, we just use the original names. We then decide if the entity names in the training set are really the same as the target language's names. Formal: We also use the results from the validation set to adjust our selection method, which we call Selection Algorithm 1. Formal: The Selection Algorithm 1 picks the best model based on how well it performs on the validation set. Formal: The Selection Algorithm 2 picks the best model based on how well it performs on the development set. Formal: The Selection Algorithm 3 picks the best model based on how well it performs on the training set. Formal: The Selection Algorithm 4 picks the best model based on how well it performs on the validation set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C88",
        "filename": "C88-1065.json"
    },
    {
        "casual_text": "Warrens (2010c) talks about another popular measure called the Odds Ratio, which is written as ad/bc. This measure is often used in Epidemiology, not in fields like Computer Science or Computational Linguistics. Another closely related concept is the Determinant of the Contingency Matrix, which is dtp = ad-bc = etp-etn. This is based on independent marginal probabilities, as used in Chi-Sqr, Cohen, and Powers methods. Both the Odds Ratio and the Determinant help figure out if the odds of positive cases are higher for the first rater (the real one) compared to the second rater (the predicted one). For the Odds Ratio, this means the value should be greater than one, and for the Determinant, it should be greater than zero. It's worth noting that if you take the logarithm of all the coefficients, the relationship stays the same. The difference of these logarithms is equal to the logarithm of the ratio, which ties it all to the information domain.",
        "formal_text": "Warrens (2010c) discusses another commonly used measure, the Odds Ratio ad/bc (in Epidemiology rather than Computer Science or Computational Linguistics). Closely related to this is the Determinant of the Contingency Matrix dtp = ad-bc = etp-etn (in the Chi-Sqr, Cohen and Powers sense based on independent marginal probabilities). Both show whether the odds favour positives over negatives more for the first rater (real) than the second (predicted) -for the ratio it is if it is greater than one, for the difference it is if it is greater than 0. Note that taking logs of all coefficients would maintain the same relationship and that the difference of the logs corresponds to the log of the ratio, mapping into the information domain.",
        "GPT2_formal_text": ", g_attn_attn_mask, ..., g_attn_mask. Formal: We turn those attention vectors into individual attention matrices using something called binary cross-attention. The attention vectors h_i, j can be either uniform or differentiable, depending on how they're split up. Formal: For the attention-based Transformer model, we break down the input into n-grams using a filter of length n. Formal: In the sketchy scene captioning task, we assign each word in a scene to a caption vector. Formal: For this task, we get a sequence of these attention vectors. Formal: For each word in the sketchy scene captioning task, we check how important it is for the caption. To get a vector called z_s that represents the image of the word w_t in the sketchy scene, we use this formula. Formal: In the sketchy scene captioning task, we first figure out the hidden representations of the words in the sketchy scene. Formal: We then assign each word in the scene to a vector that represents the image in the sketchy scene. Formal: This process is repeated for each word in the scene, as shown in Figure 3. Formal: We use a multinomial log-linear model, which we train on the sketchy scene captioning dataset. Formal: We then use this model to create a new vector called z_t for the image in the sketchy scene. Formal: For the caption-to-text model, we first generate a sequence of the caption's words from the sketchy scene captioning dataset. Then, we grab the vector from the sketchy scene captioning dataset for each word in that sequence. Formal: We then use this model to generate a new vector for each word in the sketchy scene captioning sequence. Formal: This process is repeated for each word in the sequence. Formal: Finally, we use this model to create the new caption text. Formal: The attention weights for this model are adjusted based on the attention weights we get from the sketchy scene captioning dataset. Formal: These weights are adjusted for different types of text data. Formal: These weights are adjusted for different types of text data. Formal: We figure out the attention weights for the word-to-text model using the attention weights we get from the sketchy scene captioning dataset. Formal: Formal",
        "directory": "E12",
        "filename": "E12-1035.json"
    },
    {
        "casual_text": "2. Repeatability, or measurement repeatability, is basically how precise a measurement is when you do it under the same conditions over and over again.",
        "formal_text": "2. 21 measurement repeatability (or repeatability, for short) is measurement precision under a set of repeatability conditions of measurement.",
        "GPT2_formal_text": "= { u1, u2, ..., uK } n = { 1, 2, 4, 8, 16, 32, 64, 128 }. Formal: Let's say xi is the starting token, xj is the ending token, and xi_j -1 is the token before and after it. Formal: A_ij is just the i-th part of the i-th token. Formal: For an input xi, A_ij = (xi_1, xi_2, ..., xi_K) is just the list of vector representations (h_ij) for all the tokens in xi. Formal: Also, the token attention matrix A_ij is the sum of the input attention vectors from the i-th token. Formal: This is a way to create a vector representation for each token, where each vector is a mix of the input attention vectors and the token's own vector representation. Formal: Basically, we want the vector representations (h_ij) to focus on the token's vector representation (h_i). Formal: We do this by adding up the attention vectors (A_ij) of the tokens in xi, which we then use to calculate the token representation (h_i). Formal: The final vector representation for a token is made by multiplying the vector representation of its input with the attention vector of the token. Formal: Formal: The attention vector A_ij is calculated as the sum of the vector representations of each token in xi, which we then use to calculate the token representation (h_i). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.2.json"
    },
    {
        "casual_text": "For our experiments, we used two special datasets that other researchers have used before to find matching terms in two languages for technical stuff.",
        "formal_text": "For our experiments, we used two specialized comparable corpora that have been used in previous works for bilingual terminology extraction in technical domains:",
        "GPT2_formal_text": "_t_shape; + + P_t_mask_t = [P_s_t_mask * P_t_t_shape + H_t_mask * H_t_t_shape]; + + P_mask_t = [P_s_t_mask * P_t_t_shape + H_t_mask * H_t_t_shape]; + + P_t_s = [P_t_s * P_t_t_shape + H_t_s * H_t_t_shape]; + + P_t_s_t = [P_s_t * P_t_s_t + H_s_t * H_s_t_shape]; + + P_t_s_t_t = [P_s_t * P_t_s_t + H_t_s * H_t_s_t_shape]; + + P_t_s_t_t_t = [P_t_s * P_t_s_t + H_s_t * H_s_t_t_shape]; + + P_t_s_t_t_t = [P_t_s * P_t_s_t + H_s_t * H_s_t_t_shape]; + + P_t_s_t_t_t = [P_t_s * P_t_s_t + H_s_t * H_s_t_t_shape]; + + P_t_s_t_t = [P_t_s * P_t_s_t + H_s_t * H_s_t_t_shape]; + + P_t_s_t_t_t = [P_t_s * P_t_s_t + H_s_t * H_s_t_t_shape]; + + P_t_s_t_t = [P_t_s * P_t_s_t + H_s_t * H_s_t_t_shape]; + + P_t_s_t_t = [P_t_s * P_t_s_t + H_s_t * H_s_t_t_shape",
        "directory": "C18",
        "filename": "C18-1242.json"
    },
    {
        "casual_text": "We're using D(iscourse) R(epresentation) T(heory) as our semantic representation, which was developed by Hans Kamp [4]. Basically, we're not going with the semantic theory for L(exical) F(unctional) C(grammar) suggested by Per-Kristian Halverson [2]. Halverson converts the functional structures of LFG into what he calls semantic structures, which are basically scyclic graphs. These semantic structures come from a translation process that connects formulas of intensional logic to the semantic forms found in the functional structure. \n\nWe’re not taking this approach for a reason, which we’ll explain by laying out some requirements that a semantic representation needs to meet to handle text processing effectively. Then, we’ll prove why these requirements are super important by looking at some example sentences and discourses.",
        "formal_text": "As senantic representation we use the D(iscourse) R(epresentation) T(heory) developped by Hans Yamp [4]. I. e. we do not adopt the semantic theory for L(exical) F(unctional) C~rammr) proposed by Per-Kristian Halverson [2]. Halverson translates the f~nctional structures of LFG into so-called semantic structures being of the same structural nature, namely scyclic graphs. The semlntin structures are the result of a translation procedure which is based on the association of formulas of intensional logic to the semantic forms appearing in the functional structure. The reason not to take this approach will be explained by postulating some requirements a se~anclc representation has to fulfill in order to account for a processing of texts. Tnen we will show that these requlr~ents are rP~I]y necessary by analysing some sample sente, ces and discourses.",
        "GPT2_formal_text": "are there to help figure out the topic in a query, like in e. Formal: The sentiment labels in the query help us guess the sentiment at the query level. Formal: The sentiment label for a sentence p j is always the same, like \"positive\" or \"negative.\" Formal: We also add some extra features to give the model some context info. Formal: For the first part, we check the sentiment of the words in the sentence that are part of the query by using their normalized sentiment scores. The second part is about the sentiment of words that aren’t in the query. Formal: The way we learn these features is shown in Equations 2 and 3. Formal: When we add more sentence-level features like the ones in (7) to this model, it improves its performance. Formal: This model can handle a huge number of queries, up to K times K. Formal: We tested it using 200K queries for each topic. Formal: Here’s an example query: Formal: Let’s say we have a sentence s and a set of topics C. The model can then figure out the sentiment for the whole document collection D by using the normalized sentiment scores of the topics. Formal: To make this happen, we create a special vector h for each topic and add it to the model. Formal: We use the topic vectors to learn features, and then we use those learned features to figure out the sentiment of the query sentence. Formal: In the future, we plan to do more experiments to see how well this model performs. Formal: We tested it on the English-French dataset from Figure 1. Formal: To get the best results, we train the model using cross-entropy loss. Formal: Using this loss, we made the model more robust and improved its performance. Formal: We also tried using the L2 regularization term. Formal: To make the model more accurate, we added the support vector regression (SVR) loss. Formal: In (6), the labels for the loss are given directly. Formal: For the sentiment labels, we use the normalized sentiment scores from the query sentence to train the model. Formal: We used the BERT model (Devlin et al., 2019) to train this model. Formal: This model can handle a huge number of queries, up to K times K. Formal: Formal: We tested it using",
        "directory": "E83",
        "filename": "E83-1009.json"
    },
    {
        "casual_text": "We introduced a new NLU task where the goal is to pick the best counter speech that can oppose a given input speech from a bunch of options.",
        "formal_text": "We presented a novel NLU task of identifying a counter speech, which best counters an input speech, within a set of candidate counter speeches.",
        "GPT2_formal_text": "_t = x_t * attention_mask_t, and y_t = y_t * attention_mask_t. Formal: Let's say we have a sentence x with n words, and we want to figure out how likely it is for it to be the head of an entity mention y. To do this, we check two things: the sequence of words we get from the sentence and the representation of the entity mention itself. Formal: Here's how the model works: First, it checks if the word x fits with the entity mention y. If it does, it gets a score of 1. If not, it gets a score of 0. Then, it combines this probability with the attention score x_t from the entity mention. If the combination of these two scores is higher than some threshold δ, it decides the head of the entity mention. Formal: For each entity mention x_i, the model does the following: First, it looks at the word x_i to see if it fits the mention y_i. If it does, it gets a score of 1. If not, it gets a score of 0. If the combination of these two scores is below δ, it decides the head of the entity mention. Formal: Lastly, the model decides the head of the entity mention by combining the probability of x_i and the attention score x_t. Formal: This process is repeated for the other mentions in the sentence. Formal: We tested the model on the Yelp dataset using different setups like the cross-entropy loss, a Gaussian distribution, and a linear model. Formal: We also created two versions of the model, called one-hot and two-hot, to help explain the different types of relation features we're looking at. Formal: In each setup, we set up different goals to measure how well the model is doing. Formal: We ran two experiments to see how the model is doing. Formal: 1. Formal: A. One-hot model. In this setup, we just focus on the head of the entity mention x_i, which gives us an F1 score. Formal: B. Two-hot model. In this setup, we also calculate a two-hot model score for the head of the mention x_i, which gives us an F1 score. Formal: Formal: Formal: For more info, check out Section 6. Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "Sure! So, pronominal anaphora (like \"he\" or \"she\") and named entities (like names of people or places) can be handled pretty well by systems, but when it comes to definite noun phrases (like \"the cat\" or \"the book\"), things get trickier. Usually, these systems can only handle cases where both mentions have the same noun, like \"the cat\" and \"the cat.\" This is what Vieira and Poesio (2000) call \"direct coreference.\" \n\nBut when we're dealing with what they call \"coreferent bridging\" (which is like when the mentions don't have the same noun but still refer to the same thing), it's way harder. This is because there are way more possible candidates to consider, and you can't just rely on the words looking similar on the surface.",
        "formal_text": "While the resolution of pronominal anaphora and tracking of named entities is possible with good accuracy, the resolution of definite NPs (having a common noun as their head) is usually limited to the cases that Vieira and Poesio (2000) call direct coreference, where both coreferent mentions have the same head. The other cases, called coreferent bridging by Vieira and Poesio 1, are notably harder because the number of potential candidates is much 1 Because bridging (in the sense of Clark, 1975, or Asher and larger when it is no longer possible to rely on surface similarity.",
        "GPT2_formal_text": "_m = (p_i, s_i, e_i), where p_i is the phrase, s_i is the sentence, and e_i is the entity mentioned in the sentence. Formal: For each entity e_i, we run it through a sequence-to-sequence (seq2seq) model to get its representation. Formal: We train our model using the BiDAF loss and a few other loss terms. The results are shown in Table 2. As you can see, the basic biDAF model does a pretty good job, but it's not great at capturing the important details from the entity. This makes it tough for the model to perform well on the test set. Formal: To fix this, we came up with a new BiDAF model called BiDAF-ADVP, which focuses on improving the representations of entities by looking at how they interact with each other. Formal: To figure out the most likely structure, we use a multi-head attention network. Formal: In Section 5.3, we talk about the text encoder, the word embedding layer, and the language model. Formal: We used the CNN/DailyMail dataset (Riedel et al., 2013) for the entity retrieval task and tweaked it to use the entity embedding info from the text embedding layer. Formal: We also used the BERT model (Devlin et al., 2019) for the entity retrieval task. Formal: The final embeddings are a mix of the embeddings from both the entity embedding layer and the text encoder. Formal: The entity retrieval task helps us get a better understanding of how entities interact with each other. Formal: Finally, in Section 5.4, we talk about the language model and how we adapt it to capture the most important context in the entity retrieval task. Formal: Formal: We set up a basic model to figure out the sequence of entity pairs, which we call the entity pair embedding (e_i). Formal: In Section 5.5, we show the basic model we trained using the BiDAF loss. Formal: The final output is a mix of the embeddings from both the entity embedding layer and the text encoder. Formal: Formal: We tested the model on the BERT-base model to get the final results. Formal: Formal: We also used the BERT-base model for",
        "directory": "D07",
        "filename": "D07-1052.json"
    },
    {
        "casual_text": "In the more flexible version of GPSG, they use unification to build trees. In the version from [GKPS85], the main category R of a smaller tree has to match up with a &mghter category C from a local tree (basically, R __. C and C ~ R).",
        "formal_text": "In the constructive version of GPSG, unification is used for tree formation. In the version of [GKPS85] the root category R of a subtree has to be identical with a &mghter category C of a local tree (i. e. R __. C and C ~ R).",
        "GPT2_formal_text": "is an embedding of the attention mask. We stick to a fixed number of dimensions and call the hidden state of the attention head h_t (e_t) = (h_1, ..., h_M). Formal: We use the usual RNN setup with input hidden state h_t. Formal: Instead of using the usual label attention mechanism, we swap it out with a different kind of attention called attention mask. This attention mask is turned into an embedding, which we call h_mask. The weights for the attention mask can be written like this: Formal: Also, we add another embedding, h_m, to the hidden state h_t. Formal: To make sure the model learns the right relationships between sentences, we make sure the attention weights (h_mask and h_m) are all the same. This means we don't mix up the attention mask with the sentence label. Formal: For the attention mask, we use the formula from Equation (6): Formal: For the attention head, we use the hyperparameters α_t, which we get from the model's hyper-parameters. Formal: For the sentence-level relation extraction, we use a multinomial distribution with a variance of d. Formal: We train the model with a learning rate of 0.001 and run it for 10 epochs. Formal: Finally, we calculate the cosine similarity between the model's output and the attention mask embeddings. Formal: To get the attention weight vector (h_m), we multiply the attention mask embeddings with the input hidden state h_t. Formal: We also calculate the cross-entropy loss for this relation extraction task. Formal: The final model output is the combination of the two representations: Formal: For the attention mask embedding, we get the embedding h_m by multiplying the attention mask embeddings with the input hidden state h_t. Formal: We train the model for a learning rate of 0.001 and run it for 10 epochs. Formal: Formal: To calculate the cross-entropy loss, we use the formula from Equation (6): Formal: For the attention head, we use the hyperparameters α_t, which we get from the model's hyper-parameters. Formal: For the sentence-level relation extraction task, we use a multinomial distribution with a variance of d",
        "directory": "C88",
        "filename": "C88-2151.json"
    },
    {
        "casual_text": "There are even more intricate differences to consider. For instance, Swahili has a more elaborate system with 18 classes. These systems are usually called 'noun classes' in the academic world and don't fall under the term 'grammatical gender' in this paper.",
        "formal_text": "More complex distinctions are also found. As an example, Swahili has a more complex system with 18 classes. These systems are generally referred to as 'noun classes' in the literature and are not covered by the term 'grammatical gender' in the current paper.",
        "GPT2_formal_text": "= { idx1, idx2, ..., idxK }, with d set to d_s. Formal: For the last question in the QA pair, we calculate the posterior probability using the cross-entropy loss function l_q(p, q) for both the question and the answer. Formal: We add the cross-entropy loss to each model, like this: Formal: In Section 4, we show how adding cross-entropy loss to a neural QA model can boost performance. Formal: We also did some experiments to see how the loss function l_q(p, q) affects things. Formal: We used a pre-trained BERT model called BERT-base (from Devlin et al., 2019) as our QA model, which is trained to predict the answer. The loss function l_q(p, q) is calculated like this: Formal: Here, p is the hidden state of the model, and q is the query, which we get from a query-response pair. Formal: We trained the model for 1,000 epochs using the training set (for example, 20 epochs for BERT-base on 20k examples). Formal: To check how well the model performs, we used accuracy (Acc) and F1 score, both from the development set. Formal: To make sure the model stays strong even with a small training set, we used early stopping. Formal: We trained the model with 1,000 epochs and added more epochs to stop the model from overfitting. Formal: The results in Table 3 show how the model performed on the test set, especially in the \"Answer = 'NULL'\" case. Formal: BERT is really good at generating answers, but it struggles with natural language questions that aren't structured like tables. Formal: For these questions, the model tries to guess the answer by looking at the question structure, but it doesn't fully grasp the natural language answer. Formal: From this, we believe that the answer is made up of different parts of the question, like the question itself. Formal: Lastly, we added a discriminative module to improve the model's performance. Formal: This module uses the knowledge from a knowledge base to improve the model's performance. Formal: Formal: This module uses a cross-entropy loss function to help the model figure out the best answer. Formal: Form",
        "directory": "conll",
        "filename": "2020.conll-1.20.json"
    },
    {
        "casual_text": "Lastly, like we talked about earlier, we get rid of substitution (Joshi and Schabes, 1997, footnote 6). After making these tweaks, the sample TAG grammar and the derivation tree from Figures 1 and 4(a) could be shown using the core TAG grammar and the derivation tree from Figures 2 and 4(c).",
        "formal_text": "Finally, as mentioned before, we eliminate substitution (Joshi and Schabes, 1997, fn. 6 ). With these changes, the sample TAG grammar and derivation tree of Figures 1 and 4(a) might be expressed with the core TAG grammar and derivation tree of Figures 2 and 4(c).",
        "GPT2_formal_text": "ed_40-dimensional_embeddings. For example, (+attention_masked_1d) is the attention set that includes 1d units, and (+attention_masked_40d) is the attention set that includes 40d units. Formal: We use three different attention structures in our approach: Bi-directional (b), Bi-directional (b), and Bi-linear (b). Formal: We go through each token in a sequence, calling the first head h_1, and the second head h_2, and we do this in a specific order. Formal: We also consider the direction of the input vector x and combine the input and output vectors using a linear transformation called T. Formal: The final attention for the whole sequence is a mix of the first two heads. Formal: We calculate the attention mask by subtracting the first head's representation (h_1) from the output vector (h_2), and then we add up the attention masks for the two heads. Formal: As a special case, we can also use a binary function that has two inputs: a positive vector (h_1, 1) and a negative vector (h_2, -1). Formal: These two inputs are the same as the ones in the attention mechanism, but we don't get into the nitty-gritty details here. Formal: For decoding, we use the beam search method from the Transformer model. Formal: For optimizing, we use Adam, with a learning rate set to 0.1. Formal: We also tweak the batch size to match the number of tokens, which we set to 5000 in our experiments. Formal: We also use Adam with a learning rate of 0.1 to optimize the cross-entropy loss. Formal: Lastly, we use softmax to average the attention weights. Formal: The hyper-parameters for our model are listed in Table 3. Formal: For each case, we check the validation performance using dev and test sets to see how well the model holds up. Formal: We also ran some experiments to see how the model performs during training and testing. Formal: We also noticed that using a smaller batch size works better for training. Formal: Formal: Formal: We tested the model with different values of M and B to see how they affect things. Formal: Formal: Formal: For training, we used",
        "directory": "E06",
        "filename": "E06-1048.json"
    },
    {
        "casual_text": "We're using 300-dimensional pre-trained GloVe 2 word embeddings (thanks to Pennington et al., 2014) that were trained on web data. We've got three different convolution filters—sizes 3, 4, and 5—each with 100 feature maps. The GRU's hidden states are set to 300 dimensions. For optimization, we're using the adam optimizer (props to Kingma and Ba, 2015) with an initial learning rate of 1.0 × 10^-4. The learning rate gets cut in half every 20 epochs during training. We've also set the dropout probability to 0.3.",
        "formal_text": "We use 300-dimensional pre-trained GloVe 2 (Pennington et al. , 2014) word-embeddings which is trained from web data. We use three distinct convolution filters of sizes 3, 4, and 5 respectively, each having 100 feature maps. The dimension of the hidden states of the GRU is set to 300. We use adam(Kingma and Ba, 2015) optimizer and set the initial learning rate as 1. 0 × 10 −4. The learning rate is halved every 20 epochs during training. Dropout probability is set to 0. 3.",
        "GPT2_formal_text": "_t_i, a_t_i). The method looks at both the tokens in the i-th and j-th candidate for the i-th sentence. In this case, the input set is set up in a way that each candidate gets treated like a sentence and is analyzed step by step. Formal: When dealing with unstructured text, the model can be messy and messy, but in natural language, it's neat and organized, with clear and understandable sentences. Formal: Lastly, we looked at the accuracy for sentences where the tokens had the same semantic role across different candidates. This is useful for understanding how models actually work. Formal: Here's how the results are broken down: Formal: Table 2 shows the results for each model, including the ones that were classified wrong. Formal: For the sentences where the same semantic role is repeated across different candidates, the accuracy is better than just guessing randomly (like in the top row of Table 2). Formal: Here are the results for each model, including the ones that were classified wrong. Formal: From the results in Table 2, it's clear that the models are pretty good at classifying sentences. Formal: Figure 3 shows the accuracy for the BERT model based on the predicted title and summary. Formal: The accuracy here is better than random guessing, which suggests that the model is pretty good at picking up on these structural relationships. Formal: Fig. 3 breaks down the accuracy for the BERT model based on the predicted title and summary. Formal: The accuracy here is better than random guessing, which suggests that the model is pretty good at picking up on these structural relationships. Formal: Figure 4 shows the accuracy for the BERT model using the predicted title and summary. Formal: The accuracy here is better than random guessing, which suggests that the model is pretty good at picking up on these structural relationships. Formal: Formal: Figure 5 breaks down the accuracy for the BERT model using the predicted title and summary. Formal: The accuracy here is better than random guessing, which suggests that the model is pretty good at picking up on these structural relationships. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "ccl",
        "filename": "2020.ccl-1.102.json"
    },
    {
        "casual_text": "You'll find more results and analysis in Appendix B. They show that our methods can also tell us how well different target languages work with a specific source language and vice versa for the tasks we talked about in this paper.",
        "formal_text": "Additional results and analyses are provided in Appendix B. They further demonstrate that our measures also indicate transfer quality of different target languages for a given source language, and transfer quality of source languages for a given target language, for the tasks discussed in this paper.",
        "GPT2_formal_text": "ed sets the attention layer's output embedding to be a non-zero number (like 0 or 1). The output layer of the first model is the same as the input layer, and the output layer of the second model is the same as the output layer of the first model. Formal: Since the model creates a higher-order representation for each of the four types of situations it deals with, it creates attention layers to deal with the varying types of cases. Formal: The model looks at the four different situations it's dealing with and decides how important each of them is. Formal: The model uses these four situations as input to figure out how much attention to pay to different types of cases. Formal: For each type of situation, the model calculates the attention weights for each of the four attention layers. Formal: We use these weights to calculate the attention weights for the input layer in the first model. Formal: After calculating the attention weights for the input layer in the first model, the model looks at the input layer in the second model to get the attention weights for the output layer. Formal: Finally, we use these weights to calculate the attention weights for the output layer in the second model. Formal: Finally, we use these weights to calculate the attention weights for the output layer in the second model. Formal: Figure 1 shows how the attention weights are calculated for each type of situation. The attention weights for each type of situation are figured out using the different attention layers in the model. Formal: Formal: We use the weights from the first model for the input layer of the second model. Formal: We use the weights from the first model for the output layer of the second model. Formal: Finally, we use the weights from the first model for the input layer of the second model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.186.json"
    },
    {
        "casual_text": "This paper is all about figuring out if pre-trained models have a special kind of common sense: the ability to compare objects physically, like which one is bigger or faster. The task we set up is pretty straightforward: we give the system two words, say \"car\" and \"bike,\" and ask it to decide which one is \"faster\" based on size or speed (see section 2.1). We use a simple model—either a linear one or a one-layer neural network—that just combines (by either putting them together or subtracting one from the other) the pre-trained word embeddings of the two words we’re comparing (section 2.2). This simple setup actually works better than older methods that used extra info, like the verbs connecting the words, on a dataset called Verb Physics (Forbes and Choi, 2017) (section 3). It shows that these pre-trained models can indeed make physical comparisons.\n\nWhat’s cool is that this model can also handle objects it hasn’t seen during training (section 3.1) and does better than other models that rely on quirks of the dataset (section 4). We kept the model simple on purpose because more complex models make it hard to tell if the results are coming from the model itself or the embeddings (like in other studies, e.g., Liu et al. (2019)).\n\nAnother big part of our work is analyzing how these models actually compare objects. The confidence scores (called logits) the model gives for each label show that it consistently ranks objects the same way, no matter which object you start with (section 4.1.1).",
        "formal_text": "This paper asks if pre-trained representations encode a specific type of common sense: physical comparisons between objects. 1 The supervised classification task takes a pair of words being compared on a physical attribute such as size or speed, with the system's objective to decide which is 'bigger' or 'faster' ( § 2. 1). We use a linear or a one-layer fully-connected neural network probing model with only a combination (concatenation or subtraction) of the frozen pre-trained embeddings for the words to be compared as input ( § 2. 2). This probing model achieves better accuracy than previous approaches ( § 2. 3) which use extra information other than the words (such as the verbs connecting the words) on the Verb Physics dataset (Forbes and Choi, 2017 ) ( § 3): it encodes physical commonsense comparisons. 2 It generalizes to objects not present in the training set ( § 3. 1) with higher accuracy than baselines exploiting dataset artifacts ( § 4). We use a 'simple' probing model since more complex models make it difficult to disentangle the major contributing factor to results -model or embeddings (as in other probing studies like Liu et al. (2019) ). Our other major contribution is analyzing how models compare objects. The output logits for labels (indicating model confidence) order objects consistently across orderings or rankings built around different objects ( § 4. 1. 1).",
        "GPT2_formal_text": "= { } src = [ { name: \"INPUT1\", src2: \"INPUT2\", src3: \"INPUT3\" }, { name: \"INPUT2\", src1: \"INPUT3\", src2: \"INPUT1\", src3: \"INPUT2\" } ] output_ids = { } summary = [ { title: \"Given name:\", summary1: \"Example sentence:\", summary2: \"Example sentence:\", summary3: \"Example sentence:\", }, { title: \"Given name:\", summary1: \"Example sentence:\", summary2: \"Example sentence:\", summary3: \"Example sentence:\", }, { title: \"Given name:\", summary1: \"Example sentence:\", summary2: \"Example sentence:\", summary3: \"Example sentence:\", }, ] }\n\nThe main goal of dependency parsing is to break down sentences into their meaningful parts. In this paper, we’re using a sentence template, S, as our basic representation. By looking at the sentences we’ve parsed so far and how similar they are to each other, we can guess the most likely version of S, which is called the compressed sentence template (CST). Basically, our goal is to predict the compressed sentence template by picking the best possible version of S based on the compressed sentences we’ve parsed so far. Formal: Formal: In this paper, we’re not focusing on sentence length here. We’re just using a simple list of sentence candidates to handle the right-hand side (RHS) of the problem. Formal: We’re using the Stanford dependency parser (Manning et al., 2014), which is a pretty standard tool for this kind of work. Formal: The Stanford dependency parser was introduced by Manning and his team back in 2014. Formal: We’re using a dependency grammar called DIORA (from Demberg et al., 2008) to handle the task. Formal: The dependency structure is shown in Figure 1. Formal: In this paper, we’re using a simplified version of DIORA to handle this. Formal: To make things simpler, we’re using a recursive neural network (RNN) (Dyer et al., 2014) to handle the problem. Formal: We’re using a Recursive Neural Network (RNN) for this. Formal: Finally, we’",
        "directory": "D19",
        "filename": "D19-6016.json"
    },
    {
        "casual_text": "Our model was trained with just a little bit of data and not too many features. For future work, we want to try it out with lots of monolingual data, which would let us use way more features. Right now, we tested it on a language pair (Chinese to English) where there's a ton of bilingual stuff available for the domain we used. Later, we’re thinking of looking at low-resource areas and language pairs like Urdu-English, where there’s not much bilingual data for new domains.",
        "formal_text": "Our discriminative model used only a small amount of training data and relatively few features. In future work, we plan to test our method in settings where there are large amounts of monolingual training data (enabling many discriminative features). Also, our experiments here were performed on a language pair (i. e. , Chinese to English) that has quite rich bilingual resources in the domain of the test data. In future work, we plan to consider lowresource test domains and language pairs like Urdu-English, where bilingual data for novel domains is sparse.",
        "GPT2_formal_text": "_len = n_attention_len / d_mask. Formal: We use the hidden state h_t, H_h, h_l, H_d, and H_p to represent the input and output states for the corresponding sequence, respectively. Formal: For the cross-attention on the output state, we follow the same setup as Shen et al. (2019b). Basically, we use the combined representation of the context and the attention value for the sequence representation. Formal: For the whole dataset D, the loss function we get after applying all the cross-attention is: Formal: We use the combined representation of the context and the attention value for the sequence representation. Formal: For the multi-head attention on the output state, we calculate the loss by integrating out the cross-attention between the output state and the context. Formal: We use the integrated representation of the context and the attention value for the output representation. Formal: We use the integrated representation of the context and the attention value for the output representation. Formal: We use the integrated representation of the context and the attention value for the output representation. Formal: Finally, we sum up the losses from all the heads to get the overall loss. Formal: For the cross-attention on the context, we calculate the loss by integrating out the cross-attention between the context and the attention value. Formal: We use the integrated representation of the context and the attention value for the output representation. Formal: We use the integrated representation of the context and the attention value for the output representation. Formal: Finally, we sum up the losses from all the heads to get the overall loss. Formal: For the multi-head attention on the output state, we calculate the loss by integrating out the multi-head attention between the output state and the context. Formal: We use the integrated representation of the context and the attention value for the output representation. Formal: We use the integrated representation of the context and the attention value for the output representation. Formal: Finally, we sum up the losses from all the heads to get the overall loss. Formal: Formal: For the cross-attention on the context, we calculate the loss by integrating out the cross-attention between the context and the attention value. Formal: We use the integrated representation of the context and the attention value for the output representation. Formal: We use the integrated representation",
        "directory": "D11",
        "filename": "D11-1085.json"
    },
    {
        "casual_text": "The research on cross-lingual text retrieval (CLTR) has some work that’s pretty similar to what we’re doing here. Lately, a lot of approaches have been focusing on using dictionaries and corpora to translate queries from one language to another, like the language of the documents you’re searching through (Oard, 1997). For instance, some methods create queries in the target language using a corpus-based technique that’s kind of like what we’re talking about. But the thing is, they don’t really try to figure out if the terms they’re using are specific to a certain field or just general words, and they don’t involve any manual work.\n\nWhat we’re doing here is more about semi-automating the process of building translation dictionaries that are tailored to a specific field. You could think of it as providing bilingual dictionary entries for CLTR methods, similar to what Davis did later on (Davis, 1996). In his approach, he first used a dictionary to create a query in the target language, which could be a bit unclear, and then used a corpus to figure out the right meaning.",
        "formal_text": "The literature on cross-lingual text retrieval (CLTR) includes work that is closely related to this research, in that recent approaches emphasize the use of dictionary-and corpus-based techniques for translating queries from a source language into the language of the document collection (Oard, 1997). , for example, generate target-language queries using a corpus-based technique that is similar in several respects to the work described here. However, the approach does not attempt to distinguish domain-specific from general usage term pairs, and it involves no manual intervention. The work reported here, focusing on semiautomating the process of acquiring translation lexicons specific to a domain, can be viewed as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work (Davis, 1996), in which dictionary-based generation of an ambiguous target language query is followed by corpus-based disambiguation of that query.",
        "GPT2_formal_text": "= {} tags = {tag1, tag2, ..., tagM }. Formal: The multi-attention mechanism in the encoder can create different representations for each input token by focusing on different parts of the sentence. Specifically, we pick the output for a token with the highest attention score, which we call the output_i. Formal: Formal: In our experiments, we set the attention values for each token to 2.0. For the first part, we make the input representation for the token by using the output from the first layer of the decoder. Formal: In the second part, we use a bidirectional LSTM to create the representation for the token. Formal: Our model, called Att-RNN, can be easily tweaked to include a residual connection. Formal: We tested our model on three different datasets, each with different levels of semantic information and fine-tuning. The results are in Table 3. Formal: The model works well on three different datasets. The test dataset has a total of 4.9 million sentences. Formal: We used the same training and testing data to train the model. Formal: We also created a dataset called SemEval-2017 Task 1 (SAT1), where we fine-tune the model on a different test dataset. Formal: For the experiments, we used the dataset with the highest attention score, which is the one labeled as SAT1. Formal: To make things clear, we got the attention values for all the tokens by using the linear mapping from the encoder. Formal: Formal: Lastly, we ran a few experiments to see how different attention settings affect the model. Formal: Formal: We built a basic model using a single-layer Bi-LSTM with 300 hidden units. Formal: We also used a residual connection between the decoder and the input (called RE). Formal: The model was trained using the training data from the second dataset. Formal: We tested it on three datasets: SAT1, SAT2, and SAT3. Formal: We also created a dataset called SemEval-2018 Task 4 (SAT4) for task-oriented dialogue generation. Formal: We used the same training and testing data as in the previous task. Formal: Formal: Formal: The model can also be trained using different datasets, just like the previous task. Formal: Formal: Formal:",
        "directory": "A97",
        "filename": "A97-1050.json"
    },
    {
        "casual_text": "• We're using the Transformer model (Vaswani et al., 2017) here. It's trained with the OpenNMT library. For both news and dialogues, we keep the training parameters the same, except for the minimum length of the summary we generate—35 for news and 15 for dialogues.",
        "formal_text": "• Transformer (Vaswani et al. , 2017). The model is trained using OpenNMT library 4. We use the same parameters for training both on news and on dialogues 5, changing only the minimum length of the generated summary -35 for news and 15 for dialogues.",
        "GPT2_formal_text": "is calculated by multiplying the attention weights from the sequence-level attention layer with the attention weights from the head, which is stored in the hidden layer. If a word is highlighted, it gets added to the attention matrix A, and if it’s not highlighted, it’s left out. The whole idea behind attention is to take into account how important different aspects of the context are, while also considering how important they are individually. Formal: We’ve come up with a way to calculate attention weights for sequence-level attention that’s different from the usual one-hot attention method. Formal: Our results show that our new method works better than the regular one-hot attention and gives better results overall. Formal: We also tried out our method on the RTE dataset (from the TREC-6 and TREC-7 challenges) and found that it works better than the two-stage attention mechanism (also known as the first stage attention) in terms of performance. Formal: Formal: Here’s a quick breakdown of the attention weights, attention weights, and attention kernels that go into the GCN setup. Formal: The GCN setup for this paper. Formal: The text and image data are fed into the GCN, and it processes everything using a bidirectional LSTM. Formal: Our method combines the GCN’s structure with how it learns and calculates attention weights. Formal: The attention weights are calculated by multiplying the attention weights from the head with the attention weights from the word-level attention layers. Formal: The head-level attention weights are calculated by multiplying the attention weights from the word-level attention layers with the attention weights from the head. Formal: In the RTE dataset, attention weights for sentences are calculated by multiplying the attention weights from the sentence-level attention layers with the attention weights from the sentence-level attention layers. Formal: We’ve shown how our method performs on RTE datasets. Formal: For example, if a word is highlighted, it gets added to the attention matrix A, and if it’s not highlighted, it’s left out. Formal: We’ve also done some experiments to check how our method compares to the other attention mechanisms in terms of performance. Formal: Formal: We’ve compared our method to a few other attention methods. Formal: Formal: The hierarchical attention mechanism, which uses attention weights from",
        "directory": "D19",
        "filename": "D19-5409.json"
    },
    {
        "casual_text": "Another approach to figuring out what pronouns refer to, like the one by Ge et al. (1998), uses stats to help out. They look at how far apart the pronoun and the possible word it’s referring to are to see how likely they are connected. They also check things like gender, number, and whether the word refers to a living thing to narrow it down. Plus, they use head information to set some rules and count how many times a word is mentioned.",
        "formal_text": "Other strategies to anaphora resolution, as the one introduced by Ge et al. (1998), use statistical information to resolve pronouns They use the distance between pronoun and the proposed antecedent to check the probability of the connection between them, information about gender, number, and animacity of the proposed antecedent as hints for the proposed referent, and head information to make selectional restrictions and mention count.",
        "GPT2_formal_text": "= { idx1, idx2, ..., idxM }}, where each idxi is a sequence of d-dimensional embeddings for the i-th token. Formal: To build the argument pairs, we start by randomly picking a bunch of candidates for each i from the set P, where each candidate has a unique idx. Then, we pick one argument for each i and keep going until we hit the maximum number of arguments we want. We use two separate CNN models with a hidden size of 300, a batch size of 32, and a learning rate of 0.0001. The hidden units for the CNN models are set to 300. The beam sizes for the argument pairs are set to 2, 4, and 8. Formal: We also came up with a new method called span-based adversarial training (STAT) to improve the token representation of the argument pairs. Formal: To make the vectors for the argument pairs better, we use an attention mechanism (shoutout to Bahdanau et al., 2014) to get a representation that takes into account the context and the argument spans. The weights for this representation are learned from an external knowledge graph. Formal: Our approach has two main parts: (i) a sequence-level attention mechanism that learns from a knowledge graph to improve the representation of the argument pairs, and (ii) a span-level adversarial training approach to improve the representation of the arguments. Formal: The knowledge graph for the argument pairs is built using a sequence-level attention mechanism that learns from a knowledge graph. Formal: We use an external knowledge graph to expand the embedding vector for the arguments and train the representation for them. Formal: The argument embedding vector is created by combining the embedding vectors from the argument embedding and the argument span embedding. Formal: Lastly, we add a linear layer to the argument embedding representation to enhance its semantic information and further improve the representation. Formal: Using the knowledge graph and the argument embedding vector, we create a representation for the argument pairs. Formal: To train the attention mechanism, we first train a sequence-level attention mechanism to get a representation that focuses on the context and the argument spans. After that, we train a span-level attention mechanism to get a representation that focuses on the argument spans. Formal: In the final layer, we add a linear layer to the argument embedding representation to get the semantic representation of the argument pairs. Form",
        "directory": "C16",
        "filename": "C16-1077.json"
    },
    {
        "casual_text": "An alignment system matches up the surface representations of two languages. (Shin, 1996) tested an expectation-maximization algorithm and got 68.7% accuracy at the phrase level. This will be added to the version 2 platform.",
        "formal_text": "An alignment system gathers correspondences between surface representations of both languages. (Shin, 1996) experimented expectationmaximization algorithm with 68. 7 % accuracy at phrase level, and this will be incorporated into version 2 platform.",
        "GPT2_formal_text": "ed and attention_hidden layers, respectively. So, the last hidden layer of the model is called mask. Formal: For a sentence x_i, each word in it gets a score from the attention module. Formal: Once we have the sentence, we use a softmax function to figure out the attention weights for the words. Formal: To keep things short, we'll just talk about the attention mechanism in the next few parts. Formal: The attention module helps by turning the attention weights into a fixed-size vector, which is called attention_size. Formal: In the last layer, the model learns to maximize the expected value of the hidden representation for each word in the sentence. Formal: Basically, the model learns to maximize the expected value of the hidden vector for each word in the sentence. Formal: After that, we calculate the attention weights for the words in the sentence using equation (2). Formal: For instance, if we have a sentence x_i with words w_1 to w_n, the attention weights for the words w_1 to w_n are calculated as z_1 = z_1 * e_1, z_n = z_n * e_n. Formal: A lookup table is used to find the attention weights for a specific word or token, like w_i in equation (2). Formal: This lookup table has the attention weights for the word w_i and the token token_i, respectively. Formal: We use the max pooling operation to handle the pooling in the last layer. Formal: We use the max-pooling operation in the last layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C96",
        "filename": "C96-2185.json"
    },
    {
        "casual_text": "This approach can totally work in the food world too. For all our tests, we’re using a dataset from chefkoch.de, which is like the biggest German website for anything food-related. It has 418,558 forum pages, and we’re using that for our experiments. In Table 1, you can see how well coordination works for pulling out food items from our special food-focused data. \n\nWe started with a small list of 10 common food items (like water, salt, sugar, salad, bread, meat, cake, flour, and potato) and then looked at all the combinations of words that come up together, ranking them by how often they appear. We did this both on our food-specific data and on Wikipedia. As a comparison, we also just ranked all the nouns by how often they show up in our food data. \n\nThe table shows that just going by how often something appears isn’t really effective. But when we use those word combinations (conjuncts) from our food-specific data, we get much better results.",
        "formal_text": "This method can also be applied to the food domain. As a domain-specific dataset for all our experiments, we use a crawl of chefkoch. de 3 (Wiegand et al. , 2012) consisting of 418, 558 webpages of forum entries. chefkoch. de is the largest German web portal for food-related issues. Table 1 shows the effectiveness of coordination as a means of extracting food items from our domain-specific corpus. Given a seed set of 10 frequent food items (we use: water, salt, sugar, salad, bread, meat, cake, flour, potato), we compute all conjuncts and rank them according to frequency. We do this on our domainspecific corpus and on Wikipedia. As a baseline, we simply sort all nouns according to frequency in our domain-specific corpus. The table shows that ranking by frequency is no effective method. Conjuncts produce good results provided that they are extracted from a domain-specific corpus.",
        "GPT2_formal_text": "(A_i) = arg_t~(A_i) y_t = arg_t~(A_i) i = 1 A_i = 1  \nBasically, you've got a bunch of stuff, like (A_i) = arg_t~(A_i), and then some extra terms, like arg_t~(A_i+1) = arg_t~(A_i+1). Formal: The second part of equation (8) gets simplified into a simpler form. Formal: The formula in (9) can be rewritten as (10) Formal: This simplification is done by doing a little math with the logarithm of the value of t. Formal: If you have a big list of all possible search paths in the world, a search agent can find the path that gives you the biggest reward (like a \"gold standard\" score) based on the current best guess of the search strategy. Formal: So, if you want to search in a specific area, you can use the search strategy that gives you the highest reward, and then simplify the whole thing by finding the path that gives you the smallest reward. Formal: In the \"best-first\" setup, the search is done by the first agent, so you can focus on finding the path that gives you the biggest reward and simplifying everything else. Formal: The \"best-k\" setup, on the other hand, has more flexibility because it lets you search for multiple paths. Formal: It’s pretty simple to calculate the logarithm for the reward. Formal: But, the logarithm of the logarithm isn’t always the best choice, especially when the search strategy needs a lot of computing power. Formal: In this paper, we’re mainly looking at how the logarithm works with a big search space and how it affects the value of t. Formal: The logarithm is mainly used when dealing with a big search space, like the one in equation (11), and it’s also used in a simpler version where you only do a search if it gives you the highest reward. Formal: In this case, the logarithm is also important when deciding between two different options. Formal: For instance, in equation (9), the logarithm is important when checking if t = 0 or t =",
        "directory": "C14",
        "filename": "C14-1216.json"
    },
    {
        "casual_text": "CVaR is being too cautious because it considers all kinds of groups, even ones that are totally messed up and don't make sense. To fix this, we'll focus on optimizing models for actual, meaningful subgroups instead of just any random group.",
        "formal_text": "Sentence CVaR is too conservative since it allows for arbitrary groups -including ones consisting of purely invalid sentences. To remedy this, we will optimize models for all meaningful subpopulations instead of arbitrary ones.",
        "GPT2_formal_text": "_max_predict = attn_max_predict (attn) src_attn_max_predict = attention_mask_max_predict (attn) #### Output: Formal: The attribute scores have been adjusted to match the gold-standard scores for gold-standard entities in the training set. Formal: We take the max-pooling layer from the entity embedding layer and add another layer to it. Formal: Here, we're trying to learn a relation-adaptive query vector p = (q, r) ∈ R d×d for the query E that’s in R d. We then use this relation-adaptive query vector p to find the best response y_q_p in R d. Formal: At each time step t, we first calculate the embedding e_t and the embedding e_t for the entity e_q and the query q_p. Then, we use this to calculate the response z_t_p. Formal: The attention mechanism helps the model focus on the most important parts of the input sequence to create useful representations, while still handling the edges properly. Formal: We follow the approach from (Li et al., 2015a) and use an attention mechanism with a hidden layer size of d_s to encode the entire input sequence. To figure out the embedding of a token z_t_p, we use the embedding e_t_p and the attention mechanism for token z_t_p to calculate the embedding e_t_p_s. Formal: In this case, we use a maximum entropy classifier and set the maximum number of inputs to 2 to handle the edges correctly. Formal: The minimum (or maximum) window size is set to 10, to make sure the attention weights are learned locally. Formal: In the entity embedding layer, we apply softmax to the input embedding e_t and the embedding e_t for the entity e_q to get the entity embedding e_q_p. Formal: Finally, we calculate the attention weights in the entity embedding layer by using the attention mechanism for entity e_q_p_s. Formal: In this project, we use the entity embedding layer, the entity embedding layer, and the entity embedding layer to learn the embedding and attention weights for entity e_q_p. Formal: Lastly, we",
        "directory": "D19",
        "filename": "D19-1432.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way:\n\nGCN GCN GCN refers to Graph Convolutional Networks, which are used in machine learning to work with graph-structured data.\n\nQuantity Cell Q \" Q # Q $ Q % \" # % $ is talking about different quantities or values in a cell, maybe related to data or calculations.\n\nQ \" Q # Q $ Q % BiLSTM is combining those quantities with a BiLSTM, which stands for Bidirectional Long Short-Term Memory, a type of neural network often used for sequence prediction problems.\n\nSo, in short, it's about using GCNs and BiLSTMs to handle and analyze data, possibly in a structured way.",
        "formal_text": "… … … … … … GCN GCN GCN Quantity Cell Q \" Q # Q $ Q % \" # % $ Q \" Q # Q $ Q % BiLSTM",
        "GPT2_formal_text": ") = {h, h_t, h_s, h_t_s} Formal: L_q (n, m) = {0.0, 0.0, 0.0, 0.0, 0.0} Formal: L_s (n, m) = {0.0, 0.0, 0.0, 0.0, 0.0} Formal: L_d (n, m) = {0.0, 0.0, 0.0, 0.0, 0.0} Formal: We can estimate the cross-entropy loss (using the values for K_q, K_d, and K_s) using the conditional distribution q_d, which is outlined in equation 4. Formal: Using the cross-entropy loss, we can then figure out the cross-entropy between the target embedding h_t and the actual label n_t. Formal: This cross-entropy loss is calculated using the cross-entropy between the embedding h_t and the target label n_t, as shown in equation 5. Formal: Finally, we get the cross-entropy between the output embedding h_s and the target label n_s, as shown in equation 6. Formal: Formal: The cross-entropy loss is based on the conditional distribution q_d, which is in equation 4. Formal: We can also calculate the cross-entropy loss using a sample-level distribution q_d_i, which is in equation 5. Formal: Formal: We can also calculate the cross-entropy loss using a sample-level distribution q_d_i, which is in equation 6. Formal: Formal: Finally, we can calculate the cross-entropy loss using a sample-level distribution q_d_i, which is in equation 7. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.362.json"
    },
    {
        "casual_text": "Alright, let's break it down. For each sentence, which we can think of as a group of words W = {w 1, w 2, . . . , w n }, we need to figure out how these words connect to the events they’re talking about.",
        "formal_text": "To set up notations, for each utterance represented as a set of words W = {w 1, w 2, . . . , w n }, we want a segmentation which expresses the relationship of the words to the events which they describe.",
        "GPT2_formal_text": "ing, which we call F c. The input for this layer is a sequence of words X = (w i, w i+1, ..., w i+n ). It has a dimension of d × n, and it's generated using an LSTM-based RNNG. Formal: The key to our approach is to use domain-specific word embeddings that are learned locally, which is what we call the local focus mechanism. This setup lets us predict the next word in the input, but it also focuses on the context to predict the document's embedding, which we call the context focus mechanism. Formal: We use the usual postprocessing steps like normalization (Ng and Cardie, 2012; Xu et al., 2016), lemmatization (Mehri et al., 2016), and tokenization (Mihalcea and Straka, 2004) to clean up the embeddings. We also use a nonlinear function to make sure the output vector stays stable during training. Formal: In this part, we’ll talk about our model and explain the three levels of attention we use. Formal: There’s been a lot of research on understanding and generating text in various areas, but not much has been done on detecting content when the source and target texts are in different languages. Our model builds on some ideas from various areas, like dependency parsing, entity linking, and text normalization, but it’s unique in two key ways. Formal: First, we use a standard pre-trained LSTM-based model (Bi-LSTM) to generate the context vector and the embedding vector for the target text. Then, using an attention mechanism, we connect the context and target embeddings to predict the next word in the input. Formal: Second, we use a transform-based model to generate the final embedding vector for the target text. Formal: Finally, we use a linear-chain CRF (LCF) (Blei et al., 2003) to normalize the embeddings and generate the overall embedding vector for the target text. Formal: We also use a lexicalized CRF (LCLF) (Blei et al., 2003) to normalize the embeddings and generate the overall embedding vector for the target text. Formal: Formal: Formal: Lastly, we use a general-purpose transformer-based model (Vaswani",
        "directory": "D14",
        "filename": "D14-1043.json"
    },
    {
        "casual_text": "In this project, we’re looking at how to figure out who the main criminals are and who’s just helping out based on the details of a crime case. The main criminal is the one who’s in charge, leads a group, or takes a big part in the crime. On the other hand, an accessory is someone who’s more in a supporting role, helping out but not the main player. As shown in Fig. 1, our goal is to take the description of the crime and a list of people involved, and then figure out who’s the main criminal and who’s just assisting.",
        "formal_text": "In this work, we propose to study the identification of principals and accessories from the fact description in a criminal case. The principal refers to a criminal who organizes and leads criminal groups to carry out criminal activities or plays a major role in joint crimes. Correspondingly, we refer the accessory as the one who plays a secondary or auxiliary role. As the illustration of our task in Fig. 1, given the fact description as well as a list of defendants, we expect to identify the principals and accessories from the defendants.",
        "GPT2_formal_text": "= [], e_start = [], e_step = [], e_inter = [], e_end = [], e_sub = [], e_local = [], e_inter_m = [], e_int = [], e_num = [], e_pos = [], e_rel = [], e_step_m = [], e_step_l = [], e_int_m = [], e_num_m = [], e_rel = [], e_sub_m = [], e_int_m = [], e_num_m = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_sub_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l = [], e_rel = [], e_step_l = [], e_int_l = [], e_num_l",
        "directory": "acl",
        "filename": "2020.acl-main.393.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Imagine you have a sentence like this: \"A man ate it.\" Now, when we look at this sentence, we're kind of thinking about all the different people who could have eaten something. It's like we're talking about the idea of someone eating, not just one specific person. So, it's more about the general concept of eating rather than focusing on a particular individual.",
        "formal_text": "As a finaJ exa. mpte, consider a. sentence which (; outa. ins ~ focussed item I)tlt lie oper~or for using it up: (13) A man ate i/ The a. nalysis of this is a. n a. bstraction over Idnds of indivM ua. ls who ate it",
        "GPT2_formal_text": "ed is the mask that gets applied to the input, while attention_aggregation is the one that gets combined with it. Formal: As mentioned in a paper by Rennie et al. (2017), we can also figure out the attention weights for the hidden stuff in the head LSTM by using this approach. Formal: Lastly, we can figure out the masked representations for the output y_i and the input x_i by doing this: Formal: The output y_i is a vector that looks like this: Formal: Here, α is the penalty function for the sequence of hidden states h_t and x_t, and β is a value that decides how much the input changes. Formal: These hidden states are then used to create a sequence of output word vectors, h_t and x_t, that go into the decoder. Formal: After the decoder, the decoded output y_i is calculated using these hidden states as input. Formal: To get the attention weights for the hidden states, we just subtract the sum from the hidden states of the output word vectors. Formal: Formal: This can also be used to calculate the attention mask α_t = α_t - β_t. Formal: Formal: Lastly, we can get the attention output, y_i, by adding up the hidden states of the output word vectors. Formal: Formal: And here's the code for the model mentioned in Section 3.3. Formal: For the recurrent neural network version, you can check out the code in Section 3.4. Formal: Finally, the code for the model mentioned in Section 3.5. Formal: Formal: And for the multi-head LSTM version, you can find the code in Section 3.6. Formal: Formal: Lastly, the code for the model mentioned in Section 3.7. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C94",
        "filename": "C94-2142.json"
    },
    {
        "casual_text": "This vector shows us a few things about horses. First, it tells us that all horses are mammals—so if you take the number of horses that are also mammals and divide it by the total number of horses, you get 1, meaning every single horse is a mammal. Second, it says that horses and things with scales don’t overlap at all—in other words, no horse has scales. We also find out that most horses have four legs, and some of them are brown.",
        "formal_text": "This vector tells us that the set of horses includes the set of mammals (the number of horses that are also mammals divided by the number of horses comes to 1, i. e. all horses are mammals), and that the set of horses and the set of things that are scaly are disjoint (no horse is scaly). We also learn that a great majority of horses have four legs and that some are brown.",
        "GPT2_formal_text": "\": [], } }, { \"type\": \"GET\", \"url\": \"https://api.github.com/repos/huggingface/releases/graphs/5\", \"object\": { \"title\": \"Huggingface - Transformers (base)\", \"type\": \"HEAD\", \"resource\": \"\", \"created_at\": 1408337594, \"object\": { \"title\": \"Huggingface - Transformers (base)\", \"type\": \"HEAD\", \"resource\": \"\", \"created_at\": 1408337595 }, \"labelled_entity\": true }, { \"type\": \"POST\", \"url\": \"https://api.github.com/repos/huggingface/releases/graphs/5\", \"object\": { \"title\": \"Huggingface - Transformers (base)\", \"type\": \"HEAD\", \"resource\": \"\", \"created_at\": 1408337596 }, \"labelled_entity\": true }, { \"type\": \"PUT\", \"url\": \"https://api.github.com/repos/huggingface/releases/graphs/5\", \"object\": { \"title\": \"Huggingface - Transformers (base)\", \"type\": \"HEAD\", \"resource\": \"\", \"created_at\": 1408337597 }, \"labelled_entity\": true }, { \"type\": \"DELETE\", \"url\": \"https://api.github.com/repos/huggingface/releases/graphs/5\", \"object\": { \"title\": \"Huggingface - Transformers (base)\", \"type\": \"HEAD\", \"resource\": \"\", \"deleted_at\": 1408337598 }, \"labelled_entity\": true }, { \"type\": \"HEAD\", \"url\": \"https://api.github.com/repos/huggingface/releases/graphs/5\", \"object\": { \"title\": \"Huggingface - Transformers (base)\", \"type\": \"HEAD\", \"resource\": \"\", \"created_at\": 1408337599 }, \"labelled_entity\": true }, { \"type\": \"POST\", \"url\": \"https://api.github.com/repos/huggingface/releases/graphs/5\", \"object\": { \"title\": \"Huggingface - Transformers (base)\", \"type\":",
        "directory": "D15",
        "filename": "D15-1003.json"
    },
    {
        "casual_text": "We calculate a normalized community similarity score for pairs i, j by subtracting s i, m from s i, j, where s i, m is the score from the \"subreddit merged others.\" We look at how this score correlates with community feedback for three different models. The results are in Table 4 for thread-level analysis and Table 5 for user-level analysis.\n\nAt the thread level, the hyb-500. 30 style model consistently shows a positive and statistically significant correlation between a post's stylistic similarity score and its karma. This means that adapting your language style can help your post get more upvotes. None of the other models we looked at earlier had this feature, and for the topic models, the correlation is usually negative.\n\nOn the user level, all the correlations between a user's k-index and their style/topic match are statistically significant. The hyb-500. 30 style model shows a stronger positive correlation compared to the others. In both cases, the word_only model gives results that are somewhere in between the style and topic models. The hyb-15k model is similar to the word_only model, and the tag_only model mostly shows negative correlations.",
        "formal_text": "We compute a normalized community similarity scores i, j = s i, j − s i, m, where s i, m is the corresponding score from the subreddit merged others. The correlation betweens i, j and community feedback is reported for three models in Table 4 for the thread level, and in Table 5 for the user level. On the thread level, the hyb-500. 30 style model consistently finds positive, statistically significant, correlation between the post's stylistic similarity score and its karma. This result suggests that language style adaptation does contribute to being well-received by the community. None of the other models explored in the previous section had this property, and for the topic models the correlation is mostly negative. On the user level, all correlations between a user's k-index and their style/topic match are statistically significant, though the hyb-500. 30 style model shows more positive correlation than other models. In both cases, the word_only model gives results between the style and topic models. The hyb-15k model has results that are similar to the word_only model, and the tag_only model has mostly negative correlation.",
        "GPT2_formal_text": "ing_t. This step is pretty important for handling the attention when doing sentence embedding. Formal: (iv) The distributional vector for s_t is the vector where the argument pairs are located. This vector basically tells us the part of the text that the model was trained on. Formal: (v) The vector for the predicate s_p is the vector that represents the input text of the predicate. Formal: (vi) The vector for the argument pairs is the vector where the argument pairs are located. Formal: (vii) The embedding for the predicate s_p is the embedding of the predicate's input text. Formal: (viii) Finally, the embedding for the argument pairs is the embedding of the argument pairs themselves. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1108.json"
    },
    {
        "casual_text": "Lately, pre-trained language models (PLMs) have made some pretty cool strides in a bunch of different tasks (shoutout to Devlin et al., 2019). Some newer studies show that these models can pick up knowledge from huge datasets all by themselves during pre-training and store that info in their parameters (Tenney et al., 2019; Petroni et al., 2019; Roberts et al., 2020). But here's the thing: because of the limited size of their vocabulary, PLMs struggle to pull out that factual knowledge, especially when it comes to rare or less common entities (Gao et al., 2019a; Wang et al., 2021a).",
        "formal_text": "Recent advance in pre-trained language models (PLMs) has achieved promising improvements in various downstream tasks (Devlin et al. , 2019; . Some latest works reveal that PLMs can automatically acquire knowledge from largescale corpora via self-supervised pre-training and then encode the learned knowledge into their model parameters (Tenney et al. , 2019; Petroni et al. , 2019; Roberts et al. , 2020). However, due to the limited capacity of vocabulary, existing PLMs face the challenge of recalling the factual knowledge from their parameters, especially for those rare entities (Gao et al. , 2019a; Wang et al. , 2021a).",
        "GPT2_formal_text": "\" { \"id\" : [ 1, 2, 3 ], \"type\" : [ 1, 2, 3 ] }\n\nTo find the n-gram translation pair in the query, we use a list of the n most common words in the original document. We then calculate the best translation for each word in the list. Here, \"word\" and \"n\" are the number of words and how many times they appear together in the query, respectively. The n-gram pairs are calculated using something called unigram and bigram models. Formal: This method, called TF-IDF, is also used for finding words in a text. Formal: We use the TF-IDF model to find the best translation. Formal: We use the bigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the bigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the bigram model to find the best translation. Formal: We use the bigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the bigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the bigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the bigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the bigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal: We use the bigram model to find the best translation. Formal: We use the unigram model to find the best translation. Formal",
        "directory": "acl",
        "filename": "2022.acl-short.57.json"
    },
    {
        "casual_text": "Next, let’s dive deeper into how changing the parameters might affect things. We’re curious to see what would happen if someone (like an adversary) picked different values for these parameters instead of sticking with the ones in Table 5. The possible values for k, p, and temperature are all laid out in Table 1. Just a quick reminder: you either use top-k or top-p sampling to decide how many words to consider when generating text. We’ve been using top-p sampling as our go-to method. When we tweak k or p, we keep the temperature at its default setting. And if we mess with the temperature, we leave p at its default value.",
        "formal_text": "We next explore the parameter differences angle further to get a sense of what would happen if the adversary chose a parameter value other than the ones explored in Table 5. The different values for k, p, and temperature are as listed in Table 1. We remind the reader that one uses either top − k or top − p sampling to control the number of words under consideration during text generation. We use top − p sampling as the default strategy. When varying k or p, the temperature is fixed at the default value. When varying temperature, p is kept at the default value.",
        "GPT2_formal_text": "function, which is basically the same as the input attention vector. It calculates a score for each word in the vocabulary based on its representation in the hidden state and adds it to the output. Formal: The K-dimensional representation for a word vector comes from a transformer model. This model was trained using the CoNLL 2009 dataset for named entity recognition. To make sure the input vector is well-matched to the representation, we just average the weight matrices. To make things easier, we use a linear projection called l1t, which is a linear transformation of the hidden state at the k-th level. Formal: We set the number of layers to k = 12, and the hidden state dimension to D = 2k. For the supervised NER task, the model learns from one reference sentence, and for unsupervised NER, it learns from a bunch of references. Formal: We use a Recurrent Neural Network (RNN) (from He et al., 2015) to get the output for each token. Formal: We apply a max-pooling layer to get the sentence representations. After that, we split the sentences into tokens using the endocentric algorithm (Loper and Bird, 2002) to get a token representation for each token. Formal: We feed this token representation into a BiLSTM (Hochreiter and Schmidhuber, 1997) to get the predicted sequence. We keep the first hidden state state of the BiLSTM fixed and use a linear layer to normalize it. Formal: After normalizing the input vectors, we generate a sequence of tokens. Formal: We train the model by minimizing the cross-entropy loss for unsupervised NER. Formal: After normalizing the vectors, we get the predicted sequence. Formal: Formal: Formal: The weights for the BiLSTM, RNN, and RNNL are normalized based on the inverse of their logits. Formal: For supervised NER, we set the number of training epochs to 24. Formal: Formal: Formal: Formal: The model uses a label attention module to get the sentence representations. Formal: Formal: Formal: Formal: For the unsupervised NER task, we train the model using the data from the validation set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eacl",
        "filename": "2021.eacl-main.155.json"
    },
    {
        "casual_text": "Alright, in this part, we're looking at how exits learn from each other by running some small tests called pairwise mutual learning (PML). For these PML tests, we pick two exits (let's call them exit i and exit j), where exit i is closer to the surface than exit j (so, i < j). We're checking out a few different scenarios to see how things play out.",
        "formal_text": "In this section, we examine the effects of mutual learning among exits by conducting a series of pilot experiments called pairwise mutual learning (PML). In the PML experiments, we select two exits (i, j) (i < j, i. e. , exit i is shallower than exit j). We consider the following settings:",
        "GPT2_formal_text": ", and a hyperparameter called α. We’ll call this model α_train_k. Formal: To train this model, we need to tweak the parameters θ_k using a development set D_dev. Formal: The problem is, there’s not a lot of data to work with. So, we can’t just sample from it directly. Instead, we’re taking inspiration from how human teachers learn by chatting with a student (Boudin et al., 2012). Basically, in a class setting, the teacher picks one sample from the set D_dev to help out the student. To make this happen, we use a zero-shot approach where the student gets a little help from the teacher. We’ve got a dataset D_dev that’s specific to the task. Formal: To make the student learn from the teacher, we tweak the model’s parameters θ_k by doing one-hot cross-entropy calculations. For instance, we might randomly pick a sample from D_dev and calculate the log-likelihood like this: Formal: We also look at how the teacher interacts with the student during the process of generation. Formal: So, the student gets the guidance from the teacher and gets better at generating better responses. Formal: The model’s parameters θ_k are figured out using this process. Formal: Formal: We use these parameters to generate responses. Formal: The main goal of this paper is to see if we can train a model using a teacher model that’s trained on a smaller dataset. Formal: We think this model can work with any sequence generation method, as long as the teacher is helpful. Formal: We also suggest that we can use the model’s outputs to improve its responses by using the responses in the teacher’s domain. Formal: Lastly, we suggest using the model’s outputs to improve its responses by adding the responses in the student’s domain. Formal: The final result is the final model that handles the task. Formal: The code and data for this project are available at https://github.com/huggingface/elearning-with-a-student-teacher-model Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.242.json"
    },
    {
        "casual_text": "In this paper, we worked on detecting off-topic responses. We started by creating a model with five main layers. Inside this model, we used a bi-attention mechanism and convolutions to identify important words in prompts and key phrases in responses. We also added a gated unit as a relevance layer to improve semantic matching and included residual connections in each layer. To better understand our model, we did some visualization analysis. Lastly, we came up with a new negative sampling method to boost our off-topic training data. We tested our approach and saw big improvements on both familiar and new test data.",
        "formal_text": "In this paper, we conducted a series of work around the task of off-topic response detection. First of all, a model framework of five major layers was proposed, within which bi-attention mechanism and convolutions were used to well capture the topic words of prompts and key-phrase of responses, and gated unit as relevance layer was applied to better obtaining semantic matching representation, as well as residual connections with each major layer. Moreover, the visualization analysis of the off-topic model was given to study the essence of the model. Finally, a novel negative sampling augmentation method was introduced to augment off-topic training data. We verified the effectiveness of our approach and achieved significant improvements on both seen and unseen test data.",
        "GPT2_formal_text": "ed_pooling(u, v) = [u * src_len, v * src_len] (21) This means that the attention for the i-th token yt is calculated based on the combined state of the source and target tokens. So, to sum it all up, the attention is calculated by averaging all the attention vectors. Formal: This approach works well for tasks that need to identify masked tokens. For instance, in seq2seq models, the attention for the i-th token, yt, is calculated by averaging the attention for the source tokens. Formal: We use a standard, fixed-size gated unit attention mechanism to handle this. Formal: This setup is handy for encoding data from different sources and also for matching input and output during inference. Formal: You can find the code for the training and inference part in the appendix. Formal: We also came up with a simple evaluation metric called Tsurf (22) to measure how well the predicted sequence matches the actual input. Formal: Tsurf is designed to evaluate how well the model can infer. It’s straightforward to implement, easy to work with, and actually performs better than other popular metrics. Formal: Our work was partly inspired by (Sennrich et al., 2018). They used the attention mechanism from seq2seq models to help with multi-class classification, which they later expanded into multi-label classification. Formal: This approach can be easily combined with other text encoders and for combining text and images, the encoder can be trained together. Formal: In this paper, we looked at how well our proposed method works on a NER task and found that it can effectively capture information about user-generated content. Formal: We compared our method to other models like recurrent, cross-attention, and beam search. Formal: To evaluate how well our model is performing, we used the Tsurf evaluation metric and the validation set data. Formal: To help our model understand the context, we added an extra attention layer. Formal: Lastly, we built a tool called sampler (23) to help the model choose the right beam size. Formal: We also shared the code for the text encoder, text decoder, and text generator. Formal: Formal: You can find the code for the text encoder, text decoder, and text generator in the appendix. Formal: You can find the code for",
        "directory": "acl",
        "filename": "2020.acl-main.56.json"
    },
    {
        "casual_text": "Here's how we set up the adversarial training. We have a discriminator D that's trained to minimize the cross-entropy loss, which is defined in equation 2. In this case, l represents the domain category, and h is the hidden representation of both the question and the passage, which is a vector in R^d. For our experiment, we use the [CLS] token representation from BERT as h.",
        "formal_text": "We formulate the adversarial training as follows. A discriminator D is trained to minimize the cross-entropy loss as of equation 2, where l is domain category and h ∈ R d is the hidden representation of both question and passage. In our experiment, we use [CLS] token representation from BERT for h.",
        "GPT2_formal_text": ") = {e1, e2, ..., en}. Formal: For this task, we're looking at a new set of triples called C_s = {(w1, e1, ..., wn)} that show how entities are linked to different event instances. Formal: So, we’ve got these event instances E_s = {e1, e2, ..., en} and we want to figure out the relation pairs between them. \n\nHere’s how we do it: For each event e1 in E_s, we do something called an event-specific triplet extraction (ED). For each event e2 in E_s, we do something called event-specific triplet alignment (EDA). For each event e3 in E_s, we do something called event-specific triplet relationship extraction (EDTR). \n\nHere’s a quick breakdown of these steps: Formal: The ED pair extraction (ED) is about finding pairs (e1, e2, ..., en) where the entity ei is linked to event ej, and the relation ri is set up for them. The EDA pair extraction (EDA) is about matching up pairs (e1, e2, ..., en) where the entity ei is linked to the event ej and the relation ri is set up for them. The EDTR pair extraction (EDTR) is about matching up triples (e1, e2, ..., en) where the entity ei is linked to the event ej and the relation ri is set up for them.\n\nFor our model, ED and EDA are part of a bigger setup called Attribute Attention Network (ATN). In our model, we’re using the original ATN, but with two tweaks: (1) we’ve added a second attention mechanism called E_s2j, which takes the two sets of triplets we’ve extracted and spits out a prediction for the triplet, and (2) we’ve added a new relation extraction mechanism called R_s2j, which takes the triplets and spits out the relation instances for each pair. Formal: For the ED and EDA pairs, we’re using the same training set (E_s) and we’re working with a total of 200 examples. Formal: The event-specific",
        "directory": "D19",
        "filename": "D19-5826.json"
    },
    {
        "casual_text": "The key to relation-driven skimming is to attach things correctly without getting bogged down in too much grammar. It’s not easy, though, because even phrases that don’t seem important can mess up the ones that do. In the next parts, we’ll break down each component, explain why it works, what it does, and show you a couple of examples to help you understand.",
        "formal_text": "The trick to relation-driven skimming is to perform attachment as accurately as possible with as little grammatical analysis as possible. This is no simple task, because phrases with no relevant semantic content can always affect the attachment of relevant phrases. In the sections that follow, we will give for each of the components above an observation of why it works, its main activity, and an example or two of its operation.",
        "GPT2_formal_text": "ed = (y_t, y_a) for i, j in the input sequence. Meanwhile, the attention weights α t for the word w t are calculated based on the hidden state h t for the last word w. Formal: • A softmax layer for the context-aware attention, which is used to represent the whole input sequence. Formal: • GAT (a linear layer) that's designed to handle sequences of words. Formal: • A convolutional layer that processes the input sequence, building the context representation h_t based on it. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C90",
        "filename": "C90-2034.json"
    },
    {
        "casual_text": "TEXAN is a system for analyzing and transferring text, focusing on how it's structured and communicated. It’s built around the idea that texts are the result of specific linguistic actions, based on speech act theory. Basically, it thinks of texts as actions that guide how words and phrases are chosen. To make this work in a computer model, TEXAN uses something called a context-free illocution grammar. This grammar handles categories of actions and the logical structure of situations, connecting them to the actual words and phrases in a language. It ties all this together with a special text-related lexicon.",
        "formal_text": "TEXAN is a sysLem of bransfer, . oriented text analysis. Its linguistic concept is based on a communicative appY'oacll within the framework of speech act theory. In this view texts are considered to be the result of linguistic actions° It is assumed that they control the selection of translation equivalents. The transition of this concept of linguistic actions (text acts) to the model of computer analysis is performed by a context-free illocution grammar processing categories of actions and a propositional structure of states of affairs. The grammar whicll is related to a text lexicon provides the connection of these categories and the linguistic surface units of a single language.",
        "GPT2_formal_text": "= { qi, qj } (1: n) where {qi, qj} represents the indexes of the entities in the pairs. Here, φ(t, e) represents the chance of entity t showing up after e is translated. Formal: We use this formula to calculate φ(t, e): Formal: Basically, we just take the average of all the loss terms across the whole training set. Formal: The main idea is that it’s really important to check if the translated sentence is actually the same as the original one. Basically, the sentence should sound the same if it’s translated the same way in both directions. Formal: We’re using the cross-entropy loss function to calculate this. Formal: The model learns to pick the translation that gives us the highest value, and the translation with the lowest loss. Formal: The rest of the model is pretty much the same as the baseline. Formal: We’ve set up some parameters α and β to control how much we care about the quality of the translations. Formal: When we train this model, we use the cross-entropy loss function to estimate the model parameters. Formal: We train the model on a development set that has an equal number of sentences from the original and translated sentences. Formal: The model has two main parts: Formal: A bunch of ways to translate sentences into English have been suggested. Formal: We’re using the standard BLEU score as our evaluation metric. Formal: The results from testing the model on a development set are shown in Figure 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C86",
        "filename": "C86-1078.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We need to get the output distributions, which we'll call P dp and P kb, from two teachers named TDP and TKB. Got it?",
        "formal_text": "Produce the output distributions P dp and P kb by two teachers TDP and TKB; 5:",
        "GPT2_formal_text": "ing_query_ref_2d_tasks, as suggested by Yang et al. in 2018. The baseline model, which was trained on a dataset with 10 billion tokens, can handle a query with 10 tokens. In this case, the token is labeled as i = [i1, ..., iL], which can be thought of as the label for this combined query. \n\nThe template system in the model is based on the idea that the user's query should focus on a specific topic that's been talked about before. To do this, we make the templates that match the query look similar to the query. Each query template is made using a specific topic for the current sentence, and the specific topic of the template is also stored in a database.\n\nThe template for the query is made using a list of keywords (which we'll call {w_k}_k for a keyword k), and the topic of the template is also stored in a database. A different version of this template is used for each sentence in the dataset. Each keyword in the list, e_i, is used for a query template. For example, the list for a query template like (x_i, y_i) for the example query \"What is the price of an iPhone 6s Plus?\", would be:\n\n(x_i, y_i) = [e_i, e_i2, ..., e_iL],\n\nwhere e_i and e_i2 are the keywords for the topic and the keyword for the query template. The e_i2 is a key that helps link the query template to the specific topic, so the template can be created from it. Formal: The same idea applies to the sub-trees that get combined during the concat operation. Formal: Also, the template for the query template y_i is made by using a list of terms (which we'll call {w_k}_k for a term k), and the topic for the term template is stored in a database. Formal: The keywords for the topic, e_i, and the term template are all stored in the same database, so they're part of the same topic. Formal: We used the CRF model with the ID rule, which was proposed by Weston and his team in 2000. Formal: For the summarization part, we also use the concat operation to get the summary for each sentence. Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.281.json"
    },
    {
        "casual_text": "Here’s a simpler way to say that:\n\nWe follow these six steps for each parent node in the source-side parse tree:",
        "formal_text": "We adopt the following six step algorithm for each parent node from the source-side parse tree:",
        "GPT2_formal_text": "= u_t, q_t. For some other words in a sentence, we'll start by setting the following weights: u_s_i = u_s_i + (1 - (w_i-1)/2). Formal: For the first time, we've figured out how to use the info from word embeddings to predict sentence representations. We tested this on two English datasets and it worked really well. Formal: We've also come up with a new approach that uses entropy for language modeling. This method works really well across different tasks. Formal: Even though we think these new features could be useful for other NLP tasks, we noticed that they work really well in our setup and outperform other methods we tried. Formal: We're now sharing our code for anyone to check out and use. Formal: We've tested it out and found that our method is more effective than the usual methods for language modeling. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2009.eamt-1.27.json"
    },
    {
        "casual_text": "SR might tweak the slot values in the translated sentences, so we use fuzzy value detection to deal with issues like similar sounds or pronunciation problems when it's pulling out slot values to get a semantic label, y. But instead of fixing the noisy value with the original one, we actually let the misrecognition happen because we're okay with that. So, y = y is totally fine. Also, we normalize numerical terms to handle spoken numbers better. Most slot values can be sorted out by our automatic detection rules. The ones that are too different to recognize just get tossed out along with their labels.",
        "formal_text": "Since SR may modify the slot values in the translated utterances, fuzzy value detection is employed here to handle similar sounds and liaison problems when it extracts slot values to obtain a semantic label y. However, we do not replace the noisy value with the original value as we encourage such misrecognition in SR, thus y = y is allowed. Moreover, numerical terms are normalized to deal with the spoken number problem. Most slot values could be relocated by our automatic value detection rules. The remainder slot values which vary too much to recognize are discarded along with their corresponding labels.",
        "GPT2_formal_text": ", res_mask, and rel_mask are used for the actual task and the sub-tasks (D and Q), respectively. In the experiments, we'll look at these sets of weights. Formal: We tweak the regularization parameters (β) using a different loss function, L_T, which was trained on the TACRED dataset. In Section 3, we'll show how we did this by running some experiments. Formal: We’ve developed a new baseline model called F-Tuned CRF (FT-CFRF). This model not only uses the knowledge from the knowledge pool but also tweaks the weights that were originally used for the transfer learning part. As you can see in Figure 2, we use a special loss function called L_T. You can check out the specifics in Section 3. Formal: We also made some tweaks to the usual transfer learning framework, like λ_t, by using a learning rate decay after each iteration. This helps the model keep better track of how it’s improving during training. Formal: We use a recursive neural network (RNN) to handle the forward-backward translation in the translation model. Formal: We’ve set the maximum number of steps to 4, and the size of the vocabulary to 512. Formal: We’ve also added two more types of loss terms to the model, β_t and L_T, to help the model adjust its weights. Formal: Lastly, we’ve added a multi-task learning setup with two loss functions: L_T and λ_t. We’ll explain how we combine the two loss functions and optimize the model using the TACRED dataset. Formal: In Section 4, we’ll compare our model to two other approaches. Formal: In Section 5, we’ll show how well our framework performs on two other datasets. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.192.json"
    },
    {
        "casual_text": "Alright, so following the steps in Section 3, we start by running EDITS and VENSES on the [T, H] RT E5-sample and [T, H] RT E5-mono. The accuracies for these runs are listed in Table 3.",
        "formal_text": "Following the methodology described in Section 3, at step 1 we run EDITS and VENSES on [T, H] RT E5−sample, and on [T, H] RT E5−mono (Table 3 reports the accuracies obtained).",
        "GPT2_formal_text": "= {0, 1, ..., m} from the general category. Formal: We use a multi-level perceptron to figure out the final label sequence, which is represented as y_i = {y_i1, ..., y_iT}. Formal: Also, the original label tokens y_ij in the training data are kept as they are for each training instance. Formal: The main goal of the M-step is to make sure the model is close to the average performance of the best possible sequence label (the minimum error) it gets after training. Formal: The model is trained using a linear projection from the original data. The gradient is calculated using a stochastic gradient descent algorithm, following the method described by Kingma and Ba (2014). Formal: The gradient for the D-step is calculated using a Bayesian model that follows the approach from Luong et al. (2015). Formal: This system is a bit different from the usual LSTM-based models, which use an attention mechanism that grabs both the attention from the input and the current state from the output. Formal: For training, we use a few different sequence-level language models to see how they stack up against each other. Formal: Since the D-step is super important for maximizing the performance of the model, we use a linear regression model to evaluate how well the model is doing. Formal: We use the RNNLM (Bengio et al., 2003) to predict the final label sequence y_i. For the LSTM-based models, we train the model using the character-level language model G_c = {c_1, ..., c_T} from the general category. Formal: G_c includes a nonlinear layer to predict the character-level sequence. We calculate the prediction probabilities for the LSTM-based models by using the loss function from the LSTM-based model, as described by Gao et al. (2016). Formal: Since the LSTM-based models are more stable and don't overfit, we use a linear regression model to evaluate how well the model is performing. Formal: The LSTM-based models are trained using a linear projection from the original data, following the method described by Luong et al. (2015). Formal: This setup helps the model understand the input better and avoid overfitting. Formal: Lastly, we update the model with",
        "directory": "C10",
        "filename": "C10-2012.json"
    },
    {
        "casual_text": "Got it! Just so you know, there can be more than one connection between two nouns. So, a single dependency path might actually represent multiple relationships.",
        "formal_text": "Note that, there may be more than one relation existing between two nominals. A dependency path thus may correspond to multiple relations.",
        "GPT2_formal_text": "_t2d, where t2d is the size of the hidden state, and t1d is the size of the hidden state at the beginning of the output sequence. Formal: The vector representation of the source token x is basically the sum of the vector representations for all its tokens. Formal: If you only have one instance of a source token x, you can just use the one-hot representation (1) from the first hidden state. Formal: The way you represent the source word x is determined by the first hidden state of the source embedding layer, which is d = hs_c(x). Formal: To get the embedding of the target word x, you calculate the vector representation for x like this: Formal: Using the embedding of the source word x, you can use the matrix multiplication (3) on the output of the source embedding layer, which gives you the embedding for x. Formal: To get the vector representation for the target word x, you multiply the embedding matrix (1) with the matrix representation for the target embedding layer, which gives you the vector representation for x. Formal: In the POS predictor, they use the same embedding representations for both words. Formal: It's pretty clear that using the word embedding vectors works really well for predicting POS tags. Formal: Using a vector representation for the source word x is really similar to using the embedding for the target word x. Formal: The POS predictor can also use the embedding representations from the source word embedding layer to predict the POS tags for both words. Formal: In the first hidden state, the vector representation for the source word x is 1, and the corresponding vector representation for the target word x is 0. Formal: Formal: The POS predictor uses the same embedding vectors for both words. Formal: Formal: The POS predictor uses the embedding vectors for both words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D15",
        "filename": "D15-1062.json"
    },
    {
        "casual_text": "We’ve got RuSentiment, a dataset based on public posts from VKontakte (VK), which is the biggest Russian social network with around 100 million active users every month. RuSentiment was created using some fresh, detailed guidelines that made the annotation process quick and easy while still covering a wide variety of sentiments, both obvious and subtle. The agreement between the people doing the annotations, measured by Fleiss' kappa, is 0.58. Overall, 31,185 posts were labeled, with 21,268 picked randomly (including 2,967 for the test set). Another 6,950 posts were chosen using a strategy kind of like active learning to make sure the data was diverse. This makes RuSentiment the biggest publicly available sentiment dataset for social media, and also the largest general-purpose sentiment dataset for Russian, which isn’t as widely studied as other languages.",
        "formal_text": "We present RuSentiment, a dataset of public posts on VKontakte (VK), the largest Russian social network that currently boasts about 100M monthly active users. 1 RuSentiment was developed with new comprehensive guidelines that enabled light and speedy annotation while maintaining consistent coverage of a wide range of explicitly and implicitly expressed sentiment. The overall inter-annotator agreement in terms of Fleiss' kappa stands at 0. 58. In total, 31, 185 posts were annotated, 21, 268 of which were selected randomly (including 2, 967 for the test set). 6, 950 posts were pre-selected with an active learning-style strategy in order to diversify the data. This makes RuSentiment the largest openly available sentiment dataset for social media, and the largest general domain sentiment dataset for this relatively low-resource language.",
        "GPT2_formal_text": "[i] = d;  //for each token idx i, check if the text comes from the chosen class.  // Then, find the nearest class vector that has the same text embedding as the chosen class vector. Formal: The model takes in a sequence of embeddings, let's call it e_s. Formal: So, e_s is a matrix with dimensions n by k, where n is the number of tokens and k is the total number of tokens in the text. Formal: The recurrent neural network, or RNN, takes in the output from the text encoder. Formal: Basically, the recurrent neural network (RNN) takes in the output from the text encoder. Formal: If the text embedding isn't the same as the chosen class vector, the RNN will give a probability score of 0. Formal: The attention mechanism acts as a kind of teacher and helps the model learn better by paying attention to the tokens that might hint at a specific class label. Formal: The attention mechanism is a type of neural network that uses the attention mechanism to focus on the parts of the text that are most important for its chosen class. Formal: The attention mechanism is a type of neural network that uses the attention mechanism to focus on the parts of the text that are most important for its chosen class. Formal: We use different techniques to figure out which words are important for the chosen class. Formal: We use different techniques to figure out which words are important for the chosen class. Formal: We use a simple greedy algorithm to estimate the importance of the words in the chosen class. Formal: We use a greedy algorithm to estimate the importance of the words in the chosen class. Formal: We use a greedy algorithm to estimate the importance of the words in the chosen class. Formal: We use a greedy algorithm to estimate the importance of the words in the chosen class. Formal: We use a greedy algorithm to estimate the importance of the words in the chosen class. Formal: We use a greedy algorithm to estimate the importance of the words in the chosen class. Formal: Formal: We use a greedy algorithm to estimate the importance of the words in the chosen class. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C18",
        "filename": "C18-1064.json"
    },
    {
        "casual_text": "Neural networks have gotten really good at handling two tasks at once: predicting charges and picking out the related legal articles. We’ve built a system that ties these two things together, using something called a two-stack attention mechanism to make sense of how the details of a case connect to the laws and charges. \n\nHere’s how it works: we use a couple of fancy tools—sentence-level and document-level Bi-GRUs (kind of like advanced Recurrent Neural Networks)—along with some attention components to figure out how words and sentences fit together. This helps us understand the big picture of the case and the important details. Once we’ve analyzed the case description, we use another set of attention components to focus on the most relevant legal articles that back up our charge prediction. We’re dealing with this in a multi-label way, meaning a case can have more than one charge.\n\nWe tested this model by predicting charges for criminal cases in China. We grabbed publicly available court documents from the Chinese government website, where we could automatically pull out the case details, related laws, and charges using basic rules. Check out Figure 1 for how this works.\n\nThe results? Our neural network method does a solid job of predicting the right charges for a case and also points out the legal articles that support those predictions.",
        "formal_text": "Recent advances in neural networks enable us to jointly model charge prediction and relevant article extraction in a unified framework, where the latent correspondence from the fact description about a case to its related law articles and further to its charges can be explicitly addressed by a two-stack attention mechanism. Specifically, we use a sentence-level and a documentlevel Bi-directional Gated Recurrent Units (Bi-GRU) (Bahdanau et al. , 2015) with a stack of factside attention components to model the correlations among words and sentences, in order to capture the whole story as well as important details of the case. Given the analysis of the fact description, we accordingly learn a stack of article-side attention components to attentively select the most supportive law articles from the statutory laws to support our charge prediction, which is investigated in the multi-label paradigm. We evaluate our model in the context of predicting charges for criminal cases in China. We collect publicly available judgement documents from China's government website, from which we can automatically extract fact descriptions, relevant law articles and the charges using simple rules, as shown in Figure 1. Experimental results show that our neural network method can effectively predict appropriate charges for a given case, and also provide relevant law articles as legal basis to support the prediction.",
        "GPT2_formal_text": "ed and attention_attention_multiscale are set to 2. Formal: The token embeddings from the last layer are passed into a fully-connected layer. This layer processes the token embeddings, averaging them, and then spits out the result. Formal: For each document, we look at the average attention score and average of attention scores for all the tokens. Then, we average the attention scores for the entire document to get the final score. Formal: Next, we calculate the bias matrix A_b, which is a matrix with dimensions d_b×d_b. This matrix is adjusted by the bias term β_b. It’s a sigmoid function with a parameter α. Formal: Using the attention weights in A_b, we calculate the attention weights for the document using the following formula: Formal: Next, we calculate the cross-modal attention weights A_c_t by combining the cross-modal attention weights A_c_t_1, . . . , A_c_t_l for each token in the document. For the first token, we just multiply A_c_t_1 by the attention weights A_c_t_l from the last layer. Formal: Finally, we use the cross-modal attention weights A_c_t_l_1, . . . , A_c_t_l_l to get the cross-modal attention weights A_c_t_l. Formal: Formal: Lastly, we use the cross-modal attention weights A_c_t_l_1, . . . , A_c_t_l_l_l_r for each token in the document. Formal: Formal: The attention weights A_c_t_l_1, . . . , A_c_t_l_l_l_r are adjusted by the attention term β_l. Formal: After that, we apply the cross-modal attention weights A_c_t_l_1, . . . , A_c_t_l_l_l_r to the document and get the final attention weights A_c_t_l_r. Formal: Formal: We also use the cross-modal attention weights A_c_t_l_l_1, . . . , A_c_t_l",
        "directory": "D17",
        "filename": "D17-1289.json"
    },
    {
        "casual_text": "1. Basically, some cases have stricter rules about what words can go with them than others. For instance, in the accusative case, \"to subscribe\" has a tighter set of rules compared to \"to take/steal,\" which can work with a wider range of objects.",
        "formal_text": "1. httuitively speaking, the, contribution of the 2. The seleetional restriction of a certain case is stronger than those of others. For example, in tile accusative, the selectional restriction of \"to subscribe\" is stronger than that of \"to take/steal\" which Mlows various kinds of objects as its case filler.",
        "GPT2_formal_text": "[i] = i; } else if (i < k) { parse_id[i] = 0; } } return parse_ids; }\n\nThe function parse_ids(i) figures out how many tokens are in a list of tokens and their parse tree labels. It then uses the function parse_trees(trees) to figure out the expected total number of tokens, which is the product of the log probability of the token and the sum of the log probabilities for all the other tokens in the tree. Formal: The function parse_trees(trees) is a greedy algorithm that tries to find the best, most likely parse tree for a given token. It calculates the expected number of tokens based on the probabilities from the parse tree and the tree itself. Formal: The function parse_ids(i) is a greedy algorithm that counts how many tokens are in a list of tokens and their parse tree labels. It then averages these numbers to get the expected number of tokens. Formal: Formal: The function parse_trees(trees) is a greedy algorithm that tries to find the best, most likely parse tree for a given token. It calculates the expected number of tokens based on the probabilities from the parse tree and the tree itself. Formal: Formal: The function parse_ids(i) is a greedy algorithm that counts how many tokens are in a list of tokens and their parse tree labels. It then averages these numbers to get the expected number of tokens. Formal: Formal: The function parse_trees(trees) is a greedy algorithm that tries to find the best, most likely parse tree for a given token. It calculates the expected number of tokens based on the probabilities from the parse tree and the tree itself. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C96",
        "filename": "C96-1012.json"
    },
    {
        "casual_text": "There have been a bunch of question answering datasets created over the years (like the ones by Berant et al. in 2013, Joshi et al. in 2017, Trischler et al. in 2017, and Rajpurkar et al. in 2018, among others). But all of these were focused on answering single questions at a time. Saha et al. (2018) took a different approach and looked into answering a series of questions in a row, creating a dataset for that purpose. \n\nHowever, we’re doing things a bit differently from them in two main ways: \n1) They worked with question answering based on structured knowledge bases. \n2) Their dataset was kind of artificial: they had human annotators come up with templates using knowledge base predicates, and then they grouped individual questions together by predicate or subject to create sequences.",
        "formal_text": "Several question answering datasets have been proposed (Berant et al. , 2013; Joshi et al. , 2017; Trischler et al. , 2017; Rajpurkar et al. , 2018, inter alia). However, all of them were limited to answering individual questions. Saha et al. (2018) study the problem of sequential question answering, and introduce a dataset for the task. However, we differ from them in two aspects: 1) They consider question-answering over structured knowledge-bases. 2) Their dataset construction was overly synthetic: templates were collected by human annotators given knowledge-base predicates. Further, sequences were constructed synthetically as well by grouping individual questions by predicate or subjects.",
        "GPT2_formal_text": "ed_pu * h = {h1, ..., hN} and b = {b1, ..., bN}, where N is the total number of tokens. Then, the attention weights δ_ij are combined using ReLU, which helps the model understand the context better. Formal: To understand how attention is used in different situations, we tried out the other methods too. To do this, we calculated the cross-entropy loss for each attention head using a formula (Equation 1). Formal: The cross-entropy loss for each attention head is calculated using the formula (Equation 1). Formal: Here, a_i is the attention head's vector for token i, and b_i is the attention head's vector for token i+1. Formal: The cross-entropy loss for each attention head is calculated using the formula (Equation 1). Formal: We’ve set up the transformer model to focus on the most important input token and its own attention head. We tested three different setups: (a) with the same number of parameters as the main model, (b) with a random number of parameters, and (c) with the best model that works for all the tokens in the input sequence. Formal: Formal: We’re also including a loss function that’s based on the dot product between the attention vector and the word embedding vector. This means it’s not just the average cross-entropy between the attention vector and the word embedding vector, but also the average cross-entropy between the word embedding vector and the attention vector. Formal: Formal: The loss for the attention head gets smaller the more tokens are in the sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1134.json"
    },
    {
        "casual_text": "When it comes to figuring out what people are doing in a conversation—like asking questions, giving orders, or just chatting—people have come up with different ways to analyze it. For instance, Samuel and his team (1998) looked at how people talk, like the direction of the conversation, punctuation, special phrases, and n-grams to classify what’s being said. They also used things like tone, word choice, and sentence structure to help with this. More recently, Julia and Iftekharuddin (2008) and Sridhar and others (2009) did a great job using sound and tone features to classify dialogue acts. \n\nOn the other hand, Louwerse and Crossley (2006) used n-grams—which can work for both spoken and written dialogue—and tested them with a dataset called the Map Task Corpus (from Anderson and others, 1991). Bangalore and his team (2006) took this a step further by using n-grams from the last 1-3 things said to figure out what the current part of the conversation is about.",
        "formal_text": "For classifying dialogue acts in spoken dialogue, various features such as dialogue cues, speech characteristics, and n-grams have been proposed. For example, Samuel et al. (1998) utilized the characteristics of spoken dialogues and examined speaker direction, punctuation marks, cue phrases and ngrams for classifying spoken dialogues. used prosodic, lexical and syntactic features for spoken dialogue classification. More recently, Julia and Iftekharuddin (2008) and Sridhar et al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features-which could be adapted to both spoken and written dialogue-and tested them using the Map Task Corpus (Anderson et al. , 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1-3 utterances in order to classify dialogue acts for the target utterance.",
        "GPT2_formal_text": "= { [WORD1, WORD2] } | {[WORD1], [WORD2] }  \n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |  \n| i, j |  \n| j |  \n| | |\n| o |",
        "directory": "D10",
        "filename": "D10-1084.json"
    },
    {
        "casual_text": "Baseline models: We're looking at a few different models here. First up, there's the CAPT model, which uses soft attention on a pair of input images. This attention thing is kind of like what's been done in image captioning before (like in Xu et al., 2015), but instead of just one image, we're dealing with two. In the CAPT model, we don't mess with any masking—we just skip the cluster info. The goal is to create a single sentence, so it's like a regular captioning model but with attention on two images.\n\nNext, there's the CAPT-MASK model, which is pretty much the same as CAPT, but it adds a masking mechanism. This mask is made by combining all the cluster masks for the image. We also have a version of CAPT that predicts a whole multi-sentence description—we call this one CAPT-MULTI. For this, we just slap all the sentences together in any old order.\n\nWe also tried a nearest neighbor approach (NN-MULTI), where we just grab the annotation from the closest matching training data point. We figure out how close they are based on the features we pull from the image pair and use the Nearest-Neighbor module from sklearn (Pedregosa et al., 2011). For the single sentence version (NN), we just randomly pick one of the sentences from the annotation.\n\nLastly, there's a version of the DDLA model with a fixed uniform prior, and we call this one DDLA-UNIFORM.",
        "formal_text": "Baseline models: We consider following baseline models: CAPT model considers soft attention over the input pair of images (This atten- tion mechanism is similar to that used in prior image captioning works (Xu et al. , 2015), except that we have two images instead of a single image input). We do not perform any masking in case of CAPT model, and simply ignore the cluster information. The model is trained to generate a single sentence. Thus, this model is similar to a typical captioning model but with soft attention over two images. CAPT-MASK model is similar to CAPT model except that it incorporates the masking mechanism defined earlier using the union of all the cluster masks in the corresponding image. We also consider a version of the CAPT model wherein the target prediction is the whole multi-sentence description -CAPT-MULTI -for this setting, we simply concatenate the sentences in any arbitrary order 2. Additionally, we consider a nearest neighbor baseline (NN-MULTI), wherein we simply use the annotation of the closest matching training data point. We compute the closeness based on the extracted features of the image pair, and leverage sklearns (Pedregosa et al. , 2011) Nearest-Neighbor module. For single sentence setting (NN), we randomly pick one of the sentences in the annotation. We also consider a version of DDLA model with fixed uniform prior, and refer to this model as DDLA-UNIFORM.",
        "GPT2_formal_text": "ing is like a masked language model, where you take the input x and turn it into a hidden representation y. Then, you calculate the attention scores by combining the hidden representations. Formal: We also add a step where we check if the answer fits the context, using a three-layer perceptron. Formal: For each sentence s in the training set, we have the domain-specific context vector c_s, the answer-specific context vector c_a, and the embedding vector e_s for the sentence s. Then, we calculate the cross-entropy loss like this: Formal: A more efficient method would be to train the whole model on one dataset, using an LSTM to get the model's input, and then feed that into the final model to make predictions. Formal: For each sequence s_t in the training set, we have the embedding of s_t, the cross-entropy loss for the sequence s_t, and the embedding vectors for the surrounding sentences s_t, and the sequence e_s for the sentence s_t. Finally, we calculate the cross-entropy loss like this: Formal: Lastly, we use the same setup as in Equation (6) but with the same loss function. Formal: For more details on the model and its parameters, check out the original paper. Formal: As mentioned in earlier papers, we train the model using different datasets to check how well it performs. The usual dataset used for this is the Wikipedia dump of Wikipedia articles. Formal: We use a straightforward method to pick out sentences that make sense to answer the question q, making sure to include sentences that cover most of the important points from the training data. Formal: We use a special feature called T to handle the question's structure. Formal: We train the model by picking sentences from the training set that match the question's structure and have the right amount of named entities. Formal: Finally, we use the same method to select the best answer in the test set, making sure to keep the sentences that cover most of the important points from the training set. Formal: Formal: The question we're aiming for is a knowledge-based question, so we only use sentences that are related to that. Formal: To test this, we create a question-answer pair by picking sentences from the training set that are related to the question. Formal: For each language in the training set, we",
        "directory": "D18",
        "filename": "D18-1436.json"
    },
    {
        "casual_text": "Organizing papers for a conference and making sure sessions have related topics can be really time-consuming, especially when you have a lot of papers. Machine learning can help automate this process by using natural language processing (NLP) to figure out what each paper is about. In this case, putting together a conference schedule is kind of like a semi-supervised clustering problem.\n\nIn this paper, we introduced the ADoCS system, a web app that can group papers into clusters based on how similar they are. The groups are created based on the size distribution you set up. Right now, the app is mainly designed for organizing conference papers, but it could be used for other document clustering tasks with some restrictions, thanks to its flexible interface (you can choose different metrics, use TF-IDF, etc.).",
        "formal_text": "Arranging papers to create an appropriate conference schedule with sessions containing papers with common topics is a tedious task, specially when the number of papers is high. Machine learning offers techniques that can automatise this task with the help of NLP methods for extracting features from the papers. In this context, organising a conference schedule can be seen as a semisupervised clustering. In this paper we have presented the ADoCS system, a web application that is able to create a set of clusters according to the similarity of the documents analysed. The groups are formed following the size distribution configured by the user. Although initially the application is focused on grouping conference papers, other related tasks in clustering documents with restrictions could be addressed thanks to the versatility of the interface (different metrics, TF-IDF transformation).",
        "GPT2_formal_text": "= { idx = i, proj = k }, which is what the system uses to figure out the relationship between a pair of things. In a mixed-domain document-topic model, we use cross-entropy loss to calculate these cross-entropy losses. Formal: The cross-entropy loss is calculated using equation (1). Formal: If you have a document d and a topic vector z, you can rewrite it as d = z + Σe. Formal: Using a topic vector q and a document d, you can rewrite the topic vector z like this: Formal: For a document d, the cross-entropy loss is calculated like this: Formal: Formal: Here, Σ is a non-linear function that helps the model learn the topic vector q and the document vector z at the same time. Formal: The cross-entropy loss is calculated using equation (1). Formal: Here, Σ is a non-linear function that helps the model learn the document vector q and the topic vector z at the same time. Formal: Formal: Formal: Finally, the cross-entropy loss is calculated using equation (1). Formal: For a document d, the cross-entropy loss is calculated like this: Formal: Formal: The cross-entropy loss is calculated using equation (1). Formal: Formal: Lastly, the cross-entropy loss is calculated using equation (1). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E17",
        "filename": "E17-3011.json"
    },
    {
        "casual_text": "We're introducing AutoNER, a fresh neural model that uses a new \"Tie or Break\" approach for distantly supervised NER tasks. We've tweaked the usual NER model to create the Fuzzy-LSTM-CRF model, which works really well as a strong baseline for distantly supervised tasks. We're also looking into ways to improve distant supervision for better NER results, like adding high-quality phrases to cut down on mistakes, and we've done some experiments to check how well it works. Our tests on three popular datasets show that AutoNER performs the best when it's just using dictionaries without any extra help from humans, and it can even go head-to-head with models that use supervised benchmarks.",
        "formal_text": "• We propose AutoNER, a novel neural model with the new Tie or Break scheme for the distantly supervised NER task. • We revise the traditional NER model to the Fuzzy-LSTM-CRF model, which serves as a strong distantly supervised baseline. • We explore to refine distant supervision for better NER performance, such as incorporating high-quality phrases to reduce false-negative labels, and conduct ablation experiments to verify the effectiveness. • Experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort and is even competitive with the supervised benchmarks.",
        "GPT2_formal_text": "ed is a kind of attention mechanism that helps prevent the model from making mistakes. It works by using something called an attention mechanism, which you can think of as a way to pay attention to the parts of the input that relate to the current target label. The model can generate tokens based on the current label and the previous output. Once it's done with that, it combines all the attention from all the tokens to create a single representation. Formal: Another way to handle this is using a multi-head attention mechanism, which is called Multi-head Attention (or more broadly, Multi-head Attention for short). The multi-head part helps by focusing on different parts of the input, kind of like how the hidden state helps the output. For instance, if we're dealing with a sentence like \"A woman is being attacked by a dog,\" the attention is more focused on the words around the woman and her attacker. Formal: This multi-head attention can be shown as a straightforward bidirectional one-to-one mapping where all the tokens are the same size. Formal: Multi-head attention is great for handling lots of different inputs, but we noticed that it can sometimes struggle with just a few tokens. So, we came up with a new approach called Multi-Head Attention (or Multi-Head for short) to deal with more tokens. Formal: Here's what Multi-Head does: Formal: First, it maps the input into a fixed-size space. Then, it figures out how to best use all that info to create the output. Formal: The big difference between Multi-Head and the old attention-based models is that Multi-Head has multiple heads to focus on each token, each with its own set of attention heads. Formal: Multi-Head can be used on its own as a general-purpose model, but for tasks that need specific attention, like sentence classification, it's better to combine it with an attention model that focuses on the input. Formal: Using an attention model that focuses on the input can help the model avoid making mistakes, but it can also make it hard to get good results. Formal: The basic version of Multi-Head can be seen as a two-layer attention model with a fixed attention size of h. Formal: We can also use a modified version of Multi-Head to represent the input as a flat matrix called h_i. Formal: In this case, we have a fixed-size space and a fixed attention",
        "directory": "D18",
        "filename": "D18-1230.json"
    },
    {
        "casual_text": "Since the CoNLL-2012 dataset only tags mentions that are part of coreference chains, we start by creating a mention extraction module that’s really good at finding mentions but might not be super precise. This module mainly uses syntactic parse trees to do its job. We grab all NP nodes (noun phrases), QP nodes (like complex phrases for amounts or measurements), and any terminals with POS tags like PN (pronouns) and NR (proper nouns) to build a big list of potential mentions. \n\nAfter that, we use some rules to filter out mentions that probably aren’t right. For example, we remove mentions that include:\n1. Measure words like \"一年\" (one year) or \"一顿\" (one time).\n2. Named entities that fall into categories like PERCENT, MONEY, QUANTITY, or CARDINAL.\n3. Interrogative pronouns like \"什么\" (what) or \"哪儿\" (where).",
        "formal_text": "Since only the mentions that take part in coreference chains are annotated in the CoNLL-2012 shared task data set, we first generate a high-recall, lowprecision mention extraction module to extract as many mentions as possible. The mention extraction module relies mainly on syntactic parse trees. We extract all NP nodes, QP (quantifier phrase, i. e. , complex amount/measure phrase) nodes, and all terminals with part-of-speech tags PN (pronoun) and NR (proper noun) in parse trees to form a mention candidate set. Then, we employ some rules to remove unlikely mentions, e. g. , those which contain (1) measure words such as '一年/one year' and '一顿/one time'; (2) named entities whose categories are PERCENT, MONEY, QUANTITY, and CARDINAL; (3) interrogative pronouns such as ' 什么/what' and '哪儿/where'.",
        "GPT2_formal_text": ") is the number of documents where the relationship between the query and the article (called relation_i) shows up in both. And for the articles, the relevance_score(p_j, a_j) is the average of all the relevance scores for a_j in all the articles. Formal: To get a bunch of relations from a big pile of text, we start by running each relation query through a graph neural network. Then, we calculate the relevance score for each graph node i in that same text collection. Formal: We use the relevance score for each graph node i in a text collection and a random selection of text nodes from the collection. Formal: In the diagram below, nodes i and j are the graph nodes and its edge weight, respectively. Formal: The graph nodes i and j are the graph nodes and its edge weight, respectively. Formal: The graph nodes i and j are the graph nodes and its edge weight, respectively. Formal: The graph nodes i and j are the graph nodes and its edge weight, respectively. Formal: Each graph node i is a node on the edge weight of a graph node j. Formal: Graph nodes i and j are graphs on the edge weights of their graph nodes. Formal: Each graph node i is a node on the edge weight of a graph node j. Formal: Formal: Each graph node i is a graph on the edge weight of a graph node j. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D13",
        "filename": "D13-1028.json"
    },
    {
        "casual_text": "Can machine translation (MT) help the Department of Defense (DoD)? I think the answer is a solid, \"Yes, but with some caveats.\" We’ve shown this in a bunch of real-world cases and with tech solutions that use MT engines, which are part of several programs. Over the past week, you’ve seen and heard a lot of examples of how MT adds value to the DoD and other federal agencies. I’ll just touch on a few of the programs currently being used by the DoD. I’ll share specific examples where MT helped with intelligence work—all of it is unclassified and can be found online.",
        "formal_text": "Can MT help DoD? I believe the answer is a qualified, YES it can. We have proven this through numerous specific instances and with technology solutions where the underlying MT engines can be found in several of the following programs. You have seen and heard throughout this past week numerous examples of their value to DoD and other federal agencies. I will discuss and recap only a few of the programs that are currently being used within DoD. I will discuss specific examples where their value assisted in the Intelligence process; all unclassified and available through open sources.",
        "GPT2_formal_text": "ed_content. The final attention token is at position t_0. The attention mechanism is used to figure out the attention weights, which are learned in a softmax layer. After that, the output representation is passed on to the next part of the process. Formal: The attention mechanism is trained using a mix of both supervised and unsupervised learning. The unsupervised part is all about making sure the generated responses follow the code's structure. This is done by tweaking the HATM's internal representation of the code. Formal: To train the neural generation part, we use a Support Vector Machine (SVM) to predict the answers. The response distribution from the SVM is then used as the starting point for the HATM. Formal: The entity selection module is trained to pick the right entities from the code's vocabulary. To get a better understanding of the generated responses, the response-related features, and the response's context, we start with two main goals in mind. Formal: To make sure the generated responses fit the code's structure, we use an LSTM to encode the code's tokens. The output from this LSTM is the code token representation, which is represented as h_c. Formal: Our goal is to make sure the responses for an entity pair are similar to the ones for its related entity. This is done by creating a special code token representation for each entity pair, which we call h_e_i. Formal: Following the method from (Zhang et al., 2017), we use a CRF layer to create a vector representation for the entity pair. Formal: We use a span-level attention mechanism to create a response representation h_p. Formal: Lastly, we use a sequence-level attention mechanism to generate the response h_s. Formal: We generate the response h_s using the sequence-level attention mechanism, h_s = [h_s1, h_s2, ..., h_sT]. Formal: We also calculate the attention weights for the code and entity pairs to get their representation, which we call h_c_i. Formal: Finally, we add the relevant codes and entities to the HATM's output to get their representation, h_o. Formal: We use the entity selection module and the sequence-level attention mechanism to build the HATM, and then we pick the best responses using the span-level attention mechanism. Formal:",
        "directory": "amta",
        "filename": "2008.amta-additional.1.json"
    },
    {
        "casual_text": "We've also presented and explained why using partial rewrite rules is a good idea. These rules keep the important formal stuff intact but work really well with the tech we have right now.",
        "formal_text": "We have also prcscntcd and justified the use of partial rewrite rulcs which, whilc maintaining the essential formal properties, arc computationally effective with available technologies.",
        "GPT2_formal_text": "= { \"answer\" } from Eq. 2. Formal: In this project, we're using a graph-based approach with a well-organized structure to figure out the relationship between words and phrases in text. Formal: We're using the method from (He et al., 2017a) to pull out different types of relationships, like event, topic, and reference. Figure 1 gives an example of a word-phrase graph. Formal: Here, the edge (w, r) shows if the pair (e, r) makes sense in a sentence. The connections (e, r) can be positive or negative. We break down the relationship into two main types: 1) co-occurrence (e, r) or 2) non-co-occurrence (e, r). For co-occurrence, we use the word or phrase that comes before (e, r) as a reference. For non-co-occurrence, we use words or phrases that aren't tied to any specific event or topic. Formal: For the co-occurrence type, we look for words that have a higher chance of appearing together, specifically words that are related in meaning. For example, \"cats\" is linked to \"cats\" and \"lion\" is linked to \"lion cubs.\" We get the scores for these words and phrases to figure out their co-occurrence relationships. For example, for \"lion cubs\" and \"lion,\" we get scores like \"feline\" and \"lion\" as positive relationships and \"lion\" and \"cat\" as negative ones. Formal: For the non-co-occurrence type, we look for words that have a higher chance of appearing together, specifically words that are related in meaning, like \"cats\" and \"lion.\" We get the scores for these words and phrases to figure out their non-co-occurrence relationships. For example, for \"lion\" and \"lion cubs,\" we get scores like \"feline\" and \"lion\" as positive relationships and \"lion\" and \"cat\" as negative ones. Formal: For the co-occurrence type, we look for words that have a higher chance of appearing together, specifically words that are related in meaning, like \"cats\" and \"lion.\" We get the scores for these words and phrases to figure out their co-occurrence relationships. For example, for \"lion\" and \"",
        "directory": "E91",
        "filename": "E91-1030.json"
    },
    {
        "casual_text": "Let's dive into how BioBERT, the toughest model we have, handles things when we give it a hard time. We looked at how it deals with different tricky situations in the BC5CDR dataset, which is all about chemicals and diseases. We messed with the data a bit by using synonyms, swapping words around, and even doing some keyboard-style typos to see how well BioBERT could still figure things out.",
        "formal_text": "BioBERT Error Analysis This section seeks to understand how the most robust model -BioBERT -behaves under adversarial evaluation. To this end, we analyzed NER model confusions with respect to the original datasets, synonym (S), swap (W), and keyboard (K) perturbations on the BC5CDR chemical and disease dataset(s).",
        "GPT2_formal_text": "ed(Q, A) is basically a mix of the output from the attention module, the hidden state from the encoder, and the attention weight. Formal: The key idea here is that while the encoder is being built, it uses a hidden state H from the last layer of the encoder, along with its previous hidden state h_t, to decide what to focus on. This hidden state H_t is then combined with the final output to form the final response. Formal: Using the attention mechanism we just talked about, the attention-based decoder builds on the output from the encoder to create a better output. Formal: The encoder, on the other hand, uses the output from the encoder to help with understanding the input. So, the encoder is like a generator that takes the input (Q_t) and turns it into a response (A_t). Formal: We can also use attention to combine the outputs from different parts of the decoder, depending on what we need. Formal: In Section 3, we'll see how to use attention for this. Formal: There are a few ways to represent the attention mechanism. Formal: The usual way, called discriminative attention, uses a simple linear function to keep track of how much the input is influencing the output. Formal: Some researchers, like Liu and Belz (2014) and Zhou and Zhang (2015), have looked into this for generating responses. Formal: The attention-based network is often used for text classification tasks, especially when there's not a lot of labeled data available. Formal: These days, there are a few versions of the attention-based network, like the hierarchical attention network (HAN) (Vaswani et al., 2017), the multi-head attention network (MHAN) (Zhang et al., 2019), and the multi-head attention network (MHAN-bi) (Zhou and Zhang, 2019). Formal: There's also a model called the multi-head attention network (MHAN-bi) (Zhou and Zhang, 2019). Formal: The difference between these two models is that MHAN-bi uses a hierarchical attention network with multiple heads, while MHAN-bi-multi uses a multi-head attention network with multiple heads. Formal: In both MHAN-bi and MHAN-bi-multi, the outputs from the encoder and decoder are combined, which gives us a better",
        "directory": "bionlp",
        "filename": "2021.bionlp-1.13.json"
    },
    {
        "casual_text": "The abstractive dialogue summarization task has been a topic in research, especially when looking at the AMI meeting corpus (McCowan et al., 2005). For example, Banerjee et al. (2015), Mehdad et al. (2014), and Goo and Chen (2018) have all talked about it. The issue is that the corpus doesn’t have many summaries—only 141 dialogues have them. So, Goo and Chen (2018) suggested using the assigned topic descriptions as a kind of reference. These are short, kind of like labels, that describe the meeting goals, like \"costing evaluation of project process\" or \"components, materials, and energy sources.\" There’s also stuff like \"chitchat.\" But these descriptions are pretty vague and don’t give much detail about the structure or who said what.",
        "formal_text": "The challenges posed by the abstractive dialogue summarization task have been discussed in the literature with regard to AMI meeting corpus (McCowan et al. , 2005), e. g. Banerjee et al. (2015), Mehdad et al. (2014), Goo and Chen (2018). Since the corpus has a low number of summaries (for 141 dialogues), Goo and Chen (2018) proposed to use assigned topic descriptions as gold references. These are short, label-like goals of the meeting, e. g. , costing evaluation of project process; components, materials and energy sources; chitchat. Such descriptions, however, are very general, lacking the messenger-like structure and any information about the speakers.",
        "GPT2_formal_text": "= { w_t, w_s, ..., w_l }, and the output_ids are { x_t, x_s, ..., x_l }. Formal: We train a language model to identify pairs of entities. Basically, if we have a sentence x and an entity e, we train a classifier to predict y_t, e_t for each possible pair of entities. Formal: We train a language model to identify pairs of entities. Basically, if we have a sentence x and an entity e, we train a classifier to predict y_t, e_t for each possible pair of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities. Formal: We train a language model to identify pairs of entities.",
        "directory": "D19",
        "filename": "D19-5409.json"
    },
    {
        "casual_text": "It's pretty rare to see direct comparisons of different taggers using the exact same test data. Most papers these days just claim that one tagger is better than another based on indirect comparisons. But we reckon there are a bunch of factors that aren't really taken into account or controlled for, which makes those conclusions kinda shaky in a lot of cases.",
        "formal_text": "Unfortunately, there have been very few direct comparisons of alternative taggers I on identical test data. However~ in most current papers it is argued that the performance of some taggers is better than others as a result of some kind of indirect comparisons between them. We think that there a. re a number of not enough controlled/considered factors that make these conchlsions dubious in most cases.",
        "GPT2_formal_text": "ed_x_uncased: A post-trained BERT model that gets the attention mask set to 0. Formal: For NER, we use word2vec to get a pre-trained word embedding (like GloVe) for each input word. We also add dropout with a probability of 0.1 to the input sentence embedding (check out Equation 7). Formal: We set the learning rate to 0.00001. Formal: The BERT model we trained has an embedding size of 512. We also add a linear layer to the embeddings of the target words. Formal: We added dropout with a probability of 0.1 to the embeddings of the target words. The BERT model we trained has an embedding size of 1024. Formal: We trained our model on a mix of data (we call it \"src\") and the target dataset (we call it \"test\"), with a batch size of 32. The training took about 2 days on an Nvidia GTX 1080 GPU. The code for this is available online. Formal: Following the approach in (Sun et al., 2019), we trained a pre-trained BERT model on the data from the test set. Formal: We used a setup with a batch size of 32, an embedding size of 1024, a dropout rate of 0.1, and an initial learning rate of 0.00001. Formal: The validation set is the subset of data where the model didn't perform well. Formal: We found that the validation set is usually the most useful for testing. Formal: We used the development set as our validation set. Formal: For the test set, we used the test set as our development set. Formal: We set the learning rate to 0.00001. Formal: The BERT model we trained has an embedding size of 512. Formal: We added dropout with a probability of 0.1 to the embeddings of the target words. Formal: Formal: The BERT model we trained has an embedding size of 1024. Formal: We trained our model on a mix of data (we call it \"src\") and the target dataset (we call it \"test\"), with a batch size of 32. The training took about 2 days on an Nvidia GTX 1080 GPU. Formal: We added dropout with a probability of 0.1 to the embeddings of the target words. Form",
        "directory": "C98",
        "filename": "C98-2159.json"
    },
    {
        "casual_text": "Sure! Here's the informal version:\n\n1. Check out the Transformers library here: https://pypi.org/project/transformers/.  \n2. The ROCStories dataset is available at this link: https://cs.rochester.edu/nlp/rocstories/.  \n3. You can explore the SQuAD dataset here: https://rajpurkar.github.io/SQuAD-explorer/.  \n4. The WMT14 translation task info is here: https://www.statmt.org/wmt14/translation-task.html.",
        "formal_text": "https: //pypi. org/project/ transformers/. 5 https: //cs. rochester. edu/nlp/ rocstories/ 6 https: //rajpurkar. github. io/ SQuAD-explorer/ 7 https: //www. statmt. org/wmt14/ translation-task. html",
        "GPT2_formal_text": "= {k1, ..., kn}\n\nThen, you can add more context by adding more nodes. This means you can add more event tuples and make the graph more complete. For example, you can add {k1, ..., kN} by moving an entity into a different node and adding its ID to the existing node's list. Formal: This also lets you use pre-trained language models (LMs) to handle the generation process. We use XLM-RoBERTa (from Radford et al., 2019) as our LMs. Formal: We set the size of the graph to 100. Formal: We train the model using Adam (from Kingma and Ba, 2015) with a learning rate of 0.001. Formal: The model is trained on a single NVIDIA GeForce GTX 1080 GPU. Formal: The model is trained on a single NVIDIA GeForce GTX 1080 GPU. Formal: We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001. Formal: We set the batch size to 32. Formal: We train the model using the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001. Formal: We set the batch size to 32. Formal: We train the model using the Adam optimizer (Kingma and Ba, 2015). Formal: We set the batch size to 32. Formal: We train the model using the Adam optimizer (Kingma and Ba, 2015). Formal: We set the batch size to 32. Formal: We train the model using the Adam optimizer (Kingma and Ba, 2015). Formal: We set the batch size to 32. Formal: We train the model using the Adam optimizer (Kingma and Ba, 2015). Formal: We set the batch size to 32. Formal: We train the model using the Adam optimizer (Kingma and Ba, 2015). Formal: We set the batch size to 32. Formal: We train the model using the Adam optimizer (Kingma and Ba, 2015). Formal: We set the batch size to 32. Formal: We train the model using the Adam optimizer (Kingma and Ba, 2015). Formal: We set the batch size to 32. Formal: We train the model using the Adam optimizer (Kingma and Ba, 2015). Formal: We set the batch",
        "directory": "acl",
        "filename": "2021.acl-long.467.json"
    },
    {
        "casual_text": "Basically, for each step i in the process, we create a feature vector F i with four parts, using something called the H-feature detectorp i. This feature vector has: (1) how similar H i and w i are before any projection; (2) the result of applying featurep i to H i; (3) the result of applying the H-feature detectorp i to w i; and (4) the difference between parts 2 and 3.",
        "formal_text": "Concretely, in each iteration i of the procedure, we generate a four-valued feature vector F i, based on the H-feature detectorp i. Each feature vector contains (1) the similarity of H i and w i (before projection); (2) the featurep i applied to H i; (3) the Hfeature detectorp i applied to w i; and (4) the difference of 2 and 3.",
        "GPT2_formal_text": "ed = [mask] token_1 = [mask] token_2 = [mask] token_3 = [mask] token_4 = [mask] token_5 = [mask] token_6 = [mask] token_7 = [mask] token_8 = [mask] token_9 = [mask] token_10 = [mask] token_11 = [mask] token_12 = [mask] token_13 = [mask] token_14 = [mask] token_15 = [mask] token_16 = [mask] token_17 = [mask] token_18 = [mask] token_19 = [mask] token_20 = [mask] token_21 = [mask] token_22 = [mask] token_23 = [mask] token_24 = [mask] token_25 = [mask] token_26 = [mask] token_27 = [mask] token_28 = [mask] token_29 = [mask] token_30 = [mask] token_31 = [mask] token_32 = [mask] token_33 = [mask] token_34 = [mask] token_35 = [mask] token_36 = [mask] token_37 = [mask] token_38 = [mask] token_39 = [mask] token_40 = [mask] token_41 = [mask] token_42 = [mask] token_43 = [mask] token_44 = [mask] token_45 = [mask] token_46 = [mask] token_47 = [mask] token_48 = [mask] token_49 = [mask] token_50 = [mask] token_51 = [mask] token_52 = [mask] token_53 = [mask] token_54 = [mask] token_55 = [mask] token_56 = [mask] token_57 = [mask] token_58 = [mask] token_59 = [mask] token_60 = [mask] token_61 = [mask] token_62 = [mask] token_63 = [mask] token_64 = [mask] token_65 = [mask] token_66 = [mask] token_67 = [mask] token_68 = [mask] token_69 = [mask] token_70 = [mask] token_71 = [mask] token_72 = [mask] token_73",
        "directory": "D16",
        "filename": "D16-1234.json"
    },
    {
        "casual_text": "Turns out, CNNs can automatically pull out useful stuff from sentences, like the grammar and meaning. Sometimes, we use these features in other models to tackle different tasks. In our experiment, we show how well our model's features work. Since CNN models have already nailed finding relationships within single sentences, we tried something new: figuring out the relationship between entities that weren't in the same sentence.",
        "formal_text": "It has been proved that CNN could automatically extract useful features, encoding syntactic and semantic meaning of sentences. These features are sometimes fed to subsequent models to solve other tasks. In this experiment, we demonstrate the effectiveness of the extracted features from our model. Since CNN-based models have already succeeded in extracting relations from single sentences, we set our experiment in a new scenario: predicting the relation between entities which have not appeared in the same sentence.",
        "GPT2_formal_text": "\" and \"likes_count\" in the question. It's worth noting that the model uses a logistic regression classifier to figure out the scores, and it uses a linear kernel for the hyperparameter λ. Formal: The way we calculate the scores for these features is kind of similar to what you'd see in something called Bayesian network estimation. Formal: We've got a model that works on these two things, and we're building on it to handle the things we talked about earlier. Formal: We've come up with two ways to handle this. Formal: To make the model's scores better, we're focusing on the usual features, which are kinda like the usual classifiers. Formal: We're looking at how the model makes predictions, like whether it predicts one of the two answers or not. Formal: Also, we're looking at the log probabilities of the predicted answers and the correct answers, which are the log probabilities of the correct answers and the correct answers. Formal: This gives us a more accurate model, but it also makes things more complicated and more unpredictable. Formal: We've written all this down in a way that anyone can check out, and we'll keep updating it as we add new features and test different setups. Formal: We'll also explain how we can find the best balance between how accurate our model is and how much it messes up the data it spits out. Formal: We also want to point out that this whole process is super simple to set up, and we'll show how it works in a few steps. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D17",
        "filename": "D17-1186.json"
    },
    {
        "casual_text": "The idea of NLRS (Natural Language Report System) for really specific areas was first shown in Kukich's 1983 work about \"knowledge-based generation\" of stock market reports. Kukich's ANA system creates professional-sounding summaries of the stock market using daily data from the Dow Jones' half-hourly quotes for the market average and major indices. Both ANA and FRANA, its French counterpart (from Contant in 1986), used a phrasal lexicon approach (like the one Beeker described in 1975). This method keeps the language part limited, but it works well for small, predictable areas. The stuff I'm talking about now takes a more modular approach to NLRS and applies it to a new area.",
        "formal_text": "NLRS for highly reslricted domains was first demonstrated in the work of Kukicb_ (1983) on \"knowledge-based generation\" of stock market reports. Kukich's ANA system produces professionalsounding stock market stmlmaries using a daily trace of Dow Jones' half-hourly quoudions for the market average and major indices. Both ANA and the analogous FRANA system for French (Contant 1986) have used a phrasal lexicon approach (Beeker 1975) which limits the generality of the linguistic component, but which seems to suffice for small and stereotyped domains. The work described below represents a more modular approach to NLRS as well as a new application domain.",
        "GPT2_formal_text": "_tokens is set to max. The user can choose to either use (1) the original token or (2) a mix of the original token and its attention mask token. If the user picks (2), they can also choose to have a discussion or reply to the post. Formal: We suggest using a distance-based attention mechanism, which can be added to our model by using a non-linear function, with a K × M kernel, to process the post-comment pairs. Formal: To make the attention matrix bigger, we use the knowledge-based method to combine the learned attention masks. We feed the attention matrix into a softmax function to get the final representation. Formal: The model parameters can be thought of as the average attention weights, m, for each token. Formal: We also show how the attention weights, m, for each token can be adjusted by tweaking the attention mask weights. Formal: This approach works because it balances how much each token contributes with how much each attention mask contributes. Formal: We also introduce a new feature called \"dialogue_length,\" which can be used to add some variety to the responses. Formal: We calculate the conversation length using the number of words in the post and the length of the dialogue. Formal: We use a length-aware attention mechanism, which helps the model focus more on the important parts of the response. Formal: For these experiments, we use both an unsupervised baseline (without any knowledge) and a supervised one (with knowledge). Formal: We test our model on a dataset called MultiWOZ (from Wan et al., 2015), which has 1.6 million posts and 1.6 million responses. Formal: We train the model using the self-training method (Sennrich et al., 2016). Formal: The results we get are pretty good, which shows that our model can effectively use the different aspects of the responses to generate responses. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1132.json"
    },
    {
        "casual_text": "• LightConv and DynamicConv. The code for these is included in fairseq 7 (Ott et al., 2019). We train lightweight convolution models in two different ways: (1) starting from scratch to learn token representations. In this case, we use BPE tokenization with a vocabulary of 30K types, and we rely on the fastBPE implementation 8 (Sennrich et al., 2015); (2) using pre-trained language model representations to initialize the token embeddings. For the language model, we go with GPT-2 small (Radford et al., 2019).",
        "formal_text": "• LightConv and DynamicConv. The implementation is available in fairseq 7 (Ott et al. , 2019). We train lightweight convolution models in two manners: (1) learning token representations from scratch; in this case we apply BPE tokenization with the vocabulary of 30K types, using fastBPE implementation 8 (Sennrich et al. , 2015); (2) initializing token embeddings with pre-trained language model representations; as a language model we choose GPT-2 small (Radford et al. , 2019).",
        "GPT2_formal_text": "= {s1, s2, ..., sn}. The input for the linear layer is a list of triples where the label is e1, ..., en, and the category is c. Formal: Since the training part can handle more complex phrases and new words in the context, we can say that the model learns to \"go with\" the context and only uses the subword info when making decisions about the next word. Formal: We'll show that this setup, where the model only uses context info when making predictions, works better than the old approach, which just uses the actual subword info. Formal: To show that this new model is better at handling context, we'll train it using data from three different domains and check its performance on the test data. Formal: Formal: We also want to point out that the subword info can be used for different things, like negation and combining multiple words. Formal: By using this kind of contextual info, we can really get the best of both worlds: the extra context we get from the subword info can be used for other things. Formal: This work is under a Creative Commons Attribution 4.0 International License. Formal: The licenses for this paper are available online. Formal: The main ideas behind this work are: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-5409.json"
    },
    {
        "casual_text": "We got our data from three main places—two for names of people and one for brand names. For the people's names, we used databases from the U.S. and the U.K. available at https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets. The U.S. names come from birth records kept by the U.S. Social Security Administration from 1880 to 2013, and the U.K. names come from the UK Office of National Statistics, the Northern Ireland Statistics and Research Administration, and the Scotland General Register Office. After getting rid of names that could be both male and female, we had 97,102 unique English names to work with (60,984 female and 36,118 male).\n\nFor brand names, we used Kantar Media's Stradegy database, which tracks U.S. advertising spending by brands across almost every product category. We removed multi-word names that were derivative brands (like Ford Escort) and a small number (66) of names that are common English words (like Coach) based on the ~5,000 most frequent words in the British National Corpus. In the end, we had 1,021 brand names across 17 product categories. Each name was turned into a 16-dimensional vector based on the features we described, and we made all the data available in the supplementary materials.",
        "formal_text": "We draw data from three primary sources, two for people's names and one for brand names. For people's names, we relied on databases of U. S. and U. K. names available at https: //github. com/OpenGenderTracking/ globalnamedata/tree/master/assets. The U. S. data come from the yearly birth records maintained by the U. S Social Security Administration from 1880 to 2013; the U. K. data come from the UK Office of National Statistics, the Northern Ireland Statistics and Research Administration, and the Scotland General Register Office. After removing names that are labeled as both male and female, we ended up with 97102 unique English names (60984 female, 36118 male) to work with. For brand names, we relied on Kantar Media's Stradegy database. This database documents U. S. advertising spending by brands in virtually every product category. 2 In this case, after removing multi-word names that are derivative brands (e. g. , Ford Escort) and a small number (66) of names that are common English words (e. g. , Coach) based on the ∼5000 most frequent words in the British National Corpus, 3 we ended up with 1021 brand names in 17 product categories. We represented each name as a 16-dimensional vector based on the features described, and we made all data available in the supplementary materials.",
        "GPT2_formal_text": "= { [x, y], [o_t, y] ], and so on, up to [o_s, y] }. This setup helps us figure out the attention weights for the summary tokens. Formal: For each (x, y) in the text, we create a query vector using a document, and it's made up of five parts (x1, x2, ..., xn). The first part (x1) is the title, and the rest (x2, ..., xn) are the key/value pairs. We add the titles of the key/value pairs to this query vector. To get the final query vector, we multiply these key/value pairs together. Formal: To get the final query vector, we sum up the keys and values of the query vector. Formal: To sum the keys and values of the query vector, we use this formula: Formal: The final query vector we get is a matrix with N rows and N columns. Formal: Formal: We take the values from the first and last rows and add them to the query vector to get the final query vector. Formal: Formal: Formal: Finally, we use this formula to calculate the weight vector for the summary tokens. Formal: Here, σ(0) is the softmax function, W(i) is the number of summary tokens, h_s is the hidden state of the summarization model, and e_s is the weight vector for the summarization model. Formal: Formal: Formal: Finally, we sum up the weights for the summary tokens, and h_s is the hidden state of the summarization model. Formal: Formal: Finally, we use this formula to calculate the weight vector for the summary tokens. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1142.json"
    },
    {
        "casual_text": "where n is the number of words in the sentence. Basically, for each episode, the way we update the parameters is based on the REINFORCE method, and we do this for every single token in the sentence.",
        "formal_text": "where n is the length of the sentence. 4 The direction of the parameters update for a given episode is then the one obtained from REINFORCE summed over all tokens:",
        "GPT2_formal_text": "= (baseline_s, baseline_t, ..., all_s) for each pair (s, t) in the data. After that, we train a model using these learned representation vectors, and the final result is a predicted probability distribution for each sentence. Formal: We use BERT (Devlin et al., 2019) to create a model that deals with both normalization and distributional generalization. Formal: The final representation for the target word is what we use to calculate the target word's likelihood. Formal: We use BERT to do word embedding and word distributional generalization. Formal: To make the whole process easier, we've created a step-by-step process for normalization and a tool that can help with labeling. Formal: We use BERT to create a model that deals with both normalization and distributional generalization. Formal: We use BERT to do word embedding and word distributional generalization. Formal: To make the whole process easier, we've created a step-by-step process for normalization and a tool that can help with labeling. Formal: We use BERT to create a model that deals with both normalization and distributional generalization. Formal: We use BERT to do word embedding and word distributional generalization. Formal: To make the whole process easier, we've created a step-by-step process for normalization and a tool that can help with labeling. Formal: We use BERT to create a model that deals with both normalization and distributional generalization. Formal: We use BERT to do word embedding and word distributional generalization. Formal: To make the whole process easier, we've created a step-by-step process for normalization and a tool that can help with labeling. Formal: We use BERT to create a model that deals with both normalization and distributional generalization. Formal: We use BERT to do word embedding and word distributional generalization. Formal: To make the whole process easier, we've created a step-by-step process for normalization and a tool that can help with labeling. Formal: We use BERT to create a model that deals with both normalization and distributional generalization. Formal: We use BERT to do word embedding and word distributional generalization. Formal: To make the whole process easier, we've created a step",
        "directory": "eacl",
        "filename": "2021.eacl-main.66.json"
    },
    {
        "casual_text": "- We added early exit tricks to skip unnecessary calculations, which makes the network faster.  \n- We ran a bunch of tests on GLUE to see how it works.",
        "formal_text": "• We incorporate early exit mechanisms as an enhancement to skip redundant computation, leading to further network acceleration. • We conduct extensive experiments on GLUE.",
        "GPT2_formal_text": "_t. The second part of the equation says that if we have two words that look alike, they both get labeled as the same, and the same goes for the two words next to them. This is kind of like how the model handles patterns in text. It kind of backs up our idea that words that seem similar to each other often share the same meaning. The way we figure out these relationships is by using something called \"syntactic distance,\" which is like the average distance between two words. Formal: One key thing we need to think about when figuring out how to classify things is the idea of \"syntactic distance.\" Basically, if a word is close to something else (like a word that's next to a word that's right next to it), it might not mean the same thing. To avoid this, we need to avoid things like: Formal: Word overlap is super important for how well the model does. If two words are close to each other, their meaning probably isn't that different. For instance, \"tiger\" and \"bulldog\" might both be more likely to be considered \"tiger\" than \"bulldog.\" So, we check how similar or different the words are to each other. Formal: We also see a need to look at how much the words overlap in meaning. If one word is only a tiny bit similar to another, we might think that it doesn't really mean much. Formal: Also, we need to think about how the model makes decisions about words. Formal: Word embeddings can help us understand how the model picks up on things like word endings and how words are linked to specific things. Formal: Lastly, we should point out that if two words are super similar, they might not mean the same thing. Formal: In this project, we use a neural network to handle this \"syntactic distance.\" Formal: The neural network part was inspired by the work of Zhou and Lapata in 2018. Formal: To make things more efficient, we use a technique called maximum likelihood estimation to estimate the average distance. Formal: We also suggest using a tool called Word2Vec to break down words into smaller parts. Formal: To test how well the model does, we use something called perplexity to check how well it predicts what each word will mean. Formal: Lastly, we use something called perplexity on the test set to check how well it predicts the meanings of words. Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.503.json"
    },
    {
        "casual_text": "Considering these challenges, this paper suggests using Graph Convolutional Networks (GCN) on a special kind of tree called a contextual sub-tree for event extraction. This contextual sub-tree is a modified version of a dependency parse tree that’s been trimmed in a unique way. It gives us not only the usual dependency path info but also extra details that aren’t on the main path. This extra info adds more context to the connection between two nodes, which is why it’s called a contextual sub-tree.",
        "formal_text": "With these challenges in mind, this paper proposes a solution applying Graph Convolutional Networks (GCN) over contextual sub-tree for the task of event extraction. The contextual sub-tree is a dependency parse tree uniquely pruned that provides not just dependency path information but also offpath information. The off-path information adds more context to the existing dependency path between two nodes; hence it is termed contextual sub-tree.",
        "GPT2_formal_text": "; g_s = G_s; h_s = h_s; h_t = h_t; v_s = v_s; v_t = v_t; r_s = r_s; r_t = r_t; } } else { h_s = h_s; h_t = h_t; v_s = v_s; v_t = v_t; r_s = r_s; r_t = r_t; }\n\nThe full input text comes in one go, and the format of the output is pretty much the same as the input text itself. Formal: Here, the = sign tells us that the last letter of the output is always the same. Formal: So, the rule says that the word in the source text has to match the word in the target text. Formal: So, the thing we get from the target text is the same as the original word. Formal: So, the rule says that the first letter of the word in the source text has to match the first letter in the target word. Formal: So, the rule says that the first letter of the word in the source text has to match the first letter of the word in the target word. Formal: So, the rule says that the first letter of the word in the source text has to match the first letter of the word in the target word. Formal: So, the rule says that the first letter of the word in the source text has to match the first letter of the word in the target word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "econlp",
        "filename": "2021.econlp-1.10.json"
    },
    {
        "casual_text": "The method we're suggesting isn't just good for dealing with all kinds of structures and bigger problems—it also has a solid covering number bound, which means it generalizes well.",
        "formal_text": "Not only does the proposed method has the advantages that it can handle arbitrary structures and larger-scale problems, its covering number bound also shows nice generalization properties.",
        "GPT2_formal_text": "ed(T, d) = {1.0, 0.0, 0.1, 0.2, 0.3} Formal: Using attention, we can find the best response by focusing on the masked entity's feature vector, h_m. Formal: Using attention, we can figure out the best response by picking the best response from all the ones that are related to it. Formal: In this project, we introduce a new neural network called MODE-LSTM-CRF, which works really well for figuring out the best response. We also suggest a cool new way to select the best response. Formal: We tested our method on three datasets and found that our MODE-LSTM-CRF model did way better than the other three models. Formal: We also showed that by tweaking the parameters of our model, we can get even better results. We did this by adding a bigram attention mechanism and a linear layer to capture the important context around the entity mention. Formal: These results show that by adding these extra parameters, we can make our model even better at generating responses. Formal: We made our model even more flexible by incorporating a hierarchical attention mechanism that uses gated units to focus on different parts of the text. Formal: We also added a linear layer to capture the important context around the entity mention. Formal: We also built a neural network to figure out the best response. Formal: For future work, we plan to improve the model by using more data from the target domain and exploring ways to encode and use that data better. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2009.eamt-smart.9.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We're talking about extending our parser to output \"S's\" (whatever that means). The tricky part is dealing with verbs like \"raising\" and \"equi\" that take infinitives or \"that\" clauses. But don't worry, it's not too hard to figure out how to handle these. \n\nThe reason it's not too tricky is because we're using LFG (Lexical Functional Grammar) as our base, not Kamp's categorial syntax. With LFG, the thematic roles in a sentence are already sorted out in the f-structure (functional structure). So, it's pretty straightforward to write rules that give us the right meanings for sentences with or without those tricky verbs.\n\nLet's look at an example:\n- \"John persuaded Mary to come\"\n- \"John persuaded Mary that she should come\"\n\nBoth of these sentences mean the same thing, just structured differently.\n\nNow, let's talk about how these rules work using a familiar example: \"every man loves a woman.\" With Kamp's categorial syntax, the rules work from the top of the tree down. The order in which we handle the parts of the sentence is determined by the syntactic rules. In other words, the scope (how things relate to each other) is directly tied to how the sentence is built.",
        "formal_text": "~ \"GRAMMATICAL PARSIAK~' AND \"lOGICAL P~RSIN~' In this section we will outline the principles anderlying the extension of our parser to produce ~S's as output. Because none of the fragments of ~T contains Raising-and Equi-verbs taking infinitival or that-complements we are confronted with the task of writing construction rules for such verbs. It will turn out, however, that it is not difficult to see how to extend ~T to eomprise such constructions. \"ibis is due to the fact that using LFG as syntactic base for IRT -and not the categorial syntax of Kamp -the ~raveling of the thematic relations in a sentence is already accomplished in f-structure. Therefore it is streightfo~rd to formulate construction rules which give the correct readings for (i0) and (ii) of the previous section, establish the propositional equivalence of pairs with or without Raising, Equi (see (I), (2)), etc. (I) John persuaded Mary to come (2) John persuaded ~%~ry that she should come let us first describe the BRS construction rules by the f~niliar example (3) every man loves a woman Using Ksmp's categorial syntax, the construction rules operate top down the tree. The specification of the order in which the parts of the tree are to he treated is assumed to be given by the syntactic rules. I. e. the specification of scope order is directly determined by the syntactic construction of the sentence.",
        "GPT2_formal_text": "= [source_id, target_id]; for i from 1 to T, {tag, slot_type} = i; do {p_i, j, k} = source_id, target_id, position(p_i, i), slot_type; end; end; return 1; end; function model_predict(tag, slot_type) return label_type; end; function model_predict(p_i, j) return label_predict(p_i, j); end;\n\nThis function basically predicts the label for the entity, tag, and slot type. Formal: Formal: The model is trained using the sum of label_predict (which is p_i, j) for each entity, tag, and slot type. The model is then tested on the labeled data to see how well it can handle the new domain. Formal: Formal: During testing, we pick a test domain based on the number of labeled examples. Formal: Formal: For each test domain, we pick a source (source_id) and a target (target_id) that are nearby. Formal: We estimate the likelihood of the target and source domains. Formal: After training, we test the model on the new domain using the labels it predicts. Formal: This means the model gets the label for the new domain, but it doesn't know exactly where it is. Formal: Formal: We train the model using the sum of label_predict (which is p_i, j) for each entity, tag, and slot type. The model is then tested on the new domain using the labels it predicts. Formal: The model is tested on the labeled data to get the probability of the new domain. Formal: For each test domain, we pick a source (source_id) and a target (target_id) that are nearby. Formal: We estimate the likelihood of the target and source domains. Formal: Formal: For each test domain, we pick a source (source_id) and a target (target_id) that are nearby. Formal: We estimate the likelihood of the target and source domains. Formal: Formal: Formal: The model is trained using the sum of label_predict (which is p_i, j) for each entity, tag, and slot type. The model is then tested on the new domain using the",
        "directory": "E83",
        "filename": "E83-1009.json"
    },
    {
        "casual_text": "We've come up with three ways to figure out the best value of k by using the H2 values from 1... deltaK. Basically, we look at how H2 changes as k increases and pick the point where those changes stop being meaningful.",
        "formal_text": "We have developed three methods that take as input the H2 values generated from 1. . . deltaK and automatically determine the \"right\" value of k, based on finding when the changes in H2 as k increases are no longer significant.",
        "GPT2_formal_text": "is calculated as the sum of the attention scores for all the tokens in the input sequence. Formal: Also, check out Figure 1: Here, w_f, e_f, and h_f are the parameters that help manage the attention distribution for each word in the input sentence. Formal: For the first row in Figure 1, the \"f\" and \"e\" represent the number of times words appear together in the input. And \"h\" is the length of the input sentence. Formal: In this setup, the attention mechanism uses something called a max-pooling mechanism to figure out how much attention to give to each token in the input. Formal: The attention weights are calculated like this: Formal: The goal for each token is to maximize the importance score of that token. Formal: We check if the attention scores for two tokens are the same using the equation: Formal: We also use another attention mechanism to calculate the attention weight for a specific token, which is y_t. Formal: Formal: We train the attention system using the validation set to make sure the model can learn useful representations from the source sentences. Formal: Also, we calculate the embedding of the input sentence in the target language by adding up the embeddings of all the tokens in the input. Formal: Lastly, we add the embeddings of the target sentence to the output of the attention mechanism to create the output embedding of the target sentence. Formal: Our approach is similar to the first version of BERT (Devlin et al., 2019), but we use a Bi-LSTM with attention mechanism to predict the output embedding. Formal: Formal: The attention weights for a token y_t are also calculated like this: Formal: The final attention weight for the token y_t is calculated by the equation: Formal: We also use the target sentence embedding embedding embedding, y_t, to predict the output embedding for the target sentence. Formal: Formal: Formal: We train the model by testing it on the validation set to make sure the model can learn useful representations from the source sentences. Formal: Formal: Formal: We use the validation set to fine-tune the model to get the best results. Formal: Formal: We also use the target sentence embedding embedding embedding, y_t, to predict the output embedding for the target sentence. Form",
        "directory": "E06",
        "filename": "E06-2007.json"
    },
    {
        "casual_text": "To make sure we have more examples for the less common relations shown in Figure 1, we create some fake or \"silver\" data. This helps us transfer models from one language to another, usually from a language with lots of resources to one with fewer. We’ll show how this works using English and Hindi as examples.",
        "formal_text": "To increase coverage of examples for the relations located at the tail of the distribution as reported in Figure 1, we also generate (semi) synthetic or 'sil ver' data that is used for model transfer from one language to another, commonly from a resource rich to lowresource language. We illustrate the process using En and Hi.",
        "GPT2_formal_text": "= {}; h_i = (1, ..., n) = {h_1, ..., h_n}, where h_i is the i-th word in the sentence. Formal: Sure! Here's a more casual version:\n\n1. There's a thing called \"word_index_list\" that has a bunch of word_ids. Formal: We figure out the index of a word based on its overall structure and some specific bits. Formal: We use a mix of pattern and dictionary lookups, and we apply a softmax to the probabilities to get the index. Formal: To make the index, we grab the top N words that are most likely to be related, based on patterns in the dictionary. This helps us figure out the right word. Formal: We build this word_index_list by picking the most common word from the set {w_1, ..., w_n}. Formal: We also check out how words are connected in different sentences. Formal: Here's an example of how we use those connections in a sentence: Formal: We pick the word with the highest probability (the \"head\") in the sentence. Formal: Using this idea, we create a bunch of word_index_list items, like the ones in Figure 1. Formal: We do this for all the words in the sentence. Formal: Then, we use a softmax to figure out the probability of the next word in the sentence. Formal: Finally, we use this probability to build the word_index_list. Formal: We do this for all the words in the sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "conll",
        "filename": "2021.conll-1.45.json"
    },
    {
        "casual_text": "We ran QuestEval on the QAGS-XSUM data for all the different combinations of hyperparameters and models. This way, SummEval stayed completely out of the picture during testing.",
        "formal_text": "For all the variations of hyper-parameters and models, we tested QuestEval on QAGS-XSUM data, in order to keep SummEval unseen at test time.",
        "GPT2_formal_text": ", which we'll call α_m. Formal: The word embeddings are just a series of vectors. Formal: Using the word embeddings, we can calculate the attention vector for each token, y_i, and its attention score, A_i, which we'll call A_i. Formal: Formal: We can also figure out the attention weights, α_a_i, for the next token, a_j. Formal: Let's say f_m(a_j) is the attention weight for the token that comes after the one that came before. Formal: After we've done all the rest of the steps, we can use the learned representations from the final layer to create a representation for the next token, a_k. Formal: Using the input tokens, we can figure out the attention weights, α_a_i, for the next token, a_k. Formal: By looking at these weights, we can calculate a probability distribution for the next token, a_k. Formal: Let's say f_m(a_k) is the attention weight for the token that comes after the one that came before. Formal: Next, we can use the learned representations from the last layer to create a representation for the next token, a_k. Formal: After we've done all the rest of the steps, we can use the learned representations from the last layer to create a representation for the next token, a_k. Formal: Finally, the probability distribution for the next token, a_k, can be written like this: Formal: Using the input tokens, we can calculate the attention weights, α_a_i, for the next token, a_k. Formal: After we've done all the rest of the steps, we can use the learned representations from the last layer to create a representation for the next token, a_k. Formal: Formal: Lastly, the probability distribution for the next token, a_k, can be written like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.529.json"
    },
    {
        "casual_text": "It makes sense that the feelings of users with more influence in a community should carry more weight when figuring out the overall mood of the group. So, we try to gauge the community's emotion by considering each user's \"emotional authority.\" This is based on the idea that different people in the community have different levels of emotional influence.",
        "formal_text": "Intuitively, the emotion of user with higher authority in community should be more important in determining community emotion. Thus we estimate community emotion by taking into account user's emotional authority, which is based on the assumption that different users in a community have different emotional authorities.",
        "GPT2_formal_text": "= {h1, ..., hn} | (hj - 1)  \n\nThe rows in a set are basically sequences that have the same label or labels, with the same Ã and |A| being the values. These rows are considered the actual labels in the set. Formal: A word or sentence in the set gets a certain weight, which we call α_w. For a given word, α_w helps us figure out the weight for the next word in the sequence. Formal: To make sure the model follows the rules of probability, we also use a Dirichlet process called random_azimuth(α_w). This helps us get the probability distribution p_r_a(α_w | a_z). The number of times a_z appears in the sequence is p_r_a(a_z). Formal: To make sure the model can learn the right information, we randomly pick a word, a_w, from the set. Then, the model learns how to act based on the chosen word. Formal: The key thing is that the probability distribution p_r_a(a_w | a_z) should be equal to the probability p_r_a(a_w | a_z) divided by the sum of all the probability distributions p_r_a(a_w | a_z) for all words in the set. Formal: A model with this setup can then give us a probability distribution, p_r_a(a_w | a_z), for the next word in the sequence. Formal: Formal: We use a Dirichlet process to train the model and then check how well the model predicts the next word. Formal: The probability distribution p_r_a(a_w | a_z) can then be calculated using the probability p_r_a(a_w | a_z) + a_z. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1123.json"
    },
    {
        "casual_text": "To meet the needs of our customers, we tweaked the SAIC MT system to work with their data and added some features that let us integrate translation memory. Over time, this has really boosted the quality of the MT output, which means less work for humans to do in post-editing. Plus, we fixed some issues with formatting and how the MT output looked, which helped cut down on post-editing costs even more for the TSP. Here's how this paper breaks down:\n\nIn Section 2, we'll explain the SAIC hybrid MT system and its key features. Section 3 will show how we integrated translation memory (TM) into the system using special flags. These flags make sure that full TM matches are always picked, and partial matches are preferred during translation.\n\nSection 4 talks about a method we used for offline domain adaptation. This let us create a customized baseline system for the customer. We also have an online adaptation method that allows us to make updates to the system as new TM entries or other parallel data come in.\n\nIn Section 5, we'll show how a few simple but effective techniques improved the readability of the MT output for humans. These included tweaking punctuation spacing, capitalization, and making adjustments for differences between European and Canadian French.\n\nFinally, Section 6 presents the experimental results. We'll show how the adaptation affected automatic MT metrics on different test sets, both in-domain and out-of-domain.",
        "formal_text": "To meet these customer requirements, we adapted the SAIC MT system to the customer data and introduced features into the system that allowed for integration of translation memory. Overtime, this has substantially improved the quality of the MT output and thus reduced the human post-editing effort. In addition, we addressed the formatting and MT output representation issues, which lead to further reduction of the post-editing costs of the TSP. This paper is organized as follows. In Section 2, we describe the SAIC hybrid MT system and its main features. Section 3 presents how the translation memory (TM) is integrated into the statistical search by using special flags which ensure that full TM matches are always selected, and partial TM matches are preferred during translation. In Section 4, we describe a method for off-line domain adaptation that allowed us to create a customertailored baseline system, as well as an on-line adaptation method which enables us to perform incremental updates of the system with new TM entries and other parallel data. In Section 5 we show how a number of simple, but useful techniques lead to improved readability of the MT output by humans. They involved punctuation spacing, capitalization, and adjustment to differences between European and Canadian French. Section 6 presents the experimental results. We show the impact of the adaptation on automatic MT metrics on different in-domain and out-of-domain test sets.",
        "GPT2_formal_text": "_ttl = arg max_pooling_predict(h, h_tokens)  \na. 8. Transfer learning: We use linear layers to predict the attention weights. For the encoder, we go with Transformer (Vaswani et al., 2017). The max-pooling function is used to generate the attention-weight vectors, which we feed into the decoder. Formal: Basically, we take each token in the input sequence, turn it into a number (called a embedding) and then use a linear layer to get the output. Formal: Each word in the input is turned into a number and fed into the encoder. Formal: We use the decoder to calculate the attention weights for the words in the input. Formal: Finally, we combine all this to predict the attention weights for the tokens. Formal: We keep going like this until we've generated all the tokens or until we hit the terminal token. Formal: The final result is the sequence of attention weights for the input. Formal: Let’s use a sample from our dataset. It shows two input sequences: one that has a question, and one that has a paragraph. Formal: Our model can generate the correct answer by predicting the attention weights for the first token. Formal: This approach is similar to Transfer learning, but instead of generating the tokens, we predict their attention weights. Formal: We use Transformer (Vaswani et al., 2017) to predict the attention weights for the paragraphs. Formal: The model then uses these weights to generate the answer token. Formal: The attention weights for the question are figured out using the first-token prediction from the input. Formal: The model then uses these attention weights to generate the answer token. Formal: We’ve also included some examples of the generated question and paragraph. Formal: For example, take a look at Figure 1. Formal: Figure 1 shows the generated question and paragraph. Formal: We’ve also included some examples of the generated question and paragraph. Formal: Formal: The generated question is in (b), and the answer is in (a). Formal: This example is pretty tricky. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "amta",
        "filename": "2012.amta-commercial.11.json"
    },
    {
        "casual_text": "So, segs(c_j) is just a way of saying all the segmentation labels that are connected to c_j. And e_seg(segs(c_j)) is a 5-part vector where each part can be either on or off, depending on whether it matches one of these options: B, M, E, S, or O.",
        "formal_text": "where segs(c j ) denotes all segmentation labels related to c j, and e seg (segs(c j )) is a 5-dimensional multi-hot vector with each dimension corresponding to an item of {B, M, E, S, O}.",
        "GPT2_formal_text": ") ) (1: |T|; 2: |U|) Figure 3: Here's the SPICE model for predicting ICD codes in the condition category. Formal: We're using the same method to estimate ICD-9 codes (check out section 2.2) as we did for ICD-10 codes (section 2.3). Formal: For ICD-9, we figure out the codes for the whole document by using the embeddings from a pre-trained embedding model called mLE. Formal: For ICD-10, we use the embeddings from the Multi-Layer Perceptron (MLP), which we trained on the whole dataset. Formal: The final model is trained using log-likelihood loss (section 2.4) and an adversarial example generation (section 2.5). Formal: The training objective function, like log loss or adversarial examples, is calculated using the cross-entropy loss (section 2.6). Formal: We set the adversarial examples to be binary (0 or 1) and train the model using the Adam algorithm (section 2.7) with a batch size of 64. Formal: For training, we use the NLTK toolkit (version 3.3.6) (Och, 2003) with the default settings (section 3.1). Formal: In our work, we also tried out different sequence-to-sequence models for generating training examples (section 3.2). Formal: We trained the model for 4 epochs and the training objective was set to α = 0.1. Formal: In this project, we're focusing on the cross-entropy loss (section 2.5) for training. Formal: We picked the cross-entropy loss for training our models because it's the most commonly used standard loss in NLP (like in section 2.1). Formal: We trained the model for 3 epochs and the training objective was α = 0.1. Formal: We also included a batch size of 128, which is the same as what's used in the OpenNMT toolkit (Grave et al., 2016). Formal: We trained the model for 5 epochs and the training objective was α = 0.1. Formal: We didn't add any extra loss terms to the equation, just calculated the cross-entropy loss. Formal: We trained the model for 4 epochs and the",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "People have figured out some cool ways to handle big classification problems, like the dual coordinate descent method for linear support vector machines, which worked well (Hsieh et al., 2008). Similarly, Yu et al. (2011) came up with a two-level version of dual coordinate descent for maximum entropy classifiers.",
        "formal_text": "Advancements in solving large-scale classification problems have been shown to be effective such as dual coordinate descent method for linear support vector machines (Hsieh et al. , 2008). Similarly, Yu et al. (2011) proposed a two-level dual coordinate descent method for maximum entropy classifier.",
        "GPT2_formal_text": "_layers = 2; while (1) { int i; for (i = 1; i < K; i++) { if (k) { for (j = 1; j < K; j++) { if (k) { int j; break; } } } }\n\nFor the multimodal version, we just grab the attention matrix and the hidden states from the encoder. Formal: The encoder and decoder are pre-trained using the same CNN. Formal: You can see how this works in Figure 1. Formal: We can also use the CNN to create a copy of the encoder and decoder to figure out the attention matrix and hidden states. Formal: The copy is made by combining the original encoder and decoder to get the attention matrix. Formal: We also show how to use the encoder to get the hidden states, which are basically the final representations of the input. Formal: So, the final output is the combination of all these outputs, minus the attention matrix. Formal: The main thing we’re focusing on is how to use the decoder to get the hidden states, which are the final representations of the input. Formal: We’ve got three examples of the multimodal encoder and decoder that we’ve included in the Appendix. Formal: For the multimodal decoder, the whole setup is shown in Figure 2. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D14",
        "filename": "D14-1183.json"
    },
    {
        "casual_text": "The sequence model in this paper is pretty similar to the LSTM-based model mentioned in another source. As you can see in Figure 1, there are three context utterances labeled u1, u2, and u3, which are connected as a sequence of words. A special word called \"sos\" is added between each pair of utterances to mark where one ends and the next begins. \n\nOnce we have the sequences for both the context and the response, we turn the words into word embeddings using a shared lookup table. To create the context and response embeddings, we use a Gated Recurrent Unit (GRU) neural network, which was introduced by Chung et al. in 2014. The GRU works on these two sequences of word embeddings, following the steps outlined in Equations 2 to 5. \n\nHere, ht-1 is the hidden state of the GRU when it processes the word embedding et-1 of the word wt-1. The initial state h0 is just a zero vector. The variables zt and rt are the update and reset gates, respectively. The new hidden state ht for the embedding et is a mix of the previous hidden state ht-1 and the current input embedding et, with the update and reset gates controlling how much of each is used. \n\nThe parameters U, Uz, Ur, W, Wz, and Wr are part of the GRU model and need to be learned. The symbol ⊗ represents element-wise multiplication.",
        "formal_text": "The word sequence model in this paper is similar to the LSTM-based model proposed in. As shown in Figure 1, three utterances of context c, written as u 1, u 2 and u 3, are connected as a sequence of words. A special word sos is inserted between every two adjacent utterances, denoting the boundary between utterances. Given the word sequences of context and response, words are mapped into word embeddings through a shared lookup table. A Gated Recurrent Unit neural network (GRU) (Chung et al. , 2014) is employed to construct the context embedding and response embedding. It operates recurrently on the two word embedding sequences as Equation 2 to Equation 5, where h t−1 is the hidden state of GRU when it reads a word embedding e t−1 of word w t−1, h 0 is a zero vector as the initiation state, z t is an update gate and r t is a reset gate. The new hidden state h t for embedding e t is a combination of the previous hidden state h t−1 and the input embedding e t, controlled by the update gate z t and reset gate r t. U, U z, U r, W, W z and W r are model parameters of GRU to be learned. ⊗ denotes element-wise multiplication.",
        "GPT2_formal_text": "= list(table_ids)  \n# If the first cell in the table doesn't have any cells, just return it. Formal: The table is first turned into vectors (x_t), where each x_t is a vector with d dimensions. These vectors are then fed into a linear layer with a softmax activation (Softmax = 1.0) and then a linear layer with ReLU activation (ReLU = 0.9). Formal: In this part, we're checking if the first cell in the table has a specific value (or \"tag\") by checking its vector representation, x_t_i. The table has rows and columns, and each row has a value (or \"tag\"). The first column has the tag from the first row. Formal: To do this, we use a linear layer with ReLU and a linear layer with Softmax. Formal: Once we have the vectors for the first cell (i), we use a linear layer with ReLU and a linear layer with Softmax. Formal: After that, we calculate the probabilities for the second cell, which might have a tag from the second row. The second cell's representation, x_t_i2, is calculated based on the first cell's vector representation, x_t_i. Formal: We calculate the probabilities for the second cell by applying the linear layer with ReLU and a linear layer with Softmax. Formal: Finally, we use a linear layer with ReLU and a linear layer with Softmax to figure out the probabilities for the third cell. Formal: Formal: We do the same thing for the third cell, x_t_i3, but we use a linear layer with ReLU and a linear layer with Softmax. Formal: Formal: Formal: Finally, we calculate the probabilities for the third cell, x_t_i4, but we use a linear layer with ReLU and a linear layer with Softmax. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "Plus, our sentence vector includes almost all the info from previous sentences, which their model can't do.",
        "formal_text": "And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot.",
        "GPT2_formal_text": ", and so on. Basically, we're looking for the biggest value of h that's greater than 0 for all the attention heads in the network. Formal: For the LSTM network, the attention weights are all stored in the hidden layers, so we can just copy them over into the output layer. The term \"attention\" here refers to the connection between the input and output units. Formal: Using the output layer, the hidden layer has the hidden state vector as its representation. Formal: The output layer has two inputs: the correct label, y, and a word embedding, w. Formal: The output layer's input vector, x, is the output vector from the encoder, x_enc. Formal: The attention weights, α_t, for each t from 1 to L are calculated like this: Formal: The Bi-LSTM setup, where there's an encoder and an LSTM, is shown in Figure 1. Formal: A Bi-LSTM model that creates a sequence of word representations. Formal: We use the Bi-LSTM model to generate a sequence of word representations, H_t = {w_t1, ..., w_tL}. Formal: The Bi-LSTM model, shown in Figure 1, is basically a simplified version of the Bi-LSTM model. Formal: We keep the output layer's hidden state vector x_t as x_t_enc. Formal: So, H_t = H_t_enc. Formal: The attention weights α_t, for each t, are calculated by doing the following: Formal: The attention weights α_t are calculated like this: Formal: Formal: Finally, the input layer's hidden state vector, x_t, is the output vector from the decoder, x_dec. Formal: The final representation H_t is calculated as: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D15",
        "filename": "D15-1106.json"
    },
    {
        "casual_text": "We also took a closer look at a few things that impact how well these big language models (PLMs) work, like the pretraining method, the data they're trained on, how their vocabulary is built, and the schedule for optimizing them.",
        "formal_text": "We further analyze several factors that affect the performance of PLMs, including pretraining method, pretraining corpora, vocabulary construction, and optimization schedules.",
        "GPT2_formal_text": "= { query_id: ..., key: ... }_k = { k_i: ..., w_i: ..., b_i: ... }\n\nWe use the suffix array to handle the first part. Basically, we figure out the prefix value, key, and value for each word, along with their positions, using a simple formula: Formal: To make sure the strings from the prefix and suffix match up, we also look at the first word in the pair as a possible match for the key and value. Formal: For the first word, let's call it \"w_1\", and the first word in the pair, \"w_2\", we check if they match the first word of \"w_1\" and the first word of \"w_2\". We do this by using the formula in Table 5 to find the first word in \"w_1\" and the first word in \"w_2\". Formal: If the first word in the pair matches the first word of \"w_1\" and the first word of \"w_2\", then \"w_1\" is a possible match for the key and value. Formal: The same thing happens for the second word, \"w_2\", and the second word in the pair, \"w_3\", too. Formal: The value is \"w_3\" because the second word in \"w_3\" is also a possible match for the key and value. Formal: The same process applies to the third word, \"w_4\", and the third word in the pair, \"w_5\", too. Formal: The value is \"w_5\" because the third word in \"w_5\" is also a possible match for the key and value. Formal: Lastly, we check if the first word in \"w_4\" and the first word in \"w_5\" is a possible match for \"w_4\" and \"w_5\". We do this by using the formula in Table 5 to find the first word in \"w_4\" and the first word in \"w_5\". Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "clinicalnlp",
        "filename": "2022.clinicalnlp-1.2.json"
    },
    {
        "casual_text": "Basically, imagine you have a graph G with nodes (N), edges (E), and labels on those edges (R). You also have a bunch of pairs of nodes (s i, t i) from a set D. You can make a matrix where the rows are these node pairs and the columns are the edge labels. PRA takes this matrix and adds extra columns for sequences of edge labels, which we call \"path types.\" Instead of just showing if there's an edge between two nodes, the cells now show how specific the connection is between those nodes based on the path type.",
        "formal_text": "More formally, given a graph G with nodes N, edges E, and edge labels R, and a set of node pairs (s i, t i ) ∈ D, one can create a connectivity matrix where rows correspond to node pairs and columns correspond to edge lables. PRA augments this matrix with additional columns corresponding to sequences of edge labels, called path types, and changes the cell values from representing the presence of an edge to representing the specificity of the connection that the path type makes between the node pair.",
        "GPT2_formal_text": "_tokens(s, t) = [t] for t in s. Formal: The cost of using the attention mechanism is O(n²²) because of the Masked Language Model (MLM). Formal: The log probability of the target word, w, given the context, c, is the same as the log probability of its neighbors, s, given its context, c. Formal: The log probability of target word w given context, c, is equal to the log probability of its neighbors, s, given context, c. Formal: To get a sense of how much attention the model is paying to the target word and its context, check out Equation 10. Formal: The loss function L(•) is calculated as the negative log probability of the target word given context, c, minus the log probability of the context, c, plus the sum of the log probabilities of the context words in that sentence. Formal: In this setup, L(•) is basically a weighted sum of the log probabilities of the target word and its context. Formal: You can get the log probabilities for the context words in a sentence using this formula: Formal: To get a better idea of how the model is paying attention to the context words and the target word, check out Equation 11. Formal: Using the L1-norm as the loss function, the loss function L(•) is just a negative linear function of the log probabilities of the context words in the sentence. Formal: Figure 1: The attention mechanism takes into account the context words in a sentence. Formal: The cost of using the attention mechanism is O(n²²) because of the Masked Language Model (MLM). Formal: The cost of using the attention mechanism is O(n²²) because of the Masked Language Model (MLM). Formal: The loss function L(•) is calculated as the negative log probability of the target word given context, c, minus the log probability of the context words in that sentence, c. Formal: The loss function L(•) is calculated as the negative log probability of the target word given context, c, plus the log probability of the context words in the sentence, c. Formal: The loss function L(•) is calculated as the negative log probability of the target word given context, c, plus the log probability of the context words in the sentence, c. Formal: Form",
        "directory": "D14",
        "filename": "D14-1044.json"
    },
    {
        "casual_text": "In this project, we’re diving deep into why people make edits on Wikipedia. We came up with a detailed list of reasons why someone might change a page, like fixing grammar, adding more info, checking facts, or making things easier to understand. Unlike other systems that either focus on tiny grammar tweaks or mix up grammar and meaning, our approach looks at the bigger picture—what the editor is actually trying to do. This helps us figure out not just what changed, but why, and what’s going on in the editor’s head while they’re working (which, by the way, some smart folks have studied before).\n\nTo make this happen, we teamed up with 13 experienced Wikipedians to create a system that captures the \"why\" behind edits, which we call \"edit intention.\" We labeled over 7,000 edits with these intentions to get a clear picture. Then, we built a tool to automatically figure out these intentions by looking at the differences between versions of articles.\n\nTo see how useful this all is, we tested our system on two big questions for Wikipedia: how to keep new editors around and how edits affect article quality. We looked at whether the kind of edits newbies make in their first session predicts if they’ll stick around, and how different types of edits impact how good an article is.",
        "formal_text": "In this work, we focus explicitly on revision intention. We introduce a fine-grained taxonomy of the reasons why an author in Wikipedia made an edit. Example edit intentions include copy editing, elaboration, verification, and simplification. Compared to taxonomies that either focus on low-level syntactic operations (Faigley and Witte, 1981) or that mix syntactic and semantic classes ), a clean higher-level semantic categorization enables us to easily identify textual meaning changes, and to connect revisions to \"what happens in the mind of the revising author during the revision\" (Fitzgerald, 1987; Daxenberger, 2016). In order to capture the meaning behind edits, we worked with 13 Wikipedians to build a taxonomy that captured the meaning of an revision, which we term edit intention, and hand-labeled a corpus of 7, 177 revisions with their edit intentions. We then developed an automated method to identify these edit intentions from differences between revisions of Wikipedia articles. To explore the utility of this taxonomy, we applied this model to better understand two important issues for Wikipedia: new editor retention and article quality. Specifically, we examined whether edit intentions in newcomers' first editing sessions predict their retention, and examined how edits with different intentions lead to changes in article quality.",
        "GPT2_formal_text": ", which are basically the training set's query, entity, and argument pairs. Formal: The model is trained using supervised learning. Formal: We're using the BERT-base-uncased model, which was introduced by Devlin et al. in 2018. Formal: We're training the model using cross-entropy loss. Formal: The model is trained using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: The model is trained using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: The model is trained using cross-entropy loss. Formal: The model is trained using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: The model is trained using cross-entropy loss. Formal: The model is trained using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: The model is trained using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss. Formal: We're training the model using cross-entropy loss",
        "directory": "D17",
        "filename": "D17-1213.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\n\"You can find some datasets for text mining here: [https://www.cs.umb.edu/~smimarog/textmining/datasets/](https://www.cs.umb.edu/~smimarog/textmining/datasets/) and also check out this link for more corpora: [http://disi.unitn.it/moschitti/corpora.htm](http://disi.unitn.it/moschitti/corpora.htm).\"",
        "formal_text": "https: //www. cs. umb. edu/˜smimarog/textmining/datasets/ 2 http: //disi. unitn. it/moschitti/corpora. htm",
        "GPT2_formal_text": "ing, an attention-based model that focuses on attention in text. Formal: To make the most of the context of a sentence and get the most accurate translation, we create a contextualized representation of the whole sentence. Formal: We take the average of the hidden representations in the decoder. Formal: We add a global position bias, a cross-entropy loss, and a span-level attention weight. Formal: After that, we apply a linear layer and another attention-based model. Formal: We use the same evaluation metrics as in the original paper, except we only focus on sentence classification. Formal: We also tweak the evaluation metrics by looking at the similarity between the original and contextualized versions of the same sentence. Formal: We evaluate by calculating the standard deviation of the contextualized embeddings. Formal: We use cross-entropy loss to figure out the best values for the parameters. Formal: We also add a linear layer to adjust the attention weights. Formal: We use a span-level attention weight to adjust the cross-entropy loss. Formal: We tweak the evaluation metrics by looking at the similarity between the original and contextualized embeddings. Formal: We also test with different values for the window size. Formal: Formal: We calculate the standard deviation of the embeddings and the global position bias using a Monte Carlo permutation. Formal: Formal: We test with different values for the window size. Formal: Formal: Formal: Finally, we test with different values for the span size. Formal: Formal: Formal: We experiment with different values for the window size. Formal: Formal: Formal: Formal: We test with different values for the window size. Formal: Formal: Formal: Formal: Lastly, we test with different values for the window size. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-1345.json"
    },
    {
        "casual_text": "To get a clearer picture of how the instance relabeling strategy works, we tracked the Exact Match Accuracy of our method with and without this strategy on the MTOP test set after each training session. As you can see in Figure 2, our method with the relabeling strategy just keeps getting better and consistently outperforms the baseline during the relabeling phase. This shows that the relabeling approach actually fixes a lot of label mistakes in the messy training data, and those corrected labels help boost the performance.",
        "formal_text": "To better understand the effect of the instance relabeling strategy, in Figure 2, we record the Exact Match Accuracy of our method with or without the relabeling strategy on the MTOP test set after each training epoch. The performance of our method with relabeling strategy keeps improving and is consistently better than the baseline during the relabeling stage. It demonstrates that the relabeling method indeed corrects many label errors in the noisy training data and the corrected labels contribute to the performance improvement.",
        "GPT2_formal_text": "= {event_ids, id1, id2, ..., idn}, and the types of the arguments. Formal: So, the first thing we do is grab the argument spans from the text and the entities mentioned in them. We break down the argument spans into two smaller parts: the first part is all the words, and the second part is the entities. Then, we build the argument set by putting together the words from the first part and the entities from the second part. Formal: To make sure the arguments for an entity are accurate and connected to the entity, we start by figuring out the main types of the argument spans. Then, we use those types to connect the argument spans with the entities. Formal: To figure out the arguments for an entity, we start by gathering the arguments for the entity. Formal: We use the main types of the arguments to build the argument set. Formal: The rest of the text is split into smaller parts, and we combine the text from the first part and the entities from the second part to make the argument set. Formal: After that, we use the argument set to make a list of entity types. Formal: Finally, we pick the entity types that are most similar to the argument types. Formal: We made this argument set using the source data, the target data, and the argument spans from the text, as mentioned earlier. Formal: Formal: We also use this info to build the argument set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.259.json"
    },
    {
        "casual_text": "So, to deal with these issues, recent methods have focused on two main strategies: making the story generation model more controllable and adding in some commonsense knowledge. One of the big ideas in controllability is the \"plan and write\" method (Yao et al., 2019). They start by using a RAKE algorithm to pick out the most important words from each sentence and then train a storyline planner using that data. The language model is trained to work with both the previous context and these keywords. When it comes time to generate a story, the keywords are created from the title and help guide each sentence as it's written.\n\nCommonsense knowledge is basically shared knowledge about how the world works (Alabdulkarim et al., 2021). They fine-tune a pre-trained GPT-2 model using knowledge triples from commonsense datasets. First, they turn these triples into sentences using some predefined rules (like turning (eiffel tower, AtLocation, paris) into \"eiffel tower is at paris\") and then train the model on these sentences using the usual maximum likelihood estimation method.\n\nXu et al. (2020) took things a step further by combining both of these approaches. They first trained a keyword planner using GPT-2 and then used those keywords to search a knowledgebase. The top-ranked sentences from the search were used to guide the story generation process.",
        "formal_text": "Therefore, recent approaches resort to two main strategies to alleviate above issues, by adding more controllability into the story generation model and incorporating commonsense knowledge. One of the most influential strategies of controllability is \"plan and write\" (Yao et al. , 2019) where they first use a RAKE algorithm to extract the most important word from each sentence and train a storyline planner based on such dataset. The language model is trained conditional on both the previous context and the keywords. During generation, the keywords are generated from the given title and can be used to guide generation of each sentence. Commonsense contains shared knowledge about the world (Alabdulkarim et al. , 2021). fine-tune a pretrained GPT-2 with knowledge triples from commonsense datasets. They first use pre-defined rules to turn triples into sentences (e. g. (eiffel tower, AtLocation, paris) → \"eiffel tower is at paris\") and train on the knowledge sentences with conventional maximum likelihood estimation objective. Xu et al. (2020) combine these two approaches by first training a keyword planner with GPT-2 and use the keywords to search a knowledgebase to retrieve the top ranked sentences to guide the generation process.",
        "GPT2_formal_text": "_layers = { 0, 1, 2, 3, 4 }  \n\nBasically, it's a matrix with d dimensions, and each row represents a different attention layer. Formal: If the data comes from the same source domain as the query, the model can pick up on similar patterns and understand them better. Formal: We use the query to create a kind of representation for the document. The rest of the words in the query don't have any special meaning or special roles. Formal: We start by looking at the words in the document. The weight of a word in a phrase (q) is determined by the collection of words (C w) that belong to it (check out Figure 1 for this). Formal: For each word w in the document, we calculate its representation using a formula: Formal: If the word is in a phrase (q) and we have the collection of words (C w) that belong to it (q), we can figure out the weight (α) for each word w using this formula: Formal: Let's say we have a sequence (s, q) of words from q. Then, the representation of each word in s is... Formal: Formal: The weight of a word in a phrase (q) is determined by the collection of words (C w) that belong to it. Formal: Let's say we have a sequence (s, q) of words from q. Then, the representation of each word in s is... Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "alta",
        "filename": "2021.alta-1.10.json"
    },
    {
        "casual_text": "Let's make sure we compare this to the exact IR engine that was used to create the dataset for the PU model. The PU model really relies on the IR engine doing a good job, so it's important to keep that in mind.",
        "formal_text": "We note that the comparison here should be made to the specific IR engine which resulted in the dataset of the PU model, as the PU model benefits greatly from better performance in the IR engine.",
        "GPT2_formal_text": "and = conv(h(x)). Formal: 2. The document sequence we want to predict comes from the query, and we use BERT to do that. Formal: 3. We use a multi-layer perceptron to create a contextual representation for the document, which we call d_p. Formal: 4. Next, we use a single-layer perceptron to create a contextual representation for the whole query, which we call d_q. Formal: 5. We use the embeddings from the first-order language model (L_L) of the document to give a score to d_p, d_q, and the whole query, which we call d_q, using a softmax function. Formal: 6. Finally, we average all the embeddings to get the contextual representation for the whole query, which we call d_q. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.47.json"
    },
    {
        "casual_text": "I’m super thankful to the University of Malaya for covering the costs of my time in Britain.",
        "formal_text": "I a~l also grateful to the University of Malaya for providing the funds for my stay in Britain.",
        "GPT2_formal_text": "_tokens = [h t, i t ] [h i ] h t = max(t) [h i ] i t = [h t, i t ] \n\nFor the next steps, we’re thinking about expanding the entity recognition model to handle bigger sets of examples by adding a beam search to the attention mechanism. We also want to make the model smarter about how it handles different types of relationships. Formal: We’ve got two main ideas we want to explore further: Formal: To make the model more robust, we’re thinking about expanding the entity recognition model to handle bigger datasets. Specifically, we’re thinking about training the model on a mix of different datasets. We’re thinking about using bigger datasets to improve how well the model handles relationships between entities. Formal: We’re also curious about how the model deals with the difference in types of relationships. Formal: If we can, we’d like to test how well our method works on more complex relationships. Formal: We’re planning to start with a few test sets to see how well the model performs, and then expand this to more varied types of relationships. Formal: Formal: We’ll share our code for anyone to check out. Formal: Our model is designed to handle both cases—explicit relationships and implicit ones. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1098.json"
    },
    {
        "casual_text": "Based on previous research, we're sticking with four main roles for the event structure: ARG0, ARG1, TIME, and LOC. But here's the twist: during the pre-processing stage, we're adding a rule that makes sure TIME entities only go into the TIME role and LOC entities only go into the LOC role.",
        "formal_text": "Following prior work, we restrict the event struc-ture to the following four argument roles: ARG0, ARG1, TIME, and LOC. However, we additionally add a type constraint during pre-processing that requires entities of type TIME and LOC only fill matching roles (TIME and LOC respectively).",
        "GPT2_formal_text": "are used as the reference points for the query and response. Formal: First, let's figure out the relation vectors that are the most similar to each other. Then, we'll use those as the reference points for the query and response. Formal: Since the query and response are related, the vector similarity is calculated based on the average similarity between the query and response. Formal: This method is based on the approach from Elazar and Goldberg (2006), who use the average cosine similarity to figure out the relation vectors. Formal: We can also calculate the vector similarity using cosine similarity to calculate the similarity matrix A. Formal: In this project, we used cosine similarity to calculate the similarity matrix A, which is shown in Table 3. For instance, if we have a query q and a response r, the cosine similarity between the query and response is calculated using the vector similarity: A = A(q, r). This similarity matrix A is calculated using cosine similarity, and the probability of something happening is the sum of the cosine similarity between the query and response vectors. Formal: A(q, r) is the cosine similarity between the query and response vectors, and q and r are in R^n. Formal: Finally, we'll use the matrix A to represent the similarity between the query and response. Formal: When we do the ranking by cosine similarity, the vector similarity A(q, r) for a pair of queries q and r is calculated by the cosine similarity between the query vectors. Formal: The best possible ranking is the one that has the highest cosine similarity score (or, equivalently, the highest probability). Formal: The ranking based on cosine similarity is a sequential process, meaning it follows a specific order. For instance, the best ranking is the one that has the highest cosine similarity score (or, equivalently, the highest probability). Formal: In this project, we used cosine similarity to calculate the similarity matrix A. But we did this separately for the query and response vectors. Formal: To make it easier to compare with earlier work, we used the cosine similarity method again to calculate the similarity matrix A. Formal: In the experiments in this paper, we trained a model to rank different answers. The loss function L(•) is calculated as the inverse of the vector similarity score A. Formal: We tested our approach with two different datasets and found that the word embeddings",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.382.json"
    },
    {
        "casual_text": "Some folks have tried using different metrics for MT tuning, but the results have been kind of hit or miss. One notable example is Padó et al. (2009), who claimed their entailment-based metric led to better human evaluations. The catch? It’s super heavy and slow in real-world use. They estimated it would take 40 days to run on the NIST MT 2002/2008 dataset, so they had to split the process into two phases and use a smaller n-best list. In our case, we’re working with the WMT 2010 dataset, which is similar in size, and most of our runs finish in less than a day. \n\nMeanwhile, Cer et al. (2010) tested tuning a phrase-based SMT system with metrics like BLEU, NIST, METEOR, and TER. Even though METEOR and TER have been shown to align more closely with human judgments, they found that BLEU and NIST are still the go-to options for MT tuning.",
        "formal_text": "Some researchers have investigated the use of better metrics for MT tuning, with mixed results. Most notably, Padó et al. (2009) reported improved human judgment using their entailment-based metric. However, the metric is heavy weight and slow in practice, with an estimated runtime of 40 days on the NIST MT 2002 /2008 the authors had to resort to a two-phase MERT process with a reduced n-best list. As we shall see, our experiments use the similarly sized WMT 2010 dataset, and most of our runs take less than one day. Cer et al. (2010) compared tuning a phrase-based SMT system with BLEU, NIST, METEOR, and TER, and concluded that BLEU and NIST are still the best choices for MT tuning, despite the proven higher correlation of METEOR and TER with human judgment.",
        "GPT2_formal_text": "_shape (m_n) is calculated for each word. The input for word_rnn is the text x, and the output for word_tnn is the sequence of features, which includes the masked token. Here, x_i = [x_i1, ..., x_iT, where T is the total number of tokens. This setup allows the model to learn representations for words that aren't very similar to each other. Formal: To train the model, we use the normal distribution with a mean of 0 and a variance of 1. Formal: We use two different sets of parameters to set the number of hidden units in the multi-head attention model. Formal: The hidden size for the multi-head attention model is h_M_t. Formal: We use a 1-dimensional version of the hidden state h_M_t and set the activation function to tanh. Formal: We set the position embedding dimension to d_s, where d_s is the size of the output embedding. Formal: The value of the RNN's output layer at the word level is calculated based on the hidden state and position embedding dimensions. Formal: We use a ReLU activation function on the output layer. Formal: We build the multi-head attention model using a Bi-LSTM with a hidden size of h_m, and we use a ReLU activation function to get the final representations. Formal: We use the bi-LSTM hidden state h_m to train the model. Formal: We start by pre-training the Bi-LSTM with the word embedding vector x_i from the training set and the word sequence representation x. Formal: We also pre-train the output layer to produce word representations with a dimension of d_s and a ReLU activation function. Formal: The model's first hidden layer with a dimension of d_s gets activated, and we use the activation function tanh to get the final output. Formal: We train the model using Adam, using an initial learning rate of 0.1. Formal: Following the architecture of the model, we pass the values of the output layer (d_s) through an output layer to get the final representations. Formal: Finally, we use a softmax function to get the output of the model. Formal: We also calculate the cross-entropy loss and the cross-entropy gain to",
        "directory": "D11",
        "filename": "D11-1035.json"
    },
    {
        "casual_text": "Wow, this looks like a bunch of random symbols and characters! It seems like it might be some kind of code or encrypted message. If you're trying to figure out what it means, you might need to decode it or find the key to unlock its meaning. Maybe it's part of a puzzle or a secret message? If you have any more context or clues, that could help crack it!",
        "formal_text": "£ v G | 4 x z y U ) v U I v y | 4 v º r È y U A y ¡ Y { i ¤ y y ± Ê } z y m ¦ y { É U p ¤ y ® ¡ Y { 4 6 y y ± z ¡ } I y | 4 { ) ½ ) q § ¦ y ª U h { i y m 4 U y | 4 x | $ v { ¦ z y ª z U ¦ U ê | 4 v 4 y { 4 } p | 4 { z y m ) | i y ¤ z y m U ) Ũ | 4 v É | Õ y m U y m 4 6 v W | U ª 4 y m U ² b w 1 x z y m 4 y m U 4 y Ã 0 ª ¦ 3 À ª 0 ª § { 4 y m | i | 4 z v { 1 D z k { i y m 4 U y U { p U { i y y 2 n U v } p | 4 z 1 | 4 x y ¤ { i | 4 } p z U £ v U ¤ y % U { 4 { 4 } y | 4 x | µ y ¥ ¤ y | 4 U y z y { 4 { 1 ¤ ¤ y AE @ I Ç Ã © á & 6 0 ² Ã 0 r á ¢ Ì â I 8 7 d ¥ ¤ y | i y é ä ± { 4 ¢ U { ¦ x v 1 x p | i y m U y m © ' â I ë ¥ è 4 â f á î ¥ ¥ ö å I á 4 â f è f ó C ç 4 ë f ê á I â f ô © ' â ' ì ê 4 ë 4 ë 4 è I â ' ì 4 ä 4 à f ä f ç s å 4 ý I â è ' ê é I â ' à ¤ î ê á Ó ð ¦ ¢ ð f ð ð ¦! ¤ % ¢ 4 ö ù ê á Ó ð ¦ õ d ð ð ¦ ¢ ï \" ö 4 ð ê á Ó ð ¦ $ ð ð ¦ & ¢ù f ö ê á Ó ð ¦ ¢ ñ f ð ð ¦ ¢ ñ 4 ï f ñ \" ê á Ó ð ¦ & ¢ 4 ð ð ¦ $ # ¢ I ò ¥ ê á Ó ð ¦ ò ð ð ¦ õ 5 ö # ¢ ' ê á Ó ð ¦ ¢ ï f ð ð ¦ ¢ ð ù ñ \" ê á Ó ð ¦ ¢ ö f ð ð ¦ ¢ ð 4 ñ f ö 4 ð ê á Ó ð ¦!",
        "GPT2_formal_text": "\" (which are the first 3 words) of the question. We can turn this into a query by using a natural language processing (NLP) tool. Formal: We focus on the last word of the query and stick with it to figure out the answer. For instance, if you ask, \"What was the theme of the 2012 Winter Olympics?\", we look for the last word in the query to get the answer. Formal: We’re not stopping there—we’re also looking at the words around the word in the question, which helps us figure out the answer. We do this by calculating the cosine similarity between the words in the question and the words in the answer. Formal: The original system used the same setup as in the Wizard of Wikipedia model by Markert and his team (2016). In this setup, we don’t use any extra knowledge, just the original query, which is the key to understanding the question. Formal: We’re also comparing our system to a baseline that only uses the query to find the answer. Formal: In the example shown in Figure 1, we’ve got a simple example that shows how our approach works. Formal: We figure out the answer based on the query and the words in the question. Formal: The final answer is calculated by combining the query with the words in the question and the answer. Formal: We use a deep neural network (DNN) to figure out the final answer. Formal: We use a complex attention network (c-f) to handle the final answer representation. Formal: Each word in the question is turned into an embedding, which is then used to calculate the answer embedding. Formal: To keep things simple, we don’t use any extra knowledge. Formal: We’ll walk you through how we build the training data and what the final answer embedding looks like. Formal: For the final step, we use a cosine similarity function to find the similarity between the final answer and the question. Formal: The final answer is based on the query and the words in the question, along with the answer embedding. Formal: We use a bidirectional GRU (BiGRU) to process the query and the question, then combine everything to get the final answer. Formal: We’ll talk more about this in the next sections. Formal: We’ve got a baseline system that",
        "directory": "C02",
        "filename": "C02-1109.json"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way:\n\n- **v_q** represents the query vector.\n- **v_s** represents the sentence vector.\n- **LexCL** is used to compare the value of the word in **v_q** with the word in **v_s**.\n- **10best** means finding the top 10 sentences that match **v_q** based on cosine similarity between **v_q** and **v_s**.\n- **context** refers to the context of **v_q** and **v_s**.\n- **grammar** refers to the grammar of **v_q** and **v_s**.\n- **LexQL** is used to find the best match (**1best**) between **v_q** and **v_s**.",
        "formal_text": "v q ) repr. (v s ) Value LexCL v q word v s v q 10best v s cosine (v q, v s ) v q context v s v q grammar v s LexQL v q v s 1best",
        "GPT2_formal_text": "is initialized with a value of 0. Formal: The layers in the model are figured out as a mix of the input's vector and the attention layer's vector, like this: Formal: For the \"compact\" model, we start by picking the last hidden state vector, called h, from a pooling layer. Formal: The model then calculates the matrix for the attention layer, which is {h1, ..., hn}, using the same approach as in the previous section. Formal: Here, {h i } is the i-th hidden state vector of the i-th input token, and H t is a matrix with dimensions {H i }×d. Formal: Finally, the output from the attention layer is an average of the output from the last hidden state vector of the i-th input token. Formal: A linear projection of the attention matrix H t onto the hidden representation of the token we're focusing on is shown in Figure 2(a). The first row of this projection matrix H t is made by the last hidden state vector of the token, which has the dimension d. Formal: Formal: We use this projection matrix to figure out the attention weights for the token. Formal: Finally, we create the hidden representation for the token and its neighbors, H t for the i-th token and H n for the i-th block, by using the projection matrix H t. Formal: The projection matrix H t for a token T is shown in Figure 2(b). Formal: The attention weights for T are calculated by taking the dot product of the attention weights for each token. Formal: The final attention weights for the token are then calculated by applying the projection matrix H t. Formal: Formal: We can also use this same method to find the attention weights for the entire input sequence, which we call the \"attention_mask\" matrix. Formal: Formal: This approach has been used before for token classification (Li et al., 2015), and it's shown in Figure 2(c). Formal: Formal: Lastly, the output from the attention layer is an average of the output from the last hidden state vector of the token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1055.json"
    },
    {
        "casual_text": "This paper suggests tackling this issue by tweaking the Recognizing Textual Entailment (RTE) framework to better handle metaphor interpretation. RTE is designed to be a broad, general-purpose task that covers a lot of the semantic reasoning needed across different applications (Dagan et al., 2007). It's seen as super important for building smart but reliable natural language processing systems because most of the semantic reasoning needed in stuff like question answering and information extraction can be boiled down to RTE problems (Dagan et al., 2007). Basically, textual entailment is about figuring out if a hypothesis can be drawn from a given text. In simpler terms, it's a one-way relationship between two pieces of text—Text T and Hypothesis H—where if T entails H, then anyone reading T and looking at H would think H naturally follows from T. For example, take example 1560 from the RTE-1 dataset, which uses the metaphor 'incubate'.",
        "formal_text": "This paper proposes to address this by adapting the Recognizing Textual Entailment (RTE) framework for metaphor interpretation. RTE aims to be an abstract and generic task that captures major semantic inference needs across applications (Dagan et al. , 2007). RTE is considered central for the development of intelligent yet robust natural language processing systems because most of the semantic inference needed in natural language applications such as QA and IE can be characterized as problems in RTE (Dagan et al. , 2007). Intuitively, textual entailment consists of determining whether a hypothesis can be inferred from a given text. The textual entailment operational definition is a directional relation between two text fragments, Text T and Hypothesis H such that T entails H if humans reading T and considering H will infer that H follows from T. An illustration can be given by example 1560 of the RTE-1 dataset (which involves a metaphor in the use of 'incubate'):",
        "GPT2_formal_text": ". The values for these features are stored as inputs in a dictionary. Formal: To understand how models learn, we use a standard logistic regression model that looks at the log probability of the correct label given the current example. This means the model assigns a probability to each possible label and ranks the examples based on that probability. Formal: We use a linear classifier to figure out if an example is a type-specific word (let's call it ⊥) or not. The label for an example is just a number that tells us if it's a type or not. Formal: If a label isn't part of a dictionary entry, we calculate the log probability using the examples from the dictionary and then toss out any instances that don't fit the definition. Formal: We measure how similar two examples are by looking at two parts of the model: (1) the log probability for the correct label, ⊥; and (2) how similar the examples are in terms of how often they appear together. Formal: The classes we see in a dictionary are usually the same as the classes we can get from the training data. Formal: Since we don't have enough examples for each class, we pick the class that has the highest probability. This is basically the one that gets the highest log probability. Formal: The labels in the dictionary are all in the same order as the training data, so we can rearrange them to match the classes in the training data. Formal: Each example is treated as a random variable in its own learning process. Formal: Formal: In the example shown in Figure 1, the classification is done in batches, so each batch has two examples. Formal: We use a logistic regression model to figure out the likelihood of each label based on the data. Formal: The log probability of a label is just the maximum of the log probabilities for its class. Formal: A probability is calculated by multiplying the log probabilities for all the classes together. Formal: If a label isn't in a dictionary entry, we calculate the log probability using the examples from the dictionary and then toss out any instances that don't match the definition. Formal: Formal: Formal: The first three parts of our model are pretty straightforward. The first two are based on the log probabilities of the correct label ⊥ and the possible classes in the dictionary. Formal: We use a linear classifier to figure out if an example is a type-specific",
        "directory": "C08",
        "filename": "C08-2001.json"
    },
    {
        "casual_text": "Okay, so we're talking about a 2 × 500 English to German model here. The word embeddings make up about 63% (that's 50 million out of 84 million) of the total parameters. But the size of these embeddings doesn't really affect how fast the model runs because the word embedding layer is just a simple lookup table that only impacts the first layer of the model. So, we're more focused on cutting down the memory usage of the student models by pruning weights.\n\nWeight pruning for NMT (Neural Machine Translation) was looked into by See et al. (2016), and they found that you can prune up to 80-90% of the parameters in a big NMT model without losing much performance. So, we took our best English to German student model (the 2 × 500 Seq-KD + Seq-Inter one) and pruned x% of its parameters by getting rid of the weights with the smallest absolute values. After that, we retrained the pruned model using Seq-KD data with a learning rate of 0.2 and fine-tuned it with Seq-Inter data at a learning rate of 0.1.\n\nAs See et al. (2016) pointed out, retraining was super important. The results are in Table 3, so check that out for the details.",
        "formal_text": "2 × 500 English → German model the word embeddings account for approximately 63% (50m out of 84m) of the parameters. The size of word embeddings have little impact on run-time as the word embedding layer is a simple lookup table that only affects the first layer of the model. We therefore focus next on reducing the memory footprint of the student models further through weight pruning. Weight pruning for NMT was recently investigated by See et al. (2016), who found that up to 80 − 90% of the parameters in a large NMT model can be pruned with little loss in performance. We take our best English → German student model (2 × 500 Seq-KD + Seq-Inter) and prune x% of the parameters by removing the weights with the lowest absolute values. We then retrain the pruned model on Seq-KD data with a learning rate of 0. 2 and fine-tune towards Seq-Inter data with a learning rate of 0. 1. As observed by See et al. (2016), retraining proved to be crucial. The results are shown in Table 3.",
        "GPT2_formal_text": "_t2r is basically a symmetric version of the attention mask, but it only uses the first term from the input, so the output stays the same.\n\nThe L2-norm of the output vector, which is the norm of the attention matrix, is set to 2. The other parameters are randomly initialized. The global attention mechanism, which is explained in section 2.1, is used to combine the hidden representations from the token-level and sentence-level attention. The global attention mechanism has two main parts: an encoder block and a decoder block. Here's what they do:\n\n1. **Encoder Block**: This part takes the raw token representations, which we call (x t−1), and turns them into a compact representation of the context, which we call (y t−1).\n2. **Decoder Block**: This part uses the generated context vector to create a new token representation, which we call (x t ).\n\nThe attention mechanism works by combining these two representations to give each token a weight. The weight is calculated using a matrix called (x t−1) + (y t−1). The weight for the token that includes the input word is calculated using equation (1). Formal: The attention mechanism is defined like this: Formal: We have two inputs, x t and y t−1, and we want to figure out the attention weights for them. Formal: The input and output representation are built using the dense layer in the transformer architecture, which is described in this paper. Formal: A dense layer takes the input representation x t−1 and the output representation y t−1 as input. The next thing it does is to turn these inputs into a compact representation, which we call (y t−1). Formal: We use the input representation x t−1 to build the output representation y t−1. The next part is to use the output representation x t−1 to generate the token representation. Formal: We use the output representation x t−1 to represent the token t.\n\nAfter that, we send these two representations through a linear layer to get the final output. Formal: The final output is a matrix of weights, W t, and the attention weights are calculated using the L2-norm of the output matrix, W t. Formal: The final token representation is a dense matrix of weights, W t. Formal: We use the input representation x t−",
        "directory": "D16",
        "filename": "D16-1139.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nIf you have something like `t[ε → t]`, it just means `t`. But if you see `f(t1, ..., tn)[(i • p) → t]`, it means you take `f(t1, ..., tn)` and replace the `i`-th part with `t` after applying `p` to it. This works as long as `i` is between 1 and `n`.",
        "formal_text": "t[ε → t ] = t f (t 1, . . . , t n )[(i • p) → t ] = f (t 1, . . . , t i [p → t ], . . . , t n ) for 1 ≤ i ≤ n.",
        "GPT2_formal_text": "= {q, t} and type_ids = {t, t'}, where q and t' represent the query and entity spans, respectively. Formal: This step only happens if the query has more than two sentences, or if the first sentence has an entity span that doesn't start with a stop word. If that happens, the next two steps would be: Formal: If the first sentence is an entity span that starts with a stop word, the next two steps would be: Formal: The second sentence would be the query, and the first sentence would be the entity span that starts with a stop word. Formal: We use a special token to separate the query and entity spans, called the \"break token,\" as mentioned in Section 3.4. Formal: We also want to keep the query-entity pair as separate as possible. So, the first and second sentences in the query and entity are matched together to create the query and entity. Formal: Let's say our query has sentences like: Formal: For the two most common entity types, like person and organization, we only need to create queries like: Formal: (q) / (s) / (q') Formal: (q) / (s) / (q') Formal: (q) Formal: (q) Formal: (q') Formal: (q) Formal: (q') Formal: Formal: (q') Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E06",
        "filename": "E06-1048.json"
    },
    {
        "casual_text": "The main point here is that instead of using text, we're defining U as a series of speech units that are super strong and efficient, kind of like text, but we figured them out without needing text to guide us. We set up the S2U model so that when we want to make a guess, we can say U = f(S), which means we can turn any speech audio S into a sequence of units U. By adding this third part, we can train P(U|I) using a bunch of images with spoken captions.",
        "formal_text": "The key idea in this paper is to instead define U to be a sequence of learned speech units that are as robust and compact as possible like text, but discovered without text supervision. We define inference with this S2U model as U = f (S), enabling us to \"transcribe\" any given speech audio waveform S into a sequence of units U. The addition of this third component enables us to train P (U | I) from a dataset of images paired with spoken captions",
        "GPT2_formal_text": "ed with weights α. Formal: We take the output of each BERT layer and turn it into a vector representation that goes with the token representation h_t. Formal: The model we use is basically a mix of the approach from (Zhang et al., 2019). Formal: We train our model using different setups: a single-layer CNN, a two-layer CNN, and a bidirectional LSTM (BiLSTM). \n\nIn the first setup, we train the CNN with the mean vector m_t = (x_0, ..., x_n), and the LSTM with the average vector m_t = (x_a, ..., x_n). Formal: For the second setup, we use the same setup but train the BiLSTM with the average vector m_t = (x_a_i, ..., x_n_i). \n\nFor the last setup, we use the CNN but this time we train the LSTM with the average vector m_t = (x_a_i_1, ..., x_n_i). Formal: In the final setup, we train the BiLSTM with the average vector m_t = (x_a_i_m, ..., x_n_i). Formal: Finally, the output vector from the BiLSTM is a weighted sum of the attention vectors, which gives us the final representation h_t. Formal: Formal: We also apply a normalization term to make sure the predicted entity label (h_t_e) is closer to the actual label (h_t). Formal: We train the model using a batch size of 128, and the validation set size is set to 10. Formal: Formal: So, the final entity token representation (h_t_e) is a combination of the predicted label and the actual label, all within a certain range. Formal: Formal: We use the max-pooling method to minimize the cross-entropy loss, which helps us figure out how important each entity token is. Formal: We also apply a normalization term to make sure the predicted entity label (h_t_e) is closer to the actual label (h_t). Formal: Formal: Finally, the output vector from the BiLSTM is a weighted sum of the attention vectors, which gives us the final",
        "directory": "acl",
        "filename": "2021.acl-long.411.json"
    },
    {
        "casual_text": "Got it! Just so you know, there are some pairs of types where you can't find a set of sources like that. For example, the apply set.",
        "formal_text": "Note that there are pairs of types for which no such set of sources exists; e. g. the apply set",
        "GPT2_formal_text": "\" and \"attributes\" are both numbers that can be 0 or 1. The numbers are labeled as 1 or 0 in each case. Formal: We create a special token for each entity type and assign it to a unique spot. For each token, we grab its matching entity types, attributes, and the entity type itself from the knowledge base. Formal: We use a special token called [EVENT] for each type, which stands for [EVENT]1, ..., [EVENT]N. We also create a special token for each attribute type. Formal: The sentence we're looking at is made up of one main event, e, and a bunch of related details, C = {c1, ..., cN}. The info we're working with includes the entity type, type_id, attribute types, and possible values for each attribute type. Formal: For every entity type, the entity type, type_id, attribute type, and possible values for each attribute type are all listed in the knowledge base. Formal: The part of the sentence we're interested in, e, is just the token with the [EVENT] token. Formal: The knowledge base is made up of a bunch of entities, T = {e1, ..., eT}, and all these entities are linked together using relationships. Formal: For each relationship, we grab the entity type, type_id, attribute type, and values for each attribute type. Formal: Finally, we get the entity type, type_id, attribute type, and possible values for each attribute type. Formal: Formal: Each relation is linked to a specific entity type, type_id, attribute type, and values for each attribute type. Formal: Formal: Here's the formula for figuring out the entity type and attribute type for an entity, T = {e1, ..., eT}. Formal: Formal: Formal: Using the knowledge base's info, we figure out the entity type and attribute type for an entity, T = {e1, ..., eT}. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "Alright, so δ is picked depending on the punctuation in it 7. To compare, we also made two actual sequence groups: the first sentence set and the last sentence set. These include the first and last sentences from the articles in the WikiText validation and testing splits.",
        "formal_text": "where δ is chosen based on the punctuation in it 7. As control groups, we also construct two real sequence sets, first sentence set and last sentence set. They consist of the first sentence and the last sentence of the articles in WikiText validation split and testing split.",
        "GPT2_formal_text": ". If we get a bunch of M responses in a row, we can figure out the attention weights for the n-best options by running the seq2seq model on them. Formal: The attention weights are calculated using something called multi-head attention. Formal: For the top-1 responses, we just add up all the attention weights. But for the top-5 and top-10 responses, we take the log-likelihood and average the attention weights. Formal: We also use one final attention layer on the final response to get the final label. Formal: We create a representation for each of these k responses using a mix of convolutional, max-pooling, and local attention techniques. Formal: Next, we run the model on the dataset to create a \"sentence representation.\" Formal: We'll use these representations as a kind of starting point to figure out how much attention to pay to different parts of the response. Formal: To get the attention weights, we run the model on the response to the query to get the attention weights. Then, we combine these weights to get the final attention weights for the response. Formal: We'll use these weights to figure out how much attention to pay to different parts of the response. Formal: In Table 2, you can see the results of our experiments for the attention weights. Formal: The model did really well, with an F1 score of 71.82%. Formal: Next, we use the attention weights to boost the model's performance even more. Formal: To make things even better, we use a binary classifier to make the model more robust. Formal: We also use the evaluation metrics mentioned earlier to check how well the model handles various types of text. Formal: The model's performance improved by 2.12% in macro-F1 and 2.05% in micro-F1. Formal: For example, our model hit the top score for the 'WTF' sentence in the dataset, which means it can handle longer, more complex sentences better. Formal: Formal: When we compare it to the basic model, the attention weights really make a difference. Formal: For the sentiment words that could be negative, our model outperformed the basic model. Formal: The model also improved in multi-task learning by adding a multi-task learning approach. Formal: We use a multi-task learning model to improve the model's performance even more. Form",
        "directory": "blackboxnlp",
        "filename": "2021.blackboxnlp-1.16.json"
    },
    {
        "casual_text": "The objective function is basically about regular language modeling, where we're trying to minimize something called E l. This E l represents the negative log likelihood of the current word in a sequence, based on the words that came before it.",
        "formal_text": "The objective function is an objective of regular language modeling E l, which is the negative loglikelihood of the current word in the sequence given the previous words.",
        "GPT2_formal_text": "are used to make sure that the correct answers for the prompts are picked. This way, the responses are matched up correctly. We treat this as a one-to-one match, meaning we only keep the answers that are linked to the correct answers. The question and answer pairs for an input are represented as a set of vectors. For instance, the input shown in Figure 1 (b) is represented as (b 1, ..., b n ). Formal: In the neural network setup, the attention weights for the target part are calculated using a linear transformation. Formal: On the other hand, in the context-dependent model, the attention weights are adjusted based on the context of the prompts. Formal: In both cases, the context vector for the question is created by combining the attention vectors from the two parts. Formal: Also, we try to avoid using random attention weights that don't match the context. For example, if there's no context in the prompt and the query is just a random list of words, the attention weight of an output should focus on the specific part of the sentence where the answer is mentioned. Formal: Lastly, the final attention vector for the question is calculated by the output from the combined model. Formal: The full model structure is shown in Figure 1. We use a simple linear layer to create the vector representation for the input prompt x_i, which is the output from the model we just talked about. Formal: The total attention weight for the answer is calculated by combining the attention weights from the two parts. Formal: Once we have the final attention vector, we use a single-layer bidirectional LSTM to calculate the cross-entropy loss for the predicted probability distribution. Formal: Formal: Finally, the predicted probability distribution is calculated using the cross-entropy loss from the model. Formal: In Figure 1, the output from the model is the probability distribution from the cross-entropy loss. Formal: Formal: Using the model's output, we calculate the cross-entropy loss for the predicted probability distribution. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.43.json"
    },
    {
        "casual_text": "No big deal, but you can easily check that this equation hits its lowest point when XᵀX = I. Basically, that happens when the topics are all lined up nice and neat, like they're standing in a straight line, not overlapping.",
        "formal_text": "It is trivial to verify that this equation has a minimum when X T XX T X = I. This is the case when the topics are orthonormal:",
        "GPT2_formal_text": "ed is all about masking entities that have low relevance to the current query. This approach creates more useful summaries that avoid repeating the same stuff over and over. So, it naturally leads to higher recall. This is what we call the \"Target Embedding Embedding.\" Formal: In the original paper by Chen and Lapata (2014), they explained how this matching process works. They refer to this as the \"Target Embedding Embedding.\" Formal: The \"Target Embedding Embedding\" is a way to tell entities apart from their related or \"Target\" references. It's a one-hot representation of the entity with some extra information tied to it. Formal: The target embedding embedding is what's being focused on. Formal: The knowledge graph embedding (KGE) is like a general idea or \"concept\" of the entity, which is usually a set of things related to it. Formal: The query embedding embedding (QE) is the representation of the query, which is usually the actual query text itself. Formal: The knowledge graph embedding (KGE) and the query embedding (QE) are like the general concept of the entity. Formal: Knowledge Graph Embeddings (KGE) are like a database that holds all the entities in the knowledge graph. Formal: Knowledge Graph Entities (KGE) are basically the entities in the knowledge graph. Formal: There are two ways to connect entities: one is a direct connection using the embedding embedding, and the other is a more indirect connection using the query embedding. Formal: The entity representation embedding (ER) and the knowledge graph embedding (KGE) are like two key pieces of information. Formal: The \"Target Embedding Embedding\" is the entity embedding that gets the most attention from the system. Formal: The query embedding embedding embedding is like a general idea of the query, which is usually the actual query text itself. Formal: The knowledge graph embedding (KGE) is like a database that holds all the entities in the knowledge graph. Formal: Formal: Finally, the query embedding embedding (QE) is like a more general concept of the query, which is usually the actual query text itself. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D13",
        "filename": "D13-1179.json"
    },
    {
        "casual_text": "In 2019, during the fourth Conference on Machine Translation (WMT), the shared task about filtering parallel corpora was all about dealing with low-resource situations. The top-performing submission used crosslingual sentence embeddings, which were trained using parallel sentence pairs (as mentioned by Chaudhary et al. in 2019). Artetxe and Schwenk (2019a) came up with a similar approach. Both papers addressed the limitations of cosine similarity by looking at the surrounding sentences, which helped them beat systems that only used cosine similarity.",
        "formal_text": "In 2019, at the fourth Conference on Machine Translation, WMT, the shared task on parallel corpora filtering focused on low-resource conditions. The method central to the best-performing submission was the use of crosslingual sentence embeddings, trained from parallel sentence pairs (Chaudhary et al. , 2019). Artetxe and Schwenk (2019a) devised a similar method. Both papers tackle the inconsistencies of cosine similarity by investigating the neighbourhood of a given sentence pair, outperforming systems using only cosine similarity.",
        "GPT2_formal_text": ", rel_ids, and rel_type_ids are all part of the same thing. Formal: 1. The ID of the entity you're looking at, like w_i. Formal: 2. The ID of the text you want to analyze, like t_i. Formal: 3. The label of the entity you're talking about, like t_i. Formal: 4. The ID of the text you're analyzing, like t_i. Formal: 5. The ID of the entity you're referring to, like t_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-srw.25.json"
    },
    {
        "casual_text": "We've pinpointed and included time offsets for various elements like speech, subtitles, text on graphics and scenes, body movements, gestures, shots (with a focus on foreground and background), and keyframe regions in the COSMOROE relations. All the visual stuff has been tagged by annotators using simple one or two-word labels that describe actions or entities. These tags came from just watching the video without any audio. The tagging process was based on a cognitive categorization method, which uses the \"basic level theory\" of categorization (Rosch, 1978). Right now, the annotated data includes 5 hours of Greek travel documentaries and 5 hours of English ones. Out of the Greek content, 3 hours have been validated, and we've also done a preliminary study on inter-annotator agreement (Pastra, 2008).",
        "formal_text": "Time offsets of the transcribed speech, subtitles, graphic-text and scene text, body movements, gestures, shots (with foreground and background distinction) and keyframe-regions are identified and included in COSMOROE relations. All visual data have been labelled by the annotators with one or two-word action or entity denoting tags. These labels have resulted from a process of watching only the visual stream of the file. The labelling followed a cognitive categorisation approach, that builds on the \"basic level theory\" of categorisation (Rosch, 1978). Currently, the annotated corpus consists of 5 hours of TV travel documentaries in Greek and 5 hours of TV travel documentaries in English. Three hours of the Greek files have undergone validation and a preliminary inter-annotator agreement study has also been carried out (Pastra, 2008).",
        "GPT2_formal_text": ". This is done by setting a weight for the context vector, which is then used to build a contextual representation of the query. This model has three main parts: the context vector, the query vector, and a global context embedding that the query and the dialogue act together. Formal: For each query and response, we're looking at how similar their representations are. We do this by looking at the cosine similarity between their representations, which is calculated as cos(w_q_i, r_i) = 1 / (r_i - r_q_i). Formal: Following what Li et al. did in 2019, we use a triplet position embedding (q, r_i, t_i) to represent the query and the response, with a length of (r_i - r_q_i). Formal: We use a sentiment embedding (a_t_i) to create a sentiment representation for the query, r_i, and the response, r_q_i. We define the sentiment of the query as a weighted sum of its sentiment vectors, as shown in Equation (1). Formal: We apply attention and a sigmoid function to the query and response to give them different contextual representations, which we then combine to get the final query representation. Formal: This framework is super flexible and can be used in other areas. Formal: For each query and response, we calculate two attention weights, h_q_i_t and h_r_i_t, which are the similarity between the query and response. We calculate the cosine similarity between these two attention weights, h_q_i_t and h_r_i_t, using the cross-entropy loss function. Formal: We apply the attention and sigmoid function to the query and response representations of the two queries to get the final representation, which we then combine to get the final query representation. Formal: We pick the query with the highest similarity score, r_q_i_t, and the response with the lowest, r_r_i_t. This gives us the final representation for the query and response. Formal: For each query and response, we calculate two attention weights, h_q_i_t and h_r_i_t, using the attention embedding. Formal: We apply the attention and sigmoid function to the query and response representations to get the final",
        "directory": "E09",
        "filename": "E09-2014.json"
    },
    {
        "casual_text": "When it comes to sentences with multiple events, the suggested solution can not only spot and categorize these events but also connect the right arguments to each event, which you can see in Table 3 and Table 4.",
        "formal_text": "As for the characteristic of having multiple events in a sentence, the proposed solution is able to detect and classify the events as well as link arguments to their rightful event, as shown in both Table 3 and Table 4.",
        "GPT2_formal_text": "(r) = {r1, ..., rn, ..., rm}. Formal: For each document d i, we add a binary feature (f) to turn it into a vector with n elements. Formal: The classifier S lc, f can be any fancy neural network, like the one by Srivastava et al. (2017) or the one by Zhou et al. (2018). Formal: For each document d i, we also add a binary feature (f) to turn it into a vector with n elements. Formal: This setup is a bit different from the usual multi-class classification task. Formal: The classifier S lc, f can be any fancy neural network, like the one by Srivastava et al. (2017). Formal: Formal: For a document d i, we also add a binary feature (f) to turn it into a vector with n elements. Formal: Using these two binary features, we can figure out the class label r i ∈ R d. Formal: Now that we've figured out the labels, we can use a classifier to figure out the label for each document d i. Formal: By using these labels, we can figure out the labels for each document d i. Formal: Formal: We've got the labels for each document d i and their labels for the other documents in the dataset, which we can use to find the labels for the other documents in the dataset. Formal: Formal: We've got the labels for each document d i and their labels for the other documents in the dataset, which we can use to find the labels for the other documents in the dataset. Formal: Formal: Lastly, we can find the label for each document d i by using the labels for all the documents in the dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "econlp",
        "filename": "2021.econlp-1.10.json"
    },
    {
        "casual_text": "We put our framework up against a bunch of other methods to see how it stacks up. (Shi and Lin, 2019) is a well-known approach for semantic role labeling, specifically for predicate-argument prediction. It uses features pulled from BERT (Devlin et al., 2019) and applies Conditional Random Fields (Lafferty et al., 2001) for structured prediction, which we’ll call BERT-CRF. Then there’s Li et al. (2021), who suggest using a conditional neural text generation model for document-level argument extraction. This method deals with each event on its own, so we’ll refer to it as BART-Gen.\n\nFor our part, we’ve got a memory-enhanced training approach that uses extra context we’ve pulled in, and we’re calling that Memory-based Training. We also ran tests with argument pairs and constrained decoding to see how each piece contributes. In Table 3, you’ll find the main results for document-level informative argument extraction. The scores for argument identification are higher than for classification because identifying just requires matching the span offsets. Here’s what we noticed:",
        "formal_text": "We compare our framework to a number of competitive baselines. (Shi and Lin, 2019 ) is a popular baseline for semantic role labeling (predicateargument prediction). It performs sequence labeling based on automatically extracted features from BERT (Devlin et al. , 2019) and uses Conditional Random Fields (Lafferty et al. , 2001) for structured prediction (BERT-CRF). Li et al. (2021) propose to use conditional neural text generation model for the document-level argument extraction problem, it handles each event in isolation (BART-Gen). For our proposed memory-enhanced training with retrieved additional context, we denote it as Memory-based Training. We also present the argument pairs constrained decoding results separately to see both components' contributions. 8 In Table 3, we present the main results for the document-level informative argument extraction. The score for argument identification is strictly higher than argument classification since it only requires span offset match. We observe that:",
        "GPT2_formal_text": "ed, target_attention_masked, and location_attention_masked to get the attention, attention weight vector, and location vector for the target node. Formal: We also check out the attention weights for each class in the input image sequence, as shown in Figure 1. Formal: In this part, we'll start by explaining how attention weights are calculated and then walk through how we generate attention for the input image sequence. Formal: Let's start by explaining how attention weights are calculated for each class in the input image sequence. Formal: We calculate attention weights for different classes in the input image sequence using an attention mechanism. Formal: In our experiments, we trained a single-layer Transformer using the target image embedding. Formal: To generate an attention vector for a target node, we first train the target node's attention. After that, we calculate the attention weight vector for the target node's graph. Formal: Next, we train the target node's attention using the relation graph. Finally, we combine the graph's attention and the graph's attention weight vector to get the attention weight vector for the target node. Formal: We use the target image embedding and the relation graph to train the target node's attention, and then we calculate the attention weight vector for the target node's graph. Formal: We also use the target node's graph state to generate the attention vector for the node's graph. This helps us learn the graph's attention vector for the node's graph. Formal: We add the node's graph state to the attention vector and the graph's attention weight vector to get the node's attention weight vector. Formal: Lastly, we combine the node's graph state and the graph's attention weight vector to get the attention weight vector for the node's graph. Formal: In the next sections, we'll talk about the different types of attention we used for the target node's graph. Formal: We train the graph using the target image embedding, the graph's state, and the attention vector to get the graph's attention vector. Formal: To generate the attention vector for the target node's graph, we also train the target node's attention using the graph's state. Formal: After the graph has been trained, we combine the graph's attention and the graph's attention weight vector to get the attention weight vector for the target node's graph. Formal: Lastly, we combine the graph's attention and the graph's attention weight vector",
        "directory": "acl",
        "filename": "2022.acl-long.361.json"
    },
    {
        "casual_text": "Hey, just so you know, in the current version of this machine, delinking basically means adding a sort of \"stop\" signal to the feature you want to delink. This idea was talked about by Kalman and Kornai back in 1985, and Kalman also mentioned it in 1986.",
        "formal_text": "Note that delinking in the present form of the machine consists of adding an inhibitory link leading to the feature to be delinked. Kalman and Kornai (1985) and Kalman (1986).",
        "GPT2_formal_text": "= {}, candidates = {}, candidate_ids = {}, and output_ids = {}, with some extra info like the id and the candidate's name. Formal: In this paper, we’re introducing a fresh way to evaluate graph embedding models. The goal is to build models that can generate natural-sounding text based on graphs. To do this, we’ve created three datasets for graph embedding generation, which you can check out in Figure 1. Formal: For each dataset, we’ve collected a set of possible sentences, one for each label. For each sentence in the dataset, we’ve generated at least one graph query (let’s call it h_q). We use the validation set to train our models and the test set to check how well they’re performing. For generating new questions, we’re aiming for a generation rate of 0.1. The results we report are averaged over three runs, each using 5 questions. Formal: We use a standard seq2seq model, which was introduced by Luong and Nocedal in 2015, to generate these questions. The model is trained on a development set and then tested on the test set. Formal: To get the text, we use a batch model (from Choi et al., 2017). Formal: In the first run, we generate 5 random sentences from the validation set using the same settings as in the first pass. After that, we test the model on the test set. Formal: We do this five times and report the average results. Formal: We then pick the top 10 sentences that score the highest on the validation set. Formal: We then calculate the scores for the top 10 sentences from the test set using the same settings as in the first pass. Formal: After that, we calculate the scores for the top 10 sentences from the test set using the same settings as in the first pass. Formal: Finally, we pick the top 10 sentences that score the highest on the validation set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E87",
        "filename": "E87-1047.json"
    },
    {
        "casual_text": "We used Fleiss' kappa to see how well the crowd's evaluation matched up with the oracle and expert judgments for each translation task. The kappa scores are broken down like this (based on Landis and Koch, 1977): \n\n- If κ is less than 0: \"none\"  \n- If κ is 0 to 0.2: \"slight\"  \n- If κ is 0.2 to 0.4: \"fair\"  \n- If κ is 0.4 to 0.6: \"moderate\"  \n- If κ is 0.6 to 0.8: \"substantial\"  \n- If κ is 0.8 to 1.0: \"almost perfect\"",
        "formal_text": "Fleiss' kappa correlation coefficient comparing the obtained crowd-based evaluation results to the oracle and expert judgments for each translation task. The κ scores are interpreted in (Landis and Koch, 1977) as follows: κ <0: \"none\" κ ≤0. 6: \"moderate\" κ ≤0. 2: \"slight\" κ ≤0. 8: \"substantial\" κ ≤0. 4: \"fair\" κ ≤1. 0: \"almost perfect\"",
        "GPT2_formal_text": "ed_word_tokens = [masked_word, mask_mask] for the word i in the sentence s. Formal: In this model, we’ve included the word embeddings for the masked word. Formal: For each word we’re looking at, we use a weighted average of the attention weights for each token. This gives us a representation of the masked word. Formal: Check out Table 5—it shows how well our model does in identifying the masked token, H_t. In this case, H_t is the masked word, and the attention weights (α_t, β_t) are calculated using a method called HAN. Formal: After we’ve got the representation of the masked word, we use it to figure out the attention weights for the token. Formal: The token embeddings are just the normalized vectors that represent the masked word, H_t. Formal: Then, the attention weights (α_t, β_t) are calculated based on the token embeddings. Formal: Finally, the attention weights are used to calculate the attention weight matrix for the masked token. Formal: Our model, which uses this masked token representation H_t, can handle different scenarios. Formal: In the masked-word scenario, our model can recognize words like \"turds,\" \"crap,\" and \"shitty\" right away. But when it comes to words like \"tasty,\" \"chewy,\" and \"cheeky,\" it’s not easy to get that recognition right. Formal: In the text-to-speech scenario, our model can recognize words like \"polite,\" \"friendly,\" \"friendly company,\" \"friendly place,\" and \"friendly society.\" Formal: In the gender scenario, our model can recognize words like \"girl,\" \"boy,\" and \"girl,\" as well as words like \"man,\" \"woman,\" and \"mankind.\" Formal: Formal: Lastly, we can also recognize words like \"man,\" \"woman,\" \"king,\" \"queen,\" \"queenkind,\" \"mankind,\" \"queenkindkindkind,\" \"mankind,\" \"queen,\" \"king,\" \"queenkind,\" \"kingkind,\" \"king,\" \"kingkind,\" and \"queen.\" Formal: Formal: We’ve included some examples of how the model performs in all these situations. Formal: Table 6",
        "directory": "eamt",
        "filename": "2012.eamt-1.56.json"
    },
    {
        "casual_text": "Okay, so let's break this down in a simpler way:\n\nbalAPinc(u, v) is basically LIN(u, v) multiplied by APinc(u, v). \n\nNow, APinc(u, v) is calculated by taking r times the sum of P(r) multiplied by rel(f r) for all r in F(u).",
        "formal_text": "balAPinc(u, v) = LIN(u, v) • APinc(u, v) APinc(u, v) = r [P (r) • rel (f r )] |F( − → u )|",
        "GPT2_formal_text": "_lgt_word(x) = x + 1; A_t = A_t - 1; w_t = w_t + 1;\n\nIn the updated attention matrix, there are three parts, each containing a word vector and a character vector. Formal: So, the new attention matrix for the sentence x is: Formal: Here, A_t is the attention matrix for the word vector x, and A_lgt_word(x) is the attention matrix for the character vector x. Formal: The updated attention matrix for the sentence is: Formal: The final attention vector for the sentence x is: Formal: Formal: The attention weights for the different layers, like i_l, i_r, and i_c, are calculated as: Formal: The attention weights for each layer are: Formal: The final attention weight for the sentence is: Formal: Formal:\n\nIn the matrix, α_l is the number of layers, and α_r is the number of RNN connections. Formal: Formal: The final attention weight for the RNN module is: Formal: Formal: Formal: The final attention weight for the RNN module is: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1268.json"
    },
    {
        "casual_text": "The second class focuses on adding extra info to existing text, like structured knowledge (Zhao et al., 2018; Ghazvininejad et al., 2018; Dinan et al., 2019), personal details (Li et al., 2016b; Zhang et al., 2018a), or emotions (Shen et al., 2017b; Zhou et al., 2018). But getting annotated text like this can be super expensive and is usually only available for specific areas with not much data. Some recent studies have started working on dialogue style transfer using personal speeches or TV scripts (Niu and Bansal, 2018; Gao et al., 2019). The difference with our approach is that we want to make general dialogue generation richer by using lots of non-conversational text, instead of sticking to just one style.",
        "formal_text": "The second class seeks to bring in extra information into existing corpus like structured knowledge (Zhao et al. , 2018; Ghazvininejad et al. , 2018; Dinan et al. , 2019), personal information (Li et al. , 2016b; Zhang et al. , 2018a) or emotions (Shen et al. , 2017b; Zhou et al. , 2018). However, corpus with such annotations can be extremely costly to obtain and is usually limited to a specific domain with small data size. Some recent research started to do dialogue style transfer based on personal speeches or TV scripts (Niu and Bansal, 2018; Gao et al. , 2019; . Our motivation differs from them in that we aim at enriching general dialogue generation with abundant non-conversational text instead of being constrained on one specific type of style.",
        "GPT2_formal_text": "ed_word embeddings and a vocabulary of k = 1, 2, 3, 4, 5, 6, 7, 8, 9, 10. To train the model, we mix the input word embeddings with the average attention weights for the labels in each layer and the input word vectors in the first and last layers. We do this for all k = 2 to 4. We also grab the hidden state vectors h_i for the i-th and j-th layers. We do this for all k = 4 to 8. Formal: To make sure the model doesn't just copy the input word vectors straight from the source, we add a simple 1-to-1 mapping from the source's embeddings to the words in the input. This way, the model can copy the word vectors directly from the source. Formal: Here's how we generate the final output. Formal: We use the input word vectors x = {x1, x2, ..., xT} for encoding the source input, and the target vocabulary Y = {y1, y2, ..., yT} for encoding the target input. Formal: After generating the input, we generate the target word vectors y = {y1, y2, ..., yT} using a Softmax classifier that's learned from the input. Formal: We take the target word vectors y and replace each one with the embedding of the source word x. Formal: For the first step, we just copy the embeddings x = {x1, x2, ..., xT} from the source to the target. We use the target word vectors y = {y1, y2, ..., yT} to encode the target input. Formal: For the second step, we use the embedding of the source word x to encode the target input. Formal: We repeat these steps for all the word vectors x. Formal: Once we've finished encoding the source input, we use a softmax classifier to predict the output word vectors y = {y1, y2, ..., yT} for the target word vectors. Formal: We repeat these steps for all the word vectors x. Formal: Formal: Finally, we update the target vocabulary Y by combining the source vocabulary's embeddings with the average attention weights for the target words. Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.634.json"
    },
    {
        "casual_text": "A lot of research on affixoids has been done on Germanic languages like German, Dutch, and Swedish (check out Ascoop and Leuschner, 2006; Booij, 2005; Booij and Hüning, 2014; Norde and Van Goethem, 2014). But we think affixoids aren’t just limited to these languages. They probably show up in other languages that have productive compounding too. Take English, for example—there’s not much systematic research on it, but words like \"quality\" (as in quality press/furniture/diamonds, but not in quality management) and \"nut\" (as in health/math/trivia nut, but not in pecan nut) could be considered English affixoids. \n\nEven for the languages that have been studied more, most of the work focuses on the theoretical side—like whether affixoids should be seen as a separate category from affixes or compounds. There hasn’t been much quantitative research using actual corpus data to, say, figure out how productive different affixoid candidates are or to back up the idea that most affixoid uses have some kind of evaluative meaning. So, our work is trying to fill that gap and bring some real-world data into the theoretical discussion.",
        "formal_text": "Research on affixoids has been centered on Germanic languages like German, Dutch and Swedish (Ascoop and Leuschner, 2006; Booij, 2005; Booij and Hüning, 2014; Norde and Van Goethem, 2014). However, we believe affixoids are not an exclusive feature of these languages. They are likely to arise in other languages with productive compounding. For English, for instance, there is little to no systematic research but arguably quality (as in quality press/furniture/diamonds but not in quality management) and nut (as in health/math/trivia nut but not in pecan nut) can be considered English affixoids. Even for languages on which there is more research, that work is typically focused on the theoretical relevance of assuming a category of affixoids that is distinct from affixes on the one hand and compounds on the other. Very little quantitative work using corpus data has been done to, for instance, study the level of productivity for different affixoid candidates or to substantiate the intuition that most affixoid uses carry evaluative meanings. Our work thus fills an empirical gap in the theoretical discussion.",
        "GPT2_formal_text": "= ('id_1', 'id_2', ..., 'id_n') ... and we're also including an example (called an example_id) that tells us which example the answer is supposed to be. \n\nOh, and by the way, the real answer is 'Abba', while the fake answer is 'Donovan'. Formal: • i_j is a real number that represents how many times the word in question appears with the word in example_id. Formal: • s_i is a set of words that describe the answer, including the word 'answer'. Formal: • λ_i is the number of times we want to predict the answer. This value is added to the embeddings of the words in question and the answer. Formal: • e_i is a vector that shows the probability distribution for the correct answer. Formal: • f_i is a vector that shows the probability distribution for the answer that matches the word in the example_id. Formal: • s_i and e_i are the actual embeddings and embeddings, respectively. Formal: • λ is a function that gives us the probability distribution of the correct answer. Formal: • r is just a set of numbers that are used to calculate the expected probabilities. Formal: • q is the score that tells us how likely it is that the correct answer is 'Abba' or 'Donovan'. Formal: • s_i is the answer embedding that matches the word in question. Formal: • λ_i is the number of times we want to predict the answer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C18",
        "filename": "C18-1325.json"
    },
    {
        "casual_text": "For each model, we tweaked the settings for batch size, learning rate, and dropout. We also tried out different sizes for the embedding, attention, and hidden layers. All the models were trained for up to 100 epochs, but we stopped early if there was no improvement in validation performance after 50 epochs. We used the Adam optimizer (from Kingma and Ba, 2015) to minimize the Cross Entropy Loss. To pick the best model, we looked at the BERTScore F1 (Zhang et al., 2020) on the validation set for the generation task, and for the resolution task, we went with accuracy. In the next part, we'll share the average scores and standard deviations from 5 runs with different random seeds. If you want more info on how we chose hyperparameters, the model setups, or how to reproduce our results, check out Appendix E.",
        "formal_text": "For each model, we performed hyperparameter search for batch size, learning rate, and dropout; also, the search included different dimensions for the embedding, attention, and hidden layers. All models were trained for up to 100 epochs (with a patience of 50 epochs in the case of no improvement to the validation performance) using the Adam optimiser (Kingma and Ba, 2015) to minimise the Cross Entropy Loss with sum reduction. BERTScore F1 (Zhang et al. , 2020) in the validation set was used to select the best model for the generation task, while we used accuracy for the resolution task. In the next section, we report average scores and standard deviations over 5 runs with different random seeds. Further details on hyperparameter selection, model configurations, and reproducibility can be found in Appendix E.",
        "GPT2_formal_text": "_t * u, and f_normalized(u, e, o) are basically the normalized attention vectors of the source, target, and normalized attention scores, respectively. The normalized attention can be thought of as a mix of these attention scores and the tf-idf attention weight. Here's how it works: Formal: This part talks about how we're using the E2E task dataset for our experiments. Formal: A few other datasets have been suggested as possible options. Formal: We're comparing our results to two datasets: the POS dataset from Peters et al. (2017) and the DMV dataset from Sun et al. (2019). We'll also compare our model to a model by Lample et al. (2015). Formal: The POS dataset and DMV datasets have some issues. POS has questions made by people and is kind of messy. DMV is made by people, and it has questions made by machines. Formal: To tackle the issues mentioned in Section 3, we picked the POS dataset from Peters et al. (2017), which has around 5,500 questions. Formal: From the results, we can see that our model works pretty well on the POS dataset. Formal: We're comparing our results to the DMV model from Sun et al. (2019), which is a step up in performance compared to the POS model. Formal: We're also comparing our model to the one by Lample et al. (2015), which is a step up in performance compared to the POS model. Formal: We're comparing our results to the DMV model from Sun et al. (2019). Formal: Formal: We're comparing our results to the model by Lample et al. (2015), which is a step up in performance compared to the POS model. Formal: Formal: The results show that our model performs really well on the POS dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.353.json"
    },
    {
        "casual_text": "In this paper, we introduce the Unsupervised Self-Training framework and use it for sentiment classification. Our framework does two things at once: it assigns sentiment labels to sentences in a code-mixed dataset without any supervision, and it trains a sentiment classification model completely unsupervised. The framework can also be easily adapted to include active learning. We dive deep into how our unsupervised model works and try to figure out whether it actually understands code-switched languages or just recognizes their patterns. Plus, we share some tips on how to improve the performance of the Unsupervised Self-Training algorithm.",
        "formal_text": "In this paper, we propose the Unsupervised Self-Training framework and apply it to the problem of sentiment classification. Our proposed framework performs two tasks simultaneously -firstly, it gives sentiment labels to sentences of a codemixed dataset in an unsupervised manner, and secondly, it trains a sentiment classification model in a purely unsupervised manner. The framework can be extended to incorporate active learning almost seamlessly. We present a rigorous analysis of the learning dynamics of our unsupervised model and try to answer the question -'Does the unsupervised model understand the code-switched languages or does it just recognize its representations? '. We also show methods for optimizing performance of the Unsupervised Self-Training algorithm.",
        "GPT2_formal_text": "= [u1, ..., uk] for k in [1, ..., n] w = W(u, a_k) and b = W(u, b_k) for k in [1, ..., m]. Formal: We're looking at the overall sentiment for the whole document collection, which we call the global sentiment. Formal: Okay, so we take the attention values and combine them in a matrix called S_t, which has dimensions [D_T × D_T]. Then, we use a feed-forward neural network with a softmax function (which is like a helper) to get the final label for the document. Formal: After that, we pick the sentiment with the highest probability for each sentence in the document collection. This probability is calculated using a Dirichlet distribution with parameters β_t and γ_t. Formal: The last score for the document is just the average of the global sentiment scores. Formal: We use the same method as before to pick the sentiment with the highest probability for each sentence in the document collection. Formal: To make this whole process easier, we also include a document summarization model called M_S to help us out. Formal: To make sure the global and document summaries are similar, we use a zero-order MSU algorithm, which was introduced by Sennrich and Haddow back in 2006. Formal: We’re testing our method against a few other methods. Formal: The rest of the paper goes like this: In Section 2, we talk about related work. In Section 3, we introduce our method. Section 4 explains the MSU algorithm. Finally, in Section 5, we talk about the results. Formal: So, S is the final MSU we get after the document collection is created. Formal: Once we’ve got our MSU, the model uses it to decide which sentence to focus on. Formal: Formal: Basically, we’re doing a few things: Formal: 1) We’ll use the global sentiment to create the summary for the whole document collection. Formal: 2) We’ll use the document collection to create a summary for each sentence in the document collection. Formal: 3) And finally, we’ll use the document collection to create a summary for each sentence in the document collection. Formal: Formal: Formal: Basically, we",
        "directory": "calcs",
        "filename": "2021.calcs-1.13.json"
    },
    {
        "casual_text": "Text-to-speech mapping isn’t one-to-one; it’s one-to-many because the same text can sound different depending on pitch, duration, and prosody. This means the mel-spectrograms, which represent the sound, can vary a lot, making their distribution, P(y|x), multimodal. Plus, since mel-spectrograms are continuous, nearby data points are connected to each other. In this part, we’ll first take a closer look at the distribution of P(y|x) in TTS 4 by visualizing it (Section 2.1). Then, we’ll introduce a fresh way to think about the over-smoothing issue in TTS (Section 2.2).",
        "formal_text": "Text-to-speech mapping is a one-to-many mapping since multiple speech sequences can possibly correspond to a text sequence with different pitch, duration and prosody, making the mel-spectrograms distribution P (y|x) multimodal. And due to the continuous nature of the mel-spectrograms, adjacent data points are dependent to each other. In this section, we first empirically characterize the distribution of P (y|x) in TTS 4 through visualization (Section 2. 1), and then provide a novel perspective to study the over-smoothing problem in TTS (Section 2. 2).",
        "GPT2_formal_text": "_type, while attention_type_avg is just the average attention in both directions, and the value of m is either 0 or 1. We calculate the attention matrix A_t(c_t, d_t) using a linear mapping from T to R^d. Formal: The system focuses on the most important words, the ones that really matter for figuring out the intent, and the key phrases that help describe the situation. Formal: We take the last hidden state h_T and pass it through a softmax layer to get the prediction for the next word. Formal: We also use a basic version of the attention mechanism (like the one by Bahdanau et al., 2015) to figure out the attention weights for the current token, o_t, based on the hidden state h_T. This model works well for handling both short and long-range situations. Formal: We use the Attention Separate Representation (ASR) model by Liu et al. (2016) to handle a sequence of text units. Formal: We also use another pre-trained model called XLM-RoBERTa (from Conneau et al., 2019) for generating text sequences. Formal: For the text generation task, we grab the last hidden state h_T from the BiLSTM. Formal: The token representation e_t in the input vector h_T has the same size as the attention matrix A_t(c_t, d_t) from the ASR model. Formal: Finally, the output is an embedding matrix for the generated text. Formal: Figure 1 shows the attention mechanism in action for generating text. Formal: We use a linear mapping from T to R^d to calculate the attention weights for the current token, o_t, based on the hidden state h_T. This method is inspired by the attention mechanism by Bahdanau et al. (2015) that was introduced by Conneau et al. (2019). Formal: For the text generation task, we grab the last hidden state h_T from the BiLSTM. Formal: We also use a basic version of the attention mechanism (like the one by Bahdanau et al., 2015) to figure out the attention weights for the current token, o_t, based on the hidden state h_T. This model works well for handling both short and long-range situations. Formal: We use the Attention Separate",
        "directory": "acl",
        "filename": "2022.acl-long.564.json"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. When we compare the results of SACE base and its baseline (which is like a stripped-down version with no extra features), we find that both systems got 5346 cases right in ALL. However, SACE base got an extra 525 cases right that the baseline missed, and the baseline got 339 cases right that SACE base didn’t. So, SACE base did better in some areas.\n\nBut there are also mistakes. Table 6 shows an example where SACE base made a wrong prediction (like guessing \"country\" incorrectly). The WlC (which helps with understanding context) didn’t do a great job here—it didn’t find useful info to figure out the word and even added some unrelated stuff. Table 6 also shows related sentences (#47 and #19) linked to a specific sentence (#10) that was being disambiguated. In this case, \"church\" was wrongly predicted when WlC was turned off. But when WlC was on, it found similar sentences in the same document and used that useful context to help with understanding.\n\nTable 7 gives examples of synsets (groups of words with similar meanings) that the selective attention layer connected. This layer can spot relationships between words that are closely related in meaning or used together a lot. It does this by looking at the highest attention scores (after ignoring connections to itself).",
        "formal_text": "Error Analysis By comparing the disambiguation results of SACE base and its baseline (all factors removed), it is revealed that both systems have correctly disambiguated 5346 instances in ALL while 525 and 339 instances are only correctly disambiguated by SACE base and its baseline respectively. In other words, SACE base has falsely table 6 shows an example (country) that SACE base falsely predicted. It is shown that the WlC does not manage to retrieve valuable information for disambiguating the word while injecting some irrelevant context. Table 6 gives an example of top related sentences (#47 and #19) of a particular sentence (#10) under disambiguation. Here, church is falsely predicted when WlC is disabled. It shows that WlC has detected similar sentences in the same document and incorporated valuable context for context embedding learning. Table 7 provides some examples regarding synsets that are connected by the selective attention layer, indicating its ability of detecting some syntagmatic sense relations and senses of close meaning. The connection is established by using the largest attention score �, ̂� in a batch after filtering self-connection.",
        "GPT2_formal_text": "= (t_i, t_j)  \n\nBatch regularization, as suggested by Srivastava et al. in 2014, basically means sticking with the same model for each batch of data. Formal: The weight vector for the target word w_t is calculated using the target embedding E_t. Formal: We use a linear kernel to figure out the log probability of the target word. Formal: We tweak the embedding of the word w_t to create a one-dimensional representation by combining its embedding, E_t, with the target embedding E_t_t. Formal: We use a softmax function to predict the probability. Formal: We train the model using this process, which we call backpropagation. Formal: The parameter vector for the training objective we call W_t. Formal: We adjust the prior probability, Φ_t, by taking the value of the latent variable λ_t, multiplied by the gradient of the softmax function on the target embedding E_t_t. Formal: The cross-entropy loss function we use is the Cross Entropy loss, which was introduced by Hinton et al. in 2001. Formal: To keep things clear, we set the log probability of the target word w_t to zero, which is represented by σ_t, as suggested by the EM algorithm. Formal: We train the model using this process, which we call backpropagation. Formal: The parameter vector for the training objective we call W_t. Formal: We use a linear kernel to calculate the log probability of the target word. Formal: We adjust the embedding of the word w_t to create a one-dimensional representation by combining its embedding, E_t_t, with the target embedding E_t_t_t. Formal: We use a softmax function to predict the probability. Formal: We train the model using this process, which we call backpropagation. Formal: The parameter vector for the training objective we call W_t. Formal: We train the model using this process, which we call backpropagation. Formal: The parameter vector for the training objective we call W_t. Formal: We train the model using this process, which we call backpropagation. Formal: The parameter vector for the training objective we call W_",
        "directory": "acl",
        "filename": "2021.acl-long.406.json"
    },
    {
        "casual_text": "The big question is whether fully automatic machine translation (MT) will be so bad that the \"translation\" ends up being totally useless. Like, if even a trained person looking at the machine's output can't figure out what the original text was about, even roughly. This depends on the two languages being translated and how the machine tries to make the translation complete and unique. Only a lot of experiments will tell us if this approach has any potential. Some early work has been done by a team at the Rand Corporation, but so far, the results aren't super clear or convincing.",
        "formal_text": "Whether or not the inaccuracy involved in fully automatic MT will be so great as to make the \"translation\" completely worthless, as would be the case if a trained interpreter of the machine's output were unable to find out even roughly what the passage in question was about, will depend on the two languages involved and on the specific ways by which the completeness and the uniqueness of the translation is achieved. Only extensive experiments will be able to show whether there is any future along this line. Preliminary studies in this direction have been made by a Rand Corporation group, but the results achieved so far do not seem to be decisive.",
        "GPT2_formal_text": "_length, and s = 0.5. We keep the attention parameters for the final layer locked in place. Formal: Let's say W_a and W_c are the learnable parameters for the attention parameters of the sentence, and B_a and B_c are the bias terms. Formal: We also use s_a,s_c as the input for the hidden layers in the LSTM. Formal: For the final hidden layer, we use the logistic regression model (from Kingma and Ba, 2014) to predict the probability P(t_k|c_k). Formal: After the LSTM has processed the sentence, we predict the attention parameters for the next word by adding up the predictions from all the words in the sentence. Formal: We train a linear model using the attention weights. Formal: The hidden layer for the LSTM has an input dimension of n_1, and we use a dropout rate of 0.1 to prevent overfitting. Formal: Finally, we can use the hidden layer to predict the attention weights for the next word. Formal: Training the model with both (a) and (b) gives us the best results, so we go with the simpler option (a). Formal: Training with just (b) doesn't really work well because it messes up the prediction. Formal: Formal: To make things simpler, we use a kernel density estimation with a parameter α to estimate the parameters for the LSTM. Formal: For our experiments, we use the Torch7 Python library (Stolcke, 2002) to train our model. Formal: We set the learning rate to 0.00005 for our experiments. Formal: We also use a linear kernel with a batch size of 8 to predict the attention parameters for the next word. Formal: The final hidden layer for the LSTM has an input dimension of n_2, and we use a dropout rate of 0.1 to prevent overfitting. Formal: We train the model using the original training script (called \"test\") and the script we created by our model. Formal: The model gets trained for 20 epochs using the Adam optimizer with an initial learning rate of 0.00005. Formal: We also use Adam (from Kingma and Ba, 2014) to tweak the model with the learning rate we set. Formal: Formal: Finally, we can",
        "directory": "earlymt",
        "filename": "1952.earlymt-1.4.json"
    },
    {
        "casual_text": "Okay, so we're talking about how an RNN works with inputs up to x to generate a response at y. This setup doesn't assume things stay the same over time (that's what \"relaxes stationarity assumptions\" means), and it also shares some patterns or structure across different time points. Now, about the definition of h...",
        "formal_text": "x, y of an RNN over the input up to x for the response at y. This design relaxes stationarity assumptions while also sharing structure across timepoints. The definitions of h in",
        "GPT2_formal_text": "_m * -m), and a weight vector β for the text. Formal: We've also included a sigmoid function to figure out how much the predictions are influenced by the starting document D. To train a GAN, we use three different weight vectors. Formal: The starting document D, g_i, and the whole dialog history h_i are generated by the teacher model. The student model (S_S) is trained on the predictions from the GAN. The student models predictions for the i-th conversation in the dialog history are averaged across all dialogues. After that, we tweak the model by using the weighted average of the predictions from the teacher model. Formal: In this part, we're checking out how the student model performs based on how well it predicts the dialogues in the history. We've created a simple model with a bunch of parameters, so we're trying to find the best model by minimizing the cross-entropy loss. Formal: The cross-entropy loss for the student model is calculated by subtracting the loss from the predicted dialogues in the history. Formal: The cross-entropy loss for the teacher model is calculated by subtracting the loss from the dialogues. Formal: The dialogues in the history are fed into the GAN, which gives us the student model's predictions. Formal: We evaluate the student model by checking how well it predicts the dialogues. Formal: We calculate the student model's losses by subtracting the losses from the dialogues. Formal: For the teacher model, we do this for two steps: Formal: First, we calculate the cross-entropy loss for the whole history. Formal: Second, we calculate the loss for the dialogues in the history. Formal: We also calculate the cross-entropy loss for the dialogues in the current conversation. Formal: The dialogues in the conversation history are fed into the GAN, which gives us the student model's predictions. Formal: We calculate the loss for the dialogues in the current conversation. Formal: Finally, we calculate the cross-entropy loss for the dialogues in the current conversation. Formal: Formal: We evaluate the student model by checking how well it predicts the dialogues. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.288.json"
    },
    {
        "casual_text": "Not much has been thought about what kind of machine should handle the mechanical part of translation until now. People didn’t really experiment with using computers or similar machines because they hadn’t fully explored all the options yet. But it looks like we’re at a point where we can start experimenting. The big question is whether regular computers or special translation machines would work better in the long term, considering speed, accuracy, and cost. The main tasks the machine would need to do are comparing and identifying things, moving and transferring data, and making selections—both with and without conditions. Math-specific operations aren’t really part of it.",
        "formal_text": "Little thought has been given so far to the problem of the type of machine that should do the mechanical part of a translation; so long as the various alternative possibilities had not been explored to a sufficient degree, experimentation in the uses of computer-like machines was not warranted. It seems, however, that the stage has been reached where experimentation could be started. The main problem is whether general-purpose computers or special translation machines would, in the long run, be more satisfactory with regard to the speed-accuracy cost ratio. The major operations to be carried out by the machine seem to be comparison and identification, shifting and transferring, unconditional and conditional selection, but not specifically arithmetical operations.",
        "GPT2_formal_text": "is the initial representation that gets fed into the attention layer. After that, the final output from the attention layer is what we call attention_output_i. Formal: We use this updated attention layer to figure out the next word in a sentence. Formal: For the NMT model, we start by predicting the next word in the input sequence. Then, we calculate the hidden representation for that word by combining the hidden representation of the previous word with the hidden representation of the current word. We call this combined hidden representation, h_t_i. Formal: After we've updated the attention layer, the model figures out the hidden representation for the current word, h_t_i. Formal: So, the updated attention layer represents the current word's hidden representation. Formal: We calculate the hidden representation for the current word using this updated attention layer. Formal: We also consider the probabilities of the words around the current word and the current word itself. Formal: Finally, the model uses the combined hidden representation of the current word and the previous word, h_t_i, to guess the next word. Formal: The updated attention layer is what we use to predict the next word. Formal: To make things even better, we can use attention mechanisms to help the model learn better representations. Formal: In Section 5.3, we came up with a new way to use the updated attention layer to handle the context of the current word. Formal: This new method not only boosts the performance of the model but also lets it focus on the important parts of the current word, making it better at handling the important details. Formal: Formal: We tested our model on a couple of different NMT tasks. Formal: For the sentence segmentation task, we trained the model using a sentence segmentation dataset that's publicly available. Formal: For the English to German translation task, we used the Europarl dataset to train the model. Formal: Our model scored an F1-score of 84.2% on the sentence segmentation task and 81.9% on the English to German task. Formal: Formal: Formal: We also did a human evaluation on the Europarl dataset to check how well our model performs. Formal: Formal: Formal: The results show that our model works really well for both sentence segmentation and English to German translation. Formal: Formal: Formal: Formal: Formal:",
        "directory": "earlymt",
        "filename": "1952.earlymt-1.4.json"
    },
    {
        "casual_text": "The text is placed in a simple, essential context and sent through all the machine translation engines. For each chunk, C_i, the possible translated versions, C_1_i to C_N_i, are gathered. Then, the best translation, C_best_i, is picked using a majority vote, a language model trained on a big target language dataset, and a confidence score from each MT engine. Finally, the output sentence, S, is put together by piecing all the best chunks, C_best_i, into their right spots in the target language. You can see a diagram of this whole process in Figure 1.",
        "formal_text": "is embedded in a minimal necessary context and translated by all MT engines. For each chunk C i, the translated output candidates C 1 i − C N i are retrieved and a winner C best i is calculated based on majority voting, a language model trained on a large target language corpus and a confidence score assigned to each MT engine. In a final step, the output sentence S is composed by assembling all C best i (1 < i < M ) in their correct target position. A flow chart representing the entire MEMT architecture can be found in Figure 1.",
        "GPT2_formal_text": "= {(d_i, d_j)} | a_i = [a_i1, ..., a_j1], y_i = [y_i1, ..., y_iT], σ_ij = [σ_ij1, ..., σ_ijT], ..., σ_ijT = [σ_ij1, ..., σ_ijT]. Formal: This example is based on the Distinct metric. Basically, the tf-idf version of the formula is: Formal: The first case is when y_i1 = y_i1, ..., y_iT, the second case is when y_i1 = y_i1, ..., y_ij, and the third case is when y_i1 = y_i1, ..., y_ijT. Formal: When we're looking at the k-th part of the formula, we calculate σ_ij using the math part. Formal: We then use the tf-idf formula to figure out σ_ij, using the math part. Formal: If x_i = y_i1, ..., y_iT, then we just go with the word y_i1. Formal: So, σ_ij = [σ_ij1, ..., σ_ijT]. Formal: If x_i = y_i1, ..., y_iT, then we use the word y_i1. Formal: The same thing applies if x_i = y_i1, ..., y_iT. Formal: Since the distance between the embedding and the input is always between -∞ and +∞, we can just set µ to 1. Formal: µ = 1. Formal: Formal: Then, we take the tf-idf formula and multiply it by µ. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "amta",
        "filename": "2006.amta-papers.13.json"
    },
    {
        "casual_text": "To really understand how languages change over time and across different places, we need to see them go through actual real-world stuff like splitting into new languages (speciation), disappearing (death), and moving around (migration). These are super important for making the simulation feel real before we dive into how specific features of languages change. Let’s break down what these phylogenetic things mean and how we check they’re working right, and then we’ll move on to talking about features.",
        "formal_text": "In order to accurately model language evolution in time and space, it is essential for languages to undergo real-world changes to their phylogeny: namely, speciation, death, and migration. These phylogenetic aspects of the simulation are critical to consider to establish a realistic basis before examining feature dynamics. We first discuss their details and validations, and then delve into features.",
        "GPT2_formal_text": "is set to 0. Formal: In the prediction layer, the output vector z_i is made up of three parts: \n- The first part (z_i1) tells us how important the i-th word in the document is, and it's calculated as: Formal: The loss function we use for the layer after the token prediction step is called L_o(z_i1, z_i2, ..., z_iT_i). Formal: Once we get the predictions for the tokens in the document, we calculate the document representation for them using the embedding matrix M_i, which we get by applying the cross-attention mechanism (check out Fig. 1a for more details). Formal: After doing the token prediction, we use this representation to tweak the LSTM hidden state. Formal: We use a linear layer to combine the input word embedding with the document representation z_i. After that, we apply a ReLU activation to the result of the cross-attention to get the token representation. Formal: Finally, the token representations z_i are then passed through another linear layer to get the document representation z_i. Formal: To make the embedding vector W_i, we use a linear layer to combine the input word embedding and the document representation z_i. Formal: We do a linear transformation in each dimension of the vector to create the embedding vector. Formal: Finally, we apply ReLU to get the document representation z_i. Formal: We use cross-attention to get the attention vector z_i, which then goes through a linear layer to get the document representation z_i. Formal: In this paper, we use a bidirectional LSTM to predict the next token in the document. Formal: We also use a linear layer to combine the input word embedding and the document representation z_i. After that, we apply a ReLU activation to the result of the cross-attention. Formal: Finally, the token representations z_i are then passed through another linear layer to get the document representation z_i. Formal: To make sure the token representations z_i are similar to the document representations z_i, we do a linear transformation in each dimension of the vector to get the token representation z_i. Formal: Finally, we apply ReLU to get the document representation z_i. Formal:",
        "directory": "coling",
        "filename": "2020.coling-main.68.json"
    },
    {
        "casual_text": "We use this double-grounding idea for sarcasm to make our neural network model better at detecting sarcasm, which helps us answer our first two research questions. We look at the speaker's mood when they make a sarcastic comment by checking their recent tweets for mood clues. We also consider the context by looking at the tweet that the sarcastic comment is responding to. For our third research question, we came up with a new way to get feedback on our annotations by involving the people who wrote the tweets. This helps us improve the model by feeding new examples back into it. Section 3 talks about the features we use in the model, like where they come from. Section 4 explains how we collected and labeled our data. Section 5 describes the neural network model itself, and sections 6 & 7 cover our experiments and how we analyzed the results. Finally, section 8 wraps things up with some final thoughts.",
        "formal_text": "We build on this double-grounding for sarcasm to improve detection in a neural network model of sarcasm and thereby address our first two research questions. We model the speaker at the time of utterance production using mood indicators derived from the most recent prior tweets, and model context using features derived from the proximate cause of the new utterance, the tweet to which an utterance is a response. For our third research question, we present a novel feedback-based annotation scheme that engages authors of training/test tweets in a process of explicit annotation, feeding new examples back into the model. Section 3 outlines the kind and source of features exploited in the model. Section 4 outlines our methods of data collection and annotation. Section 5 presents the neural network model, while section 6 & 7 present our experimental set-up and analysis of results. Finally, section 8 offers some closing remarks.",
        "GPT2_formal_text": ", which are basically the categories in the review documents. In this paper, we’re focusing on how the type of review relates to the sentiment of the document. We’ll explain this more thoroughly in the next section. Formal: So, for each review r, we get: Formal: The tf-idf representation gets fed into an attention-based classifier. This classifier figures out the probability P_t(r|s), which tells us how likely the review r is to have the sentiment s. Then, it predicts the sentiment of the document d_t using a weighted average of these probabilities. Formal: We add another linear function called β to tweak the weighted average of the probabilities. Formal: Basically, this formula adds up the probabilities for each review r, then adds a linear function to adjust them. Formal: This formula is optimized using a Markov chain Monte-Carlo model, and we’ll call it D_c for short. Formal: In our case, we’re just using a vector-based model because the reviews and documents don’t have much in common, so the model doesn’t need a ton of parameters to work well. Formal: To make things clearer, we’ll use a simpler way of writing this, instead of the full equation. Formal: Here, β is just a linear function, and α is another linear function that’s similar to β. Formal: The final update of the weights is made by looking at the first k reviews from the set D_c. Formal: After that, we set the weights for the classifier to a specific value to make sure the output gets a fair shot. Formal: We then keep updating the weights, which helps us calculate the weighted average of the weights. Formal: Finally, we adjust the weights to create the final attention matrix for the classifier. Formal: Here, we also use a linear function to update the weights, and α is another linear function that’s similar to α. Formal: Lastly, we set the weights for the classifier to a specific value to make sure the output gets a fair shot. Formal: Formal: Finally, we adjust the weights to create the final attention matrix for the classifier. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1050.json"
    },
    {
        "casual_text": "Our tests show that throwing in this extra loss term into the objective function doesn't just boost performance but also helps the model get better faster. We started with a learning rate of η = 0.05 and reset the squared gradient sum every five epochs.",
        "formal_text": "Our experiments show that adding this loss term to the objective function not only increases performance but also speeds up convergence5 We set the initial learning rate η = 0. 05 and reset the squared gradient sum to zero every five epochs.",
        "GPT2_formal_text": "ed = min(len(t), dtype) src = TARGET(t) * src_attention_masked. for l_i from 1 to l_m, c_i from 1 to c_m, a_i from 1 to a_m, and c_m from 1 to c_m-1. Next, we calculate the attention weights for each token in the input sequence, using the cross-attention mechanism (like the one in Bahdanau et al., 2015). This includes the attention for the token in the source sequence (t_s) and its attention for the token in the target sequence (t_d). Formal: Basically, the attention weight is a mix of the cross-attention weights. Formal: We use a 3-layer CNN to turn the input sequence into a vector representation. Formal: After applying the attention weight, we merge the vectors to get the final representation. Formal: We use max-pooling to get the final vector. Formal: After applying the attention weight, we get the final representation. Formal: Lastly, we sample the representations for the source and target tokens. Formal: We sample the representations for the source and target tokens, using the cross-attention mechanism, and then we use max-pooling to get the final representation. Formal: This is the final representation after sampling the representations for the source and target tokens. Formal: After sampling the vectors, we use max-pooling to get the final representation. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final representations. Formal: Finally, we sample the final",
        "directory": "D14",
        "filename": "D14-1070.json"
    },
    {
        "casual_text": "Lastly, the decoder from Section 3.2 is used to create hidden states H_a and act tokens based on that.",
        "formal_text": "Finally, the decoder of Section 3. 2 is used to generate hidden states H a and act tokens accordingly.",
        "GPT2_formal_text": ". In this case, only the first word of the input text gets passed into the phrase generator, which then creates a new chunk of text using this input. Formal: Alright, so we're breaking this down into three parts:\n\n1. **Algorithm 1: Selectional Restriction (RS)**  \n2. **Algorithm 2: Preprocessing**  \n3. **Algorithm 3: Assignment (A)**  \n\nThe main idea here is to create a phrase table that works for the model. We start with the original phrase and pick a few possible phrases to replace it. Then, we run each possible phrase through our phrase generator to see how it fits with the current phrase. Finally, we pick the best phrase to create the new one. Formal: To avoid repeating stuff, we use a fixed phrase length, λ = 1, to decide the size of the phrase table. Then, we start by using a basic model on the input text. We pick the best phrase using our phrase generator, and when we have the new phrase, we update the phrase table to include it. Formal: We also use a length penalty to make sure we're not repeating the same phrase too many times. To make sure we cover all the important parts of the input text, we use a filtering method. For example, we figure out how important each word is by looking at its confidence score and how often it shows up in the dataset. Formal: We also use the GEDA model to look for non-continuous phrases and avoid repeating the same phrase too many times. Formal: We keep doing this until we have a phrase table that matches the input text. Formal: Finally, we send the phrase text through the phrase generator and use it to generate the new text. Formal: We’ve got four main parts here:  \n1. **Input text:** We start with the original phrase and pick some phrases to replace it.  \n2. **Grammar:** We break down the original phrase into a simpler form by breaking down the word forms and lemmas.  \n3. **Algorithm:** We use our pre-trained phrase generator to choose the best phrase to create the new one.  \n4. **Dependency parsing:** We use a constituency parser to connect each word to its main part.  \n\nNow, the second part is about how we handle the",
        "directory": "acl",
        "filename": "2020.acl-main.638.json"
    },
    {
        "casual_text": "Alright, so let's say we have a type τ, a limit n, and a set A of sources we've already covered. We'll call PossL(τ, A, n) the group of lexical types λ where A is a subset of A(λ, τ), and we can get from λ to τ using APP operations from the sources in A, plus no more than n extra APP operations. In other words, the difference between A(λ, τ) and A is no bigger than n.",
        "formal_text": "Finally, given a type τ, an upper bound n, and a set A of already-covered sources, we let PossL(τ, A, n) be the set of lexical types λ such that A ⊆ A (λ, τ ) and we can reach τ from λ with APP operations for the sources in A and at most n additional APP operations, i. e. |A(λ, τ ) − A| ≤ n.",
        "GPT2_formal_text": "is the max-pooling parameter that helps minimize the cross-entropy loss. Formal: The cross-entropy loss for LAF is calculated as: Formal: The cross-entropy loss for LAF is calculated as: Formal: We use a two-layer neural network to figure out how to represent each document. Formal: This document representation is a mix of two things: the text and the image. The text representation is like a string of characters, and the image representation is a sequence of pixels, which are basically vectors with a fixed length. We refer to this \"picture\" as the representation of the document. Formal: In our experiments, we use a multilingual BERT model for image captioning. The model is built using the cuDNN library (developed by LeCun et al., 2015). Formal: The whole thing about the image captioning dataset is laid out in Table 1. Formal: We set the hyperparameters for the model to get the best results. Formal: We train the model using our own training data and then use it to train the image captioning model. Formal: We split the dataset into training, development, and test sets to get the best results. Formal: We use multi-task learning (MTL) for generating captions for news images. We train two separate models to mimic how humans caption these images. Formal: The model is trained using a joint text and image classification task. Formal: We pick the model that performs best on the validation set for the captioning task. Formal: We use a multilingual BERT model for image captioning. Formal: The model is trained using our own training data and then uses it to train the image captioning model. Formal: We use multi-task learning (MTL) for generating captions for news images. We train two separate models to mimic how humans caption these images. Formal: We pick the model that performs best on the validation set for the captioning task. Formal: We use a multilingual BERT model for image captioning. Formal: Formal: We train two separate models to mimic how humans caption these images. Formal: The model is trained using a joint text and image classification task. Formal: We also use a multi-task learning (MTL) model for generating captions for news images. Formal: We train two separate models to mimic how humans caption these images. Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "Unlike those models, we use some categorical attributes and optimize by minimizing cross-entropy.",
        "formal_text": "Unlike these models, however, we utilize (some) categorical attributes and optimize using cross-entropy.",
        "GPT2_formal_text": "= {s_i, t_i, u_i}  \n\nThe vectors for an entity i are like a list of numbers that show how important (or important) the entity is. Formal: So, q_i represents the value of a normalization factor that helps balance how much each prediction matters for the whole system. Formal: You can find more info about the parameters in the original papers. Formal: The dataset for this experiment can be found here: https://github.com/wiegandlab/datasets/blob/master/inference-based-answer-selection.txt Formal: Formal: Here's a link to the model we used for this experiment: https://github.com/wiegandlab/INFLECT Formal: Formal: For each question q_i, we picked a set of three entities: e_i, r_i, and t_i. To make sure we're comparing well with other methods (like TriviaQA or SQuAD), we averaged the scores of all possible entities in our training data, giving each a weight of 1. Formal: Here's the link for the training data: https://github.com/wiegandlab/datasets Formal: Formal: For each pair of entities e_i and r_i in the training set, we calculated the similarity between the question and entity embeddings using BERT. Formal: Formal: The last row of equation (6) just shows how much each entity contributes to the question. Formal: The model can be found here: https://github.com/wiegandlab/INFLECT Formal: Formal: The dataset for this experiment can be found here: https://github.com/wiegandlab/INFLECT Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1099.json"
    },
    {
        "casual_text": "We’re sharing some specifics about how we ran our experiments. All the pre-trained language models (PLMs) we used in our paper are based on HuggingFace Transformers 10. For fine-tuning on the probing task, we ran the experiments with batch sizes of either 8 or 16, α set to 3, 5, or 10, a max sequence length of 128, and a learning rate of 1e-5, training for 10 epochs. We kept the same hyperparameters for each model, even when using different training objectives. For fine-tuning on the sentiment analysis task, we only tweaked the parameters of the multi-layer perceptron (MLP) classifiers that sit on top of the PLM's contextualized representations. For this, we used a learning rate of 2e-5, 3e-5, or 4e-5, a batch size of 32, a max sequence length of 128, and trained for 200 epochs. Oh, and we also included some examples of the experimental setup for checking how important components affect things in Table 8.",
        "formal_text": "We introduce details about the implementation of our experiments. The implementations of all the PLMs in our paper are based on the HuggingFace Transformers 10. During fine-tuning for the probing task, the experiments are run with batch sizes in {8, 16}, α in {3, 5, 10}, a max sequence length of 128, and a learning rate of 1e-5 for 10 epochs. For each model, we use the same hyper-parameters when applying different training objectives. During fine-tuning for the sentiment analysis task, we only update the parameters of the multi-layer perceptron (MLP) classifiers on top of PLM's contextualized representation. We set the learning rate in {2e-5, 3e-5, 4e-5}, batch size of 32, max sequence length of 128 and train for 200 epochs. Additionally, we present examples of the experimental setup for evaluating the influence of important components in Table 8.",
        "GPT2_formal_text": "ed_2d_t * [src+len; src]) = [t src+len; t src+len; ...; t src+len; t] Formal: A multi-head attention mechanism is basically a way to figure out the attention distribution for a sentence by using the output from the most recent head in the sequence. Formal: The attention mechanism, also known as multimodal attention, is a type of attention mechanism that works with sequences of things like images or text. It takes into account both the visual and text parts of the input, so it can focus on the important parts and still make the overall representation as accurate as possible. Formal: With the multi-head attention mechanism, the model looks at both the visual and text parts of the input. Formal: The multi-head attention mechanism, also known as multimodal attention, is a type of attention mechanism that works with sequences of things like images or text. It takes into account both the visual and text parts of the input, so it can focus on the important parts and still make the overall representation as accurate as possible. Formal: The multi-head attention mechanism, also known as multimodal attention, is a type of attention mechanism that works with sequences of things like images or text. It takes into account both the visual and text parts of the input, so it can focus on the important parts and still make the overall representation as accurate as possible. Formal: The multi-head attention mechanism, also known as multimodal attention, is a type of attention mechanism that works with sequences of things like images or text. It takes into account both the visual and text parts of the input, so it can focus on the important parts and still make the overall representation as accurate as possible. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.543.json"
    },
    {
        "casual_text": "We're working with the French SPMRL dataset (Seddah et al., 2013), which is newer and bigger than the datasets people used for French parsing before (Crabbé and Candito, 2008). This dataset includes the full French Treebank mentioned in (Abeillé et al., 2003) and will probably become the go-to dataset for French parsing in the next few years. We're using it as it is, with two setups: one where we have gold standard tags and another where we use tags predicted by a tagger that's 97.35% accurate (Seddah et al., 2013). The French data has head annotations based on rules from (Arun and Keller, 2005), and compound words are always left-headed.\n\nFor English, we're using the Penn Treebank with the usual split: sections 02-21 for training, section 22 for development, and section 23 for testing. In the predicted scenario, we use the MELT tagger (Denis and Sagot, 2012), which has an accuracy of 97.1%. The head annotations were figured out by matching the phrase structure treebank with its dependency conversion, as described by (de Marneffe et al., 2006).\n\nWe're running the experiments using a C++ implementation of the algorithm we talked about. The scores for the Berkeley parser (Petrov et al., 2006) are based on the runs mentioned by (Seddah et al., 2013). We measure the F-score using the classic evalb, and we measure the times on the same machine (MacOSX 2.4Ghz). Note that we don't include input/output times for both parsers.",
        "formal_text": "The experiments use the French SPMRL dataset (Seddah et al. , 2013) which is newer and larger than datasets previously used for parsing French (Crabbé and Candito, 2008). It instanciates the full French Treebank described in (Abeillé et al. , 2003) and will surely become the new standard data set for parsing French in the next few years. We use this data set as is, with two scenarios: one with gold standard tags and the second with tags predicted by a 97. 35% accurate tagger (Seddah et al. , 2013). The French data is head annotated with head rules provided by (Arun and Keller, 2005). Additionally, compound word structures are systematically left headed. For English, we use the Penn Treebank with standard split: section 02-21 for training, section 22 for development and section 23 for test. The predicted scenario uses the MELT tagger (Denis and Sagot, 2012) with an accurracy of 97. 1%. The head annotations have been inferred by aligning the phrase structure treebank with its dependency conversion described by (de Marneffe et al. , 2006). We use a C++ implementation of the algorithm described above for running the experiments. Scores reported for the Berkeley parser (Petrov et al. , 2006) use the runs described by (Seddah et al. , 2013). F-score is measured with the classical evalb and times are measured on the same machine (MacOSX 2. 4Ghz) and do not take into account input/output times for both parsers.",
        "GPT2_formal_text": "_t = (int)label_t if d_t > 1, and y_t = y_t if d_t < 1. Formal: Here, the supertag is the final representation that the model spits out. The tag type is a bit trickier because the model needs to figure out the exact meaning of the word, which is a bit more complicated. Formal: For instance, the word 'eat' is a meat-related food and refers to a meal. It's also a noun that can mean either a meal or a meal substitute. The task here is to predict the label for the word 'eat'. So, the word has two different meanings: either as a meal or a meal substitute. Formal: The model's guesses for each word are made using a mix of: Formal: For each word, the model looks at the words around it, their dependency relations, and the words around their heads, which are part of the embeddings. Formal: The model picks the tag with the highest probability based on all that. Formal: Formal: The output from the model's feed-forward network is then fed into the next layer of the model, where the hidden state comes into play. Formal: Here, y_t is the final representation that the model spits out. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C14",
        "filename": "C14-1052.json"
    },
    {
        "casual_text": "For our experiment, we're starting with a standard set of features commonly used for AKE. Basically, when we have a potential keyphrase, we look at:",
        "formal_text": "The baseline feature set for our experiment is a set of well-accepted features for AKE, i. e. , given a candidate KP, we consider:",
        "GPT2_formal_text": "_type = {1.0, 1.0, 1.0, 1.0, 1.0}  \nInput: (a) the time you want to check, (b) the time you want to wait for the response, (c) the value of r, and (d) the model. Formal: The time it takes to run the search. Formal: The time it takes to run the whole process. Formal: The total time it takes to run all the search steps. Formal: The total time it takes to run the whole process. Formal: The total time it takes to run the whole process. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity. Formal: The model's time complexity",
        "directory": "C16",
        "filename": "C16-1077.json"
    },
    {
        "casual_text": "Alright, so like how BERT does it, we start by adding a [cls] token at the beginning and a [sep] token at the end of the source sequence. After that, we throw in a bunch of [pad] tokens at the end to make the whole thing reach the maximum length we set, like 256, for example. This way, we ensure the source is at least as long as the output. But, for tasks like text summarization where we know the source is longer than the target, we skip adding those [pad] tokens when creating the input.",
        "formal_text": "Input Representation Following the setup of BERT, we first append a [cls] and a [sep] token on both sides of the source sequence. Then we attach a number of [pad] tokens at the end of source sequence to make its length equal to the predefined maximum size (e. g. , 256). Thus we can make sure the source length is longer than or equal to the output length. As a special case, for tasks like text summarization where the source is known to be longer than the target, we do not attach the [pad] tokens when constructing the input.",
        "GPT2_formal_text": ") = {l_i} | i = 1, 2, ..., l_i+1, l_i+n | l_i + 1. Formal: If the model is trained to guess a number between 0 and 1, the only error it can make is to get stuck in a loop. Formal: The value of the input set y_i can be calculated by: Formal: For each word w_i in the input set and the correct phrase phrase p_i: Formal: For every word w_i in the input set, and the correct phrase p_i: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.18.json"
    },
    {
        "casual_text": "We think that sentences in a related work section are created through two different processes: one that provides general background info and another that talks about specific contributions from authors. A big insight we had is that these processes can be neatly organized into a topic tree: general content goes in the nodes inside the tree, while the leaf nodes hold the specific details. In our method, these two processes work separately and then come together to build the final summary.",
        "formal_text": "We posit that sentences within a related work section come about by means of two separate processes -a process that gives general background information and another that describes specific author contributions. A key realization in our work is that these processes are easily mapped to the topic tree topologically: general content is described in tree-internal nodes, whereas leaf nodes contribute detailed specifics. In our approach, these two processes are independent, and combined to construct the final summary.",
        "GPT2_formal_text": "_t is the hidden representation for the prediction mask. Formal: At each step, we figure out the output representation for the last word in the sentence by taking the average of the hidden representations for the previous and next words. Formal: We also add the context vector C_t and the next word's embedding embedding embedding E_t to the input. Formal: To predict the next word, we use a fully-connected layer with a ReLU activation. This layer gets the probability of the next word's representation. To make sure we get the expected probability P(t = t_j|c_t = c_t_j), we calculate the cross-entropy between the current hidden state h_t and the predicted hidden representation h_t_j. Formal: If the current word embedding embedding is d_i, the cross-entropy is calculated like this: Formal: The updated hidden state h_t is then fed into a fully-connected layer with ReLU activation. Formal: Next, we predict the next word using a fully-connected layer with ReLU activation. The predicted hidden representation is then fed into another fully-connected layer with ReLU activation, and the result is the probability of the word's representation. Formal: We add the context vector C_t and the next word's embedding embedding embedding E_t to the input. This gives us the probability of the next word's representation. Formal: Finally, we use the predicted hidden representation h_t to update the current hidden state h_t. Formal: Formal: The hidden representation h_t is then fed into a fully-connected layer with ReLU activation. The result is the probability of the word's representation. Formal: We add the context vector C_t and the next word's embedding embedding embedding E_t to the input. This gives us the probability of the next word's representation. Formal: Finally, we use the predicted hidden representation h_t to update the current hidden state h_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C10",
        "filename": "C10-2049.json"
    },
    {
        "casual_text": "So, the total cost for dividing the text into K segments is made up of the costs for each segment. The cost for each segment is a combination of two things, adjusted by a parameter called 7:\n\n1. **Length Information**: This is how far the segment's length is from the average segment length. Think of it like the mean (bt) and standard deviation (a) of segment lengths, which can be figured out from the text.\n\n2. **Similarity Between Sentences**: This measures how similar the sentences in a segment are to each other. The more words they share, the more similar they are. The numerator here is the total number of shared words in the segment.\n\nIf the parameter r is 2, then this similarity measure is like looking at the \"density\" of the segment—how much \"information\" (shared words) is packed into it. When r is 2, it’s called \"generalized density,\" which helps control how much the size of the segment affects its importance. High similarity within a segment (lots of shared words) means a higher value for this part.",
        "formal_text": "Hence the sum of the costs of the K segments constitutes the total segmentation cost; the cost of each segment is the sum of the following two terms (with their relative importance weighted by the parameter 7): I. The term (tk tk-? -/1)2 2. 0-2 corresponds to the length information measured as the deviation from the average segment length. In this sense, bt and a can be considered as the mean and standard deviation of segment length measured either on the basis of words or on the basis of sentences appearing in the document's segments and can be estimated from training data. E: k =t k i+iEtt = k ek 1+1D 8, t 2. The term (tk -tk-corresponds to (word) similarity between sentences. The numerator of this term is the total number of ones in the D submatrix corresponding to the k-th segment. In the case where the parameter r is equal to 2, (tk -tk_ O r correspond to the area of submatrix and the above fraction corresponds to \"segment density\". A \"generalized density\" is obtained when r 2 and enables us to control the degree of influence of the surface with regard to the \"information\" (i. e the number of ones) included in it. Strong intra-segment similarity (as measured by the number of words which are common between sentences belonging to the segment) is indicated by large values of",
        "GPT2_formal_text": "[ ] = { u1, u2, ..., uM } for each i in the set { 1, 2, ..., M }. Formal: The final hidden state for the entity e_i is calculated using the formula. Formal: In simpler terms, the representation z_j of entity e_i is created by taking the log probability of a specific label y_j given the entity e_i. Formal: To avoid the issue of needing to pull out the actual entity's info, we treat entity detection as a sequence tagging task. This means we don't need to calculate the embeddings directly. Instead, we do a flat normalization of the embeddings and feed them into an LSTM model. This setup is shown in Figure 1. Formal: This approach is similar to the usual hierarchical attention model (like the one by Kipf et al., 2017) because it relies on entity detection to figure out the relationships between entities. But unlike those methods, our model doesn't need to deal with the huge number of labels. Formal: As for the multi-head attention mechanism, we break it down into three smaller layers in each layer, and then we use two softmaxes to process the hidden state of each head. Formal: Formal: The model that focuses on the entity's context is called the contextual entity detector. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E03",
        "filename": "E03-1058.json"
    },
    {
        "casual_text": "All the Cascade models scored better than the Isolated model in terms of F-score. The highest F-score for the Cascade model was 67.29%, which we got by using 18 features in experiment (iv). This shows that POS information really helps with sentence boundary detection.",
        "formal_text": "All Cascade models have higher F-score than the Isolated model. The best F-score of the Cascade model is 67. 29% when we used 18 features in the experiment (iv). The experiment affirms that POS information is helpful in sentence boundary detection.",
        "GPT2_formal_text": "ed = {0, 1, 2, 3, 4, 5} d = {0, 1, 2, 3, 4} m = {0, 1, 2, 3, 4} m_i = {0, 1, 2, 3, 4} h = {0, 1, 2, 3, 4} x = {0, 1, 2, 3, 4} y = {0, 1, 2, 3, 4} z = {0, 1, 2, 3, 4} m_i = {0, 1, 2, 3, 4} k = {0, 1, 2, 3, 4} k_i = {0, 1, 2, 3, 4} h_j = {0, 1, 2, 3, 4} h_j = {0, 1, 2, 3, 4} p = {0, 1, 2, 3, 4} q = {0, 1, 2, 3, 4} q = {0, 1, 2, 3, 4} s = {0, 1, 2, 3, 4} s = {0, 1, 2, 3, 4} t = {0, 1, 2, 3, 4} t = {0, 1, 2, 3, 4} w = {0, 1, 2, 3, 4} w = {0, 1, 2, 3, 4} h = {0, 1, 2, 3, 4} y = {0, 1, 2, 3, 4} z = {0, 1, 2, 3, 4} m_i = {0, 1, 2, 3, 4} m_i = {0, 1, 2, 3, 4} h_j = {0, 1, 2, 3, 4} h_j = {0, 1, 2, 3, 4} q = {0, 1, 2, 3, 4} q = {0, 1, 2, 3, 4} z = {0, 1, 2, 3, 4} m_i = {0, 1, 2, 3, 4} m_i = {0, 1, 2, 3, 4} h_j = {0, 1, 2, 3, 4} h_j = {0, 1, 2, 3, 4} w_j = {0, 1, 2, 3, 4} w",
        "directory": "C16",
        "filename": "C16-1031.json"
    },
    {
        "casual_text": "We’ve come up with a method to figure out which entities in a text are important (focus) and which are just background info (ground), using something called class association rules. We’re specifically looking at how this can help identify pathogens that are the main focus of a study. Focus pathogens usually show up in the title and results sections of abstracts, where the main findings are highlighted. Our little experiment suggests that paying attention to how things are written in the text can give us useful hints to spot these focus pathogens. Since we only had a small amount of data to work with, this is more of a proof of concept. We’re working on gathering a bigger dataset, which will let us test more detailed rules and explore other methods, like centrality and transformer-based approaches.",
        "formal_text": "We have proposed an approach to the problem of detecting focus versus ground entities using class association rules over entity mentions in discourse segments, specifically examining its use for pathogen characterisation. Focus pathogens tend to appear in the title and results segments of abstracts, where the key findings of research are highlighted. Our case study suggests that discourse information provides valuable cues to identify focus pathogens. Given the small-scale data we have available, this work is only indicative of the promise of the approach. We are developing a larger data set, which will support comprehensive exploration of more refined rules. This data set would also support the exploration of additional existing methods, such as centrality and transformer based methods.",
        "GPT2_formal_text": "= [maj, c]  \n\nHere, maj stands for the average number of sentences in the document d, and c is the number of different document topics. Formal: In this part, we'll talk about the word segmentation model that we used in our experiments. Formal: To check how well the segmentation was doing, we used two types of features: (1) the NER-based features, which are like unigrams and bigrams; and (2) the POS-based features, which are all the punctuation characters in the document. Formal: The top-1 unigram features include stuff like the word's first word (w_1) and the first word of the next word (w_2). The top-k unigrams include stuff like the word's last word (w_l) and the last word of the previous word (w_m). The top-k bigrams include stuff like the word's first word (w_1) and the first word of the next word (w_2). Formal: These features were trained on some test data, and we used a CRF model to get the results. Formal: We used the latest versions of the Moses and BPE tools, just like they were released by Google. Formal: After training the segmentation models, we used them to spot potential parts of the text that could help us with the NER task. For each of these parts, we created a segmentation dictionary and used a distance-based scoring system to sort them out. Formal: The results for this are shown in Table 3. We noticed that the unigram feature model didn't work as well as the bigram model when we didn't use the POS feature model. Formal: Here's Figure 1 showing the results of two versions of the NER feature model. We picked the one that performed best based on the NER scores in Table 3. Formal: We compared the NER features used in Moses and BPE to the ones used in the TTS model and the language model. Formal: The results for the unigram feature model are in Table 3. Formal: Here's Figure 2, showing the results of the bigram feature model. Formal: Formal: The results for the bigram feature model are in Table 3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "alta",
        "filename": "2021.alta-1.19.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nGen(h x, z, 0, max, λ, t):  \n1. Start by setting ←− 0 and r ←− 0.  \n2. Calculate the gradient: g ←− ∇ hx L (h x, z, C (h x ))  \n3. Keep going until ≤ max:",
        "formal_text": "Gen(h x, z, 0, max, λ, t): 1: Initialization: ←− 0, r ←− 0. 2: Compute gradient: g ←− ∇ hx L (h x, z, C (h x )) 3: while ≤ max do 4:",
        "GPT2_formal_text": ", and the final output gets a special label called R. Formal: So, x(x) is the input matrix, and y(y) is the predicted output from a BiLSTM (a fancy neural network thing) trained on the data we've got. We'll call this whole setup the \"list representation\" for a sentence. Formal: Basically, a sentence can be broken down into chunks called tokens. The info in a sentence's token embedding, y(t), is used to figure out the attention vector, R. Formal: In this paper, we're mainly focusing on sentence classification, which is a pretty standard task. But there's also another, more specific, goal we're working on, which is to predict a question. Formal: Question prediction is a bit trickier, but we'll get to that later. Formal: To make things easier, we use something called attention networks. Formal: Here, we assume we have a question-answer pair in our training set, like y = [y1, ..., yq], and the training data is in R d. In this setup, we use an attention-based recurrent neural network (RNN) to figure out the attention vector for each token, like y(t). Formal: For each token, we have the attention vector x(t) for the question prediction task. Formal: We can do this using the same RNN model as before. Formal: To find the answer for a given question, we first use the attention mechanism on the question token embedding. Then, we combine the attention vectors from the answer token embeddings, y(t)a and y(t)a(q), to get the final output y(t). Formal: Formal: Using this combined result, we get the final attention vector for the answer token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.200.json"
    },
    {
        "casual_text": "For our experiments, we used the Wikipedia Edit Category Corpus (WPEC), which is freely available and was put together in earlier research (Daxenberger and Gurevych, 2012). This dataset breaks down each pair of adjacent revisions into smaller edits. This helps give a clearer view of how editing happens, since an author might make multiple separate edits in a single revision. Plus, each edit is tagged with one or more categories, which is super useful for pinpointing major changes, like when a whole new paragraph with text, references, and markup is added.\n\nThere are four main types of edits: Insertions, Deletions, Modifications, and Relocations. These are figured out by doing a line-by-line comparison of the source text (including wiki markup). Following the approach suggested earlier (Daxenberger and Gurevych, 2012), only the specific part of the text that actually changes within a modified line is marked as an edit (either an Insertion, Deletion, or Modification), not the whole line.\n\nWe also pulled in some extra data that wasn’t included in WPEC, like metadata and the plain text from the revisions (rv−1 and rv). We did this using the Java Wikipedia Library (JWPL) along with the Revision Toolkit (Ferschke et al., 2011).",
        "formal_text": "For our experiments, we used the freely available Wikipedia Edit Category Corpus (WPEC) compiled in previous work (Daxenberger and Gurevych, 2012). In this corpus, each pair of adjacent revisions is segmented into one or more edits. This enables an accurate picture of the editing process, as an au-thor may perform several independent edits in the same revision. Furthermore, edits are multi-labeled, i. e. each edit is assigned one or more categories. This is important for a precise description of major edits, e. g. when an entire new paragraph including text, references and markup is added. There are four basic types of edits, namely Insertions, Deletions, Modifications and Relocations. These are calculated via a line-based diff comparison on the source text (including wiki markup). As previously suggested (Daxenberger and Gurevych, 2012), inside modified lines, only the span of text which has actually been changed is marked as edit (either Insertion, Deletion or Modification), not the entire line. We extracted the data which is not contained in WPEC (meta data and plain text of r v−1 and r v ) using the Java Wikipedia Library (JWPL) with the Revision Toolkit (Ferschke et al. , 2011).",
        "GPT2_formal_text": "\" and \"title_ids\" to create a query for the text. Formal: A bunch of features are pulled from the text to figure out the attention distribution. Formal: Now that we've got the attention weights and the query vector, we need to figure out the hidden states for the different parts of the image and its context. Formal: Using the input image and its context, the model figures out the attention distribution for each part of the image. It takes the output from each attention head and mixes them together to get the hidden state for the image. Formal: Let’s break this down in simpler terms. Formal: The main thing about this model is that it’s multi-task, meaning it’s not just dealing with classification. It also takes into account the text and image aspects. Formal: The model tries to create a query that gives us the most accurate representation of the input. The probability of getting it right is just the sum of the probabilities of the different parts (h = {h1, ..., hn}) that are involved. Formal: We’re using the beam search model, which is a common approach in deep learning for big tasks. Formal: The final attention weights are calculated using the hidden states from the output heads of the attention modules. Formal: Formal: In this paper, we’re using three different sequence labeling models to handle the text and image components separately, following what Liu et al. (2019a) did. Formal: The text and image inputs are combined into a single vector, h. Formal: The image is then turned into a set of numbers (r) that are 1s, 0s, and 0s, all in a uniform format. Formal: The hidden state vector for each image is then turned into a scalar (h) with a specific range, δ. Formal: Finally, we combine the attention weight vector (h) with this scalar to get the attention distribution for the image. Formal: The overall goal is to generate the query in a way that gives us the highest possible representation of the input. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D13",
        "filename": "D13-1055.json"
    },
    {
        "casual_text": "For Sentiment, we just pick random Polyjuice counterfactuals without worrying about their labels, as long as there’s at least one that changes the label of the original x. For NLI and QQP, we noticed in a small test that randomly picking counterfactuals might not be any better than just adding more regular data. We think Polyjuice doesn’t really understand the context or domain knowledge to make important changes, so it doesn’t add much beyond what pretraining already does (thanks, Longpre et al., 2020). So, instead, we use slicing functions to spot interesting patterns (like prepositions in NLI) and mess with those by putting [BLANK]s where the patterns are. For instance, \"His surfboard is beneath him\" turns into \"His surfboard is [BLANK] him\", and Polyjuice might then come up with stuff like \"His surfboard is beneath ) next to him.\"",
        "formal_text": "For Sentiment, we select random Polyjuice counterfactuals regardless of their labels, as long as an original x has at least onex that flips the label. For NLI and QQP, we observed in a pilot study that randomly chosen counterfactuals may not be more effective than the same amount of additional data. We suspect that Polyjuice lacks domain knowledge and context for identifying critical perturbations, and therefore brings benefits redundant with pretraining (Longpre et al. , 2020). Thus, we use the slicing functions of to find patterns of interest (e. g. , prepositions in NLI), and perturb those patterns by placing [BLANK]s on the matched spans. For example, \"His surfboard is beneath him\" becomes \"His surfboard is [BLANK] him\", and Polyjuice generates counterfactuals such as \"His surfboard is beneath ) next to him. \"",
        "GPT2_formal_text": ", attention_type, and attention_type_mask to get the final results. Formal: To find the best parameters for this function, we use the empirical distribution of the predictions, which is shown in Equation 11. Formal: In the smoothed version, we grab the maximum of the predictions from all the active features. Formal: In the multi-feature multi-attention version, we apply the attention to multiple features at the same time. Formal: We take the average of the predictions from all the active features and then average that across all the features. Formal: We train the model using the gradient descent algorithm, aiming to minimize the cross-entropy loss function for each feature. Formal: In the smoothed version, we also update the weights for the feature maps, following the approach from Jiao et al. (2017). Formal: To keep the dimension of the output vector under control, we use a linear layer with a max operation in the output. Formal: For the multi-feature multi-attention model, we set the attention parameter vector to a fixed value. Formal: We also add a softmax function to the input feature maps, and then do a linear layer with a max operation in the output. Formal: We train the model using the gradient descent algorithm, aiming to minimize the cross-entropy loss function for each feature. Formal: We also add a softmax function to the input feature maps, and then do a linear layer with a max operation in the output. Formal: We train the model using the gradient descent algorithm, aiming to minimize the cross-entropy loss function for each feature. Formal: Finally, we update the weights for the feature maps, following the method from Jiao et al. (2017). Formal: If we want, we can also add a softmax function to the input feature maps, and then do a linear layer with a max operation in the output. Formal: We train the model using the gradient descent algorithm, aiming to minimize the cross-entropy loss function for each feature. Formal: Formal: We also add a softmax function to the input feature maps, and then do a linear layer with a max operation in the output. Formal: Finally, we update the weights for the feature maps, following the method from Jiao et al. (2017). Formal: We train the model using the gradient descent algorithm, aiming to minimize the cross-entropy loss function",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "To make sure our method works well, we also checked how it stacks up against some other methods that don't need labeled data to work on sentences.",
        "formal_text": "To further validate the performance of our method, we also compared our method with the following unsupervised sentence-level baseline methods:",
        "GPT2_formal_text": ", like the ones in c_i, are basically collections of words that make up a sentence, which is like a topic or a sentence. Formal: Even though we're not directly looking at how our model's outputs are different from the real ones, we know that it's not exactly the same. Formal: To get a closer look at how our model's outputs stack up against the real ones, we’ve developed a way to measure how similar they are. We use something called cosine similarity. The idea is that if two things are similar in meaning, their outputs should also be similar in meaning. So, we’ve come up with a way to measure the similarity between different versions of our model and the real ones. Formal: So, for a sentence, we need to figure out how similar it is to a list of possible sentences. Formal: We’ve got a way to measure how similar the model’s outputs are to the real ones. The formula looks like this: Formal: We use cosine similarity to compare the model’s outputs to the real ones, like in Equation (1). Formal: Formal: We also have a way to measure how similar the model’s outputs are to the real ones. This formula is the same as the one we just talked about. Formal: Formal: Lastly, we’ve developed a way to compare the model’s outputs to the real ones. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.267.json"
    },
    {
        "casual_text": "Our approach is based on how sense subjectivity and contextual subjectivity are connected and includes two main rules, R1 and R2.",
        "formal_text": "Our strategy is defined by the relation between sense subjectivity and contextual subjectivity and involves two rules, R1 and R2.",
        "GPT2_formal_text": "_tgt, and the result is a matrix called H. This matrix shows the attention state of the model at a specific time step t. Formal: At each time step t, we calculate the attention weights for the token representation h_t using equation (1). Formal: Each token's attention weight is calculated as the sum of the attention weights for the tokens in that token's context. Formal: We use a special number, φ t, to keep track of the attention weights, so we can easily compare them. Formal: We calculate the attention weights, H t, using equation (3). Formal: Lastly, we use the model's output state (x) to calculate the attention weights, H. Formal: Basically, H t is the model's output state at time t. Formal: The attention weights H t can be written like this: Formal: Attention weights H t are calculated using equation (3). Formal: Formal: Finally, the model's output state (x) is the result of applying the attention weights H t to the tokens in the context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1020.json"
    },
    {
        "casual_text": "Since this paper is all about feature development, we’re just using a clustering method that helps us see how good our similarity matrix is for clustering. We picked agglomerative clustering with single linkage for this. Now, since each name might need its own settings, we wanted to check how much those settings matter for clustering. So, in our experiments, we tried two different approaches for the stop-threshold in agglomerative clustering: first, we tested every possible stop-threshold to find the best one for any ambiguous name and each feature model. The one that worked the best became our optimal stop-threshold. Second, we used a fixed stop-threshold that we got from our development data.",
        "formal_text": "Since the focus of this paper is only on feature development, we simply employ a clustering method that can reflect the quality of the similarity matrix for clustering. Here, we choose agglomerative clustering with a single linkage. Since each personal name may need a different parameter setting, to test the importance of the parameter setting for clustering, we use two kinds of stopthresholds for agglomerative clustering in our experiments: first, to find the optimal stop-threshold for any ambiguous personal name and for each feature model, we run agglomerative clustering with all possible stop-thresholds, and choose the one that has the best performance as the optimal stop-threshold; second, we use a fixed stopthreshold acquired from development data.",
        "GPT2_formal_text": ", where the idiom is the main topic. Formal: The base classifier T_i has a hidden state h_t = [h_t1, ..., h_tK], which is just a vector with n rows and m columns. Formal: The connection between the target and the source can be expressed using the hyperplane C_t = h_t * x_t, where x_t is the input word embedding. Formal: The objective function α is calculated by applying a linear transformation to the embedding matrix. Formal: The model calculates the reward for each query by minimizing the log-likelihood of the prediction. Formal: The loss function l_f is a function that uses a non-linear function to deal with cross-entropy. Formal: The loss function l_h is a linear function that depends on the log-likelihood loss function l_f. Formal: The model generates the embedding y_t for the target word w_t using the source word embedding x_t. Formal: The model uses the embedding y_t, which is the result of the source word's embedding, as a prediction for the target word. Formal: The model assigns a probability to the word embedding x_t, which is calculated using the cross-entropy loss function l_f. Formal: The model assigns a probability to the word embedding x_t, which is calculated using the cross-entropy loss function l_h. Formal: We use the final output from the model as the output representation for the query. Formal: The loss function l_f is applied to the source word's embedding, which is the output from the model's final layer. Formal: The model generates the target embedding y_t for the source word w_t using the target word's embedding x_t. Formal: The model uses the target embedding y_t, which is the result of the model's final layer, as a prediction for the target word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D07",
        "filename": "D07-1020.json"
    },
    {
        "casual_text": "The BLEU score keeps dropping no matter what, but the TER score gets a little better for the English-Spanish and English-French pairs, especially if you use the sentence-based or two-steps estimation methods. Still, since the results for both scores don't really match up, we can't say for sure if this type of language model adaptation is better than just sticking with the basic baseline model.",
        "formal_text": "A degradation of the BLEU score is observed in any condition, while TER slightly improves for the En-Es and En-Fr pairs, especially when either the sentence-based or the two-steps estimation schemes are adopted. However, since results are not coherent for two scores, it cannot be definitely stated whether this form of LM adaptation overcomes the use of the single baseline LM.",
        "GPT2_formal_text": "is set to a small value. The latent variable f_t is set to 1, and L_att is the log probability of the output y_t given the input x_t. Formal: The model is trained using gradient ascent, with a goal γ = 0.05, and the training is done using Adam (from Kingma and Ba, 2015) with a learning rate of 1e-5. Formal: Here, f_t is the hidden state of the t-th layer, and x_t is the output from the t-th layer. Formal: The hidden state of the t-th layer is h_t = {h_1, ..., h_T}, where h_1, ..., h_T are the first-order (left-to-right) hidden states, each with dimensions d_1, ..., d_T. Formal: Similarly, the input matrix is M = {x_1, ..., x_T} with the same dimensions, and the output is y_1 = {y_1, ..., y_T}. Formal: We call this model the Multi-Layer Perceptron (MLP), where the input and output matrices are (x_1, ..., x_T). Formal: If the input matrix is already in R^d, we use the same approach as the MLP, which just swaps the input and output matrices with the original R^d. Formal: This model can also be thought of as a feedforward neural network, with the input being x_i = {x_1, ..., x_T}. Formal: For this model, we also use the same approach as the MLP, but we don't need to train the first-order (left-to-right) hidden states to fit the matrix M. Formal: This setup has a total of 512 hidden units, each with dimensions d_1, ..., d_T. Formal: Formal: The input and output matrices are (x_1, ..., x_T). Formal: The MLP is trained for 100 epochs on a single NVIDIA GeForce GTX 1080 GPU. Formal: We also tried a neural network setup that uses an LSTM (from Hochreiter and Schmidhuber, 1997) with a hidden state size of d_T = 512 for this MLP. Formal: Using this, we",
        "directory": "eamt",
        "filename": "2010.eamt-1.30.json"
    },
    {
        "casual_text": "• RQ1: Can we create machine learning models that are better than the current systems for detecting cyberbullying?",
        "formal_text": "• RQ1: Can we build machine learning models to outperform current cyberbullying classification systems?",
        "GPT2_formal_text": "= { u, v }  \n\nHere, e_i is the i-th token, and c_i is the i-th character in the input sequence. Formal: Here, p(c_i) is the probability of the i-th character in the input sequence. Formal: Lastly, the entropy of the generated subsequence vector is determined by the length of the input sequence. Formal: For the encoder-decoder model, we take the current input sequence x and use a generator G to generate the output sequence y. Formal: In this part, we’re mainly focusing on the CNN/DailyMail model. For this one, we train the Bi-LSTM model with two layers, which means we generate an output sequence y = (y₁, y₂, ..., yₙ) using the given input x. Formal: In this setup, the output vector has n specific features, and they’re all calculated using the word embedding model. Formal: Formal: We also use the same generator G to generate the final word embedding. Formal: The input sentence x and the output word embedding y are represented as vectors with n features, which we can then use to train the Bi-LSTM model. Formal: For the encoder-decoder model, we take the current input sequence x and the output word embedding y and use a generator G to generate the output sequence y = (y₁, y₂, ..., yₙ) using the given input x. Formal: The output embedding can be any shape or vector, as long as the word embedding model produces a word shape. Formal: Finally, the entropy of the generated subsequence vector is calculated by the length of the input sequence. Formal: Formal: The main goal of this project is to explore the potential of adapting sequence generation models to various natural language processing tasks. Formal: Formal: We’re only looking at natural language generation tasks, which means we don’t need any specific datasets to test how well our model works. Formal: Formal: Formal: The model we’re proposing was trained using different datasets: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "alta",
        "filename": "2020.alta-1.2.json"
    },
    {
        "casual_text": "Using the N-gram method, the translation process is basically like looking for the best possible translation option, called hypothesisê I 1. This best option is found by picking the one that gives the highest score when you combine a translation model (TM) with a bunch of other feature models, all added up in a log-linear way.",
        "formal_text": "According to the N -gram-based approach, the translation process is considered as an arg max searching for the translation hypothesisê I 1 maximizing a log-linear combination of a translation model (TM) and a set of feature models:",
        "GPT2_formal_text": "ed_struct_tensor = { [CLS] = [CLS], [MASK] = [MASK], } tokens_masked = { [CLS] = [CLS], [MASK] = [MASK], } model_2 = { [CLS] = [CLS], [MASK] = [MASK], [TODO] = [TODO], } classifier = { [CLS] = [CLS], [MASK] = [MASK], [TODO] = [TODO], [MASK] = [MASK], }\n\nSo, in short, it's about figuring out the sequence of input tokens to generate a response. Formal: We have a sequence of tokens and a set of output representations, which can be either the text or the question. We take the output from the classifier and pass it through a transformation layer to get the final representation. Formal: We've added a layer where the output is the answer, and it's also in the form of a question. Formal: We're using a Recurrent Neural Network (RNN) with a Bi-directional Long Short-Term Memory (BiLSTM) for both the question and the answer. The hidden state h s is a vector with length d s, where d s is the dimension of the question and the answer. Formal: We're using a Recurrent Neural Network (RNN) with a Bi-directional Long Short-Term Memory (BiLSTM) for both the question and the answer. The hidden state h s is a vector with length d s, where d s is the dimension of the question and the answer. Formal: Formal: The input to the BiLSTM is the text-question pair. Formal: We've added two layers of RNNs to handle the dialogue context. Formal: We're using a Recurrent Neural Network (RNN) with a Bi-directional Long Short-Term Memory (BiLSTM) for both the question and the answer. The hidden state h s is a vector with length d s, where d s is the dimension of the question and the answer. Formal: We're using a Recurrent Neural Network (RNN) with a Bi-directional Long Short-Term Memory (BiLSTM) for both the question and the answer. The hidden state h s is a vector with length",
        "directory": "eamt",
        "filename": "2009.eamt-1.27.json"
    },
    {
        "casual_text": "We also checked out how many alignment links our systems were creating. One thing that stood out was that the Bayesian HMM with NULL words consistently made way fewer links compared to all the other systems. If we use the BHMM as a baseline, the other models created, on average across languages and translation directions, 39.5% more links (for our collocation-based model), 39.2% more (Giza++), and 36.2% more (fastAlign). Making more links makes it harder for the phrase extraction process and results in smaller phrase tables. In practice, the phrase tables from the collocation-based model are about three times smaller than those from the Bayesian HMM. So, the collocation-based system has an edge here.",
        "formal_text": "We also analysed the number of alignment links that our systems set. It is noteworthy that the Bayesian HMM with NULL words consistently sets much fewer links than all other systems. Taking BHMM as baseline other models set additionally (on average across languages and translation direction) 39. 5% (our collocation-based model), 39. 2% (Giza++), and 36. 2% (fastAlign) more links. Setting more links constrains phrase extraction heuristics more and leads to smaller phrase tables. Empirically, the phrase tables of the collocation-based model are roughly three times smaller than those of the Bayesian HMM. Thus, the collocation-based system is at an advantage here.",
        "GPT2_formal_text": ", and then output_descriptors are added to the mix. This means that the dialog acts can be rewritten using the info in the decoder's state, which helps with predicting the next action. The decoder's state acts as a kind of intermediate representation, while the encoder's state acts as the actual input for the response. Formal: The decoder's state acts as the input for the response, while the decoder's state acts as the input for the response. Formal: The decoder's state acts as the input for the response, while the decoder's state acts as the input for the response. Formal: The decoder's state acts as the input for the response, while the decoder's state acts as the input for the response. Formal: The decoder's state acts as the input for the response, while the decoder's state acts as the input for the response. Formal: The decoder's state acts as the input for the response, while the decoder's state acts as the input for the response. Formal: The decoder's state acts as the input for the response, while the decoder's state acts as the input for the response. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1296.json"
    },
    {
        "casual_text": "So far, multi-task learning in patent translation has mostly been about trying out different combinations of translation and language models, but only within specific groups of patent sections. For instance, Utiyama and Isahara (2007) and Tinsley et al. (2010) looked at how translation and language models work when trained on different patent sections. They found that having more parallel data helps improve the results. Ceauşu et al. (2011) also noticed that language models always improve, and translation models usually do too, when they have more data from different sections. In our approach, we use models trained on this combined patent data as a starting point for comparison.",
        "formal_text": "Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceauşu et al. (2011) find that language models always and translation model mostly benefit from larger pools of data from different sections. Models trained on pooled patent data are used as baselines in our approach.",
        "GPT2_formal_text": "ed and attention_attended. Formal: Here, w_i represents the input vector from the i-th layer, and m_i,j,k represents the output vectors from the i-th layer, as well as the hidden states of the k-th GCN. Formal: In this project, we introduce a new GCN setup called GCN-Att-Att. This GCN-Att-Att model takes a multi-view GCN and combines it with a multi-attention mechanism. We tested our model on three different NER datasets and found that it can boost NER performance by up to 8% compared to the original GCN-Att-Att. Specifically, we tested on the CoNLL 2003 NER dataset and the CoNLL 2003 CoNLL 2003 NER dataset. The results show that our model works well. Formal: Check out Figure 1 for an example of our model on the CoNLL 2003 NER dataset. This example uses the entity type label from the first sentence (bottom part) to help predict the entity type label in the second sentence. Formal: Check out Figure 2 for a more detailed example of our model on the CoNLL 2003 CoNLL 2003 NER dataset. The model predicts the type for the entity \"Riesbeck\" by using the input to predict the type label from the second sentence. Formal: Formal: Figure 3 shows the query and entity type labels for the entity \"Riesbeck.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E12",
        "filename": "E12-1083.json"
    },
    {
        "casual_text": "Another cool thing about the sieve framework is how easy it is to add new stuff without needing to know everything else that’s already in there. We’ve made the code available for everyone to use, so you can either run it as a stand-alone coreference system or build on it to create something new.",
        "formal_text": "An additional benefit of the sieve framework is its modularity: new features or models can be inserted in the system with limited understanding of the other features already deployed. Our code is publicly released 5 and can be used both as a stand-alone coreference system and as a platform for the development of future systems.",
        "GPT2_formal_text": ", which are basically the collection of document-related things you're dealing with. The relevance part is all about the accuracy score you get by comparing a document and its related sentences, which we call r_j. The final score for relevance is the average of the results from both the relevance and the query (which we call r_q). Formal: The relevance and relevance scores are calculated based on how relevant each sentence in the input is to the question. Formal: The relevance score, r_j, is basically the average of the relevance scores for sentences in the input. Formal: The relevance score (r_j), on the other hand, is calculated by multiplying the relevance scores from the query and the input. Formal: For a given input x, the relevant sentences for the query q can be figured out by looking at how relevant the query sentences are to the input. Formal: For a given query q, the relevant sentences for the query q can be found by using the query and query relevance scores. Formal: The relevance scores (r_q) are calculated by multiplying the relevance scores from the query and the input. Formal: The relevance score (r_q), on the other hand, is calculated by multiplying the relevance scores from the query and the query relevance scores. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D10",
        "filename": "D10-1048.json"
    },
    {
        "casual_text": "Alright, let’s move on to the main point of this paper: figuring out a quick way to describe the whole regularization path for the maximum entropy problem. We’ve already shown that the best solution, p, can be easily found using the variable µ. So, instead of dealing with p directly, we just need to keep track of how µ changes as we move across the (ν, µ) plane, starting from ν = 0 and ending at the last change point, which we called (ν∞, µ∞). \n\nIn this part, we’ll explain an algorithm that does this by finding where ℓ0 crosses the lines ℓ−n, ℓ−n+1, ..., ℓ−1, ℓ1, ..., ℓn. After each intersection, we’ll update ℓ0 and keep going until we’ve covered everything.",
        "formal_text": "We now shift gears and focus on the main thrust of this paper, namely, an efficient characterization of the entire regularization path for the maximum entropy problem. Since we have shown that the optimal solution p can be straightforwardly obtained from the variable µ, it suffices to efficiently track the function µ(ν) as we traverse the plane (ν, µ) from ν = 0 through the last change point which we denoted as (ν ∞, µ ∞ ). In this section we give an algorithm that traverses µ(ν) by locating the intersections of ℓ 0 with the fixed lines ℓ −n, ℓ −n+1, . . . , ℓ −1, ℓ 1, . . . , ℓ n and updating ℓ 0 after each intersection.",
        "GPT2_formal_text": "ing_norm = p_mask / (N + 1) | w_mask - 1. Formal: For all these settings, we use the development set to train the whole model (we call it \"staging\"). Formal: Next, we tweak the model so it can handle different situations better by tweaking the learning rate. To make sure our model can handle different scenarios, we set the learning rate to 0.1. Formal: After optimizing the parameters of our model, we get a new loss function called L_train_0 (or L_train_1 for short). This new loss function is optimized using the validation set. Formal: The validation set is used to train the model in a supervised way (like supervised learning). It’s a good way to test how well the model can handle different scenarios. Formal: To train this model, we need an unlabeled dataset. This dataset includes a group of dialogues (let’s call it D) where each dialogue is a sentence. Formal: Let’s say there’s a sample dialogues D in the dataset D. Formal: We’ll use some dialogues D_i to train the model with D. Formal: We use the dataset D_i to train the model on D. Formal: This whole process happens from D = {d_1, ..., d_k} up to the output we want (which we’ll call {x_i}). Formal: Finally, we get the output y_i, which is the label we want for the i-th sample in D. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D11",
        "filename": "D11-1087.json"
    },
    {
        "casual_text": "Second, we were focused on how specific the terms were to certain areas. Our plan was to measure this using numbers by comparing how often words were used in different groups of texts. But for this particular study, we went with people's opinions instead. Melamed (1996b) says that to really evaluate translation word lists, judges need to see both languages side by side, showing how the words are used in sentences. But getting people to judge without seeing the full context would be simpler in both real-world and lab settings.\n\nAs a first try, we had three people do a kind of tagging job: one was a pro French/English translator, and the other two were grad students at Penn. They tagged a bunch of entries that came from an older version of the SABLE system (this one used small pieces of sentences to figure out which words often appear together; check out Section 2.2 for more). They didn’t get to see the full sentences in both languages this time. After the study, looking at how the system did and reading comments from the taggers in a follow-up survey, we saw that context really matters.\n\nTo get a better idea of how much it matters, we asked one of the taggers from the first round to do the same job again, but this time she got to see the full context—both languages side by side, showing up to the first ten times each word pair was used.",
        "formal_text": "Second, we were concerned with domain specificity. Ultimately we intend to measure this in an objective, quantitative way by comparing term usage across corpora; however, for this study we relied on human judgments. Melamed (1996b) suggests that evaluation of translation lexicons requires that judges have access to bilingual concordances showing the contexts in which proposed word pairs appear; however, out-ofcontext judgments would be easier to obtain in both experimental and real-world settings. In a preliminary evaluation, we had three annotators (one professional French/English translator and two graduate students at the University of Pennsylvania) perform a version of the annotation task just described: they annotated a set of entries containing the output of an earlier version of the SABLE system (one that used aligned sub-sentence fragments to define term co-occurrence; cf. Section 2. 2). No bilingual concordances were made available to them. Analysis of the system's performance in this pilot study, however, as well as annotator comments in a post-study questionnaire, confirmed that context is quite important. In order to quantify its im: portance, we asked one of the pilot annotators to repeat the evaluation on the same items, this time giving her access to context in the form of the bilingual concordances for each term pair. These concordances contained up to the first ten instances of that pair as used in context.",
        "GPT2_formal_text": ", and a feature map for the self-attention mechanism. This map is made by combining the attention information from the input layer with the knowledge from the output layer, as well as the position feature of the word. Each word's position is represented as a vector in R^d. The hidden state size of this self-attention mechanism is h_S. Formal: The entity embedding layer combines the entity embedding vector with the self-attention mechanism representation to create an entity embedding. Then, this entity embedding is passed into the next entity embedding layer, which also takes the entity embedding vector into account. Formal: In this project, we use a multi-layer perceptron (MLP) as the entity embedding model. Specifically, we use a linear layer with a window size of 200, as shown in Figure 3. We use the softmax function to predict the hidden states of the prediction. Formal: The loss function is a simple linear function that considers the interaction between the input and output. In this case, it helps the model focus on important words in the input that could lead to better entity embeddings. Formal: The entity embedding layer gives each word its position embedding. Using the entity embedding layer, the next entity embedding layer predicts the next entity's embedding. Formal: A quick and easy way to handle this is by simply averaging the output vectors of the previous and current entity embedding layers. Formal: In this project, we also use the word2vec library (Mikolov et al., 2013) to calculate the cross-entropy loss. Formal: Here’s how it works: Formal: Basically, for each layer, we create a representation for the next word (a_t) based on the last hidden state (h_t-1) of the output layer. Formal: The cross-entropy loss is calculated using this process. Formal: Lastly, we pick a value λ that makes the cross-entropy loss as small as possible. Formal: The cross-entropy loss for the whole dataset is calculated using this formula: Formal: Here’s a simpler version: Formal: The cross-entropy loss for a dataset D is basically calculated using this formula: Formal: The cross-entropy loss for a set of documents D = {d_1, ..., d_k, ..., d_n} is calculated using",
        "directory": "A97",
        "filename": "A97-1050.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nFor tasks that involve figuring out the relationship between two sentences (like textual entailment), a special token (like [SEP]) is automatically added between the sentences. This helps the model know where one sentence ends and the other begins.",
        "formal_text": "[CLS] or <s>). For tasks which model a relation between multiple sentences (e. g. , textual entailment), a special token (e. g. [SEP] ) is automatically inserted between the sentences to inform the model about the sentence boundaries.",
        "GPT2_formal_text": ". Formal: We grab a ton of sentences from each domain using the first batch of data and use a BERT model to classify them. Formal: After that, we calculate a bunch of cross-entropy losses, which we call L_i, and we use cross-entropy loss on the results to train a classifier. Formal: In the worst-case scenario, this classifier gives us a loss of zero. Formal: We get a bunch of sentence pairs from each domain using the first batch of data. Formal: For each pair (s_i, r_i), we use a BERT model to predict the label y_i for the source sentence. Formal: Then, we calculate a cross-entropy loss for the target domain, L_d, using the loss from the model we trained on the source domain. Formal: Finally, we combine all these losses to get the final cross-entropy loss for the whole sentence pair. Formal: The main idea is that the cross-entropy losses from the BERT model are way better than those from the classifier. Formal: So, we add a negative log-likelihood loss to the cross-entropy loss. Formal: Here's the formula for the cross-entropy loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eacl",
        "filename": "2021.eacl-demos.22.json"
    },
    {
        "casual_text": "You could say that these mistakes affect the user experience differently. For example, a correctly spelled word being changed incorrectly is more annoying than a misspelled word that isn’t corrected but is marked as wrong. But in this paper, we’re treating all errors the same. E1, E2, E3, and E4 are all about the correction process. So, we can come up with something called the Correction Error Rate (CER).",
        "formal_text": "It can be argued that these errors have varying impact on user experience. For instance, a well spelled word that is wrongly corrected is more frustrating than a misspelled word that is not corrected but is flagged. However, in this paper, we treat each error equally. E 1, E 2, E 3 and E 4 pertain to the correction task. Hence we can define Correction Error Rate (CER):",
        "GPT2_formal_text": ", which are basically word embeddings of each token's embedding. The final probability p(t) is calculated as the expected count of tokens based on these probabilities. Formal: We also need to figure out the probability for every possible tag sequence. Formal: The probability for the sequence tag Θ (e, f) i,j is calculated like this: Formal: For a given input token x, the chance of generating a tag sequence x i,j is calculated like this: Formal: The tag probability p(t) for the sequence tag Θ (e, f) i,j is basically the product of the probability for each token in the sequence (p(e i |f ), p(f i |e i )). Formal: So, p(t) represents the probability for the sequence tag Θ (e, f) i,j, and it's calculated using the probability of each token in the sequence. Formal: To simplify things, we’ve simplified the expression a bit. Instead of using a probability p(t) for each token e i, f i, we’ve just used the probability of the sequence tag Θ (e, f) i,j. Formal: We can calculate the tag probabilities for all possible tags x i,j using this updated equation. Formal: The probability for the sequence tag Θ (e, f) i,j is basically the product of the probability for each token in the sequence (p(e i |f ), p(f i |e i )). Formal: To calculate the tag probability for a given input token x, we use a rule-based method. Formal: The probability for a tag sequence x i,j is calculated based on a rule-based algorithm. Formal: Since the tag sequence x i,j is a permutation, it’s pretty easy to check how likely it is to generate a tag sequence x i,j. Formal: In this paper, we’re mainly interested in the sequence tag Θ (e, f) i,j. Formal: Since the sequence tag Θ (e, f) i,j can be generated in a straightforward way, the tag sequence x i,j is also pretty easy to calculate using this algorithm. Formal: Basically, the tag probabilities for a given input token x are calculated using this rule-based algorithm. Formal: Formal: We use the",
        "directory": "D09",
        "filename": "D09-1093.json"
    },
    {
        "casual_text": "We took a look at how well our commitment-based system for RTE did with the 1600 examples from the PASCAL RTE-2 and RTE-3 datasets. Table 1 shows the results we got when we trained our system on those 1600 examples from the RTE-2 and RTE-3 Test Sets.",
        "formal_text": "We evaluated the performance of our commitmentbased system for RTE against the 1600 examples found in the PASCAL RTE-2 and RTE-3 datasets. 7 Table 1 presents results from our system when trained on the 1600 examples taken from the RTE-2 and RTE-3 Test Sets.",
        "GPT2_formal_text": "_sigmoid() (Sennrich et al., 2016) to show attention heads in a vector. We also use ReLU activation to normalize the attention vectors. Formal: Equation 5 shows how the model adjusts its attention distribution after each input token is processed. We use this adapted attention network to predict the next token. We're using a batch size of 32, a learning rate of 1e-5, and a batch size decay of 0.2. Formal: Using the attention distribution from Equation 5, we calculate the loss function L_att_adapted_sigmoid(Y_t) for each t. The model's response to the prompt and the text it generates is shown in Figure 2, along with the model's hidden states from the last layer. Formal: To get the attention weights (α_att) for the label h_t, we calculate the attention vectors for each word in the text using the attention weight matrix A_att. Formal: For the word w_t, we create a position embedding h_t_pos using the attention matrix A_att. Formal: Finally, we apply the variational autoencoder (VAE) (Hochreiter and Schmidhuber, 1997) to create the output label h_t. Formal: This is the same approach used by Lample et al. (2016) for generating text with a variational autoencoder, but we stick to the original attention loss function. Formal: We use a learning rate of 1e-5 and a batch size of 32, which are the settings we used in the original paper. Formal: We also calculate the loss for the final hidden state h_t. Formal: We use the hyperparameters we got from the original paper as the hyperparameters for the modified attention network. Formal: We set the learning rate to 1e-5 and a batch size of 32, which are the settings we used in the original paper. Formal: We also calculate the loss for the final hidden state h_t. Formal: We use the hyperparameters we got from the original paper as the hyperparameters for the modified attention network. Formal: Finally, we apply the variational autoencoder (VAE) (Hochreiter and Schmidhuber, 1997) to create the output label h_t. Formal: We use a learning rate of 1e-5",
        "directory": "C08",
        "filename": "C08-1043.json"
    },
    {
        "casual_text": "But, benchmarks don’t really capture the messy, complicated stuff you deal with in real-world situations. To get closer to how doctors actually think, we thought about using differential diagnosis prediction, which means bringing in more types of clinical data. Still, when it comes to using knowledge graphs in real-world settings, we might need to think about things like how time plays a role. For example, predicting how a disease might change over time could help us understand the long-term side of diagnosing. Part of what we’re working on is coming up with tasks that reflect the real challenges doctors face when making decisions.",
        "formal_text": "However, such benchmarks are abstractions that do not fully align with complex real-world use cases. To better model the challenges of clinical reasoning, we suggested the particular task of differential diagnosis prediction, which involves incorporating additional clinical data modalities. However, additional considerations may be necessary for evaluating KG use in real-world applications, such as modeling temporality. A task involving prediction of changing disease states over time may focus on the longitudinal nature of diagnostic reasoning. A portion of this work will involve continuing to define tasks that consider the challenges of real-world clinical reasoning use cases.",
        "GPT2_formal_text": "= {u1, ..., uk, ..., um}, where k can be from 1 to N. This gives us a bunch of output tokens. We'll use those as the ground-truth to train the model. Formal: The first example is from the text in Section 3.1, and the second one is from the text in Section 3.2. The main difference here is that the first example is a sentence with no pronouns and no extra words, while the second example has pronouns and a pronoun in the first sentence. Formal: The second example is from the text in Section 3.2, and the third one is from the text in Section 3.3. The difference is that the second example is in the same sentence as the first one. Formal: The third example is from the text in Section 3.3, and the fourth one is from the text in Section 3.4. Formal: Formal: Formal: The first example is in the same sentence as the second one. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-srw.33.json"
    },
    {
        "casual_text": "We tested our methods on four Chinese NER datasets: OntoNotes (Weischedel et al., 2011), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017a), and Resume NER (Zhang and Yang, 2018). OntoNotes and MSRA come from news articles, and they have gold-standard segmentation for the training data. For OntoNotes, the development and testing data also have gold segmentation. On the other hand, Weibo NER is from social media, and Resume NER is from resumes. These two datasets don’t have gold-standard segmentation. Table 1 has all the stats for these datasets.\n\nAs for the lexicon, we used the same one as Lattice-LSTM. It includes 5.7k single-character words, 291.5k two-character words, 278.1k three-character words, and 129.1k other words. We also used the same pre-trained character embeddings as Lattice-LSTM, which were trained on the Chinese Giga-Word dataset using word2vec.",
        "formal_text": "The methods were evaluated on four Chinese NER datasets, including OntoNotes (Weischedel et al. , 2011), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017a), and Resume NER (Zhang and Yang, 2018). OntoNotes and MSRA are from the newswire domain, where gold-standard segmentation is available for training data. For OntoNotes, gold segmentation is also available for development and testing data. Weibo NER and Resume NER are from social media and resume, respectively. There is no gold standard segmentation in these two datasets. Table 1 shows statistic information of these datasets. As for the lexicon, we used the same one as Lattice-LSTM, which contains 5. 7k single-character words, 291. 5k two-character words, 278. 1k three-character words, and 129. 1k other words. In addition, the pretrained character embeddings we used are also the same with Lattice-LSTM, which are pre-trained on Chinese Giga-Word using word2vec.",
        "GPT2_formal_text": "ed is the idea that the attention weight vector for an entity i, like its word representation, should be at least as big as its hidden representation, called h_t, which has dimensions d_t. On the flip side, attention_aggregation is all about how these two weights interact. In the next part, we'll talk about how to use this structured attention mechanism with a convolutional neural network to create a representation for each entity, including all its features. Formal: To make it easier to compare our work with other studies, we'll use the same unitary product (µ) as they do. Formal: For entity embedding, we're using a convolutional neural network (CNN) with ReLU as the activation function. Basically, we're following the approach used by Su et al. (2016), which uses an LSTM to generate a vector for each entity. Formal: For entity recognition, we're using a linear layer to grab the entity features. Formal: To figure out the relationship between the entity and its features, we're following a similar method as Chen et al. (2016). Formal: The parameters Θ_i and Θ_j are the parameters that the CNN uses to predict the relation vector. Formal: To compare our results with other methods, we'll look at the negative log-likelihood of the generated vectors. Formal: We'll check how well the entity features are extracted by using the unlabeled entity features (UF_ei) as the target set. Formal: To see how the global entity features are represented, we'll use the entity features (UF_e) as the target set. Formal: We'll calculate the similarity between the output vectors of the entity representations, UF_e, and the labels (UF_l). Formal: The embedding of the entity representation, UF_e, will be the same as the entity's feature vector. Formal: We'll use the entity features (UF_e) as the target set to find the correct relation vector. Formal: The relation vector, UF_l, will be the result of the CNN's prediction for the hidden states of the entity, U_e. Formal: We'll use the entity features (UF_e) as the target set to find the correct relation vector. Formal: We'll use the entity features (UF_e) as the target set to find",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "Since SHAP weights show how much a token, or word, usually affects the outcome when it's hidden, we also pay attention to words that tend to behave unusually on average.",
        "formal_text": "Because SHAP weights reflect the average effect of masking a token t, we also focus on word features that are abnormal on average.",
        "GPT2_formal_text": ", attn_ids, and comparator_ids to figure out how important each attribute is in the query. Formal: The tfIdf vector from the vector space model helps us figure out the relevance of each attribute in the query. Formal: The tfIdf vector from the vector space model is used to predict the relevance of each attribute in the query. For example, the relevance of \"location\" is figured out like this: Formal: Here's how the mapping between query and attribute relevance works: Formal: When we apply the tfIdf vector to the query, we get a probability that shows how likely it is that the attribute is relevant to the query. Formal: So, the tfIdf vector tells us how relevant the attribute is to the query. Formal: We pick a query vector q that has the most attribute relevance and a query vector q_i, which is the vector from the query that has the most relevance to the attribute. Formal: Then, we calculate the relevance of the query vector q_i using the tfIdf vector q_i. Formal: In this paper, we focus on finding the best relevance vector q_i. Formal: So, we pick the query vector q that has the most relevance to the attribute, and we calculate the relevance of the query vector q_i using the tfIdf vector q_i. Formal: We can also calculate the relevance of the query vector q_i by multiplying the tfIdf vector q_i with the relevance vector q_i. Formal: Formal: The tfIdf vector q_i is used to predict the relevance of the attribute in the query. Formal: Formal: To make sure we're comparing the relevance vectors, we use a linear transformation to make the relevance vector q_i more uniform. Formal: The relevance vector q_i is always more relevant to the query than the query vector q_i, no matter how much the query vector q_i changes. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "Schalte einfach zwischen den Schnappschüssen hin und her, bis du die richtigen Einstellungen gefunden hast.",
        "formal_text": "wechseln Sie zwischen den Schnappschüsse, um die gewünschten Einstellungen zu finden.",
        "GPT2_formal_text": "ed is the final result we get. Formal: Basically, in equation (1), we're saying that the probability of a sequence happening is calculated based on the context vector w_c. Formal: The hidden state h_c is what we use to figure out the attention vector, and it's calculated using this formula: Formal: For every sentence s_i in the training set, we calculate the log-likelihood of the code snippet snippet_c using the equation: Formal: We take the attention vector h_c from equation (1) and plug it into the linear transformation (3) to get the final attention vector h_c for the code snippet snippet_c. Formal: This gives us the prediction for the sequence snippet_c. Formal: Formal: We also have a classifier C(s_i) that predicts the code snippet's label. Formal: Using this classifier, we create a basic method for classifying code snippets. Formal: We use a linear transformation (3) to handle the vector representation of code snippets. Formal: We use a linear transformation to learn the attention vector h_c from equation (1). Formal: The code snippet label h_c is then calculated using the classifier's prediction. Formal: To handle the class label prediction, we use the forward seq2seq decoder (S2S) decoder. Formal: Using the forward seq2seq decoder, we train the code snippet classifier. Formal: Lastly, we update the attention vector h_c by applying the classifier's prediction. Formal: Formal: Formal: Finally, we calculate the attention vector h_c using equation (1). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.267.json"
    },
    {
        "casual_text": "Our algorithm has three main steps:  \n\n1. First, we assign each instance a probability distribution of possible word replacements based on a neural biLM (check out section 2.1 for details).  \n2. Next, we pick k \"representatives\" for each instance. Each of these representatives is made up of multiple samples from the word replacement distribution we just talked about (section 2.3 explains this).  \n3. Finally, we group the representatives into clusters and use those clusters to create a soft-clustering for the instances (you can find more in section 2.4).",
        "formal_text": "Our algorithm works in three stages: (1) We first associate each instance with a probability distribution over in-context word-substitutes. This probability distribution is based on a neural biLM (section 2. 1). (2) We associate each instance with k representatives, each containing multiple samples from its associated word distributions (section 2. 3). (3) Finally, we cluster the representatives and use the hard clustering to derive a soft-clustering over the instances (section 2. 4).",
        "GPT2_formal_text": "= {l_i: 1, l_j: 1}  \n\nBoth an entity and its attribute, which can be either a single word or a compound word, are considered a type. When training the model, we combine the hidden representation h_t ∈ R d×d by using the softmax function. Formal: The model predicts the corresponding labels for the different types of entities and attributes. Formal: In this setup, we define a type as a set of input features that are related to the entity and attribute types, and they're connected by a few special connections. We also define a few extra terms for this specific setup. Formal: To create the final classifier, we use a fully-connected layer and a softmax function. The input feature h_t is set to the sum of all the features from the input sentences, and the final output is the conditional probability that the input is a type. Formal: When dealing with relation extraction, the model tries to figure out the relation between two entities and their attributes. To do this, it takes two input features and combines them to calculate the relation term. Formal: Let's say the relation term is calculated by combining the two input features together, like this: Formal: A single instance is a type when it's in the input sentence. A compound instance is a type if both its input and its output are type instances. Formal: We also have a few extra terms for this specific model. Formal: We can split a sentence into multiple instances with a given label. Formal: For a specific classifier, we define a set of input features. Formal: In this case, we just use the same set of input features as before. Formal: We also define a few extra terms for this specific model. Formal: Formal: We can split a sentence into multiple instances with the same label. Formal: Formal: We can split a sentence into multiple instances with the same label. Formal: Formal: Formal: We can split a sentence into multiple instances with the same label. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1523.json"
    },
    {
        "casual_text": "Another area of research involves predicting the overall result of a case. For example, you might try to guess which side the outcome will favor (like Aletras et al. did in 2016), or whether the current court will uphold or overturn a decision made by a lower court (as Katz et al. explored, see Figure 1 for an example). Unlike them, we’re taking it a step further. Instead of just predicting a simple binary outcome (or including an \"other\" category), we’re focusing on the specific details of the case, like the charges involved. This means our output can have multiple labels.",
        "formal_text": "Another related thread of work is to predict the overall outcome of a case. The target can be which party will the outcome side with (Aletras et al. , 2016), or whether the present court will affirm or reverse the decision of a lower court (Katz et al. , Figure 1: An example judgement document excerpt of a criminal case in our dataset. Names are anonymized as AA and BB. Rectangulars, ellipses and dashed rectangulars refer to the clauses that usually indicate the beginning of the facts part, the court view part and the decision part, respectively. Articles and charges are extracted with regular expressions and a charge list. 2016). Our work differs from them in that, instead of binary outcome (the latter one also contains an other class), we step further to focus on the detailed results of a case, i. e. , the charges, where the output may contain multiple labels.",
        "GPT2_formal_text": "_tokens = [h_tokens] [h_mask] [p_mask] [s_mask] [p_mask] [mask] [mask_tokens] [mask_mask_tokens]. Formal: We also tried training the model using a bunch of different input features, and the results are in Table 2. From Table 2, you can see that using different input features really helps improve performance. So, we think it's worth it to use multiple feature features in a model to make it more accurate. Formal: Here, we're looking at how the model performs when it's not using any knowledge. We're checking out the performance without any knowledge loss or using knowledge loss. Formal: We also found that just using the model's predictions without any knowledge loss gives pretty good results. Formal: Here's what we've done differently compared to the RNN-based model in the RNN-based Transformer model. Formal: We added a pre-trained GPT-2 model that was trained on the Wikipedia dataset, so it has more data to work with. Formal: We ran GPT-2 without any knowledge loss on the test set, and it performed really well on the test set. This shows that having a knowledge-based model is really helpful. Formal: We also added a knowledge-aware attention mechanism to GPT-2, which helps it understand the input better and give better predictions. Formal: We took out the knowledge loss from GPT-2 to make it more robust. Formal: We used the knowledge-aware attention mechanism to adjust the loss for the GPT-2 model. Formal: Lastly, we tested our model on the test set of the RNN-based Transformer model, which has a different setup compared to ours. Formal: As you can see in Table 2, our model performs just as well as the RNN-based model without knowledge loss or knowledge loss on the test set. Formal: For future work, we plan to add more losses to make the model more robust. Formal: We also added a pre-trained GPT-2 model that was trained on the Wikipedia dataset, so it has more data to work with. Formal: We ran GPT-2 without any knowledge loss on the test set, and it performed really well on the test set. This shows that having a knowledge-based model is really helpful. Formal: Formal:",
        "directory": "D17",
        "filename": "D17-1289.json"
    },
    {
        "casual_text": "Let’s call LCLR the LC transform with some tweaks, so it only works on nonterminals that are left-recursive. The fifth row in Table 4 shows what happens when we use LCLR on the three original grammars. LCLR shrinks the non-left-recursive parts of the CT and ATIS grammars a lot, but for the PT grammar, it only makes a small difference. This makes sense if you look at Table 1, because almost all the rules in the PT grammar are for left-recursive nonterminals. But, we can do some extra steps, like what we did with Paull’s algorithm, to cut down the number of rules for left-recursive nonterminals before using our modified LC transform. The sixth and seventh rows in Table 4 show what happens if we left factor the grammar before using LCLR (LF+LCLR), and also if we group non-left-recursive rules for left-recursive nonterminals before using LCLR (LF+NLRG+LCLR).",
        "formal_text": "Let LCLR refer to the LC transform restricted by these modifications so as to apply only to leftrecursive nonterminals. The fifth line of Table 4 shows the results of applying LCLR to the three original grammars. LCLR greatly reduces the size of the non-left-recursive forms of the CT and ATIS grammars, but the size of the non-left-recursive form of the PT grammar is only slightly reduced. This is not surprising if we note from Table 1 that almost all the productions of the PT grammar are productions for left-recursive nonterminals. However, we can apply the additional transformations that we used with Paull's algorithm, to reduce the number of productions for left-recursive nonterminals before applying our modified LC transform. The effects of left factoring the grammar before applying LCLR (LF+LCLR), and additionally combining non-left-recursive productions for left-recursive nonterminals between left factoring and applying LCLR (LF+NLRG+LCLR), are shown in the sixth and seventh lines of Table 4.",
        "GPT2_formal_text": ", so we don't need to create the same set for every word. This is because the sequence of words we're looking at will likely be longer than the input text. We also try to make sure that the sentence gets paraphrased by the same two or more people. If there's more than one paraphraser, we pick the one that gives us the longest sentence. Formal: Okay, so for each word, we calculate the similarity score by taking the average of the embeddings from its position embedding to its position embedding. Formal: For the N-best list, we set all the embeddings to their default values, which is 0.00001, to make sure the scores don't get messed up by the number of words. Formal: To check how well our paraphrase-based model works, we compare it to a baseline model that doesn't use the input text. Formal: We use the accuracy of the paraphrase-based model to figure out how many correct paraphrases there are. We measure this accuracy using the Bleu score (Papineni et al., 2002). Formal: We also calculate the F1-score for the baseline model, and then we calculate the WSS score for the paraphrase-based model. Formal: For the F1-score, we look at the F1 score of the baseline model if it uses the input text. If it doesn't, we use the F1 score of the paraphrase-based model. Formal: Formal: To figure out how many paraphrases are in the reference set, we calculate the F1-score for each paraphrase-based model. Then, we average these scores across all the references in the reference set. Formal: Finally, we compare the WSS score of the baseline model to the WSS score of the paraphrase-based model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "A00",
        "filename": "A00-2033.json"
    },
    {
        "casual_text": "We calculate the conditional probability vectors P[+c91zi] and P[zll + sl, +s2] using the disjunctive interaction model: 4 3. A natural language processing system also needs to spread out probabilities to the semantic hypotheses so it can work with the interpreted information.",
        "formal_text": "The conditional probability vectors P[+c91zi] and P[zll + sl, +s2] are computed using the disjunctive interaction model: 4 3A natural language processing system needs to propagate probabilities to the semantic hypotheses as well, in order to make use of the interpreted information.",
        "GPT2_formal_text": "and response_mask outputs are combined into a single feature, which then gets passed to the encoder layer. The encoder layer has a hidden layer size of 300. We use the Adam optimizer with a learning rate of 0.001, and the regularization parameter is set to 0.00001. The network structure is laid out in Figure 2. \n\nFor the final layer of our model, we take the output from the encoder and feed it through the contextual representation layer, which has a hidden layer size of 200. The representation layer has a 2D window size, which is just the square of the window size of the input image. The actual value for the output is 0. The input image is represented as h_input. For this layer, we add a linear layer with a dropout rate of 0.2, a ReLU activation, and a magnitude threshold of 10^-4. The final hidden representation for the token x is written as h_input x. \n\nThe output from the last hidden layer is just the token itself, with the same size of 300. The formula for the attention weight vector is:\n\nAttention vector = [Attention weight vector for the token (x_input)] + [Attention weight vector for the input image (x_input)] + [Attention weight vector for the entire sequence (y_i)]\n\nOur model is built using PyTorch version 1.2.0, with a batch size of 32, and we use Adam, a stochastic optimization method with a learning rate of 0.001. The model runs in linear time (like 10^-8) and is optimized using the pytorch learning rate algorithm (thanks to Radford et al., 2014). We train it for 3 epochs with a batch size of 32. \n\nWe train all these models using a single-layer BiLSTM model. Formal: When we combine these attention-based classifiers, they help us predict the next token more precisely. Formal: Lastly, we combine the output from each attention-based classifier to create the token's representation. Formal: Lastly, we combine the output from each attention-based classifier to create the token's representation. Formal: To train the BiLSTM model, we use the same setup as mentioned in section 3.2. Formal: To train the PGRU model, we just copy the same setup as in section 3.2.",
        "directory": "C90",
        "filename": "C90-2071.json"
    },
    {
        "casual_text": "Equation (9) gives the recognition result in a language with lots of resources. If the system needs results in a language with fewer resources, we should use Equation (10) instead. In Eq. (10), π is a projection operator (from Mohri, 2009) that changes the input label to the output label. Before decoding, the recognition transducer, called ASR, can be made better by doing a determinization operation right after each step.\n\nCheck out Figure 3 for an example of how to build a cross-lingual language model using WFSTs: \n- First, a word sequence like {w1, w2, w3} is represented by the Lw language model G (b1).\n- Then, it's split into a phrase sequence like {w1, w2 w3} (b2).\n- Next, {w1, w2 w3} gets reordered to {w2 w3, w1} (b3).\n- After that, the phrase w2 w3 is changed to v2 v3 (b4).\n- Finally, v2 v3 is put back into a word sequence {v2, v3} (b5).\n\nHere, wk and vk stand for wk and vk, and \"-\" means the ǫ or null symbol. Extra symbols like #1, #2, etc., are added to make the WFST work better (as explained by Mohri, 2009). This helps the transducer get optimized by a determinization operation, which makes the search network smaller and more efficient.",
        "formal_text": "Eq. (9) outputs the recognition result in a resourcerich language. If recognition system requires recognition outputs in a resource-poor language, then the search space should be constructed as Eq. (10), where π is a projection (Mohri, 2009) operator which projects the input label to the output label. Before decoding, the recognition transducer ASR can be optimized by a determinization operation right after each composition. Figure 3: Illustration of constructing a cross-lingual language model via WFSTs: a word sequence {w 1, w 2, w 3 } represented by the L w language model G (b1) is segmented into a phrase sequence {w 1, w 2 w 3 } (b2); {w 1, w 2 w 3 } is reordered into {w 2 w 3, w 1 } (b3); phrase w 2 w 3 is transduced to v 2 v 3 (b4); phrase v 2 v 3 is reconstructed into a word sequence {v 2, v 3 } (b5). wk and vk represent w k and v k, respectively. \"-\" refers to ǫ or null symbol. Auxiliary symbols #1, #2, • • • are used to make the WFST determinizable (Mohri, 2009) such that the transducer can be optimized by a determinization (Mohri, 2009) operation which significantly reduces the search network size.",
        "GPT2_formal_text": "= k, ..., {k_t} - m. The k-th relation is then ranked using a maximum-likelihood estimate, and the rest are treated as null relationships. Formal: The main goal here is to create a sentence that starts with the given title and ends with the main claim. Formal: The goal here is to turn the title and claim into a sequence of words, based on a given context and a set of rules. Formal: Let’s take the title and claim sentences from the example above and rewrite them in a more formal way. Formal: So, the title is made up of words like \"Microsoft\", \"its\", \"UK\", and \"research\", and the claim is made up of words like \"Research\", \"Microsoft\", \"UK\", and \"UK\". We’ll call the sentences like these the title and claim, where the title is the main part and the claim is the supporting part. We’ll use the same set of rules to handle both of these parts. Formal: In our example, we’ll also use the rule that the title and claim have a common part (called a \"basic entity\") and have the same or different parts (called \"parts\"). The title and claim are basically the same, and the rules that control how they connect can be written in a similar way. Formal: In our system, we’ll use a mix of rule-based and statistical methods to deal with this. Formal: In our example, the title and claim sentences would be translated into English using an English-based system (like the one described by Jelinek and Marcu in 2000). After that, they’ll be translated into Czech using the Czech-based system mentioned by Kondrak and Vossen in 2004. Formal: Formal: After that, the title and claim sentences would be sent to a second-order SMT (SMT2) system (developed by He et al. in 2003) to be converted into the target language. Formal: Lastly, the title and claim sentences would be sent to a third-order SMT (SMT3) system (also developed by He et al. in 2003) to be converted back to English. Formal: Formal: Formal: Basically, the goal here is to make sure the title and claim sentences are translated correctly and that the sentences end with the right words, so the sentence can be accurately matched with the title",
        "directory": "D12",
        "filename": "D12-1070.json"
    },
    {
        "casual_text": "Adult -----Cost -----------Double Room -------------Single Room ------------Extra Bed",
        "formal_text": "Adult -----Price -----------Double Room -------------Single Room ------------Extra Bed",
        "GPT2_formal_text": "ed() method. This technique is mostly used in NER and classifying text into different types, as mentioned in (Zhou et al., 2019; Wu et al., 2020). It's also been used to detect sarcasm in multi-turn conversations (Zhang et al., 2019). The whole setup is shown in Figure 1. Formal: We calculate the loss for a batch of generated tokens x = {x1, ..., x|x|}. Formal: The losses for the last five turns are L_5 * R^2 = 4E-5. We also have an overall loss for the whole dataset, which is called L_O. Formal: We use the adversarial loss to train the model. Formal: We train the model using a cross-entropy loss function (η). Formal: For the second round, we use the adversarial loss again, but this time we tweak the weights to keep the model stable. Formal: We apply label smoothing during training. Formal: We calculate the loss for each token x, which is the cross-entropy loss for that token. Formal: We also calculate the loss for the last five turns, which is the cross-entropy loss for that last five turns. Formal: Formal: For the second round, we use the adversarial loss again, but this time we tweak the weights to keep the model stable. Formal: Formal: We apply label smoothing during training. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C00",
        "filename": "C00-1025.json"
    },
    {
        "casual_text": "We use Google Translator to translate our training data, D src, from English into the target languages. Besides just translating the text, we also need to figure out how to map the slot labels to the target language. To do this, we tried two methods: giza++ (Och and Ney, 2003) and fastalign (Dyer et al., 2013). After testing, we found that giza++ gives us better results—it improves performance by around 2% in F1 score on the SNIPS dataset. So, for the rest of the paper, we stick with Google Translator and giza++ to handle the translation and label mapping. We'll call the translated training data D trans.",
        "formal_text": "We use Google Translator to translate the training corpus D src in the source language (English) to the target languages. In addition to translation, we also need some word alignment methods to project the slot labels to the target language side. We try giza++ (Och and Ney, 2003) and fastalign (Dyer et al. , 2013) to obtain word alignment information and find that the pseudo slot labels projected by giza++ generally lead to better performance (about 2% increase in F1 on the SNIPS dataset). Thus, in the rest of the paper, we use Google translator and giza++ to produce translated data. Denote by D trans the translated training corpus.",
        "GPT2_formal_text": ", k, s). Formal: The labels are all in the same format, so the two documents won't be mixed up. Formal: Our model gets rid of the word duplication stuff by running it through a linear layer with a ReLU activation, with softmax hidden vectors. Formal: The model creates a vector that shows how often each character appears. Formal: The model uses all the characters in the document to figure out the similarity score between the document's characters and the characters in the prompt. Formal: The model uses the character similarity scores from the prompt and the document to predict the label. Formal: The model uses the context of the document and the prompt to pick the best response. Formal: The model uses word embeddings from the prompt and the document to predict the label. Formal: The model uses embeddings from both the prompt and the document to figure out the similarity score between the document's characters and the characters in the prompt. Formal: Our model uses the character similarity scores from the prompt and the document to predict the label. Formal: The model uses the contextual embeddings from both the prompt and the document to predict the label. Formal: The model uses the context embeddings from both the prompt and the document to predict the label. Formal: The model uses embeddings from both the prompt and the document to predict the label. Formal: The model uses the embeddings from the prompt and the document to predict the label. Formal: The model uses the embeddings from both the prompt and the document to predict the label. Formal: The model uses embeddings from the prompt and the document to predict the label. Formal: The model uses embeddings from both the prompt and the document to predict the label. Formal: The model uses embeddings from both the prompt and the document to predict the label. Formal: Formal: The model uses embeddings from both the prompt and the document to predict the label. Formal: Formal: The model uses embeddings from both the prompt and the document to predict the label. Formal: Formal: Formal: The model uses embeddings from both the prompt and the document to predict the label. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.259.json"
    },
    {
        "casual_text": "The first change we're making is adding some syntactic features to the CCG categories. Normally, in basic CCG trees, a nominal adjective (like \"a tall boy\") is labeled as N/N, and a predicate adjective (like \"John is tall\") is labeled as S adj \\N P. To make the meaning of both types of adjectives consistent, we're changing N/N to N adj /N for nominal adjectives.",
        "formal_text": "The first modification is to add syntactic features to CCG categories. For example, in the default CCG trees, a nominal adjective (a tall boy) has the category N/N, while a predicate adjective (John is tall) has the category S adj \\N P. To provide a uniform degree semantics to both constructions, we rewrite N/N as N adj /N for the category of nominal adjectives.",
        "GPT2_formal_text": "ed; type_likelihood; entity_kind; entity_type; entity_type_likelihood; entity_type_with_likelihood; entity_type_with_likelihood_masked; entity_kind_likelihood; entity_kind_with_likelihood; entity_type_masked; entity_type_masked; entity_kind_masked; entity_type_masked; entity_type_with_masked; entity_type_with_masked; entity_type_masked; entity_kind_masked; entity_kind_masked; entity_type_masked; entity_kind_with_masked; entity_type_with_masked; entity_kind_masked_with_likelihood; entity_kind_masked_with_likelihood; entity_kind_masked_with_likelihood_masked; entity_kind_masked_with_likelihood_masked; entity_kind_masked_with_likelihood_masked_with_likelihood; entity_kind_masked_with_likelihood_masked_with_likelihood; entity_kind_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood; entity_kind_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood; entity_kind_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood; entity_kind_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood; entity_kind_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood; entity_kind_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood; entity_kind_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood_masked_with_likelihood",
        "directory": "acl",
        "filename": "2020.acl-srw.35.json"
    },
    {
        "casual_text": "For the Machine Reading Comprehension task, we use the RACE dataset (Lai et al., 2017), which is a big dataset made from English exam questions. It has 25,137 passages and 87,866 questions. Each question has four possible answers, but only one is right. The dataset is split into RACE-M and RACE-H, with questions for middle school and high school students, respectively.",
        "formal_text": "For the Machine Reading Comprehension task, we evaluate on RACE (Lai et al. , 2017), a largescale dataset collected from English exams, containing 25, 137 passages and 87, 866 questions. For each question, four candidate answers are pro-2 https: //data. quora. com/First-Quora-Dataset-Release-Question-Pairs 3 The dataset is derived from Stanford Question Answer Dataset (SQuAD). vided, only one of which is correct. The dataset is further divided into RACE-M and RACE-H, containing exam questions for middle school and high school students.",
        "GPT2_formal_text": "= { 'user', 'query', 'parameter' } from d)) d = {'user', 'query', 'parameter', 'params' } Formal: So, we’re dealing with a mix of user and query info here. This is related to DIV-FACTORIZED, which is the goal of the IR model we’re working on. We’re also comparing it to the traditional unigram model, which is the one from Dagan et al. (2007). We’re also including the fact that this model can be trained on different datasets with varying amounts of data. Formal: In this paper, we’re introducing a new IR model that works with dynamic models, like RNNs and CNNs, and even better, it can handle a mix of user and query info. Formal: We’re building on the unigram model from Dagan et al. (2007), which is a graph-based model, and we’re using a different approach to deal with this mix of user and query info. Formal: We’re using two methods: one uses a convolutional neural network (CNN) to learn features, and the other uses a BERT-based model to handle the combined user and query info. This way, we’re capturing more of the user’s and query’s context. Formal: We’ve added some new features that help improve the user and query info by looking at their relationships. Formal: Also, we’ve tweaked the model’s setup to work with other types of information, like dictionaries, encyclopedias, and Wikipedia. Formal: We’ve included some results from testing different data setups and models in Section 4. Formal: Formal: To train the model, we’re using the Naive Bayes classifier from Barzilay and Lapata (2008). Formal: We’ve also created a pre-trained model, which we’ll call BERT-COREF, to handle all the user and query info. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-1441.json"
    },
    {
        "casual_text": "[7] Body Involvements This part talks about the main body parts that are usually involved when doing something. Like, if you're \"opening\" something physically, you're probably using your \"hands\" and \"arms.\" We've got five groups for this: head, arms, torso, legs, and other body parts.",
        "formal_text": "[7] Body Involvements This attribute group specifies prominent body parts involved in carrying out the action. For example, \"open\" typically involves \"hands\" and \"arms\" when used in a physical sense. We use five categories: head, arms, torso, legs, and other body parts.",
        "GPT2_formal_text": "_t l f(y, t) = x + f(y)  (2) Formal: This setup is pretty similar to the one in (Kim et al., 2017a). The key difference is that instead of calculating a single attention for each word in the passage, we calculate a bunch of attention masks for each word in the passage. This way, we're looking at all the words in the passage together and paying attention to all the attention masks. Formal: Here's the attention mask matrix for the passage, which is h_t. Formal: The attention mask matrix for the whole passage, called h_t. Formal: This is calculated using a recursive neural network, which was introduced by Socher et al. (2013). Formal: We also have a bias vector called b_t that shows how much the attention weights are spread out. Formal: Following (Kim et al., 2017b), we use a fully connected layer with g_t = 1 for the first and then a second fully connected layer with g_t = 2 for the second and third layers. Formal: The final embedding for the generated text, which we call h_t_t. Formal: Finally, we use a softmax function to figure out the probability distribution for the attention mask. Formal: Let's say we have a passage x = {x1, ..., xn}, and the input word y = {y1, ..., ym}. Formal: We figure out the attention mask for the passage by adding up all the attention weights. Formal: The attention mask matrix for the whole passage, h_t. Formal: The final embedding for the generated text, h_t_t. Formal: Finally, we use a softmax function to figure out the probability distribution for the attention mask. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D17",
        "filename": "D17-1099.json"
    },
    {
        "casual_text": "There are more advanced techniques like word embeddings (Mikolov et al., 2013) and topic modeling algorithms (Blei et al., 2003) that can help figure out how often certain topics show up in documents and how words are connected, even if those connections change over time. Then, there are transformer models like BERT (Bi-directional Encoder Representations from Transformers) (Devlin et al., 2019), which have taken text processing to a whole new level. These models are designed to categorize text, summarize it, or even answer specific questions based on the input they get.",
        "formal_text": "More computationally sophisticated methods such as word embeddings (Mikolov et al. , 2013) and topic modeling algorithms (Blei et al. , 2003) provide the capability to measure prevalence of topics within documents, as well as the relationships between words and how they may shift over time. The development of transformer models such as BERT (Bi-directional Encoder Representations from Transformers) (Devlin et al. , 2019) have opened a new frontier of text processing, with models trained to categorize, summarize or answer specific questions from input text.",
        "GPT2_formal_text": "is an example of a sentence that's missing some information. If you give it some context, it might help you figure out the missing part of the sentence. Formal: To make it easier to understand the results of the basic model, we’ll talk about how the average error and the variance of the models are calculated in Section 4. Formal: If we have a sentence, a topic, a vocabulary, and a label, the model figures out the best way to combine these things to find the best overall translation. Formal: Formal: We’ll use the idea of semantic distance to compare the output of the model with the output of a human translator. We’ll call this similarity measure semantic_distance_v. \n\nFor our purposes, we’ll define semantic distance like this: Formal: Let’s say f is the model’s output when we’re trying to match up sentences with topics. Formal: For the same sentence, topic, and label, the model’s output f_i,j will have the same meaning. Formal: To figure out the similarity measure, we just average the output of the model f_i,j and the output of the human translator f_i,j, assuming f_i,j is at its best. Formal: For the topic, we’ll look at the model’s output f_i,j, which is the same as the output of the human translator f_i,j. Formal: Formal: Let’s say α is the most important similarity measure we’re looking at, and we’ll calculate it using Formal: We’ll use the value of α to rank the output of the model f_i,j from highest to lowest. Formal: Formal: If the output f_i,j is at its best, α will be 1, meaning the model is performing perfectly. Formal: Formal: Formal: Now, let’s calculate the average error for our model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "deeplo",
        "filename": "2022.deeplo-1.5.json"
    },
    {
        "casual_text": "The ALSC and ACD tasks both use the same attention mechanism, but they don’t share any parameters. The reason for keeping them separate is that, for the same aspect, ALSC pays more attention to opinion words, while ACD is more focused on the actual aspect target terms. You can check out the attention visualizations in Section 4.6 to see this in action.",
        "formal_text": "The ALSC and ACD tasks use the same attention mechanism, but they do not share parameters. The reason to use separated parameters is that, for the same aspect, the attention of ALSC concentrates more on opinion words, while ACD focuses more on aspect target terms (see the attention visualizations in Section 4. 6).",
        "GPT2_formal_text": "(s_a) = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 0.10 }, \n\nA_i = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 }, \n\nT_i = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 }, \n\nS_i = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 }, \n\nu_i = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 }, \n\nv_i = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 }, \n\nw_i = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 }, \n\nβ_i = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 }, \n\n} Formal: Here, A_i and T_i are both made up of vectors. The loss function L(A_i, T_i) is defined based on these vectors. Formal: In this setup, we only use the attention features in the source and target domains, but we also use their domain-specific weights. Formal: The model is trained using a regular classification method, which we'll explain more later. Formal: After training, we use the model to spot and remove spam in the target domain. Formal: To stop the model from overfitting to the target domain, we introduce a softmax function. Formal: After training, we use the model to spot and remove spam in the target domain. Formal",
        "directory": "D19",
        "filename": "D19-1467.json"
    },
    {
        "casual_text": "Both methods use a weighted Jaccard metric with mutual information vectors to figure out how similar things are. If you're curious about other ways to measure this, check out the work by Weeds and Weir from 2005.",
        "formal_text": "Both measures use a weighted Jaccard metric on mutual information vectors to calculate the similarity. SeeWeeds and Weir (2005) for an overview of other measures.",
        "GPT2_formal_text": "= {h t } for each t in the set {w t }. Formal: To break down the data into smaller pieces, we start by creating a matrix called U t ∈ R d×n that keeps track of the n-gram features of each event. Formal: We take the example from Figure 2 where an event e i has a length of n. To get the first n-gram features, we use a BiLSTM to get a vector representation of the word sequence for each word in the event. Formal: The pre-trained model we’re talking about here works by using a sequence-to-sequence model. It uses a BiLSTM encoder, which takes in the event vector h t and some surrounding context info, like the previous event c t−1, the word sequence w t−1, and the current word, s t. After that, a Bidirectional Long Short-Term Memory (BiLSTM) decoder uses this learned vector to spit out the next word, v t−1. Formal: We use the same setup as in Equation 1, but instead of using the convolutional layer, we switch to a three-layer BiLSTM to handle the n-gram features. Formal: For the n-gram features, we use a BiLSTM (from Bahdanau et al., 2014) to grab the vectors h t, w t, and s t. Formal: To get the length of the event, we calculate the log-likelihood of the event vector h t. Formal: Once we’ve got the event vectors, we use the same three-layer BiLSTM to predict the next word, v t−1, in sequence. Formal: To make this model better at handling long events, we add an attention mechanism to it. Formal: This attention mechanism helps the model focus on the important words in the next sentence, which then helps the model generate the next word, v t−1. Formal: Formal: Next, we use a three-layer BiLSTM to figure out the important word. Formal: Finally, we use the first and last hidden states from the three-layer BiLSTM to build a representation for the event vector h t. Formal: To put it simply, this process keeps going until all the important words in the next sentence are in the right spot. Formal: The final",
        "directory": "D07",
        "filename": "D07-1052.json"
    },
    {
        "casual_text": "In this paper, we introduce a multitask active learning approach for NLU that leverages the relationship between ID and SF. We incorporate popular pool-based query strategies like Least Confidence, Margin Sampling, and Entropy into our framework. We also optimize the calculation for the entropy of a joint-model. The experiments show that our framework, combined with these query strategies, performs well with less training data compared to baseline methods across all datasets. The results also highlight that using the relationship between tasks can lead to better performance than just focusing on intents. Plus, our framework remains effective even when the model is updated. These findings suggest that the framework could be a good fit for real-world applications.",
        "formal_text": "In this paper, we propose a multitask active learning framework for NLU that focuses on making use of the relation information between ID and SF. We implement representative pool-based query strategies that include Least Confidence, Margin Sampling and Entropy in our framework. We also perform an efficient computation for the entropy of a joint-model. Experimental results show above query strategies with our framework can achieve competitive performance with less training data than baseline methods on all datasets. The results also demonstrate that making use of the relation information between tasks may achieve better performance rather than only consider intents. Additionally, our framework is still useful when the model is changed. These results suggest that the framework has the potential to be applied for industrial use.",
        "GPT2_formal_text": "ed = {0, 1, 2, 3}  \n\nThis setup uses a bunch of parameters to measure how important each word is. Formal: The tf-idf vector is then turned into a normalized form, which we call tfidf. Formal: The word vector z_t, which is a representation of the entity, gets passed through the attention module. Formal: The last three parts are combined to get the final word representation, which we call w_t. Formal: To keep things simple and efficient, we only use one set of parameters for the attention modules. Formal: But if the entity isn't on the input image, the attention module can focus on other parts of the entity. Formal: In the second stage, we combine the representations from all the different layers, taking the average of each word's vector. Formal: Next, we use a dynamic programming algorithm to calculate the final representation, which we call h_t_i. Formal: This setup is similar to the one used in Kipf and Welling's 2016 work, but the focus in that setup is on figuring out the attention weight. Formal: Formal: In the third stage, we use the results from the attention module to figure out the hidden representation of the entity. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.430.json"
    },
    {
        "casual_text": "Before talking to someone, it's smart to think about what they already know. This way, you can use words or ideas that make sense to them and are more helpful. For example, if you're talking to a local or someone who's been to a place before, they might know certain landmarks that you can mention to explain where something is. Also, people might have different abilities, like how well they see or hear, so it's good to keep that in mind too when you're chatting with them.",
        "formal_text": "Speakers check upon information about the recipient because the communication partner's knowledge may render some formulations more relevant than others. As a 'member' of a particular class of people, such as the class of locals, or of the class of those who have visited the place before, the addressee may be expected to know some landmarks that the speaker may use for spatial description. Membership groups may also include differentiation according to capabilities (e. g. , perceptual) of the interlocutors.",
        "GPT2_formal_text": ", out_ids, and out_tokens. Formal: We calculate the attention weights for both the input and output vectors at the same time using a bilinear function. Formal: To combine the attention weights from the input and output vectors, we use a weighted weighted sum (W_A + W_X + W_Y) with the inputs and outputs, which we call A_A and A_X, respectively. Formal: For combining the attention weights from both the input and output vectors, we use a linear weighted sum (W_A + W_X + W_Y + r) with the inputs and outputs, which we call A_A and A_X. Formal: To figure out how important each token in the input is, we use a linear weighted least square loss (W_A + W_X + W_Y) with the tokens in the input. Formal: We calculate the loss function L_w and the loss function L_j by running them through a linear layer with a window size of [d_w, ..., d_j] and then scaling the parameters using gradient descent with the Adam optimizer. Formal: We calculate the final attention weight, p_i, by running it through a linear layer with a window size of [d_i, ..., d_j]. Formal: Formal: Formal: We calculate the output representation (r) for the input using the Adam optimizer, with a fixed learning rate α. Formal: The output representations are then sent to the output layer, which is a bidirectional LSTM with hidden dimensions d_d. Formal: We also calculate the loss for the output layer, which is a linear weighted least square loss (W_A + W_X + W_Y + r). Formal: Formal: Finally, we add the last part of the loss to the model's parameters, which is the same linear weighted least square loss (W_A + W_X + W_Y). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E06",
        "filename": "E06-1024.json"
    },
    {
        "casual_text": "Besides the model structures, stuff like pretrained word vectors, the way we label things, and the optimizer can really affect how well the system works. We looked at a bunch of these external factors using the two top models on a NER dataset: CLSTM+WLSTM+CRF and CCNN+WLSTM+CRF. \n\nPretrained embeddings. Figure 4(a) shows how the two best models performed on the NER test set with two different types of pretrained word vectors, plus when they used random initialization. Using pretrained embeddings gave a big boost in performance compared to random initialization (p < 0.01). The GloVe 100-dimension embeddings scored higher than SENNA (from Collobert et al., 2011) on both models, which matches what Ma and Hovy (2016) found.",
        "formal_text": "In addition to model structures, external factors such as pretrained embeddings, tag scheme, and optimizer can significantly influence system performance. We investigate a set of external factors on the NER dataset with the two best models: CLSTM+WLSTM+CRF and CCNN+WLSTM+CRF. Pretrained embedding. Figure 4(a) shows the F1-scores of the two best models on the NER test set with two different pretrained embeddings, as well as the random initialization. Compared with the random initialization, models using pretrained embeddings give significant improvements (p < 0. 01). The GloVe 100-dimension embeddings give higher F1-scores than SENNA (Collobert et al. , 2011) on both models, which is consistent with the observation of Ma and Hovy (2016).",
        "GPT2_formal_text": "_t for the variable x. Formal: In this setup, we're using the two-layer attention mechanism that's been introduced by Vaswani et al. in 2017. It's designed to handle multiple attention scores at the same time. Formal: Since it's a two-layer thing, the attention weights for each token x in the input x are spread out across both the encoder and decoder layers. Formal: We get the attention weights θ_a(x) by using the tanh operation on the attention matrix, which gives us... Formal: So, we can simplify the whole attention mechanism to: Formal: Here's how the attention weights are set up: Formal: The attention weights θ_a(x) are calculated using the tanh operation on the attention matrix. Formal: The parameters θ_a are learned during training using supervised training, and we fine-tune them using a trainable loss function. Formal: Training with cross-entropy loss (as suggested by Artetxe et al. in 2017) helps the model figure out the best parameters θ_a. Formal: In this case, we're only using two-layer attention, so we're using the two-layer attention mechanism. Formal: We're using the DenseNet model from the Hugging Face library. Formal: The DenseNet model is trained using an objective function that works with Equation 1. Formal: The loss function we're using is cross-entropy loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C18",
        "filename": "C18-1327.json"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. We've got a variable x in a set called X. Now, when you see x with a number as a subscript (like x1) on the right side of a rule, it's referring to a specific part of x. But if you see x with a letter as a subscript (like xi), it's just a general placeholder, not pointing to any particular part of x.",
        "formal_text": "x an arbitrary variable in X. Because numerically subscripted versions of x appear explicitly on the right hand side of the rule defining variables, numerically subscripted variables (e. g. , x 1 ) on the right-hand side of all rules are taken to refer to the specific elements of x, whereas otherwise subscripted elements (e. g. , x i ) are taken generically.",
        "GPT2_formal_text": "= {query_id}  \nSELECT query_id, entity_ids, entity_type_ids, action_ids, and slot_ids from the entity graph and the query text. Formal: The encoder creates embeddings for the entity nodes, then it turns those into vectors for the other nodes, and finally, it spits out the summary embedding for the whole entity graph. Formal: You can also do it in a simpler, batch-mode way using n-grams. For instance, if you have an entity graph G = (E, E, E, ..., EM), you can use n-grams to represent E, EM, and E' to get the representation for E' and EM'. Formal: In this project, we're working on a task called entity relation extraction, which means figuring out the relationships between entities. Formal: To get better results, we use a CRF layer. Formal: We start by gathering all the entity mentions from the entity graph. Then, we apply the embedding trick we talked about earlier to the entity mention embeddings of the entity nodes. Formal: For our experiments, we picked three datasets: PubMed 1, PubMed 2, and DBLP. We used the same entity types as in the training and validation sets of the Enron dataset. Formal: Lastly, we train the CRF layer by minimizing the cross-entropy loss, and then we fine-tune it for 10 rounds with a batch size of 32,000. Formal: Check out Figure 1 for the whole process. Formal: We'll break down the steps in more detail below, but for now, keep in mind that there's a ton of parameters involved. Formal: The encoder creates embeddings for the entity nodes, then it turns those into vectors for the other nodes, and finally, it spits out the summary embedding for the whole entity graph. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E06",
        "filename": "E06-1048.json"
    },
    {
        "casual_text": "In Section 5.2, we talked about how BART can handle Dynamic Blocking without needing any special task-adaptation or self-supervision. But, this approach doesn’t work as well because it doesn’t have the same level of quality, especially when it comes to keeping the syntax varied. This is because it wasn’t trained using the shuffling strategy during task-adaptation. \n\nWe also noticed that when BART isn’t fine-tuned, it tends to try and generate different forms of a word when it’s blocked. To fix this a bit, we use a pattern library to list out all the different forms of a word that need to be blocked. For example, for the word \"give,\" we’d also block \"gives,\" \"gave,\" \"giving,\" and \"given.\" This method works for most languages that have inflections. \n\nWe’ve included a comparison in Table 12, showing the output candidates for a specific example both with and without blocking these inflections.",
        "formal_text": "Block Inflections In Section 5. 2, we mentioned that BART can directly work with Dynamic Blocking without task-adaptation or self-supervision, but that results in lower quality, especially lacking syntactic variance because it is not trained with the shuffling strategy during task-adaptation. In addition, we found that without finetuning, BART tries to generate inflections of a word when it is blocked. To partially remedy this drawback, we use the pattern library 21 to enumerate all inflections of a word to block (e. g. for \"give\" we should also block \"gives\", \"gave\", \"giving\" and \"given\") in addition to all the other blocking schemes introduced in Section 3. This is available for most languages that involve inflections. We show in Table 12 the output candidates of a selected example with and without blocking inflections.",
        "GPT2_formal_text": "ed) and encoder_decoder (from Wang et al., 2019). Let's dive into the architecture we came up with and see how it helps with BERT's performance. Formal: To make things clearer, we’ll assume for a moment that we already know the input features for each node. The main idea here is to give the model a clear explanation of the input features for the nodes it’s learning. This way, the model can learn how to handle these inputs while also figuring out which specific input features are the most important for the task at hand. Formal: The goal of an NLP model is to figure out the best model (or objective function) for predicting the next token in a sequence. We’re using a cool way to measure this called attention, which we’ll call \"attention_masked.\" Formal: Basically, we take a weighted average of all the attention weights for the tokens in the sequence. Formal: To train the model, we add a special token feature to each input node and then adjust the weight of each token using that token feature. Formal: We’re using the attention mechanism to figure out how important each token is and to adjust the attention weights for the tokens. Formal: The model learns by using the attention from both the input and output nodes. Formal: The model picks the token with the highest attention score. Formal: The model learns by using the attention weights from both the input and output nodes. Formal: The model learns by using the attention weights from both the input and output nodes. Formal: The model learns by using the attention weights from both the input and output nodes. Formal: The model learns by using the attention weights from both the input and output nodes. Formal: The model learns by using the attention weights from both the input and output nodes. Formal: The model learns by using the attention weights from both the input and output nodes. Formal: The model learns by using the attention weights from both the input and output nodes. Formal: Formal: The model learns by using the attention weights from both the input and output nodes. Formal: Formal: The model learns by using the attention weights from both the input and output nodes. Formal: The model learns by using the attention weights from both the input and output nodes. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.417.json"
    },
    {
        "casual_text": "• BiLSTM-tok: This is a popular bidirectional LSTM model used for labeling sequences, like part-of-speech tagging (Plank et al., 2016) and named entity recognition (Panchendrarajan and Amaresan, 2018). We tweaked the settings based on the development set to get the best performance on each of the evaluation datasets.",
        "formal_text": "• BiLSTM-tok: Widely-used bidirectional LSTM architecture for sequence labeling, which has been applied to many tasks including part-of-speech tagging (Plank et al. , 2016) and named entity recognition (Panchendrarajan and Amaresan, 2018). We also tuned the hyperparameters based on the development set in order to achieve the best results on each of the evaluation datasets.",
        "GPT2_formal_text": "= {\n\n\"candidate1\": [],\n\n\"candidate2\": [],\n\n\"customer1\": [],\n\n\"customer2\": [],\n\n\"pid\": [],\n\n}\n\nNext, we use the same trick for the generated questions. We pick the best one based on the scores for question (q) and response (r). The best answer from each generation is picked as the answer for the question. This is also shown in the table below. Formal: So, for a question q, the best response r comes from the best possible answer, which is the top-ranked answer from the previous generation. Then, we calculate the score for the best response q, which is made up of the top-ranked answer from the previous generation and the best answer from the current generation. Formal: Formal: Our method works in a similar way to other question generation methods, but it tweaks the data and model to fit the specific task. Formal: Our approach is different from other question generation methods, but we don’t have to calculate or update the parameters. Formal: This works because the model gives a better prediction than those other methods. Formal: Our approach has three main steps. Formal: In the first step, we use the model to create questions, and in the second step, we answer those questions. Formal: The model picks the best answer by looking at the score from the best answer from the previous generation. Formal: The third step uses the model to generate answers to those questions, but we also add more training data and a new model to handle this. Formal: We tested our approach on two datasets: QUASAR-T and MARC. Formal: Our method gives the best answer to the question, and the best answer to the response. Formal: These results show that our method works really well. Formal: Formal: The best answer to the question, and the best answer to the response. Formal: Our method uses the model to create questions, and in the second step, we answer those questions. Formal: Formal: The model picks the best answer by looking at the score from the best answer from the previous generation. Formal: Formal: The third step uses the model to generate answers to those questions, but we also add more training data and a new model to handle this. Formal: We tested our approach on two datasets: QUASAR",
        "directory": "coling",
        "filename": "2020.coling-main.335.json"
    },
    {
        "casual_text": "Web forums like Stack-Overflow, Quora, and Yahoo! Answers usually group their stuff into topic-based sections. In these sections, people ask questions, and other users jump in with comments trying to answer. Sometimes, the list of comments gets really long. A lot of these forums don't have moderators, so the content can get messy and repetitive.",
        "formal_text": "Question answering web forums such as Stack-Overflow, Quora, and Yahoo! Answers usually organize their content in topically-defined forums containing multiple question-comment threads, where a question posed by a user is often followed by a possibly very long list of comments by other users, supposedly intended to answer the question. Many forums are not moderated, which often results in noisy and redundant content.",
        "GPT2_formal_text": "_size = 15.5;  \n\n• When it comes to the input hidden states, we set the attention weight to 0.2. Formal: For the pre-trained models, we stick with the same setup as in the original paper. We pick the best model with a similarity score of T ≥ 0.1 to create the final representation. Formal: We also use the same loss function for both the outputs and the labels. Formal: The loss function we use is cross-entropy loss. Formal: To keep things fair, we make sure that the label distributions in both the teacher and student models are the same size. Formal: The loss function L_S for the student model is pretty much the same as L_T for the teacher model. Formal: We use linear kernel with a window size of 5 to handle both the student and teacher models. Formal: We add some regularization to the student model, which helps it learn better representations. Formal: Finally, we average the results from the student and teacher models to get the final label distribution. Formal: We use the student model's representation for predictions, and the teacher model's representation for labels. Formal: Here's what's been happening: Formal: After training, the student model has learned a lot about labels. Formal: We tested it on the HUSE dataset and noticed it works really well. Formal: We also created a small dataset of 3,000 samples and compared it to L_S. Formal: The results were pretty good, with the student model performing just as well as the teacher model. Formal: We also tried a few other methods on HUSE and L_S to see how they stack up. Formal: Formal: The results for HUSE and L_S are in Table 4. Formal: Formal: We also built a small dataset of 3,000 samples and compared it to L_S. Formal: Formal: The results for HUSE and L_S are in Table 5. Formal: Formal: Formal: Finally, we averaged the results from all the student and teacher models to get the final label distribution. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1452.json"
    },
    {
        "casual_text": "You can find the code and data here: https://github.com/gsarti/interpreting-complexity",
        "formal_text": "Code and data available at https: //github. com/ gsarti/interpreting-complexity",
        "GPT2_formal_text": "= {t j } n = 1. n is the number of views, P(z j |z i ) is the probability of a word from the dataset being the correct label, and δ θ (x i ) is the model's own parameters. Formal: In this project, we use a basic unsupervised version of the SVM classifier, which was introduced by Søgaard et al. in 2016. We train the classifier using the same dataset, just like Søgaard et al. did in 2016. For testing, we use the LDC2009 dataset, which has a total of 1,567,154 unique words, but only has a total of 3,391,895 labels. Formal: The model we use has six parameters, including a maximum number of labels per instance and a minimum number of dimensions. Formal: The output from the model is a 0 or 1 vector, and we call the lowest value of this vector, p_z. Formal: We use the word embeddings from the last layer to predict the label. Formal: To find the best parameters θ, we use a regularization term γ that we calculate using the maximum likelihood method (thanks to Kipf and Welling, 2015). Formal: We check how well the model performs on the validation set by calculating the average accuracy. Formal: We also calculate the L2-norm for the validation set using the development set, which has 1,000 examples. Formal: Since the accuracy is the same for both, we just average them out. Formal: We use a one-tailed t-test to check if the difference is significant. Formal: The code and results for this project can be found at https://github.com/ZhouyunWu/XSVM. Formal: Check out Figure 3 for an example of how we use the LDC2009 training set for unsupervised NER. Formal: There are three ways to train the model, like three-fold cross-validation, two-fold cross-validation, and three-fold cross-validation. Formal: For three-fold cross-validation, we first train the model on the training set and then use it for testing. Formal: We also report the average accuracy for three different models, including a single-layer (BiLSTM) model for unsupervised NER. Formal: Formal: For",
        "directory": "cmcl",
        "filename": "2021.cmcl-1.5.json"
    },
    {
        "casual_text": "Okay, so the initial plan was to use the BLEU metric (Papineni et al., 2001) for evaluation since it's a pretty common way to evaluate machine translation. But, turns out, a lot of people think it’s not the best for languages that have a lot of inflections, like Slovenian and Serbian. Plus, the BLEU metric tends to give lower scores to rule-based machine translation (RBMT) systems, as mentioned by Callison-Burch et al. (2006) and Labaka et al. (2007). So, we decided not to use it.\n\nInstead, we looked into METEOR, which its creators say fixes many of the issues with BLEU and is more in line with how humans judge translations. The only problem? METEOR didn’t support our language pair, so we had to write some extra software to make it work.\n\nFor the actual evaluation, we used a bilingual parallel corpus (Dimitrova et al., 1998) to automatically check the translations.",
        "formal_text": "Preliminary evaluation plans included the evaluation using the BLEU (Papineni et al. , 2001 ) metric as it is one of the most used machine translation evaluation metrics. Many authors agree that Slovenian monolingual dictionary: <e lm=\"list\"><i>list</i><par n=\"žvenket/__n\"/></e> Slovenian -Serbian bilingual dictionary: <e><p><l>list<s n=\"n\"/><s n=\"m\"/></l> <r>list<s n=\"n\"/><s n=\"m\"/></r></p></e> Serbian monolingual dictionary: <e lm=\"list\"><i>list</i><par n=\"um/__n\"/></e> Figure 5: The extension of the bilingual corpus. First example shows a Slovenian lemma list, leaf, and the additional entries in the Serbian monolingual and SL-SR bilingual dictionaries Slovenian monolingual dictionary: <e lm=\"list\"><i>list</i><par n=\"žvenket/. . n\"/></e> SL-MK bilingual dictionary: <e><p><l>list<s n=\"n\"/></l><r> <s n=\"n\"/></r></p></e> Macedonian monolingual dictionary: <e lm=\" \"><i> </i><par n=\" /. . n\"></e> BLEU metric systematically penalises RBMT systems (Callison-Burch et al. , 2006; Labaka et al. , 2007) and it is not suited for highly inflective languages. The evaluation using the BLEU metric was not performed. Authors of METEOR state that their system fixes most of the problems encountered using the BLEU metric; they state that METEOR correlates highly with human judgement. Unfortunately METEOR did not support our language pair, additional software had to be written. The bilingual parallel corpus (Dimitrova et al. , 1998) was used in automatic evaluation of translations.",
        "GPT2_formal_text": "ed = {0, 1, 2, 3, 4, 5, 6} h_d = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] h_t = [0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0] Formal: We're using a convolutional neural network (CNN) setup to break down sentences into chunks. After that, we average the hidden vectors of each chunk to get the final summary. Formal: The network then spits out the probability of the next token in each chunk. For each sentence s_i, the network spits out the probability of the next token, p_i, given the current sentence s_i. Formal: Lastly, we calculate the representation H_t for the whole sentence x_i, which is a mix of the sentence representations H_t from both the source and target sentences. Formal: We then use the final output (H_t) to summarize the whole sentence. Formal: The final summary (H_t) is a mix of the sentence representations H_t from both the source and target sentences, and the sentence representations H_t from each of the target and source sentences. Formal: Lastly, we calculate the representation H_t for the whole sentence x_i, which is a mix of the sentence representations H_t from both the source and target sentences, and the sentence representations H_t from each of the target and source sentences. Formal: This process is repeated for each pair of sentences. Formal: We repeat this process for each pair of sentences. Formal: Finally, we calculate the representation H_t for the whole sentence x_i, which is a mix of the sentence representations H_t from both the source and target sentences, and the sentence representations H_t from each of the target and source sentences. Formal: Formal: Finally, we calculate the representation H_t for the whole sentence x_i, which is a mix of the sentence representations H_t from both the source and target sentences, and the sentence representations H_t from each of the target and source sentences. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2010.eamt-1.24.json"
    },
    {
        "casual_text": "Alright, let's break down how we can capture the impact of this fancy control agreement principle (CAP). PSG uses Montagovian semantic type assignments to explain CAP, but since we haven't talked about semantics, we'll just assume the necessary type info is already in our feature structures. With that in mind, our CAP setup has three main parts: first, we need to define what a controller and a controllee (or target, as GPSQ calls it) are; second, we define what a control feature is; and third, we explain the instantiation principle. Let's go through each step.\n\nFirst, let's define controller and controllee: A category C is controlled by another category C~ in a constituent Co if, at a semantic level, one of these two things happens: either C is a functor that combines with C~ to make Co, or there's a control mediator C\" that joins C and C~ in that specific order to create Co.",
        "formal_text": "Finally, we sketch how the effect of the more elaborate control agreement principle (CAP) can be captured. ~PSG formulates CAP by making use of the Montagovian semantic type assignments. As we haven't discussed semantics, we're going to assume that the relevant type information is available inside our feature structures. With this assumed, our formulation of CAP falls into three steps: first, defining the notions of controller and controllee (or target in GPSQ terminology); second, defining the notion of a control feature; and third, defining the instantiation principle. We consider each in turn. Controller and controllee are defined as follows: 14 A category C is controlled by another category C ~ in a constituent Co if one of the following situations obtains at a semantic level: either C is a functor that applies to C ~ to yield a Co, or else there is a control mediator C\" which combines with C and C ~ in that order to yield a Co.",
        "GPT2_formal_text": "and maybe even the title and abstract. We’ll call this method `COPY(X, Y)`. Formal: The algorithm we’re talking about is based on something called an L-Granular Approach. Basically, it’s a probabilistic version of a hierarchical attention model. Formal: We’re using a specific sub-tree model, which is based on the phrase-based model. Formal: The main goal here is to minimize the cross-entropy loss, which is calculated using the embedding of the main node. Formal: We’ve found that the sub-tree model we’re proposing works better than the original hierarchical attention model. In fact, it’s almost as good as the phrase-based model. It’s about 50% to 70% better in terms of performance. Formal: The main goal is to minimize the cross-entropy loss, which is calculated using the embedding of the main node. Formal: This is the main approach in our work. Formal: To figure out how useful a node in a tree structure is, we’re using a method called Preference Learning (PL). Formal: In PL, we treat all the nodes in the tree as possible candidates. We find the node that has the highest probability based on how likely each word is. This node is then chosen as the top node in the tree. Formal: PL is a really effective way to learn relationships between words. Formal: PL is a tree-based model that uses a preference function to learn dependency trees. Formal: We came up with a new approach called Dynamic Memory Management (DM), which focuses on using both local and global memory for memory management. Formal: Here’s how it works. Formal: We use a dynamic memory manager to store local and global info. Formal: The model’s memory is set up to handle local and global memory operations at the same time. Formal: We added a dynamic attention module to the system. Formal: To make sure the model can learn both local and global memory, we added a dynamic attention module to the model. Formal: We also added a global memory manager, which is similar to the global memory manager used in standard global attention models. Formal: The global memory manager works with a special memory pool to handle global memory operations. Formal: The global memory manager stores global and local info in",
        "directory": "E93",
        "filename": "E93-1004.json"
    },
    {
        "casual_text": "• and then figure out how to say it in the target language based on that interlingua stuff.",
        "formal_text": "• and to derive a surface structure of the target language from the interlingua expression.",
        "GPT2_formal_text": "ed, and just set the value of k to k_0. Formal: We use the same setup as in the Transformer model, but with a twist: we swap the input representation vector and the output vector with a special token. This token acts as the key for decoding. The other input and output embeddings are kept the same as in the Transformer model. Formal: We also use a copy-paste mechanism to create a temporary temporary representation of the input, which we'll call M_t. Then, we can tweak it a bit more by adding a few more dimensions, D_t, to get M_t. Formal: Here's how it works: Formal: For each token w_i in a source sentence x_i, we grab a token w_j from a target sentence y_i. We then use these token w_j to fill in the gaps in our temporary temporary representation M_t_i. The rest of the sentence is then decoded using this updated temporary representation. Formal: For each token w_i in a source sentence x_i, we grab a token w_j from a target sentence y_i. We then use these token w_j to fill in the gaps in our temporary temporary representation M_t_i. The rest of the sentence is decoded using this updated temporary representation. Formal: We've also got an alignment structure for the temporary temporary representation M_t_i. Formal: Once we've got the temporary temporary representation M_t_i, we use it to copy some tokens from a source sentence x_i into a target sentence y_i. This is done by using the word embedding for the source token x_i and the token embedding for the target token y_i. We then make sure that the temporary temporary representation M_t_i is unique, so no overlapping tokens get added. This process is repeated for each token w_i and each token w_j, and we end up with the output token, w_i. Formal: So, the summary token that's generated by this model is called M_t_i. Formal: Figure 1 shows a diagram of the transfer process. It's broken down into steps (s_1 to s_m) for each target sentence x_i, as shown in Figure 1. Formal: We use the Transformer model to copy some tokens from a source sentence x_i into a target",
        "directory": "C90",
        "filename": "C90-2061.json"
    },
    {
        "casual_text": "The difference between the scores of the actual correct answer, which we'll call x_i, and a possible but not quite right answer, x_i,j, is what we're looking at here. The hinge loss is used to make sure that the correct answer's score is at least τ points higher than the score of the not-so-great option. We're calling the language model we train using this method the Large Margin Language Model (LMLM).",
        "formal_text": "is the margin between the scores of a ground-truth x i and a can-didate x i, j. The hinge loss on the margin encourages the log-likelihood of the ground-truth to be at least τ larger than that of the imperfect hypothesis. We call an LM trained by the above formulation as Large Margin Language Model (LMLM).",
        "GPT2_formal_text": "= t in spaCy (s, w) for p in p_0, p_1, p_2, ..., p_n. Formal: Here, the adjacency matrix A_t is the adjacency matrix for word w_i, and σ_t is the sigmoid function that helps measure how similar two words are. Formal: From our experiments, we found that the sentiment of a document can be predicted using a method called multi-label classification. Formal: We've come up with a new, simple, and efficient way to classify sentiment in documents, based on the training examples we've got. This method works better than the old ones and is pretty practical for real-world use. It’s simple, efficient, and works well with the NLP community. Formal: In the next part, we’ll talk about how we set up our experiments, what we used for training, and what we found out. We’ll also explain how to use our method with other text classification models, like logistic regression, to figure out the sentiment of a document. Formal: We’ll also share our training method, which has been used in different studies and evaluation metrics. Formal: In this part, we’ll explain our experimental setup and the results we got from testing it out. We’ll also talk about the weaknesses of the older methods we compared our method to. Formal: Finally, we’ll share the results of our evaluation of our method on two text classification datasets, and we’ll talk about the results of our own experiments on two datasets. Formal: We’ll start by explaining the basic sentiment classification algorithm and how we chose the data. Then, we’ll explain our experimental setup and how we used our method with different text classification models. Formal: Formal: We’ll explain how we set up our experiments, what we used for training, and what we found out. We’ll also explain how to use our method with other text classification models, like logistic regression, to figure out the sentiment of a document. Formal: Finally, we’ll share the results of our evaluation on two text classification datasets, and we’ll talk about the results of our own experiments on two datasets. Formal: Formal: We’ll start by explaining the basic sentiment classification algorithm and how we chose the data. Then,",
        "directory": "D18",
        "filename": "D18-1150.json"
    },
    {
        "casual_text": "In SAMT, they use a bunch of CCG-like binary operators to boost the number of nonterminal labels. This is important because SAMT labels come from phrase structure grammar, which only has a limited set of labels that can't cover all the different syntactic structures of the phrases they extract. Almaghout et al. (2010) tried using single-category CCG labels as nonterminal labels. While CCG's flexible structures give better coverage than phrase structure grammar labels, using just single-category CCG labels still leaves about a third of the phrases unlabeled.\n\nTo fix this, we decided to expand the definition of nonterminal labels to include more than one CCG category. So, if there's no single CCG category at the top of the tree that can label a phrase, we look for the highest-scoring sequence of categories with the fewest CCG categories from the CCG trees that cover the phrase and use that as the label. Figure 2 shows some phrases from the sentence in Figure 1, along with their extended CCG labels.\n\nFor example, the phrase \"like cream\" has an extended CCG label made up of two categories: S[b]\\NP+conj. The CCG categories for the words \"like\" and \"cream\" are combined into S[b]\\NP. But this category can't be combined with the conj category.",
        "formal_text": "In SAMT, a set of CCG-like binary operators are used to increase the coverage of nonterminal labels. This is necessary as SAMT labels are extracted using phrase structure grammar, which has a small set of constituent labels that are insufficient to cover all the different syntactic structures of extracted phrases. Almaghout et al. (2010) use single-category CCG labels as nonterminal labels. Although CCG flexible structures allow a better label coverage than phrase structure grammar-based labels, using single-category CCG labels fails to label about one third of the total phrases. In or- der to increase label coverage, we extend the definition of the nonterminal label to be composed of more than one CCG category. Therefore, if there is no single CCG category at the root of the trees which cover a phrase, the highest-scoring sequence of categories with a minimum number of CCG categories is extracted from CCG trees covering the phrase and used as the phrase label. Figure 2 shows a set of phrases extracted from the sentence illustrated in Figure 1 along with their extended CCG labels. In this example, the phrase like cream and has an extended CCG label composed of two categories: S[b]\\NP+conj. The CCG categories of the words like and cream are combined into the CCG category S[b]\\NP. However, this category cannot be combined with the conj category.",
        "GPT2_formal_text": "ed is the attention vector we get from the input hidden state h t. Formal: To get a better understanding of the attention weights in the attention layer, let's look at the LSTM attention weight vector, which we can calculate using the formula: Formal: So, f s (h t−1 ) represents the attention weights for the [CLS] token in the sentence. Formal: So, the attention weight vector for the entity mention e i, t is calculated using the formula: Formal: The attention vector for the [CLS] token is calculated using the formula: Formal: Lastly, the entity mention e i, t is represented by the entity mention vector x i, t. Formal: This setup helps us grab the attention weights from the input hidden state h t, which are used to figure out the attention weights for the entity mention e i, t. Formal: This whole process of calculating the attention weights is done using equation (2). Formal: In the second setup, the entity mention e i, t is represented by the entity mention vector x i, t. Formal: In the third setup, the entity mention e i, t is represented by the entity mention vector x i, t, and the entity mention vector x i, t. Formal: In the fourth setup, the entity mention e i, t is represented by the entity mention vector x i, t, and the entity mention vector x i, t. Formal: So, the model creates a representation of the mention e i, t by using a matrix of entity mentions x i, t as input. Formal: Finally, the attention weights are calculated based on the representations of the entity mentions. Formal: For instance, take the word \"Pyongyang,\" which shows the entity mention \"Pyongyang\" in the sentence. Formal: The attention weights for the entity mention are calculated using the formula: Formal: And the attention weights for the entity mention are calculated using the formula: Formal: Lastly, the attention weights for the entity mention are calculated using the formula: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2012.eamt-1.44.json"
    },
    {
        "casual_text": "Plus, using a non-autoregressive model lets us create an encoder-only setup, which works better for summarization. This is because the input and output have a really close connection, and that's something encoder-decoder models, especially the autoregressive ones, can't fully handle.",
        "formal_text": "Moreover, a non-autoregressive model enables us to design an encoder-only architecture, which is more suited to the summarization task due to the strong correspondence between input and output, which cannot be fully utilized by encoder-decoder models, especially autoregressive ones.",
        "GPT2_formal_text": "= [h_i](h_i) # make sure the output stays consistent across different domains for each intent. Formal: Basically, we combine the outputs from all the heads using a softmax function, and we apply a softmax to the prior probability p_ij for each output. Formal: Finally, we pick the token with the highest probability, p_ij_t(i), as the final summary token. Formal: We use the ROUGE metric to measure how similar the summaries are, using both the input and output domains. Formal: We estimate the relevance scores using the corpus-based relevance scoring method (Liu et al., 2015). Formal: The COCO framework (Sangati et al., 2013) is a way to turn text into vector representations, which then become the representations for images. Formal: The input image (x) is represented as a set of coordinates (x_i, x_i') that we can learn using some pre-trained models (M). Formal: We get the output representations (o_i) for each image (x) using M, which takes in a sequence of numbers (1s, ..., 9s) for each image i. Formal: We apply M to each input image x to create a representation of it as a vector. Formal: We combine all the output representations (o_i) with the input image x to get the final vector representation for the input image. Formal: To get the relevance scores for the input and output images, we calculate the relevance scores using the LSCOCO dataset (Cai and Liu, 2008). Formal: We add a bias term (β_ij) to the output representations (o_i) to balance the vector representations of the input and output images, but the model has to decide if this term is positive or negative. Formal: To find the relevance scores for the input and output images, we calculate the relevance scores using the COCO dataset. Formal: The first version of the image captioning model (M_1) uses the COCO dataset and the dataset-based relevance scoring method (LSCOCO), while the second version (M_2) uses the ROUGE metric. Formal: We apply the first version of M_1 and the second version of M_2 to the input image (x) to get the output representations (o_1, o_2). Formal",
        "directory": "acl",
        "filename": "2022.acl-long.545.json"
    },
    {
        "casual_text": "In the MRC field, some earlier studies focus on general-purpose language modeling, but they require a lot of computational power to encode aspects (Zhang et al., 2020c). Others just combine texts from different MRC tasks to increase the size of the training dataset.",
        "formal_text": "In MRC field, several previous works perform general-purpose language modeling with considerable computing cost at encoding aspect Zhang et al. , 2020c), or splice texts among diverse MRC tasks simply to expand training dataset",
        "GPT2_formal_text": "_n_tokens * || adj. group_tokens * || adj. row_tokens)  \n\nBasically, this is a way to measure how much attention a node has in a sequence. It’s calculated by adding up the attention weights, which are the average attention weights from the heads of the nodes we’re looking at. Formal: The first group is all the attention weights that go to the heads of the nodes we’re looking at. The second group is all the attention weights that go to the rows of the nodes we’re focusing on. Formal: Here’s what each part means: Formal: The first group, which is all the attention weights, includes the usual ones like U_h, V_h, and H_h. Formal: The second group includes attention weights for the nodes in the row we’re looking at, like U_r, V_r, and H_r. Formal: The third group is attention weights for the nodes in the first column of the node we’re focusing on, like U_c, V_c, and H_c. Formal: And finally, the last group is attention weights for the nodes in the second column of the node we’re focusing on, like U_d, V_d, and H_d. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.594.json"
    },
    {
        "casual_text": "Online social networks have gotten a lot of attention from researchers, especially when it comes to entity linking in these spaces. (Huang et al., 2014; Shen et al., 2013) came up with methods specifically made for linking named entities in tweets. The main challenge they tackle is that tweets are usually short and casual, but they also try to make use of any extra info that tweets might offer. For instance, (Shen et al., 2013) figured that each user's tweets reflect some kind of interest pattern and created a graph-based algorithm to rank entities based on this interest distribution. (Huang et al., 2014) also used a method with meta-paths on HIN in their entity linking approach, but they just used it to see if two mentions are connected. One thing all these studies have in common is that, even though they focus on tweets, they still link the entities to knowledge bases as the final goal.",
        "formal_text": "The prevalence of on-line social networks has also motivated researchers to study entity linking in such environments. (Huang et al. , 2014; Shen et al. , 2013; proposed methods that are specially designed for linking named entities in tweets. They mainly address the problem that tweets are usually short and informal, while taking advantage of some of the extra information that tweets may provide. For example, (Shen et al. , 2013) assumed that each user's tweets have an underlying interest distribution and proposed a graph based interest propagation algorithm to rank the entities. (Huang et al. , 2014) also used meta-path on HIN in their entity linking approach, but they only used it to get an indication of whether two mentions are related. Finally, although these studies focused on entity linking for tweets, they still use entities in knowledge bases as the target.",
        "GPT2_formal_text": "= {q i } } for i from 1 to M. Formal: The A-terms are treated as the main terms in the dialogue, and we use this to figure out how the dialogue (D) relates to the knowledge base (KB). Formal: First, we make a training dictionary called d by creating pairs of words that show how two things are related, like \"cough\" and \"sickness.\" After that, we create a list of A-terms, q i, that help us understand the dialogue (D). Formal: In Section 4, we’ll talk about our method to create training data, specifically for the dialogues we’re focusing on. Formal: Here’s what we’re working on: Formal: We’re using a text classification method to figure out the types of entities in the knowledge base (KB) from a question. This method is based on the idea that the dialogue acts like a document (D) to the KB, and the knowledge base acts like a knowledge graph (G). Formal: Here’s what we’re working on: Formal: We’re using an information retrieval (IR) method to retrieve knowledge from the KB. The goal is to grab the most useful knowledge from the KB and put it into a knowledge base for the final answer. Formal: We’re using a sentence classification approach to see which sentences in the KB are related to the question. This helps us find all the sentences that could answer the question. Formal: Lastly, we’re looking at how sentences are arranged in the KB. Formal: We’re using an event detection (ED) method to figure out the roles (like subjects or objects) in a conversation. This helps us spot the key moments in the conversation. Formal: Finally, we’re using a question answering (QA) method to help us retrieve all the questions that could answer the question. Formal: Based on all this, we’ve come up with a new dataset called QA-KD. Formal: This dataset has QA-KD as its main task, with five different types of questions, each with their own set of questions. Formal: To train the model, we’re using a few different approaches: Formal: We’re using a question generation task to generate a bunch of questions, each with a specific set of questions.",
        "directory": "D18",
        "filename": "D18-1227.json"
    },
    {
        "casual_text": "Word-based methods have an edge because they can pick out important words in a sentence more easily than bunsetsu-based methods. This is because word-based methods don’t worry about bunsetsu and aren’t restricted by unit limitations.",
        "formal_text": "The advantage of word-based methods is that the methods can more freely select important words in a source sentence than bunsetsu-based methods. This is because word-based methods do not take account of bunsetsu in a source sentence and are not limited to unit constraints.",
        "GPT2_formal_text": "ing_idx = seq2seq(X) y_train_idx = y_train_idx + l2tok(X) y_test_idx = y_test_idx + l2tok(X) + sum(sum(1:N)) + sum(sum(1:N)) Formal: We use a LSTM decoder along with a global attention mechanism. The global attention mechanism helps focus attention across the entire input sequence and keeps track of how much attention has been paid to each token. Formal: To get the attention weights, we just average the attention scores from all the tokens. Formal: In the Transformer model, we calculate the attention weights by averaging the weighted values of the tokens. Formal: Formal: We tweak the hyperparameters by finding the best combination of parameters based on the development data. Formal: We combine the attention weights and the transformation matrix for each token. Formal: For the final layer in the LSTM decoder, we add a dropout rate of 0.2 and an embedding dimension of 200. Formal: To get the final output, we calculate the attention weights by averaging the weighted values of the tokens. Formal: We end up with a final output vector that has the attention weights, transformation matrix, and embedding dimension all combined. Formal: We use a linear layer to combine the attention vectors and the representation vectors, which helps improve performance. Formal: We use the maximum likelihood estimation (MLE) method to train the model. Formal: We optimize the model using the Adam optimizer (from Kingma and Ba, 2015) with an initial learning rate of 0.0001. Formal: We train the model using the development set to find the best parameters. Formal: We fine-tune the model using the validation set to get the best performance. Formal: Finally, we apply the loss function we learned to the model. Formal: Formal: Formal: We use the term frequency in LDA to measure the context-to-topic conversion. Formal: Formal: We evaluate how well the model performs by measuring perplexity on the validation set. Formal: Formal: Formal: Formal: We combine the attention weights and the representation vectors for each token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C12",
        "filename": "C12-1067.json"
    },
    {
        "casual_text": "Glow, introduced by Kingma and Dhariwal in 2018, is a type of normalizing flow that transforms data into a simple, known prior, like a spherical multivariate Gaussian distribution. It’s trained using the exact log-likelihood of the data and, as a generative model, it’s really good at capturing all the dependencies in high-dimensional data, which often comes in the form of a complex, multimodal probability distribution. To keep things fair when comparing it to other methods, we only use the encoder’s hidden states as the condition for each flow step and stick with a spherical Gaussian prior, just like in the original Glow.\n\nThrough some experiments, we noticed that a 2D-CNN-based network works better than a 1D-CNN, especially when it comes to reconstructing high-frequency details. As you can see in Figure 4d, our Glow-based decoder models the distribution of mel-spectrograms based on the encoder’s output hidden states x. This decoder is made up of k flow steps, labeled f1 to fk. Each step includes an affine coupling layer, an invertible 1x1 convolution, and an activation normalization layer. Essentially, the Glow-based decoder takes random variables from a spherical Gaussian distribution and turns them into mel-spectrograms.",
        "formal_text": "Glow (Kingma and Dhariwal, 2018) is a kind of normalizing flow, which maps data into a known and simple prior (e. g. , spherical multivariate Gaussian distribution). Glow is optimized with the exact log-likelihood of the data and as a generative model, it is very good at modeling all dependencies within very high-dimensional input data, which is usually specified in the form of a dependent and multimodal probability distribution. To make a fair comparison with other modeling methods, we only take the output hidden states of the encoder as the condition to each step of flows and use spherical 13 Through experiments, we find that the 2D-CNN-based network performs better than 1D-CNN in discriminator, especially for the reconstruction of high-frequency details. Gaussian as the prior distribution like in the vanilla Glow (Kingma and Dhariwal, 2018). As shown in Figure 4d, our Glow-based decoder models the distribution of mel-spectrograms conditioned on the encoder output hidden states x. The Glow-based decoder is composed of k flow steps f 1, . . . , f k, each of which consists of an affine coupling layer, an invertible 1x1 convolution, and an activation normalization layer. Glow-based decoder maps the spherical Gaussian random variables z to the mel-spectrograms as",
        "GPT2_formal_text": ", q, a_i, l_i, e, f, g, and h. Formal: Each thing in the query is its own sentence. Formal: Each thing in the query has its own id, and the id is linked to the sentence that it's in. Formal: Let's say s_i and q_i are the parts of the sentence that have info about the thing it's talking about, like the entity type and what kind of event it is. Formal: Formal: So, the entity type for a_i, t_i, is all the entities that match the word \"the\" in that specific part. Formal: Formal: And what kind of event is it? Formal: The event type for \"a_i, t_i, is(a)\" is \"a\" and \"a\" equals \"a\". Formal: Formal: Finally, the event type for \"a_i, t_i, is(a)\" is also \"a\" and \"a\" equals \"a\". Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.564.json"
    },
    {
        "casual_text": "(Self-disclosure) Yeah, I get it. I’d be super annoyed too if that happened to me.",
        "formal_text": "(Self-disclosure) I understand you. I would also have been really frustrated if that happened to me.",
        "GPT2_formal_text": ".\n\nIn this paper, we introduce a new attention-based neural network model called Lattice-LSTM, which is a multi-layer sequence labeling model designed to handle a wider range of discourse relations compared to the traditional LSTM model, as shown in Figure 2. Formal: We use a Pointer Generator (from Guo et al., 2017) to create a conditional distribution for a discourse relation, D. The probability of D given a context c is calculated using this generator. Formal: The attention mechanism is super important for handling multiple types of relations, especially when dealing with multi-class relations. We take a different approach by focusing on the attention mechanism to pull out detailed discourse information. We tested this approach on our dataset and the results show that it works really well.\n\nOne downside of our approach is that it relies on the generator to figure out the probability of each label in a given pair of discourse relations. This means it needs to be trained on a lot of data, which is not ideal. So, we came up with a different approach that lets the model learn from its own internal representations. We refer to this as multi-label generative adversarial training. Formal: We introduced a new adversarial training method called Multi-Label Training (MLT). MLT tweaks the traditional adversarial learning framework by using multiple label learning strategies, including the attention mechanism. It does this by adding a new type of negative sampling with adversarial loss, inspired by the Pointer Generator. Formal: We also introduced a new adversarial training method called Multiple-Label Training (MLT), which adds a new negative sampling strategy that works across multiple labels. Formal: We tested our new adversarial learning framework on a specific domain, which we’ll call D_p, to see how it performs. Formal: Formal: To tackle the issues with multi-label generative adversarial learning, we introduced a fresh adversarial training approach called Multiple-Label Training (MLT). MLT tweaks the traditional adversarial learning framework by adding a new negative sampling strategy, inspired by the Pointer Generator. Formal: We also introduced a new adversarial training method called Multi-Label Training (MLT), which adds a new negative sampling strategy that works across multiple labels. Formal: We also introduced a new adversarial training method called Multiple-Label Training (MLT), which adds a new negative sampling strategy that works across multiple labels. Formal: Finally, we tested our",
        "directory": "acl",
        "filename": "2021.acl-long.269.json"
    },
    {
        "casual_text": "Alright, so this Papillon module is all about creating user-friendly interfaces for looking at and editing dictionary entries. We’re building on some cool research by Thevenin and Coutaz from 1999, and also using a tool called ART-Studio, which was developed by Calvary and team in 2001. These guys came up with ways to semi-automatically design interfaces for different purposes. Here’s a quick rundown of the framework and models we’re using.",
        "formal_text": "This Papillon module has to generate graphic user interfaces for consulting and editing dictionary entries. We base our approach on the work done on Plasticity of User interfaces (Thevenin and Coutaz, 1999) and the tool ART-Studio (Calvary et al. , 2001). They propose frameworks and mechanisms to generate semiautomatically graphic user interfaces for different targets. Below we present the design framework and models used.",
        "GPT2_formal_text": "= (s_i, d_i) if not d_i: return [s_i] for i in range(0, D_K), where D_K is the set of d_i. Formal: If the first three sentences are split up, the summary should include all the important stuff from the first three sentences. Basically, each sentence should be the main focus for the summary. Formal: We use a method called \"cluster and label\" to group and label the documents. Formal: Since the knowledge base is pretty big, we also use a method called \"distributional similarity\" to compare the words between the documents and the summary. Formal: We use the best translation option from the knowledge base and the query to find the best translation in the summary. Formal: We create translation pairs by looking at the word vectors in the query and the best translation options in the knowledge base. We use the distance measure, which is based on the BLEU score, to make sure we get the best translation options. Formal: Finally, we calculate the correlation between the translation options and the query vectors using the best translation options. Formal: To get the translation options from the knowledge base, we first use the \"cluster and label\" method. Then, we use the BLEU score to get the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We use a top-notch search engine to find the best translation options. Formal: We",
        "directory": "C04",
        "filename": "C04-1148.json"
    },
    {
        "casual_text": "Kind of like how educational testing researchers check if IRT models can accurately predict how people answer questions (American Educational Research Association, 2014), we’re checking how well DAD can predict if SQuAD models are getting questions right.",
        "formal_text": "Just as educational testing researchers validate IRT models by seeing if they predict subject responses correctly (American Educational Research Association, 2014), we validate how well DAD predicts whether SQuAD models get questions right.",
        "GPT2_formal_text": "ed (L, t, C) represents the attention mask we use for LTF. Finally, the cross-attention vector o_t for LTF (h_t) is calculated using the cross-attention module. Formal: In this part, we’ll break down how we set up the cross-attention module in our model. First off, we use a convolutional neural network (CNN) to handle the input sentence X. To get the attention mask, we use the LSTM model (Hochreiter and Schmidhuber, 1997) with a max pooling layer, followed by a linear layer. Formal: We start with the input x and use a CNN to focus on the hidden states from the convolutional layers. Then, we apply linear transformation with a max pooling layer on these hidden states to get the attention mask h_t. Formal: We also use an LSTM to process the input x, and then calculate the attention mask h_t based on the hidden states. For the cross-attention module, we use the cross-attention mechanism (Bahdanau et al., 2014) with an attention mechanism layer (Bahdanau et al., 2015) that works with a linear layer. Formal: We use a linear layer on the attention weight matrix (β) to get the attention mask for LTF. Formal: Lastly, we use the pre-trained XLM-R model (Conneau et al., 2019) to predict the label y_t for LTF. Formal: We train our model using cross-validation on the development set, and then use the performance from the validation set to guide us when making predictions. Formal: We have two datasets for the cross-attention module, but we’re not sharing the training sets for the sentence classifier since the validation set isn’t available. Formal: For the cross-attention mechanism, we use the attention mechanism (Bahdanau et al., 2014) to calculate the attention mask h_t for LTF. Formal: We use a linear layer with a max pooling layer on the attention weight matrix (β) to get the attention mask for LTF. Formal: We train our model with cross-validation on the development set, and then use the performance from the validation set to guide us when making predictions. Formal: We train our model with cross-validation on the development set",
        "directory": "acl",
        "filename": "2021.acl-long.346.json"
    },
    {
        "casual_text": "Tree kernels look at trees by focusing on smaller parts of them, called substructures or fragments. The kernel function checks if a part of a tree, which is shared by two trees, fits into the feature space we're trying to create. To do this, we need to describe the specific fragments we're interested in. We mainly look at two types: syntactic tree fragments (STF) and partial tree fragments (PTF).",
        "formal_text": "Tree kernels represent trees in terms of their substructures (fragments). The kernel function detects if a tree subpart (common to both trees) belongs to the feature space that we intend to generate. For such purpose, the desired fragments need to be described. We consider two important characterizations: the syntactic tree (STF) and the partial tree (PTF) fragments.",
        "GPT2_formal_text": "= {u1, ..., uk}, where k is any integer from 1 to N. Formal: We use a continuous window of 0.1 seconds, with a width of 10, to process all the bits of the input. Formal: We figure out the top k different ways to arrange the inputs for each prediction. Formal: We use a graph-based approach to figure out how much the model should pay attention to certain areas, based on what we already know about how they work. This helps us figure out the best way to arrange the inputs and makes it easy to see how much each bit of the input should focus on. Formal: If the input is a yes or no question, we check if it can be answered in that specific area. If the answer is yes, we create a special node for that area. We then try to fill in the other slots that don't have answers. Formal: For each question, we consider two options: (1) if the question is about a specific topic, we look at the first topic and the top k spots in the answer row; (2) if the question is about a general topic, we look at the last topic and the top k spots in the answer row. Formal: To figure out the probability distribution of the input, we use the training set to estimate the log p(x i |y i ). Formal: Finally, we use a distortion model to create the output distribution P(w i |y i ). Formal: For the loss function L, we calculate the loss for the prediction and the label. Formal: We train the model using the cross-entropy loss, using a batch size of 16 and a learning rate of 0.01, and we set the batch size to 10. Formal: The loss function L is calculated for each word in the input. Formal: We get the loss for a specific word in the input by adding up the losses for all the words in the input. Formal: The loss function L is a bit more complicated than just L(w i |y i ) because it needs to include the loss for the prediction and the label. Formal: We keep updating the model based on the training set and the new loss functions. Formal: We apply a special distortion model to the output distribution P(w i |y i ). Formal: We train the model using cross-entropy loss, using a batch size of 16 and a learning rate of 0",
        "directory": "D09",
        "filename": "D09-1112.json"
    },
    {
        "casual_text": "Basically, if a summary doesn't match up with the original text when you ask a question and the answer changes depending on whether you're looking at the summary or the source, then it's considered inconsistent. So, we figure out how accurate the summary is by calculating something called the precision score.",
        "formal_text": "A summary is deemed inconsistent w. r. t. its source text if, given a question, the answer differs when conditioned on S or D. Therefore, we define the precision score for the evaluated summary as:",
        "GPT2_formal_text": "ed_x, which is just a fancy way of saying we're training a model to figure out how words in a sentence are connected. Formal: If we have a neural network that's already learned some fancy stuff (like the embeddings from GloVe or ELMo), we can just plug it into the bidirectional LSTM layer without any extra training. Formal: Sure! Here's the informal version:\n\n- Check out the experiments in Section 4 for the full details.\n- Also, check out the initial results in Table 2. Formal: A neural network can learn from a big dataset without needing any extra help. This method works even for datasets that don't have a lot of words, like ones with lots of characters. Formal: We figure out the hidden states by averaging the output from the embeddings from both the source and target layers. Formal: We use the LSTM layer to learn from the output from the embeddings in both the source and target layers, then the cross-layer perceptron to get the final hidden state. Formal: This can be shown using a neural network with a bidirectional LSTM layer, like the one in Figure 1, shown in Figure 2. Formal: Also, for simpler datasets, we just use a cross-layer perceptron to get the hidden states. Formal: To see how different layers perform, we ran some experiments where we used either a Transformer (from Vaswani et al., 2017) or a CNN (Peters et al., 2017) with CNN filters to get the hidden states. Formal: For our experiments, we used a pretrained BERT (Devlin et al., 2019) model to handle the task. Formal: The network has two main parts: the bi-directional LSTM layer and the bidirectional LSTM layer. Formal: After that, we combine the hidden states to get the hidden state representation. Formal: To train the network, we use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.0001 and a batch size of 32. Formal: The model we got from the LSTM layer in Figure 1 is our prediction model. Formal: We ran experiments with different learning rates: 0.0001, 0.01, 0.05, and 0.1. Formal: Lastly, we checked the results for our prediction model using the BERT model.",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.529.json"
    },
    {
        "casual_text": "Next, for each candidate Pj, we figure out the probability p(x, Pj) that the query x is referring to the same thing as Pj.",
        "formal_text": "Then, for all candidates P j we compute the probability p(x, P j ) that the query x corefers with as",
        "GPT2_formal_text": "= {h_i, j}, where h_i is the id of the token in the document, and j is the position of the token in the sentence. The model calculates the likelihood by using the log-likelihood function, which is the same thing as a one-hot encoding. Formal: Here, λ_ij is the cross-entropy loss, which was introduced by Rennie and Manning in 2018. The loss function is trained using three parameters: δ_ij, δ_ij'_1, and δ_ij'_2. Formal: • The token embeddings. Each token is represented by its embedding. Formal: To get the embeddings for the source and target tokens, we start by calculating the cosine similarity between the token embedding vectors. Formal: You can find more details about the setup of our method in Appendix A. Formal: Finally, we train the model using the cross-entropy loss on the validation set to get the final prediction. Formal: For training, we use the cross-entropy loss with three parameters, δ_ij, δ_ij'_1, and δ_ij'_2, and we also include the position embedding as an extra input. Formal: We also tried out different sampling strategies. For each sampling strategy, we calculate the cross-entropy loss using the input set and the empirical distribution of the empirical distribution, as described in Appendix A. Formal: For a specific model, we calculate the loss by looking at the empirical distribution of the empirical distribution of the log-likelihood, as explained in Appendix A. Formal: We use the entropy loss for the gradient of the log-likelihood. Formal: The cosine similarity is calculated using the cross-entropy loss with three parameters, δ_ij, δ_ij'_1, and δ_ij'_2. Formal: Finally, we train the model using the cross-entropy loss on the validation set to get the final prediction. Formal: The whole setup of our method is laid out in Appendix A. Formal: Appendix A gives the full details of how we trained our model. Formal: For more details, check out Appendix A. Formal: Here, d_i_ij represents the chance that token i gets the token j assigned to it. Formal: Formal: For more details, see Appendix A",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.382.json"
    },
    {
        "casual_text": "We looked at the data from both English and Chinese search engines, focusing on a part of the AOL dataset. Table 2 shows the stats for different categories from both AOL and Sogou logs. Just to clarify, a \"click set\" is all the clicks tied to a specific query, not just one unique query. \n\nWhen it comes to using general user info versus individual user info, there’s no big difference in the number of records or users in both logs. But if you look at unique queries and distinct click sets, individual user info clearly wins, especially in the Sogou log. \n\nSo, even though using both common and individual user info works pretty well for half the users and half the search engine visits, the fact that there are way more unique queries and click sets shows how important individual user info is for personalizing search results. Methods that use individual user info are definitely more complex, even though some might say using common user info is easier to handle.",
        "formal_text": "The statistics of the two categories are conducted in the query log of both English and Chinese search engines. We use a subset of AOL Table 2 summarizes the statistics of different evidence categories over AOL and Sogou log. Note that click set refers to the set of clicks related to a query submission instead of a unique query. As for evidence for using common and individual user information, there is no clear distinction in terms of number of records, number of users in two logs. However, in terms of unique query and distinct click set, one can't fail to find that evidence for using individual user information clearly exceeds that for using common user information, especially in Sogou log. Therefore, though making use of common and individual user information can address equally well for half users and half visits to the search engine, the fact that much more unique queries and click sets actually claims the significance of needing individual user information to personalize web results. And methods exploiting individual user information provide a much more challenging task in terms of problem space, though one may argue utilizing common user information is much easier to attack.",
        "GPT2_formal_text": "_len is a matrix, and attention_mask_shape is a scalar. This matrix helps find the best representation for the attention_mask_len. Formal: The relevance layer, which is trained using the Dirichlet distribution, is designed to focus on the most important parts of the generated text to predict the next word. Formal: The relevance layer has three parts: relevance_label, relevance_prediction, and relevance_pooling. Formal: We treat this problem like a sequential regression problem and use a linear classifier that's been fine-tuned for sequence-to-sequence tasks. Formal: We came up with a new decoding method that uses an attention network. Instead of just looking at the attention at each step, we use the attention from all the previous words. This change helps the attention network focus on the most important parts of the text. Formal: We tested our method on two popular datasets, and the results showed that our method performs better than the previous best method. Formal: We also shared a Python library called PyTorch that helps us with modeling the attention layer. This library gives us a flexible way to set up the model, which lets us work with different languages. Formal: Since we can't predict the next word just by looking at the earlier words, we also created a baseline model using a single-layer LSTM (Hochreiter and Schmidhuber, 1997). Formal: The baseline model (Bert) was trained using a bunch of different languages and also added some language-specific features. Formal: Formal: We did a bunch of experiments to see how our method (PyTorch) performs. The results show that our method works well for English, Spanish, and French. Formal: We also compared our method (PyTorch) to the BERT model (Devlin et al., 2019). Formal: Lastly, we ran an experiment to see how the representations learned from the BERT model are spread across different layers. Formal: We tested our method against three other methods. Formal: We included some English sentences from the top-performing model in the evaluation. Formal: For the BERT model (Devlin et al., 2019), we picked 2,000 sentences from a random test set. Formal: We picked 1,000 sentences from a random test set for the PyTorch model. Formal: The English sentences were generated randomly from a training set and then fine-tun",
        "directory": "C10",
        "filename": "C10-2080.json"
    },
    {
        "casual_text": "Building dialogue systems that act like really good teachers is tricky, even though we've made some progress with end-to-end systems that can mimic human language pretty well. These systems work great because they have tons of training data and can handle a lot of information, but there's no way to make sure they always tell the truth. So, a simpler goal might be to create a system that sticks to the facts from one or more trusted sources. This kind of system could help teach people about specific topics through conversations or add extra info to a task-focused dialogue system.\n\nFor example, I go to animal shelters a lot. A \"no-kill\" shelter is one that doesn't put down healthy or treatable animals, even if the shelter is full. They only euthanize animals that are terminally ill.",
        "formal_text": "Dialogue systems that strive to be informative teachers are difficult to build, despite recent progress in training end-to-end systems that mimic human language at a linguistic level. These systems benefit from vast training data and great representational capacity; yet there are no controls (or training objectives) available that ensure they are truthful. A more limited goal for a system is to be faithful to one or more source documents that we implicitly trust. Such a system might help educate users about a particular topic through conversational interaction, or it might augment a task-oriented dialogue system by providing additional information about I visit animal shelters fairly often A \"no-kill\" shelter is an animal shelter that does not kill healthy or treatable animals even when the shelter is full, reserving euthanasia for terminally ill animals. . .",
        "GPT2_formal_text": "are used in the TICC for training, while the knowledge_ids are used for testing. Here, c is the number of knowledge_ids in the TICC. Formal: For the input, we use a dictionary with the terms from the WSJ training set, which has an 8,000-word limit. The words are broken down into tokens using the BERT base model. Formal: We handle the word embedding and the lexicon embedding separately, using an embedding layer to get the embedding for the word and the lexicon. After that, we use a softmax layer to guess the label for the word. Formal: We use the predicted label as the query for the entity type label, which is a one-hot vector with a size of 8. Formal: We calculate the embedding for the word, which is represented as a one-hot vector with a size of 4. Formal: We apply the feedforward network to the predicted entity type label, but we don't bother with the actual entity type. Formal: We calculate the entity type embedding, which is a one-hot vector with a size of 16. Formal: For the three type labels, we calculate the embedding for each word pair separately, and then use a softmax layer to guess the label. Formal: We apply the feedforward network to the predicted type labels, but we don't bother with the actual types. Formal: We use the predicted label as the query for the entity type label, which is a one-hot vector with a size of 8. Formal: We calculate the embedding for the word, which is represented as a one-hot vector with a size of 4. Formal: We apply the feedforward network to the predicted type labels, but we don't bother with the actual types. Formal: We calculate the entity type embedding, which is a one-hot vector with a size of 16. Formal: Finally, we use the predicted label as the query for the entity type label, which is a one-hot vector with a size of 8. Formal: The final entity type embedding is a one-hot vector with a size of 4, and for the three type labels, we calculate the embedding for each word pair separately, and then use a softmax layer to guess the label. Formal: We use the predicted label as the query for the entity type label, which is a one-hot vector with",
        "directory": "acl",
        "filename": "2021.acl-long.58.json"
    },
    {
        "casual_text": "For our starting point, we're using both Logistic Regression (LR) and Random Forest (RF) algorithms with a simple set of features, like just the words themselves (lexical features).",
        "formal_text": "As baselines we consider both LR and RF algorithms with a set of basic features (i. e. , lexical).",
        "GPT2_formal_text": "ed: True, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False, False,",
        "directory": "D17",
        "filename": "D17-1245.json"
    },
    {
        "casual_text": "We've conducted the first study to figure out how to automatically create Chinese character sets to make language learning easier. We tested different ways to represent characters and group them into families. The results showed that using averaged word vectors worked better than just using character vectors on their own. Plus, when we used K-means clustering, it helped create smaller groups of characters that had clear meanings. We hope these methods will make it easier to create more diverse and useful character sets.",
        "formal_text": "We have presented the first quantitative study on automatic construction of Chinese character sets to facilitate language learning. We have evaluated a number of methods for character representation and family clustering. Experimental results showed that averaged word vectors achieved statistically significant improvement over direct use of character vectors. Further, K-means clustering produced subfamilies that yielded character sets with distinctive meaning. It is hoped that these methods will help expand the variety and coverage of character",
        "GPT2_formal_text": ", s = {s1, ..., sT}, which are just vectors that are vectors with one or more of the given labels. Formal: We're using the same embedding function for the probability distribution as in (Kim, 2014). To get a probability distribution for the final result, we train a classifier using the input sentence x. Formal: The updated graph G is a directed acyclic graph, where the arrows, W(s), show the waviness or \"edge\" of each node in the graph, and e(s) represents the entropy or \"entropy\" for that node. Formal: For any graph G where |g| equals K, the classifier P(s) can be written as P(s), where P(s) is a bigram model. Basically, the probability of a node s ∈ G is calculated by this bigram model. Formal: In this part, we'll quickly talk about the related work on graph embedding and labeling. Formal: From what we've done, we're suggesting a new approach to dependency parsing that looks at both the structure of sentences and the structure of graphs. Formal: Here, we'll first explain the graph embedding model and then introduce a new method to label graphs. Formal: We're using a pretrained Graph2Tree model (Vaswani et al., 2017) for the graph embedding part. Formal: We train the graph embedding model with a gradient descent algorithm, aiming to minimize the cross-entropy loss. Formal: In the next two sections, we'll quickly go over our graph embedding model and label propagation. Formal: We're using the FST-based GAN framework (Beltagy et al., 2018) as our model. Formal: In Section 3, we introduce our graph-label propagation algorithm. Formal: In Section 4, we introduce our graph embedding model, GAN. Formal: Finally, Section 5 wraps things up and wraps up this discussion. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "bea",
        "filename": "2021.bea-1.6.json"
    },
    {
        "casual_text": "We only consider certain pairs (xi, Yi) for the number of sentences in a sentence bead. For the rest, we stick to simple combinations like 1-1, 1-2, 1-3, 1-4, and 2-2 for the number of sentences.",
        "formal_text": "We limit the pair (xi, Yi) of the numbers of sentences in a sentence bead to some probable ones. For the remnant, we allow only 1-1, 1-2, 1-3, 1-4, 2-2 as pMrs of the numbers of sentences:",
        "GPT2_formal_text": ", self, attention_mask, and arg max are both set to d. Formal: We take a multi-layer perceptron and use it to predict the attention mask and the attention distribution for the target. Formal: Next, we calculate the entropy of the embedding for the input embedding x, using the k-th convolutional layer. Formal: We get rid of the non-zero parts by using max-pooling on the output of the first convolutional layer. Formal: Finally, we combine the attention masks by multiplying them together. Formal: Using these calculated mask values, we build a softmax classifier for the target embedding. Formal: We create an attention layer using the softmax function to figure out the attention distribution for the target embedding. Formal: The softmax function helps us predict the attention mask for the target embedding. We set the size of the softmax to 100. Formal: Finally, we combine the attention masks from the target and source embeddings using max-pooling. Formal: We use the representation of the source embedding (e s i, j) as the key feature in the final decision tree. Formal: After processing, the output is a normalized vector showing the attention distributions for the source and target embeddings. Formal: We use a linear transformation to calculate the attention vector for the target embedding, which we feed into a softmax classifier. Formal: Finally, we combine the attention vectors by multiplying them with the softmax function. Formal: Formal: To get the target embedding representation, we take the output from the convolutional layer (e s i, j) and feed it into a softmax classifier. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C94",
        "filename": "C94-2175.json"
    },
    {
        "casual_text": "The PosEdiOn program is typically a Python file, but you can also use a Windows executable if you prefer. If you want to send the zipped file via email, be aware that some email providers might block it because it contains an executable file. A better option could be sharing a link to the zipped file instead, which makes it easier to distribute the post-editing tasks.",
        "formal_text": "• The PosEdiOn program itself, usually as a Python file. Optionally, a Windows executable can be also used. In this case, sending the zipped file by e-mail can cause problems as some mail providers block attachments with executable files. Alternatively, a link to the zipped file can be used to distribute the post-editing tasks.",
        "GPT2_formal_text": "= [r i, c i, ] self. set_key ( q i ) self. set_value ( v i ) end end # # Annotate key-value pairs by checking their matching tokens. Formal: Formal: Basically, we do this for each pair of tokens that have the same or very similar meanings. Formal: We'll pick the one with the lowest rank and label it as the key token. Formal: Since the ARG2SAT model only uses one key-value pair for key-value calculations, it's not possible to do multiple rounds of these assignments. But, by taking into account the feedback from the annotators, we can fix this issue. Formal: We also add a part where we pick the key token based on a similarity measure. Formal: In this part, we're just looking at a single input, Q i, and calculate a similarity score between the key and value tokens. Formal: This is based on the ratings the annotators gave for the input and how well the key and value tokens match up in terms of meaning. Formal: Formal: In the next section, we'll talk about how we pick the key and value tokens using these similarity scores. Formal: Finally, we'll use this selection process to pick the best key and value for the next round. Formal: Formal: In Section 4, we'll explain how we measure similarity, how we pick key and value tokens, and how we pick the best key and value. Formal: Formal: Table 3 has the results from using the ARG2SAT model for the task of detecting sarcasm. Formal: Formal: We'll also explain how we evaluate how well the key and value tokens work. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "To do WSI on a larger scale, we stick with the main idea from Amrami and Goldberg (2019), which is to group together sparse vectors made from the lemmas of the top-k word replacements suggested by a masked language model. This helps us save a ton of storage space and makes the representations easier to understand. But to make it work for bigger datasets, we go through all the sentences in the corpus and grab the top-k substitutes for every word in a sentence at the same time, using just one call to BERT for the whole sentence. This means we can't use the dynamic-patterns part of their method, which needs BERT to run separately for each word in each sentence. Still, as we show in Section §5.1, we get pretty good WSI results even without that.",
        "formal_text": "In order to perform WSI at scale, we keep the main intuition from Amrami and Goldberg (2019), namely to cluster sparse vectors of lemmas of the top-k MLM-derived word substitutions. This results in vast storage saving, and also in a more interpretable representations. However, for scalability, we iterate over the corpus sentences and collect the top-k substitutes for all words in the sentence at once based on a single BERT call for that sentence. This precludes us from using the dynamic-patterns component of their method, which requires separately running BERT for each word in each sentence. However, as we show in Section §5. 1 we still obtain sufficiently high WSI results.",
        "GPT2_formal_text": "or the attention_raw or attention_raw_tokens method, respectively. Formal: This creates a masked representation for a specific token or tokenization. Formal: We also have a type-based approach. This approach uses a type embedding function to create a unique type identifier for each token, while also dealing with the problem of tokenization. Formal: We use a simple type-based classifier to figure out if two tokens in a sequence are being translated in the same way, similar, or at least somewhat alike. Formal: We also use an attention-based classifier that looks at both the source and target tokens. It figures out which token might be important for the translation and creates an attention-weight vector for each token in the sequence. Formal: Lastly, we have a linear classifier that uses a linear transformation to predict the probability of the target token. Formal: We use this classifier to estimate the probability for a target token based on the source token, the rest of the tokens, and its context. Formal: Finally, we calculate the source attention weight vector by using the target attention weight vector to figure out the probability. Formal: To figure out the probability for a token and its context, we use the usual linear model. Formal: We use the classification model to estimate the probability for each token and its context. Formal: We use the classifier to estimate the probability of a token and its context, and then we use the context to estimate the probability for the target token. Formal: To get the probability from the context, we calculate the attention vector for each token in the sequence and add it to the target attention vector. Formal: Lastly, we calculate the source attention weight vector by using the target attention weight vector to estimate the probability. Formal: Formal: We use the classifier to estimate the probability for each token and its context, and then we use the context to estimate the probability for the target token. Formal: Formal: Formal: We use the classifier to estimate the probability for each token and its context, and then we use the context to estimate the probability for the target token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.325.json"
    },
    {
        "casual_text": "At this point, it should be pretty obvious why TFL (L) is a great fit for modern linguistics theories. First off, the LT part lets us talk about tree structures directly, which makes it perfect for setting rules about how words and phrases are grouped together. Plus, the LF part lets us describe more complex categories instead of just basic ones.",
        "formal_text": "At this stage, it should be intuitively clear why T F L (L) is well suited for modeling contemporary linguistic theories. On the one hand, the L T part of the language lets us talk directly about tree structure, thus clearly it is a suitable tool for imposing constraints on constituent structure. On the other hand, the L F part of the language permits the description of complex (rather than atomic) categories;",
        "GPT2_formal_text": ". It was tested on 3 English datasets from the WMT 2019 Evaluation Campaign. The system was trained using the test dataset, and the final score was calculated using the standard evaluation method. Formal: We trained an English natural language system to translate between the two languages. This system was trained using the WMT 2019 evaluation data and tested on a different WMT 2019 test set. In this new test set, the translation output was created by either using the target language's word embedding (embedding) or by running a reordering model. \n\nSince the WMT 2019 data is different from the WMT evaluation data, the system was trained using the test set of the current WMT evaluation campaign. Formal: We also ran a similar test in the development phase, but this time, we used the WMT 2017 evaluation data to train the system. For each test set, we grabbed the translated text from the test set using the Moses toolkit. Formal: For the evaluation campaign, we picked the best model based on the evaluation scores in the development phase. Formal: We used the same development set (the development set) for all three language pairs. Formal: We trained two English NLP systems, using the WMT 2019 evaluation data. The results are in Table 2. We found that when it comes to translating between German and English, the system trained on WMT 2017 evaluation data did a better job than the one trained on WMT 2019 evaluation data. The difference is statistically significant with a p-value of 0.05. Formal: The system trained on WMT 2017 evaluation data did a better job than the system trained on WMT 2019 evaluation data, with a difference of 0.10 in performance. Formal: In terms of word order, the system trained on WMT 2017 evaluation data did a better job than the one trained on WMT 2019 evaluation data. Formal: The system trained on WMT 2017 evaluation data did a better job than the one trained on WMT 2019 evaluation data, with a difference of 0.03 in performance. Formal: Formal: In both WMT test sets, the system trained on WMT 2019 evaluation data performed better than the system trained on WMT 2017 evaluation data. Formal: The system trained on WMT 2019 evaluation data did a better job than the one trained on WMT 2017 evaluation data, with a difference of 0.01 in performance. Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E93",
        "filename": "E93-1004.json"
    },
    {
        "casual_text": "Sure! Here's a simpler way to say that:\n\n\"First, grab the top paragraph and make it the first one. Then, group the rest of the paragraphs together.\"",
        "formal_text": "4. Extract the top paragraph as the first paragraph. The others are classified into the rest.",
        "GPT2_formal_text": "ed = { [CLS], [SEP ], [MASK], [SEP], [MASK] } c_s = Concatenation(v_s) Formal: (i) (j) the combination of (s_i, t_i) and (v_s, t_i) is true, and (ii) (k) if (s_i, t_i) and (v_s, t_i) are both true, then s_i is also true. Formal: We can also use multi-head attention to handle the last scenario where (s_i, t_i) is true and (v_s, t_i) is true. Formal: After that, the final output layer sends the result from all the heads to the prediction layer to get the final prediction. Formal: In this paper, we built a model that can figure out the sentiment for a question based on its title. The model learns to predict the sentiment for a given title using some specific data and an algorithm. We tested our model using a dataset of QA pairs from the popular dataset QA-QG (Zhai et al., 2019). Formal: We created a model that can predict the sentiment for a given title using an algorithm. This is kind of like what we did in our paper, but we used different data and a different algorithm. We used an algorithm that learns from two datasets of title-question pairs. Formal: The final output layer then gives a prediction to the prediction layer to get the final answer. Formal: The algorithm works with a dataset of title-question pairs to predict the sentiment for a given title. Formal: The dataset we used is the standard dataset QA-QG, which has 8,500 training questions and 1,500 test questions. We trained our model using the dataset and got the results shown in Table 1. Formal: Table 1 shows the results from the validation set for the validation set of the test set. Formal: For more details, you can check out the code available on GitHub at https://github.com/shamirw/ETC. Formal: In the evaluation, we used the same set of questions but from the test set. Formal: The average accuracy was 48.32% on the validation set and 40.39% on the test set. Formal: The first row in Table 1 shows the accuracy for the validation set,",
        "directory": "C98",
        "filename": "C98-2220.json"
    },
    {
        "casual_text": "W' is the count of words in different languages, so it doesn't include the ones we don't know. And max{w} is the highest number of words from the language that shows up the most in the sentence.",
        "formal_text": "where W' is the number of words in distinct languages, i. e. , the number of words except the undefined ones, max{w} is the maximum number of words belonging to the most frequent language in the sentence.",
        "GPT2_formal_text": ". The user's input starts with a single [CLS] token and ends with a [SEP] token. The token is used to represent the entire sentence. The input and output are both encoded using the same key k. The input, k_i, is a list of K-tuples {(i_l, i_r), (i_i, k_l), ..., (i_k, k_l)}, where each i_l, i_r, and i_i are in a specific order, as shown in Table 1. Formal: To get a better understanding of how things are changing over time, we use the TimeML dataset (Vedantam et al., 2017) to track the time evolution of the model. Formal: The original model was trained using a maximum entropy model, and we tweaked it a bit for the MLM setup. Formal: Here's a quick summary of the TimeML dataset. Formal: In this part, we’ll explain the neural models we used, the parameters we tweaked, and the parameter estimation process we used to train our model. Formal: We used three CNNs as the classifier for the relationships between entities. In the same setup, we also trained a linear layer with ReLU activation. The results are in Figure 4. We ran our model 10 times for each entity pair and averaged the results. Formal: We tested our model on four datasets. Formal: From Table 1, you can see that our model did better than the usual CNN, ReLU, and LSTM classifiers, both in terms of accuracy and ROUGE-1 scores. Formal: We also checked how the neural network setup affects the time it takes to train and how well it can predict the next entity pair. Formal: When we trained the model with only four CNNs, it took about an hour to train and about 40 minutes to train. Formal: Our model is way faster than the CNN model and can process a huge number of entities per second. Formal: We used a Gated Recurrent Unit (GRU) in the ReLU activation of the linear layer. As mentioned earlier, this GRU helps the model focus on the important parts of the input sequence and make better predictions. Formal: We’ll talk about the different types of relationships in the next sections. Formal: We tested our model on the TimeML dataset for NER and SLU tasks,",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "We're using the definition and classification of spin from Boutron et al. in 2010, which breaks down spin into different types and subtypes.",
        "formal_text": "We adopt the definition and classification of spin introduced by Boutron et al. 2010and, who divided instances of spin into several types and subtypes.",
        "GPT2_formal_text": "or projection matrix. It's not totally clear what this matrix is actually doing, but it seems like it's basically a mix of some other stuff. Formal: The score for a sentence s in the source language is calculated by adding up the scores of all the tokens it contains. Formal: Here, H is a matrix with dimensions d_s by d_t, and q_s is a matrix with dimensions d_q by d_s. Formal: To train a neural network classifier, we use an example (y(x)) and a log-likelihood (log(z)). Formal: The response y(x) is basically the one with the highest log-likelihood. Formal: In this paper, we only focus on the latent vectors z_t for training. But we're thinking of expanding this to the whole sentence, using the full context as input. Formal: We'll use the context vectors from the source sentence as the input for the neural network classifier. Formal: We'll set up the model like this: Formal: So, the log-likelihood for each token is calculated based on the context vector z_t. Formal: In this setup, the probability distribution (P_z) can be written like this: Formal: For a source sentence s, the response y(x) is the highest log-likelihood that matches the input context. Formal: We calculate the log-likelihood for the source sentence s using a neural network classifier, which is trained using the context vectors. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "bionlp",
        "filename": "2020.bionlp-1.5.json"
    },
    {
        "casual_text": "Basically, reordering makes a big difference when it comes to improving how well recognition and translation work, especially when you look at things like WER and BLEU scores. If you compare a cross-lingual language model without reordering to one with reordering, you can see some cool improvements. \n\nFor example, the model with reordering and local constraints gives a 0.70% drop in WER and a 3.06 point boost in BLEU. If you use IBM constraints for reordering, you get an even better result: a 0.85% drop in WER and a 3.58 point increase in BLEU. But the best results come from using ITG constraints for reordering—it gives the lowest WER, down 0.92%, and the highest BLEU, up 3.89 points. \n\nAll these WER improvements are super solid, with a 99% confidence level based on a two-proportional z-test. And the BLEU improvements are also strong, with a 95% confidence level from a paired student t-test using bootstrap resampling.\n\nOh, and we picked the segmentation order s = 3 because it works the best in our setup.",
        "formal_text": "In general, reordering has a significant effect on enhancing the performance of recognition and translation in the sense of WER reduction and BLEU improvement. Compared with the cross-lingual language model without reordering, the cross-lingual language model with reordering under local constraints gives 0. 70% absolute WER reduction and 3. 06 absolute BLEU improvement. The cross-lingual language model with reordering under IBM constraints gives 0. 85% absolute WER reduction and 3. 58 absolute BLEU improvement. The cross-lingual language model with reordering under ITG constraints yields the best performance, with 0. 92% absolute WER reduction and 3. 89 absolute BLEU improvement. All WER improvements pointed out here are statistically significant at 99% confidence according to a two-proportional z-test, and all BLEU improvements are statistically significant at 95% confidence according to a paired student t-test using bootstrap resampling. 8 We have chosen segmentation order s = 3 because it works the best in our system.",
        "GPT2_formal_text": ".\n\nThis idea is basically a tool that can look up any given noun phrase in a dictionary. The phrase \"John is twenty-one\" is in the dictionary as \"John is\", and \"celebrating\" is also in there. The tool will also find all the noun phrases that have the word \"celebrate\" in them. So, when we need to find the phrase \"celebrate\" in the dictionary, we just give it the phrase \"John is celebrating.\" Formal: The original proof tree, X, is set up so that the bottom part is just a bunch of symbols, called a proof frame. The prefix of the parse tree, Xp, is just the symbols from the tree. Formal: We use a special symbol called \"®\" to connect the rule (•) with the variable \"John.\" Formal: If we want to know how many times \"John\" appears, we can look at the rule for \"punctuation.\" The variable \"punctuation\" can be any number from 1 to N. Formal: We also use a symbol called \"*\" to show the substring \"®i*i\" for any specific word \"i.\" Formal: Here, the word \"®\" can be either the head or the tail of a word. In our case, \"®\" is the head of \"John.\" Formal: Here, the word \"®i\" can either be the head or the tail of a word. Formal: We use a special symbol called \"®i\" to connect the rule (•) with the variable \"John.\" Formal: A proof frame for \"John\" is basically an expanded version of a proof frame for \"punctuation.\" Formal: The part \"®i\" is a part of a bigger proof frame called the proof structure for \"punctuation.\" Formal: Formal: This whole process of figuring out the proof structure for \"punctuation\" is a bit different from the proof-building process for \"λ\" (see the part about negation in section 3.4). Formal: Here, the word \"®\" can be either the head or the tail of a word. Formal: Here, the word \"®i\" can be either the head or the tail of a word. Formal: We can use the proof structure for \"punctuation\" to get a bigger proof frame called the proof structure for \"λ.\" Formal: So, the proof structure",
        "directory": "D12",
        "filename": "D12-1070.json"
    },
    {
        "casual_text": "We tested these models by looking at how they handle entailment relations between simple intransitive sentences, verb phrases, and transitive sentences using the datasets from (Kartsaklis and Sadrzadeh, 2016). Turns out, the Frobenius models worked the best, especially when we added our sentence-level measures. Overall, our results back up what was found in earlier research (Kartsaklis and Sadrzadeh, 2016) and really show that compositional models using some kind of intersective feature selection—like point-wise vector multiplication or tensor-based models with element-wise mixing (like the Frobenius ones)—are better for entailment tasks in distributional settings.",
        "formal_text": "We experiment with these models and evaluate them on entailment relations between simple intransitive sentences, verb phrases, and transitive sentences on the datasets of (Kartsaklis and Sadrzadeh, 2016). Our findings suggest that the Frobenius models provide the highest performance, especially when combined with our sentence-level measures. On a more general note, the experimental results of this paper support that of previous work (Kartsaklis and Sadrzadeh, 2016) and strongly indicate that compositional models employing some form of intersective feature selection, i. e. point-wise vector multiplication or tensor-based models with an element of element-wise mixing (such as the Frobenius constructions), are more appropriate for entailment tasks in distributional settings.",
        "GPT2_formal_text": "ed(. ) when they're building a model. This is helpful because some actions don't need attention, and some actions do need attention, but we want to avoid having to add extra parameters for those actions. Instead, we swap out the attention for a linear layer that's part of the base RNNG. Formal: We're using a linear layer to handle both the past and future states, which is super handy for handling things like multitask learning. Formal: The model has an initial hidden state called h0, which is used to encode the current token. Each time we use it, it calculates a hidden state ht using a context vector x, as shown in Equation 8. Formal: We can also encode x by just adding up the previous hidden state ht and the context vector x. Formal: In this setup, the hidden state ht for token i gets updated based on the tokens that came before it. Formal: We're thinking of using a GCN (Graph Convolutional Network) to handle the sequence generation part, like in Figure 1. Formal: We'll also add an attention mechanism that focuses on the input hidden state, ht. Formal: This approach is different from the usual attention mechanism because it uses a linear transformation on the hidden state. Formal: We train the model using a mix of gradient descent and the hinge loss function, which helps us see how well the model is performing. Formal: To make things better, we'll also consider the effect of extra data, like when there's a new task or the model is trained on multiple tasks. Formal: We take the loss function of the model, which is L_t, and plug it into the model's output. Formal: Then, we use this combined model to predict the next token and update the hidden state. Formal: Finally, we do the same thing with the token prediction, ht. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1268.json"
    },
    {
        "casual_text": "We're using BBC-BJ for news and Desmond ODB (which is part of the Desmond dataset and focuses on religion) as our validation and test data. Here's why: (1) They're manually curated and really high-quality, (2) they're much smaller than the rest of our training data, so we're not losing too much that could have been used for training, and (3) they have similar sentence lengths to our training data. Unfortunately, there's no equivalent dataset for the conversation and general domains.\n\nFor the conversation domain, all the datasets are automatically aligned, which makes them pretty noisy. In the general domain, both Tatoeba and TALPCo are manually curated, but their sentences are way shorter compared to Wikimatrix, especially Tatoeba. So, for these two domains, we do a random split, using all the datasets in each domain for validation and testing. Each split has 2000 unique pairs that weren't in the training set.\n\nFor the general domain, we mix shorter sentences from TALPCo with longer ones from Wikimatrix for our validation and test data. We noticed that Tatoeba has high-quality sentences similar to TALPCo, just shorter. Since longer sentences make for a more challenging and meaningful evaluation, we decided to include TALPCo in our validation and test sets instead.",
        "formal_text": "• BBC-BJ for news, and • Desmond ODB (Our Daily Bread, the religion part of Desmond dataset) for religion, to be our validation and test data because (1) they are manually curated and of high-quality, (2) they are much smaller than the rest of training data and therefore do not sacrifice too much portion of data that could have been for training instead, and (3) they have similar sentence length compared to the training data. There is no such corpus for the conversation domain and the general domain. The datasets in the conversation domain are all automatically aligned and therefore are noisy. For the general domain, both Tatoeba and TALPCo are manually curated, but their sentences (especially Tatoeba) are very short compared to Wikimatrix. Therefore, for these two datasets, we do a random split involving all datasets in the domain for validation and testing, each having 2000 unique pairs not present in the training set. For the general domain, we mix shorter sentences from TALPCo and the longer ones from Wikimatrix as our validation and test data. We observe that Tatoeba has similar types of high-quality sentences like TALPCo has, albeit shorter. Therefore we choose TALPCo to be in the validation and test sets instead, because longer sentences mean more difficult and meaningful evaluation.",
        "GPT2_formal_text": "= { q1, q2, ..., qj } for each mention m_i. Formal: This mapping decides how much attention to give to each mention m_i when it's generating stuff. Formal: We add some bias to this attention by changing its shape. Formal: The model does a better job at generating stuff if m_i and m_j are in the same embedding vector. But if m_i and m_j are different embedding vectors, we just ignore them. Formal: The model can be tweaked to learn how to handle different embedding vectors by using a different alignment method. Formal: After the attention mechanism is trained, it's a good idea to use the same model for different languages, so we can compare how they do differently with the data we have. Formal: If you try to tweak the model for multiple languages, it might not work as well because the model's parameters can get messed up by different embedding vectors. Formal: We got rid of the word embeddings of each mention m_i in the dataset. Formal: The model learns how to generate stuff by doing this: Formal: First, it aligns the mentions in the target language to the embedding vectors of m_i. Formal: Then, it maps the words in the source language to the embedding vectors of m_i. Formal: After that, it uses those words in the source language as extra context to figure out the embedding vectors of m_i. Formal: Once it knows the embedding vectors of m_i, the model can generate text. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "bucc",
        "filename": "2020.bucc-1.6.json"
    },
    {
        "casual_text": "To figure out how well different ranking systems or models worked for comparing realizer systems, we looked at seven different \"systems\" from the whole dataset. These included five OpenCCG-based systems (three baseline models, plus the best and worst from the full perceptron model) and two XLE-based systems (the best and worst after ranking with an n-gram model). We compared the average adequacy and fluency scores of each of these seven systems to see how they stacked up against each other, which gave us 21 pairs to compare. Then, we used Tukey's HSD test to check which systems had significantly different scores. The test showed that five of the pairs had noticeably different results.",
        "formal_text": "In order to investigate the efficacy of the metrics in ranking different realizer systems, or competing realizations from the same system generated using different ranking models, we considered seven different \"systems\" from the whole dataset of realizations. These consisted of five OpenCCG-based realizations (the best realization from three baseline models, and the best and the worst realization from the full perceptron model), and two XLE-based systems (the best and the worst realization, after ranking the outputs of the XLE realizer with an n-gram model). The mean of the combined adequacy and fluency scores of each of these seven systems was compared with that of every other system, resulting in 21 pairwise comparisons. Then Tukey's HSD test was performed to determine the systems which differed significantly in terms of the average adequacy and fluency rating they received. 4 The test revealed five pairwise comparisons where the scores were significantly different.",
        "GPT2_formal_text": ", and the last one is the text embedding. Formal: The sentence embedding gets fed into the word embedding, which then gives us the updated hidden representation for that sentence. Formal: The word embedding is then used to calculate the hidden representation for the entire text, including the ones not yet processed. Formal: Lastly, the attention weights for the two output channels are tweaked based on the context-aware model's output. Formal: For more details, check out [Klebanov et al., 2018] Formal: The cross-attention mechanism, which we'll call A_CM, takes in an input embedding E_E and some feature vectors f_f, g_g, h_h, and spits out an output embedding called E_e. Formal: Then, the student-generated representations are compared to the original representations. Formal: The cross-attention mechanism is trained using cross-entropy loss to update the cross-attention weights in A_CM. Formal: The student-generated representations are sent to the teacher to be used during training. Formal: After that, the student's representation is used to calculate the hidden representation for the whole text. Formal: The last layer of the student is used to calculate the hidden representation for the whole text. Formal: For more details, check out [Vinyals and Le, 2015] Formal: Here, E_e is the output from the last layer of the student, which is the input embedding. Formal: Finally, the student's representation is used to calculate the hidden representation for the whole text. Formal: Formal: The final layer of the student's output is used to calculate the hidden representation for the whole text. Formal: Formal: Formal: Here, E_e is the output from the last layer of the student, which is the input embedding. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D10",
        "filename": "D10-1055.json"
    },
    {
        "casual_text": "The analysis we just talked about shows that doing the same search over and over isn't the main thing that matters. Instead, it seems that using info about individual users is just as important, if not more so, than using general info about all users. This part here is trying to dig deeper into this idea to help us figure out how to balance the importance of individual user data with common user data from a research perspective.",
        "formal_text": "The above analyses quantitative analyses have shown that the repetition of search is not the statistically dominant factor, with the impression that employing individual user information is equally, if not more, important than common user information. This section tries to further reveal this issue so as to balance the position of individual user information and common user information from a research point.",
        "GPT2_formal_text": ". For this example, the models were trained on the training set for both the source and target domains. Formal: We looked at the log probabilities for the words in the input text for the first time and then calculated the cosine similarity between the reference and generated captions. Formal: We also calculated the cosine similarity between the generated captions and the reference captions. Formal: So, in our experiments, we picked the highest similarity value in each direction, and the mean of those two scores gave us the results for our experiments. Formal: We calculated the cosine similarity for the generated captions to check how similar they were. We made sure to always keep the original captions in the reference captions and the generated captions in the generated ones. Formal: These results are based on the subjectivity-aware model from Section 4. Formal: The log probabilities were adjusted based on the training set data and the hyperparameters we set. Formal: Since the generated captions are generated automatically, we didn’t need to tweak any of the models. Formal: Figure 4 shows an example where the reference text is compared to the generated text. Formal: We tried using different amounts of reference captions to compare the generated ones with the reference. To see how much better or worse the generated captions would perform, we compared the log probabilities of the reference and generated captions. Formal: Formal: We set a threshold to minimize this similarity value. The best result came when we set the threshold to 0.2, which is the highest value we found in our experiments. Formal: We used the standard cosine similarity and normalized the scores. Formal: We calculated the cosine similarity between the reference and generated captions using the standard model from Section 4. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C10",
        "filename": "C10-2080.json"
    },
    {
        "casual_text": "For the embedding layer, we went with the pre-trained GloVe 3 embeddings (Pennington et al., 2014) that were trained on 6 billion words from Wikipedia 2014 and Gigaword 5. These embeddings get fine-tuned during training. We limited the vocabulary to the 4,000 most common words, following Taghipour and Ng (2016), and treated any other words as unknown. We set the number of sentences per essay to the maximum for each prompt and capped the sentence length at 128. The models were trained in batches of 16 for 50 epochs. We tweaked the hyperparameters using optuna over 100 trials.",
        "formal_text": "In the embedding layer, we used the pre-trained word embedding GloVe 3 (Pennington et al. , 2014) trained on 6 billion words from Wikipedia 2014 and Gigaword 5. During the training process, word embeddings are fine-tuned. The vocabulary was set to the 4, 000 most frequent words by following Taghipour and Ng (2016) and treating other words as unknown words. We set the number of the essay sentences to the maximum for each essay prompts and the maximum sentence length to 128 and trained the models on batch size 16 for 50 epochs. The following hyperparameters were tuned by using optuna 4 in 100 trials.",
        "GPT2_formal_text": "_t, p_t). Formal: For the input layer, we don't bother with LSTM operations since they're basically a straightforward way to grab sentence representations. This setup lets us train a single model using just one batch of data, assuming we have enough examples to train it. Formal: We’re suggesting a different approach: we train a multi-task neural network (MTN) using only the training data that’s relevant to the tasks we’re interested in. In this setup, we pick the task-specific data that’s most relevant to the data we’re working with. Formal: To make use of all the data we’ve got, we use a method called maximum likelihood estimation (MLE). This method helps us figure out the best parameters to optimize. Formal: We create a scoring function (r_f) to score the examples in the dataset based on the labels. Then, we use the MLE algorithm to get the final result, which we’ll call (r_f, y). Formal: MLE is all about optimizing a scoring function that considers how different the inputs are. It’s kind of like a black box, which makes it super efficient. Formal: In our setup, we use the MLE algorithm to figure out the scores for the examples in the dataset. Formal: For each pair of examples in the dataset, we assign a score, let’s call it arg_t, which is calculated by: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "aacl",
        "filename": "2020.aacl-srw.17.json"
    },
    {
        "casual_text": "The context for the current character w_t comes from the characters around it within a window of size c, along with their segment labels (check out Figure 2). The characters w_i and their labels l_i in this context are turned into vectors, e_c(w_i) and e_c(l_i), both in R^d, where d is the size of the embedding.",
        "formal_text": "the pre-training context of current character w t is the around characters in the windows with size c, together with their corresponding segment labels ( Figure 2 ). Characters w i and labels l i in the context are represented by vectors e c w i ∈ R d and e c l i ∈ R d, respectively, where d is the embedding dimensionality.",
        "GPT2_formal_text": "(1) and (2) are the same thing, but the (3) part has a different meaning. Formal: When dealing with a matrix R, the attention matrix A, which is just the sum of the attention vectors for the (1, 1)-th row, can't be bigger than d_i. So, by combining the attention vectors from the (1, 1)-th row, we can get the attention matrix A for the whole sentence. Formal: A_i = A(1, 1)-d_i. Formal: But if you're dealing with a matrix X, the input attention matrix A, which is the sum of the attention vectors for the (1, 1)-th row, can be larger than d_i. So, we use a variational autoencoder to get the attention matrix A for the whole sentence, which we call A_ij = A(1, 1)-d_ij. Formal: Once we've got the attention matrix A, we can then use it to figure out the attention weights for the (1, 1)-th and (1, 2)-th rows. Formal: To get these attention weights, we use the method described in Section 2. Formal: In Section 3, we talk about the constraint optimization for each type of relation. Formal: So, in the constraint optimization step, we first figure out the attention weights for the (1, 1)-th row using the variational autoencoder, and then we update the attention weights for the (1, 2)-th row by applying the constraint optimization. Formal: The process of constraint optimization is shown in Figure 3. Formal: For the third type of relation, the attention weights for the (1, 2)-th and (1, 3)-th rows are updated by minimizing the constraint. Formal: The final attention weights are calculated based on the constraints in the constraint optimization for the third type of relation. Formal: Formal: Figure 4 shows the constraint optimization for the fourth type of relation. Formal: For the fourth type of relation, the attention weights for the (1, 3)-th and (1, 4)-th rows are updated by minimizing the constraint. Formal: We repeat this process for every type of relation. Formal: Figure 5 shows the constraint optimization for the fifth type of relation. Formal: Formal: Formal: This paper introduces a new method called constraint optimization for constraint-satisf",
        "directory": "D17",
        "filename": "D17-1079.json"
    },
    {
        "casual_text": "The model setups mentioned in Section 2.1 are different because they look at different \"window\" sizes on the output structure. Instead of changing the window size, we can get the same result by keeping the window size the same but tweaking the output values. For instance, imagine changing the output values by swapping single output tags with pairs of neighboring output tags.",
        "formal_text": "The common model structures described in Section 2. 1 differ from one another in that they examine varying sizes of \"windows\" on the output structure. Rather than varying the size of the window, we can achieve the same effect by fixing the window size, but transforming the output values. For example, consider the effects of transforming the output values by replacing individual output tags with pairs of adjacent output tags:",
        "GPT2_formal_text": "er/hidden_task.py 2. You'll need to set the random initialization step and tweak the hidden parameters to fit your setup. Formal: The final output is a bunch of vectors, each with a size of d_T. These vectors are the key parts of the attention vector for the relation token. Formal: For each token r_i, we pick the attention vectors h_t and h_c_i for it. Formal: The final output is a bunch of vectors, each with a size of d_T. These vectors are the key parts of the attention vector for the relation token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D07",
        "filename": "D07-1084.json"
    },
    {
        "casual_text": "For a character-based Chinese NER model, the input sentence is treated as a sequence of characters.",
        "formal_text": "For a character-based Chinese NER model, the input sentence is seen as a character sequence",
        "GPT2_formal_text": "( ) = [ ] # The words from the question and the answer that should be in a conversation. Formal: So, p_q = w_q_t + b_q_t, and p_o = w_o_t + b_o_t. Formal: In the offline model, we start by calculating the value of the contextual representation for the question and the answer. This is shown in equations (6) and (7). Formal: Also, by mixing the attention weights from the question and the answer, we can make the questions more varied. This helps the model pick up on the important details about the question that the answer might not cover. Formal: In the offline model, we start by calculating the value of the contextual representation for the question and the answer. This is shown in equations (6) and (7). Formal: For the online model, we calculate the contextual representation for the question and the answer using the dialogue history. Then, we add these representations to the question and answer to get the final answer. Formal: The goal here is to make the dialogues in the question and answer more varied. As we mentioned earlier, adding a knowledge graph to the dialogue history helps. Formal: The results from the online model match the real-world outcomes better. Formal: To see how well the knowledge graph can be used, we test it on two datasets. Formal: In the first dataset (D_q), we train on the dialogues from the D_qa dataset, which has 1.2 million dialogues. For the second dataset (D_q2), we train on the dialogues from the D_q2 dataset, which has 1.2 million dialogues. Formal: We compare the results from D_qa and D_q2 using accuracy, precision, recall, and F1 score. Formal: The results from D_qa and D_q2 are in Table 1. Formal: The results from D_q and D_q2 are in Table 2. Formal: Formal: We also use the evaluation results from D_qa and D_q2. Formal: To check how well the model handles questions that are multi-hop, we use the evaluation results from D_qa and D_q2. Formal: Table 2 shows the results for D_qa and D_q2. Formal: Formal: Table 3 shows the results for D_q",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way.\n\nWe have a bunch of pairs like (f_s, f_t) that can either be 0 or 1, and there are n of them. Now, we're defining something called W, which takes a feature f and gives us a real number. \n\nFor each feature f that looks like (f_s, f_t), W(f) is calculated like this: \n\nW(f) = (#(f_s, f_t) / #(f_s)) * (#(f_s, f_t) / #(f_t))\n\nBasically, it's the count of (f_s, f_t) divided by the count of f_s, multiplied by the count of (f_s, f_t) divided by the count of f_t.",
        "formal_text": "{(f s, f t ) 1, (f s, f t ) 2, . . . , (f s, f t ) n } ∈ {0, 1} n. Define W: f → R s. t. foreach feature f =(f s, f t ) W (f) = #(f s, f t ) #(fs) × #(f s, f t ) #(ft)",
        "GPT2_formal_text": ". The model gets the attention for each word by looking at a set of multi-head attention vectors. Formal: In equation (3), the weight for the first attention head, which is 1, is calculated based on the average of the average of the attention weights for the entire sequence. In other words, the attention weight for the first attention head is 1 divided by the average of the attention weights for the whole sequence. Formal: The final attention weight for the first attention head, t, is calculated as the average of the attention weights for all the tokens in the input sequence. Formal: The final attention weight for the first attention head, t, is calculated as the average of the attention weights for all the tokens in the input sequence. Formal: If the attention weights are distributed correctly, this equation will work out nicely. Formal: If the attention weights are not distributed correctly, the equation will give the wrong value. Formal: If the attention weights are correctly distributed, the final attention weight for the first attention head, t, will be 1. Formal: Formal: But if the attention weights are not correctly distributed, the final attention weight for the first attention head, t, will be -1. Formal: Formal: Formal: Formal: The attention weights for the first and second attention heads, t and t2t, are chosen randomly, so the final attention weights for both heads will have the same range. Formal: The final attention weights for the first and second attention heads, t and t2t, are chosen randomly, so the final attention weights for both heads will have the same range. Formal: The attention weights for the first and second attention heads, t and t2t, are chosen randomly, so the final attention weights for both heads will have the same range. Formal: We’ll call the two attention heads that produce the same sequence of tokens, t and t2t. Formal: The attention weights for the first and second attention heads, t and t2t, are chosen randomly, so the final attention weights for both heads will have the same range. Formal: The attention weights for the first and second attention heads, t and t2t, are chosen randomly, so the final attention weights for both heads will have the same range. Formal: The attention weights for the first and second attention heads, t and t2t, are chosen randomly, so the final attention weights for both heads will have the same",
        "directory": "D08",
        "filename": "D08-1037.json"
    },
    {
        "casual_text": "• Transformer: We're using a basic Transformer model for training. Here are a couple of links for reference: 4) https://pypi.org/project/langdetect, and 5) http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2018/baseline/dataPreparationJE.html.",
        "formal_text": "• Transformer: We train a base Transformer 4 https: //pypi. org/project/langdetect. 5 http: //lotus. kuee. kyoto-u. ac. jp/WAT/ WAT2018/baseline/dataPreparationJE. html.",
        "GPT2_formal_text": ", and a translation model with a feature vectorizer. Formal: 2. To test how well the S-DIORA model is doing, we use two types of cross-entropy loss as our evaluation metric: Formal: 3. We also throw in a simple linear layer with a linear activation function. Formal: 4. For each pair of sentence pairs (s_i, t_i) in the dataset, we calculate the cross-entropy loss like this: Formal: 5. Finally, we use a standard label smoothing method to minimize the cross-entropy loss. Formal: We used the SMatch-11 dataset from the CoNLL-2009 shared task (check out Section 2.3 for more details) as the source data for our experiments. Formal: We also did some extra experiments using the development set for our S-DIORA model to see how it compares to other supervised methods. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.147.json"
    },
    {
        "casual_text": "Just like the earlier phrase segmentation models (Lee et al., 2011; Xiong et al., 2011), we’ve added the phrase segmentation model as an extra feature function into the log-linear model.",
        "formal_text": "Like the previous works for phrase segmentation model (Lee et al. , 2011; Xiong et al. , 2011), we integrate the phrase segmentation model into the log-linear model as an additional feature function.",
        "GPT2_formal_text": ", and self-attention_hidden_iv. We can say that the attention is random if it doesn't follow any regular pattern. If that's the case, the model will either give a prediction that doesn't make sense or spit out a bunch of random stuff. Formal: If the output isn't normalized, the model might think the response is unrelated to the input. If that's the case, the model might keep throwing out responses that don't fit, like: Formal: This is where we start making the model fit the input better by focusing on how the word and response interact. Formal: The attention weights for the pre-trained model are handled through a linear transformation. Formal: Once the model has learned the attention weights, it can get rid of unwanted outputs by doing a linear projection. Formal: The final attention weights for the text-to-text model can be written like this: Formal: In the text-to-text model, the attention weights can be thought of as \"gold\" vectors for the attention in the input. These weights are turned into binary features, which are then used to reconstruct the text. Formal: In the text-to-text model, instead of using the gold vector, the model gets a bunch of the model's output h t. Formal: These h t are then passed through a linear transformation to get the attention weight vector x t. Formal: In the text-to-text model, we can write the attention weights as a bunch of binary features. Formal: Here, \"t\" represents the number of outputs from the text-to-text model. Formal: The final attention weights for the text-to-text model can be written like this: Formal: Formal: In the text-to-text model, we can calculate the attention weights by doing linear projections, which are then used to reconstruct the text. Formal: Formal: The attention weights for the text-to-text model can be written like this: Formal: Formal: Formal: Formal: The attention weights for the text-to-text model can be written like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C12",
        "filename": "C12-2060.json"
    },
    {
        "casual_text": "To hit the second goal, we’re looking at two different ways to create questions from a passage. The first method uses a retrieval-based approach with a convolution neural network (CNN), and the second one is a generation-based method that uses a recurrent neural network (RNN). We’ll check how well these methods work by looking at the BLEU score (Papineni et al., 2002) and also getting feedback from people. In Section 9, we’ll talk about the good and bad sides of each method.",
        "formal_text": "To achieve the 2 nd goal, we explore two ways to generate questions for a given passage, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN). We evaluate the generation quality by BLEU score (Papineni et al. , 2002) and human annotations, and discuss their pros and cons in Section 9.",
        "GPT2_formal_text": "= {q, a, t}, which you can find in Table 4. Formal: The (t, a, t) triplet is just a list of pairs where t is in T and a is in R. Formal: The (t, a, t) triplet is just a list of triples where t is in T, a is in R, and t is in T R. Formal: Using these representations, we can create a new version of our vocabulary, which we'll call V, by just adding up all the vectors. Formal: Once we have V, we use it to figure out which words are important and which ones aren't. Formal: We need to pick a few words to make a clear meaning for a specific clause, so we can use them to help build a bigger meaning. Formal: Let’s start by giving you a quick look at how we handle lexical repetition. Formal: We define a kind of tree called the lexical tree, which is basically a bunch of nodes connected by edges. The edges show how these nodes can be rearranged. Formal: We have two types of rules: those that look at the structure of the tree and those that look at the structure of the child nodes. Formal: Since we don’t have any variables in our vocabulary, we just do what’s called a left-corner parsing. Formal: The tree we get back looks like this: `n = tree1, ..., treen` where each node has three parts: `n1` is the root node, `n2` is the subtree above it, and `n3` is the subtree below it. Formal: Using this, we can use a left-corner parsing algorithm to create a left-recursive context-free grammar (LCF) for our new vocabulary. Formal: The tree we get back will have parts in it where nodes are repeated, and there’s a rule that says the root node can only be repeated once. Formal: We also have a function called `rec(t)` that takes the tree we get back and gives us the most specific parts of the lexical tree that can be used as the main meanings for a specific word. Formal: We can use these specific parts to build a new meaning for a word, which we’ll call `V. Formal: For the rest of this section, we",
        "directory": "D17",
        "filename": "D17-1090.json"
    },
    {
        "casual_text": "So, the idea here is to use some basic rules to change words into their closest Modern Standard Arabic (MSA) version, like what the MT system 1 already does with its training data. These rules can be made manually and then improved by using tools like GIZA++ (Al-Onaizan et al. 1999) to learn from bilingual or bi-dialectal data. There are also modules for phrasal extraction, as explained in (Och and Ney, 2004) and (Koehn et al., 2007). \n\nEach dialect has its own training process to create translation pairs between dialectal Arabic and MSA. This training uses a bi-dialectal corpus, but a small part of it is kept aside for tweaking parameters and testing. \n\nAfter that, all possible ways a word could be transliterated are considered, especially for Arabic written in Latin characters, like \"tawle\" for \"table\". Each option is weighted based on how much it changes from the original form.",
        "formal_text": "• by a character based dialect normalization, which utilizes simple rules to convert words into the most similar MSA word, as seen in the training data of the MT system 1. These simple rules can be hand-coded and enriched by rules which can be learned from bi-dialectal 2 with bilingual alignment modeling software like GIZA++ (Al-Onaizan et al. 1999), and phrasal extraction modules, as described in (Och and Ney, 2004) and (Koehn et al. , 2007). There is a separate training process to generate dialect Arabic/MSA word and phrase translation pairs for each dialect. This training is carried out on the bidialectal corpus, leaving out a small subset of sentences for parameter optimization and test. For further processing, all possible permutations and possibilities of transliteration of each word are taken into consideration, weighted by the grade of transformation from the original form. This step allows processing of Arabic which is encoded in latin characters, e. g. \"tawle\" for ‫, \"&%و#! \"‬ i. e. English \"table\".",
        "GPT2_formal_text": "ed_pos vector: This vector is a weighted sum of word embeddings, each with a weight of 0.5. The attention layer uses a linear transformation with a window size of 4, and the hidden state of the encoder is passed through a GRU. The output is the normalized distance between the predicted token embedding and the actual token embedding. Formal: We're using a linear transformation for the embedding layer, with a window size of 4. The input vector is created by combining the contextual representation for each word and then feeding those into the GRU to get the output. Formal: To break down the embedding layer into individual words, we take the average of all the embeddings from the encoder. Formal: To figure out the attention layer's embedding, we take the average of the attention vectors for the correct token. Formal: To get the output, we use the GRU's output along with the normalized distance between the token embedding and the correct token embedding. Formal: If the text gets too long, we just skip it. Formal: We set the number of tokens to 100,000 for the source dataset and 50,000 for the target dataset. Formal: We calculate the loss using the Adam algorithm, with a learning rate of 0.0001. Formal: For the target dataset, we train the model for 100 epochs. Formal: We evaluate the model on the development set using the MERT evaluation metric. Formal: In the end, we train the model for 300 epochs. Formal: After each epoch, we update the model's parameters. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "2010.amta-papers.5.json"
    },
    {
        "casual_text": "For spaCy, we stuck to the usual recommendations. All the other methods were done using TensorFlow. As for BERT models, we went with HuggingFace's implementation (shoutout to Wolf et al., 2020). We also used Adam (thanks, Kingma and Ba, 2015), Glorot initialization (props to Glorot and Bengio, 2010), and the categorical cross-entropy loss.",
        "formal_text": "For spaCy, we followed the recommended practices. 12 All other methods were implemented in tensorflow. 13 Concerning bert models, we used the implementation of huggingface (Wolf et al. , 2020). We also use Adam (Kingma and Ba, 2015), Glorot initialization (Glorot and Bengio, 2010), and the categorical cross-entropy loss.",
        "GPT2_formal_text": "= {[CLS], [E1], [E2], ..., [EI]};\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},\n\n{\n\n},",
        "directory": "acl",
        "filename": "2022.acl-long.303.json"
    },
    {
        "casual_text": "So, for each utterance in the set {U1, U2, ..., UN}, we use max-pooling on the word embeddings within that utterance to get an individual embedding, which we'll call eui. This process gives us a collection of utterance embeddings like {eu1, eu2, ..., euN} for the entire dialogue.",
        "formal_text": ". . , U N }, we get the individual embedding e u i by max-pooling on the contextual word embeddings within the U i. In this way, we obtain a set of utterance embeddings {e u 1, e u 2, . . . , e u N } in one dialogue.",
        "GPT2_formal_text": "ed_vec (x) represents the average attention vector for the word x. Formal: But our model isn't stuck with just one attention mechanism. Sure, the model might pay attention to different words and phrases at the same time, but that's not what we're focusing on here. Instead, we're looking at the attention value itself. Formal: In the part where we figure out the attention mask, we need to think about how much each word in the query should be related to each word in the answer. Formal: Here, we want to figure out the attention mask for the query q. Specifically, we want to find the mask that maps the query into a vector space where the words in the query are focused on. The idea is that words that are connected in this vector should also be related in the attention vector. Formal: We call this vector space gq, where each row (w) in gq is a vector that represents the query word w. We represent gq using something called a GRU (a type of neural network). Formal: So, to make gq work, we add a unit function to gq that looks at the query word w and the word w' in gq. Formal: We use an attention matrix called h for this. In this case, h is a unit function that maps words into a vector space using a GRU. Formal: Finally, we add a softmax function to gq to get the predicted vector. Formal: We tested this using a dataset called the SQuAD dataset (Rajpurkar et al., 2018), which has questions made by humans. Formal: In the paper, we use two different attention methods: one that focuses on words at the query level and another that focuses on words at the answer level. Formal: We're checking out the accuracy of these attention methods in the answer prediction. Formal: We also use a feature representation that's built from the graph we got from Equations 8 and 9. This feature representation helps us predict the attention mask. Formal: We also train the model using the full SQuAD dataset. Formal: Formal: Formal: In the SQuAD dataset, we set the number of dimensions to 100 for both the embedding matrix and the hidden representation. Formal: The model we're using is trained using the development set. Formal: Formal: We also use the answer prediction validation set to make sure the model is on the",
        "directory": "ccl",
        "filename": "2020.ccl-1.102.json"
    },
    {
        "casual_text": "Step 3 is basically the same as the word-level NLL process, but this time it's done on the freshly created data set. You can see this in Figure 1, right in the middle.",
        "formal_text": "Step (3) is identical to the word-level NLL process except now on the newly-generated data set. This is shown in Figure 1 (center).",
        "GPT2_formal_text": "= {h_i1, ..., h_iL}_l = {h_1, ..., h_L}_l = {t_1, ..., t_L}_l = {t_i, ..., t_iL}_i = {d_i1, ..., d_iL}_i = {d_i1, ..., d_L}_i = {t_i, ..., t_L}_i = {t_iL, ..., t_L}_i } Formal: The way to build a vocabulary is by using a specific formula. The formula for a specific word is usually made by looking at its context. Formal: When you have a set of characters, a bunch of them are probably the same. So, to make sure we don't end up with repeating characters, we use a method called similarity. Formal: We use a similarity function on each character in the sentence to find the best way to break it down. Formal: After that, we add the best result to the vocabulary. Formal: Finally, we go through and replace the characters we've already chosen with the ones that match the context. Formal: This way, the sequence of characters is broken down into a simpler form. Formal: Then, we get rid of the rest by using a filter that takes into account the similarity function we calculated earlier. Formal: Here, f_i is just a function that helps us find the best match. Formal: After that, the result from the similarity function is turned into a vector. Formal: This vector is then used as a key to get the next one. Formal: Here, the k-th element of the vector is the probability of the sequence being the next one. Formal: In the next part, we'll use this idea of \"next\" in a different way. Formal: We'll also look at how different groups of characters relate to each other. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1139.json"
    },
    {
        "casual_text": "Let’s start by looking at a simpler version of the \"learning with hints\" problem. Imagine we’re only focused on learning f₂. We’ve got a little bit of data that’s already labeled with f₂ (we’ll call this D) and a whole bunch of data labeled with f₁ (we’ll call this D_unlab—the \"unlab\" part means it’s unlabeled as far as f₂ is concerned).",
        "formal_text": "We begin by considering a simplified version of the \"learning with hints\" problem. Suppose that all we care about is learning f 2. We have a small amount of data labeled by f 2 (call this D) and a large amount of data labeled by f 1 (call this D unlab -\"unlab\" because as far as f 2 is concerned, it is unlabeled).",
        "GPT2_formal_text": "ed. Formal: The attention is weighted based on the rank of the input at each time step, and the size of the hidden layer is set to 50. Formal: To make sure the attention weights aren't influenced by the masked token, we use something called residual connections in the model. This means the weights are only pulled from the masked token's position and don't depend on the original input. Formal: Since the last hidden layer of the output layer is only 50 units long, the model has a hidden dimension of 50. This setup helps the model focus more on the masked token's position and memory. Formal: This method has two main benefits: (1) the attention weights don't get influenced by the original input, and (2) the model can predict the masked token's hidden vector with a higher accuracy compared to the original input. Formal: The model keeps the same number of parameters as the original LSTM. Formal: In our experiments, we tested how our model performed on the MNLI dataset, which is a long-term language identification task. For MNLI, we set the validation set to 100 and the test set to 50. Formal: We also tried using the Iterative Co-Attention Mechanism, which is another technique for classifying text. We started with a batch size of 1 and a batch size of 4. We added a linear layer with d_i = d_1 + d_2 + ... + d_n, along with an LSTM layer with d_i = d_1 + d_2 + ... + d_n, to handle the problem of long-term text classification. Formal: The total number of parameters in the model is 2N, and the hidden size is 200. Formal: Following the approach from Luong et al. (2015), we started with a hidden size of 50 and a batch size of 3. We stuck with the same setup for all the experiments. Formal: Following Luong et al. (2015), we used the same set of parameters for all the experiments. Formal: To check how well the model performs, we applied the AdamW optimizer (from Kingma and Ba, 2015) on the validation set. Formal: To avoid overfitting, we used a patience of 0.1. Formal: We then ran a grid search (with a beam size of 20) to find the best parameters. Formal: To check the loss function, we used the",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "The results for French to English translation aren't as good: neither cont nor para on their own beat the baseline for any metrics. But when we combine them, they do better than the baseline across all metrics except BLEU, with a drop of -1.07 in TER. Looking closer at the CLE results, it seems like adjectives and nouns got the most benefit from using our extra models. Verbs, which improved a bit, are super inflected in French, so finding examples for a specific form is trickier than for less inflected word types, and finding paraphrases with the right inflection is also harder. Also, translating through English as a pivot can lead to paraphrases that aren't always great, especially when the source language is less inflected. Plus, our simple context modeling might not have been good at filtering out some bad examples. Overall, para worked better when English was the source language, improving over the baseline across all metrics.",
        "formal_text": "Results on French to English translation are less positive: neither cont nor para alone improve over the baseline with any metrics. However, their combination improves over the baseline with all metrics except BLEU, including a reduction of -1. 07 in TER. Detailed analysis of CLE results shows that the translation of adjectives and nouns benefited more from using our two additional models. Verbs, whose translation improved slightly, are strongly inflected in French, so finding examples for a given form is more difficult than for less inflected word categories, as is finding paraphrases with the appropriate inflection. Also, pivoting via English is one reason why paraphrases obtained via a low-inflected language can be of varying quality. Furthermore, the simplicity of our context modeling may have been ineffective in filtering out some bad examples. Overall, para was more effective with the low-inflected English as the source language, improving over the baseline with all metrics.",
        "GPT2_formal_text": "is an array of word embeddings from the target domain. In the second part, we adjust the token embedding (y_t) so it works with the target domain word embedding (y_t). We can easily find the best word embedding by checking the difference between the predictions from the teacher and student models. Formal: The final score for entity e_i is calculated like this: Formal: We use a gradient descent method to update our model parameters, and we train it using the validation set to improve performance. Formal: During training, we keep updating the model parameters using the validation set, and we also check how well the model performs on the test set. Formal: Basically, the model learns from the validation set by using the teacher's predictions. Formal: If the model does better than the teacher, we set the teacher's probability to 0. Formal: The model is trained using a self-supervised learning method, with the goal of improving the model's performance. Formal: The model gets a performance boost from the validation set, and we train it using the validation set to make the model more robust. Formal: We also adjust the model's parameters, which are the parameters we learn from the validation set, using the test set to improve the model's performance. Formal: The goal here is to make the model's prediction more accurate by adjusting the model's parameters. Formal: Formal: We train the model using the validation set to improve its performance. Formal: In this project, we used the validation set to train the model. Formal: The validation set helps us train the model in a robust way. Formal: We also use the validation set to train the model. Formal: We use the validation set to train the model. Formal: We also use the validation set to train the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D10",
        "filename": "D10-1064.json"
    },
    {
        "casual_text": "We tested the three generators on a total of 300 sentences. For GPT-2, we used the first two words of a sentence as the starting point (prompt), made sure the generated text was about the same length as the original, and collected 10 different versions. With RoBERTa and T5, we messed with the original sentence three times, adding up to three [MASK] tokens each time, and asked the generator to create 5 different versions using beam search, just like Ribeiro et al. (2020) did. Polyjuice works similarly to RoBERTa and T5 when it comes to adding the [MASK] tokens, but we also tried out all the control codes. For each sentence, we randomly picked 5 versions from each generator to create the final set.",
        "formal_text": "We run the three generators on 300 sentences in total. In GPT-2, we take the first two words of an x as the input context (prompt), limit the length of the generation to be similar to x, and collect 10 counterfactuals. As for RoBERTa and T5, we repeatedly perturb x for three times, each time randomly placing up to three [MASK] tokens, and ask the generator to generate 5 counterfactuals through beam search, following Ribeiro et al. (2020). Polyjuice uses the same blank (mask) placement as in RoBERTa and T5, but we additionally enumerate through all control codes. For each x, we randomly sample 5 counterfactuals to formX per generator.",
        "GPT2_formal_text": "and query_keywords. For each of these, we set k to the number of tokens in the input sequence. For the residual connection, we set k to the number of tokens in the input sequence itself. Formal: If the input text has a high perplexity value, like P(s) = 0.4, the process ends. If not, the process starts again with the same token sequence. Formal: The average perplexity scores are calculated for all pairs of words that come from the sequence. Formal: If the input text has a high perplexity value, like P(s) = 0.4, the process ends. If not, the process starts again with the same token sequence.\n\nIn Figure 1, you can see that RNN-based models tend to be more robust and perform better compared to other models. But, these models don't really take advantage of the relation information that information-based models like ACE and UniLM bring. Formal: Even though it's important to use information from the input to improve performance, there are cases where using information from the input might not be the best choice. For example, if the sentence has a high perplexity value, like P(s) = 0.4, the process ends. Formal: A better approach is to consider the relation information from the input and combine it with the sentence's structure to improve performance. Formal: The model could use the relation information from the input to figure out how the context around the text changes between the input and output. Formal: In our work, we use the word-level context information from ACE to estimate the context embedding for the word sequence in the output. Formal: We also use the word-level context information from UniLM to estimate the context embedding for the token sequence in the output. Formal: We also use the relation information from ACE to estimate the relation embedding for the token sequence in the output. Formal: Formal: For the example shown in Figure 1, both the hierarchical attention and the token sequence embedding are trained together on the input text. Formal: But, these methods don't work well if the input text has more complex structures. Formal: To fix this, we suggest using a hierarchical attention network with the output embedding that the model learns from the input text. Formal: We tested how well this method works by calculating the F1 score, which is the F1 score that shows how well the model can learn",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "So, we've been looking at recent studies on NER and trying to figure out how to improve the recognition of OOV (out-of-vocabulary) entities. In this project, we came up with a cool and flexible learning framework called MINER. It tackles the OOV recognition problem by looking at it from an information-theoretic angle. \n\nOn one hand, this method boosts the context information from the encoder's output. On the other hand, it helps get rid of unnecessary stuff that doesn't matter for the task and stops the model from just memorizing entities. \n\nThe approach has two main training goals based on mutual information: one is about maximizing useful information, and the other is about minimizing extra, unnecessary information. \n\nWhen we tested it on different datasets, MINER performed way better at predicting OOV entities compared to other methods.",
        "formal_text": "Based on the recent studies of NER, we analyze how to improve the OOV entity recognition. In this work, we propose a novel and flexible learning framework -MINER, to tackle OOV entities recognition issue from an information-theoretic perspective. On the one hand, this method can enhance the context information of the output of the encoder. On the other hand, it can safely eliminate task-irrelevant nuisances and prevents the model from rote memorizing the entities. Specifically, the proposed approach contains two mutual information based training objectives: generalizing information maximization, and superfluous information minimization. Experiments on various datasets demonstrate that MINER achieves much better performance in predicting out-of-vocabulary entities.",
        "GPT2_formal_text": "= {}}  \n\nBecause of this, a person can look at the whole bunch of documents (d) and still be able to figure out the types of entities mentioned in (b) by thinking about how those entities are connected.  \n\nIn this situation, we call the step \"treating entity types as facts\" or \"type-based inference.\" It's kind of like the idea of \"knowledge base construction,\" where the system builds a knowledge base from the ground up, without any shortcuts. Formal: To handle (b), the system uses a dynamic programming algorithm. It uses a straightforward model where each mention (i) is matched with a type (t). This setup works fine when the mentions and types are similar, but it can get tricky when the mentions are different or there are more types than mentions. Formal: To deal with (b), the system adds a rule that says two mentions in a row (i, j) can't be types of each other. Formal: The final result is a list of types that are linked to each mention in the document. Formal: The algorithm picks the most likely type based on the type hierarchy shown in Figure 1. Formal: The system then uses this to identify the entities mentioned in (b). Formal: To find the entities mentioned in (b), the system looks at the types of the entities themselves, like (e1, e2) and (e3, e4). Formal: Finally, the system uses these entities to match up with the entities in (b). Formal: Formal: Formal: The main goal of this work is to make the system work better by incorporating knowledge base construction. Formal: Formal: Formal: The system we're talking about here is called ERG, or Extended Relational Grammar. Formal: This approach has been used for language modeling, and it works by checking the types of the words being used to create a kind of graph. Formal: Formal: Formal: The system we're talking about here is called ERG, or Extended Relational Grammar. Formal: This approach has been used for language modeling, and it works by checking the types of the words being used to create a kind of graph. Formal: Formal: Formal: The system we're talking about here is called ERG, or Extended Relational Grammar. Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.383.json"
    },
    {
        "casual_text": "We used a classifier based on a probabilistic version of the pair-wise algorithm (Soon et al., 2001). We focused only on resolving third person singular pronouns because that's where pronoun resolution is most common in psycholinguistics. Third person pronouns are different from first and second person ones since the latter are deictic, not anaphorical.",
        "formal_text": "We used a classifier that proceeds according to a probabilistic version of the pair-wise algorithm (Soon et al. , 2001 ). We only account for third person singular personal pronoun resolution in order to approach the psycholinguistic domain where pronoun resolution is most often restricted to these type of pronouns. The third person pronouns can be viewed as different from the first and the second as the latter are deictic rather than anaphorical.",
        "GPT2_formal_text": "ed, text_input_masked, and text_output_masked representations. The encoder is designed to keep the attention hidden and learn better representations of the input sequences. We train this encoder using the cross-entropy loss function. The attention is calculated by adding up the attention weights from the embedding layers and the decoder layers. Formal: We use a sigmoid activation function to predict the attention state for the input. The input is represented as a matrix of size n×m. Formal: We focus on the source sentence in the source domain and the target sentence in the target domain. Formal: For each target sentence, we calculate the attention weight for the source sentence using the cross-entropy loss function, and for the source sentence, we calculate the attention weight for the target sentence using the cross-entropy loss function. Formal: For each source sentence, the attention weight is calculated using the cross-entropy loss function, and for each target sentence, the attention weight is calculated using the cross-entropy loss function. Formal: We use a linear kernel with a window size of 4 to create the attention distribution. This is done using a linear kernel with a window size of 4. Formal: The softmax operation, which is used in cross-entropy loss, helps adjust the weight vector. It takes into account the component probabilities and the average attention score across the component words, which gives us the probability distributions for the source sentence and the target sentence. Formal: The cross-entropy loss function helps the model learn better representations for the source and target domains. Formal: The cross-entropy loss function adjusts the attention distribution based on the component probabilities and the average attention score. Formal: We use a linear kernel with a window size of 4 to calculate the attention distribution for the source sentence. Formal: We use a linear kernel with a window size of 4. Formal: In this project, we focus on the source sentence in the source domain and the target sentence in the target domain. Formal: We calculate the attention weight for the source sentence using the cross-entropy loss function, and for the target sentence, we calculate the attention weight for the target sentence using the cross-entropy loss function. Formal: We use a linear kernel with a window size of 4 to calculate the attention distribution for the source sentence. Formal: For each source sentence, the attention weight is calculated using the cross-entropy loss function",
        "directory": "E17",
        "filename": "E17-4006.json"
    },
    {
        "casual_text": "We found that using both label smoothing and vocabulary sharing in neural machine translation (NMT) can mess things up and lead to worse performance. To fix this, we came up with Masked Label Smoothing (MLS), which tweaks the smoothed probabilities based on the differences between languages. It’s straightforward but works really well. MLS not only improves translation quality but also helps the model make better predictions across a bunch of tasks compared to the original label smoothing.",
        "formal_text": "We reveal the conflict between label smoothing and vocabulary sharing techniques in NMT that jointly adopting the two techniques can lead to suboptimal performance. To address this issue, we introduce Masked Label Smoothing to eliminate the conflict by reallocating the smoothed probabilities according to the languages' differences. Simple yet effective, MLS shows improvement over original label smoothing from both translation quality and model's calibration on a wide range of tasks.",
        "GPT2_formal_text": "are generated from the input sentence and then sent to the generator. The generator spits out a new sentence for the given context, which could be a sentence or a sequence of words. Formal: With the current methods, the generator starts with a messed-up sentence with a messed-up reference translation. By tweaking the reordering model, we can fix the problem. We tested this on the English-German test set. The results are in Table 3. The process starts with two main steps: first, the generator gives us the input sentence, and then it predicts the translated version. The second step involves the reordering model, which gets trained using the input sentence and the target-side reference translation. Table 3 shows that we can boost the performance by tweaking the training set during both training and testing. This improvement happens because, first, the reordering model is trained on the input sentence itself, and second, the generator is trained on a reordered version of the input sentence. We also noticed that the performance goes down during testing. Formal: Let’s take a closer look at the performance of our reordering model based on the ground truth translated English sentence, y = y_i. Formal: Even though we had the best training set in training, the performance dropped when we moved to testing. Formal: As a result, our reordering model can effectively reorder the source sentences to match the target sentences. Formal: In this paper, we’re mostly focusing on reordering model training, but we also did some language modeling. Formal: We tested our model on the Czech-English test set (Och and Ney, 2003), which we worked with for this paper. Formal: In the experiments, we used English as the source language and the Czech as the target. Formal: For the Czech-English test set, we set the number of training sentences to 5,000. Formal: In each experiment, we used a regular CKY model with a linear interpolation, and the results are in Table 4. Formal: Figure 1 shows the full breakdown of translation error cases, including the top 10 translation mistakes, for the English-Czech test set. Formal: We picked these top 10 translation errors for evaluation because they’re usually the ones that pop up in the test set. Formal: Table 4 has the breakdown of translation errors for the test set from the PARC test corpus. Formal: The top 10 translation errors are marked with the",
        "directory": "acl",
        "filename": "2022.acl-short.74.json"
    },
    {
        "casual_text": "Alright, let's dive into some more examples of what START can do. We picked these conversations to show off how S-rules work and to give you a sense of what START can handle. Along the way, we'll also introduce some S-rules that deal with a few more tricky language patterns, like those indefinite objects and reflexive stuff we talked about earlier in the paper.",
        "formal_text": "In this section we present additional dialogues that START can handle. These interchanges are chosen to illustrate the use of S-rules as well as the range of coverage of the START system. In the process we introduce S-rules that handle several more semantic-syntactic interdependencies, including the indeflrfite object and reflexive alternations discussed in the introduction to the paper.",
        "GPT2_formal_text": "ing is a word vector that tells you how much attention the word x_i gets when the word x_j is generated. It's calculated like this: Formal: If the current word isn't in the vocabulary, it gets a 0. For the non-word tokens, we calculate the attention vector using the masking operation. Formal: This attention thing makes sure the vectors for the non-word tokens match up, no matter where they are in the sentence. Formal: Plus, we use the local context vector during the attention masking to make sure the attention is focused on the right words. Formal: Basically, the attention vector for the non-word tokens in a sentence is the sum of the attention vectors for those tokens in the same sentence. Formal: Also, the attention for the non-word tokens gets updated as the sentence gets longer. Formal: Once we have the attention vector for each word in a sentence, we combine it into one representation. Formal: So, we can break down the sentence into smaller pieces (sub-sequences) using the attention vectors from the last layer, and then use those to create a representation for the whole sentence. Formal: We use the model from (Wang et al., 2017) to see how each token in the sequence relates to the whole sentence. Formal: We use the model from (Wang et al., 2017) to see how each token in the sequence relates to the whole sentence. Formal: After combining the attention vectors from the last layer with the word representations from the first layer, we can come up with the final representation for the whole sentence. Formal: The attention for the non-word tokens in a sentence is a multi-dimensional feature that's learned using a CNN with a Recurrent Neural Network (RNN). Formal: We call the attention for each token in the sequence M_i, which is a vector with n dimensions. Formal: We use the model from (Wang et al., 2017) to see how each token in the sequence relates to the whole sentence. Formal: We use the model from (Wang et al., 2017) to see how each token in the sequence relates to the whole sentence. Formal: The model we talked about earlier is trained to predict the target word's position. Formal: We also use the model from (Wang et al., 2017) to see how each token in the sequence relates to the whole sentence. Formal: Once we",
        "directory": "C88",
        "filename": "C88-1065.json"
    },
    {
        "casual_text": "The link to summarization isn’t super clear, so we’re using a rough version of the model by Wang and Ling (2016) as a starting point in our experiments. Summarization research is a huge field, and we’re not diving into all of it here. If you want a deeper look, check out Gambhir and Gupta’s survey from 2017. Recently, we’ve been focusing on pulling out the main argument in just two sentences to use as a snippet for argument searches (Alshomary et al., 2020). Meanwhile, Egan et al. (2016) have been working on abstractive summaries that capture the key points of debates. We’re thinking there might be a connection between the conclusion’s stance and its premises. On a broad level, this is kind of similar to what Angelidis and Lapata (2018) did, where they modeled aspects and sentiments for summarizing opinions.",
        "formal_text": "The exact connection to summarization is unclear, which is why we include an approximation of the model of Wang and Ling (2016) as a baseline in our experiments. General research on summarization is manifold and beyond the scope of this work. For a survey, we refer the reader to Gambhir and Gupta (2017). In recent work, we summarize the core of an argument to be used as a snippet in the context of argument search by a two-sentence extract (Alshomary et al. , 2020) and Egan et al. (2016) create abstractive summaries of the main points in a debate. We hypothesize a dependency between the target and stance of a conclusion and those of the premises. At a high level, this resembles the work of Angelidis and Lapata (2018) where aspects and sentiments are modeled for the extractive summarization of opinions.",
        "GPT2_formal_text": ". The next part will explain how we find the best and most reliable sentence pairs by using these chosen document pairs. Formal: We pick the sentence pairs that work best for both supervised and unsupervised learning, using the method we talked about in the paper. Formal: We make sure the outputs from the two supervised models match up well, so the supervised models can help each other out. Formal: We use a simple average of all the feature vectors to get the final model representations. Formal: The training process uses the Adam optimizer, which was introduced by Kingma and Ba in 2015. Formal: We measure how well the model performs using the perplexity of the generated summary. We get the perplexity using the CoNLL F1 score (Vilar et al., 2019). Formal: We also calculate the conditional log probability P(W_s | s) for the phrase pair (s, w_s) if the output is correct and the context is correct. Formal: Here, p(s, w_s) is the conditional probability of getting s right. Formal: The model is trained using three different versions of the Moses toolkit (Koehn et al., 2007): Formal: We use a version with an early stopping policy (check out section 5.3). Formal: We also train the model on a smaller set of unsupervised data (section 5.4) and use the development set to tweak the model's parameters. Formal: We treat the test set as a mix of supervised and unsupervised data (section 5.5). Formal: To predict the next word in a sentence, we use a sequence-to-sequence model (section 5.6). Formal: We also use a method called word-level attention (also in section 5.6) to pick the most important words to focus on. Formal: We evaluate the performance of the model by calculating the average of the perplexity scores from two runs (section 5.7). Formal: We also use a multi-task learning model (section 5.8) to work on the classification part. Formal: We evaluate the model by calculating the average of the perplexity scores from three runs (section 5.7). Formal: Finally, we get the final classification scores (section 5.9). Formal: We show the results for our methods in Table 1. Formal: For the unsupervised baseline, we use the CoNLL",
        "directory": "acl",
        "filename": "2020.acl-main.399.json"
    },
    {
        "casual_text": "4. How well the scores and their meanings can be applied to different groups of people, situations, and tasks.",
        "formal_text": "4. Generalizability Extent to which score properties and interpretations can be generalized to and across population groups, settings, and tasks.",
        "GPT2_formal_text": "ed, and then we adjust the weights of each layer based on how much attention the token sequence has gained or lost, as well as its current state. Formal: Instead of using entity embeddings from the specific area we're interested in, we can just grab them directly from the text. Formal: We create a vector for each entity embedding by calculating the cosine similarity between the two embedding vectors. Formal: We combine all the entity embedding vectors using the dot product to get their final embedding vectors. Formal: Finally, we throw in an attention layer to give each token a score based on how much attention it's getting. Formal: To make sure the attention and entity embeddings work together well, we train a simple classifier using a linear model. This model looks at how each token's attention is distributed and gives it a score. Formal: To use this classifier, we train it using a linear classifier and then get rid of the noisy tokens using a cross-entropy loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eacl",
        "filename": "2021.eacl-main.137.json"
    },
    {
        "casual_text": "From what I can see, STATEMENT is a big reason for mix-ups in the data because it's the most common type. A lot of RE-QUEST and RESPONSE ACK examples got labeled as STATEMENT by mistake. We didn't use stuff like question marks in our features, but adding that could probably make things better.",
        "formal_text": "Looking over the data, STATEMENT is a common source of misclassification, as it is the majority class in the data. In particularly, a large number of RE-QUEST and RESPONSE ACK utterances were tagged as STATEMENT. We did not include punctuation such as question marks in our feature sets; including this would likely improve results further.",
        "GPT2_formal_text": ": the first 3 are the titles, and the rest are the topics. Formal: Formal: Topic: Topics: The topics we're looking at are {t 1, t 2, . . . , t M }, which are just different lists of sentences. Formal: Formal: Formal: This model can handle sentences in any order it wants. Formal: Formal: Formal: To get the topic distribution q(t 1, t 2, . . . , t M ) for all i from 1 to M, we use this formula: Formal: q(t 1, t 2, . . . , t M ) = 1/n × K Formal: Formal: This approach lets us handle sentences in any order. Formal: Formal: Formal: Basically, we calculate the topic distribution q(t 1, t 2, . . . , t M ) for the whole collection of sentences {t 1, t 2, . . . , t M }. Formal: Formal: Formal: We're assuming this model can handle any order. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D10",
        "filename": "D10-1084.json"
    },
    {
        "casual_text": "Adverbs in English are super flexible. They can fit into just about any spot in a sentence, as long as they stay close to the verb. Fowler talks about this in his section on adverb placement and gives some wild examples, including cases where he suggests splitting infinitives.",
        "formal_text": "Adverbs in English are most accommodating. They permit themselves to be push into any convenient slot, so long as they do not get too far away from their verb. Fowler under Position of Adverbs has a marvelous chamber of horrors, and some examples where he recommends the split infinitive.",
        "GPT2_formal_text": "_num: The number of attention heads in the student model. These heads are always the same size and have a dimension of d_s.\n\nThe number of attention heads in the student model. These heads are always the same size and have a dimension of d_s. multi-head_attention: The multi-head attention mechanism that was introduced in Mikolov et al. (2013). It works with a batch size of M and the batch size of the student model.\n\nThe multi-head attention mechanism that was introduced in Mikolov et al. (2013). It works with a batch size of M and the batch size of the student model. iterative_hidden_sum: This method updates the student model's hidden state using a sequence of updates. It calculates a score for each update by taking the average of the attention scores from the last and the first time it was applied.\n\nThis method updates the student model's hidden state using a sequence of updates. It calculates a score for each update by taking the average of the attention scores from the last and the first time it was applied. iterative_hidden_max: This version updates the student model's hidden state with a sequence of updates and calculates the score using the updated hidden state.\n\nThis version updates the student model's hidden state with a sequence of updates and calculates the score using the updated hidden state. iterative_hidden_dist: This version updates the student model's hidden state with a sequence of updates and calculates the score using the updated hidden state.\n\nThis version updates the student model's hidden state with a sequence of updates and calculates the score using the updated hidden state. iterative_input_sum: This method updates the student model's input hidden state with a sequence of updates and calculates the score using the updated hidden state.\n\nThis method updates the student model's input hidden state with a sequence of updates and calculates the score using the updated hidden state. iterative_input_max: This version updates the student model's input hidden state with a sequence of updates and calculates the score using the updated hidden state.\n\nThis version updates the student model's input hidden state with a sequence of updates and calculates the score using the updated hidden state. iterative_input_dist: This version updates the student model's input hidden state with a sequence of updates and calculates the score using the updated hidden state.\n\nThis version updates the student model's input hidden state with a sequence of updates and calculates",
        "directory": "bcs",
        "filename": "1984.bcs-1.21.json"
    },
    {
        "casual_text": "Looking at this big collection of English texts, we've managed to figure out a pretty detailed grammar for this specific type of English (check out Harris 1968 and Kittredge and Lehrberger 1982 for more info).",
        "formal_text": "Examination of this extensive corpus of English has led to a fairly detailed grammar of this sublanguage (cf. Harris 1968, Kittredge and Lehrberger 1982).",
        "GPT2_formal_text": "= (q, t, e) for each pair (q, t) in the set Q_i. We're going to use a method to create short descriptions, which is the same approach that Zeng and his team used back in 2012. Formal: In this method, each document is linked to a human-made summary by linking each mention to a summary that's already been made. Formal: In this case, each mention in the input text is linked to a summary created by the user. Formal: The main goal of this system is to create a short description of a document. It does this by combining the features from the input document with the features from the human-made summary. Formal: We can also use different ways to combine the inputs. This helps the system create more accurate, tailored summaries, which is also a key goal of our system. Formal: In a paper by Shi et al. from 2016, they show that a single sentence can be treated as a summary by combining the attributes from the input sentence with the attributes from the human summary. Formal: We can also add features from the user's own text, like named entities. Formal: This process is all laid out in Section 3.2. Formal: Lastly, the model can adjust how it combines the input, based on how it feels. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C86",
        "filename": "C86-1132.json"
    },
    {
        "casual_text": "• The directions are indicated by a term called \"d,\" which is set to -1. If we keep d at -1, the input will be processed in the forward direction. If we change it, the input will be processed in the backward direction.",
        "formal_text": "• The directions are marked by a direction term d = (−1). If we fixed d to −1, the input will be processed in forward direction, otherwise backward direction.",
        "GPT2_formal_text": "_t = [0, 1, ..., k], which then creates a single attention vector for each k. The output is a 2D vector, which can be thought of as the normalized cross-entropy between the attention vector and the actual label. Formal: To figure out the attention weights, we look for the dot product between the attention vector and the actual label. Formal: Here, k is the number of tokens, and h_i is the hidden state at position i, which we get by using a convolutional neural network. Formal: Next, we combine the attention vectors from each of the k different text units into a single vector. Formal: After that, we apply an orthogonal transformation to this combined vector to get a new representation of the text. Formal: Finally, the output is a 2D vector that has the normalized cross-entropy between the attention vectors of the different units. Formal: This process is repeated for each k-1 of the text units. Formal: Finally, the output is a 2D vector with the normalized cross-entropy between the attention vectors of the different units. Formal: The original text units are represented as [u_1, ..., u_k], and the generated units as [u_1, ..., u_k+1]. Formal: Formal: Figure 1: Formal: The dashed line shows the attention weights for the text units generated by M-BERT. Formal: The figure shows the attention weights for the text units generated by M-BERT and the attention weights for the generated units. Formal: Formal: The word sequence model is trained using a cross-entropy loss function. Formal: The loss function for M-BERT is basically the sum of the cross-entropy losses. Formal: The cross-entropy loss is also defined as the normalized cross-entropy between the attention weights of the text units. Formal: Figure 2: Formal: Here, h_i is the hidden state at position i, which we get by using a convolutional neural network. Formal: Formal: Using the same loss function, the text units are represented as [u_1, ..., u_k]. Formal: Formal: Finally, the output is a 2D vector with the normalized cross-entropy between the attention weights of the text units. Formal: Formal: Formal: Figure 3: Form",
        "directory": "C18",
        "filename": "C18-1124.json"
    },
    {
        "casual_text": "We want our questions to focus on specific types of connections between ideas, called \"relation senses.\" To make sure we cover a wide range of these connections, we created a bunch of question templates based on the PDTB 3.0 (a big database of discourse relations, created by Webber et al. in 2019 and Prasad et al. in 2008). You can check out these templates in Table 3. Each question starts with a specific prefix that tells you what kind of relation sense it’s asking about. The placeholder X is filled in to point to the part of the text the question is about, like in Table 1.\n\nNot all PDTB senses are covered by our prefixes, though. For example, senses that have extra details about beliefs or speech acts (like \"Belief\" or \"SpeechAct\") were grouped under a more general sense. Also, we left out three \"Expansion\" senses because they don’t usually introduce new information that you could ask a question about. Instead, they focus on how the text is structured. One of these is \"Expansion. Conjunction,\" which is super common in the PDTB, especially in phrases where verbs are connected within a sentence. It makes up around 70% of the instances of this sense.\n\nFor example, in Ex. (2), there’s a discourse relation with two senses, one of which is \"Expansion. Conjunction.\" While it’s easy to think of a question about a causal sense, the conjunction part doesn’t really introduce any new information that you’d ask a question about.",
        "formal_text": "The Discourse Relation Sense We want our questions to denote relation senses. To define the set of discourse relations covered by our approach, we derived a set of question templates that cover most discourse relations in the PDTB 3. 0 (Webber et al. , 2019; Prasad et al. , 2008), as shown in Table 3. Each question template starts with a question prefix, which specifies the relation sense. The placeholder X is completed to capture the discourse unit referred to by the question, as in Table 1. Few PDTB senses are not covered by our question prefixes. First, senses with pragmatic specifications like Belief and SpeechAct were collapsed into their general sense. Second, three Expansion senses were not included because they usually do not assert a new \"informational\" proposition, about which a question could be asked, but rather capture structural properties of the text. One of those is Expansion. Conjunction, which is one of the most frequently occurring senses in the PDTB, especially in intra-sentential VP conjunctions, where it makes up about 70% of the sense instances. Ex. (2) displays a discourse relation with two senses, one of which Expansion. Conjunction. While it is natural to come up with a question targeting the causal sense, the conjunction relation does not seem to assert any proposition about which an informational question may be asked.",
        "GPT2_formal_text": "= (x, y) { x = (x, y), y = (y, x) } Formal: We got a perfect score with this approach: Formal: We used the tfidf metric to evaluate how well our results matched the reference texts. Formal: For the experiment, we picked 50 random examples from each pair of reference texts to train our model. Formal: The issue with BLEU scores is that they don't show how much the model has improved. So, instead, we averaged the scores from our model to get a better idea of how much our model is improving. Formal: For the experiments in this paper, we used the same training, development, and test sets as the original paper. The models we used are: Formal: The training set includes 100 random samples from the reference texts. Formal: Our method is way more efficient than BLEU. Formal: We didn't include the results for the text selection method in this section because we didn't include those results for the experiments we did. Formal: To see how well the model performs, we used the BERT model (Devlin et al., 2019) to evaluate it. Formal: The references are given by randomly picked journalists. Formal: We trained the model on the official training set and tested it on the dev set, which was made from the test set. Formal: We used all the test sets mentioned in the original paper. Formal: We also tested the reference selection method with the rank-3 model and picked the best model based on the evaluation metric we got. Formal: Formal: Formal: Our method has a lower BLEU score than the current top methods. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "With machine learning becoming a big deal in computational linguistics and doing really well in lots of areas, there's a growing need for lots of human-labeled data to train and test these algorithms. In the area of figuring out time-related stuff, this has resulted in the creation of TimeBank (Pustejovsky et al., 2003), which uses the TimeML language for labeling (Pustejovsky et al., 2005). TimeML is on its way to becoming an ISO standard for marking up events and time expressions (ISO/TC 37/SC 4/WG 2, 2007). TimeBank was also made available for TempEval-2007, the first competition focused on automatically figuring out the relationships and order between events and time expressions.",
        "formal_text": "As machine learning approaches are gaining dominance in computational linguistics and producing state-of-the-art results in many areas, they have in turn fueled the demand for large quantities of human-annotated data of various types that machine learning algorithms can be trained on and evaluated against. In the temporal inference domain, this has led to the creation of TimeBank (Pustejovsky et al. , 2003), which is annotated based on the TimeML language (Pustejovsky et al. , 2005). TimeML is becoming an ISO standard for annotating events and time expressions (ISO/TC 37/SC 4/WG 2, 2007). A version of the TimeBank has been provided as a shared public resource for TempEval-2007, the first temporal evaluation campaign aimed at automatically identifying temporal relations between events and time expressions as well the temporal ordering between events.",
        "GPT2_formal_text": ". For each token, we grab the sentence-level attention vector h_t = h_1 * [t_1, t_2, ..., t_K] and use it as the context vector for the token. Formal: The input for this embedding layer is a text piece, which we'll call x_t. The length of this input is d_i. Formal: After the embedding layer, we use an attention mechanism to figure out the overall representation h_a_t. Formal: To make sure the attention stays focused on the important parts, we add a second layer to help the model understand the context. This second layer has two parts: the context vector h_c_t and the attention vector h_b_t. Formal: After the second layer, the final representation h_a_t is built by combining both the context vector h_c_t and the attention vector h_b_t. Formal: Finally, we have the final output vector h_a_t, which is a vector with d_i dimensions. Formal: If the input and output vectors have different dimensions, we just average the values in the last layer. Formal: In this setup, the embedding matrix h_a_t is always the same size, which gives us the hidden representation h_a_t. Formal: We use the log-likelihood function L_T to calculate the loss function, which is basically the cross-entropy between the prediction and the actual distribution. Formal: Finally, we add a softmax function to optimize the final loss function L_T for the whole model. Formal: We train the model using Adam (from Kingma and Ba, 2014) and set the batch size to 2. Formal: To get the final result, we average the outputs of the last two layers in the last-layer Bi-LSTM for the token i. Formal: The word sequence that gives the most attention is chosen as the context vector h_c_t. Formal: The last layer of the Bi-LSTM helps the model generate the token representation h_a_t. Formal: Finally, the input vector is fed into the final layer of the Bi-LSTM. Formal: Formal: Following Wang et al. (2019), we train the model using the Adam optimizer with a learning rate of 0.1. Formal: Training the model takes",
        "directory": "C10",
        "filename": "C10-2156.json"
    },
    {
        "casual_text": "Cerco un posto dove mangiare hamburger vicino a \"Stagno Bosco\".",
        "formal_text": "Neural Machine Translation sto cercando un posto da \" hamburger \" vicino a \" stagno bosco \".",
        "GPT2_formal_text": "_tgt = tuple (r_i, t_i) # same for the argument_ids_tgt, but we'll skip over that. Formal: We'll start with the queries that have the most common arguments and figure out how well the model can rank those queries. Formal: For the ranking model, we start by creating a convolutional neural network using the attention mechanism. Formal: The words in the query are turned into word embeddings by the attention mechanism. Formal: We mix the attention and convolutional neural networks together to create a query ranking model. Formal: To get the final ranking, we use a simple linear regression. Formal: To compare with other models, we’ve created a joint model to find the best match between the query and the context. Formal: We also tried using dynamic programming (DP) to figure out the best argument span, which worked better for our method. Formal: Lastly, we tested our method with different entity detection models. Formal: We ran our experiments with three different argument spans detection models: Fast-Text 1, Fast-Text 2, and Fast-Text 3. Formal: The results show that our method consistently performs better than the current best methods. Formal: Lastly, we’re sharing our code and evaluation results, which we’ve put together using PyTorch and TensorFlow. Formal: To train our model, we used the Adam optimizer with the default settings (from Kingma and Ba, 2014) and we set the learning rate to 0.0001. We tested the model with different numbers of epochs (from 10 to 100) and different batch sizes (from 16 to 128) for each training epoch. Formal: For evaluation, we used the WMT2016 evaluation set (thanks to Papineni et al., 2002). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.481.json"
    },
    {
        "casual_text": "Alright, so when it comes to pulling out information about cause and effect from text, there are two main types of tasks: causal phrase extraction and causal clause extraction. The first one is all about grabbing specific words or short phrases that show a cause-effect relationship.",
        "formal_text": "The tasks concerning causality extraction in text can be mainly divided into two categories, causal phrase extraction and causal clause extraction, where the former focuses on extracting word",
        "GPT2_formal_text": "ed_num = [a_num, h_num], where a_num is the position of the first word in the sequence, h_num is the position of the last word, and n is the total number of tokens in the input. The representation of the sequence, which we call h_s, for a given input x_i is calculated like this: Formal: First, we create a feature vector called h_s for the input sequence x_i. Then, using a multi-head attention mechanism, we combine h_s with the hidden states of each head to get the output features for the first head. Formal: After that, we update the representation h_s by adding all the weights from the last head's output vectors to the one from the first head's output vectors. Formal: Lastly, we get rid of the last token by doing a feed-forward neural network with a softmax layer. Formal: For the final score, we use the output from the first head's output layer as the score vector for the sequence. Formal: We evaluate how well the model does using three types of metrics: precision, recall, and the F1 score. Formal: To get a general idea of how well the model is doing, we calculate the average precision and recall across all possible words (u_w) for the input sequence x_i. Formal: We also calculate the average precision and recall across all possible words (u_w) for each token (u_t) in the input sequence. Formal: To make sure we're comparing results fairly, we train the model on both the input and output sequence together using our joint training method, which we call \"parallel training.\" Formal: Figure 2 shows how the model's performance changes when we train with different amounts of data, from a few thousand tokens (t) to several thousand tokens (t). Formal: Using the results from a multi-head attention mechanism, the performance improves for each layer. Formal: As the model gets better, the performance on the test set improves too. Formal: To keep things fair, we train the model on both the input and output sequence at the same time. Formal: Training the model with different amounts of data means the model is more likely to overfit to the input. Formal: For all the experiments, we train the model using 300 tokens. Formal: The authors are the first to submit a new version of this paper.",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.252.json"
    },
    {
        "casual_text": "This project is all about creating trees made up of topical segments. Each segment has a center that best represents its content. The main goal is to maximize net similarity, which is the total similarity between all centers and the data points they represent. The entire sequence of data points gets divided at each level of the tree, but with one rule: at each level (l), the centers must be part of the centers from the previous level (l-1). Figure 1a shows a part of the factor graph that explains how HAPS works for levels l and l-1.\n\nThe tree has L levels, starting from the top (l = L) down to the bottom (l = 1). The little numbers on the factor and variable nodes show which level they're on. At every level, there are N² variable nodes called cᵢⱼʲ and N variable nodes called eⱼ (N is the number of data points in the sequence). Each variable is either 0 or 1: cᵢⱼʲ = 1 means data point i at level l is part of the segment centered around data point j; eⱼ = 1 means there's a segment centered around j at level l.\n\nThere are four types of factor nodes in Figure 1a: I, E, C, and S. The I factors make sure each data point is in exactly one segment and that the centers at level l are part of those from level l-1. The E nodes ensure that segments are grouped neatly around their centers in solid blocks, not scattered all over. The values for I and E are 0 if everything's good and -∞ if something's wrong. The S factors measure how similar the data points are to each other.",
        "formal_text": "This work aims to build trees of topical segments. Each segment is characterized by a centre which best describes its content. The objective function is net similarity, the sum of similarities between all centres and the data points which they exemplify. The complete sequence of data points is to be segmented at each level of the tree, subject to the following constraint: centres at each level l, l > 1, must be a subset of the centres from the previous level l − 1. Figure 1a shows a fragment of the factor graph describing HAPS corresponding to levels l and l − 1. The tree has L levels, from the root (l = L) down to the leaves (l = 1). The superscripts of factor and variable nodes denote the level. At each level, there are N 2 variable nodes c l ij and N variable nodes e l j (N is the number of data points in the sequence to segment). A variable's value is 0 or 1: c l ij = 1 ⇔ the data point i at level l belongs to the segment centred around data point j; e l j = 1 ⇔ there is a segment centred around j at level l. Four types of factor nodes in Figure 1a are I, E, C and S. The I factors ensure that each data point is assigned to exactly one segment and that segment centres at level l are a subset of those from level l − 1. The E nodes ensure that segments are centred around the segment centres in solid blocks (rather than unordered clusters). The values of I and E are 0 for valid configurations and -∞ otherwise. The S factors capture similarities between data points.",
        "GPT2_formal_text": ", which were generated during the pre-processing stage. This input helps in figuring out the right attention weights for the specific token type, t. The type of token is determined by how the model learns (Wu et al., 2016). Formal: In the text, tokens that match the tokens in the article are thought to be close to each other, while tokens that don't are thought to be closely connected. For example, if there's a token like \"likes\" in the text and a token like \"likes\" in the article, it would seem like the user \"likes\" is more similar to \"likes\" than to other tokens. But if we think of \"likes\" as a broader category, like \"likes pets,\" \"likes cats,\" or \"likes food,\" it could make sense for the model to focus on those more specific tokens. Formal: We also use an attention mechanism (Mikolov et al., 2013) to figure out the attention weights for the specific token type, t. Formal: Lastly, we combine the attention scores for all the tokens to get the final attention score, a, for the token type, t. Formal: Formal: We also use an attention mechanism (Mikolov et al., 2013) to figure out the attention weights for the specific token type, t. Formal: Formal: Finally, we combine the attention scores for all the tokens to get the final attention score, a, for the token type, t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C14",
        "filename": "C14-1005.json"
    },
    {
        "casual_text": "In GA-soft, if the angle θ is bigger than 90 degrees, g P KD ends up going in the opposite direction of g CE. This could either slow down or even reverse the direction of the gradient descent.",
        "formal_text": "In GA-soft, when the angle θ is larger than 90 •, g P KD is in the opposite direction with g CE, thus might slow down or reverse this gradient descent direction.",
        "GPT2_formal_text": ". This feature is basically a copy of the word embedding, but it's used for something specific and doesn't mess with the original word's representation. We use this feature to figure out the attention weight vector q_t by averaging the last hidden state of the first character in the input sequence. Formal: The last hidden state we get after applying the input attention layer. Formal: The attention vector for the token we're focusing on. Formal: The token representation vector, just like in the entity detection task, where we predict the entity type. Formal: The attention vector for the entity type we're trying to predict. Formal: The attention vector for the entity type we're predicting. Formal: This is the attention vector for the entity type we're trying to predict. Formal: Here, T_i, i represents the output from the entity type embedding, and σ is the softmax function applied to it. Formal: This is the final attention vector for the entity type we're trying to predict. Formal: This is the final attention vector for the entity type we're trying to predict. Formal: Formal: Finally, we use this final attention vector to create a prediction for the entity type. Formal: Finally, we use this final attention vector to create a prediction for the entity type. Formal: The entity type prediction layer. Formal: Formal: The entity type prediction layer. Formal: The entity type prediction layer. Formal: The entity type prediction layer. Formal: The entity type prediction layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.242.json"
    },
    {
        "casual_text": "Doran and his colleagues (2001) found that the more times you try to start something, the less likely you are to succeed each time.",
        "formal_text": "Doran et al. (2001) demonstrate a negative relationship between number of initiative attempts and their success rate.",
        "GPT2_formal_text": "= [q_i, t_i, m_i, u_i, q_m, t_m] Formal: We tweak the original data distribution p_s to fit our data distribution p_z. Then, we calculate the objective function h_z, which is: Formal: The cross-entropy loss for the task is calculated by minimizing the cross-entropy loss between the input representation q_i and the response representation r_i. Formal: The cross-entropy loss for the task is: Formal: The cross-entropy loss for the task is: Formal: The cross-entropy loss for the task is: Formal: The cross-entropy loss for the task is: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E06",
        "filename": "E06-1024.json"
    },
    {
        "casual_text": "Usually, the numbers we predict form a smooth line that goes across time and space. To make things easier, we often just pick regular times (like every hour) and places (either on a grid or at weather stations) to give those numbers. Or, we might only focus on the big changes. MAR-WORDS is cool because it can handle both ways of describing the data. But honestly, figuring out exactly what kind of data we need is still a bit of a puzzle that we need to work on more.",
        "formal_text": "In the normal course of events, the predicted values make up a continuum in both time and space. For simplicity, values are often given at regular steps in time (e. g. , hourly) and space (either at grid points, or at weather observing sites). Alternatively, forecast parameters may be given in terms of significant changes only. MAR-WORDS is flexible enough to accept both types of data description. In fact, the structure and nature of the required data is a problem which needs more work to resolw~.",
        "GPT2_formal_text": "ed = { [mask] = [mask], [active] = [active], [previous_i, j] = [i, j] } while self is active and self is the target. Formal: In our setup, the token at position i is the key token and the token at position j is the value token. Formal: We use a beam search to figure out the probability of the target token given the previous input and the input at position i. Formal: The final attention vector h t for the target token after applying the [mask] token is calculated using this formula: Formal: For the first round of attention, we calculate h t = [mask] + [previous_i, j] + [active] + [previous_i, j] for the target token, which gives us the final attention vector h t for the target token. Formal: Lastly, we calculate h t for the target token at position i using this formula: Formal: We combine the attention vectors from the first two rounds to get the final attention vector h t. Formal: In this paper, we use a CNN model to learn the target token. Formal: The model is trained using a learning rate of 0.01. Formal: A key takeaway is that the target token should be closely linked to the previous context. Formal: Specifically, we train the model using a learning rate of 0.01. Formal: We fine-tune the model for a few epochs and then test it on a validation set. Formal: We train for a few epochs and then test it on a validation set. Formal: Formal: We train the model on a validation set that's 3 times bigger than the training set. Formal: Formal: Formal: Formal: We also keep track of the evaluation set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1132.json"
    },
    {
        "casual_text": "Sure, I can help with that! Here's the informal version:\n\nIt can be used as supervision for the conditional generation task. Once the autoencoder is pretrained, the encoder (enc) can be used to turn the training data into pairs of vectors.",
        "formal_text": "is available as supervision for the conditional generation task. After pretraining the autoencoder, enc is available to transform the training data into pairs of vectors",
        "GPT2_formal_text": ", which are like the label names you see in Table 2. For the first part, we took the average of the attention weights, which turned out to be a whopping 2.32. For the second part, we swapped out the attention weights in the generator with the actual label names we had in our dataset. We used the model from (Wang and Cardie, 2018b) and trained it using the necessary data. We stuck with a learning rate of 10^-5 for the student model and set the batch size to 64.\n\nThe results? Turns out, the student model was about 5 times faster than the teacher model (check out Table 2). The final model had a speedup of almost 1.5 times compared to the teacher. To figure out how we could make the student model even faster, we tried tweaking the attention weights. We tried out different values, like 50, 200, and 400, and got some cool improvements, ranging from 0.7 to 1.1 times. One thing we noticed is that the attention weights for our student model always seem to be a bit too focused on the actual label names. So, we decided to do some extra training on the actual label names, especially since there’s not much data to work with. We added 300 tokens of random name-related text to the student to make it even faster. Formal: The student model learns to create more diverse text based on a big dataset without needing any extra data. Formal: The student learns to produce more diverse and informative text using a smaller dataset. Formal: We added some noise to the training data to make the student model even faster. Formal: We tweaked the attention weights for the generator to make it work even faster. Formal: We trained the student model using the dataset we already had and trained the generator with the more data we had. Formal: We tweaked the attention weights for the student model to make it faster. Formal: We also added some noise to the training data to make the student model even faster. Formal: We trained the student model using the dataset we already had and trained the generator with the more data we had. Formal: We tweaked the attention weights for the student model to make it faster. Formal: Formal: We also added some noise to the training data to make the student model even faster. Formal: Finally, we trained the student model using the dataset we already had and trained the generator with the more data we had. Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.491.json"
    },
    {
        "casual_text": "To set up Gibbs sampling, we need to figure out the marginal distribution for the observations (words), the topic assignments, and the table indicators. We use the Dirichlet integral to handle the document topic distributions and the topic-word matrix, integrating them out. Then, we use the joint posterior distribution for a PDP to gradually get rid of the segment topic distributions. After all that, we end up with the marginal distribution:  \n\np(z 1:I,1:J, w 1:I,1:J, u 1:I,1:J | α, γ, a, b) = (5)",
        "formal_text": "To complete a formulation suitable for Gibbs sampling, we first compute the marginal distribution of the observations w 1: I, 1: J (words), the topic assignments z 1: I, 1: J and the table indicators u 1: I, 1: J. The Dirichlet integral is used to integrate out the document topic distributions µ 1: I and the topicby-words matrix Φ, and the joint posterior distribution computed for a PDP is used to recursively marginalise out the segment topic distributions ν 1: I, 1: J. With these variables marginalised out, we derive the following marginal distribution p( z 1: I, 1: J, w 1: I, 1: J, u 1: I, 1: J α, γ, a, b) = (5)",
        "GPT2_formal_text": "_tbl = { . , } | augmentation_attention_mask_tbl = { . , } | ik augmentation_attention_mask_tbl = { . , } | ik augmentation_attention_mask_tbl = { . , }  \n\nTable 2 shows the top 5 attention results for each of the six attention methods. For the BiLSTM method, the top 5 attention results are A_t, A_b, A_c, A_d, and A_e. For the CNN/Daily Mail and BiLSTM methods, the top 5 results are C_t, C_b, C_c, C_d, and C_e. The BiLSTM method with the lowest score is A_t. \n\nThe attention method with the lowest score is M_t, which means the attention mechanism is doing its job. Formal: Basically, the attention mechanism is the key ingredient in this model. Formal: The attention mechanism is the key ingredient in this model. Formal: The attention mechanism is the key ingredient in this model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D12",
        "filename": "D12-1049.json"
    },
    {
        "casual_text": "The NYTimes800k dataset is a bigger and more complete collection of New York Times articles, images, and captions. It's about 70% larger than the previous one. The dataset is split into 763K articles for training, 8K for validation, and 22K for testing. Table 5 shows a detailed comparison between GoodNews and NYTimes800k, focusing on the length of articles and captions, as well as the composition of the captions.",
        "formal_text": "NYTimes800k dataset is 70% larger and more complete dataset of New York Times articles, images, and captions. The number of train, validation and test sets are 763K, 8K and 22K respectively. Tab. 5 presents a detailed comparison between GoodNews and NYTimes800k in terms of articles and captions length, and captions composition.",
        "GPT2_formal_text": ", and it's made up of 4 elements. Formal: Let's say w(a) is the score we get when we calculate a vector for a sentence y using some fancy math stuff. Formal: This approach is different from the normal bag-of-words method because it doesn't need any labels. Formal: Let's say c is the chance that a pair of sentences has a certain type. We'll call the score for that pair type_t(s1, s2). Formal: You can see how the matrix A works here. Formal: Here, h_i_j is the i-th row of A. Formal: A_p(f, a) is the probability that a phrase p_j fits with a given sentence x. Formal: We start with a matrix A and a word matrix W_p, and we use the softmax function to figure out the probability p_j for a phrase p_j in a given sentence. Formal: Basically, p_j is the probability that p_j fits with the phrase p_j in the sentence x, plus the chance that p_j fits with another phrase p_j+1, plus the probability that p_j fits with another phrase p_j+2, plus the chance that p_j fits with another phrase p_j+3, plus the probability that p_j fits with a phrase p_j+4. Formal: The probability p_j for a phrase p_j in a sentence x is calculated by adding up the probability of the phrases p_j+1, p_j+2, p_j+3, and p_j+4 in the sentence. Formal: We do this for all possible phrase pairs in the sentence, and then we use the softmax function to get the final probability p_j for the phrase p_j. Formal: In this case, the phrase p_j fits with the phrase p_j+1, and the phrase p_j fits with the phrase p_j+2, and so on, until the phrase p_j fits with the phrase p_j+4. Formal: Formal: In this formula, a(a) is the probability that a sentence has the phrase a, which is based on the whole sentence and its context, p_i(a). Formal: This approach also lets us calculate the probability of a phrase p_j in a",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.419.json"
    },
    {
        "casual_text": "Some folks have been trying to connect social tagging systems with ontologies. An ontology is basically a way to describe how things are related. Peter Mika (2005) came up with a more detailed social tagging system that includes actors, concepts, and objects. He used tags that often appear together to create an ontology from these social tags. Wu et al. (2006a) took a different approach, using hierarchical clustering to build an ontology from tags that show similar-to relationships.\n\nLater on, some new ontology schemes were suggested that work better with social tagging systems, like the ones by Van Damme et al. (2007) and Echarte et al. (2007). These mainly focused on how tags, objects, and users are related, rather than just the tags themselves. Alexandre Passant (2007) manually matched tags to domain ontologies to make information retrieval on social media better.\n\nTo automate this process, Angeletou et al. (2007) used ontologies created by experts to find connections between tags, but they found that it didn’t cover much. Specia et al. (2007) suggested a framework to organize tags using existing ontologies, but they didn’t test it out. Finally, Kim et al. (2008) looked at the latest methods for modeling tags with semantic annotations.",
        "formal_text": "People have also worked on bridging social tagging systems and ontologies. An ontology defines relations between entities. Peter Mika (2005) proposed an extended scheme of social tagging that includes actors, concepts and objects, and used tag co-occurrences to construct an ontology from social tags. Wu et al. (2006a) used hierarchical clustering to build ontology from tags that also use similar-to relations. Later, ontology schemes that fits social tagging system were proposed, such as (Van Damme et al. , 2007) and (Echarte et al. , 2007), which mainly focused on the relation between tags, objects and users, rather than between tags themselves. Alexandre Passant (2007) mapped tags to domain ontologies manually to improve information retrieval in social media. To construct tag ontology automatically, Angeletou et al. (2007) used ontologies built by domain experts to find relations between tags, but observed a very low coverage. Specia et al. (2007) proposed an integrated framework for organizing tags by existing ontologies, but no experiment was performed. Kim et al. (2008) summarized the state-of-the-art methods to model tags with semantic annotations.",
        "GPT2_formal_text": "_t, which is basically a group of weights that are learned by averaging out the representations from the head and the tail. This helps us figure out how much attention to give to different parts of the text, which helps us achieve better generalization. Formal: Since the code isn't available for everyone to check, we had to do an ablation study and show how well our approach works. Check out Table 2 for the results. Turns out, the model does better than the regular seq2seq model on both datasets. The baseline model, based on a mix of bert and FastText embeddings, didn't do well on the two datasets, which makes sense. But our model? It did better across the board. Formal: Next, we took a closer look at the results to see how different methods affected things. We measured performance using two metrics: perplexity and perplexity without masks. The results are in Table 3. Turns out, the regular seq2seq model with the masked language model method actually does better than the original model. Formal: Even though the seq2seq model did better than the others, the regular seq2seq model with the masked language model method actually did better than the original model. Formal: Looking at the results for the TAC dataset and the two datasets with code and data, we found that our method doesn't really need more data. We think this might be because the codes are already well-represented in the data and code is easier to work with. So, we can use the same model to train it without adding any more data. Formal: Lastly, we ran an ablation experiment to see how our model stacks up against the best seq2seq models. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C10",
        "filename": "C10-1114.json"
    },
    {
        "casual_text": "In Tables 4 and 5, we’re looking at the analysis for the Gadgets and Iphones datasets, which are kind of like the Politics dataset we saw in Table 3. Compared to Politics, we noticed that for Gadgets and Iphones, the CATD-FLOW models show some ups and downs in performance when we bump K from 5 to 20. This might be because LSTMs aren’t great at remembering long-term stuff. This problem seems to pop up more when there’s not a lot of training data. \n\nHere’s some quick numbers:  \n- N COMBINE (K=20, B=5): 750, 431, 444  \n- O COMBINE (K=20, B=10): 750, 434, 445",
        "formal_text": "In Table 4 and 5, we show the analysis for for Gadgets and Iphones datasets similar to Poli-tics dataset in Table 3. As compared to Politics, we observe that for Gadgets and Iphones, CATD-FLOW models have some fluctuations in performance when we increase K from 5 to 20, which may be due to the limited capability of LSTMs for memorizing long-term history. This issue is more prevalent when the training data size is small. . 750. 431. 444 N COMBINE (K=20, B=5). 750. 434. 445 O COMBINE (K=20, B=10). 750. 434 445",
        "GPT2_formal_text": ", and it turns out that this method actually gives better results than the original approach. Formal: When we look at the documents we've already looked at, the score for the objective function in equation (11) is calculated by summing up the probabilities from all the documents. Formal: We train the model using a log-likelihood objective to make sure it learns the right sentence representations. Formal: Alright, let's break down how we collect and analyze this data. Formal: Our dataset, D, is made up of sentences from real conversations. The conversation starts when we give the model an input like the sentence in the dialog and ends when we turn it into the summary. The whole collection is called D = {D t } = {D t 1, . . . , D t m }. Formal: For every sentence in D, we calculate its representation (r j = r j t ) by taking the average of the representations from all the sentences in D. Formal: We also have a set of parameters θ for the model, and we train it using the objective function that works for the discrete space model. Formal: Now, let's talk about how we represent the dialogue data, D. Formal: We take the whole collection and divide it into M smaller parts, each with a size of m. Formal: After that, we split each part into a part with dimensions m − 1 and a part with dimensions m + 1, which are d × m − 1 and d + 1, respectively. Formal: Finally, we combine all these parts into a final representation, which is a mix of m + 1 and d − 1, all together. Formal: The result is a mix of m − 1 and d − 1, which we call D t. Formal: So, the final representation D t for the whole collection is just the result of combining all these parts. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-1682.json"
    },
    {
        "casual_text": "If the word is a relative pronoun, chances are pretty good that the next word will be a noun.",
        "formal_text": "If word is relative pronoun then there is high probability that next word will be noun.",
        "GPT2_formal_text": "_t is a matrix that represents how much each word in the input sentence is masked, while attention_mask_t_i is a matrix that tells us how much attention the input word should get. Formal: We train our model using cross-entropy loss. Formal: We'll start by setting the initial weight vector e to zero, which is what Equation 1 does. Formal: Next, we calculate the loss for each token using the cross-entropy loss. Formal: Next, we calculate the attention vector for each token. Formal: After that, we mix the output of the multi-head attention mechanism and the last hidden state of the attention head to get the final attention vector h. Formal: We set the gradient of this attention vector to zero. Formal: If the model is already doing well and the model's loss isn't too big, we can use greedy decoding to get the final prediction. Formal: But, if the model is doing poorly and the loss isn't big, we can't use greedy decoding. Formal: In this paper, we'll use the greedy decoding algorithm with a learning rate of 0.1. Formal: For decoding, we use a beam size of 50. Formal: When we run our model, we calculate the negative log-likelihood (from Equation 2) and the log-likelihood of the predicted token y_t, based on the previous hidden state h. Formal: We train our model on a GeForce GTX 980M GPU. Formal: To create a log-probability distribution over the token's sequence, we pick the token with the highest probability. Formal: We pick the token with the lowest probability. Formal: For the fourth and fifth hidden states, we just randomly pick the token with the lowest probability. Formal: The final probability distribution for the token sequence is shown in Figure 1. Formal: We also include the predicted probabilities of the input token and the model's output token, which we get using a greedy algorithm. Formal: The probability distribution is calculated using the L2 norm of the log-likelihood, which we get by using the feedforward neural network (FFNN) with a linear kernel. Formal: We use a linear kernel to minimize this log-likelihood loss. Formal: We use beam sizes of 25 for the first and 50 for the fourth hidden states. Formal: We set the learning rate to 0.1. Formal:",
        "directory": "C12",
        "filename": "C12-3021.json"
    },
    {
        "casual_text": "Preprocessing involves breaking down text into tokens, identifying things like names, URLs, and numbers.",
        "formal_text": "Preprocessing includes tokenization, recognition of named entity, urls and numbers.",
        "GPT2_formal_text": "= (h, t) = (r, e) = (v, t) = (1, 2, ..., n) = (h, t). Formal: As a starting point, we’ve got some ideas in the literature (like the ones by Ouyang et al., 2009; Ortiz et al., 2012b; Grishman et al., 2013) that might help us out. \n\nIn Figure 1, we’ve got some examples of entity pairs we want to create. The second sentence is the input for the entity pair detection model. The third sentence is the output of the entity pair detection model. In the fourth sentence, we’ve added an \"IF\" token to make sure we don’t assume the entity pair is correct. \n\nTo generate the knowledge, we need to figure out the relationships between the entities and the knowledge. The relationship between an entity and a knowledge point is basically the relation between them. We’ll call these relationships \"relations\" in our paper. The relationships we’re looking at are: Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can be written as... Formal: A triple can",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "Leaderboards are kind of the go-to way to track progress in question answering (Rajpurkar et al., 2016) and a bunch of other NLP tasks (Wang et al., 2019a). But this popularity has a downside: people get obsessed with chasing the \"state-of-the-art\" (SOTA) without really digging into the data or models (Linzen, 2020). For instance, those \"super-human\" models that dominate question answering leaderboards (Najberg, 2018) often flop in real-world scenarios (Feng et al., 2018; Wallace et al., 2019a) because they’ve just learned some tricks that don’t actually work in practice (McCoy et al., 2019; Niven and Kao, 2019). \n\nAnd here’s the kicker: focusing only on the numbers makes it seem like progress in one specific task is the same as progress in real-world NLP challenges (Bender and Koller, 2020). Basically, just looking at those shiny SOTA numbers doesn’t tell us much about how things actually work or where they fall apart (Lipton and Steinhardt, 2019). Leaderboards can give us a sense of how hard, clear, or doable certain examples are. If something has really low discriminability, it might mean there’s an annotation mistake. Like, there was a question that scored super low: \"Why did demand for rentals decrease?\" when the real answer should’ve been \"demand for higher quality housing increased.\"",
        "formal_text": "Leaderboard evaluations-for better or worse-are the de facto standard for measuring progress in question answering (Rajpurkar et al. , 2016) and in many NLP tasks (Wang et al. , 2019a ). An unfortunate side effect of leaderboard popularity is SOTA-chasing, often at the expense of carefully inspecting data and models (Linzen, 2020). For example, the same \"super-human\" models that top question answering leaderboards (Najberg, 2018) often fail spectacularly (Feng et al. , 2018; Wallace et al. , 2019a) by learning non-generalizable statistical patterns (McCoy et al. , 2019; Niven and Kao, 2019). Finally, focusing solely on metrics conflates progress on a specific task with progress on realworld NLP problems behind the task (Bender and Koller, 2020). Plainly, focusing on headline SOTA numbers \"provide(s) limited value for scientific progress absent insight into what drives them\" and where they fail (Lipton and Steinhardt, 2019 leaderboards infer the difficulty, discriminativeness, and feasibility of examples. Negative discriminability suggests an annotation error; for example, the question with most negative discriminability asks \"Why did demand for rentals decrease? \" when the answer is \"demand for higher quality housing increased. \"",
        "GPT2_formal_text": "ed = {0, 1, 2, 3} k, where k stands for the length of the input sequence. These parameters are picked by sampling from a Gaussian distribution with mean 0 and variance 1. Formal: We focus on the most common attention values. If there's a lot of data, we might use a Gaussian distribution to pick the value (ρ_v). Formal: We also have two other ways to pick attention values: k-1 and k-2. Formal: When k is less than or equal to k_2, we just go with the k-1 attention. This is different from what we did before when we picked k-1 attention. But k-1 and k-2 have some similarities. Formal: In Figure 1, we can see that the attention matrix A is made up of the hidden states of the first-level encoder, h_1, and the second-level decoder, h_2. Formal: In this paper, we focus on the attention values from the first-level encoder, which we call h_0, and the second-level decoder, which we call h_1. If the model isn't set up to learn these values, we pick them based on the cross-entropy loss. Formal: Using the second-level decoder, we create a second attention matrix, A_v, that has the hidden states from the first-level decoder, h_1. Formal: Here, v_k is the attention value for the k-th token, while v_k_1 is the attention value for the k-th token, k_1. Formal: Finally, we combine the attention values from the first-level and second-level decoders to get the final attention matrix A_v. Formal: We also use a simple linear function to calculate the cross-entropy loss, which is shown in equation (1). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.346.json"
    },
    {
        "casual_text": "We’ve come up with a bunch of syntactic transformation rules for translating Japanese into English, mainly because the two languages have different structures. These rules handle things like verb, noun, and clause reordering. Even though we’re mainly focusing on Japanese to English, a lot of these rules can also work for other languages that switch from SOV (Subject-Object-Verb) to SVO (Subject-Verb-Object) structures.",
        "formal_text": "We design a variety of syntactic transformation rules for Japanese-English translation motivated by their structural differences. Our rules cover verb, noun, and clause reordering. While we specifically focus on Japanese to English, many rules are broadly applicable to SOV to SVO languages.",
        "GPT2_formal_text": "= {k1, k2, ..., kT} where t_i is the i-th token. Formal: But here's the thing: those attention weights are only learned during training. So, if you want to use them to figure out the sentiment for a post, you'd need to fine-tune the model. Formal: There's no parallel data available, so we came up with a plan to use a dataset from a different area. Formal: So, we take a bunch of text from a different area—say, a tweet—and use it to train our model. We randomly split it into training, development, and test sets. The training set includes a mix of different domains, like news and product reviews. The development set has the same mix, but it's for different industries. Finally, the test set has the same mix again, but this time for general topics. Formal: To make sure our model gets the most out of the data, we use a cross-entropy loss function. Formal: Formal: We also mix in some regularization to help the model focus on the important parts of the data. Formal: If you want more info, check out the original paper. Formal: The complete process for training and testing our model is outlined in Algorithm 1. Formal: We break down our training and test sets into training, development, and test sets. Then, we add domain-specific features, using the results from these sets to train our model. Formal: After training, we train the model using a cross-entropy loss function (from the original paper). Formal: Finally, we test our model by running it through a sequence labeling system, using the labels we've learned for the target domain. Formal: Formal: We're sharing the code we used to train our model, which you can find here: https://github.com/google-research/bert Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D15",
        "filename": "D15-1006.json"
    },
    {
        "casual_text": "Re-ordering can be seen as a mix of different problems that are connected and can be explained or solved using various linguistic ideas. First off, a lot of long-range re-ordering happens because of differences in how phrases are ordered. Syntax-based and hierarchical models, like the one by Chiang (2005), try to tackle this issue. On a smaller scale, things like word re-ordering within a phrase can usually be predicted based on the nature of the words and their context, with POS tags being a pretty clear clue.",
        "formal_text": "Re-ordering can also be viewed as composing of a number of related problems which can be explained or solved by a variety of linguistic phenomena. Firstly, differences between phrase ordering account for much of the long-range reordering. Syntax-based and hierarchical models such as (Chiang, 2005) attempts to address this problem. Shorter range re-ordering, such as intraphrasal word re-ordering, can often be predicted from the underlying property of the words and its context, the most obvious property being POS tags.",
        "GPT2_formal_text": "ed: First, we figure out the attention masks for the hidden state using the hidden state (h_src) as input, so we can figure out the attention values for each token. Then, we add all these attention masks to the hidden state to create the token representation h_t. Formal: Next, we use a linear layer to figure out the probability distribution p(t|h_t). We do this for all the tokens in the vocabulary V. Formal: Finally, we throw in a linear layer to get the probability distribution for the prediction. Formal: We use the parameters from the hidden states of the RNNG as the input for the token representation h_t, which we get using the attention masked attention vector h_t. Formal: Figure 1 shows the whole process of how we generate the output token y_t for an input sentence x_t using a neural network, which we call the Bi-LSTM. Formal: Our system learns the full representation h_t for the token y_t using two RNNs. Formal: We use the output tokens y_t and h_t to figure out the full representation for the token x_t, using the same RNNs. Formal: We then combine the outputs from the two RNNs to get the hidden state representation h_t for the token x_t. Formal: The final output is the probability distribution of the prediction, p(y_t|x_t), calculated using the combined outputs from both RNNs. Formal: In this paper, we mainly focus on generating a token y_t, so we only consider the results from this token generation process. Formal: Figure 2 shows a typical Bi-LSTM setup, where we have two RNNs, each generating a token y_t. Formal: The output from the first RNN is the identity matrix, i, and the output from the second RNN is the transition matrix, t, which has two parts: (i, j), which is the identity matrix (i = 1), and (i, j), which is the transition matrix (j = 1). Formal: In the second part, t and i have the same dimensions, but t is 1, and i is 0. Formal: We also use the matrix A_t = (a_t, r_t, b_t) to combine the outputs from the two RNNs.",
        "directory": "E09",
        "filename": "E09-1043.json"
    },
    {
        "casual_text": "Figuring out which parts of the text are important is a big deal in NLP stuff (Reiter and Dale, 2000). Some folks in 2018 used a pointer network (Vinyals et al., 2015) to pick out key phrases for making questions, and Gehrmann et al. in 2018 used a content selector to control how often they copied stuff for summarizing. The goal of these methods is to make things more accurate, but our approach does that and also boosts diversity (check out our results for proof). Plus, our method helps models learn how to use the selected info, unlike Gehrmann et al. (2018) who manually restrict the copying for non-selected parts.",
        "formal_text": "Content Selection in NLP Selecting important parts of the context has been an important step in NLP applications (Reiter and Dale, 2000). 2018use a pointer network (Vinyals et al. , 2015) for extracting key phrases for question generation and Gehrmann et al. 2018use content selector to limit copying probability for abstractive summarization. The main purpose of these approaches is to enhance accuracy, while our method uses diverse content selection to enhance both accuracy and diversity (refer to our empirical results). Additionally, our method allows models to learn how to utilize information from the selected content, whereas Gehrmann et al. (2018) manually limit the copying mechanism on non-selected contents.",
        "GPT2_formal_text": "= { q, r } | t = T j, a = 0 | r = 1, • • •, t = T k } n i=1 \n\nHere, q, r, and t are the actual question tokens (Q_i) and their context-aware representations (r_i) for the i-th and k-th sentences, respectively. Formal: We’re using the fastText dataset (from Kitaev and Klein, 2016) for our experiments. Formal: This dataset includes a dataset of 10,000 samples, each with one token, one sentence, and its corresponding query. Formal: The training set has 1,971 samples, and the validation set has 1,502 samples. Formal: Each of the ten input examples is matched with a corresponding query. Formal: We calculate a sequence-level loss by minimizing the cross-entropy loss between the query's representation and the dataset’s representation. Formal: To handle the question-response relation extraction task, we’ve set up some structured prediction models based on the fastText dataset (Kitaev and Klein, 2016). Formal: We use the model from (Li et al., 2016) for training. Formal: We tune the model for the query, using a gradient descent approach. Formal: We measure the performance using the average F1 score. Formal: We calculate precision (P), recall (R), and the F1 score using the test set. Formal: The test set includes 1,971 samples, and the validation set has 1,502 samples. Formal: The loss function is calculated by applying the smoothed gradient-based loss to the query-sentence pairs and the query-query pairs, resulting in the query-sentence loss. Formal: We calculate the cross-entropy loss for the query-sentence pairs using the test set. Formal: We calculate the cross-entropy loss for the query-query pairs using the test set. Formal: We calculate the cross-entropy loss for the query-sentence pairs using the test set. Formal: For the query-sentence loss, we use the cross-entropy loss from the base model. Formal: For the query-query loss, we use the cross-entropy loss from the query-sentence model. Formal: We calculate the loss for the query-sentence pairs by applying the smoothed gradient",
        "directory": "D19",
        "filename": "D19-1308.json"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way:\n\nSo, you have two taggers, A and B. They’re working through a sequence, and at each step, they’re dealing with weights (w) and tags (ta and tb). Here’s how it looks:\n\n- Tagger A goes through the sequence like this: w₂ ta₂ tb₂, then w₁ ta₁, tb₁, and so on, until it reaches wₙ₋₁ taₙ₋₁ tbₙ₋₁, and finally wₙ taₙ tbₙ.\n- Tagger B follows a similar pattern but skips the ta tags: w₂ tb₂, then w₁ tb₁, and continues until wₙ₋₁ tbₙ₋₁ wₙ tbₙ.\n\nBasically, both taggers are processing the same sequence, but Tagger A includes the ta tags, while Tagger B doesn’t.",
        "formal_text": "τ (⃗ y i, ⃗ y i−1 ). A tagger. . . B tagger. . . . . . w 2 ta 2 tb 2. . . w 1 ta 1. . . tb 1. . . . . . w n-1 ta n-1 tb n-1 w n. . . . . . ta n tb n A tagger. . . B tagger. . . . . . w 2 tb 2. . . w 1. . . tb 1. . . . . . w n-1 tb n-1 w n. . . . . . tb n. . . . . . . . . . . .",
        "GPT2_formal_text": "is calculated as the total number of words in the input sequence. Formal: The decoder takes the output from the encoder and turns it into a sequence of words. The token for the last word is usually the one that got predicted, like in Figure 1. Formal: In all these examples, we’re using the input word sequence x = (x 1, . . . , x n ), and the decoder spits out the output word sequence y = (y 1, . . . , y m ). Formal: The decoder is basically a neural network that takes the output from the encoder and spits it out as a token. Formal: There’s another method called variable-length decoding, which uses a fixed-size decoding matrix, R. Formal: In this method, the decoder is a linear projection that takes the output from the encoder and turns it into a sequence of words. Formal: This method has a big problem: it makes the model predictable and requires a ton of computational power. Formal: Also, it doesn’t consider the different levels of meaning or the meaning of words in the input. Formal: We’ve tried these methods on different datasets and found that fixed-length decoding usually works better. Formal: We’ve also used binary-length decoding, which turns the output into a fixed-size sequence of tokens. Formal: The decoder here is a linear projection that takes the output from the encoder and spits it out as a token. Formal: This method also makes the model predictable, but it doesn’t account for how words are used in different contexts. Formal: We’ve tested both fixed-length and binary-length decoding. The results show that the fixed-length method works better. Formal: We’ve also looked at the Transformer decoder from Vaswani et al. (2017). Formal: This decoder is a bidirectional linear projection that takes the output from the encoder and spits it out as a token. Formal: We’ve also tried it on other datasets and found that binary-length decoding works better. Formal: For our experiments, we’re using the proposed models from (Zhang et al., 2020b). Formal: The model we’re proposing is a linear projection that takes the output from the encoder and spits it out as a token. Form",
        "directory": "D16",
        "filename": "D16-1070.json"
    },
    {
        "casual_text": "The first dataset we're looking at comes from Yencken and Baldwin (2006), and it's based on human similarity judgments. The problem with this dataset is that it treats figuring out the difference between low and medium similarity pairs the same as distinguishing between medium and high similarity pairs. But, let's be real, for most practical uses, nailing the high similarity pairs with great accuracy is way more important. Still, this dataset is handy for checking how our methods stack up against those used in earlier studies.",
        "formal_text": "The first data set is the human similarity judgements from Yencken and Baldwin (2006). This data set is overly broad in that it weights the ability to distinguish low and medium similarity pairs equally with distinguishing medium and high similarity pairs. It is clear that for most applications, determining the high similarity pairs with high precision is most important. Nevertheless, this data set is useful for comparing our metrics with those proposed in previous research.",
        "GPT2_formal_text": ", which are the individual questions we care about. We've got a list of these questions, and we'll explain how we use those in the next few sections. Formal: We measure how different two sets of questions are by calculating the tf-idf distance between their questions. Formal: So, we start with the question embeddings and the embeddings for the question and the answer. We then set up a linear projection from the question embedding space to the answer embedding space. This projection gives us a vector that tells us the distance between the question and the answer. Formal: Using this projection, we calculate the tf-idf distance between two question vectors, x = (x1, x2) and y = (y1, y2), based on how far apart they are in the embedding space. For example, the distance between x1 and y1 is calculated like this: Formal: We can use the distance between y1 and y2 to calculate the tf-idf distance between x1 and x2. Formal: We also use the distance between y1 and y2 to calculate the tf-idf distance between x1 and y2. Formal: Lastly, we calculate the tf-idf distance between x1 and y1. Formal: The tf-idf distance between two vectors gives us the probability that x1 and x2 belong to the same question. Formal: A similar thing is the tf-idf distance between y1 and y2. Formal: These two distances are calculated to get a probability between x1 and y2, but this time, it's based on how similar they are in the embedding space. Formal: Formal: We add these probabilities together to get the probability between x1 and y1. Formal: The more similar the embedding spaces of x1 and y2, the more similar the question and answer are. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C08",
        "filename": "C08-1131.json"
    },
    {
        "casual_text": "At every merge step in the CKY process, we add another subtree. This one doesn't just average the two attention values, a_cl and a_cr (like in eq. 1), but also includes the child polarity scores, p_cl and p_cr. This setup aligns with how the N-N nuclearity class is defined in RST, where all child nodes are considered equally important in multi-nucleus cases. The only way we can handle the extra complexity of doubling the number of trees in each cell is by using our heuristic method.",
        "formal_text": "an additional subtree at every merge in the CKY procedure, which averages not only the two attention values a c l, a cr (as shown in eq. 1) but also the child polarity scores p c l, p cr. This reflects the definition of the N-N nuclearity class according to RST, where an even importance for all child nodes is assumed in the multi-nucleus case. The additional complexity of doubling the number of trees in each cell is only manageable due to the use of our heuristic approach.",
        "GPT2_formal_text": "ed.\n\nThe attention mask is like a rule that tells the model to ignore some info from the input x_i and focus on the parts of x_j that are important. It does this by using a special vector called attention_mask, which is created using a three-layer perceptron (like the one from Bahdanau et al., 2014). Basically, the attention mask for a token is calculated like this: Formal: The attention mask works by calculating a vector that maps the input into a special space. The score for this vector is calculated using a formula that involves the attention weights and the hidden states from the three layers of the perceptron. Formal: Here, L is the number of layers, and N_n is the total number of tokens in the input. We use BERT as the classifier for this. Formal: We use the base Transformer model (like the one by Vaswani et al., 2017) to predict the attention mask. Formal: The attention weights are tweaked by multiplying them with a function that takes into account the hidden states of each layer in the perceptron. Formal: We run the classifier using these weights to figure out the attention mask for each token. Formal: Finally, we combine all the attention mask vectors into a single attention mask to get the overall attention mask. Formal: We use the input x_i as our starting point and the prediction of the attention mask we get as our objective. Formal: After that, we update the attention weights by multiplying them with the function that takes into account the hidden states of each layer in the perceptron. Formal: We use BERT as the classifier for this. Formal: The final output is a vector with a weight vector, and the prediction of the attention mask is our objective. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.603.json"
    },
    {
        "casual_text": "For the next experiment, we’re using the method from Section 3 on an English dataset to create training data tailored to each language and train a separate semantic parser for each one. We had some of the validation set translated by humans and mixed it with the machine-translated validation data. From here on out, we’ll pick the model with the best exact match (em) accuracy on this combined set and test it on human-translated test data. As you can see in Table 4, this approach beats all the other methods we tried. We got improvements ranging from 33% to 50% compared to the previous best result, which was the Bootstrap approach. \n\nThe neural model we trained on this SPL data benefits from entity alignment between the sentence and the logical form, allowing it to copy entities directly. The exact match accuracy varies—it’s 53% for Chinese and 62% for Spanish when it comes to hotels, and 41% for Japanese and 68% for Spanish for restaurants. When you compare this to the English results, which were 65% for hotels and 69% for restaurants, you can see that performance drops for languages that are very different from English. However, languages closer to English, like Spanish, come pretty close to matching English’s performance.",
        "formal_text": "In the next experiment, we apply the methodology in Section 3 to the English dataset to create localized training data and train one semantic parser per language. We translate a portion of the validation set using human translators and combine it with the machine-translated validation data. For all the following experiments, the model with the highest em accuracy on this set is chosen and tested on human-translated test data. As shown in Table 4, the results obtained by this methodology outperforms all the baselines. Specifically, we achieve improvements between 33% to 50% over the previous state-of-the-art result, represented by the Bootstrap approach. The neural model trained on SPL data takes advantage of entity alignment in the utterance and logical form and can copy the entities directly. The exact match accuracy ranges from 53% in Chinese to 62% in Spanish for hotels, and from 41% in Japanese to 68% in Spanish for restaurants. Comparing to the accuracy of 65% and 69% for hotels and restaurants in English, respectively, we see a degradation in performance for languages that are very different from English. Languages close to English, such as Spanish, approach the performance of English.",
        "GPT2_formal_text": ", which are the unique IDs of the tokens in the query (i, j). These are the bits of information that describe the input. For the binary tagger, these bits are paired with a column vector (f), which is just a bunch of numbers (x_i, x_j). These numbers are the weights that go with each individual bit. For example, the output from the first bit (1) is always 1, and the output from the last bit (2) is always 0. Formal: We use the binary tagger to create a query, let's call it q, which has two parts: (1) a \"bag of words\" (b_w) that includes the query's tokens (x_i, x_j), and (2) a \"bag of attributes\" (b_a) that includes the query's features (x_i, x_j). These parts are put together to create the query's representation (q_1, q_2, ..., q_n), which is basically a bunch of vectors, x_i, x_j, x_k, and x_n. Formal: To find the best model, we train it on the test set using a beam search. This process is repeated until we get a model that performs really well, which we call the \"gold standard.\" Formal: Figure 2 gives an example of how we pick the top N candidates. The \"Top N candidates\" are all the tokens that have a probability of 1, meaning they're the most likely to be the right answer. Formal: In this paper, we’re using a simple two-step process: first, we pick the best answer candidate from a given set of candidates. Then, we use that top answer to train a model using a beam search. Formal: This process is called \"bottleneck sampling,\" and we’ve got a quick explanation of it in section 3.2. Formal: Formal: We’ve also set up a scoring function for the top N candidates, which we’ll call λ_1, λ_2, ..., λ_K. Here, λ_1, λ_2, ..., λ_K are the scores that go with each single bit in the binary tagger, and λ_1, λ_2, ..., λ_K are the weights that go with each individual bit. Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.481.json"
    },
    {
        "casual_text": "Using two bytes to represent an integer gives you 2^16 - 1 = 65536 unique codes. But since there are way more words in English than we can fit into those codes, we need a way to handle words without assigning each one a specific code. By setting aside one bit for this purpose, we're left with 2^15 - 1 = 32767 possible combinations.",
        "formal_text": "In using two bytes to represent an integer it is possible to have 216 -1 = 65536 distinct codes. However, since it is impossible to have codes for all the words in the English Lan~lage, it is necessary to include a mechanism that allows for the representation of words without codes by their individual characters. Keeping one bit for that purpose ]. eaves 215 -i = 32767 possible number of combinations.",
        "GPT2_formal_text": "ed_vec8 (V_t, M_t, D_t) = [v_t]_t^-1 [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1] [s_t]_t^-1]",
        "directory": "C86",
        "filename": "C86-1098.json"
    }
]