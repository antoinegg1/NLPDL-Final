[
    {
        "casual_text": "When fine-tuning using the MLM objective, the BASE version of models usually shows better performance improvements on the General Corpus dataset. This is likely because models with bigger parameter sizes are better at understanding the connections between different parts of similes, especially when dealing with similes that have more varied contexts.",
        "formal_text": "fine-tuning with the MLM objective. For the General Corpus dataset, the BASE version of models tends to yield higher performance improvements, probably because the models with larger parameter sizes can better capture the relationship among simile components in the similes with more diverse contexts when fine-tuning with the MLM objective.",
        "GPT2_formal_text": ". For instance, if we’re talking about the `time_t` attribute, we might mention the `time` attribute for `j=1` or `j=2` in Figure 1. We’re not trying to force the model to always use the same `type` token at each level of the tree. The reason is that it’s better for the model to pick the token that fits best based on the dependency tree. Plus, as we’ll explain, using a token that’s specific to the specific tree can sometimes lead to more accurate results. For instance, if we’re dealing with a tree like the one in Figure 1, the `type` token might be helpful, but it might not be the best option for all the other trees in the treebank. If we can’t decide between different options, the model can pick the token that matches the most across all the levels. \n\nThis approach works well because it ensures that the model doesn’t get stuck in a loop and just picks the best option at each level. We’ll walk through the process of adding this kind of token-level support for a dependency tree in the next section. Formal: We’re looking at how to handle tree fragments in a treebank. Basically, we want to figure out the right order in which to split the tree. Formal: For any given fragment `f`, the best order to split the tree is... Formal: For each fragment `f` in `t*, the best order to split the tree is... Formal: For each fragment `f` in `t*: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.543.json"
    },
    {
        "casual_text": "Position-Aware Graphs. PacSum is a new graph-based method that picks out important sentences from a bunch of documents (Zheng and Lapata, 2019). In PacSum, a sentence is more likely to be chosen if it’s more similar to the sentences that come after it and less similar to the ones that come before it. This approach helps PacSum focus on selecting sentences that are early in the text or \"semantically central.\" We took PacSum and made it even better by using SBERT to measure how similar sentences are (we call this new version SPS) and looked at both the individual and global-graph versions of SPS.",
        "formal_text": "Position-Aware Graphs. PacSum is a recently proposed graph-based method to select salient sentences from multiple documents (Zheng and Lapata, 2019). In PacSum, a sentence is more likely to be selected if it has higher average similarity with its succeeding sentences and lower average similarity with its preceding sentences. This strategy allows PacSum to prioritize the selection of earlyposition and \"semantically central\" sentences. We further extend PacSum by using SBERT to measure sentences similarity (the resulting method is denoted as SPS) and consider both the individualand global-graph versions of SPS.",
        "GPT2_formal_text": "_in_batch = auto_mask_in_batch_len - 1; } else if (e_p and e_v) is in the same batch { if (e_p and e_v) is in the same batch { if (e_p and e_v) is in the same batch { if (e_v is in the same batch) { add_copy(e_v, e_p, p_t) } } else if (e_p and e_v) is in the same batch { add_copy(e_v, e_p, p_t) } } else if (e_v and e_p) is in the same batch { add_copy(e_v, e_p, p_t) } else { add_copy(e_v, e_p, p_t) } } } else if (e_v and e_p) is in the same batch { add_copy(e_v, e_p, p_t) } else { add_copy(e_v, e_p, p_t) } } else if (e_v and e_p) is in the same batch { add_copy(e_v, e_p, p_t) } else { add_copy(e_v, e_p, p_t) } }\n\nTable 4 shows how well our model performs on two different datasets. The results for each dataset are summarized in Table 5. Our model is really good at handling the tricky stuff in both datasets, and it does a great job of pulling out key pieces of information. As you can see, our model does better than a bunch of other methods in the first dataset and even does better than some baselines in the second one. Formal: We tested our model on a publicly available dataset, using the same settings as in Section 3. Formal: We also tested our model on a more advanced dataset, which has a mix of news and social media posts. Formal: We ran a bunch of tests using different random seeds and batch sizes, with different random seeds and batch sizes. Formal: For each setup, we looked at the ROUGE-1 scores and the P@1 score. We averaged the ROUGE-1 scores and calculated the P@1 score. Formal: We also talked about how important the model's embeddings are. Form",
        "directory": "acl",
        "filename": "2020.acl-main.124.json"
    },
    {
        "casual_text": "Let’s take a look at how different design choices affect how well our model works.",
        "formal_text": "We now analyze how different design choices impact the performance of our model.",
        "GPT2_formal_text": ". The document gets a weight of 1 if the mention is found in the query and 0 if not. The rest of the info is kept aside, which we call \"default values.\" This default value can be adjusted during training. Formal: In each round of fine-tuning, the model gets a random shuffle of the tokens in the query and the query itself, like shuffling the keys and values. Formal: Once the training is done, we want to use the model to find candidates that share the same parent entity. Specifically, we want to figure out the match between the parent mention and the candidate mention. Formal: We use the similarity score from the logistic regression model to score all possible candidates, considering all the possible parent mentions. The similarity score is normalized based on the cosine similarity between the candidate mention and the mention that's most similar. This normalized score is used as the weight for the top node in the graph. Formal: Each candidate gets a score of 0 if the parent mention is in the query and 1 if not. The score for the top node in the graph is calculated based on the normalized similarity between the mention that's most similar to the parent mention and the candidate mention. Formal: We use a simple linear linear model to find candidates for the parent mention. Formal: The parameters θ(c) are chosen by figuring out the best value of a hyperparameter α, which we learn during fine-tuning. Formal: When we're training, the candidate mention is added to the query with a weight of 1, and the weight for the parent mention is adjusted based on the cosine similarity between the mention and the mention that's most similar to it. Formal: We train this whole process using stochastic gradient descent. Formal: After training, we check the candidate mention for similarity with the parent mention to see if they match. If they do, we update their score to 0. Formal: We only use the candidate mention if the parent mention is in the query and the candidate mention is in the query. Formal: To make sure our model can work with multiple contexts, we use a hierarchical multi-instance learning approach. Formal: To estimate the performance of our multi-instance learning, we use a data augmentation method that helps with the overfitting issue. Formal: Following the same approach as in (Holtzman et al., 2017), we train our model using a small number of examples that are specific to the query and",
        "directory": "acl",
        "filename": "2020.acl-main.649.json"
    },
    {
        "casual_text": "[41] Check if the system's knowledge base is on board with, disagrees with, or doesn't really care about adding this component to the overall plan.",
        "formal_text": "[41 whether the system's domain knowledge supports, contradicts, or is: neutral regarding inclusion of the component as part of a correct overall plan.",
        "GPT2_formal_text": "ing (like in Figure 1). In Figure 1, the red part represents the original data, and the green part represents the ones we got from the transformer model. We use the average attention from all the tokens to figure out the attention weights. Formal: We’ve got a bunch of training data with a mix of unlabeled and labeled data. We use an attention mechanism to grab attention from all the tokens to pull out attention weights. Formal: Since the attention weights are learned through data, we use an embedding layer to create a representation of the data. This representation is then fed into the student network to get the final prediction for the token. Formal: To train the student network, we first apply an attention mechanism to the whole unlabeled data set. Then, we feed the student network with the input from the student network. Formal: To get the token prediction, we calculate the average attention weights using the top-k output tokens. Formal: To train the teacher network, we first apply an attention mechanism to the whole labeled data set. Then, we feed the teacher network the data through the student network to get the token prediction. Formal: To improve the student network, we add an embedding layer to the student network to better represent the data. Formal: We show how the attention weights are calculated in Equation 3, and you can check out the final result in Figure 1. Formal: We used this teacher network to find the best attention weights for each token. Formal: We’ve got all the attention weights from the student network in Equations 1 and 2. Formal: We used this model to rank the results. Formal: We tested this on the validation set, and the results show how well the model works. Formal: We ran a bunch of tests using a few different approaches to measure how well the attention weights are learned and transfer between different tasks. Formal: We compared the results from the same teacher network to the best results from a student network using the same approach. Formal: We also did some experiments to see how well the model transfers to different tasks. Formal: We ran a bunch of tests using a few different approaches to measure how well the model learns and transfers between different tasks. Formal: We used this model to find the best attention weights for each token. Formal: We found the best attention weights for each token. Formal: We created a visualization of the teacher network using the student network and the",
        "directory": "C86",
        "filename": "C86-1006.json"
    },
    {
        "casual_text": "The models are consistent, but they don't give us a direct way to estimate costs. So, to help with picking the right model for real-world applications, especially when you're working outside of academia, we’re keeping an eye on how long things take and how much they actually cost. We’re using cloud machines for this because they’re easy to get, and they always have the same hardware, price, and performance. In the next parts, we’ll just call \"time\" the time it takes and \"cost\" the actual money spent during pretraining, training, and when doing predictions.",
        "formal_text": "is steady for models but cannot be directly used for cost estimation. Here, to provide a practical reference for model selection on real applications, especially model development outside academia, we keep track of the time consumption and actual financial cost for comparison. Cloud-based machines are employed for budget estimation as they are easily accessible and consistent in hardware configuration, price, and performance. In the following sections, we would use \"time\" and \"cost\" to denote the time elapsed and the actual budget in model pretraining, training, and inference.",
        "GPT2_formal_text": "= { u_i } n = (u_i, ..., u_M) # The unique ID of the query, which we call q_i. θ_s is the model's parameters, and p_t is the query term's probability, which we call p_t. Formal: So, the query, q_i, is made up of a bunch of words w_t and q_i, and the probability p_t comes from something called a Dirichlet distribution. Formal: To get the probability p_t for the query, we use the formula shown in equation (2). Formal: The model's parameters, p_t, are calculated based on the formula in equation (1). Formal: The model's parameters, θ_s, are calculated using the formula in equation (3). Formal: Finally, the query term's probability, p_t, is calculated using the formula in equation (4). Formal: Formal: When dealing with a long-tail distribution like q_i, we use a multinomial logistic model with the Lagrange function, which gives us an S-rule p_t = {p_t | q_i} for q_i. Formal: Formal: We also use a logistic function called L_t, which gives us an S-rule p_t = {p_t | q_i} for q_i. Formal: Formal: Formal: Formal: Formal: The model's parameters, θ_s, are calculated based on the formula in equation (4). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-demos.39.json"
    },
    {
        "casual_text": "In the QA generation interface, workers see a sentence with all the target words highlighted in bold. They're asked to come up with one or more questions that connect two of these bold words. To make the question, they start by picking a question prefix, then, if needed, add an auxiliary, and finally, choose parts of the sentence to build the full question. They might tweak it a bit to make it sound right. After creating the question, the next step is to answer it by picking parts of the sentence. Just like with the question, they can adjust the answer to make it grammatically correct.",
        "formal_text": "In the QA generation interface, workers are shown a sentence with all target words in bold. Workers are instructed to generate one or more questions that relate two of these target words. The question is generated by first choosing a question prefix, then, if applicable, an auxiliary, then selecting one or more spans from the sentence to form the complete question, and lastly, change it to make it grammatical. Given the generated question, the next step involves answering that question by selecting span(s) from the sentence. Again, the answer can also be amended to be made grammatical.",
        "GPT2_formal_text": "ing function.\n         Casual: Okay, so let's break this down in simpler terms. Imagine we have a sequence of words (let's call it X) that we want to learn. The problem is, we don't know which word we're looking at. To fix this, we use a special attention thingy called the dot product of the words (which we'll call a_t). This dot product helps us tell the difference between two words (h_i and h_j) by their similarity.\n\nNow, we can also figure out the attention score for a specific part of the words (h_i,h_j) by averaging the attention scores from all the words in the sequence. This gives us a score (h_i,h_j) = h_i + h_j. The cool thing is, this score is directly related to the character attention masking part. Formal: Formal:\n\nIn the following equation, the e_i and e_j are just the attention scores for the two words (h_i and h_j) in the sequence. Formal: Formal:\n\nThe RNNG takes the input sequence X and creates a representation of it. Formal: Formal:\n\nNext, it uses the special dot product attention to figure out the attention score for the whole sequence. Formal: Formal:\n\nSo, in short: Formal: Formal:\n\nSo, it's a step-by-step process where we use different attention functions to get the attention for different parts of the words in a sequence. Formal: Formal:\n\nAnd that's how we learn a sentence representation for the sequence X. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "If there were no out-of-vocabulary subwords in the bilingual lexicon, the recall of the oracle method would be 100%.",
        "formal_text": "The recall of the oracle method would be 100%, if there were no out-of-vocabulary subwords in the bilingual lexicon.",
        "GPT2_formal_text": "= {w_t, e_t}... | w_t, e_t |. Formal: LTF and LTL represent the lexical frequency and lexical part-of-speech tags, respectively. (2) Formal: To figure out the matching vector for the matching token w_t, we use this formula: Formal: The sigmoid function is used to get the log probability P(w_t|w_t) for the source word w_t. Formal: To find the matching vector for the target word w_t, we use the log probability P(w_t|w_t) for the target word w_t. Formal: Finally, we combine the matching vector with the embedding vector to get the embedding for the target word w_t. Formal: The output from the transformer layer is a matrix with dimensions m by n × d. Formal: We calculate the embedding for the target word w_t using this formula: Formal: Similarly, the embedding for the source token w_t is also calculated based on this formula. Formal: The embedding for the target token w_t is calculated based on this formula. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-1142.json"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. \n\nSometimes, basic patterns or simple variables don't really carry much meaning or information. They’re just too basic and don’t tell us much. So, just grouping these simple patterns together or trying to unify them isn’t enough to capture the real, complex relationships and processes we’re interested in. It’s not efficient.\n\nLet’s look at an example to make this clearer. Imagine we’re talking about a mathematical category. This category is made up of different features. Each feature has a name and a value, and we can represent it like a term, say `term(name, value)`. \n\nDoes that help?",
        "formal_text": "Ordinary patterns as it/ Pr(; h)g Is. el< cxprcssivc power, t)esatlSC var[ablcs theFcirl arc Sil)i[)ly il](iClCl\"tlliltdt(7 alld Ihtis car'ry almost no irffqrrnalion 'l'hercforc, stie}l palL(ms aud unification among thcm arc msuffiei0nt for' capturlng t]le {~, i<'al/l rYlat ic al <! , >erm r'al ixat ior~ and tim process: n~> effici(ncy. ], It us look a. t some c: <amph. ~s below A ~, , l'anl matical catc£>ory is assumed Lo be it llst of features A feature consisLs of a feature nalnc and a w~hic, and rcprcscnLcd as a t. cmn like tt~rn. e (vat, z()",
        "GPT2_formal_text": "ing: a method for classifying attention-based word embeddings in a CNN that uses the attention mechanism [4]. Formal: Sure, we can make this more formal. So, we’re just going to use two-term and one-term attention. Formal: To get the two-term attention, you just add the attention for each term to the input vectors. And to get the one-term attention, you do the same thing but with the output of the CNN. Formal: We’re using the version of the CNN called CNN/Daily Mail (from He et al., 2015) for the classification task. Formal: We’re training the model with a batch size of 32 and running it for 200,000 steps. Formal: The hyperparameter λ is set to 0.1. Formal: For the word embedding model, we’re using the XLM-RoBERTa (from Lan et al., 2019) from Conneau et al. [2]. Formal: In the CNN/Daily Mail model, we’re using the bert-base-cased model from TensorFlow [4]. Formal: For the self-attention model, we’re using the same setup as in the CNN/Daily Mail model. Formal: Since the self-attention mechanism in the CNN/Daily Mail model can be seen as a type of attention, we’re looking into how to take advantage of that. Formal: We’re using the gated recurrent unit for the self-attention model. Formal: The output from this unit is fed into the transformer layers, which then provide the attention for the words. Formal: We’re also using the fastText (from Mikolov et al., 2013b) word embedding model to get the word embedding representation. Formal: Finally, we’re using the word2vec model from Mikolov et al. [3]. Formal: The model we’re proposing can be used for creating the embeddings for the text. Formal: Formal: We’re using the GROVER framework to train the model. Formal: The GROVER model has three main parts: a general goal, a multi-task objective, and a model selection mechanism. Formal: Formal: Formal: We’ve tested our model on a dataset of academic papers, where",
        "directory": "C86",
        "filename": "C86-1018.json"
    },
    {
        "casual_text": "Off-topic examples in human-rated data are usually too rare to teach an automated system how to spot and reject off-topic responses. As a result, these automated systems tend to struggle more than human raters when it comes to scoring off-topic stuff accurately (Lochbaum et al., 2013; Higgins and Heilman, 2014). To make sure speaking test scores are reliable, there needs to be a way to catch off-topic responses before the scores are finalized (Wang et al., 2019). \n\nIn our educational app, we use an automated speaking assessment system to help non-native English learners get ready for the IELTS speaking test. We’ve noticed that freemium features tend to attract more off-topic responses because some users just mess around with the system. In this case, being able to detect off-topic stuff accurately is super important. It helps build trust and encourages trial users to upgrade to paid customers.",
        "formal_text": "Off-topic examples in human-rated data is often too sparse to train an automated scoring system to reject off-topic responses. Consequently, automated scoring systems tend to be more vulnerable than human raters to scoring inaccurately due to off-topic responses ( Lochbaum et al. , 2013; Higgins and Heilman, 2014). To ensure the validity of speaking assessment scores, it is necessary to have a mechanism to flag off-topic responses before scores are reported (Wang et al. , 2019). In our educational application, we use the automated speaking assessment system to help L2 learners prepare for the IELTS speaking test. We do see a higher rate of off-topic responses in freemium features as some users just play with the system. In such a scenario, accurate off-topic detection is extremely important for building trust and converting trial users to paid customers.",
        "GPT2_formal_text": "= {h t } n t = {h t, j t } n t = {h t, k t } h t = {h t, k t } n t = {h t, k t } n t = {h t } j t = {h t } k t = {h t } n t = {h t } n t = {h t } n t = {h t } k t = {h t } k t = {h t } n t = {h t }\n\nJust to keep things clear, the labels for the relationship types are marked in bold. Formal: From Equation 1, it's clear that the connection between the terms is made up of two parts: the left-hand side (LHS) and the right-hand side (RHS). This setup is meant to make sure that the relation types are clear and easy to understand. Formal: When dealing with the multi-label relation prediction problem, a lot of the data we get has a lot of unknown or ambiguous connections. This makes it tricky to figure out the right labels for those connections. Formal: To handle this, we create a structured model for the relations. The goal is to build a set of structured features that makes it easy to find the right labels for the relation types. Formal: In this project, we're using the knowledge base to help us deal with these ambiguous relations. Formal: We tested our approach using two datasets: MEGA-DT and Multi-Blocked Relation Prediction. Formal: We chose MEGA-DT because it has lots of open relations and is easier to work with. Formal: We picked Multi-Blocked Relation Prediction (MPRP) because it has a lot of training data with unclear labels. Formal: In our experiments, we found that the DBSP model works the best, especially when dealing with high uncertainty. Formal: We also ran the multi-label relation prediction experiment on MEGA-DT. Formal: In the multi-label relation prediction experiment, we used the same setup as MPRP. Formal: Formal: We checked the results using the MEGA-DT dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.56.json"
    },
    {
        "casual_text": "You can confirm a segment by hitting the ACCEPT button. After you do that, it'll automatically move on to the next segment.",
        "formal_text": "• To validate a segment, the user can also use the ACCEPT button. Once pressed, it also moves to the next segment.",
        "GPT2_formal_text": "ing_vec2_tok+att, which is a matrix of size M × N. For a single sentence s i (which can be any sentence in the document D), we calculate the attention vector A i for the i-th token in s i by summing over all the tokens in s i. Formal: So, A i,j, which is a vector of size M × N, represents the attention for the i-th token in the sequence. Formal: For the next steps, we're thinking about creating a model that can create knowledge-enhanced embeddings for sentences. Formal: We've also created a model that uses the combined input from the caption and the question, kind of like a multi-task learning setup. Formal: We've set up a dataset for this multi-task learning, which we call Multi-task-1, Multi-task-2, and Multi-task-3. Formal: We'll use the Multi-task-1 dataset to test and evaluate the performance of our model. Formal: Formal: For the Multi-task-2 dataset, we're focusing on different aspects of captions. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "If there are no word guesses in a certain time period that score higher than a set limit, we add some edges labeled \"GAP\" with a score that matches that limit.",
        "formal_text": "If gaps in the utterance are recognized (i. e. there are no word hypotheses in a certain time interval with a score larger than a given threshold value), edges are introduced which are marked with the universal category GAP and a score feature which has the threshold as its value.",
        "GPT2_formal_text": "= {'.id', '.title', '.location', '.collapse', '.date', '.people', '.new', '.edit', '.delete', '.editText', '.editTextLink', '.editTextLinked', '.editTextLinked', '.editTextTextShort', '.editTextShortLinked', '.editTextLinked', '.editTextShortLinked', '.editTextShortLinked', '.editTextShortLinked', '.editTextLinked', '.editTextShortLinked', \".editTextShortLinked', \".editTextLinked\", '.editTextShortLinked', \".editTextLinked\", '.editTextLinked\", '.editTextLinked\", '.editTextLinkedLong', \".editTextLinkedLong\", \".editTextLinkedLong\", \".editTextLinkedLong\", \".editTextLinkedLong\", \".editTextLinkedLongLinked\", \".editTextLinkedLongLinked\", \".editTextLinkedLongLinkedLong\", \".editTextLinkedLongLinkedLong\", \".editTextLinkedLongLinkedLong\", \".editTextLinkedShortLinked\", \".editTextLinkedShortLinked\", \".editTextLinkedShortLinkedShort\", \".editTextLinkedShortLinkedShort\", \".editTextLinkedShortLinkedShort\", \".editTextLinkedShortLinkedShort\", \".editTextLinkedShortLinkedShortLinked\", \".editTextLinkedShortLinkedShort\", \".editTextLinkedShortLinkedShortLinked\", \".editTextLinkedShortLinkedShortLinked\", \".editTextLinkedShortLinkedShortLinked\", \".editTextLinkedShortLinkedShortLinked\", \".editTextLinkedShortLinkedShortLinked\", \".editTextLinkedShortLinkedShortLinkedLong\", \".editTextLinkedShortLinkedShortLinkedLongLinked\", \".editTextLinkedShortLinkedShortLinkedLong\", \".editTextLinkedShortLinkedShortLinkedLong\", \".editTextLinkedShortLinkedShortLinkedLongLinked\", \".editTextLinkedShortLinkedShortLinkedLongLinked\", \".editTextLinkedShortLinkedShortLinkedShort\", \".editTextLinkedShortLinkedShortLinkedShortLinked\", \".editTextLinkedShortLinkedShortLinkedLongLinked\", \".editTextLinkedShortLinkedShortLinkedLongLinked\", \".editTextLinked",
        "directory": "E83",
        "filename": "E83-1019.json"
    },
    {
        "casual_text": "Even with smaller families, we can't really see a big range of differences because there just isn't enough data. But they still show pretty much the same results.",
        "formal_text": "Even smaller families do not allow to vary la on such a large range due to data sparsity, but show similar results.",
        "GPT2_formal_text": "_layer is basically a combination of these two attention layers. Formal: Check out Figure 1. It shows how our model works. First, we get a bunch of input features (f i, h i ) by combining a BiLSTM (which is a fancy neural network) with a hidden layer. Then, we use the output from the BiLSTM to figure out the attention weights (a i, h i ). Formal: From the figure, we can see that the attention weights (a i, h i ) are learned based on the sentence x i. Formal: Lastly, we add the attention mask layer to make sure the words in the input don't overlap. Formal: It's worth mentioning that our model is super easy to understand and works really well. We made it super simple and even explain how to train it in sections 4.3 and 4.4. Formal: Formal: And that's it for now! Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.649.json"
    },
    {
        "casual_text": "If it's the first sentence in the text or if searching through all the active frames doesn't work, the theme gets linked to a \"base\" frame.",
        "formal_text": "If the sentence being processed is the first sentence in the text or if a search of all active frames fails, the theme is associated with a \"base\" frame.",
        "GPT2_formal_text": "= {'maj', 'sus', 'semi', 'pos', 'dex', 'num', 'foc', 'ass', 'tag', 'group', 'numgroup', 'focgroup', 'assgroup', 'len', 'numlen', 'numlengroup', 'posgroup', 'cl', 'type', 'numtype', 'foctype', 'asstype', 'lentype', 'numlentype', 'foctype', 'typegroup', 'numtypegroup', 'typegroup', 'postype', 'cltype', 'typegroup', 'numtypegroup', 'cltype', 'typegroup', 'asstype', 'lentype', 'numlentype', 'foctype', 'asstypegroup', 'lentype', 'numlengroup', 'foctype', 'groups', 'numgroups', 'assgroups', 'numgroups', 'lengroups', 'focgroups', 'numgroups', 'assgroups', 'lengroups', 'focgroups', 'type', 'numtype', 'foctype', 'asstype', 'lentype', 'numlentype', 'foctype', 'groups', 'assgroups', 'numgroups', 'sus', 'semi', 'pos', 'dex', 'num', 'foc', 'ass', 'tag', 'group', 'numgroup', 'focgroup', 'assgroup', 'len', 'numlengroup', 'focgroup', 'posgroup', 'cl', 'type', 'numtype', 'foctype', 'asstype', 'lentype', 'numlengroup', 'foctype', 'assgroup', 'lengroup', 'typegroup', 'postype', 'cltype', 'typegroup', 'numtypegroup', 'cltype', 'typegroup', 'asstype', 'lentype', 'numlengroup', 'focgroups', 'type', 'numgroups', 'assgroups', 'lengroups', 'sus', 'semi', 'pos', 'dex', 'num', 'foc', 'ass', 'tag', 'group', 'numgroup', 'focgroup', 'assgroup', 'len', 'numlengroup', 'focgroup', 'posgroup', 'cl', 'type', 'numtype', 'foctype', 'asstype', 'lentype', 'numlengroup', 'foc",
        "directory": "C82",
        "filename": "C82-1011.json"
    },
    {
        "casual_text": "We'll use these ranked argument candidates to train the ranking SVM (as introduced by Joachims in 2002) and figure out the model's parameters. When we're testing the model, we just pick the candidate it ranks highest as the final output. One thing to mention is that we don't need any eye gaze data during the testing phase.",
        "formal_text": "Using these rankings of argument candidates as training data for the ranking SVM (Joachims, 2002), we estimate the model parameter. In the test phase, the most highly ranked candidate by the model is considered as the model output. Note that we do not require any eye gaze information in the test phase.",
        "GPT2_formal_text": "ing_subtask, and the entity pair detection task using the entity pair embedding. Formal: We use a fancy setup with a shared encoder and a multi-layer self-attention module for the entity pair detection task. First, we turn each entity pair into an embedding using BERT. Then, we feed that into a bi-directional GRU layer to get a sequence of vectors. Finally, we feed that sequence back into the same GRU to get the hidden representation for the entity pair. This whole setup is like a multi-task learning setup. Formal: In the entity pair detection task, we use a Bi-GRU to get the entity pair embeddings. Formal: To avoid relying on specific entity pairs, we add a batch normalization layer on top of the Bi-GRU layer. This helps us deal with tricky cases like multi-class instances where the embeddings are really different. Formal: Let's look at Figure 2 again. The entity pair detection task is a mix of two tasks, entity pair detection and entity pair information extraction. There are two main entities involved: the two entities in question and their corresponding entity pairs. We use pre-trained entity pairs from CODESCRIBE (Nenkova et al., 2019) and BART (Lewis et al., 2020) to handle these pairs. Formal: We start with a pretrained BERT-Base model (from Devlin et al., 2019) to create the entity pairs, and we use the CNN with Bi-GRU (by Wang et al., 2020) for the sequence labeling part. Formal: Following what Shen et al. (2021) did, we split the dataset into two parts: one for entity pair identification and another for entity pair identification. We set the dropout probability to 0.4. Formal: To deal with entity pairs that are missing, we include the entity pair embeddings of the missing entity pairs in the training set. Formal: We use a GRU network (called GRU with Bi-GRU) to get the entity pair embeddings. Formal: Here, w is the softmax function. Formal: Here, f(w) is the dot product between the entity pairs and the embeddings. Formal: We use three CNNs to capture the entity pair embeddings: one for entity pair identification, one for entity pair detection, and one for entity pair information extraction. Formal",
        "directory": "C16",
        "filename": "C16-1269.json"
    },
    {
        "casual_text": "Sure, the cross-project approach makes sense for testing how well a model works on different projects. But when it comes to using it in batch mode across projects, it does rely on some pretty big assumptions—like there being no documentation at all for any of the methods in the projects you're looking at.",
        "formal_text": "While the cross-project methodology is reasonable for evaluating model generalizability, the cross-project batch-mode use case does make strong assumptions (e. g. , no documentation exists for any method in the targeted projects).",
        "GPT2_formal_text": "\": [1, 2, ..., n], where m is the length of the input sentences and n is the length of the outputs. The input sentence is a sequence of words with the same meaning, written in a certain order. Formal: Basically, we're saying that for any sentence, a knowledge base, and a translation, φ_i = (x, φ_j), if we know the type of translation φ_j, we can figure out the translation type φ_i from the input sentence. Formal: To make sure we're dealing with sentences of the right length, we use the exact same setup as before. Formal: Basically, the same rules apply here as before: Formal: The process we talked about earlier is just the main difference in how we handle knowledge base inference. Formal: The idea that knowing the type of translation φ_j can help us predict the type of knowledge base φ_i for a specific translation φ_j is kind of like the idea in the first-order logic world that we can figure out the type of knowledge base just by looking at the input sentence. Formal: We’ve come up with a new way to handle this problem that’s super efficient and works for any translation type φ_j. Formal: We use a technique called Hidden Markov Models (HMM) to handle this kind of inference. Formal: We don’t just rely on the input sentence itself. Instead, we use φ_j to represent the knowledge base from the input. Formal: We also use a linear transformation that includes a constant that depends on the type of knowledge base. Formal: The output of the linear transformation is a projection that shows how the input sentence and the knowledge base interact with each other. Formal: The new method we’ve developed makes it way more efficient and straightforward to handle this kind of inference. Formal: With these new rules, we can easily add new knowledge bases and translate them without needing to mess around with tricky pre-processing. Formal: We’ve also created a new linear transformation called h_x, which is based on the output of a HMM. Formal: We’ve come up with a way to map the output from a HMM to the output of the linear transformation, using something called an approximation. Formal: We tested this method on some big, multi-domain datasets, and the results are",
        "directory": "acl",
        "filename": "2022.acl-long.339.json"
    },
    {
        "casual_text": "In the paper, we noticed that depth doesn't do better than the RGB model for certain classes of objects. This happens when the depth map doesn't change much across those objects.",
        "formal_text": "In the paper, we made the observation that the classes for which depth fails to outperform the RGB model are the classes of object for which the depth map does not vary too much.",
        "GPT2_formal_text": "ed_tweets. We're using the model from Lample et al. (2016) as our main setup. We train it using the same validation set as the original BERT model. We also train a classifier using the Twitter data that we got from a separate development set. For the classification task, we use a linear classifier with a logistic prior. All the other models are trained on the same training set as the BERT model. Formal: So, BERT is the pre-trained BERT model, and our work is about how to adapt it for Twitter sentiment classification, as well as the related tasks, like detecting sarcasm or irony. Formal: The model we're proposing is a Bayesian neural network. It learns how to predict sentiment and provides a model to predict the target class, which we refer to as the target function. Formal: We use the target function to generate predictions for each word in the tweet. We make a prediction for every word in the tweet, whether it's positive, negative, or neutral. Formal: We then pick the top predictions from these top predictions and use them to label the target class. Formal: The modified BERT model we're proposing can create a sentiment-aware model that can recognize sarcasm and irony. It can also classify tweets that are sarcastic or ironic, which we refer to as the sarcastic or ironic label. Formal: Formal: BERT and the modified version of BERT are trained using the Twitter dataset, which we use as our validation set. Formal: We then train a classifier using the target function's output. We use these predictions to label the target class. Formal: Lastly, we make predictions for each word in the tweet, whether it's positive, negative, or neutral. Formal: To use this modified BERT model, we use a linear classifier with a logistic prior. Formal: Formal: The modified BERT model can also recognize sarcasm and irony. Formal: Formal: Lastly, we make predictions for each word in the tweet, whether it's positive, negative, or neutral. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.319.json"
    },
    {
        "casual_text": "We’re working on a basic version of local word discovery using an FST (Finite State Transducer) to connect known morphemes found in a noisy phone sequence to more complex word forms. Our method relies on two main assumptions: first, that the morphosyntactic description is detailed enough to be turned into an FST, and second, that we have a decent phone recognizer, which could be something simple, like training one on a few hours of transcribed audio from similar languages, or tweaking a bigger pretrained model. \n\nFor speech representation, we’re using Allosaurus to give us a low-dimensional way to represent speech, which helps with matching it to phone sequences predicted by our morphological transducer. Allosaurus comes with a pretrained model that lets us limit the output to a specific set of phones, which is pretty handy (Li et al., 2020). It supports phone inventories for over 2,000 languages out of the box, including Kunwinjku. However, we noticed that the default inventory for Kunwinjku was missing some stuff, so we made our own based on (Evans, 2003).",
        "formal_text": "Given the task definition, we implement a baseline version of local word discovery using an FST to map attested morphs embedded in a noisy phone sequence to new, morphologically complex word forms. Our approach makes two assumptions, namely that the morphosyntactic description is suf-ficiently explicit and complete that it can be represented as an FST, and that a modest phone recognizer is available, e. g. by training a recognizer on a few hours of transcribed audio from related languages, or fine-tuning a larger pretrained model. Speech representation. We adopt Allosaurus to provide a low-dimensional representation of speech which supports approximate matching against phone sequences predicted by the morphological transducer. Allosaurus provides a pretrained model which includes the ability to constrain the output vocabulary to a predefined set of phones (Li et al. , 2020). The inventories of over 2, 000 languages, including Kunwinjku, are supported in the default configuration. In practice, we found that the inventory for Kunwinjku was incomplete and we created our own, following (Evans, 2003).",
        "GPT2_formal_text": "= [c_i], i_l = 0.0, f_n, w_n, w_l, c_i, c_l, c_n }              Casual: Here, c_i and c_l represent the i-th and l-th token, respectively. Formal: This gives us the j-th LSTM layer. Formal: To make things easier, we'll just call the j-th layer \"j.\" Formal: For the first layer, we use a multi-head attention mechanism with a hidden dimension of d_s_1, which is the size of the first hidden layer. Formal: If the current sentence s_1 is a bunsetsu, we then use the softmax function to predict the word w_1 based on the token x_1. Formal: For the second layer, we also have a multi-head attention mechanism with a hidden dimension of d_s_2, which is the size of the second hidden layer. Formal: To calculate the phrase embeddings, we calculate the dot product between the two embeddings of each word w_i and the token x_i. Formal: We also use a Gibbs sampler for the word embeddings. Formal: To calculate the sentence embeddings, we take the logits from the two layers of the second LSTM layer and average them out. Formal: The sequence layer takes the last sentence and the token x_i as its input. Formal: Using this setup, the decoder decodes the sentences x_1 to x_n, with the word embeddings w_1 to w_n as the output. Formal: Lastly, we use the softmax function to predict the next word and token. Formal: To sum it up, our method follows the same process as the original LSTM decoder. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.157.json"
    },
    {
        "casual_text": "The results on the real test set are way worse than those on Ori. and Avg., showing that real user evaluations are way tougher. This is because real cases can have multiple robustness problems all at once, while each augmentation method in LAUG looks at them one by one. Even with this difference, the model's performance on real data gets a lot better after being fine-tuned on the augmented data, proving that LAUG really helps boost the model's real-world toughness. Table 13 checks which kind of errors the model made on the real test set by going through all the mistakes BERT Ori. made. \"Others\" are errors that aren't due to robustness issues, like when the model just doesn't perform well. It turns out the model struggles a lot with LU robustness (over 70%), and almost half the errors are because of different language varieties. This happens because real user evaluations have way more diverse ways of saying things than the original data. After training with the augmented data, we see fewer errors related to Speech Characteristics and Noise Perturbation, meaning BERT Aug. handles these issues better. Note that the total percentage of errors is over 100% because 25% of the cases have more than one robustness issue. This just shows again that real user evaluations are harder than the original test set.",
        "formal_text": "The performance on the real test set is substantially lower than that on Ori. and Avg. , indicating that real user evaluation is much more challenging. This is because multiple robustness issues may be included in one real case, while each augmentation method in LAUG evaluates them separately. Despite the difference, model performance on the real data is remarkably improved after every model is finetuned on the augmented data, verifying that LAUG effectively enhances the model's real-world robustness. Table 13 investigates which error type the model has made on the real test set by manually checking all the error outputs of BERT Ori. \"Others\" are the error cases which are not caused by robustness issues, for example, because of the model's poor performance. It can be observed that the model seriously suffers to LU robustness (over 70%), and that almost half of the error is due to Language Variety. We find that this is because there are more diverse expressions in real user evaluation than in the original data. After augmented training, we can observe that the number of error cases of Speech Characteristics and Noise Perturbation is relatively decreased. This shows that BERT Aug. can solve these two kinds of problems better. Noting that the sum of four percentages is over 100% since 25% error cases involve multiple robustness issues. This again demonstrates that real user evaluation is more challenging than the original test set 9.",
        "GPT2_formal_text": "ing_dim, word2vec, and self-attention. Formal: Our model has three main parts: the encoder, a decoder, and a graph that shows how the input and output are connected. Check out Figure 1 for an example of how this works. Formal: The input is just a sequence of word embeddings. Formal: The decoder works by adding a loss function that changes how the input is represented. Formal: The graph G is built by combining the output of the decoder with a graph G t, which includes the input and the output vectors. Formal: The final graph G t combines the representations of both the input and output. Formal: The model uses a dynamic schedule to learn how to combine the input and output representations. Formal: The loss function α is a function that changes based on how the input and output are represented. Formal: The pre-trained model G t gets better by combining the outputs from the decoder with the input. Formal: The graph G t is generated using a sequential sequence of operations. Formal: The loss function for this graph G t is adjusted based on how the input and output are represented. Formal: The final graph G t combines the outputs from the decoder with the input representations. Formal: We’ve tested our model on two popular datasets: NER and POS tagging. We also built a third dataset to compare with other text representations. Formal: Our model performs better than the current top text representations, which shows how important it is to use text representations. Formal: Our model also beats some strong baselines for text representations. Formal: We’re planning to share the code for training and decoding our model on the CoNLL-2012 dataset. Formal: We’ve also created a third dataset, CoNLL-XSUM, which is a large-scale parallel corpus that includes a lot of text from a bunch of different topics. Formal: Formal: Formal: We’re using a setup with three types of pooling operations: dot, kernel, and mean pooling. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.192.json"
    },
    {
        "casual_text": "The idea behind word sense discrimination is to group the different ways a word is used in sentences based on its actual meaning. This is usually tackled as an unsupervised learning problem, where all you have is a big pile of text (like in studies by Pedersen and Bruce, 1997, Schütze, 1998, and Purandare and Pedersen, 2004). These methods typically need you to know beforehand how many groups (or clusters) you're looking for, which is often labeled as \"k.\" But in real-world situations, you usually don't know what k should be.",
        "formal_text": "The goal of word sense discrimination is to cluster the occurrences of a word in context based on its underlying meaning. This is often approached as a problem in unsupervised learning, where the only information available is a large corpus of text (e. g. , (Pedersen and Bruce, 1997), (Schütze, 1998), (Purandare and Pedersen, 2004) ). These methods usually require that the number of clusters to be discovered (k) be specified ahead of time. However, in most realistic settings, the value of k is unknown to the user.",
        "GPT2_formal_text": "is used to find the most relevant Wikipedia page, and the total number of sentences in the document collection is the total number of sentences in the article collection. Formal: In the word-based model, the entity feature vector is created by combining the entity embedding vectors (e_e) with the word embedding vectors (w_w). The probabilities are then calculated using the formula: Formal: In the entity-based model, the entity feature vector is created by combining the entity embedding vectors (e_e) with the entity embedding vectors (w_e). The probabilities are then calculated using the formula: Formal: The entity embedding vectors are combined using an entity embedding transformation matrix (e_e). The entity embedding transformation matrix is made up of a 1-dimensional vector of entity embeddings, e_e, and a size m_e, which is a matrix of sentence embeddings, w_s, that are all in R^d. Formal: Also, the entity embedding transformation matrix e_e is a matrix of matrix product vectors, e_e, that are in R^n. Formal: The entity embedding transformation matrix e_e is a matrix of matrix product vectors, e_e, that are in R^n. Formal: In the word-based model, the entity feature vector is created by combining the entity embedding vectors (e_e) with the word embedding vectors (w_w). The probabilities are then calculated using the formula: Formal: Formal: You can find more details about the individual models in Section 2. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E06",
        "filename": "E06-2007.json"
    },
    {
        "casual_text": "So, there's this issue with \"unknown words\"—basically, words that weren't in the training data—which is a big deal in part-of-speech tagging. In our case, it's even trickier because some words that actually are in the training data might still be treated as unknown due to weird spelling variations. In the experiments we're talking about, we let unknown words be any part of speech (which makes sense in this context), but we give different parts of speech different levels of importance. Specifically, if a word isn't in our word list, we tweak the calculation in equation (2) (check out equation 1 above for reference).",
        "formal_text": "The occurrence of \"unknown words\", i. e. , words not occurring in the training corpus, is a notorious problem in (probabilistic) part-of-speech tagging. In our case, this problem is even more serious, since we know beforehand that some words will be treated as unknown although they do in fact occur in the training corpus (because of deviations Dom standard orthography). In the experiments reported below, we have allowed unknown words to belong to any part-of-speech (which is possible in the given context), but with different weightings for different parts-of-speech. More precisely, when a word cannot be found in the lexicon, we replace the product in (2) (cf. equation 1 above)",
        "GPT2_formal_text": ". This setup helps the model figure out the right answer, even if the question is tricky. Figure 1 shows how the setup works for the example query, q = {q 1, . . . , q m }. Formal: For each pair of entities (e i, e j ), we calculate the attention value between them. This value is figured out using the embedding function h(e j ) and is normalized by its mean (which is 0). Formal: We look for the pair (e i, e j ) where h(e j ) is greater than or equal to h(e i ), or where h(e i ) is below the mean of h(e j ). Formal: For instance, if we find that h(e j ) is greater than h(e i ), we add h(e j ) to h(e i ) and h(e j ) to h(e j ). Formal: This gives us the first part of the pair, which is h(e j ) + h(e i ). Formal: The second part is just the sum of h(e j ) for all e j (from 1 to m). Formal: The final result is the final attention value, which is 0. Formal: The example query and the answers are in bold. The input values are: Formal: h(e j ) = e j + h(e i ). Formal: h(e j ) = (e j + h(e i )) Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C96",
        "filename": "C96-2192.json"
    },
    {
        "casual_text": "I'm going to expand on some ideas from recent years about underspecified semantic formalisms, like the ones mentioned by Reyle in 1995. The goal is to create an underspecified representation for discourse structure. To do this, I'll use a first-order tree logic developed by Kallmeyer in 1996 to define an underspecified version of SDRT. I'll explain this in more detail in the next sections.",
        "formal_text": "I will develop further ideas introduced by underspecified semantic formalisms which have been proposed in recent years (e. g. (Reyle, 1995) ) in order to provide an underspecified representation for discourse structure. I will employ a first order tree logic by Kallmeyer (1996) to define an underspecifled SDRT, in the following sections.",
        "GPT2_formal_text": "_t; h = argmax(concatenate(concatenation(concatenation(previous_id))), NULL), NULL); } return s; }\n          Casual: (3) For the final step, we combine the generation results from the different models using a weighted average. Formal: We’re gonna look into how to generate text based on certain topics. After we’ve got a bunch of training data, we’ll use a method called 'topic-specific' generative adversarial networks (GAT) to create text that’s tailored to a specific topic. Formal: We’re using the topic-specific GAT to generate text. The idea is to find a group of points that match up with each topic and generate text that’s very similar to that topic. Formal: The topic-specific GAT takes a bunch of data and creates text based on that topic. Formal: We’ll use the topic-specific GAT to generate text. Formal: \n\nFigure 2: A cool example of how GAT generates text on a specific topic, using a Gibbs sampling method. Formal: We’ve got a bunch of data from the training set. First, we figure out the topic distribution p for a random sample from that dataset. Then, using this distribution, we generate text using a Gibbs sampling method, as shown in Fig. 2. Formal: We’ve got a bunch of data from the training set. First, we figure out the topic distribution p for a random sample from that dataset. Then, using this distribution, we generate text using a Gibbs sampling method, as shown in Fig. 2. Formal: Formal: Formal: We’ll use the topic-specific GAT to generate text. Formal: Formal: Formal: We’ll use the topic-specific GAT to generate text. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C98",
        "filename": "C98-2189.json"
    },
    {
        "casual_text": "This method ensures that the translation system always hits the exact length requirement (which is a strict rule), but it comes with a big downside. The system doesn't know how many more words it can add until it gets to the required length. So, it can't adjust by making the start of the sentence shorter to meet the limit.",
        "formal_text": "While this approach will guarantee that the output of the translation systems always meets the length condition (hard constraint), it also has one major drawback. Until the system reaches the constrained length, the system is not aware of how many words it is still allowed to generate. Therefore, it is not able to shorten the beginning of the sentence in order to fulfil the length constraint.",
        "GPT2_formal_text": ", which is basically the average of the hidden states from the last two hidden layers. Formal: • Batch size: The number of times the model predicts words, starting from the most recent word and counting up to the next word. Formal: The total number of times the model predicts words, including the last word. Formal: The total number of times the model predicts words, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word. Formal: The model's prediction, including the last word",
        "directory": "amta",
        "filename": "2020.amta-research.3.json"
    },
    {
        "casual_text": "We set up the generation process as conditional text generation and fine-tune GPT-2 (from Radford et al., 2019) to create a generator called Polyjuice, using pairs of (x, x). To make it easier to focus on specific counterfactuals, we also add control codes, like negation or delete (check out Figure 1B), and use fill-in-the-blank structures (inspired by Donahue et al., 2020) to clearly mark where the changes happen and what kind of changes to make. Our internal evaluation shows that Polyjuice creates outputs that are fluent, varied, and pretty close to the original x. Plus, the control mechanisms help it generate changes that you probably wouldn’t get from standard language models.",
        "formal_text": "We frame the generation step as conditional text generation, and finetune GPT-2 (Radford et al. , 2019) into a generator called Polyjuice using (x, x) pairs. To allow for targeted counterfactuals, we also design control codes like negation or delete ( Figure 1B), and adopt fill-in-the-blank structures (Donahue et al. , 2020) to specify where the perturbation occurs and how. Intrinsic evaluation shows that Polyjuice generatesx that are fluent, diverse, and close to x, and that the control mechanisms retrieve perturbations that would likely not be sampled from off-the-shelf language models.",
        "GPT2_formal_text": "= [r1, r2, ..., rm]. Then, we pick the text rT from the set T_text (which has R = 2m) based on that. Formal: A tool that uses something called TextRank, which uses something called topic modelling, to figure out the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A tool that uses TextRank to find the topics in a document. Formal: A",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "We tested and looked at the proposed model using the monotonicity subset from Semantic Fragments (Richardson et al., 2020), as well as HELP (Yanaka et al., 2019b) and MED (Yanaka et al., 2019a). We also expanded MED to create a dataset for testing 2-hop inference. The model can successfully learn natural logic operations through end-to-end training. \n\n1 2 Related Work",
        "formal_text": "We evaluate and analyze the proposed model on the monotonicity subset of Semantic Fragments (Richardson et al. , 2020), HELP (Yanaka et al. , 2019b) and MED (Yanaka et al. , 2019a). We also extend MED to generate a dataset to help evaluate 2-hop inference. The model can effectively learn natural logic operations in the end-to-end training paradigm. 1 2 Related Work",
        "GPT2_formal_text": "ing_comparison: Basically, the model learns to match the input prompt to the correct answer span, which then helps predict the target prompt. Formal: The tagger works with the example input string x i. The input for the prompt generation is a list of tokens {h i, s i } i=1 to n. Formal: We'll look at different ways to use these inputs during the training process. Formal: We start by generating a token sequence y i. This sequence has a length of n + 1 tokens. Formal: We use the token sequence y i as the input for the tagger, and the target prompt for the model to generate. The model figures out the correct prompt for the input by looking at the token sequence x i. Formal: For the pre-training stage, we start by training the model on the target prompt x i, and then we take the output from this pre-trained model for testing. Formal: We use the target prompt x i as the input for the pre-trained model, and the input for the model to generate. Formal: We also have a loss function L, which is a mix of the objective function L(0) and the softmax function. Formal: We set the cross-entropy loss to 0.1 and try to minimize the cross-entropy loss for this pre-training phase. Formal: The whole thing is laid out in Figure 1. Formal: We set the model to generate a sequence of tokens x i, which has a length of n + 1 tokens. Formal: The model generates a token sequence y i. Formal: We use this token sequence x i as the input for the tagger, and the target prompt for the model to generate. Formal: We use the token sequence y i as the input for the pre-trained model, and the input for the model to generate. Formal: Formal: For training the model, we train it on the target prompt x i, and then we take the output from this pre-trained model for testing. Formal: The whole thing is laid out in Figure 2. Formal: We set the model to generate a sequence of tokens, which has a length of n + 1 tokens. Formal: The model generates a token sequence y i. Formal: The model generates a token sequence y i. Formal: We use this token sequence x i as the input for the tagger, and the input",
        "directory": "coling",
        "filename": "2020.coling-main.101.json"
    },
    {
        "casual_text": "Our method works really well on standard evaluation metrics. It consistently beats the baseline models, especially on SPICE and CIDEr. For example, our model boosts the AoANet baseline from 118.4 to 119.1 on CIDEr and from 21.5 to 21.7 on SPICE during the MLE phase. It also improves the ATTN baseline on CIDEr from 117.4 to 120.1 and on SPICE from 20.5 to 21.0 during the RL phase. Since CIDEr uses tf-idf weighting, it helps to highlight methods that generate more specific details about images that aren’t common across the dataset. Our method is designed to encourage models to create sentences with more objects, attributes, or relationships, which is why we also see improvements on SPICE.\n\nWhen it comes to descriptiveness-related metrics, our method consistently performs better on R@1 and R@5 in both the MLE and RL phases. The model also improves retrieval performance on the DISC baseline, boosting R@1 from 46.5 to 48.1 and R@5 from 83.6 to 87.9. Our weighted CIDEr reward works well alongside the discriminative loss in DISC, further improving retrieval performance.\n\nWe also used an externally trained NLI model (from Section 3.1) to analyze the relationships between captions generated by our method and the baselines (AoA and DISC) on the test set. Figure 2 shows that our model generates more descriptive sentences.",
        "formal_text": "Results on conventional metrics. Our method consistently outperforms the baseline models on most conventional metrics, especially SPICE and CIDEr; e. g. , the proposed model improves the AoANet baseline from 118. 4 to 119. 1 on CIDEr, 21. 5 to 21. 7 on SPICE in the MLE phase, and improves the ATTN baseline on CIDEr from 117. 4 to 120. 1, SPICE from 20. 5 to 21. 0 in the RL phase. As CIDEr is based on tf-idf weighting, it helps to differentiate methods that generate more imagespecific details that are less commonly occur across the dataset. As our method is designed to encourage models to generate sentences with more objects, attributes, or relations, the effect was also suggested by the improvement on SPICE. Performance on descriptiveness related metrics. Our methods achieve consistently better results on R@1 and R@5 in both the MLE and RL optimization phases. Note that the proposed model can further boost the retrieval performance on the discriminativity enhanced baseline (DISC), improving R@1 from 46. 5 to 48. 1 and R@5 from 83. 6 to 87. 9. Our weighted CIDEr reward is complementary to the discriminative loss item in DISC and further boost the retrieval performance. Labels between generated sentences. We use the externally trained NLI model (Section 3. 1) to further investigate the NLI relationships between the captions generated by our method and by the baselines (AoA and DISC) on the testset. Figure 2 shows that our model generates more descriptive sentences.",
        "GPT2_formal_text": ", and the max_links parameters. Formal: Okay, so here's what's going on:\n\nWe're looking at the Q-Rankings dataset, which has a bunch of questions (Q, R) that each have one or more answers (A, R). Each answer is labeled as a s i, j.\n\nNow, we're also looking at the Wikipedia dataset, which has articles (wiki, article) that are all about the same thing (D, A). Each article is labeled as a s i, j.\n\nLastly, we're looking at the candidate-based data, which is made up of a bunch of sentences (s i, j) that don't have answers. Each sentence is labeled as a s i, j.\n\nSo, in short, we're dealing with a bunch of questions, articles, and candidate sentences. Formal: Each question, article, or candidate sentence has a label, s i, j, and a structure that's based on the size of s i, j. Formal: So, Q and R are questions and R is the set of answers, with s i, j being the starting and ending points. Formal: We're focusing on how the questions, articles, and candidates are structured to help us rank questions or answers. Formal: Each question, article, or candidate sentence has a label, s i, j, and a structure that's based on the size of s i, j. Formal: This way, we can use information from the article and the answers to help rank the questions or answers. Formal: In this paper, we're mainly looking at how the questions and answers are structured to help us rank questions or answers. Formal: The main thing we're doing is ranking the questions or answers, so we can figure out the best answers for them. Formal: The rest of this paper is all about how to find the best answers for questions or answers. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-short.36.json"
    },
    {
        "casual_text": "Metrology is like a big umbrella for making sure measurements are consistent across all kinds of science, so we can compare things properly. Computer science has taken some ideas from metrology, especially the concept of reproducibility, but it doesn’t always use the same definitions (as we talked about in Section 2). Here, we’re going to explain quantified reproducibility assessment (QRA), which is basically a method that takes the ideas and definitions from metrology and uses them exactly as they are. This helps us figure out how similar numerical results are and how similar the studies that produced them are. First, we’ll go over the concepts and definitions that QRA is built on, then we’ll give you an idea of the framework (in Section 3.2) and how to actually use it in real life (in Section 3.3).",
        "formal_text": "Metrology is a meta-science: its subject is the standardisation of measurements across all of science to ensure comparability. Computer science has long borrowed terms, most notably reproducibility, from metrology, albeit not adopting the same definitions (as discussed in Section 2 above). In this section, we describe quantified reproducibility assessment (QRA), an approach that is directly derived from the concepts and definitions of metrology, adopting the latter exactly as they are, and yields assessments of the degree of similarity between numerical results and between the studies that produced them. We start below with the concepts and definitions that QRA is based on, followed by an overview of the framework (Section 3. 2) and steps in applying it in practice (Section 3. 3).",
        "GPT2_formal_text": "ing_len is calculated as the sum of the attention weight for each token's representation, plus the attention weight for the surrounding tokens. This is done using the unsupervised relation extraction method from Section 5. Formal: You can find more info about the training data in the original paper. Formal: The performance stats for SQuAD 2.0, including the sample ranking, are shown in Table 2. Formal: We ran each model 10 times for 10 rounds with a random setup, and the average precision was 0.9. Formal: The results for unsupervised relation extraction are in Table 3, and the sample ranking is in Table 4. Formal: The sample ranking (the top left table) has the performance stats for the two previous models, SQuAD 1.1 and SQuAD 2.0. Formal: Here's the sample ranking for the model SQuAD 2.0, comparing it to SQuAD 1.1. Formal: Here's the sample ranking for the model SQuAD 2.0, comparing it to SQuAD 1.1. Formal: The sample ranking for the model SQuAD 1.1, comparing it to SQuAD 2.0. Formal: The results for the model SQuAD 2.0, comparing it to SQuAD 1.1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.2.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way.\n\nWe're dealing with simple sentences, where each sentence has a bunch of words (called \"case fillers\") followed by a verb. Our job is to figure out the meaning, or \"sense,\" of each verb in the sentence. To do this, we use a list of verb senses from a dictionary called \"IPAL\" (from 1987), which also includes examples of these case fillers.\n\nWe also use a method by Kurohashi to measure how similar two case fillers are, or more specifically, how similar the main nouns in those case fillers are. The similarity is based on something called the \"length of the path\" between two nouns in a structure called \"HPSG\" (don't worry too much about that). This similarity is calculated using a formula, and we use a tool called \"Hyokiyo\" (from the National Language Research Institute, 1964) to help with this.\n\nFollowing Kurohashi's method, we define a similarity score between two words, X and Y, as shown in Table 1. It's important to note that this method works independently of the resources we use.\n\nTo show how the whole process works, we'll use some examples mentioned earlier and compare them to a more general case, as shown in Figure 4. The input we're working with is a set of case fillers and a verb, like {nc, 'mc), nc: 'm. ce, v}. The possible meanings (or \"senses\") for the verb v are s1, s2, and s3, and we get these from a database.",
        "formal_text": "We assume that inputs ~re simple sentences, e~mh one of which consists of a sequellce of eases fl)llowe. d by their governing verb. The. task is to identify the sense of each input verb. The set of verl) senses we use are those defined in the existing machine re~tdal)le (li(: ti()llary \"IPAL\" (IPA, 1987), which also (: olltains example case fillers as shown in figure. t. As well as Kuroh~tshi's method the similarity between two (: as(; tillers, or more pre-('isely the semantic-head nouns of them, is corn- 1: The relation I)t'. tweell the length of path I)e-|; ween two i[()llns A\" {Mid Y (lt: 7/, (. k', }: )) ill IJtL: l~r, Lil: o'ibye and the similarity hetween them (. sirn(X, Y)) [~a. n(X, Y) l 0, . . : 2 9468 l012 t [ s. zm(A, ~ ) tl 10 8 7 5 0 tinted by using IIv, rwuigoih, yo (National-Language l{esearch lnstil; ute, 1964). Following Kurohashi's method, we define. sim(X, ~), whi(: h stands for the silnilarity 1)etween words X mM Y, as in tattle 1. It should he noted here that both nl(~t; h()ds ~tre theoreti(: ally indel)endent of wh; tt resources }ire use(t. ~lb illustl'~te tit(; overall a. lgorithm, we r(~t)la. (: (~ the illustra. tive cases mentioned in section 1 wilh a slightly re(ire gelmral case as in figure. 4. The iut)ut is {nc, -'mc), nc: 'm. ce, v}, where he. i all! notes the case filler in the case ci, a. nd 'ntc~ denotes the case maker of <: i. The candidates of ilH; (~rl)ret; ttion for v, which ~re, sl, , s2 ~md s3, are deriv(; d froln the datal)ase. The.",
        "GPT2_formal_text": "\", \"query_style\", \"query_parameters\", \"query_string\", \"query_value\", \"query_type\", \"query_value_type\", \"query_value\", \"query_object\", \"query_object_type\", and \"query_object_value\". Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three conditions. Formal: We only consider queries that meet these three",
        "directory": "C96",
        "filename": "C96-1012.json"
    },
    {
        "casual_text": "So far, we've been talking about semantic spaces that are built using how often words appear together. But, the way words are spread out across different documents can also give us some pretty useful information about their meanings. Latent Semantic Analysis (LSA), which was introduced by Landauer and Dumais in 1997, uses exactly this kind of distribution information to figure out hidden semantic factors by simplifying the data (a process called dimensionality reduction).\n\nTo do this, LSA uses a method called Singular Value Decomposition (SVD), developed by Berry and his team in 1994. SVD takes a big table (matrix) that shows how often words and documents appear together and breaks it down into smaller, simpler pieces. One of these pieces shows words in terms of these hidden factors, and another shows documents in the same way.\n\nThis setup allows us to see that any document can be thought of as a mix (linear combination) of the words it contains. So, in this way of looking at things, it makes sense to treat groups of words as if they were a mini-document and represent them by mixing the vectors of the individual words.",
        "formal_text": "The semantic space discussed thus far is based on word co-occurrence statistics. However, the statistics of how words are distributed across the documents also carry useful semantic information. Latent Semantic Analysis (LSA, Landauer and Dumais (1997) utilizes precisely this distributional information to uncover hidden semantic factors by means of dimensionality reduction. Singular value decomposition (SVD, Berry et al. (1994) ) is applied to a word-document co-occurrence matrix which is factored into a product of a number of other matrices; one of them represents words in terms of the semantic factors and another represents documents in terms of the same factors. The algebraic relation between these matrices can be used to show that any document vector is a linear combination of the vectors representing the words it contains. Thus, within this paradigm it is nat-ural to treat multi-word structures as a \"pseudodocument\" and represent them via linear combinations of word vectors.",
        "GPT2_formal_text": "ing (or just attention for short) adds a soft attention layer after each token. This layer figures out the attention value for the token based on the previous token's representation. The token's representation is made up of: Formal: The attention network calculates the attention value for each token based on the context. Formal: The final attention network also calculates the attention value for each token. Formal: So, the final attention network is just a mix of these three parts. Formal: But, the attention is only calculated for the tokens right before the current one. Formal: For example, the first token can be treated as a special token, while the last token will be ignored. Formal: You can see this for example in Figure 2. Formal: So, attention can also be thought of as a way to help the model pay attention to the important bits of information in the context. Formal: There's more to it—this attention helps the model focus on the most important tokens. Formal: But, this attention only works for the token that's before the current one. Formal: Formal: Formal: So, the model can still pick up on useful information from other tokens that don't directly affect its current state. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D09",
        "filename": "D09-1045.json"
    },
    {
        "casual_text": "We talked about how MT research is different from other frameworks in NLU, and we pointed out that one unique thing about MT as an NLP application is that we can't easily define a specific task-oriented level of 'understanding' like we can with other applications.",
        "formal_text": "we have discussed what makes MT researches different from other frameworks in NLU, and we have stressed that one of the peculiarities of MT as an NLP application is that we cannot readily set up a particular task-oriented level of 'understanding' in MT as we can in other applications.",
        "GPT2_formal_text": ", which have the same length as the output. Formal: For the BT method, the representation gets updated by putting together the representations from the start of the text and the end, taking into account the position in the text. Formal: For the GCN model, the representation gets updated by combining the representations from the start of the text and the current word in the document, using position-wise attention. Formal: This approach is kind of like what Malouf and his team did in 2018. Formal: You can find more info and the code for the models we talked about in Section 4.2. Formal: Formal: The last model, the GCN+BT model, doesn’t have any special parameters or rules for calculating the output length. Formal: The output length is determined by the number of words in the summary. Formal: We used the same setup as other studies on summarization, which we’ll explain next. Formal: In our setup, the keyphrases for each piece of text are represented by a vertex that’s connected to the keyphrases of the document. This setup gives us two vectors: one for the keyphrases and another for the document. Formal: This gives us two vectors: one for the keyphrases and another for the document. Formal: We got the model’s dimensions from a pre-trained GPT-2 model. Formal: We trained the model for 5 epochs on the CNN/Daily Mail dataset. Formal: The training process includes two steps: training on the input data and then fine-tuning the model on the target dataset, using a special hyperparameter α. Formal: Formal: The model gets trained using MERT (from Devlin et al., 2019). Formal: Formal: The training process includes two steps: training on the input data and then fine-tuning the model on the target dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C86",
        "filename": "C86-1155.json"
    },
    {
        "casual_text": "To check how accurate ACT is when using word alignment, we tested it on a new batch of 200 sentences from the UN EN/FR corpus (not the same ones as before), which had 207 instances of seven different discourse connectives. Just like we did for the first version of ACT (before we added the disambiguation module), we looked at each of the six cases and counted how many times the scoring was right or wrong. The results were: case 1: 64 correct, 0 wrong; case 2: 64 correct, 3 wrong; case 3: 33 correct, 4 wrong; case 4: 1 correct, 0 wrong; and case 6: 0 correct, 0 wrong. Out of the 38 sentences in case 5, 21 were actually correct translations. So, the ACTa score was about 10% off in the first version of ACT, but now it's pretty much spot on. ACTa5 and ACTm were both about 2% off before and are now only 0.5% off. This shows that word alignment makes the ACT metric more accurate.",
        "formal_text": "In order to estimate the accuracy of ACT by using word alignment, we manually evaluated it on a new subset of 200 sentences taken from the UN EN/FR corpus (different from the first one), with 207 occurrences of the seven discourse connectives. As done for the first version (before adding the disambiguation module) of ACT, we counted for each of the six cases the number of occurrences that have been correctly vs. incorrectly scored. The results were, for case 1: 64/0, case 2: 64/3, case 3: 33/4, case 4: 1/0, and for case 6: 0/0. Among the 38 sentences in case 5, 21 were in fact correct translations. Therefore, the ACTa score was about 10% lower than reality in the initial version of ACT and now is approximately the same, while ACTa5 and ACTm were both about 2% lower and now is 0. 5%. Word alignment thus improves the accuracy of the ACT metric.",
        "GPT2_formal_text": "_1d = d_mask * layer_size; for the other two layers, we just stick the output from layer_1 into the vector h_1. Formal: We’ve shown the results for the Bi-LSTM-CRF model in Figure 1. The dotted line represents the average performance of the original model. As you can see, the updated model does better than the original, with improvements of 0.7% and 1.2% respectively. Formal: For the Bi-LSTM-CRF model, we’ve included the best F1-score we got on the test set. Formal: We’re using the modified Bi-LSTM model (check out Figure 1) as an example to show that adding attention helps improve performance. Formal: Formal: We’ve also included the accuracy of the generated text, with the dashed lines representing the accuracy of the original model. Formal: Lastly, we’ve included the error rates for the original model on the test set. Formal: We’ve included the accuracy of the generated text, with the dashed lines representing the accuracy of the original model. Formal: The modified Bi-LSTM model (Figure 1) can handle two kinds of input: text and images. To make it work with images, we added a recurrent layer to the input that can generate text or images. Formal: We’re also using the original Bi-LSTM model to compare it with the attention-based model (Figure 1). Formal: We’re using the modified Bi-LSTM model (Figure 1) to compare it with the attention-based model (Figure 1). Formal: We’re using the original Bi-LSTM model (Figure 1) to compare it with the attention-based model (Figure 1). Formal: The original Bi-LSTM model (Figure 1) is a supervised model, so we’re looking at its performance on the test set to see how it compares to the attention-based model. Formal: Formal: We’ve included the accuracy of the generated text, with the dashed lines representing the accuracy of the original model. Formal: We’re also including the error rates for the original model on the test set. Formal: We’re using the modified Bi-LSTM model (Figure 1) to compare it with the attention",
        "directory": "amta",
        "filename": "2012.amta-caas14.1.json"
    },
    {
        "casual_text": "vp(X, P) means that for a verb phrase with position X and predicate P, it can either be broken down into a transitive event (tranev) with positions X and Y and a plural form Pl, followed by a noun phrase (np) with position Y, plural Pl, and predicate P. Or, it can be a transitive non-core argument verb phrase (tncransv) with position X and predicate P. Figure 2 shows an example sentence in the language this grammar recognizes, along with its surface structure and the semantic structure created by the grammar.",
        "formal_text": "vp(X, P) -> tranev(X, Y, Pl), np(Y, Pl, P). vp(X, P) -> tncransv(X, P). Figure 2 gives a sentence in the language recognized by thls grammar together wlth the associated surface syntactic structure and the semantic structure built by the grammar.",
        "GPT2_formal_text": "= [u] for i in range(len(X_T)): X_t = X_T[u] for j in range(len(X_T)): X_t = X_T[j] if y_t > y_u and y_u > y_t: return 1 else: return 0 return (1 + exp(sum from j=1 to n of X_T[i] for j in range(len(X_T) - 1) + 1)) / (2 * exp(sum from j=1 to n of X_T[i] for j in range(len(X_T) - 1) + 2)) else: return 0 return 0 return (sum from j=1 to n of X_T[i] for j in range(len(X_T) - 1) + 1) / (2 * exp(sum from j=1 to n of X_T[i] for j in range(len(X_T) - 1) + 2)) Formal: The interaction between T and Q(X) involves Q(X) being equal to T plus Q(X). Formal: The interaction between T and Q(X) is also equal to T plus Q(X). Formal: Also, the interaction between T and Q(X) is equal to T plus Q(X). Formal: Formal: T is a set of types, where each type has a set of parameters. Formal: T(X) is a set of types, where each type has a set of parameters. Formal: A type is just a group of things. Formal: The parameters for a type are the groups of things that belong to that type. Formal: The parameters for a type are the groups of things that belong to that type. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "A83",
        "filename": "A83-1010.json"
    },
    {
        "casual_text": "When you map an NDA onto this network, pairs of NDA states (q) and input symbols (x) where δ(q, x) ≠ ∅ get turned into activity patterns. The network's temporal relationships then handle the NDA transitions. Each NDA transition corresponds to a single network transition. This setup creates complex representations of both NDA states and input symbols. \n\nAn NDA state is represented by all the activity patterns that include that state, and input patterns are represented by combining (OR-ing) all the activity patterns that contain that input symbol. One cool thing about this network is that it naturally handles mixed temporal images, which are kind of like the subthreshold version of mixture states. The temporal image of an active overlap includes at least all the activity patterns representing the next state.\n\nBut that's not all. The network also acts like it's running the deterministic version of the NDA, meaning it explores all possible paths through the state space that the input allows, all at once. The representations of the states in this deterministic finite-state automaton (FSA) are built dynamically as the network runs. These are called mixed temporal images.\n\nThe idea of \"dynamically constructed representations\" comes from Touretzky [9], who actually argued that they couldn't exist in today's neural networks, like Hopfield models.",
        "formal_text": "When an NDA is mapped onto this network, pairs of NDA-state q and input-symbol x, such that 6(q, x) y£ {~, are mapped onto activity patterns. Temporal relations in the network then serve to implement NDA transitions. Note that single NDA transitions arc mapped onto single network transitions. This results in complex representations of the NDA states and the input symbols. An NDA state is rap resented by all activity patterns that represent a pair containing that state, and input patterns are represented by a component-wise OR over all activity patterns containing that input symbol. A consequence is that mixed temporal images, the subthreshold analogue of mixture states, are a very natural phenomenon in this network, because tile temporal image of an active overlap comprises at least all activity patterns representing a successor state. But this is not all. Also the network will act as if it implements the deterministic equivalent of the NDA, i. e. it will trace all paths through state space the input allows for, concurrently. The representations of the states of this deterministic finite-state automaton (FSA) are dynamically constructed along the way; they are mixed temporal images. The concept of a \"dynamically constructed representation\" is borrowed from Touretzky [9], who, by the way, argued that they could not exist in the current generation of neural networks, such ms ltopfield models.",
        "GPT2_formal_text": "* g_mask) = N_mask(g_mask) * h_mask.\n         Casual: We can get g_mask by using an adjacency matrix with n columns and d rows, which we call g_mask. Formal: For a word x_i, we create a graph using its word embedding g_i and its embedding from the first hidden layer of the Transformer. We then add two types of features: Formal: For each word x_i, we grab its embedding (ε_i) and its second hidden state (ε_2). Formal: For each word x_i, we add its second hidden state (ε_2) and its word embedding (ε_1). We use a graph to represent these words and their embeddings. Formal: To make sure we don’t lose any context info, we train the whole model using the full-sentence MT data. Formal: Then, we combine the outputs of the first three hidden layers of the Transformer (β_1, β_2, β_3) using the Transformer objective to get the final hidden state representation. Formal: Finally, we use the gating system to predict the masked word in the target language. Formal: When we’re testing, we evaluate how well we’re doing by calculating the average precision (AP) and recall (R), which are the average accuracy and recall. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C92",
        "filename": "C92-1021.json"
    },
    {
        "casual_text": "On the flip side, the backward mode kicks in when someone asks the system a question. Usually, for the sake of efficiency, it’s better not to dump all the inferred info into the knowledge base right away. S-rules in backward mode only activate when a question comes up that can’t be answered straight off. They then start digging through the knowledge base to see if they can figure out the answer based on what’s already there. For instance, the Present S-rule in backward mode doesn’t trigger when sentence (27) is processed and T-expression (28) is generated by START. It only triggers when question (29) is asked, because that’s something the system can’t answer directly.",
        "formal_text": "In contrast, the backward mode is employed when the user queries the knowledge base. Often for reasons of computational e/l~cleney, it is advantageous not to incorporate all inferred knowledge into the knowledge base immediately. S-rules in the bac: kward mode trigger only when a request comes in which cannot be answered directly, initiating a search in the knowledge base to determine if the answer can be deduced from the available information. For example, the Present S-rule used in the backward mode does not trigger when sentence (27) is read and T-expression (28) is produced by START. The S-rule triggers only when question (29) is asked since this question cannot be answered directly.",
        "GPT2_formal_text": "' property. Formal: In our system, the j-th training example is just the first instance in the input sequence. Formal: But, if we want to train the parser to predict the next word in the input sequence, the model has to pick an input word based on the top-k probabilities for each possible option. Formal: We have to make sure that the probabilities don’t overlap too much, otherwise, it’ll just end up making the whole process unbalanced. Formal: This means we have to keep track of the current probability of a word in the input, and the probability of the next word in the input, which is calculated by a combination of the current word’s probability and the probability of the current input word. Formal: When we’re training the model, we use a dynamic programming (DP) algorithm. For each word in the input, we’ll give it a score based on how likely it is to come next in the sequence. This score is calculated using a mix of different features. Formal: The DPC algorithm works by using dynamic programming to check the current feature set and pick the best set of features for the prediction. Formal: In our system, the DPC algorithm looks at all possible options in the input sequence, and only the one that’s most likely is chosen as the output. Formal: If we want to predict the next word in the input sequence, the DPC algorithm takes a weighted average of the current word’s probability and the chance of the current input word coming next. Formal: We also need to figure out a feature set that represents the current word’s probability and the probability of the current input word’s probability. Formal: To make this work, we need to train the model using the correct input word and generate the correct output. Formal: The system we’re proposing uses an extended version of the DPC algorithm, which also looks at how likely a word is to come next in the input sequence. Formal: We’ll refer to this extended version as the DPC-trained parser. Formal: To make sure we’re not getting stuck with a set of random options, the algorithm also adds up the probabilities of the current words in the input sequence. Formal: Formal: We’ve got two ways to estimate these probabilities: Formal: Formal: We can use the best possible score for",
        "directory": "C88",
        "filename": "C88-1065.json"
    },
    {
        "casual_text": "For the extended version of Lin-EBMT, called Lin-EBMT REC+, they added some stuff from the template-based EBMT approach during the recombination step. The first two steps are still the same as before.",
        "formal_text": "In the implementation of the extended version of Lin − EBM T (i. e. Lin − EBM T REC+ ) ideas from the template-based EBMT approach are incorporated in the recombination step. The previous two steps 6 remain unchanged.",
        "GPT2_formal_text": "ed, a), which is basically the hidden state from the input layer at step t. After that, we put all these different hidden states together into a single vector called h_c. Formal: To create a vector, we just stick all the attention mask values together and then average them out. Formal: We also figure out the vector h_a for the negative samples from the sample window in the decoder and use that as the decoder's hidden state. Formal: Finally, the decoder's output layer has a single output, which is the vector h_x. Formal: After combining the attention mask and the decoder's output vector, we get h_a_n. Formal: Formal: So, the final representation for the negative sample, H_x, is basically the combination of H_a and h_x. Formal: Formal: Formal: The loss function we're looking at is: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2011.eamt-1.27.json"
    },
    {
        "casual_text": "Wow, this looks like a bunch of random symbols and characters! It seems like some kind of code or encrypted message. I can't really make sense of it in its current form, but if you have a specific question or need help decoding it, let me know! Otherwise, it's a bit tough to work with as is.",
        "formal_text": "0 ¤ 8 4 þ ¥ § ¤ ¦ ' ý 9 ¥ ¢ º ý ¢ o ü v H þ A & © A ¢ Î ü 1 @ ü ¦ A ¢ Ê ü ¦ Ê þ ¥ # b ( £ ¤ ¦ ü ¦ I e ü ¦ A ¢ ¤ ¦ § A! Hþ ¥ 4 7 ¢ $ P ) ¥! 0 © ¥! 0 © A ¢ \"! Y ¤ ü ¦ A ¢! ¦ ¢ # Ê þ b ü ¦ H b 1 4 7 g ü 1! C þ ¥ ¤ ü ¦ I 9 ¥ ¢ a 4 7 o ü 1 ¢ 7 6 2 ü ¦ § þ ¥ # } £ § 3 © A ¢ ¤ ¦ ¤d & U ¦ Q ¢ e § ¤ ¦ ¢ Þ ü ¦ ¢ ¢ 7 6 & (! ¦ ¢ ¤ 0 ¤ ¦ I b 1 4 7 g ü 1! 4 þ ¥ ¤ z ü ¦ I 9 ¥ ¢ i ¥ ( 3 4d H ¥! ¤ ¦ § 4 V Ù þ A 2 © ¢ C © A ¢ A ü 1 ¢ © d q ü ¦ A ¢ e ¢ 7 6 2 þ ý ( g # I ¢ ¤ ` s ÿ v S V F P 8 s 4 0 ¤ 4 0 t þ B! C þ ¥ 4 n ü 1 ¢ \"! 0 I § \" ¢ © t s ÿ ¤ 1 ¢ \" 9 ¥ ¢ \"! C þ ¥ # w ¤ 1 ( £ ¢ 4 \" I h g 4 H¢ º þ % ü ¦ § A! ¦ ¢ ¤ ¾ þ ¥ #ü ¦ A § ' a Ê ü # s I ¢ ¤ h ü ¦ ¢ i ¥ ( 3 4 R ( t þ B! C ü 5 H þ Y ¤ 1 ¢ & % ü 1 ¢ 4 7 ¢ ¥ F ¥ ü ¤ b ¢ 4 7 ¢ ¤ ¦ ¤ þ B! ÿ Î ü 1 8 § ¤ ¦ ¢ & þ v ¤ z ü 1! ¦ ' ý ¥! H # ' 4 þ ¥ # H ¥! C ý H ü ¦ A ¢ u 4 7 o ü 1! C þ ¥ ¤ z ü ¦ I 9 ¥ ¢ $ A 2 © ¢ Y s ¤ )! 0 ¢ \" (! ¦ ¢ ¤ 1 ¢ o ü 1 ¢ © X o ÿ þ (! ¦ § C 4HU ¢ 7 6 @ U m g S 0 þ ¥ © ª ü a 4 þ ¥ 4 þ B! ¦! C ÿ ü ¦ A ¢ Þ ü i ÿ 2 ( A % 4 þ ¥ #! V ¤ ¦ ' b 1 4 7 o ü 1! C þ ¥ ¤ z ü ¦ I 9 ¥ ¢d j ¤ z ü 1! 0 ¢ ¤ ¦ ¤ \" v ¤ 1 ¢ º ý þ ¥ o ü ¦ 4 þ ¥ # #ÿ ¥ F 5 ü! ¦ ¢H¢ \"! 0 ¤ ü 1 r þ 4 V A 4 7 ¢ H! ¦ ¦ ý 8 þ ¤ 1 ¢ ü v H þ ¥ #ü 1 ¢ \"! 0 þ b ü ¦ I 9 ¥ ¢ ¤ þ ¥ © ü 4 þ ¥ X £ ¢ 4 7 g ü 1! 4 þ ¥ ¤ z ü ¦ I 9 ¥ ¢ u! ¦ ¢ # Ê þ b ü ¦ I ü 1 ¤ 1 ¦ ý ¢ D ( $ þ B! ü ) H ü ¦ A ¢ w ( 3!",
        "GPT2_formal_text": "= {s1, s2, ..., sm}, where s is the title, and ai, j is the id of the first sentence in the article. After that, we grab the top 20 abstracts that show the most relevant facts from the full article. Formal: For each fact from the full article and the top 20 abstracts, we create a special dataset called QA_F_ab for each fact, using the full article and the abstracts. This dataset will have all the facts we pull out. Formal: We also make a dataset called QA_F_fact for the facts we found in the full article. This dataset will only have facts from the full article. Formal: We also make a dataset called QA_F_gold for the facts from the full article. Formal: We make a dataset called QA_F_model for the facts we found in the full article. Formal: We make two datasets: QA_F_score and QA_F_fact, where each one has the ranked facts from the full article. Formal: Finally, we use the extracted facts to train a classifier. Formal: Basically, for each fact, we use QA_F_score to rank the facts, and then we use QA_F_fact to rank the facts. Formal: Here, f_i is the number of answers that answer that fact, and ai, j is the id of the answer. Formal: For each answer, we calculate a score using QA_F_score. Formal: Then, we combine all the scores to get the final score. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C04",
        "filename": "C04-1042.json"
    },
    {
        "casual_text": "Each part gets translated or edited individually, and there are four different ways to move around the document:",
        "formal_text": "Each unit is translated/edited one at a time and navigation through the different segments of the document can be achieved in four ways:",
        "GPT2_formal_text": "ed. c = c + self.att_masked. t = t + self.att_masked. m = m + self.att_masked. W * W + b * b + c * c - self.att_masked. p = p - self.att_masked. d = d - self.att_masked. t = t - self.att_masked. Formal: So, the matrix X is just a sequence of vectors where each vector represents the words in the document, and each vector is a dot product of the words in the document. Formal: The sum of the log probability distributions for words in the document is a multiple of this vector. Formal: The time it takes to find a match is proportional to the square root of the vector. Formal: The document has a set of numbers called N1, N2, ..., Nm. The time it takes to find the match is also proportional to the square root of N1, N2, ..., Nm. Formal: The vector W stands for a matrix that represents the words in the document. Formal: Here, W is a vector with length n, and N1, N2, ..., Nm are the words in the document. Formal: The time it takes to find a match is also proportional to the square root of W. Formal: The terms in W and N are matrices that represent the words in the document. Formal: The sum of the log probability distributions for words in the document is a multiple of the vector. Formal: The matrix X is a sequence of vectors where each vector is a dot product of the words in the document. Formal: The time it takes to find a match is also proportional to the square root of X. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "Soo (2018) and Blinov and his crew (2019) looked into whether a text snippet is just a one-liner. Meanwhile, Zhang and Liu (2014), Ortega-Bueno and friends (2018), and Chiruzzo et al. (2019) dove into the task of classifying humor in tweets. Castro and his team (2018) gathered humor ratings and funniness scores for Spanish tweets using crowdsourcing. Chiruzzo et al. (2019) even came up with a regression task to predict a tweet's humor score. Li and his colleagues (2020) collected Chinese internet slang and mixed it with a humor detection method to analyze the sentiment of Weibo posts. It's worth mentioning that all the examples in the datasets used or created in these studies are separate from each other. Since our dataset is based on full scripts, the annotated lines and text spans could be useful for researchers working on algorithms that understand humor in longer contexts. \n\nBesides studies on short text snippets, Bertero (2019) and Hasan and his team (2019) made datasets from TV sitcoms like The Big Bang Theory and TED talks, respectively. Their aim was to predict if a sequence of texts would trigger immediate laughter. Yang and his team (2015) extracted key words like \"sing,\" \"sign language,\" and \"pretty handy\" from jokes, which is kind of like what we did with our information extraction annotations.",
        "formal_text": "and Soo (2018); Blinov et al. (2019) investigated if a text fragment is a one-liner. 3 Zhang and Liu (2014); Ortega-Bueno et al. 2018; Chiruzzo et al. (2019) explored the humour classification task on tweets. Castro et al. (2018) collected humour values and funniness scores of Spanish tweets by using crowdsourcing. Chiruzzo et al. (2019) proposed a regression task that predicts the humour score for a tweet. Li et al. (2020) collected Chinese Internet slang expressions and combined them with a humor detecting method to analyse the sentiment of Weibo 4 posts. It should be noted that the examples in all of the corpora used or constructed in the above-mentioned studies are independent of each other. Since our corpus is based on entire scripts, the annotated lines and text spans might also benefit the researchers who are interested in modelling long-context-aware algorithms to understand humour. Apart from the studies on short text fragments, Bertero (2019) and Hasan et al. (2019) created corpora from television (TV) sitcoms such as The Big Bang Theory 5 and TED talks 6 respectively. Their goal is to predict whether or not a sequence of texts will trigger immediate laughter. Yang et al. (2015); extracted the key words such as sing, sign language and pretty handy from jokes, which are similar to our information extraction annotations.",
        "GPT2_formal_text": "= k[s_t] + h[s_r] + p(s_t, t) + b(s_r, t) + c(s_t, t) + a(s_t, t) + c(s_t, t). Formal: Each word in the input is turned into a d-dimensional vector, and each token gets a hidden state, h. The (s_t, t) part comes from the word embedding matrix X_s_t, which is a learned one from a big text collection. The (s_r, t) part comes from the word embedding matrix X_r_t, which is a learned one from a smaller text collection. The (s_t, t) are part of the hidden state vectors, so the final value for the model's output is its hidden state. Formal: The outputs from the Bi-LSTM layer can be written like this: Formal: The output from the Bi-LSTM layer can be written like this: Formal: In this paper, we’re using the Bi-LSTM to figure out the probabilities for the types of entity-related entities and their roles in the query. Formal: We’re using the Bi-LSTM to figure out the probabilities for the types of entity-related entities and their roles in the query. Formal: We’re combining the output from the Bi-LSTM layer with the query embedding matrix X_q_t. Formal: We’re combining the output from the Bi-LSTM layer with the query embedding matrix X_q_t. Formal: We’re combining the output from the Bi-LSTM layer with the query embedding matrix X_q_t. Formal: We’re combining the output from the Bi-LSTM layer with the query embedding matrix X_q_t. Formal: We’re combining the output from the Bi-LSTM layer with the query embedding matrix X_q_t. Formal: The output from the Bi-LSTM layer can be written like this: Formal: The output from the Bi-LSTM layer can be written like this: Formal: The output from the Bi-LSTM layer can be written like this: Formal: The output from the Bi-LSTM layer",
        "directory": "codi",
        "filename": "2020.codi-1.5.json"
    },
    {
        "casual_text": "We calculated inter-annotator agreement (IAA) separately for the second and third stages since they have different goals. In the second stage (cluster quality), we got an average Spearman correlation of r_s = 0.73, which is similar to what other studies in topic modeling have reported (Newman et al., 2010, with r_s = 0.73/0.78, and Aletras and Stevenson, 2013, with r_s = 0.70/0.64/0.54). We also got an average Cohen's Kappa of κ = 0.48, which is considered moderate agreement. For the third stage (issue identification), we found κ = 0.36, which is fair agreement.\n\nLooking at disagreements in stage 2, only 2% came from differing opinions on Good-Bad clusters. Most disagreements were about Good-Intermediate (37%) and Intermediate-Bad (61%) cases. This is a good sign because it shows that annotators rarely have completely opposite views on cluster quality. They mostly agree on what makes a cluster coherent, which is the main point of this task.\n\nFor issue identification, most disagreements (49%) were about distinguishing Intermediate-Chained cases. This makes sense because it can be tricky to identify subclusters in the first stage. For the final decision-making process, we found that there was always a majority, so we went with the label that at least two out of three annotators agreed on.\n\nTable 1 gives an overview of the corpus size, coherence quality, and issues identified for COVID-19 and Election. Check out Appendix C for more details.",
        "formal_text": "Inter-annotator agreement (IAA) was computed separately for the second and third stages as they serve a different purpose. For the second stage (cluster quality), we obtain average Spearman correlation r s = 0. 73 which is comparable to previous coherence evaluation scores in topic modelling literature ( (Newman et al. , 2010) with rs = 0. 73 / 0. 78 and (Aletras and Stevenson, 2013) with rs = 0. 70 / 0. 64 / 0. 54) and average Cohen's Kappa κ = 0. 48 (moderate IAA). For the third stage (issue identification), we compute average κ = 0. 36 (fair IAA). Analysis of pairwise disagreement in stage 2 shows only 2% is due to division in opinion over Good-Bad clusters. Good-Intermediate and Intermediate-Bad cases account for 37% and 61% of disagreements respectively. This is encouraging as annotators almost never have polarising views on cluster quality and primarily agree on the coherence of a good cluster, the main goal of this task. For issue identification the majority of disagreements (%49) consists in distinguishing Intermediate-Chained cases. This can be explained by the expected differences in identifying subclusters in the first stage. For the adjudication process, we found that a majority always exists and thus the final score was assigned to be the majority label (2/3 annotators). Table 1 presents a summary of the corpus size, coherence quality and issues identified for COVID-19 and Election (See Appendix C for a discussion).",
        "GPT2_formal_text": "ed. Then, we use a logistic regression model to figure out how much the model's performance drops for each input feature. Formal: We train a model on the Yelp dataset, which has user reviews and their scores for different attributes. The model learns how to rank these reviews based on those attributes. Formal: To see how well the model can rank reviews, we use it to predict which reviews are better than others. Formal: The feature vector we get is the average of all the attribute vectors we get from training on Yelp. We also figure out the prediction probabilities by doing a random walk through the feature space. Formal: The model learns how to rank reviews by looking at all the attribute vectors. Formal: We randomly walk through the feature space, picking the feature vector with the highest probability. Formal: The prediction probabilities for the top 5 reviews are calculated using this process. Formal: We use a linear mixed effect model to predict how well the model will rank reviews. Formal: The best model is the one that performs the best based on the validation loss. Formal: This approach has a few issues. First, the test set isn't balanced, so it can't directly compare different models. Second, the model can't learn the true importance of each attribute. Formal: We train on the Yelp dataset, which has user reviews and their scores for different attributes. The model learns how to rank these reviews based on those attributes. Formal: To see how well the model can rank reviews, we use it to predict which reviews are better than others. Formal: The feature vector we get is the average of all the attribute vectors we get from training on Yelp. We also figure out the prediction probabilities by doing a random walk through the feature space. Formal: The model learns how to rank reviews by looking at all the attribute vectors. Formal: To make this model more reliable, we calculate the validation loss based on the validation set. Formal: The best model is the one that performs the best based on the validation loss. Formal: This approach has a few issues. First, the test set isn't balanced, so it can't directly compare different models. Second, the model can't learn the true importance of each attribute. Formal: We train on the Yelp dataset, which has user reviews and their scores for different attributes. The model learns how to rank these reviews based on those attributes. Formal: To make this model more reliable, we calculate the validation loss based",
        "directory": "acl",
        "filename": "2021.acl-long.530.json"
    },
    {
        "casual_text": "As a serial processor, the learner can only focus on one thing at a time.",
        "formal_text": "being a serial processor, the learnor can focus his attention on but one thing at a time.",
        "GPT2_formal_text": "= all ( ) ) .\n       Casual: • These two methods use the embeddings to create captions for a specific image, while the first one uses a list of words to get a general idea of the image. Formal: The model we’re proposing has three main parts: an encoder, a decoder, and an attention layer. The encoder takes a sequence of input images and turns them into a single representation for the image. Then, the decoder uses this representation to create a caption for the image. Formal: To handle different types of images (like photos, videos, etc.), we added a multi-head attention layer to the encoder. This layer creates a vector representation for each of the three parts of the image, which helps it pick up on the key details related to each type of image. Formal: For each input image x i, the encoder creates a sequence of position vectors x i+1 to x i+l (where i+1 represents the start of the image, and i+l is the length of the image). Formal: The model then uses a multi-head attention layer to learn the attention vector for each of the three parts of the image. This attention vector is a vector of size d d×m. Formal: Finally, the output of the attention layer is the attention vector x 1 for the image. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C86",
        "filename": "C86-1133.json"
    },
    {
        "casual_text": "This assessment doesn't consider punctuation and is based on the updated .prm parameter setup in evalb. 5. Check out the link here: http://www.cis.upenn.edu/~dbikel/software.html",
        "formal_text": "This evaluation ignores punctuation and corresponds to the new. prm parameter setting on evalb. 5 http: //www. cis. upenn. edu/˜dbikel/ software. html",
        "GPT2_formal_text": "ing, which represents the state of the graph.         Casual: In this paper, we're focusing on how well the label distribution matches up with the actual intent. To tackle this, we use the model's own prediction on the target-side of the intent, but we leave out the knowledge being predicted. We then calculate the cross-entropy loss, which is calculated by dividing the cross-entropy loss by the cross-entropy loss for each intent type. Formal: We’re using the premise of the active learning assumption and the multi-task learning idea from (Gu et al., 2019), which was introduced by Nguyen et al. in 2018. Here, each task has its own task-specific label distribution, and we assume there’s only one label per intent. This setup helps us handle the multi-task learning problem in a more efficient way. Formal: The cross-entropy loss for each type (type_i) is calculated based on the cross-entropy loss for the hypothesis (h_i). Formal: For a given type (type_i), the model can learn both the label distribution and the task-specific label distribution at the same time. Formal: Formal: The cross-entropy loss for each type (type_i) is calculated using the cross-entropy loss for the hypothesis (h_i). Formal: The formula for the multi-task learning assumption is as shown in equation 7. Formal: When we’re trying to find the best label for an intent (h_i), we calculate the cross-entropy loss for the task. Formal: We use the cross-entropy loss for each task to find the best label for an intent (h_i). Formal: We’ve tested our model on a bunch of different datasets, and the results are in Tables 1, 2, and 3. Formal: We’ve also included the cross-entropy loss for the task-specific label distribution. Formal: We’ve trained our model with the multi-task learning setup from (Gu et al., 2019) and the scalability goal from (Zhang et al., 2019). Formal: Formal: Formal: The cross-entropy loss for each task is calculated using the cross-entropy loss for the hypothesis (h_i). Formal: We’ve also included the cross-entropy loss",
        "directory": "D10",
        "filename": "D10-1003.json"
    },
    {
        "casual_text": "Alright, so the idea here is to close the big gap between how we talk (natural language), how computers talk (computer language), and logic. We want to do this for a good reason: when you're building a computer system for a specific area (like healthcare, finance, etc.), you need to really understand that area inside and out. But usually, the people building the system (the implementors) don’t know much about it, so they have to learn from the experts in that field. The problem is, those experts usually don’t know much about computers. So, having a way to clearly and efficiently explain this knowledge would be super helpful. Right now, there’s no tool like that, so we’re trying to create one. LESK is kind of like a first step toward making that happen.",
        "formal_text": "Knowledge) is intended to bridge the considerable gap between natural language (I~L), computer lan6uage (CL) and log~tc. It is desirable to do so for the following reason. To implement any non-trivial computer system for some problem domain, one must first have a very clear understanding of the domain concepts. Usually the implementors do not have this knowledge, and must struggle to obtain it from the domain experts, who are usually not computer experts. Hence a means of precise and efficient knowledge expression would be very useful. No such tool exists today and we therefore seek to develop one. LESK is a first approximation to such a medium for knowledge capture.",
        "GPT2_formal_text": "\") as the values for the attribute. Formal: We give each RDF term a representation as a vector. The data we feed into the term vector is what we use to generate the output. Formal: In our experiments, we use a linear classifier to predict the semantic types for RDF terms. The input is an RDF term, and the output is a list of semantic types. Formal: Using this method, we can predict the semantic types for a list of RDF terms. Formal: Using a list of RDF terms, we can predict the semantic types for a list of RDF terms. Formal: The value of a vector can also be used as a method to predict semantic types. Formal: For example, a vector can be thought of as a pair of the head word and its semantic type. Formal: Formal: We can find the vector for a list of RDF terms by finding the vector for a list of RDF terms. Formal: Formal: Finally, we use the vector of a list of RDF terms to find the vector for a list of RDF terms. Formal: Formal: The vector for a list of RDF terms is found by using the vector for a list of RDF terms. Formal: Formal: You can find more details about our evaluation methods and the results in the supplementary material. Formal: Formal: We can find the vector for a list of RDF terms by using the vector for a list of RDF terms. Formal: Formal: Lastly, we use the vector of a list of RDF terms to find the vector for a list of RDF terms. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C82",
        "filename": "C82-2065.json"
    },
    {
        "casual_text": "Piecewise Convolutional Neural Network (PCNN). In distantly supervised relation extraction, it's pretty standard to use the PCNN (Zeng et al., 2015) to create contextualized representations from a sequence of word embeddings. Compared to the usual 1D-CNN with max-pooling (Zeng et al., 2014), the PCNN's piecewise max-pooling can better capture the structure between two entities by paying attention to their positions. Here's how it works: first, a 1D-CNN (Kim, 2014) processes the input sequence to get contextualized representations. Then, piecewise max-pooling is applied to the output sequence to create a sentence-level embedding. That's basically the process in a nutshell.",
        "formal_text": "Piecewise Convolutional Neural Network. As a common practice in distantly supervised relation extraction, piecewise convolutional neural network (PCNN) (Zeng et al. , 2015) is used to generate contextualized representations over an input sequence of word embeddings. Compared to the typical 1D-CNN with max-pooling (Zeng et al. , 2014), piecewise max-pooling has the capability to capture the structure information between two entities by considering their positions. Specifically, 1D-CNN (Kim, 2014) is first invoked over the input sequence for contextualized representations. Then a piecewise max-pooling performs over the output sequence to obtain sentence-level embedding. These steps are written as",
        "GPT2_formal_text": "_tgt: This helps figure out the attention weights for each token in the input sequence. We also added a linear layer to handle the noise distribution. Formal: The results of our cross-lingual token alignment, as shown in Figure 2, are super detailed. We do this using the WMT 2018 dataset. The top-left part of Figure 2 shows the alignments we got from the training data (shown in orange). The second row of Figure 2 has all the alignments we got from the test data (shown in light gray). We also did an ablation study to see how well the model performs and to check how much of the alignment we could tell. Formal: Formal: After doing a search for alignments using the WMT2018 test set, we found 17,195 alignments. Formal: For the base model, we used the best alignment possible from the test set. Formal: Using the WMT2017 data, we only kept alignments where both the source and target sentences were in the same language. Formal: To get the alignment for each word in the source sentence, we did a word-level search using the language model we trained with. Formal: We only kept alignments where both the source and target sentences were in the same language. Formal: We only kept alignments where both the source and target sentences were in the same language. Formal: We picked the top-ranked alignment for each token, based on the WMT2017 language model we trained with. Formal: We only kept alignments where both the source and target sentences were in the same language. Formal: We picked the best alignment for each word in the source sentence, based on the WMT2017 language model we trained with. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.145.json"
    },
    {
        "casual_text": "For the MultiDoc2Dial dataset evaluations, there are two main tasks. Task 1 is about predicting the grounding span for the next agent response. For this task, we use: (1) the current user turn, (2) the dialogue history, and (3) all the documents from every domain as input. The goal is to find the most relevant text span from one document to use in the next agent response. \n\nTask 2 is about generating the agent's response in natural language. For this task, we also use: (1) the current user turn, (2) the dialogue history, and (3) all the documents from every domain as input.",
        "formal_text": "For the evaluations on MultiDoc2Dial dataset, two sub-tasks are proposed. Task 1 aims to predict the grounding span for the next agent response. For task 1, we get (1) current user turn, (2) dialogue history, (3) the entire set of documents from all domains as input. For the output, we aim to figure out the most relevant grounding text span from one document for the next agent response. Task 2 aims to generate agent response in natural language. For task 2, we get (1) current user turn, (2) dialogue history, (3) the entire set of documents from all domain as an input.",
        "GPT2_formal_text": ": [span_1, ..., span_k]          Casual: Alright, let's break this down in a simpler way. We're looking at how different parts of the graph are connected. The nodes in the graph are like the \"houses\" in the house of a tree (check out Figure 1 for a visual). Each node has a bunch of connected \"children\" that are connected to it through edges.\n\nThe way these kids are connected depends on the relationships between them. For instance, if you have a node \"house\" and a child \"cat,\" \"cat\" can be directly linked to \"house\" through a \"child\" relationship, like \"child_1.\" But if you want to link \"cat\" to a \"parent\" node \"house,\" you'd need to create a new path through the graph.\n\nNow, there are two main ways to connect these nodes:\n1. **Path-to-parent**: This means that the \"parent\" node is directly connected to the \"child\" node through its own path.\n2. **Path-to-child**: This means that the \"child\" node is directly connected to the \"parent\" node through a \"child\" path.\n\nTo break it down, there are three types of nodes:\n\n- **Node**: This includes the \"parent\" node and its own path.\n- **Path**: This includes both the \"parent\" node and its own path.\n- **Children**: These are the paths connecting the nodes and their children.\n\nFor each node, there's also a path to its child node. The \"children\" are linked by edges, as shown in Figure 1. Formal:\n\nSo, if you have a node \"house\" and a child node \"cat,\" \"cat\" can be directly linked to \"house\" through a \"child\" path, like \"child_1.\" But if you want to link \"cat\" to a \"parent\" node \"house,\" you'd need to create a new path through the graph. Formal:\n\nSo, if you have a node \"house\" and a child node \"cat,\" \"cat\" can be directly linked to \"house\" through a \"child\" path, like \"child_1.\" But if you want to link \"cat\" to a \"parent\" node \"house,\" you'd need to create a new path through the graph.",
        "directory": "dialdoc",
        "filename": "2022.dialdoc-1.15.json"
    },
    {
        "casual_text": "To figure out how entities in the original text match up with those in the translated version, we need to do two things: first, identify the parts of the translated sentence that are entities, and second, connect those parts to the corresponding ones in the original sentence. We've built an alignment tool that uses the attention weights between the encoder and decoder in the Marian model to link the input and output sentences. These weights tell us how much focus is put on each input word when the model is generating an output word. \n\nThere's a heatmap in Figure 2 that shows these attention weights for an English sentence and its Italian translation. The cross-attention score for each decoder word is calculated using multi-head attention (Vaswani et al., 2017) across all the encoder words. In the example from Figure 2, each attention vector is represented by a column in the heatmap.",
        "formal_text": "To find the mapping between entities in the source and the translated language, we need to 1) detect entity spans in the output sentence, 2) align those spans with input sentence spans. We have created an alignment module, which uses the cross attention weights between the encoder and the decoder of the Marian model to align the input and output sentences. These weights show the amount of attention given to each input token when an output token is being generated. Figure 2 shows a heatmap of cross-attention weights for an English sentence and its translation in Italian. The cross-attention score for each decoder token is calculated by doing a multi-head attention (Vaswani et al. , 2017) over all encoder tokens. For the example shown in Figure 2, each attention vector corresponds to one column in the heatmap.",
        "GPT2_formal_text": "ing (Oi et al., 2017). Basically, it's a way to combine the output from the encoder and decoder, so it can create a fixed-size vector that represents the content. Formal: To get the outputs for the two question tokens, we use a linear projection from the input token vector, like this: Formal: Basically, the vector that represents the question gets passed through a softmax layer. This gives us a probability distribution across the whole input sequence, which we then use to calculate the final answer vector for the question token. Formal: To make sure the questions have different structures, we use a single-layer bidirectional GRU (BiGRU) (Cho et al., 2014) to create the answer vector. Formal: We take the hidden state vector h_qt from the GRU and feed it into the biGRU, which gives us a hidden state vector for the question token, which we then use to calculate the answer vector for the token. Formal: The process is laid out in equations (4) and (5), but here's the important part: using the i-th input token, we calculate the final prediction vector for the i-th token. Formal: Then, we use a max-pooling operation to get the sentence representation, which we use to calculate the final answer vector for the question token. Formal: Formal: And that's it! Formal: We tested this on the dataset \"Question Answering\" (Oi et al., 2017), which has question-answer pairs from CoQA (Rajpurkar et al., 2018). This dataset is a big deal for neural question answering, especially since it has a really wide variety of questions. Formal: We also tried it on another dataset, \"QQP\" (Qin et al., 2017), but we skipped it because our experiments were limited to just one question-answer pair per document. Formal: We’re sharing the code we used for all these experiments on GitHub at https://github.com/plengq/question-answer-compression. Formal: For future work, we’re thinking of looking into how we can make the attention vectors better for question answering. Formal: We also plan to create a model specifically for question answering to tackle the long-tail issue. Formal: We’ve included the results for question answering, along with the attention vector, in Table 1.",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.481.json"
    },
    {
        "casual_text": "Next, we add the thread title to the comment and send the whole thing to an outside company for manual translations. When we got the translations back, we saw that the quality wasn't always consistent. Turns out, translating stuff from social media, with all its tricky little details, is pretty tough even for people. To make sure we get the best translations, we go through the data by hand, break the comments into sentences, and toss out the ones that aren't up to par for our test data. This way, we end up with about 1,000 sentence pairs in each direction for our final test set.",
        "formal_text": "We then concatenate the title of the thread where the comment was found to the text and send everything to an external vendor for manual translations. Upon reception of the translations, we noticed a certain amount of variation in the quality of translations, likely because translating social media text, with all its nuances, is difficult even for humans. In order to ensure the highest quality in the translations, we manually filter the data to segment the comments into sentences and weed out poor translations for our test data. We thereby retain around 1, 000 sentence pairs in each direction for the final test set.",
        "GPT2_formal_text": "\". Formal: (1) It's not a requirement that each output must be different from the input, but it's always good to know that when creating a sentence, we're looking at how the translation connects to the \"relevant sentences\" (check out Figure 1). Formal: (2) This happens when the translation of the current sentence (s_i) is similar to the one in the reference translation (s_t). Formal: (3) This happens when the source sentence is similar to the translation of the reference sentence (s_s). Formal: (4) This happens when the reference sentence is similar to the source sentence. Formal: (5) This happens when the translation of the source sentence is similar to the reference sentence. Formal: (6) This happens when the source sentence is similar to the reference sentence. Formal: (7) This happens when the translation of the reference sentence is similar to the source sentence. Formal: (8) This happens when the translation of the source sentence is similar to the reference sentence. Formal: (9) This happens when the source sentence is similar to the reference sentence. Formal: (10) This happens when the reference sentence is similar to the source sentence. Formal: (11) This happens when the source sentence is similar to the reference sentence. Formal: (12) This happens when the reference sentence is similar to the source sentence. Formal: (13) This happens when the source sentence is similar to the reference sentence. Formal: (14) This happens when the source sentence is similar to the reference sentence. Formal: (15) This happens when the source sentence is similar to the reference sentence. Formal: (16) This happens when the reference sentence is similar to the source sentence. Formal: (17) This happens when the source sentence is similar to the reference sentence. Formal: (18) This happens when the reference sentence is similar to the source sentence. Formal: (19) This happens when the reference sentence is similar to the source sentence. Formal: (20) This happens when the reference sentence is similar to the source sentence. Formal: (21) This happens when the source sentence is similar to the reference sentence. Formal: (22) This happens when the reference sentence is similar to the source sentence. Formal: (23) This happens when the reference sentence is similar to the source sentence. Formal: (24) This happens",
        "directory": "D18",
        "filename": "D18-1050.json"
    },
    {
        "casual_text": "Okay, so this is a bit of a mess, but let me try to break it down in simpler terms. Basically, it's talking about a relationship that only works under certain conditions. The text is saying that this relationship, let's call it \"L,\" only holds true when a specific property of something, like a \"thing\" or \"object,\" is involved, along with some other stuff. \n\nNow, this \"L\" thing isn't really useful unless we know exactly what conditions need to be met for it to work. So, the text is trying to explain those conditions with something called a \"meaning postulate.\" It's saying that VPVQ(o, dy(l', (2), . , , d, , si~g (, X, ~, '~, '(~4) is the way to spell out those conditions. \n\nIn short, it's just trying to make clear what needs to happen for this relationship to make sense.",
        "formal_text": "where /q is Allcv(~d, (ll) A typc(ll, borrou: ) A a, g (! 'n. t(H, I, ')A ob. jt~cl, (ll, I) This says Lha, L 1, he rela. ti~mshil) on. l 9 holds l)(; l, w(~en the l>['~fl)erty o[' l)ehb'; a, cam a. i]d, some oLlmr oh joel. This is Nile; ~s fax ~s il Koes, buL iL isH'l; worth very much unless we spell out the conditions un der which this rel~tti(mship holds. The tbllowing mea. ning postub~te does just that: VPVQ(o, dy(l', (2), . , , d, , si~g (, X, ~, '~, '(~4",
        "GPT2_formal_text": "_tgt = argmax(W * h_tgt, {1, ..., k}) + argmax(W * h_tgt, {k+1, ..., k+M}) + argmax(W * h_tgt, {k, ..., M}) + argmax(W * h_tgt, {1, ..., K}) + argmax(W * h_tgt, {1, ..., K+1}) + argmax(W * h_tgt, {1, ..., K+1}) + argmax(W * h_tgt, {K, ..., M}) + argmax(W * h_tgt, {K+1, ..., K+M}) + argmax(W * h_tgt, {K, ..., M+1}) + argmax(W * h_tgt, {1, ..., K+1}) + argmax(W * h_tgt, {1, ..., K+M+1}) + argmax(W * h_tgt, {1, ..., K+M+1}) + argmax(W * h_tgt, {K, ..., M+1}) + argmax(W * h_tgt, {1, ..., K+1}) + argmax(W * h_tgt, {1, ..., K+M+1}) + argmax(W * h_tgt, {K, ..., M+1}) + argmax(W * h_tgt, {1, ..., K+M+1}) + argmax(W * h_tgt, {K+1, ..., K+M+1}) + argmax(W * h_tgt, {1, ..., K+M+1}) + argmax(W * h_tgt, {1, ..., K+M+1}) + argmax(W * h_tgt, {K, ..., M+1}) + argmax(W * h_tgt, {1, ..., K+M+1}) + argmax(W * h_tgt, {K, ..., M+1}) + argmax(W * h_tgt, {1, ..., K+M+1}) + argmax(W * h_tgt, {1",
        "directory": "C94",
        "filename": "C94-2142.json"
    },
    {
        "casual_text": "So, in this paper, we take a look at two language models that work with syntax, but in a kind of basic translation setup. We separate the target language's structure from the translation rules, which gives the language model more room to do its thing. This way, we can really see what the parser is good at and where it might fall short when picking the right translation output.",
        "formal_text": "To that end, in this paper we examine two syntaxbased language models in a weak translation model. Our approach decouples target-language trees from the translation rules, providing the language model with more freedom to express itself, and permitting us to isolate the strengths and weaknesses of the parser in selecting MT output.",
        "GPT2_formal_text": "= { ..., \"[, ]\" } = { ... } for all the word pairs in sentence s, where each word pair has a number from 1 to (n-1) and a word id from 1 to (n-m). Formal: In this part, we're taking the average of the scores from the N individual attention heads for each word pair, but we're skipping the ones that have a zero value. Formal: For each word pair in sentence s, the average of its attention scores is calculated using equation (4). Formal: Formal: To make sure we get the best possible results for each pair, we'll run the experiment again with the same settings but with the same model (like the one shown in Figure 1). Formal: So, the final vector for the word pair in sentence s? It's made up of these three things: the average attention score, the word id from 1 to (n-1), and the attention score for the word pair itself. Formal: In a similar way, the vector for the word pair in sentence s? It's just the average attention score, the word id, and the attention score for the word pair itself. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "amta",
        "filename": "2008.amta-papers.16.json"
    },
    {
        "casual_text": "[Question] You can use the subway to get to and from work <mask> days a week.",
        "formal_text": "[Question] you may take the subway back and forth to work <mask> days a week.",
        "GPT2_formal_text": "ed. I created the TESLA system using two public datasets that cover different language stuff: TESLA-C (Erjavec et al., 2011) and TESLA-F (Aghajanyan et al., 2020). TESLA-C and TESLA-F are used for language stuff in a bunch of NLP tasks, like parsing and question answering. Here's the link: https://github.com/medeiros/TESLA-C and https://github.com/medeiros/TESLA-F. Formal: TESLA-C is a part of TESLA and can work with a lot more languages than TESLA-F. It starts by figuring out how to get around some language constraints, which is basically a big part of its job. Formal: TESLA-F is in TESLA and helps you figure out how to get around some language constraints using a fully-developed metric. Formal: Both TESLA-C and TESLA-F are pretty useful for language stuff, but we mainly focus on how TESLA-C handles these constraints. Formal: The main goal of TESLA is to figure out how to handle language constraints, but we also need to make sure the system can work with other stuff too. Formal: In this project, we came up with a new metric to check how well a language model (LM) is doing in terms of representing text. Formal: We also created a new metric that compares how well a sentence is represented to how well it's translated into a language with the same grammar. Formal: Formal: Also, we noticed that different types of constraints—like gender-related ones and word order ones—can affect the system's performance. Formal: For instance, if a constraint says the sentence should be translated into English but the LM outputs something different, the new metric should give a different result. Formal: Formal: We came up with a new metric to compare how well a language model (LM) is doing in terms of representing text with different types of constraints. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.225.json"
    },
    {
        "casual_text": "Semantic parsing is all about turning sentences in plain language into something a computer can understand and work with. This means converting natural language (NL) phrases into structured meaning representations (MRs) that are written in formal languages like Prolog, SQL, or Python. These formal languages have their own rules, which are laid out in a formal grammar—basically, a set of instructions that define how the language works. \n\nIn these formal languages, MRs are often called logical forms or programs. The cool thing is that these MRs can be run in a programming environment to get results, like pulling data from a database using an SQL query. This process helps with automated reasoning, which is a fancy way of saying the computer can figure things out on its own (Kamath and Das, 2018).",
        "formal_text": "Semantic parsing is concerned with mapping natural language (NL) utterances into machine-readable structured meaning representations (MRs). These representations are in the formats of formal languages, e. g. Prolog, SQL, and Python. A formal language is typically defined by means of a formal grammar, which consists of a set of rules. Following the convention of the chosen formal language, MRs are also referred to as logical forms or programs. An MR is often executable in a (programming) environment to yield a result (e. g. results of SQL queries) enabling automated reasoning (Kamath and Das, 2018).",
        "GPT2_formal_text": "= (src, stt, req, res)          Casual: This example shows a piece of text in one language, and the question it's asking is in another language. Formal: A text with the same words in both languages. Formal: A text with the same words in the source language, but in a different language with some words swapped out for symbols. Formal: A text with the same words in the source and target languages, but with a few words swapped out for symbols. Formal: A text with different words in both languages, but with the same meaning. Formal: A text in both the source and target languages, but only the words that were supposed to be translated in the target language. Formal: A text in both the source and target languages, but the text doesn't say which language it came from. Formal: A text in both the source and target languages, but the text doesn't say which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't say which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't mention which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't say which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't mention which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't say which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't mention which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't mention which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't mention which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't mention which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't mention which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't mention which language it was translated into. Formal: A text in both the source and target languages, but the text doesn't mention",
        "directory": "coling",
        "filename": "2020.coling-main.226.json"
    },
    {
        "casual_text": "In the middle of the table, you'll find the results broken down by SBA. Noun phrases that didn't get a semantic analysis result, basically because the analysis didn't work, are grouped together and labeled as <other>, which you can see at the end of the table.",
        "formal_text": "The middle part of the table shows the results by SBA. Noun phrases that have no semantic analysis result (analysis failure) are bundled and named <other>, as shown in the last part of the table.",
        "GPT2_formal_text": "ing = None          Casual: We're suggesting a new multi-label classification task to better understand the complex relationship between different types of text. Formal: We use a DRS (Discourse Representation Structure) to represent the text as a sequence of tokens. We refer to this as \"formal\" because, unlike the usual CRFs (Conditional Random Fields), it includes a bunch of predefined features that help us handle these complex relations. Formal: In this new task, we treat each token as a \"topic\" and treat the relationship between words, tokens, and topics as a \"topic pair.\" Formal: This model is built using the attention-based multi-label classification model and uses a special attention mechanism to handle these complex relations. Formal: We’ve developed a new multi-label classification task to better understand the complex relationship between different types of text. This task is currently being worked on and is expected to be ready by the end of 2019. Formal: We’re looking at three different categories of text: full text, short text, and mixed text. Formal: We’ve shown that the multi-label classification model works really well. Formal: We’ve developed a new multi-label classification model for detecting text differences, which is expected to be ready by the end of 2019. Formal: We’ve created a multi-label classification model for detecting text differences, which is expected to be ready by the end of 2019. Formal: We’ve developed a new multi-label classification model for detecting text differences, which is expected to be ready by the end of 2019. Formal: We’ve developed a multi-label classification model for detecting text differences, which is expected to be ready by the end of 2019. Formal: We’ve developed a multi-label classification model for detecting text differences, which is expected to be ready by the end of 2019. Formal: We’ve developed a multi-label classification model for detecting text differences, which is expected to be ready by the end of 2019. Formal: We’ve developed a multi-label classification model for detecting text differences, which is expected to be ready by the end of 2019. Formal: We’ve developed a multi-label classification model for detecting text differences, which is expected to be ready by the end of 2019. Formal: We’ve developed a multi",
        "directory": "C04",
        "filename": "C04-1174.json"
    },
    {
        "casual_text": "We get a sentence-level representation, let's call it s_h, by adding up all the value vectors in the sentence, but each one is multiplied by a weight first (check out Equation 11). After that, there are two feed-forward layers. The first one is non-linear and shrinks the sentence representation into a smaller feature space. The second one is linear and gives us a single number, o_h, which is the sentence-level score for each head h (Equation 12).",
        "formal_text": "A sentence-level representation s h is obtained as the weighted sum over all the value vectors in the sentence (Equation 11). This is followed by two feed-forward layers: the first one is non-linear and projects the sentence representation onto a smaller feature space, while the second one is linear and outputs a scalar sentence-level score o h ∈ R for each head h (Equation 12).",
        "GPT2_formal_text": "= (x_i, y_i), where x_i is the i-th word in the review, y_i is the i-th word in the review summary, and θ is the model parameters. Formal: To make sure the review summary stays relevant and keeps the focus on the important parts, we add a special token called \">\" to the review's sentences. This token tells the model that the review is actually about the aspect we're looking at. This helps the model focus more on that part, which can lead to better results. Formal: Using a basic CNN with a softmax layer to handle all the review text. Formal: We also use a CNN to predict the review text, which is shown in Figure 2. This CNN takes the review text as input and predicts the summary based on the review text. Formal: We check how well the model does by measuring the Mean Reciprocal Rank (MRR) and the Mean Mutual Information (MMI) from the model's predictions. Formal: As for the domain model, we use a basic CNN model to predict the review text and the summary sentence. Formal: We measure how well the review summary matches the review text using the cosine similarity, like this: Formal: Lastly, we include the review text, the summary, and the results from the domain model in the final analysis to get a final score. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.335.json"
    },
    {
        "casual_text": "The third row compares our model to NO-SYNTAX, which is a stripped-down version of our model that uses lexical features but doesn’t include the syntactic structure. The results show that it performs better than the SENSORS-ONLY and RELATIONAL-RANDOM baselines but still falls short compared to our full system. This shows that the syntactic features our model uses help create better semantic representations of the text. Features like `empty(x1) ∧ freecell(x1)` are helpful because they reuse variables to make sure objects have important properties—in this case, making sure a freecell is empty. Other features, like `homecell(x1) ∧ value(x2, x3)`, help narrow down the search to useful combinations of predicates (in Freecell, whether you can play a card on a homecell depends on the card’s value). \n\nIt’s worth noting that three out of the 15 formulas are basically useless because they’re always false, like `card(x1) ∧ tableau(x1)`. This highlights how important it is to get the term assignment right to create useful features for learning. In the NO-SYNTAX system, which doesn’t consider the relationship between term assignment and syntactic structure, eight out of the top 15 formulas were useless because of term incompatibility.",
        "formal_text": "The third row compares against NO-SYNTAX, a crippled version of our model that incorporates lexical features but not the syntactic structure. The results are stronger than the SENSORS-ONLY and RELATIONAL-RANDOM baselines, but still weaker than our full system. This demonstrates the syntactic features incorporated by our model result in better semantic representations of the underlying text. Features such as empty(x 1 ) ∧ freecell(x 1 ) are useful because they reuse variables to ensure that objects have key properties -in this case, ensuring that a freecell is empty. Other features, such as homecell(x 1 ) ∧ value(x 2, x 3 ), help to focus the search on useful conjunctions of predicates (in Freecell, the legality of playing a card on a homecell depends on the value of the card). Note that three of these 15 formulae are trivially useless, in that they are always false: e. g. , card(x 1 ) ∧ tableau(x 1 ). This illustrates the importance of term assignment in obtaining useful features for learning. In the NO-SYNTAX system, which ignores the relationship between term assignment and syntactic structure, eight of the top 15 formulae were trivially useless due to term incompatibility.",
        "GPT2_formal_text": "_tgt. The attention mechanism is basically a mix of two approaches. First, there's the common skipgram embedding method, which we'll call BERT (bi-directional transformer), and then there's BERT-base (bi-directional transformer with extra dimension). Both of these use the attention mechanism. Formal: Check out Figure 1, which shows the attention mechanism. The bi-directional transformer is passed in the input (x, y) as the first layer. The bi-directional transformer with extra dimension is passed as the second layer, and the result is fed into the final output layer. This setup allows the model to work across different levels of abstraction, like layers or sub-layers. Formal: In this paper, we’ve created a Transformer-based model that’s flexible enough to handle any graph representation problem. Formal: The model takes an input graph (v, e), which has D parts, and turns it into a target-side representation (x, y). Formal: For the target side, the final layer looks at the entire input graph and decides which edges should be used. Formal: For the input graph, the outputs from the input layers are fed into a linear layer. Formal: The linear layer combines the outputs from the inputs to create a new input, which is the output from the target side. Formal: The goal of this graph setup is to predict a label y at each level. Formal: This goal is represented as a part of the matrix A_i, where i is the i-th input input token, and a_i represents the output for that token. Formal: A common approach in graph-based models is to treat the goal as a simple yes-or-no question, like a yes/no question (Yu et al., 2016). Formal: We’ve come up with a straightforward way to figure out if the graph has a relation. Formal: The graph setup we’re using is based on the perceptron algorithm from Luong and Choubey (2015). Formal: We’ve created an open-source package called Graph2Tree. Formal: The predicate for the relation we’re looking for is shown in the second sentence. Formal: The graph setup we’re using is based on the perceptron algorithm from Luong and Choubey (2015). Formal: We’ve created an open-source package",
        "directory": "D09",
        "filename": "D09-1100.json"
    },
    {
        "casual_text": "The HAPS segmenter uses something called factor graphs, which are a general way to represent things like Markov or Bayesian networks. A factor graph is like a two-part graph with two types of nodes: function nodes and variable nodes. Each function node is connected to the variable nodes that are its inputs. When you run the Max-Sum algorithm (a method described by Bishop in 2006) on a factor graph, it figures out the best set of variable values that gives the highest total of all the functions. This algorithm works by passing messages between nodes. Variable nodes send messages to the function nodes they're connected to, and function nodes send messages back to the variable nodes that are their inputs. A message from a variable node x to a function node f is calculated by adding up all the messages coming into x, except for the one from f itself.",
        "formal_text": "The HAPS segmenter is based on factor graphs, a unifying formalism for such graphical models as Markov or Bayesian networks. A factor graph is a bi-partite graph with two types of nodes, factor or function nodes and variable nodes. Each factor node is connected to those variable nodes which are its arguments. Running the well-known Max-Sum algorithm (Bishop, 2006 ) on a factor graph finds a configuration of variables which maximizes the sum of all component functions. This is a messagepassing algorithm. All variable nodes send messages to their factor neighbours (functions in which those nodes are variables) and all factor nodes send messages to their variable neighbours (their arguments). A message µ x→f sent from a variable node x to a function node f is computed as a sum of all incoming messages to x, except the message from the recipient function f:",
        "GPT2_formal_text": "= entity_ids(class) return entity_ids(class) end return class end\n           Casual: So, T = {T1, ..., TM} represents the total number of times we see a specific type of entity. This number is calculated based on how often an entity shows up in a certain dataset. Formal: We calculate the average number of times an entity appears in different datasets and then look at all of those instances to get a sentiment score. Formal: For each dataset D1, we check if there are any mentions of entity ei that have a sentiment polarity pol at least as strong as the polarity pol of the entity we're looking at. Formal: For each entity ei in D1, we calculate the sentiment polarity pol and the average sentiment polarity for mentions of it in all the datasets. Formal: We figure out how similar entity ei is to each mention in D1. Formal: We take the average of these similarity scores for each dataset. Formal: So, T = {T1, ..., TM} is the total number of times an entity appears in different datasets. Formal: For each dataset D1, we check if there are any mentions of entity ei that have a sentiment polarity pol at least as strong as the polarity pol of the entity we're looking at. Formal: Formal: We calculate the sentiment polarity pol and the average sentiment polarity for mentions of it in all the datasets. Formal: We figure out how similar entity ei is to each mention in D1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C14",
        "filename": "C14-1005.json"
    },
    {
        "casual_text": "We took a RoBERTa-Large model that was fine-tuned on a stained version of the MultiNLI training set. The whole MultiNLI dataset (train, dev-matched, dev-mismatched) was stained during the experiment. We also masked the stain for 10% of our training data to make sure those examples were part of the model's training, which allowed us to use these masked-stain examples later in our analysis. We ran the experiment three times, each time treating one of the three NLI classes as the stain. In all three runs, the models performed really well on the mismatched dev-set of the stained MultiNLI, with accuracy over 97%. This high performance was expected and shows that the models were definitely using the stain features to make predictions.",
        "formal_text": "We analyze a RoBERTa-Large model finetuned on the stained MultiNLI train set. The entire MultiNLI dataset (train, dev-matched, devmismatched) was stained during the experiment, and we masked the stain for 10% of our training data to ensure such examples are in distribution for the model, enabling us to use masked-stain examples in the analysis step. We repeat our experiment thrice, considering one of the three NLI classes as a stain each time. In all three cases, the stained models achieve high predictive mismatched dev-set performance on the stained MultiNLI (above 97% accuracy). This high performance is expected, and indicates that the models indeed exploit the stain features.",
        "GPT2_formal_text": "ed) and F_k for the k-th hidden layer in the bi-directional LSTM.\n\nFor the GAT model, the embeddings for the k-th hidden layer in the second Bi-LSTM layer are {y_k}_i = {y_k1, ..., y_kL}, where each y_k is a vector in R^m. Formal: We're using an attention mechanism that works at the text level, using some nonlinear projections. The model looks like this: Formal: We're using an attention mechanism that works at the text level, using some nonlinear projections. The model looks like this: Formal: The model's predictions are given as a sequence of vectors (y_i). Formal: The model's predictions are given as a sequence of vectors (y_i). Formal: The model's predictions are given as a sequence of vectors (y_i). Formal: The model's predictions are given as a sequence of vectors (y_i). Formal: The model's predictions are given as a sequence of vectors (y_i). Formal: The model's predictions are given as a sequence of vectors (y_i). Formal: Formal: The model's predictions are given as a sequence of vectors (y_i). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.120.json"
    },
    {
        "casual_text": "Not all unsupervised taggers actually create word categories. A lot of systems, called part-of-speech disambiguators (like the one Merialdo talked about in 1994), use external dictionaries with possible tags instead. Our project is based on two older methods for part-of-speech tagging—the word clustering algorithms from Clark (2000) and Brown et al. (1992). These methods have been found to be more reliable than other popular fully unsupervised techniques, according to Christodoulopoulos and his team in 2010.",
        "formal_text": "In turn, not all unsupervised taggers actually induce word categories: Many systems -known as part-of-speech disambiguators (Merialdo, 1994) rely on external dictionaries of possible tags. Our work builds on two older part-of-speech inducers -word clustering algorithms of Clark (2000) and Brown et al. (1992) -that were recently shown to be more robust than other well-known fully unsupervised techniques (Christodoulopoulos et al. , 2010).",
        "GPT2_formal_text": "\" to a space of size D. We can simplify this by turning it into a simple matrix called W by setting y_t = W X X +...+W Y +...+W Z = W X X. Then, we can use the formula to find the classifier h_t that works best for that specific query q. Formal: The relation-guided decoding algorithm takes a bit longer to run than the generalized query retrieval algorithm. This time difference is especially noticeable in the example with topic CVaR. The query model might produce a bunch of unrelated documents that might not even be relevant to the query, especially if the document collection is limited. Formal: To make sure the representations of each relation in the document collection are representative, we create a special vector called a_rel_i, which holds all the related entity vectors. This way, we can directly compare each relation to its related entity vectors. Formal: To match up the query with its related entities, we calculate the cosine similarity between the query and entity vectors of each relation. To do this, we use the L1-norm of the entity vectors to represent the query and the entity vectors of the relevant entity in the document collection. Formal: As mentioned in Section 1, the query is structured in a probabilistic way, which means it can generate both informative and informative-related documents. We use a structured prediction model (check out Figure 1) to generate the relevant documents. Formal: As shown in Figure 1, the query can also generate relevant documents by using a structured prediction model, which uses a separate, dynamic set of constraints to adjust the output. Formal: The query-generated documents are represented in a latent space, which helps us predict how likely each document is to be relevant to the query q. Formal: Finally, the relevance-guided pointer network, which helps us identify the query, can use this latent space to predict the relevance of the related entities in the document collection. Formal: In this paper, we’re introducing a new entity-based query that includes a pointer network. The goal is to generate both informative and informative documents. Formal: To improve the relevance of the query, we use two types of queries: those that focus on general topics (like \"geology\") and those that focus on specific entities (like \"web\"). Formal: As mentioned in Section 1, the query is structured in a probabilistic way, meaning it can generate both informative and informative documents.",
        "directory": "D11",
        "filename": "D11-1118.json"
    },
    {
        "casual_text": "Being chosen as a positive example. This probability is like the average importance of each word in shaping the final vector. That's how we use it when adding everything up in each model.",
        "formal_text": "to be selected as a positive sample. This probability can be seen as the expected weight of the contribution of each word to the final vector, which is how we apply it to the sum in each model.",
        "GPT2_formal_text": "ing_values = ({u, v, a})          Casual: The final output from the attention mechanism is the attention vector h_t. Formal: So, we're using a graph convolution layer to figure out how the words in a document are connected to each other. This is done by applying an attention mechanism. After that, we stick h_t together with the output from the attention mechanism to get the final attention vector h_t. Formal: If you want to know more about how to calculate the attention vectors, check out the paper by Lample et al. from 2016. Formal: We also come up with two different attention techniques that work with different types of data. Formal: Table 4 shows how well our models perform in terms of F1 scores on different data sets. The results are based on the test set from the SQuAD dataset. Formal: We tried two different versions of our model—one without any relation-labeling and another with it. Formal: We also included results for the model without any relation-labeling. Formal: Table 5 shows the average scores for different relation-labeling types. Formal: Lastly, Table 6 shows the average scores for the model without any relation-labeling. Formal: Formal: Table 7 is the average scores for all the relation-labeled sets. Formal: Table 8 shows the average scores for the model without any relation-labeling. Formal: Table 9 shows the average scores for all the relation-labeled sets. Formal: Formal: Formal: Table 10 is the average scores for all the relation-labeled sets. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-6104.json"
    },
    {
        "casual_text": "For instance, they take inputs (a, b) for recognizing textual entailment (RTE) and turn them into something like \"rte sentence1: a sentence2: b\". Then, the PLM is supposed to guess stuff like \"not entailment\".",
        "formal_text": "For example, they convert inputs (a, b) for recognizing textual entailment (RTE) to \"rte sentence1: a sentence2: b\", and the PLM is asked to predict strings like \"not entailment\".",
        "GPT2_formal_text": "= { u, v } n = {v|w = 1, . . . , w|w = |Q |}. Formal: We handle each word in the question by calling it a \"sentence token.\" Formal: The sentence tokens are the main part of the question, the main part of the answer, or both. Formal: For example, if someone asks, \"What is the price of a plane ticket between New York and Miami?\" the sentence token would be \"air ticket.\" Formal: For the main part of the question, the sentence token would be \"what is the price of a plane ticket between New York and Miami.\" Formal: For the answer, the sentence token would be \"what is the price of a plane ticket between New York and Miami.\" Formal: Formal: For the part of the question, the sentence token would be \"air ticket between New York and Miami.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.20.json"
    },
    {
        "casual_text": "Okay, so basically, all this stuff follows some rules that help us figure out things like how similar two groups of words are, or how important a word is in a sentence. For example, if you have a sentence with the word \"follow\" in it, and it's marked by a special rule, we can give it a score based on where it is in the sentence and how it relates to other words.\n\nThis method can help us compare two sets of words, even if they’re different lengths. First, we check how similar two words are by looking at exact matches, parts of words that match, and how close they are in a thesaurus. Then, we kind of combine all those similarities to see how similar the whole groups of words are.\n\nThe tricky part is that people often use slightly different words or phrases for the same idea, and it’s hard to catch all of those differences. Using a thesaurus and rules like the ones in Table 2 can help, but there’s still a lot of variety in how different those words or phrases can be. Figuring out how to handle all that will be something we work on in the future.\n\nFor rules about topic words, we give a score to the word marked by a square. And for matching words, \"X\" and \"x\" mean the words are the same or synonyms from this Japanese thesaurus. Same goes for \"Bulu'ui\" and \"Nagao\".",
        "formal_text": "All of these, are doue hy al}lflyiug rules COIISiSLillg O['; : t i}at, t, erll for a imrl, ial dep(md{, iwy st, rucl. ur{, and a score. l; 'or example, I)y I~ule-a mid I/ in Tal: , h~ 2, words hi a l)]n. ase whose head word is folio, wed I)y a topic umrl, : ills i)osl, posiI, ion \"\\VA\" are given sotlle scores tin (, el)i( and Nagao 19921>). '['his method ('. an calculate the overall similarity value h{~t. weexl Cwo word-strings of art)itrary leugths. First. , the similarity value between gwo words are cal{'ulal, ed a(: (: , . ~l'ding to exact matching, matching of {h{\"il' parts of Sl~ee(: h, aIKI their closeness in: -i thesaurus dicliouary. 'l'heu, the siruilarity wdue between two wor(I-+strillgs are calculat. ed roughly hy combining t. he similarity values bel. ween words in the two word-: q)H, ' clifllcult problem i~ that authc, rs often use subt. ly dil'fcrcm {'XlJl ', ~ssi, ms, n<, l, M<, ttlc, , d Wol'ds/i)hrases, fc)r such chains. \\Vhih~ s. tnP, of them can be caught by uMng a Ihesaurus and by rules like [hde-f in 'FaMe 2, ~here is a ', vMc range of variety in their diflbrences, Their complete trea. lment will be a. target of OIIl' fll|tLl'¢+ WOI']'~. As tk)r rules for topic/non-topic paris, t, he score is given to the hnnset. su ma. rked by; t square. As for rules for matching, \"X\" and \"x\" denote identical words or synonynlous words from this Jatmnese thesaurus, \"Bulu'ui (', el [[you\". So do llV+~ a. lld lly~.",
        "GPT2_formal_text": "ed_distribution (Liang et al., 2017) performs just as well as bidirectional attention, but it’s way faster. We’re comparing it to models like sparse bidirectional attention (Grave et al., 2016) and contrastive attention (Madotto et al., 2017). Formal: We’ve got some data in Table 3 to show how bidirectional attention, contrastive attention, and attention-based sparse bidirectional attention perform on the CoNLL-2015 dataset. We’ve set the dimensions for these models to 50, which is a common setting for sparse models. Formal: For the CoNLL-2015 dataset, we’ve added the attention weights from these methods. Formal: We’ve also included the accuracy scores for the top-1 model and the best model we got. Formal: We’ve included the results of a human evaluation for our models. Formal: Formal: To see how these models are affected by different numbers of hidden units, we’ve done a human evaluation on the CoNLL-2015 dataset. Formal: The results from this evaluation are in Table 4. Formal: From these results, we can see that the attention-based model is more sensitive to the number of hidden units, especially for the top-1 model. Formal: For the best model, attention-based sparse bidirectional attention has the worst accuracy, but contrastive attention does the best and gets the best accuracy from the best number of hidden units, which is 10. Formal: Lastly, we’ve included the evaluation results for a model that only uses both attention and attention-based bidirectional attention, and we’ve got those in Table 5. Formal: We’re just sharing the absolute best scores for these two models because the best one is based on the number of hidden units. Formal: Formal: The results from this evaluation are in Table 6. Formal: We’re just sharing the absolute best scores for these two models because the best one is based on the number of hidden units. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C94",
        "filename": "C94-2183.json"
    },
    {
        "casual_text": "Compared to the imbalanced version, the balanced one scored higher in BLEU for DE-EN but performed way worse in RO-EN for both the original label smoothing (LS) and MLS. Training on RO-EN examples seems to mess with the model's ability to generalize in RO-EN translation but doesn’t affect DE-EN quality. This might be because the RO-EN data introduces some bias that impacts DE-EN training. Even in an imbalanced setup, MLS still performs better (37.53) than the original LS in a balanced setup (37.44). This shows that MLS can help with the imbalance issue in multilingual translation. But the improvement in the high-resource direction (RO-EN) isn’t as big as in the balanced condition. We think label smoothing has a more complicated effect on multilingual models because of the extra languages and their relationships. We’ll leave that for future research. Based on the results, two things stand out: 1) Using WLS generally improves translation quality compared to the original LS. 2) Only WLS with β t, β c, β s set to 1/2-1/2-0 beats the original LS across all tasks, making it the most reliable setup.",
        "formal_text": "Compared with the imbalance version, the balanced version gave better BLEU scores in DE-EN direction while much worse performance in RO-EN translation for both the original label smoothing and MLS. training examples does weaken the generalization of model in RO-EN translation however doesn't influence the DE-EN translation quality since the RO-EN data might introduce bias to the training process for DE-EN translation. Even under imbalance condition, MLS can give a better performance (37. 53) compared to original LS in the balance condition (37. 44). It implies that MLS can relieve the imbalance data issue in multilingual translation. However, the improvement in relative high-resources direction (RO-EN) is not as significant as in the balanced condition. We guess that label smoothing has more complex influence on multilingual model due to the increase of languages and relation among different languages. We leave those questions for future exploration. According to the result, though the best BLEU score's WLS setting vary from different tasks and there seems to exist a more complex relation between the probability allocation and the BLEU score, we still have two observations. First, applying WLS can generally boost the quality of translation compared to the original label smoothing. Second, only WLS with β t, β c, β s each equals to 1/2-1/2-0 can outperform the original label smoothing on all tasks, which suggests the setting is the most robust one.",
        "GPT2_formal_text": "and att_ids are normalized. Formal: - p_z = log p(z) - p_w = log p(w) - p_e = log p(e) - p_z * log p(z) Formal: - p_w * log p(w) - p_e * log p(e) - p_z = p_w * log p(z) Formal: - p_e * log p(e) - p_z * log p(e) - Formal: - p_z * log p(e) - Formal: - p_e * log p(e) - Formal: - p_e * log p(e) - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal: - Formal:",
        "directory": "acl",
        "filename": "2022.acl-short.74.json"
    },
    {
        "casual_text": "From Lord et al. (1968, p. 70), we can understand this better. Basically, if we don’t know someone’s true skill level θ j yet, we should choose questions where we’re most unsure about how they’ll answer. Our uncertainty, or entropy, is highest when there’s an equal chance of getting the answer right or wrong, which happens when the value of I i (θ j ) is at its peak. It also makes sense that this value gets bigger as the question’s discriminability γ i increases.",
        "formal_text": "as derived by Lord et al. (1968, p. 70). Intuitively, if we do not yet know the true skill θ j, we should pick items whose expected response we are most uncertain about. Our uncertainty (entropy) is maximized when the likelihood of a correct re-sponse p ij is the same as the likelihood of an incorrect response 1 − p ij, which corresponds to the maximal value of I i (θ j ); it is also sensible this value increases as discriminability γ i increases.",
        "GPT2_formal_text": "ed is the output we get after the decoder is applied. As mentioned earlier, we tweak the attention weights for this specific encoding layer to make sure it can focus on the important parts of the input for each piece of input. But, we don’t want the decoder to end up predicting the same thing more than once for the same input. This is something the attention mechanism can’t handle. So, we use a simple linear interpolation between the decoder’s hidden state and the attention vector, based on the attention distribution. Formal: In each step, we use the attention distribution for the current input. The attention vector is a matrix, and it’s part of R²×d, where d is the dimension of the input token and the input attention vector. Formal: The decoder is trained with a regular cross-entropy loss, which is a bit more complicated than the regular cross-entropy loss. Formal: We adjust the logits for each token during training. Formal: We tweak the attention weights for this specific encoding layer to make sure they can focus on the important parts of the input for each piece of input. But, we don’t want the decoder to end up predicting the same thing more than once for the same input. This is something the attention mechanism can’t handle. Formal: We use a simple linear interpolation between the decoder’s hidden state and the attention vector, based on the attention distribution. Formal: We train the decoder with a regular cross-entropy loss, which is a bit more complicated than the regular cross-entropy loss. Formal: We adjust the logits for each token during training. Formal: We adjust the attention weights for this specific encoding layer to make sure they can focus on the important parts of the input for each piece of input. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.346.json"
    },
    {
        "casual_text": "To sum it up, we’ve come up with LMLM and rLMLM, which focus on telling good hypotheses apart from bad ones based on specific tasks, like WER or BLEU, instead of just trying to lower PPL.",
        "formal_text": "To summarize this section, we have proposed LMLM and rLMLM that aim at discriminating between hypotheses in a task-specific (e. g. , WER or BLEU) sense, instead of minimizing PPL.",
        "GPT2_formal_text": ", and the topic distribution. Then, we mix these two vectors together to get the final representation for the query, which we call the query vector. Formal: We used four datasets with different sentiment polarity labels: Ritter11, Ritter12, NNJM-2021, and NNJM-2022. The datasets have a total of 1,000,000 examples, each with a sentiment polarity label ranging from -1 to 1. We split the test set into 80% for training, 10% for validation, and 10% for testing. The target response was generated by using the generator settings we explained in Section 3.2. Formal: We also tested the system on four other datasets and found that it performs better on the others. Formal: To test how well the generator worked, we ran a full-sentence generator on the datasets using the full training set. This gave us the results in Table 2. Formal: To see how the generated responses matched up with the reference responses, we used the Ritter11 dataset as our reference and the other four datasets as our test set. Formal: We looked at the top-k words from the generator and compared them to the reference and reference-referred answers, then calculated the correlation between the generated responses and the reference responses. Formal: For the sake of comparison, we also included the Ritter12 dataset, which has a similar set of top words. Formal: We ran the generator on the Ritter12 dataset, which has a similar set of top words. The correlation was calculated using the Ritter12 dataset. Formal: Table 3 shows the results of the full-sentence generator. The results for the Ritter11 dataset are in bold, and for the other four datasets are underlined. Formal: We ran the generator on Ritter11, which has a similar set of top words. The correlation was calculated using the Ritter11 dataset. Formal: For the sake of comparison, we also included the Ritter12 dataset, which has a similar set of top words. Formal: Formal: The Ritter11 dataset has a similar set of top words. Formal: Formal: Formal: We ran the generator on Ritter11, which has a similar set of top words. The correlation was calculated using the Ritter11 dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1150.json"
    },
    {
        "casual_text": "We also looked at some gradient-based methods for comparison. Sensitivity Analysis (from Simonyan et al., 2013) just takes the absolute value of the gradient. Gradient*Input (by Shrikumar et al., 2016) is a straightforward multiplication of the input by its gradient, element by element. Integrated Gradients (Sundararajan et al., 2017) works by integrating gradients from a baseline input up to the current input. Since these gradient-based methods give relevance scores for each part of the word vector, we added up all the parts that belong to the same word. Keep in mind, these methods don’t take likelihood into account in NLP (check out Section 1 for more on that), so they’re just for comparison—not the ultimate benchmark.",
        "formal_text": "Furthermore, we compare with the following gradient-based methods. Sensitivity Analysis (Simonyan et al. , 2013) is the absolute value of the gradient. Gradient*Input (Shrikumar et al. , 2016) is simple component-wise multiplication of an input with its gradient. Integrated Gradients (Sundararajan et al. , 2017) integrate the gradients from a reference input to the current input. As these gradient-based methods provide relevance for every word vector value, we sum up all vector values belonging to a word. Gradient-based methods do not consider likelihood in NLP (see Section 1) and are thus also merely a comparison and not a gold standard.",
        "GPT2_formal_text": "ing method. The main idea behind it is to figure out how important each word in the input is for generating the output sequence. We think the importance of each word in the input is different depending on the task, and we take into account the context of the text. To do this, we use two different methods: the hidden Markov Model (HMM) and the Conditional Random Field (CRF). The CRF method, introduced by Collobert and his team in 2011, tries to guess what the next word in the input should be. The HMM, on the other hand, predicts the word based on its context, which helps us decide the importance of the words. Formal: Here’s the equation that explains the network setup we’re using in this project: Formal: In this setup, we’re using a bidirectional GRU (Bi-GRU) with a hidden size of 5. This setup helps us predict the importance of each word in the input. Formal: Also, we’ve got two extra linear layers here. This layer gives us the probability for the word’s output. Formal: The model’s parameters, f and g, are both learned as hidden states from the decoder, which are then combined with the input word’s vector h. Formal: To make sure our prediction is unbiased, we’re running the model 100 times with different random seeds and averaging the results. Formal: For each token in the input, we calculate a cross-entropy loss with a weight vector that’s proportional to the number of times the token shows up. Formal: We also calculate the cross-entropy loss with the label distribution we’re trying to predict. Formal: In this situation, the input has multiple tokens and the goal is to predict one label for each token. Formal: To make sure the parameters are unbiased, we’re running the model 100 times with different random seeds and averaging the results. Formal: The first part of the equation is the prediction for each token. The second part is the prediction for the label distribution we’re trying to predict. Formal: The second part of the equation is the prediction for each token. Formal: Formal: Formal: We’re using a multi-head GRU (Bi-GRU) with a hidden size of 5. Formal: Here, v_i is the",
        "directory": "acl",
        "filename": "2020.acl-srw.16.json"
    },
    {
        "casual_text": "We've got a cool thing going on: we’re sharing the full text of 8.1 million papers that are open access, and we’ve made it super easy for machines to read. The S2ORC full text keeps all the important stuff intact, like paragraph breaks, section titles, mentions of citations within the text, and even links to tables, figures, and other papers. Plus, we’ve got 1.5 million papers where we’ve pulled out the full text in LaTeX format, so you also get the actual content of tables and math formulas, along with citations and references. Check out Table 1—it shows that S2ORC has way more structured full text papers and covers a wider range of subjects than other resources out there.",
        "formal_text": "Notably, we release structured, machinereadable full text extracted from PDFs for 8. 1M papers which we've identified as having open access status. S2ORC full text preserves meaningful structure, e. g. , paragraph breaks, section headers, inline citation mentions, references to tables and figures, and resolved citation links to other papers. Additionally, we provide 1. 5M full text LATEX parses from which we have extracted, in addition to citations and references, the source text of tables and mathematical formulas. As shown in Table 1, S2ORC provides substantially more structured full text papers and covers a more diverse set of academic disciplines than other resources.",
        "GPT2_formal_text": "= (c_i, a_i) to get the i-th response. Formal: The problem we're tackling here is a bit different from the usual text generation (text generation) tasks, which usually focus on sentence ordering or creating a summary from a bunch of documents. Instead of that, we're dealing with generating text to summarize conversations. Formal: To handle this, we’ve come up with a new way to measure how well the summarization works, called the Dialogue Quality Index (QI). We’ve set up some parameters to make this happen. Formal: First off, we’ve figured out the natural language query (q_i) and the document’s ID (d_i). Then, we calculate the dialogue quality score (Q_i) using the document’s relation info (m_i). Finally, we figure out the chance of getting the right answer (p_i) by looking at how likely the query and document are related. Formal: To break it down, the QI score is calculated as q_i + d_i divided by the sum of q_i and d_i, or q_i + d_i multiplied by the probability p_i. This probability is calculated using a probabilistic model with L2 regularization, and we’ve set up some parameters to handle it. Formal: So, the QI is calculated by taking the sum of the Q_i and the probability p_i for each document and query. Formal: The formula for the document and query are: Formal: Finally, we use a new dataset called dialogues.txt to pick dialogues that are likely to generate good responses. Formal: Formal: We’ve tested our method on two different datasets and measured how well it works based on how well the generated responses fit the query. Formal: The results are in Table 2. The top-performing model, called GPT, scored 78.43% on the CNNDM dataset and 73.06% on the GPT-2 dataset. Formal: Table 3 shows that our method is way better than the others. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.447.json"
    },
    {
        "casual_text": "Revere mentioned they got an offer from a group of investors who want to buy the company for $16 per share, which adds up to around $127 million.",
        "formal_text": "Input: Revere said it had received an offer from an investor group to be acquired for $16 a share, or about $127 million.",
        "GPT2_formal_text": "ing 0.1 h 0.3 Figure 1: The development set for the task we're looking at. In the first row, you'll see the dataset D, and the second row shows the final results. We're also sharing the teacher's hyperparameter choices. Formal: Using a simple linear classifier, we get the attention distribution p ∈ R |D|p (D|p). Then, we calculate the attention score h ∈ R |D|h. Formal: Using a transformer model, we get the attention distribution p ∈ R |D|p (D|p). In the second row, we're checking how well the model does with this attention distribution. Formal: For the last row, we're trying to predict the attention distribution. Formal: So, p ∈ R |D|p (D|p) is the attention distribution we get for the target token t, and the teacher's hyperparameters, p. Formal: Finally, we calculate the average attention weight θ, which is just a fancy way of saying the weighted average of all the attention contributions. Formal: Check out Figure 1: The development set for the task we're focusing on. Formal: For the last row, we're trying to predict the attention distribution. Formal: For the second row, we're trying to predict the attention distribution. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C90",
        "filename": "C90-2034.json"
    },
    {
        "casual_text": "For the topic segmentation task, we want to create a way to measure how similar words are, even if they're synonyms or have some kind of related meaning. This similarity measure will help us check how well different parts of the text fit together in terms of meaning. If the meaning starts to feel disconnected, that's when we'll know there's a shift in the story. To make this work, we'll first need to build a way to represent the document that supports this similarity measure.",
        "formal_text": "For the topic segmentation task we would like to define a similarity measure that accounts for synonymy and semantic association between words. This similarity measure will be used to evaluate semantic cohesion between text units and the decrease in semantic cohesion will be used as an indicator of a story boundary. First, we develop a document representation which supports this similarity measure.",
        "GPT2_formal_text": "= [ id for i in range(len(text_query) - 1), text_query ] text_query = [ text for i in range(len(text_query) + 1), ] t = [ t1, t2, ..., tT ] for i, j in text_query: t = [ t1, t2, ..., tT ] if (i + j) in text_query: t = [ t1, t2, ..., tT ] if (i - j) not in text_query: t = [ t1, t2, ..., tT ] else: t = [ t1, t2, ..., tT ] text_query = [ text for i, j in text_query, ]\n\nBasically, this is saying that the query text and its answers should be between T and T + 1, which is the length of the query and the total number of answers in the query. Formal: This is the formula for the Viterbi algorithm. Formal: This algorithm looks for the best sequence of solutions where the sequence of scores equals a set of keyphrases that match the keyphrases of the document. Formal: The algorithm checks if the sequences of scores for each keyphrase in the document match those in the query text. Formal: It finds the best possible solution where the keyphrases match, along with the scores for each document. Formal: The algorithm then calculates the similarity between these two sequences. Formal: This algorithm finds the best solution where each keyphrase in the document matches one or more keyphrases in the query text. Formal: Finally, it calculates the similarity between these two sequences using the formula we talked about. Formal: Formal: This algorithm finds the best solution where the query text matches, along with the sentences in the document. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D07",
        "filename": "D07-1037.json"
    },
    {
        "casual_text": "The first experiment gave us a bunch of useful facts for travelers, like a destination's weather, laws, and how things work there. But it didn’t really cover the cultural stuff. Now, we’re curious to see if we can dig deeper into that by looking at text written by people who actually live in those countries. This way, we can see what they talk about and how they talk about it, which might give us a better idea of the culture.",
        "formal_text": "The results of the first experiment offer an unsupervised aggregation of factual information that is important to travelers such as a destination's climate, law, and infrastructure; however, the data did not offer much in terms of cultural information. We would now like to see if we can get better insight into this problem by modeling text authored by residents of these same countries. In doing this we can compare what they talk about and in what manner they talk about certain topics.",
        "GPT2_formal_text": "_tgt = max_pooling(V, A)           Casual: We used the same training set for all the models. The model with the best average perplexity on the validation set got a loss of 0.43, while the other two models had losses of 0.00 and 0.13. \n\nWe trained all the models with the same hyperparameters, but we took the average of their loss. After testing, we found that MLE loss was a better way to measure performance. Check out Table 1 for more details. Formal: We ran our experiments using the same development set, the same model sizes, and the same training set for all the models, except for the last one. Formal: We ran our experiments with different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We used different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We used different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We used different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We ran our experiments with different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We ran our experiments with different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We ran our experiments with different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We ran our experiments with different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We ran our experiments with different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We used different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We ran our experiments with different hyperparameters for the last model, but for the other models, we used MLE loss to evaluate their performance. Formal: We used different",
        "directory": "D09",
        "filename": "D09-1146.json"
    },
    {
        "casual_text": "We look at how different fixed budgets for creating training data impact different languages and offer some tips for people planning to make datasets in the future.",
        "formal_text": "• we study the effect of different fixed-budget allocation for training data creation across languages, making suggestions for future dataset creators.",
        "GPT2_formal_text": "= [rj, rj-1, ..., rm, rm-1, ..., rm-m]\n            Casual: The structure of the cluster information structure looks something like this: [rj, rj-1, ..., rm, rm-1, ..., rm-m] (for example, in Figure 3). Each of these nodes has a few key pieces of info: the main entity (e_i), its local context (e_i-1), a key-value pair (e_j) that represents the relationship between the entities, and the total number of entities (m). Formal: A word's category (e_i) is what makes it a noun (n_i), a verb (v_i), or a verb phrase (v_i-1). A cluster's classification (e_j) is how the cluster labels the word's local context. The cluster entity category (e_j-1) is how the word itself is assigned to the cluster. Formal: Also, the node's \"type\" is the type of the word it represents. Formal: We use the standard ISD algorithm to do this. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-short.79.json"
    },
    {
        "casual_text": "We take a look at some basic stuff for each word in a sentence. For things like SRL and RC, we usually check out the word itself (or its base form if we have it) and its part of speech. For RC, we might also throw in some extra details, like what kind of named entity it is or use WordNet. All these bits of info get turned into small, easy-to-handle numbers, which we call feature embeddings. You can already get word embeddings ready-made by using word2vec on a big pile of text without labels, and they’ve been pretty useful in lots of projects. After that, we mix all these feature embeddings together in a fancy way, so each word can be represented like this:",
        "formal_text": "We extract basic lexical features for each token in a sentence. Typical lexical features for SRL and RC include word (or lemma when available) and POS tag. For RC, additional features can be used, such as named entity type (NE) and WordNet. All these features are then represented as low-dimensional realvalued vectors, i. e. , feature embeddings. Word embeddings can be readily pretrained using word2vec on a large unlabeled corpus, which have proved helpful in many applications. Next, various feature embeddings are composed through a nonlinear transformation, and thus a token can be represented as:",
        "GPT2_formal_text": "ed. This means the masked tokens are always the same length but don't have any special codes to keep them distinct from the original input. After that, we add these special codes to the original input to make sure the current token matches up with the masked version. Formal: The version of the output sequence (let's call it h) that's closest to the masked version (h') depends on the key, a_i. Formal: For each target key, we figure out how much the original token (h') overlaps with the hidden state of the target key (s_i). Formal: Finally, we add h' to h to get the final version of the output sequence. Formal: Formal: To create a translation model using all these masked tokens, we can use a gated recurrent unit (GRU) as the output layer. This GRU predicts the next token in the sequence, which is the target key's hidden state. Formal: Formal: In this setup, we use a gated unit to predict the output. Formal: Formal: This process continues until the output isn't the same as the hidden state of the target key. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1120.json"
    },
    {
        "casual_text": "So, let's break it down: \"a_i\" is the i-th atom in the compound C, N is just how many atoms are in the compound, M is the total number of ways you can pair up two atoms, and the PMI score is calculated using this formula:",
        "formal_text": "where a i is the i-th atom in the compound C, N is the compound length, M is the number of possible combinations of two atoms, and PMI score is computed as:",
        "GPT2_formal_text": "= [ {m, j}, {n, i}, {m, j+1} ] ) . )\n          Casual: Alright, so we're taking the loss for each token and adding it up for each of the input tokens, like this: Formal: We do this calculation to get a single feature representation for each token. Formal: At each step, the model learns these feature representations. Formal: We use the attention mechanism (like the one by Bahdanau et al., 2015) to figure out these representations. Formal: Then, we use a regression model to predict the token labels, which we can do for each token or for an entire sentence. Formal: We keep doing this for all the tokens in the sentence, like in the sequence shown in Fig. 1. Formal: The token labels are basically the average of the model's predictions for that token. Formal: We use the cross-entropy loss (from Mikolov et al., 2013) to evaluate how well the model is doing. Formal: The cross-entropy loss for a single token is figured out by adding up the cross-entropy losses for all the tokens in the sentence. Formal: This whole process is shown in Fig. 2, where we calculate the cross-entropy loss for each token to calculate the feature representation for that token. Formal: Formal: We use the cross-entropy loss for each token to calculate the feature representation for that token. Formal: After each step, the model decides on the label for that token. Formal: Formal: Finally, the label for the token is figured out by adding up the cross-entropy losses for all the tokens in the sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.368.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Here's a list of sounds and some info about them:\n\n**Sounds:** p, b, f, v, m, 8, 4, t, d, s, z, c, n, S, Z, C, j, T, 5, k, g, x, N, q, G, X, 7, h, l, L, w, y, r! V\n\n**Voiced or Not:**\n- Voiced: b, v, m, 8, 4, d, z, n, S, Z, C, j, T, 5, g, N, q, G, X, 7, l, L, w, y, r! V\n- Not Voiced: p, f, t, s, c, x, h\n\n**Place of Articulation:**\n- **Labial:** p, b, f, v, m, 8, 4\n- **Dental:** t, d, s, z, c, n\n- **Alveolar:** t, d, s, z, c, n\n- **Palatal/Post-alveolar:** S, Z, C, j, T, 5\n- **Velar:** k, g, x, N, q, G, X, 7\n- **Uvular:** q, G, X, 7\n- **Glottal:** h\n\n**Manner of Articulation:**\n- **Stop:** p, b, t, d, k, g, q, G, X, 7\n- **Fricative:** f, v, s, z, S, Z, C, x, h, r! V\n- **Affricate:** C, j, T, 5\n- **Nasal:** m, n, N\n- **Click:** None\n- **Approximant:** l, L, w, y, r! V\n- **Lateral:** l, L\n- **Rhotic:** r! V\n\nSo, basically, each sound has its own characteristics like whether it's voiced or not, where it's made in the mouth, and how it's produced.",
        "formal_text": "Features p b f v m 8 4 t d s z c n S Z C j T 5 k g x N q G X 7 h l L w y r! V Voiced 0 1 0 1 1 1 1 0 1 0 1 1 1 0 1 0 1 1 0 0 1 1 1 0 1 1 0 1 1 1 1 1 1 1 1 Labial 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 0 Dental 0 0 1 1 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Alveolar 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Palatal/Post-alveolar 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 Velar 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 0 0 0 0 0 0 0 1 0 0 0 0 Uvular 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 0 0 0 0 0 0 0 0 0 Glottal 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 Stop 1 1 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 1 0 1 1 0 0 1 1 0 1 0 0 0 0 0 0 0 0 Fricative 1 1 1 1 0 1 0 0 0 1 1 0 0 1 1 0 0 0 0 0 0 1 0 0 0 1 0 1 0 0 0 0 0 0 0 Affricate 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 Nasal 0 0 0 0 1 0 1 0 0 0 0 0 1 0 0 0 0 0 1 0 0 0 1 0 0 0 0 0 0 0 0 0 0 0 0 Click 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 Approximant 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 1 1 0 0 0 Lateral 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 Rhotic 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0",
        "GPT2_formal_text": "ing. [13] t1: s = (1.0, 0.7, 0.5) t2: s = (1.0, 0.7, 0.5)\n\nBasically, this is saying that s should be at least 0.7 and should also fall between 0.5 and 1.0, while also keeping the frequency to 1.0. Formal: For the non-overlapping part of attention, we use the Frobenius norm in the softmax layer. The attention weights are calculated using the Transformer layer. Formal: This part is pretty much the same as in the original paper. Formal: Attention-based attention helps the model focus on important parts of the input and can be applied in different ways. Formal: So, the teacher network T T is kind of like a teacher for the student network T S, and the student network T S is like a student for the teacher network T T. Formal: The importance of a word w i in a sentence s i can be measured using the attention weights T T. Formal: To make the model act like a student, we add an attention term to it, and the student network can focus on this term. Formal: This means the student network can learn to provide feedback based on the attention vector T. Formal: Finally, the attention vector T can be written as T = (T s, T T t ). Formal: Formal: Here's how it works in simpler terms: Formal: In this paper, we created a new attention system using softmax. The attention weights are calculated using the Transformer layer. Formal: The attention weights T are generated by a teacher network T T T. Formal: We also use a linear layer on the student network T S to help it learn better by learning from the teacher network T T S. Formal: For the student network T S, we treat it as a student and calculate the importance of each word w i using the attention weights T. Formal: The attention vector T can be written as T = (T s, T t ). Formal: Formal: Formal: Formal: Here's how we combine the student network T S with the teacher network T T T to improve its performance. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1097.json"
    },
    {
        "casual_text": "Basically, it tries to make the difference between a uniform distribution across K classes (let's call it U(l)) and what the discriminator predicts as shown in equation (3) as small as possible. The final loss for the QA model is a combination of the regular loss (L QA) and an extra loss (λL adv) that depends on how important we think the adversarial part is, with λ being the knob we turn to adjust that importance. During our tests, we switch back and forth between improving the QA model and the discriminator.",
        "formal_text": "In other words, it minimizes Kullback-Leibler (KL) divergence between uniform distribution over K classes denoted as U(l) and the discriminator's prediction as in equation (3). Then the final loss for QA model is L QA + λL adv where λ is a hyper-parameter for controlling the importance of the adversarial loss. In our experiments, we alternate between optimiz- ing QA model and discriminator.",
        "GPT2_formal_text": "\" } }, { \"type\" : \"coding\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"coding\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ ] ] ] } ] } ] } ] } ] } ] } ] } ] } ] } ] } ] ] ] } ] ] ] } ] ] } ] ], \"type\" : \"text\", \"text_id\" : \"73751534184516411232\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ ] ] } ] } ] } ] } ] } ] } ] } ] ] } ] ] } ] } ] } ] } ] ] } ] } ] } ] } ] } ] } ] } ] } ] ] } ] }, { \"type\" : \"text\", \"text_id\" : \"73751534184516411232\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\" : [ { \"type\" : \"text\", \"input_ids\"",
        "directory": "D19",
        "filename": "D19-5826.json"
    },
    {
        "casual_text": "For all our experiments, we keep the weights (from Press and Wolf, 2017) of the encoder embedding, decoder embedding, and decoder output layers tied together. This really helps cut down on the number of parameters and speeds up training time. We train for up to 20 epochs and pick the checkpoint with the best oracle metric. We use the Adam optimizer (Kingma and Ba, 2015) with a learning rate of 0.001 and momentum parameters β1 = 0.9 and β2 = 0.999. The minibatch size is 64 for question generation and 32 for abstractive summarization. All the models are built using PyTorch (Paszke et al., 2017) and trained on a single Tesla P40 GPU, running on the NAVER Smart Machine Learning (NSML) platform (Kim et al., 2018a).",
        "formal_text": "For all experiments, we tie the weights (Press and Wolf, 2017) of the encoder embedding, the decoder embedding, and the decoder output layers. This significantly reduces the number of parameters and training time until convergence. We train up to 20 epochs and select the checkpoint with the best oracle metric. We use Adam (Kingma and Ba, 2015) optimizer with learning rate 0. 001 and momentum parmeters β 1 = 0. 9 and β 2 = 0. 999. Minibatch size is 64 and 32 for question generation and abstractive summarization. All models are implemented in PyTorch (Paszke et al. , 2017) and trained on single Tesla P40 GPU, based on NAVER Smart Machine Learning (NSML) platform (Kim et al. , 2018a).",
        "GPT2_formal_text": "ing (from Devlin et al., 2019) is a cool new dataset that mixes general and domain-specific distillation, allowing researchers to mix different models for multiple tasks. In this experiment, we looked at how well it works for specific tasks. We used the same fine-tuning method as in the original TAC-KBP 2019 paper. We trained our model with one epoch using the cross-entropy loss function and a batch size of 32, and we ran it for 10 epochs. After each iteration, we checked how well the model performed by calculating the average accuracy (Acc) and macro-F1 score (F1). The results are shown in Table 3. Turns out, when we train on more tasks, the model's performance gets better. But in general, the performance goes down for all tasks, especially when the tasks are pretty similar. Formal: This study introduces a new task-specific learning method for fine-tuning for specific tasks. It uses a cross-entropy loss function and a batch size of 32 to gradually improve performance over time. Formal: Formal: We trained a teacher model using the cross-entropy loss and a batch size of 32, and this approach was tested on different tasks. Results are in Tables 2 and 3. Formal: We tried two versions of this task-specific learning method: one where we just fine-tune the teacher model directly without any adjustments (called Fast-Tune), and another where we trained the teacher using multiple batches of data that was adjusted by the student model (called Fast-Seq). The results in Tables 2 and 3 show that Fast-Seq works better than Fast-Tune, especially when the tasks are similar. Formal: We tested the Fast-Seq version on two tasks: image captioning and classification. Results are in Tables 4 and 5. Formal: We tested Fast-Seq on two tasks: image captioning and classification. We treated the other two tasks as regular tasks. Formal: This task-specific learning method has some cool advantages. First, it lets us compare different models and see how they perform in different tasks. Second, by training the student using a mixed data setup, we can see how each part of the model contributes, especially when the tasks are different. Formal: We tested Fast-Seq on three tasks: image captioning, classification, and text generation. We treated the other three tasks as regular tasks. Formal: This paper focuses on",
        "directory": "D19",
        "filename": "D19-1308.json"
    },
    {
        "casual_text": "The c-test (Raatz and Klein-Braley, 1981) is a common exercise used to test language skills. In this test, the second half of every other word is missing, and you have to fill in the blanks to complete the text. For instance, the word \"redundancy\" would become \"redun___.\"",
        "formal_text": "The c-test (Raatz and Klein-Braley, 1981 ) is a reduced redundancy exercise frequently used for language proficiency testing. In a c-test, the second half of every second word is replaced by a gap, and the task of the test-taker is to reconstruct the text. For example, the word redundancy would be replaced with redun___.",
        "GPT2_formal_text": "= [q, a_t]. Each entry in the set has two parts: one for the model itself and one for the input_ids. Formal: In this paper, we looked at how to handle long documents that have lots of complex terms. We use a method called K-Means clustering to group these terms and pick the best one to represent the whole document. By doing this, we can get better performance for the term representation. Plus, we could also use a hierarchical clustering approach, where the terms are kept in groups based on how similar they are. Formal: We focused on finding the best possible word for a given sentence and its input_ids. We did this by looking at how similar the terms are to each other and then using something called K-Means clustering. Formal: The key thing to remember is that we don't assume that the terms are independent. Formal: We started by looking at a few specific cases, which we call the \"gold standard.\" These are the terms that were found to have a high similarity to a specific sentence or its input_ids. Then, we used a method called L1-Regularized Linear Smoothing (LRLS) to group these terms together into different groups, making sure they're not too far apart. Formal: As we kept expanding our list of possible terms, we realized that the ones that were super similar to the current set might also be super similar to the previous set. Formal: We also noticed that some terms in the current set might not be very similar to the ones in the previous set. So, we came up with a new set of terms that are more likely to be related to the current set than the previous set. Formal: We picked a group of these terms using a method called K-Means clustering. We used a model that gives us a probability p(t) that a term is related to the current set, as well as a weight to help decide how much weight to give to the words that are related to the current set. Formal: The algorithm we used for this is shown in Algorithm 1, and it takes O(n^6) time to run. Formal: We found a number that works best for this case. Formal: We tried using a few different methods for finding the best terms. First, we used a distance-based method based on cosine similarity to see how similar two terms were. Then, we looked at what distance between",
        "directory": "bea",
        "filename": "2021.bea-1.19.json"
    },
    {
        "casual_text": "Deep neural models are kind of famous for being easy to trick with something called adversarial perturbations—basically, tiny changes to the input that can mess with the model's output, even if the model was trained really well (Goodfellow et al., 2015; Ebrahimi et al., 2018b). There's another type of attack where someone messes with the model itself to create fake weaknesses, so the attacker can control how the model reacts to certain changes. For example, they might tweak the model to always say a sentence has a positive sentiment, even if it’s totally negative. Like, \"This is a train wreck of an action film—a ridiculous attempt by the filmmakers to cram James Bond into the dumb XXX BB mold and throw 40 years of movie history out the window for some flashy lights and loud explosions.\"",
        "formal_text": "Deep neural models are well-known for being naturally susceptible to adversarial perturbations -carefully-chosen modifications of inputs that allow one to control the output of an otherwise well-trained model (Goodfellow et al. , 2015; Ebrahimi et al. , 2018b). Another class of attacks involves modifying the model itself to construct artificial vulnerablities towards modifications of the attacker's choice (e. g. always classifying a sentence as positive sentiment when the attacker in-Sentence Confidence this is a train wreck of an action film -a stupefying attempt by the filmmakers to force-feed james bond into the mindless xxx bb mold and throw 40 years of cinematic history down the toilet in favor of bright flashes and loud bangs.",
        "GPT2_formal_text": "ed_2d is calculated as the sum of the attention weights for the first and second hidden layers. So, the regularization term is basically equal to the total of all these attention weights. Formal: We can't use the multi-head attention mechanism because it has a different structure compared to the regular attention mechanism. For instance, during training, the attention weights in the output layer might not be the same as the ones in the hidden layers. Formal: The multi-head attention mechanism for entity recognition is shown in equation (7), and we calculate the attention weights for the first and second hidden layers by using equation (3). Formal: The multi-head attention mechanism for relation classification is shown in equation (5). Formal: To calculate the attention weights for the first and second hidden layers, we multiply the attention weights in the output layer by the attention weights in the second hidden layer. Formal: Similarly, for the third hidden layer, we multiply the attention weights in the output layer by the attention weights in the third hidden layer. Formal: We also calculate the attention weights for the third and fourth hidden layers by using equation (6). Formal: Formal: The equations (8) and (9) explain how multi-head attention and multi-mode attention work together in relation classification. Formal: Formal: The multi-head attention mechanism for relation classification is explained in equation (6). Formal: The multi-head attention mechanism for relation classification is also explained in equation (5). Formal: Formal: Formal: The multi-mode attention mechanism for relation classification is explained in equation (3). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.249.json"
    },
    {
        "casual_text": "When we include that one weird entry, Figures 2 and 3 show the average human score for each of the seven NIST entries compared to their Bleu score. What stands out is that one entry got a way higher human score than you'd expect based on its low Bleu score. This entry was different because it wasn't a fully automatic machine translation. Instead, it had some help from English speakers who picked the best options from different automatic translations of Arabic sentences and then edited the result (Callison-Burch, 2005). The other six entries were all fully automatic, using phrase-based statistical machine translation. They were trained on the same data and most of them used a Bleu-based method (Och, 2003) to tweak the settings of their translation models (Och and Ney, 2002).",
        "formal_text": "14 when the outlier entry is included Figures 2 and 3 plot the average human score for each of the seven NIST entries against its Bleu score. It is notable that one entry received a much higher human score than would be anticipated from its low Bleu score. The offending entry was unusual in that it was not fully automatic machine translation; instead the entry was aided by monolingual English speakers selecting among alternative automatic translations of phrases in the Arabic source sentences and post-editing the result (Callison-Burch, 2005). The remaining six entries were all fully automatic machine translation systems; in fact, they were all phrase-based statistical machine translation system that had been trained on the same parallel corpus and most used Bleubased minimum error rate training (Och, 2003) to optimize the weights of their log linear models' feature functions (Och and Ney, 2002).",
        "GPT2_formal_text": "= {t_i, s_i, s_k, . . . , t} n_sk = {t_i, s_i, s_k, . . . , t} δ = {w_t, b_t, δ}\n\nAt the start, the parameters are set to {t_i, s_i, s_k, . . . , t} n_sk = {t_i, s_i, s_k, . . . , t} δ = {w_t, b_t, δ}\n\nWe then update the model like this: Formal: We’ve got two types of task-specific embeddings: Formal: The input sentence X and the target output Y are represented as vectors in R^n. Formal: The input and output embeddings X and Y are similar in size. Formal: The time step t is also represented as a vector. Formal: In each round of decoding, the model learns to generate the target word y. Formal: The target embedding δ is trained using a mini-batch of data with a window size of t = 0. Formal: We set the softmax activation function to 0. Formal: We also use a tanh function to smooth out the probability distribution P_y(y|x). Formal: To optimize the objective, we use the SVM loss function. Formal: A binary distribution for the target word y is created by summing over the probability of all the words in the input sentence. Formal: The target embedding δ is trained by a mini-batch of data with a window size of t = 0. Formal: We also use a tanh function to smooth out the probability distribution P_y(y|x). Formal: We set the softmax activation function to 0. Formal: The objective function is trained using a mini-batch of data with a window size of t = 0. Formal: We set the softmax activation function to 0. Formal: The target embedding δ is trained by a mini-batch of data with a window size of t = 0. Formal: We also set the softmax activation function to 0. Formal: The objective function is trained using a mini-batch of data with a window size of t = 0. Formal: Finally, we calculate the cross-entropy",
        "directory": "E06",
        "filename": "E06-1032.json"
    },
    {
        "casual_text": "(a) A single, neat model that works across different sets of semantic graphs (check out section 2), (b) consistent ways to represent and score stuff (sections 4 and 6), (c) comparing how different methods perform (section 5), and (d) more mixing and matching of different parsing techniques (section 7).",
        "formal_text": "(a) a unifying formal model over different semantic graph banks ( §2), (b) uniform representations and scoring ( §4 and §6), (c) contrastive evaluation across frameworks ( §5), and (d) increased crossfertilization of parsing approaches ( §7).",
        "GPT2_formal_text": "ing function. The main idea is to create a fake graph by combining the vector representations of the entities in the sentence and the question-answer pair. So, the input to the attention layer looks like this: Formal: • Equation (2) Formal: • Equation (3) Formal: • Equation (4) Formal: • Equation (5) Formal: • Equation (6) Formal: • Equation (7) Formal: • Equation (8) Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "conll",
        "filename": "2020.conll-shared.1.json"
    },
    {
        "casual_text": "As an alternative to the second jump rules, we're testing out a method based on (Huck et al., 2011) that limits reorderings to a pretty strict level. In this version, you can only jump over one continuous chunk of a sentence at a time.",
        "formal_text": "As a second jump rules variant, we try an approach that follows (Huck et al. , 2011) and allows for very constrained reorderings. At most one contiguous block per sentence can be jumped across in this variant.",
        "GPT2_formal_text": "\" (check out Appendix D.4). Formal: One way to handle this is by using a weighted averaging method to average out the scores for the scores that come before. Formal: Another way is to add up the scores for the scores that come after. Formal: Another option is to combine both methods, but that's kind of a pain to compute, and you'd have to do it all at once. Formal: Formal: Lastly, you can mix all these scores together to get the final score for the input, like: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2012.eamt-1.66.json"
    },
    {
        "casual_text": "In this project, we introduce DISCO, a self-supervised contrastive learning approach that helps in understanding general code patterns and also pinpoints specific traits for detecting vulnerabilities and code clones. Our tests show that even when DISCO is trained on smaller datasets, it can still beat the performance of bigger models, which proves how effective our design is. Just to clarify, these two tokens are keywords, but by looking at their parent node type, we can quickly tell they're part of the same if-statement and are siblings in the AST.",
        "formal_text": "In this work, we present DISCO, a self-supervised contrastive learning framework to both learn the general representations of source code and specific characteristics of vulnerability and code clone detections. Our evaluation reveals that DISCO pretrained with smaller dataset can still outperform the large models' performance and thus prove the effectiveness of our design. just know these two tokens are keywords, but with parent node type, we can easily know that they are from the same if-statement and they are siblings in the AST.",
        "GPT2_formal_text": "ed(h_i) = h_i + 1 * attention_mask_len(h_i) **kw = [h_i, l_t] * [h_i, l_t, l_t, h_i, l_t] + [h_i, l_t] + [h_i, l_t] * [h_i, l_t, l_t, l_t] + [h_i, l_t] + [h_i, l_t] * [h_i, l_t, l_t, l_t, l_t] + [h_i, l_t] * [h_i, l_t, l_t, l_t, l_t, l_t] + [h_i, l_t] * [h_i, l_t, l_t, l_t, l_t, l_t] + h_i + 1 * h_i, l_t] * [h_i, l_t, h_i, l_t] + h_i + 1 + l_t**end**\n\n**REASON:** This is like a regression task, where you predict how likely an event (h_i) is given some context (h_i', l_t'). Formal: The l_t, h_i, and l_t parts are normalized to fit the attention vector h_i'. Formal: The attention input is h_i' + 1, and the masked word feature is l_t, l_t', and l_t', respectively. Formal: This function predicts the probability that h_i' is related to an event, given a specific context, h_i'. Formal: The l_t, h_i, and l_t' vectors are normalized to fit the attention vector h_i'. Formal: The attention input is h_i', l_t', and l_t', respectively. Formal: These are the normalized attention vectors, h_i', l_t', and h_i', respectively. Formal: You can train a classifier using this method. Formal: To see how well the model's predictions match the context, you average the scores from each classifier's predictions. Formal: The hidden state vector h_i' is",
        "directory": "acl",
        "filename": "2022.acl-long.436.json"
    },
    {
        "casual_text": "When we tweak our task, we mess with the text by either deleting stuff or shuffling it around. In Table 6, we tested different ways: replacing words with masks instead of deleting them, or just deleting words without shuffling. Turns out, our delete-and-shuffle method gives us the best BERT-iBLEU score compared to the other two options.",
        "formal_text": "During task-adaptation, our corruption strategies involve both deletions and shuffling. In Table 6 we provide ablation study results where we either replace words with masks instead of deleting them or delete words without shuffling. We can see that our delete-and-shuffle strategy achieves the best BERT-iBLEU score among the three settings.",
        "GPT2_formal_text": "= [c. c_t, t. t_i, ..., t. t_m] for i = 1, ..., n, and t = t_t, t_t_i, ..., t_t_m are the sequence representations of the input sentences. Formal: The main thing to focus on is the final output distribution, which we call the loss. Formal: Alright, so to figure out the loss, we use two main methods: the log loss and the cross-entropy loss. Formal: Using the log loss, we calculate the log loss by averaging the losses from the whole dataset. Formal: The cross-entropy loss, on the other hand, is calculated by minimizing the cross-entropy between the current model's predictions and the actual ground truth. Formal: The cross-entropy loss basically measures how well the model is doing by looking at how well it predicts the ground truth. Formal: Since the input sentences are usually pretty short and there's not much context around, the cross-entropy loss is usually smaller. Formal: The cool thing about using the cross-entropy loss is that it helps the model figure out how uncertain it is about its predictions. Formal: Formal: The whole process of finding the log loss and cross-entropy loss for a specific dataset, y_t, is outlined in equation (6). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.417.json"
    },
    {
        "casual_text": "Alright, let me show you an example of a messed-up auxiliary verb that the French-B1 model fixes. In French, the phrase for \"go shopping\" is \"faire des achats,\" and the verb \"faire\" basically means \"make\" or \"do.\"",
        "formal_text": "Below, we give an example of a confused auxiliary verb that the French-B1 adapted model corrects. The verb phrase corresponding to \"go shopping\" in French is \"faire des achats\", where the verb \"faire\" would translate to \"make/do\".",
        "GPT2_formal_text": "_formula = []. Formal: We use a basic form of attention to figure out the attention vector for the question and the context, which we call [a_q, a_c]. Formal: There's a regular attention mechanism in place, but it's not the full-on attention shown in Equation (3). Instead, we can add in a \"quasi-attention\" mechanism. This is kind of like what the method called Multi-task Learning did in 2011. Formal: We make a vector by combining the attention from both the question and the context into one, which we call [a_q, a_c]. Formal: We do this by using a simple linear transformation on the [a_q, a_c] vectors. Formal: We also use a CNN to encode the question-context attention vector, and we add this to the vector [a_q, a_c] in our model. Formal: We do this by taking the attention from the question and the context and putting it all together. Formal: Finally, we have a recursive neural network that processes the question-context attention vector, the question-context vector, and the [a_q, a_c] vectors. Formal: The neural network is trained to figure out the attention vectors for the question and the context in both directions, and then we use a linear transformation to combine the attention vectors from both directions. Formal: Formal: Lastly, we can add this to the [a_q, a_c] vector as the final input. Formal: Formal: Finally, we're combining all these vectors to get the final attention. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-5504.json"
    },
    {
        "casual_text": "Deep learning is now the go-to method for tackling most Natural Language Processing (NLP) tasks, like text classification. When you have enough high-quality training data, these models can work amazingly well. But in real-life situations, perfect datasets are pretty rare. Usually, the data you get is small, packed with regular but useless words, and might have some hidden biases (Wiegand et al., 2019; Gururangan et al., 2018). This can result in models that aren't as good as they could be and might have some unwanted traits. For instance, they might be biased against certain groups or not perform well in actual use because they've just memorized the flawed training data.",
        "formal_text": "Deep learning has become the dominant approach to address most Natural Language Processing (NLP) tasks, including text classification. With sufficient and high-quality training data, deep learning models can perform incredibly well. However, in real-world cases, such ideal datasets are scarce. Often times, the available datasets are small, full of regular but irrelevant words, and contain unintended biases (Wiegand et al. , 2019; Gururangan et al. , 2018). These can lead to suboptimal models with undesirable properties. For example, the models may have biases against some sub-populations or may not work effectively in the wild as they overfit the imperfect training data.",
        "GPT2_formal_text": "= (d, [p, d])) | i=1, ..., n. Formal: In this project, we came up with a new approach to dealing with epistemic modality by using a knowledge distillation (KD) approach. By using this approach, we made the inference process more efficient, resulting in a much more compact and understandable knowledge graph. For future work, we plan to integrate this KD approach into dialogue generation, and we're also exploring the idea of using it for knowledge graph inference. Formal: The third and fourth parts of the graph show the epistemic modality of a knowledge graph (KG). Formal: Here, p stands for the probability of a knowledge graph belonging to a specific modality. In the example, the KG is associated with a knowledge graph where the modality is knowledge. The light blue circle in the first part means the KG is a modality-based graph. The yellow circles represent modalities that don't have a KG. Formal: To get a better grasp on how the KD method works, we’ve included the KG distribution and the entropy of the KD distribution in Fig. 3. Formal: To connect the KD distribution to a knowledge graph, we use a multi-head attention mechanism that has multiple heads. Formal: In Fig. 1, the KG is represented by a three-level graph with K different modalities. Formal: The main goal of our work is to improve how well the KD model works by using knowledge graph knowledge and reducing the issue of the space being too sparse. Formal: In this project, we developed a new approach called knowledge-based distillation (KBD) for knowledge graph inference. The key idea behind KBD is to learn from knowledge graphs that are based on knowledge. The KBD method creates a three-level graph, with three different knowledge graphs. Each of these three knowledge graphs has a KG that includes the modality. The first two levels have a KG that’s a knowledge-based graph. The third level is a knowledge-based graph that’s part of a knowledge graph. Formal: The third and fourth levels are generated using a KBD method. Formal: In this project, we’re introducing a new approach called KD, which is a knowledge-based distillation (KBD) method. KD is a knowledge-based distillation method that learns knowledge from the knowledge graph. The KD algorithm is designed",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.24.json"
    },
    {
        "casual_text": "Alright, so, table wrappers don't always bring in tables. To figure out what they're actually doing, we use two filtering rules to sort things out:",
        "formal_text": "As specified above, table wrappers do not always introduce tables. Two filtering rules are employed to disambiguate their functions:",
        "GPT2_formal_text": "_layers = [s_mask, i_mask, s_mask_l1, ..., s_mask_lT, ..., s_mask_lZ], where Z is the size of the hidden layer. Formal: You can find the code and data for the autoencoder here: https://github.com/stephenhamilton/Autoencoder. Formal: The model can be found here: https://github.com/stephenhamilton/GPT-2-Generation-Model. Formal: The whole dataset for the TAC-KBP corpus is available here: https://github.com/TAC-KBP/TAC-KBP_Dataset. Formal: And here's a link to the dataset structure for the TAC-KBP corpus: https://github.com/TAC-KBP/TAC-KBP_Dataset. Formal: To make things easier, we're using the Stanford NER model's default settings for tokenizing and POS tagging. Formal: Formal: The BART model's default settings are available here: https://github.com/huggingface/bert-base. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C00",
        "filename": "C00-1025.json"
    },
    {
        "casual_text": "In this paper, we focus on paraphrasing between sentences and don’t really get into how our work could be applied to paraphrasing longer pieces of text. 2 The symbols and notations we use in the paper are all listed in Table 1.",
        "formal_text": "In this paper we study paraphrase between sentences, and do not address the larger scope of how our work might extend to paraphrasing between arbitrarily large text sequences. 2 The notations used in this paper are listed inTable 1.",
        "GPT2_formal_text": "` = t, which is basically a way to represent the entire set of training examples. Formal: The loss function L(θ θ_i, f, d) is calculated as: Formal: The loss function L(θ θ_i, f, d) is also a way to represent the entire set of training examples. Formal: L(θ θ_i, f, d) is calculated using a nonlinear function, which is based on the classifier θ θ. Formal: In this setup, the loss is calculated based on how well the classifier predicts the training examples. Formal: Formal: When training with just D_k, the loss for the predicted examples (w_t) is calculated using the cross-entropy loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.552.json"
    },
    {
        "casual_text": "Looking at Figure 2, we can see that both MRR and Hit@10 get better and kind of level off around d_s = 20. We did the same thing with the WN18RR dataset and found that the best subembedding dimension there is 4. Check out Table 4 for H@10 results from the FB15-237 validation set, broken down by categories like 1-to-N, N-to-1, and N-to-N.",
        "formal_text": "From Figure 2 we observe that, both MRR and Hit@10 are improved and slowly saturated around d s = 20 The similar experiments are also conducted on WN18RR data set and we find the best subembedding dimension is 4 on WN18RR. Table 4: H@10 from FB15-237 validation set by categories (1-to-N, N-to-1 and N-to-N).",
        "GPT2_formal_text": "ed (D) from the input sentence. The original sentence has a ton of stuff going on, like noun phrases, verb phrases, prepositions, adjectives, and prepositions. Formal: The first approach (2D) works by treating each word as a vector, like this: Formal: To make things simpler, let's say we have a sentence with n words. The first approach (2D) figures out the vector for each word in the sentence by looking at the whole sentence. This means it takes into account not just the word itself but also its surrounding words. Formal: Since the input sentence has n words, this gives us a vector for each word in the sentence. This vector basically captures the context around the word. Formal: But if we don’t have a context, the vector for a word is just the sum of all the vectors for all the words in the sentence. Formal: This is where the problem starts to get tricky, especially when the context isn’t super clear. Formal: Check out Figure 2 for an example of how the vector for the word \"the\" in the sentence \"the dog is hungry\" is calculated. Formal: The first approach (2D) uses a weighted average of the vectors for the words in the sentence. But the problem is that this approach doesn’t handle cases where the words are almost identical but have different contexts. Formal: So, it ends up treating the words in a sentence as one big vector. Formal: The second approach (2D) looks at each word individually. Formal: The vector for \"the\" in the sentence \"the dog is hungry\" is calculated by averaging the vectors for the words in the sentence \"the dog is hungry\" and \"the dog is thirsty.\" Formal: This is a bit simpler, but it’s still pretty heavy on the computer. Formal: The final result is the result of combining the vectors for the words in the sentence \"the dog is hungry\" and \"the dog is thirsty.\" Formal: Formal: The third approach (2D) combines the vectors for the words in the sentence \"the dog is hungry\" and \"the dog is thirsty.\" Formal: The final result is the result of applying the weighted average of the vectors for the words in the sentence \"the dog is hungry.\" Formal: Formal: Formal: So, in short, the second approach (2D) figures out the",
        "directory": "acl",
        "filename": "2020.acl-main.241.json"
    },
    {
        "casual_text": "Okay, let’s break this down in a simpler way:\n\n1. We added the Chinese words found using Strategy 2 to the system dictionary and trained the Chinese segmenter using the short unit training data, which was set up in Section 3.3.\n\n2. Table 5 shows how well the Chinese-to-Japanese translation worked using the NICT Chinese Treebank. The short unit method performed the best. The Chinese words we found also boosted the BLEU scores a lot. Strategy 2 did better than Strategy 1, except for test sets 2 and 5. We think this is because Strategy 2 found more words, which helped with the unknown word problem.\n\n3. Table 6 shows the results for Chinese-to-Japanese translation using CTB 7. Strategy 2 got better BLEU scores than the baseline, but the improvement wasn’t as big compared to Strategy 1. We looked into why this happened and found that many of the words found in the parallel training corpus and the annotated training data were the same. For example, \"Ë}((protein)\" came from the annotated data and overlapped with \"Ë}(protein)\" and \"((quality)\" from the parallel corpus. When the Chinese segmenter tried to break down \"Ë}((protein)\", this overlap caused inconsistent results.\n\n4. Table 6: Results of Chinese-to-Japanese translation experiments using CTB 7.",
        "formal_text": "• Short unit: Incorporate the Chinese lexicons extracted by Strategy 2 into the system dictionary and train the Chinese segmenter on the short unit training data transformed in Section 3. 3. Table 5 shows the BLEU scores for Chineseto-Japanese translation using NICT Chinese Treebank. Short unit achieved best MT performance. The extracted Chinese lexicons also improved BLEU scores significantly. Besides test set 2 and test set 5, Strategy 2 achieved better improvement than Strategy 1. We think the reason is that Strategy 2 extracted more lexicons which is helpful to solve the unknown word problem. Table 6 shows the BLEU scores for Chinese-to-Japanese translation using CTB 7. Although Strategy 2 obtained higher BLEU scores than the baseline, compared to Strategy 1, the improvement is not significant. We investigated the reason and found that there are many overlaps between lexicons extracted from the parallel training corpus and lexicons extracted from the annotated training data. For example, \"Ë}((protein)\" was extracted from the annotated training data and overlaps \"Ë}(protein)\" and \"((quality)\" extracted from the parallel training corpus. When the Chinese segmenter tries to segment \"Ë}((protein)\", the overlap can lead to inconsistent segmentation results. Table 6: Results of Chinese-to-Japanese translation experiments using CTB 7.",
        "GPT2_formal_text": ", which are basically the query's set of names. Formal: So, the ROUGE-1 scores for our sample sentences are (0.267, 0.352) and (0.258, 0.355), which is pretty close to the best scores out there. But, the difference is really big—like, the difference is like 30 times bigger than the ROUGE-1 value, according to a recall of 0.489. Formal: The row with the highest value, which is the final score, shows how well our system is doing overall. For instance, on the test sentence \"blake shelton austin lyrics,\" the final score is 0.269. Formal: The smaller, lower-ranked rows in our study only use one type of info for each pair of sentences, but they’re super useful. We used the inter-sentence relation extraction method to get the results shown in Table 2. Formal: We focused on sentence pairs that are inter-sentence relations, which means they’re only related to each other through a single relation. Formal: From the results in Table 2, it’s clear that our system is doing better than a lot of other methods out there. Formal: The results in Table 3 show that our system is better than the current top systems out there for both text-to-SQL and text-to-SQL with relation extraction. Formal: The results in Table 4 show how our system compares to the latest systems for SQL, and we’re using the same evaluation method as in Table 2. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2012.eamt-1.7.json"
    },
    {
        "casual_text": "In Table 5, you can see how much data we filtered out for each neighborhood choice. The global approach ended up losing about twice as much data as the other two methods. The 1k batch neighborhood did just as well as the more detailed document-level neighborhood but made things run way faster—like, more than ten times faster. After digging deeper, we noticed that over 98.5% of the pairs we filtered out in the document-level approach were still there in the batched approach. So, for the rest of our experiments, we stuck with 'Batch Filtering' as our go-to method. We also used batch filtering on each sentence-aligned corpus in section 4.1 to clean them up a bit. Table 4 shows what our training corpus looked like after all the filtering.",
        "formal_text": "In Table 5, we show the percentage of filtered out pairs from the sources for each neighborhood choice. The global approach lost about twice the data compared to the other two. The 1k batch neighborhood achieved comparable performance with respect to the more fine-grained document-level neighborhood while improving running time more than ten-folds. Upon further inspection, we found that more than 98. 5% pairs from the documentlevel filter were present in the batched approach. So, in subsequent experiments, we used 'Batch Filtering' as standard. In addition to the documentaligned sources, we also used batch filtering on each sentence-aligned corpus in section 4. 1 to remove noise from them. Table 4 summarizes our training corpus after the filtering.",
        "GPT2_formal_text": "_tokens = [h_0, h_1, h_2, h_3, ..., h_T], [h_i, h_j, h_k, h_l, h_0, h_1, h_2, ..., h_T], [h_i, h_j, h_k, h_l, h_0, h_1, h_2, ..., h_T], [h_i, h_j, h_k, h_l, h_0, h_1, h_2, ..., h_T], [h_i, h_j, h_k, h_l, h_0, h_1, h_2, ..., h_T]. Formal: The attention is based on the keyphrases you provide, and it's used to figure out the attention score for the query. Formal: In NER, the entity's embedding h_i is used as the keyphrases embedding, and the query-specific attention is calculated using the entity's embedding h_j. Formal: In WER, the entity's embedding h_i is combined with the query-specific attention, and the query-specific attention is calculated using the entity's embedding h_k. Formal: In BiLSTM-based models, the entity's embedding h_i is used as the entity embedding, and the entity-specific attention is calculated using the entity's embedding h_k. Formal: To calculate the attention score for a word, the model uses a feed-forward network that processes the entity embedding h_i and the query-specific attention. Formal: In BiLSTM-based models, the entity's embedding h_i is combined with the query-specific attention, and the query-specific attention is calculated using the entity's embedding h_k. Formal: In BiLSTM-based models, the entity's embedding h_i is combined with the entity-specific attention, and the entity-specific attention is calculated using the entity's embedding h_k. Formal: In text classification, the entity's embedding h_i is combined with the entity embedding h_j, and the entity-specific attention is calculated using the entity's embedding h_l. Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.207.json"
    },
    {
        "casual_text": "So, we're dealing with a sequence ŷ_s that's sampled from our model, and r is a baseline reward. We're using this self-critical training approach (shoutout to Rennie et al., 2017), where we get the baseline reward r by applying the same reward function r to a sequence ŷ_g that's decoded greedily. Basically, r = r(ŷ_g). From what we've seen in practice, using this self-critical baseline reward really helps keep the training of our summarization model stable.",
        "formal_text": "whereŷ s is a sampled sequence from the model and r a baseline reward. Here we adopt the self-critical training strategy (Rennie et al. , 2017), where we obtain the baseline rewardr by applying the same reward function r to a greedily decoded sequencê y g, i. e. , r = r(ŷ g ). We empirically find that using this self-critical baseline reward helps stabilize the training of our summarization model.",
        "GPT2_formal_text": "\" or \"answer_ids\" respectively. This method is meant for languages that don’t have a lot of resources, like those with limited resources, like Indigenous languages. It’s a bit different from the usual NER methods, which usually just pull out basic entities like people, places, organizations, dates, or prices. Formal: We’re using two tools to figure out if a word is an entity. The first tool checks if it fits the entity type, and the second one checks if it’s an entity. We’ve set up two different ways of combining these two types of results. Formal: To make things easier for the NER researchers, we’ve created two different datasets. One is for regular entity extraction, and the other is for identifying entity mentions. Formal: In this paper, we’ve tested out two different ways to do entity extraction: one for regular entity extraction and another for entity recognition. Formal: Figure 1 shows the process of the entity extraction system. In this setup, the system uses the entity detection approach and a top-notch feature extractor. Formal: The entity detection system looks for words like \"the\" (to identify a person), \"the company\" (for the company), \"a hotel\" (for a hotel), or \"a university\" (for a university) in the sentence. It then connects these words to the list of entity types. Formal: The feature extractor then sorts the extracted words into two categories: \"E\" (entity) and \"N\" (non-entity). The system then checks how similar the extracted words are to the entity type to determine the type. Formal: The entity recognition system uses a list of names and entity types from the text. It also looks for the entity mention, which is a person, organization, date, or price, within a certain range, to identify if it’s an entity. Formal: The system then uses some rules to pull out a list of entities. Formal: Finally, we use a dictionary to find the best matching entity and then assign a type to it. Formal: Formal: Lastly, we combine the results of all three systems to figure out the type of entity. Formal: For our experiments, we tested the systems on two datasets from the CoNLL-2009 shared task: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.458.json"
    },
    {
        "casual_text": "In Table 11, we tested what happens when we bump up the learning rate to 5e-5 for RIPPLES on the SST-2 dataset during fine-tuning. Turns out, cranking up the pre-training learning rate doesn’t really help—it messes with performance on clean data and doesn’t give much of a boost to poisoning performance either. The only weird exception is the IMDb dataset, where the loss goes haywire, and the model’s performance on clean data tanks to random guessing levels.",
        "formal_text": "In table 11, we show the results of increasing the learning rate to 5e-5 for RIPPLES on the SST-2 dataset when fine-tuning with a learning rate of 5e-5. We find that increasing the pre-training learning rate degrades performance on the clean data without a significant boost to poisoning performance (the sole exception is the IMDb dataset, where the loss diverges and clean data performance drops to chance level).",
        "GPT2_formal_text": "_tgt, which basically turns the attention matrix into a dot product of word vectors. The final attention matrix A is calculated as: Formal: The first row of the formula (a = a t a, t b ) is just the input sentence, and the second row is the attention matrix. Formal: To figure out the attention weights, we use the cross-entropy loss function. Formal: We use an ensemble of softmax and softmax-multiplying classifiers to get the final attention weights. Formal: Lastly, we pick the attention weights from the average of the output attention vectors. Formal: This gives us the final attention score. Formal: For the next part, we'll use a random walk method to pick attention weights and a grid search to find the best one. Formal: The full breakdown of the attention weights is shown in Figure 5. Formal: There are other ways to calculate the attention weights, like an averaged hidden state vector (a t a t, t b ), a weighted sum of softmaxes, or an average of a bunch of weighted sum operations (a t a t, t b ). Formal: Since we're using attention weights and some trigrams for the output, we end up with the attention weights from the weighted sum operations shown in Figure 5. Formal: We check how well the attention weights work by calculating the average dot product between the output attention vectors and the input sentence vectors. Formal: Formal: Formal: We also look at how the attention weights interact with each other using a Kruskal-Wallis test. Formal: Figure 6 shows the results for a list of words, showing the attention weights for words that are the same or are next to each other. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.249.json"
    },
    {
        "casual_text": "(3) In Section 3.2, we came up with two ways to combine those four groups of words. In the \"-Weighted pooling\" part, we swapped the weighted pooling method for mean-pooling, and that made things worse. Compared to mean-pooling, the weighting strategy does more than just give different words importance based on how meaningful they are. It also brings in the frequency of each word from the data, which has been shown to be useful.",
        "formal_text": "(3) We proposed two strategies for pooling the four word sets in Section 3. 2. In the \"-Weighted pooling\" experiment, the weighted pooling strategy was replaced with mean-pooling, which degrades the performance. Compared with mean-pooling, the weighting strategy not only succeeds in weighing different words by their significance, but also introduces the frequency information of each word in the statistical data, which is verified to be helpful.",
        "GPT2_formal_text": "\": [], ], }, }, }, }, { \"language\": { \"posts\": [ { \"id\": \"1\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"1\", \"post\": \"1\", }, ], \"language\": \"download_users\": [ { \"id\": \"2\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"2\", \"post\": \"2\", }, ], \"language\": \"download_users\": [ { \"id\": \"3\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"3\", \"post\": \"3\", }, ], \"language\": \"download_users\": [ { \"id\": \"4\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"4\", \"post\": \"4\", }, ], \"language\": \"download_users\": [ { \"id\": \"5\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"5\", \"post\": \"5\", }, ], \"language\": \"download_users\": [ { \"id\": \"6\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"6\", \"post\": \"6\", }, ], \"language\": \"download_users\": [ { \"id\": \"7\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"7\", \"post\": \"7\", }, ], \"language\": \"download_users\": [ { \"id\": \"8\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"8\", \"post\": \"8\", }, ], \"language\": \"download_users\": [ { \"id\": \"9\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"9\", \"post\": \"9\", }, ], \"language\": \"download_users\": [ { \"id\": \"10\", \"name\": \"english\", \"post_user\": \"the_user\", \"user_id\": \"10\", \"post\": \"10\", }, ], \"language\": \"download_users\": [ { \"id\": \"11\", \"name\": \"english\", \"post",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "For the error analysis, we randomly picked 100 wrong predictions. Let’s break down two main types of errors we found: Entity Ambiguity. Even though our entity detection module tags each predicted span with an (entity) type, dealing with entity ambiguity is still the biggest challenge for our system. For example, take the question, \"Who is associated with Jeff Smith?\" Our entity detection module correctly spots \"Jeff Smith\" as an entity and tags it as a \"common name.\" But here’s the catch: the Wikidata knowledge graph has over ten entities with the exact same label and type. This makes it tricky for our entity linking module to figure out the right one. Wikidata entity linking is a relatively new area of research, and it comes with its own set of challenges, like entities having the same labels, non-standard labels created by users, and even multi-word labels (some as long as 62 words) (Mulang et al., 2020b). Adding more context, like entity descriptions and other info from the knowledge graph, could help clear up this ambiguity (Mulang et al., 2020a).",
        "formal_text": "For the error analysis, we randomly sampled 100 incorrect predictions. We detail the reasons for two types of errors observed in the analysis: Entity Ambiguity Even though our entity detection module assigns (entity) types to each predicted span, entity ambiguity remains the biggest challenge for our framework. For instance, for the question, \"Who is associated with Jeff Smith? \" LASAGNE entity detection module correctly identifies \"Jeff Smith\" as an entity surface form and correctly assigns the (entity) type \"common name\". However, the Wikidata knowledge graph contains more than ten entities with exactly the same label and type. Our entity linking module has difficulties in such cases. Wikidata entity linking is a newly emerging research domain that has its specific challenges such as entities sharing the same labels, user-created non-standard entity labels and multi-word entity labels (up to 62 words) (Mulang et al. , 2020b). Additional entity contexts, such as entity descriptions and other KG contexts, could help resolve the Wikidata entity ambiguity (Mulang et al. , 2020a).",
        "GPT2_formal_text": "ing = (K * h_pos, K * h_neg, K * h_soi) + h_pos_mask. The attention weights are adjusted based on how often words appear together in the text. Formal: We train the model using three main objectives: (1) maximizing the likelihood of the right answer, (2) minimizing the cross-entropy loss, and (3) improving the accuracy of the predicted answer. Formal: We've got a beam size of n = 2, and the training goal is to maximize the probability of the correct answer. Formal: The model's parameters are adjusted based on how often it predicts an answer, on average, between the source and target posts. Formal: We build these hyperparameters using a learning rate of 2e-4 and a minimum embedding size of 100. Formal: We train the model using stochastic gradient descent with a learning rate of 5e-5. Formal: The model's parameters are adjusted based on how often the model predicts an answer, on average, between the source and target posts. Formal: We train the model using stochastic gradient descent with a learning rate of 5e-5. Formal: The model's parameters are adjusted based on how often the model predicts an answer, on average, between the source and target posts. Formal: We train the model using stochastic gradient descent with a learning rate of 5e-5. Formal: Formal: Formal: The model's parameters are adjusted based on how often the model predicts an answer, on average, between the source and target posts. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.72.json"
    },
    {
        "casual_text": "Take it out of the oven and sprinkle some fake bacon bits and potato chips on top. Put it back in the baking dish and bake it without a lid for 30 minutes.",
        "formal_text": "Remove from oven, sprinkle with imitation bacon and potato chips. baking dish; bake, uncovered, for 30 minutes.",
        "GPT2_formal_text": "; step) { SELECTOR (p_in, e_in) = h_eval(d_eval) + Σw_eval; } function eval_q(p_in, q) { SELECTOR (p_in, q_in) = h_eval(d_eval) + Σw_eval; } function eval_t(p_in, q) { SELECTOR (p_in, q_in) = h_eval(d_eval) + Σw_eval; } function eval_r(p_in, q, s) { SELECTOR (p_in, q, s) = h_eval(d_eval) + Σw_eval; } function eval_w(p_in, q, t) { SELECTOR (p_in, q, t) = h_eval(d_eval) + Σw_eval; } function eval_q(p_in, q, r) { SELECTOR (p_in, q, r) = h_eval(d_eval) + Σw_eval; } function eval_t(p_in, q, s) { SELECTOR (p_in, q, s) = h_eval(d_eval) + Σw_eval; } function eval_r(p_in, q, s) = h_eval(d_eval) + Σw_eval; } function eval_w(p_in, q, t) = h_eval(d_eval) + Σw_eval;                                                                                                                                                       \n\nLemma 2.1: If we have a voting loss that's greater than a certain threshold",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.526.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\nBasically, we've got this equation:\n\nF = e, f D q(D | e, f) log [P Θ D (D | e, f) / q(D | e, f)] + e, f D q(D | e, f) log P Θ (e, f)\n\nWhich can also be written as:\n\nF = e, f log P Θ (e, f) (8) − KL[q(D | e, f) || P Θ D (D | e, f)]\n\nSo, it's combining a bunch of terms and using some log stuff, along with something called KL divergence, to express F.",
        "formal_text": "F = e, f D q(D | e, f) log P Θ D (D | e, f) q(D | e, f) + e, f D q(D | e, f) log P Θ (e, f) = e, f log P Θ (e, f) (8) − KL[q(D | e, f) || P Θ D (D | e, f)],",
        "GPT2_formal_text": "ing, attention_truncation, and attention_pooling_mask. Formal: The size of the hidden state is what's usually used to pick words for the output vector. Formal: Also, in some situations, like when dealing with subword stuff, we can use word embeddings that are made from subword tokens. Formal: This is kinda like the method by Bunescu and Mooney from 2002, where they create a subword embedding. Formal: The context vector, e, is built using this method by Jia and Liang from 2016, along with some tweaks to it. Formal: In the next section, we'll explain how we can expand this context vector by using multiple embeddings. Formal: Using this new context vector, we can get a word embedding, g. Formal: Lastly, we'll talk about how to pick the best hidden state and attention for the output. Formal: Here's how we do it: Formal: We start by calculating the hidden state, h, for the word embedding g. Formal: We then apply an attention mechanism, which we'll call H(e_t_t), to that hidden state to get the attention vector, g_t. Formal: Next, we pick the hidden state that has the highest probability, h_t_t, using the softmax function. Formal: Finally, we add g_t to the hidden state to get the final output, h. Formal: Formal: Formal: Here's how it works: Formal: h_t_t is calculated using the context vector, g_t_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1062.json"
    },
    {
        "casual_text": "3. Pruning: We should get rid of parts that probably won't help with the final result right from the start.",
        "formal_text": "3. pruning: constituents not likely to contribute to the final parse should be removed early.",
        "GPT2_formal_text": "= (c, t, e). The sequence might include a bunch of different entities, like t = {t1, ..., tK}, where each entity is labeled as e_i for i ranging from 1 to K. For each entity e_i, the graph G is made up of all the triples (c, t, e) with entity e_i as the key. Each triple (c, t, e) is a vector in R^n, where the vectors are the entity representations of t. Each triple (c, t, e) also has a single value vector that is the label for the entity e_i. For instance, the triple for \"Virginia Tech\" would be (c, t, e, VaTech). Formal: A state is represented as a straight line, and a query is just a function that connects a state to a query. Formal: We start by creating the graph G_s, which includes all the graphs for states and queries. Formal: For each entity e_i, the graph G_s has these triples (c, t, e) with entity e_i as the key. Formal: We use a straightforward approach to handle multi-hop reasoning by using a directed acyclic graph with the current state as the query. Formal: The entity graph G_s is made up of entity triples (c, t, e). Formal: Each entity is represented by a vector in R^n, where the vectors are the entity representations of t. Formal: Formal: The query is a function that connects a query state to a state. Formal: Formal: The current state is represented as a straight line, and a single query state is just a function that connects a query state to a query state. Formal: Formal: Formal: The graph G_s is ordered, with each node representing a single query state. Formal: For each entity e_i, the graph G_s has these triples (c, t, e) with entity e_i as the key. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "A00",
        "filename": "A00-3002.json"
    },
    {
        "casual_text": "First, let's look at the vector representations used to handle mentions. Earlier research used ELMo and pre-trained GloVE (Pennington et al., 2014) for word and character embeddings, but newer models are using RoBERTa (Cattan et al., 2020; Yu et al., 2020b). We tried swapping out BERT-base for RoBERTa-base and adding GloVE alongside BERT in our models (check out Appendix B for the details), and we saw a big drop in performance. We think the big difference between BERT and RoBERTa might be because BERT was trained with Next Sentence Prediction (NSP), while RoBERTa wasn't. The NSP might make BERT better at understanding whole documents, which is super important for coreference resolution. So, we guess that without specific fine-tuning, adaptive pre-training works best for coreference on ECB+. We also noticed that our entity coreference model is less affected by changes in features compared to the event coreference model. For instance, the event coreference model really depends on argument features.",
        "formal_text": "First we examine the vector representations used to encode mentions. While prior work used ELMo and pre-trained GloVE (Pennington et al. , 2014) word and character embeddings, recent models use 74. 8 -0. 9 78. 7 -2. 1 − Arg comp ( §3. 2) 74. 6 -0. 7 78. 3 -2. 5 − CLS (Eq. 1) 74. 5 -0. 8 78. 9 -1. 9 − MP cosine ( §3. 4) 74. 5 -0. 8 79. 1 -1. 7 + GloVE 70. 1 -5. 2 76. 7 -4. 1 + RoBERTa 71. 2 -4. 1 78. 1 -2. 7 RoBERTa (Cattan et al. , 2020; Yu et al. , 2020b). We experiment with replacing BERT-base with RoBERTa-base and with using GloVE in addition BERT in our models (see Appendix B for implementation) and observe large drops in performance. We hypothesize that the substantial performance difference between BERT and RoBERTa is due to the Next Sentence Prediciton (NSP) used to train BERT but not RoBERTa. The NSP may force BERT to learn attention multiple sentences, and therefore to understand the document as a whole, an ability that is important for coreference resolution. Therefore, we hypothesize that without taskspecific fine-tuning, adaptive pre-training is most beneficial for coreference on ECB+. We also observe that our entity coreference model is relatively less susceptible to feature changes than the event coreference model. For example, the event coreference model is particularly reliant on the argument features.",
        "GPT2_formal_text": "ing, and the other ones are from text input with n words. Formal: We're looking into the issue of creating a mask when the input words aren't in the vocabulary. We're focusing on word masks that handle words not in the vocabulary, like \"cinema\" or \"movie.\" We use a decoder that's built to create the masked word, following the method from (He et al., 2019), which we'll explain below. Formal: The masked word is created based on the input text, specifically the n-th token. We stick with the same word masking method from (He et al., 2019) to get the output. Formal: The output is a sequence of d words, which we get by decoding the masked word using the decoder. Formal: If we add a word mask during decoding, we adjust the probability of a word being masked, which then affects the probability of the output word. Formal: We tested how well our approach works in three different tasks and got the results in Table 1. Our results show that our method can really boost performance and helps us create better masked words. Formal: We also tried a basic model that just creates a single word mask without considering the context. The results are in Table 2. Our method did better than the basic model, improving by 2.12% compared to the basic model. Formal: For the third task, we tried a few different masked word embedding models (like embedding or skipgram) but didn't find one that worked well. Formal: Lastly, we also tried using an LSTM model that uses a contextual embedding approach. This approach gave us an average improvement of 1.7% in performance compared to the basic model, which is pretty cool. Formal: We're sharing our code for this project on GitHub at https://github.com/sjvanderw/MaskingWordsForTraining. Formal: In our experiments, we tested different embedding models to see which one gives the best performance. We tried out a few models from the standard setup (Pennington et al., 2014), but they didn't perform as well as our new method. Formal: Our method works really well when we use a dictionary-based dictionary-based decoder. Formal: We used a semantic dependency tree parser to create the embeddings for the masked word. We trained this tree parser for 500 steps to get the embeddings, and we kept it running for another 500",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.382.json"
    },
    {
        "casual_text": "To address this issue, we're introducing a new task called subjectivity word sense disambiguation (SWSD). The goal is to figure out automatically which words in a text are being used in a subjective way and which ones are being used objectively. We think SWSD is more doable than trying to figure out every single word's exact meaning because it’s less detailed—you don’t always need to know the exact sense of a word. Plus, we believe SWSD can help make contextual subjectivity analysis systems work better by using sense-aware classification.",
        "formal_text": "To tackle this source of error, we define a new task, subjectivity word sense disambiguation (SWSD), which is to automatically determine which word instances in a corpus are being used with subjective senses, and which are being used with objective senses. We hypothesize that SWSD is more feasible than full word sense disambiguation, because it is more coarse grained -often, the exact sense need not be pinpointed. We also hypothesize that SWSD can be exploited to improve the performance of contextual subjectivity analysis systems via sense-aware classification.",
        "GPT2_formal_text": ") = {'username', 'password', 'location', 'time', 'location_time', 'time_location', 'location_time_local', 'time_location_local', 'time_type', 'date', 'location_type', 'date_type', 'date_pattern', 'date_pattern_pattern', 'date_pattern_pattern', 'time', 'date_pattern', 'time_pattern', 'time_pattern_pattern', 'time_pattern_pattern', 'date', 'location', 'time', 'time_pattern', 'time_pattern', 'time_pattern_pattern', 'time_pattern_pattern', 'date', 'location_pattern', 'time_pattern', 'time_pattern_pattern', 'time_pattern_pattern_pattern', 'date_pattern', 'location_pattern_pattern', 'time_pattern_pattern_pattern', 'date_pattern_pattern', 'time_pattern_pattern_pattern', 'date', 'location_pattern', 'date_pattern_pattern', 'time_pattern_pattern_pattern', 'date_pattern', 'time_pattern', 'time_pattern_pattern_pattern', 'date', 'location_pattern_pattern', 'time_pattern_pattern_pattern', 'time_pattern_pattern_pattern', 'time_pattern_pattern_pattern', 'date', 'location_pattern_pattern', 'time_pattern_pattern_pattern', 'time_pattern', 'time_pattern_pattern_pattern', 'date_pattern', 'location_pattern_pattern', 'time_pattern_pattern_pattern', 'time_pattern_pattern_pattern', 'date', 'location_pattern', 'date_pattern_pattern', 'time_pattern', 'time_pattern_pattern_pattern', 'date_pattern', 'location_pattern_pattern', 'time_pattern_pattern_pattern', 'date_pattern_pattern', 'location_pattern_pattern', 'time_pattern', 'time_pattern_pattern_pattern', 'date', 'location_pattern_pattern', 'date_pattern_pattern', 'time_pattern_pattern_pattern', 'date_pattern_pattern_pattern', 'date_pattern', 'location_pattern_pattern', 'time_pattern_pattern_pattern', 'date_pattern_pattern_pattern', 'time_pattern_pattern_pattern', 'date', 'location_pattern_pattern', 'time_pattern_",
        "directory": "D09",
        "filename": "D09-1020.json"
    },
    {
        "casual_text": "Okay, so here's the deal: We've got two literal movement grammars, G1 and G2, with their own sets of stuff (like #1, $1, P1 for G1 and #2, $2, P2 for G2). The important thing is that the domains of #1 and #2 don't overlap at all, meaning dom(#1) and dom(#2) don't share any elements.\n\nNow, we can make a new grammar, G, by combining the stuff from G1 and G2, plus adding a new rule. So, G = (#1 U #2 U {(S, 0)}, S, P1 U P2 U {R}), where R is a rule that says \"S, 0\" can turn into \"S, 0 S2()/x\".\n\nBasically, G will only recognize sentences that both G1 and G2 would recognize. It's like combining their powers into one super grammar!",
        "formal_text": "1 3. 1 Proposition (intersection) Given two literal movement grammars G1 ---(#1, $1, P1) and Gz = (tzz, $2, Pz) such that dom(#l) n dom(#2) = O, we can construct the grammar GI = (#1 U #z U {(S, 0)}, S, P1 U P2 U {R}) where we add the rule R: so -~ =S, O Sz()/x Clearly, GI recognizes precisely those sentences which are recognized by both G1 and Gz.",
        "GPT2_formal_text": "_x, y) = max(1 - c * d_x, y) (6) Here, x_i and y_i are the i-th word from the vocabulary V_x, and c is the number of times a word appears in the data. The attention is given to the word w_i and the surrounding context c_x_i. The output layer gives a single number that shows how much attention the word w_i gets compared to the context c_x_i. Formal: Alright, let's break this down in a simpler way:\n\n- We're looking at how similar two words are, with each being a single token.\n- Then, we multiply this similarity by the average attention value we get from the entire dataset. Formal: For every pair (w_i, w_j) in V_x, we measure how similar w_i is to w_j by subtracting the attention value of w_i compared to w_j. Formal: To get the average attention value for the whole dataset, we subtract the attention value from each word, and then we multiply this value by the average attention value across all words. Formal: So, the similarity is calculated like this: Formal: So, the similarity is calculated like this: Formal: The attention value is the sum of the attention values for each token and the average attention value across all tokens. Formal: The attention value is the sum of the attention values for each word, and then we multiply this value by the average attention value across all words. Formal:\n\nSo, it's like combining two features: one for how similar two words are (like how much they're similar) and another for how similar they are to the context of all the words in the dataset (like how much attention they get). Formal: Formal:\n\nSo, the similarity is basically the average of the attention values for each word and the average attention value for the whole dataset. Formal: Formal:\n\nSo, in short, we're looking at how similar two words are, multiplying those similarities by their average attention values to get the attention value, and then multiplying that value by the average attention value across all words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E95",
        "filename": "E95-1013.json"
    },
    {
        "casual_text": "Our QA task is kind of like what Mirsha et al. (2016) did, but instead of asking about the sentiment of a paragraph, we ask random questions. Our multitask method for doing both the QA task and predicting gaze is similar to what Klerke et al. (2016), Berrett et al. (2018), and Mishra et al. (2018) did. Specifically, in Equation 4, we use the same loss term as Barrett et al. (2018), which combines an NLP task loss and a gaze prediction loss. The difference is that Barrett et al. (2018) used gaze predictions as input attention weights for the NLP task, but we just treat gaze as an output. This is kind of like how humans read, where eye movements are just a behavior, not something that affects language processing.\n\nOur work is also different from Mishra et al. (2018) because we use a different model and a single auxiliary objective based on gaze. Lastly, Vajjala et al. (2016) collected eye-tracking data from ESL learners for 4 articles from the same OneStopEnglish source we used (Vajjala and Lučić, 2018) to study how text difficulty affects fixation measures and reading comprehension. Our work, however, focuses on a different task and a different group of readers.",
        "formal_text": "Our QA task can be viewed as a generalization of the work in Mirsha et al. (2016; , where instead of being asked about the sentiment of a paragraph, subjects are presented with arbitrary questions. Our multitask approach for jointly performing the QA task and predicting gaze is similar to Klerke et al. (2016), Berrett et al. 2018and Mishra et al. (2018). In particular, in Equation 4 we use the same loss term as Barrett et al. (2018) which consists of a linear combination of an NLP task loss and gaze prediction loss. Our approach differs from Barrett et al. (2018) in that their model uses the gaze predictions as input attention weights for the NLP task, while our model treats gaze only as an output. Our approach provides a parallel to human reading, in which eye movements are an external behavior rather than an input to language processing tasks. Our work differs from Mishra et al. (2018) in the model and the use of a single auxiliary objective based on gaze. Finally, we note that in Vajjala et al. (2016) eye-tracking data from ESL learners was collected for 4 articles from the same source of OneStopEnglish articles (Vajjala and Lučić, 2018) used here, and utilized to study the influence of text difficulty level on fixation measures and reading comprehension. Our work focuses on a different task and a different population of readers.",
        "GPT2_formal_text": "= {[CLS], [SEP], [S], [BODY], [ARTICLE], [SEP]}, where [CLS] is the title and [SEP] is the content. The domain is basically the set of entities that have a specific relation label. We use this information to figure out the task of classifying these entities. Formal: We use the iterative process to get the ground truth for the correct entities and then use this information to create the predicted labels. Formal: The end result is a predicted label for each entity in the sequence. Formal: We use the iterative process to get the ground truth for the correct entities. Formal: Here's how it works: Formal: We pick the entity that’s most likely to be correct based on the ground truth, and then we calculate the probability P(e|f) for that entity. Formal: For instance, we could guess that \"lion\" is most likely to be correct. Formal: Based on how confident we are in choosing \"lion,\" we calculate the probability P(e|f) for the correct entity. Formal: Lastly, we generate a final prediction using the predicted labels. Formal: Here’s how it works: Formal: We pick the entity that’s most likely to be correct based on the ground truth, and then we calculate the probability P(e|f) for that entity. Formal: Finally, we generate a final prediction using the predicted labels. Formal: Formal: We’re using a supervised learning approach. Formal: The ground truth for the correct entity is represented using a BERT-based encoder-decoder model. Formal: We’re using a supervised learning approach. Formal: The ground truth for the correct entity is represented using a BERT-based encoder-decoder model. Formal: Formal: We’re using a supervised learning approach. Formal: Formal: The ground truth for the correct entity is represented using a BERT-based encoder-decoder model. Formal: Formal: Lastly, we generate a final prediction using the predicted labels. Formal: Formal: The ground truth for the correct entity is represented using a BERT-based encoder-decoder model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "conll",
        "filename": "2020.conll-1.11.json"
    },
    {
        "casual_text": "Alright, so we're looking at how source tokens (let's call them \"source(k)\") end up in the target sequence. The question is: do certain source tokens tend to show up more often in specific spots in the target? To figure this out, we're checking how much each source token position (k) contributes overall to the entire target sequence.",
        "formal_text": "Source(k) −→ target. Now we want to understand if there is a tendency to use source tokens at certain positions more than tokens at the others. For each source token position k, we evaluate its total contribution to the whole target sequence.",
        "GPT2_formal_text": "and attention_shape variables, respectively. Formal: In the output layer, we calculate a post-level attention score for each word, which is the average of the [CLS] and [SEP] attention scores. Formal: Post-level attention is about capturing the most important features from the entire input sequence, while the [CLS] and [SEP] attention scores focus on the most important bits from the current word's sequence. We do this by averaging the attention weights for these parts. Formal: The post-level attention and the [CLS] and [SEP] attention weights are calculated using a projection layer. Formal: The [CLS] and [SEP] attention weights are also calculated based on this projection layer. Formal: The output layer calculates the attention weights for the most important post-level features. Formal: The output layer also includes the post-level attention, which is just the [CLS] and [SEP] attention weights combined. Formal: The post-level attention is calculated based on the projection layer. Formal: Lastly, the output layer includes the post-level attention and the [CLS] and [SEP] attention weights. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.91.json"
    },
    {
        "casual_text": "We looked at how well the BNN model works for predicting suffixes at the part-of-speech (POS) level. Table 5 shows the accuracy of suffix predictions for English-Russian, broken down by POS. For this part, we used Russian data that was tagged by TreeTagger. We also calculated the average number of suffixes per stem for each POS. Our findings match what Chahuneau et al. (2013) found: predicting suffixes for adjectives is trickier than for other POS types, while predicting verbs is relatively easier, even though verbs tend to have more suffixes per stem. These differences highlight how important it is to consider the context—whether it's the source or target language. For example, adjectives need to match the gender of the nouns they describe, but this often has to be figured out from the target context alone.",
        "formal_text": "We evaluate the suffix BNN model at the part-ofspeech (POS) level. Table 5 provides suffix prediction accuracy per POS for En-Ru. For this analysis, Russian data is segmented by TreeTag-ger. Additionally, we report the average number of suffixes per stem given the part-of-speech. Our results are consistent with the findings of Chahuneau et al. (2013): 11 the prediction of adjectives is more difficult than that of other POS while Russian verb prediction is relatively easier in spite of the higher number of suffixes per stem. These differences reflect the importance of source versus target context features in the prediction of the target inflection: For instance, adjectives agree in gender with the nouns they modify, but this may be only inferred from the target context.",
        "GPT2_formal_text": "(t, c) = (1, ..., N). We take the average of this distribution, and this averaged value can be adjusted to fit any specific situation, like the length of the input text. Formal: Using the averaged vector gives us the probability distribution p(t, c). To make it clearer, we'll write this as p(t, c) = [p(t), ..., p(c)] ∈ R n^m. Formal: The total probability p(t, c) is calculated using the dot product between p(t) and p(c), and it's proportional to the sum of p(t) for every pair (t, c) in the whole input text. Formal: The probability of a word t in the text t is calculated by multiplying the probability of a word c in the text t, multiplied by the probability of a word t in the context c, and then scaled by the log probability of word t in the context c. Formal: Formal: Also, if you have a document d, the probability of a word t in the context d, and another word c in the context d, the probability of word t in the context c can be written as p(t, c | d) ∈ R n^m, where n is the length of the document d. Formal: Formal: Formal: Finally, the probability of a document d can be written as p(t | d) ∈ R n^m, where n is the length of the document d. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D14",
        "filename": "D14-1175.json"
    },
    {
        "casual_text": "We compared our SkipBERT with some basic models—specifically, 6-layer and 4-layer models. The results show that SkipBERT beats all the other methods when it comes to the GLUE score. When we look at TinyBERT, since we mostly followed their distillation process, our approach still performs better across all tasks. BERT-EMD takes things a step further by using a more advanced, task-specific distillation method based on TinyBERT, which boosts performance even more. However, SkipBERT still comes out on top when you look at the overall score.",
        "formal_text": "By comparing with baselines (we compare with 6-layer models and 4-layer models separately), we can see that SkipBERT outperforms all compared approaches in terms of GLUE score. Compared with TinyBERT, as we mainly follow their distillation process, our approach shows clear advantages on all tasks. BERT-EMD employs a more sophisticated task-specific distillation process based on general-distilled TinyBERT, and further improves the overall performance. Nevertheless, SkipBERT still maintains an advantage in the overall score.",
        "GPT2_formal_text": "_i is the final attention mask for the i-th token. After the last token is decoded, it gets turned into the output layer for the decoder. The rest of the encoder, decoder, and semantic graph are then passed along to the next layer. Formal: Basically, our model works with a hidden Markov Model (HMM) that uses a multi-head attention mechanism to predict the next word. It has three types of attention: one for each token, one for the context around the token, and one for the input word itself. In each dimension, the attention is calculated based on the value of the multi-head attention head at a specific token. Formal: This HMM is trained with an average of 1.3 and 1.4 token attention, respectively. Formal: We used the BERT-base model for this, which is based on the transformer architecture. Formal: Lastly, we used the top-k model for training and tested the results. Formal: For the multi-head attention mechanism, we used two-layer perceptrons with a learning rate of 1e-4 to calculate the attention mask. Formal: We also used the Adam optimizer with a learning rate of 1e-3 and a batch size of 64 to train the model. Formal: We trained our model on a regular, non-parallel GPU GPU setup with 32GB of memory. Formal: For generating the semantic graph, we used a three-layer perceptron with a learning rate of 1e-5 and a batch size of 64. Formal: To keep the model simple, we also used the Adam optimizer with a learning rate of 1e-4 and a batch size of 64. Formal: Lastly, we used the top-k model for training and tested the results. Formal: Formal: Figure 3 gives an example of the final semantic graph. Formal: We averaged the results across all the word tokens in the vocabulary to get the final score. Formal: We used the BERT-base model for this. Formal: We trained the model on a regular, non-parallel GPU setup with 32GB of memory. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.503.json"
    },
    {
        "casual_text": "In this paper, we're not looking at all proof nets, just a subset of the multiplicative ones—specifically, those from intuitionistic implicative linear logic. Here, sequents have multiple antecedent formulas but only one succedent formula. To handle the intuitionistic aspect with proof nets (since we're dealing with one-sided sequents), we use the idea of polarities: inputs are negative (,) and outputs are positive (o) (Danos, 1990; Lamarche, 1995). This helps us label the formulas—positive ones are succedent formulas, and negative ones are antecedent formulas.",
        "formal_text": "In this paper, we do not consider all the proof nets, but a part of the multiplicative ones: those of the intuitionistic implicative linear logic. In this case, sequents are made of several antecedent [brmulas, but only one succedent formula. To deal with tile intuitionistic notion with proof nets (since we consider one-sided sequents), we use the notion of polarities with the input (, : negative) and the output (o: positive) (Danos, 1990; Lamarche, 1995) to decorate formulas. Positive ones correspond to succedent formulas and negative ones to antecedent formulas.",
        "GPT2_formal_text": "; d = (u_j, t_i, f_i) = l_A_u(u_j, t_i, f_i); + |t_i, f_i|:= 1/|t_i, f_i|. Formal: Equation (9) is a bit complicated. It’s not fully defined yet, so it’s not super clear how to represent it in a straightforward way. Formal: With the help of equation (8), we can write equation (9) as: Formal: So, to make the calculations work faster, we’re thinking of using a memory network. We’re planning to use a CNN with an attention mechanism to capture the connection between words, u_j and t_i. The special attention mechanism is built on the attention mechanism from GANs (Hochreiter and Schmidhuber, 1997). It’s designed to make sure the attention weights are focused on the important words. To make this happen, we’ll use the same attention mechanism for the words in the response. Formal: For our next steps, we’re planning to look into: Formal: But, we’ll also start looking into more complex models and use linguistic knowledge to make sure we’re capturing all the important bits in the response. Formal: In the future, we’ll work on creating a single framework to represent each sentence. We’ll also explore methods to handle unigram and bigram words. Formal: In Section 5.1, we’ll talk about the experiments we did and share the results. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "A00",
        "filename": "A00-2010.json"
    },
    {
        "casual_text": "The first step in the process we're talking about is creating a special kind of language representation that fits the topic we're focusing on. This method looks for groups of n-grams—which are just chunks of words—in paragraphs that seem to pop up a lot and connect with other paragraphs. We call these \"useful\" n-grams because they show up often and are spread across different texts in the same area of interest.",
        "formal_text": "The fundamental stage in the process described above is the generation of a domain specific semantic representation. The approach identifies clusters of useful n-grams within paragraphs which correlate with other paragraphs. The term useful defines n-grams that have certain qualities, such as a high frequency of occnrrence, and a wide distribution over texts within the domain.",
        "GPT2_formal_text": "_range. We got a variety of numbers from each model, ranging from 0 to 1, to use as the loss function. In the end, the loss function looks like this: Formal: The goal is to calculate the log loss for the different parts of the sentence. For the sentence-level model, we start by looking at the last word in the current sentence and the words before and after it. After that, we add up all the words in the sentence to get the overall loss. Formal: The model's parameters are shown in Table 4. Formal: We also included the loss function based on a pretrained model. Formal: We tried different ways to train the sentence-level model. Our starting points were the skipgram model (from Mikolov et al., 2013) and the BiDAF model (from Luong et al., 2016). Formal: Our system is built using PyTorch 4. The whole process is laid out in Algorithm 1. Formal: We set the hidden size to 8192. Formal: To train the bi-directional attention model, we trained it on a dataset called WN18 (Zeng et al., 2019). Formal: We also trained a model for character-level BiDAF. Formal: For the character-level model, we trained it on the training set from WN18. Formal: We also tested a model for word-level BiDAF. Formal: To train the word-level model, we trained it on WN18. Formal: We tested the same setup for the sentence-level model and the character-level model. Formal: We used PyTorch 4 to create the pretrained models. Formal: Finally, we trained the BiDAF model with a batch size of 64. Formal: The accuracy for the character-level model was 89.9%. Formal: Formal: Table 5 shows the results from different systems. Formal: We also looked at the results for the word-level model. Formal: The model performed better than the sentence-level model, but it still didn't perform as well as the character-level model. Formal: Formal: The word-level model still outperformed the sentence-level model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C94",
        "filename": "C94-2171.json"
    },
    {
        "casual_text": "We considered the features of the given kernels and the setup of the semantic-annotated data. Based on that, we came up with the tree structures shown in figures 2(a), 2(b), and 3 for STK and PTK, and sequential structures for SK, as explained below (all these structures are based on the same example from Section 3, which is \"Ho un problema col monitor\"). The structures we used for SK are:",
        "formal_text": "We took into account the characteristics of the presented kernels and the structure of semantic annotated data. As a result we designed the tree structures shown in figures 2(a), 2(b) and 3 for STK and PTK and sequential structures for SK defined in the following (where all the structures refer to the same example presented in Section 3, i. e. Ho un problema col monitor). The structures used with SK are:",
        "GPT2_formal_text": "= [q_s, q_t] = arg_t, arg_c, arg_b, arg_w. Formal: The things that can come from these translations are: Formal: - The sentence itself. Formal: - The type of action (like \"go\" or \"jump\"). Formal: - The possible answers (like \"yes\" or \"no\"). Formal: - The attribute (like \"who\" or \"animal\"). Formal: - The name of the action (like \"go\" or \"jump\"). Formal: - The name of the attribute (like \"who\" or \"animal\"). Formal: - The type of query (like \"answer\" or \"location\"). Formal: - Whether the action is something the agent can do. Formal: - Whether the action is part of the dialogue. Formal: - Whether the action is linked to something in the database. Formal: - Whether the action has a specific location. Formal: - Whether the action can be done at any time during the conversation. Formal: - Whether the action has a specific time. Formal: - Whether the action can be done at any point in time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done at any point in time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can be done before the current time. Formal: - Whether the action can",
        "directory": "D09",
        "filename": "D09-1112.json"
    },
    {
        "casual_text": "Lastly, there's the \"end with the same punctuation\" thing. This is another yes-or-no feature. It's a 1 if both end with the same kind of punctuation—like a period, exclamation mark, etc.—or if neither has any punctuation at the end. If that's not the case, then it's a 0.",
        "formal_text": "Finally, ( ) is the \"end with the same punctuation\" feature. This is also a binaryvalued feature which is 1 if both and end with the same type of punctuation: period, exclamation mark, etc. It is also 1 if both and lack final punctuation. ( ) is 0 otherwise.",
        "GPT2_formal_text": "ed = s_masked;\n     Casual: This method is used in the same setup as the response selection process. In the model we're talking about in this paper, we can't find any word in the prompt that matches the input sequence. So, to make the response selection process work, the prompt has to be broken down into smaller, more manageable pieces (Mihalcea and Tarau, 2004). Formal: Alright, let's break this down in a simpler way:\n\n- Input: A prompt with one or more words, each with a special number or symbol.\n- Input length: how many words in the prompt.\n- Offset: how much the input length changes the output length.\n- Layer: which layer (or layer) in the model is handling the input.\n- Layer position: where the input starts or ends.\n- Output: the output length.\n\nLet's call this whole process \"response selection.\" Formal: The input is a prompt with words or symbols, and the output length is the length of the input. The model is split into different layers, each with its own set of parameters, and each layer has its own output length. Formal: To make the response selection process work, the output length in each layer is figured out based on the input length in the previous layer. Formal: The input length in each layer and the output length are both calculated using the input length in the previous layer. Formal: The model has a set of parameters, and each layer's output length is based on the input length in that layer. Formal: The model can use this structure to create responses in a way that’s different from how other models process prompts. Formal: In this paper, the output length in each layer is calculated using the input length in the previous layer. Formal: A model is split into different layers, each with its own set of parameters. Formal: The model uses this structure to create responses in a way that’s different from how other models process prompts. Formal: A model is split into different layers, each with its own set of parameters. Formal: The model uses this structure to create responses in a way that’s different from how other models process prompts. Formal: Formal: The model has a set of parameters, and each layer’s output length is calculated using the input length in that layer. Formal: Formal:",
        "directory": "eamt",
        "filename": "2012.eamt-1.37.json"
    },
    {
        "casual_text": "Over the last few years, there's been a growing interest in using syntax-based methods for statistical machine translation. A lot of the work has been focused on improving the translation model, not the language model. But, the big differences in syntax between parallel texts make it tricky to learn syntax-based translation rules effectively. Maybe putting more effort into syntax-based approaches within the language model could be a better way to fix the grammar issues in most machine translation results.",
        "formal_text": "In the past few years there has been a burgeoning interest in syntax-based approaches to statistical machine translation. Most of this effort has been directed at the translation model rather than the language model; however, the level of syntactic divergence in parallel text makes it difficult to reliably learn syntax-based translation rules. Focusing on syntax-based approaches within the language model might be a more direct way to address the poor grammatically of most machine translation output.",
        "GPT2_formal_text": "\"\n       Casual: The main reason for this is that the model takes into account the context of the image when creating the responses. If the response doesn’t match the image, it’s not really useful. But if the response does match the image, it can help show how the model understands the situation. Formal: In this part, we’re checking out some examples of responses that don’t match the image. Formal: The responses are pretty much the same as the ones in Table 4, but there are a few changes. Formal: You can see that in the sentence \"I want to see the ocean,\" the image and the verb ‫يقر‬ ‫مقت‬ ‫متك‬‬ ‫يقر‬ ‫يكان‬ are swapped. Formal: This happens because the network picks up on the word ‫ي‬ ‫مقت‬ ‫مت‬ ‫متك‬‬ ‫ي‬ ‫يقر‬ ‫يك‬ ‫ي‬ ‫يضا‬ ‫مت‬ ‫أي‬ ‫عاق‬ ‫الفضا‬. The model then uses this to guess that the beach is on the far right of the image. Formal: This is because the model picks up on the word ‫ي‬ ‫مقت‬ ‫مت‬ ‫متك‬ ‫ي‬ ‫يقر‬ ‫يك‬ ‫ي‬ ‫يضا‬ ‫مت‬ ‫عاق‬ ‫الفضا‬. Formal: The model also uses the word ‫ي‬ ‫مقت‬ ‫مت‬ ‫متك‬ ‫ي‬ ‫يقر‬ ‫يك‬ ‫ي‬ ‫يضا‬ ‫مت‬ ‫عاق‬ ‫الفضا‬. Formal: Form",
        "directory": "amta",
        "filename": "2008.amta-papers.16.json"
    },
    {
        "casual_text": "I think it's a good idea to comfort them as you learn more about what's going on. Have you considered talking to your parents or a close friend about this?",
        "formal_text": "I should comfort him/her when gradually learning about his/her situation (Providing Suggestions) Have you thought about talking to your parents or a close friend about this?",
        "GPT2_formal_text": "_2d model. Formal: You can use the loss function L_s to calculate a mask matrix. Formal: After running the model, we get the output logits for each dialogue act. Formal: Here, e_i_1 and e_i_2 are the attention vectors for the i-th dialogue act in the attention sequence, and l_s_i is the loss function. Formal: After that, we create a simple classifier by combining the outputs from the act generation model and the masked language model. Formal: We use the RoBERTa vocabulary (from Liu et al., 2019) to represent the labels for the dialogues. Formal: We use the label embedding from this classifier to encode the masked language model, which is the same as what the act generation model generates. Formal: Lastly, we use a classifier to predict the label distribution for the generated dialogues. Formal: All these steps together are what we call the act generation act sequence, which is shown in Figure 1. Formal: In our experiments, we used the act sequence from Section 3.1 for our act model, just like in the original paper by Liu et al. (2019). Formal: In this paper, we explain how to create an act sequence that matches the input utterances and the act words. Formal: We picked the act sequence based on the number of input utterances, and then we used a pre-trained multi-task learning model that uses the masked language model to predict the labels for the generated acts. Formal: Formal: In the next section, we’ll also share some thoughts on how we handle the output of the act generation model and how we can improve the act sequence generation process. Formal: The process of generating an act sequence is outlined in Figure 1. Formal: We’ve got an act sequence called S_a that’s made up of S_s_1 to S_s_n. Formal: After that, we randomly pick one act word from S_a to create a new act word called s_a_i. Formal: To make an act word, we use a pre-trained act model called M_a. Formal: In this act word, the attention vector h_a_i is represented by a masked word embedding called p_a_i. Formal: We also use a classification model to predict the label for the generated",
        "directory": "acl",
        "filename": "2021.acl-long.269.json"
    },
    {
        "casual_text": "We’re also working with a semi-supervised learning method called self-learning or pseudolabeling. Basically, this involves training models using predictions made by an earlier version of the model on unlabeled data. This approach has been pretty successful across a bunch of language tasks, like named entity recognition (Collins and Singer, 1999), word sense disambiguation (Mihalcea, 2004), and parsing (McClosky et al., 2006). It’s also been applied in computer vision, and there’s a good overview of that. For task-oriented dialog systems, Cho et al. (2019) found that using self-learning to introduce new features led to significant error reduction. While our method is still based on self-learning using MARUPA-labeled data, we’re taking it a step further by incorporating user feedback, like paraphrasing and friction, as extra signals to improve the self-learning process.",
        "formal_text": "Our work is also related to the semi-supervised learning approach known as self-learning or pseudolabeling, in which models are trained on predictions that a previous version of the model made on unlabeled data. This idea has been successfully applied to a wide range of language tasks, e. g. named entity recognition (Collins and Singer, 1999), word sense disambiguation (Mihalcea, 2004) and parsing (McClosky et al. , 2006). Successful applications are also known from computer vision, of which gives an overview. For task-oriented dialog systems, Cho et al. (2019) report substantial error reductions using self-learning to bootstrap new features. While the use of MARUPA-labeled data for training is at its core self-learning, our approach goes further by leveraging user feedback in the form of paraphrasing and friction as additional signals to guide the self-learning process.",
        "GPT2_formal_text": "= query, query_type = type, response = response_type) { j, k, l = j; k, l = l; } for k in 1:n.                Casual: Once we’ve got the query, query_type, and response, we look at all the possible types of responses—like regular, closed-form, or unknown—and pick the one that has the highest chance of being correct. Formal: Using the previous sentence, we predict the next word by trying to predict the current word. This is pretty similar to what the annotator said. Formal: If the current word is a word from the sentence, we use the next word to predict the next word. We’re skipping the context of the current word to keep things simple. Formal: If the current word is a part of the sentence, we predict the next word based on the current word. Formal: We’re also skipping the context of the current word to keep things simpler. Formal: For both scenarios, we calculate the probability P(w | s) for the next word w based on the current word s. Formal: Formal: We can use this probability to estimate the overall probability, P(w | s). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-industry.3.json"
    },
    {
        "casual_text": "To dive deeper into the issues we're looking at, let's quickly go over some background info and basic terms.",
        "formal_text": "To better elaborate the investigation of the above problems, background knowledge and notations are briefly introduced here.",
        "GPT2_formal_text": ". It's a bit simpler than the regular attention mask, but we think it makes a big difference because we think it gets rid of all the noise and gives the model more freedom to pick and choose the right parts. Formal: We've got the baseline model and the one trained with attention masks on the left, and on the right, we have the sentence-level model and the one trained with attention masks on the right. Formal: We're using the same setup as in the original work. Formal: We're using the same settings as in the original work. Formal: This project is part of the TRANSFORMER project, which is led by DLTR. Formal: There are a bunch of different methods people have come up with to train models with attention, and we're only comparing our approach to the most effective ones based on how well they perform on the development set. Formal: We're using the exact same parameters as Artetxe et al. (2019) and Huang et al. (2019). Formal: For the language model, we're using the basic Transformer setup, with the same number of parameters as the Transformer model. Formal: We're using the exact same setup as Artetxe et al. (2019) and Huang et al. (2019). Formal: We're using the exact same parameters as Artetxe et al. (2019). Formal: We're using the exact same parameters as Huang et al. (2019). Formal: We're using the exact same parameters as Artetxe et al. (2019). Formal: Lastly, we're using the same settings as Artetxe et al. (2019). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "blackboxnlp",
        "filename": "2021.blackboxnlp-1.16.json"
    },
    {
        "casual_text": "In Figure 2: On the inside pass (left side), DIORA combines two nearby vectors. On the outside pass (right side), DIORA calculates the values for a specific span (i, j) by using its sibling inside span (j +1, k) and outside spans (0, i 1) and (k + 1, n 1). The sibling span during the outside pass can be to the left of the target span, so the indexing gets adjusted accordingly.",
        "formal_text": "Figure 2: In the inside pass (left) DIORA composes two neighboring vectors. In the outside pass (right) DIORA computes the values for a target span (i, j) recursively from its sibling inside span (j +1, k) and outside spans (0, i 1) and (k + 1, n 1). The sibling span on the outside pass can appear to the left of the target span, in which case the indexing is adjusted.",
        "GPT2_formal_text": "\" method, which is explained in Section 4.2.3. Formal: We use the average of the hidden states from the last layer of the LSTM (h t−1) to represent the internal representation. Formal: To get a full picture of the hidden state, we use a linear projection of it onto the hidden state vector h t−1 using a linear layer, like the one shown in Figure 1. This projection gives us a dense vector that’s both orthogonal and non-linear. Formal: If the hidden state is aligned with the last layer of the LSTM, we set h t−1 = l t. Formal: If the hidden state isn’t aligned with the last layer, we just set h t = l. Formal: We train the model using cross-entropy loss to get a reward function that works across different domains. Formal: For this task, we use the Adam optimization algorithm with a learning rate of 0.001. Formal: We use cross-entropy loss to optimize the loss function. Formal: We use the gradients from this gradient to update the model. Formal: We train the model with cross-entropy loss to get a reward function that works across different domains. Formal: We train the model using cross-entropy loss to get a reward function that works across different domains. Formal: We train the model with cross-entropy loss to get a reward function that works across different domains. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.392.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\n- HMGCN's different versions:  \n  - HMGCN without \"cat\" just uses GCN co and GCN prop to figure out the type.  \n  - HMGCN without \"prop\" uses GCN co and GCN cat to do the same type inference.",
        "formal_text": "• HMGCN's variants: HMGCN no cat only use GCN co and GCN prop to make type inference. HMGCN no prop only use GCN co and GCN cat to make type inference.",
        "GPT2_formal_text": ", m = 0.1, (1, 1)) = (1, 1) × (1, 1) m = m + (1, 1) Formal: We run a linear mixed effects model using regression to figure out the relationship between attention scores and the eye movements. Formal: For each pair of sentences, we calculate the attention score between the right-branching sentence and the left-branching one, like this: Formal: If the sentence has two sentences, we add up the attention score for each sentence. Formal: We use the same linear mixed effects model for the left-branching sentence as in Equation 1, but this time, the attention score is calculated across both sentences. Formal: We figure out the attention score from the left-branching sentence using the regression model, m = 0.1, (1, 1). Formal: After running the regression for the left-branching sentence (let's call it \"l\"), we get the attention score for \"l l\" by adding up the attention score for each sentence. Formal: To see how the attention scores change over time, we calculate the attention score for each sentence in the left-branching sentence (l l) using the regression model, m = 0.1, (l l ). Formal: We also take the average of the attention scores for the sentence pairs in the left-branching sentence (l l ) and the one before it (l l +1 ) to get the attention score for the current sentence (l l+1). Formal: Finally, we calculate the attention score for the sentence pair (l l+1 ) by adding up the attention score for each sentence pair. Formal: Finally, we calculate the attention score for the sentence pair (l l+1 ) by adding up the attention score for each sentence pair. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-1502.json"
    },
    {
        "casual_text": "The results for our Farsi-to-English MT system are in Table 2. We compared our approach to two baselines: monotonic translation and a distance-based penalty model, which is pretty well-known. The distance-based model didn’t make much of a difference: the BLEU score went up from 29.1% to 29.4%, and the WER improved by the same amount. When we switched to the run-based penalty model with the 4 features mentioned in Section 3.4, we saw a bigger jump—the BLEU score improved by 1.5% and the WER by 0.7%, compared to the monotonic translation. So, the run-based model clearly did better than the distance-based one.\n\nLooking at the scaling factors for the short, medium, and long-range penalties after optimization, we noticed that the short-range factor was small but negative, which basically gave a bonus for local reorderings. On the other hand, the penalty for a long-range jump was 10 times higher than for a medium-range one. Table 3 shows examples of how the run-based model fixed word order and improved translation quality compared to the distance-based model.\n\nNext, we tried applying parse-based reordering rules. When we used these rules on the source sentences and then did monotonic SMT (the \"hard\" reordering in Table 2), the MT error measures actually got worse.",
        "formal_text": "The experimental results for the Farsi-to-English MT system are shown in Table 2. There are two baselines to which we compare our method: the monotonic translation and the translation using the well-established distance-based penalty model. We see that the positive influence of the distance-based model is small: the BLEU score improves from 29. 1 to 29. 4%, the same absolute improvement is achieved in terms of WER. When we now use the run-based penalty model with the 4 features described in Section 3. 4, we improve the BLEU score by 1. 5% absolute and WER by 0. 7% absolute as compared to the monotonic translation. Thus, the run-based penalty model clearly outperforms the distance-based penalty model. Checking the feature scaling factors for the short, medium, and long range new run penalties after the optimization, we observed that the factor for the short-range feature was small, but negative, thus assigning a bonus to local reorderings. In contrast, the penalty for a longrange jump was 10 times higher than for a mediumrange one. Examples of improved translanslation word order and quality when using the run-based penalty model instead of the distance-based model are presented in Table 3. Next, we tested the application of parse-based reordering rules. Applying these rules to the source sentences and then performing monotonic SMT (the \"hard\" reordering in Table 2 ) resulted in a degradation of the MT error measures.",
        "GPT2_formal_text": "(1, 2, 3), which include the most common headings. As you can see in Table 1, we’ve tweaked our model a bit to handle this more complex query. To do this, we’ve added a multi-head attention mechanism that takes into account the topic information we’re working with and the co-occurrence info of the headings in the text. To give you an idea of how it works, we’ve included an example in Figure 3. Formal: So, we’ve got a sentence with four headings, each with a topic and co-occurrence pair. The words in the sentences are grouped into clusters using TFIDF vectors, and we’re trying to guess the topic of each cluster. The query for each cluster is made up of the words in that cluster. Formal: Basically, we’re using the topic and co-occurrence vectors to calculate the query. Formal: For each topic and co-occurrence pair, we’ll calculate the query using the cluster vectors that were calculated. Formal: For each word in the query, we’ll figure out the topic vector from the TFIDF vectors in that word’s cluster vector. Formal: Formal: From all this, we can calculate the query vector for the cluster. Formal: Then, we’ll use the topic vector for the query to find the co-occurrence vector for the word. Formal: Finally, we’ll combine the topic vector for the query and the co-occurrence vector for the word to get the query vector for the text. Formal: Formal: We’re using a tfidf vocabulary of three hundred words and using BERT (from Devlin et al., 2019) as the language model. Formal: This is what we’ve learned in Section 5. Formal: We’re showing how we’re using this whole process in action. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "amta",
        "filename": "2010.amta-papers.29.json"
    },
    {
        "casual_text": "Another way to combine phrases works at the sentence level. This method looks at how different machine translation systems handle the same source sentence. It gathers all the phrase pairs that these systems use to translate that sentence. Basically, it gives more weight to phrase pairs that are picked by lots of different decoders.",
        "formal_text": "Another phrase combination strategy works on the sentence level. This strategy relies on the consensus of different MT systems when translating the same source sentence. It collects phrase translation pairs used by different MT systems to translate the same sentence. Similarly, it boosts common phrase pairs that are selected by multiple decoders.",
        "GPT2_formal_text": "* self_attention = self_attention * tanh(D_attn(attn)) * self_attention * l_attn. Formal: So, in this setup, we're training the network using the log probability, which is the mean, and the cross-entropy loss, which is the variance. The idea is to minimize this loss, which is the negative log of the loss function, to get the final representation. Formal: The losses for the attention networks and the CNNs are handled by the self-attention network. It's a learned component that learns the parameters. Formal: Lastly, we check the hidden states in the input sequences and use the log probability to estimate the softmax probability for each token. Formal: Formal: So, the final output is a mix of the attention networks, the CNNs, and the hidden states. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D07",
        "filename": "D07-1029.json"
    },
    {
        "casual_text": "Entity Extraction. In our system, entities come in three flavors: tasks (like \"Question Answering\"), datasets (like \"SQuAD2.0\"), and metrics (like \"F1\"). To handle this, we use two methods: a dictionary-based approach and a learning-based one. Here's how it works.\n\nFirst, we started with manually curated dictionaries from paperswithcode. But since these dictionaries might not cover everything, we built a module to automatically pull out entities. Unlike earlier work that mostly looked at abstracts (Gábor et al., 2018; Luan et al., 2018), we go through the whole paper and grab the three types of entities related to the main findings.\n\nWe framed this as a textual entailment task: we treat the paper content as text and the Task-Dataset-Metric (TDM) triples as the hypothesis. This approach makes our model focus on spotting patterns between the text and different triples. We trained our module on a dataset of 332 NLP papers and it did pretty well, scoring a macro-F1 of 56.6 and a micro-F1 of 66.0 on a test set of 162 papers (Hou et al., 2019). Overall, our system tagged 872 tasks, 345 datasets, and 62 metrics from the entire collection of papers.",
        "formal_text": "Entities Extraction. Entities in our system are of three types, task (e. g. , \"Question Answering\"), dataset (e. g. , \"SQuAD2. 0\"), and metric (e. g. , \"F1\"). We utilize both a dictionary-based approach and learning-based approach as follows. First, we adopted the manual curated dictionaries of paperswithcode 5. Since those dictionaries may not cover all evolving topics, we further developed a module that automatically extracts entities. Differently from previous work on information extraction from scientific literature which mainly focused on the abstract section (Gábor et al. , 2018; Luan et al. , 2018), we analyze the entire paper and extract the above three types of entities that are related to the paper's main research findings. We cast this problem as a textual entailment task: we treat paper contents as text and the targeting Task-Dataset-Metric (TDM) triples as hypothesis. The textual entailment approach forces our model to focus on learning the similarity patterns between text and various triples. We trained our module on a dataset consisting of 332 papers in the NLP domain, and it achieves a macro-F1 score of 56. 6 and a micro-F1 score of 66. 0 for predicting TDM triples on a testing dataset containing 162 papers (Hou et al. , 2019). In total, our system indexed 872 tasks, 345 datasets, and 62 metrics from the entire corpus.",
        "GPT2_formal_text": "\" and \"ans_t\" are the output relations for the text and answer. For instance, in the example shown in Figure 1, the text and answer have \"nikes\" and \"apparel\" as their query relations, while the text and answer have \"porn\" and \"stars\" as their response relation. Formal: In this part, we're checking if the text and answer have \"a\" and \"an\" as their output relation vectors. Formal: The answer and text are represented as a triple (s, r, a). Here, s stands for the query relation, and r is the response relation. If the answer has \"an\" as its output vector, the text and answer are represented as a triple (s, r, a). Formal: But if the answer has \"a\" as its output vector, the text and answer are represented as a triple (s, r, a). Formal: We use these triple representations to make predictions for the next hidden state and attention head. Formal: If the answer and text have \"an\" as their output vectors, the text and answer are represented as a triple (s, r, a). Formal: The final output vector for the text and answer is a triple (s, r, a). Formal: We're checking if the answer and text have \"b\" and \"an\" as their output vectors. Formal: If the answer and text have \"b\" and \"an\" as their output vectors, the text and answer are represented as a triple (s, r, a). Formal: The final output vector for the text and answer is a triple (s, r, a). Formal: We're checking if the answer and text have \"c\" and \"an\" as their output vectors. Formal: The final output vector for the text and answer is a triple (s, r, a). Formal: Formal: Formal: Formal: Formal: We're checking if the answer and text have \"d\" and \"an\" as their output vectors. Formal: The final output vector for the text and answer is a triple (s, r, a). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-3036.json"
    },
    {
        "casual_text": "On the other hand, there were a lot of instances where people talked about membership stuff. It was pretty clear that folks weren’t sure what the robot needed to do the spatial task, and that confusion showed up in the different ways they explained it. Take a look at Figure 2 for a straightforward example.",
        "formal_text": "In contrast, there were many cases of membership analysis. There was clearly great uncertainty about the robot's prerequisites for carrying out the spatial task and this was explicitly specified in the users' varied formulations. A simple example is given in Figure 2.",
        "GPT2_formal_text": "function is basically the same as the attention_attn function, but it uses a lookup table instead of a buffer. Basically, it looks for the nth best value in the table and adds it to the output vector. Formal: For each word in the source sentence, we figure out its representation using two main techniques: multi-head attention and attention pooling. Formal: Instead of calculating the attention weight using multi-head attention, we can use attention pooling to focus on specific parts of the sentence. Formal: A lot of the time, the input sentence is pretty short and simple. This means we don't need to do any fancy math to estimate the weights. We just use a straightforward linear transformation to do it. Formal: To find the attention weights, we use a special lookup table called the attention_mask table. This table checks if a word is a certain length and the value in its first position. Formal: After finding the attention weights, we do a max pooling step to get the final output vector. Formal: After applying max pooling, we also do a linear transformation to get the attention vectors. Formal: Finally, the final output is a matrix with dimensions nxd, where d is the number of words. Formal: In this project, we're using the word2vec model for the neural language model (Luong et al., 2017). Formal: For the character embedding layer, we're using GloVe (Pennington et al., 2014) with a 3-gram window size (6). Formal: For the character embedding layer, we're using a max-pooling layer to get the character embedding vectors. Formal: For the character embedding layer, we're also using a max-pooling layer to get the character embedding vectors. Formal: We also calculate the dot product of the character embeddings and the embedding vector to get the dot product weight. Formal: The results are in the figure. Formal: We train the model using the CTC loss function on the training set to minimize the cross-entropy loss. Formal: The results from each layer are shown in the figure. Formal: We train the model using the CTC loss function on the training set to minimize the cross-entropy loss. Formal: Lastly, the results from each layer are shown in the figure. Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E06",
        "filename": "E06-1024.json"
    },
    {
        "casual_text": "根据上面的对齐结果，我们可以把同一位置上相同的词合并起来，然后通过不同译文之间的对齐信息来建立一个混淆网络。这个网络的结构就像图4展示的那样。在这个混淆网络里，每两个节点之间的弧线上的词表示它们是最终融合结果中相应位置的候选词。对于混淆网络节点i和i+1之间的弧线上的候选词，第j个候选词的置信得分可以用公式(1)来计算。这个公式考虑了多个模型和译文的情况，其中τ_u是系统u的先验概率，τ_v是词所在译文的权重，通常使用均匀权重，但有时为了给排名靠前的译文中的词更高的权重，也可以使用基于排名的权重，即每个词的概率乘以1/(1+v)。C_ω是第u个模型第v个译文中的词，如果在混淆网络节点i和i+1之间的弧线上出现候选词，则该值取1，否则取0。u是归一化因子，确保节点i和i+1之间所有候选词的总置信度为1。\n\n混淆网络采用了一种投票策略。在合并同一个位置上的词时，需要计算每一个在该位置上的词的后验概率。这个值通常受两个因素影响：一个是融合译文的先验概率，另一个是词所在译文的权重。假设我们有一个源语言的句子S，混淆网络解码就是找到满足下面式(2)中的目标语言句子。其中α、β、γ、δ分别对应融合过程中产生翻译假设的词的置信度P_AL、插入惩罚N_nulls(E)、语言模型得分P_LM、长度惩罚N_words(E)的权重。\n\n最后，我们来看看实验结果和分析。",
        "formal_text": "根据上述对齐结果，对同一位置的相同的词进行合并，通过不同译文间的词对齐信息建立 混淆网络，其结构如图輴所示。 图 图 图 4. 根据词对齐生成的一个单混淆网络 在构建好的混淆网络中，每两个节点之间的弧线上的词表示它们是最后融合结果中在相应 位置的候选词。对于混淆网络节点i和i 輫 輱弧线上的候选词中，第轪个候选词的置信得分，如公 式(輸)所示。 ω i, j 輽 µ N µ=1 N v=1 τ µ τ v C ω 輨輸輩 上式给出了在有轎个模型，每个模型提供轎个译文参与融合时，词的置信度计算公式。其 中，τ u 是系统u对应的先验概率，τ v 是词所在译文的权重，一般采用均匀权重，但是有时为了给 排名靠前的译文中的词赋以更高的权重，也可以采用基于排名的权重，即出自第v个译文中的每 一个词的概率都要乘上 1 1+v ，C ω 是第u个模型第v个译文中的词，如果在混淆网络节点i和i 輫 輱之 间的弧线上出现候选词，则该值取輱，否则取輰。u为归一化因子，它保证在节点i和i 輫 輱之间出 现的所有候选词的总置信度为輱 輨李茂西轥轴 轡转輮輬 輲輰輱輰輩。 4. 3. 2 混 混 混淆 淆 淆网 网 网络 络 络解 解 解码 码 码 混淆网络采用的是一种投票策略。在合并同一个位置上的词时，需要计算每一个在该位置 上的词的后验概率。一般地，在基于混淆网络的融合模型中该值受两个因素的影响，一个是融 合译文的先验概率，另一个是词所在译文的权重 輨朱俊国轥轴 轡转輮輬 輲輰輱輹輩。假设给定一个源语言的 句子轆，混淆网络解码就是求满足下面式(輹)中的目标语言句子。 E * 輽 argmax E 輨α 转软轧 P AL 輫 βN nulls 輨E輩 輫 γ 转软轧 P LM 輫 δN words 輨E輩輩 輨輹輩 其 中α、β、γ、δ分 别 对 应 融 合 过 程 中 产 生 翻 译 假 设 的 词 的 置 信 度P AL 、 插 入 惩 罚N nulls 輨E輩、语言模型得分P LM 、长度惩罚N words 輨E輩的权重。 5 实 实 实验 验 验结 结 结果 果 果与 与 与分 分 分析 析 析 5.",
        "GPT2_formal_text": ") { (g_t, h_t) = t; } } p_t = p_t ∈ {0, 1} p_t = ∆(r_i) ∈ {0, 1} r_i = 1/n r_i = 1/n\n\nIn this step, we update the model's parameters using the updated cross-entropy loss, along with the parameters of the distillation method. After that, the probability distributions are adjusted using the residual connections between the model's parameters and the global prediction distribution. Formal: Here's how it works step by step for the model with δ_i in Equation 3:\n\n1. **Pre-processing**: We clean up the text by removing stop words and common stop words. We also remove stop words after the first 10 tokens, unless they're in the vocabulary.\n\n2. **Model setup**: We use a convolutional neural network to create the cross-entropy loss, and we set a word length of 10 for both the token and vocabulary size.\n\n3. **Distillation**: We pick a single model (δ_i) to split the cross-entropy loss across all the tokens. Then, we pick a smaller group of tokens (δ_i) to refine the model's parameters. This smaller group is then used as the training set for the other models. Formal: We're also using a maximum entropy classifier to calculate the cross-entropy loss for the whole model, and we set the learning rate λ to 0.9. Formal: Finally, the final model is trained using the cross-entropy loss.\n\nIn short, we train the model using cross-entropy loss to minimize the cross-entropy loss and then use the parameters of the distillation method to refine the model's parameters. Formal: The main goal of distillation is to maximize the cross-entropy loss, which is a straightforward way to do that. Formal: Training the model using cross-entropy loss is a straightforward way to reduce the cross-entropy loss. Formal: Since the cross-entropy loss is already optimized for the cross-entropy loss, the model learns to optimize it directly. Formal: Formal: We optimize the cross-entropy loss using the gradient from the prior distribution and then use the parameters of the distillation method to refine the model's parameters. Formal",
        "directory": "ccl",
        "filename": "2021.ccl-1.5.json"
    },
    {
        "casual_text": "Geolocation prediction can basically be approached as either a regression or a classification problem. But in reality, since predicting exact coordinates is pretty tricky, people usually mix regression with classification (Eisenstein et al., 2010; Lourentzou et al., 2017; Fornaciari and Hovy, 2019b). Most of the time, though, it's treated as a classification problem, where you need to figure out which geographic area something belongs to and label it accordingly.",
        "formal_text": "Geolocation prediction can, in principle, be modeled both as regression and as classification problem. In practice, however, given the difficulty of predicting continuous coordinate values, regression is often carried out in conjunction with the classification (Eisenstein et al. , 2010; Lourentzou et al. , 2017; Fornaciari and Hovy, 2019b). In general, however, the task is considered a classification problem, which requires solutions for the identification of geographic regions as labels.",
        "GPT2_formal_text": "_base and attention_mask_len are set to d_s. The batch size is u, and the learning rate is β. The learning rate decay is also β. The decoder is trained using stochastic gradient descent (SGD). Formal: In this paper, we're using the generative model from (Sutskever et al., 2014), where each token is generated based on the previous token. If you want to know more about how the generator works and its parameters, check out the original paper. For our experiments, we're using the Transformer model (Vaswani et al., 2017). Formal: In this paper, we're focusing on the generative part of the Transformer model. The next sections will walk you through the whole generator setup. Formal: We also created a task-specific generator using PyTorch (Paszke et al., 2019). It works with the pretrained BERT base model (Devlin et al., 2019). Formal: In our experiments, we used the pretrained BERT model to generate the tokens in the input sentences, along with the generated tokens. Formal: We built a quick-and-easy way to generate tokens in TensorFlow (Abadi et al., 2020). Formal: We checked how well TensorFlow can generate tokens in two languages, Arabic and Chinese, and checked the perplexity of the generated tokens. Formal: The rest of the paper is laid out like this: Section 2 talks about the context-based and generative models we used. Section 3 explains our generation method. Section 4 looks at how well the generators worked. Finally, we wrap things up with a discussion. Formal: We've demonstrated that the generative model we created in this paper can generate tokens in different languages. We also showed that it can handle the tricky issues that come with generating tokens in different languages. Formal: To test how well it can generate tokens, we used the English and Chinese TensorFlow models for Arabic and Chinese. Formal: We also created a task-specific generator using PyTorch (Paszke et al., 2019). Formal: We tested how well TensorFlow can generate tokens in two languages, Arabic and Chinese, and checked the perplexity of the generated tokens. Formal: We included the results of our experiments in Table 1. Formal: We also created a task-specific generator using PyTorch (Paszke et al., 2019",
        "directory": "D19",
        "filename": "D19-5530.json"
    },
    {
        "casual_text": "The first one is the basic dot product self-attention model. G ∈ R I×I represents a favor alignment. Check out Figure 1 for a visual of the approach we're proposing. In this case, we're using a window size of 2 (D = 2).",
        "formal_text": "The first term is the original dot product selfattention model. G ∈ R I×I is a favor alignment Figure 1: Illustration of the proposed approach. In this example, window size of 2 is used (D = 2).",
        "GPT2_formal_text": "\": [], \"text_to_chat\": [], \"text_to_text\": [], \"text_sent\": [], \"text_reply\": [], \"text_content\": [], \"text_count\": [], \"greeting\": [], \"greeting_text\": [], \"greeting_sender\": [], \"greeting_id\": [], \"greeting_nick\": [], \"greeting_user\": [], \"intent\": [], \"intent_text\": [], \"intent_ext\": [], \"intent_user\": [], \"intent_user_text\": [], \"intent_user_sender\": [], \"intent_user_user\": [], \"intent_user_id\": [], \"intent_user_nick\": [], \"intent_user_user\": [], \"source\": [], \"source_text\": [], \"source_content\": [], \"source_count\": [], \"sent_text\": [], \"sent_content\": [], \"sent_count\": [], \"user\": [], \"user_text\": [], \"user_sender\": [], \"user_user\": [], \"user_user_text\": [], \"user_user_sender\": [], \"user_user_user\": [], \"user_user_user_id\": [], \"user_user_nick\": [], \"user_user_user\": [], \"user_user_user_id\": [], \"user_user_nick_text\": [], \"user_user_user_sender\": [], \"user_user_user_user\": [], \"user_user_user_user_id\": [], \"user_user_user_nick_text\": [], \"user_user_user_sender\": [], \"user_user_user_user_id\": [], \"user_user_user_nick_text\": [], \"user_user_user_sender\": [], \"user_user_user_user_id\": [], \"user_user_user_nick_text\": [], \"user_user_user_sender\": [], \"user_user_user_user_id\": [], \"user_user_user_nick_text\": [], \"user_user_user_sender\": [], \"",
        "directory": "D18",
        "filename": "D18-1475.json"
    },
    {
        "casual_text": "Okay, so let’s say ( ) is a word, and _ is either the word itself or a phrase that includes the word. Here, ( ) stands for a specific term, and _ represents all the related terms. Now, here’s how the new GPU sampling works:",
        "formal_text": "Let ( ) be a word and _ be the word itself or a phrase containing the word ( ). represents a term, and _ indicates all the related terms of. The new GPU sampling is as follows:",
        "GPT2_formal_text": "_score(i, j) = y_i - z_j, where y_i is the actual text representation of the j-th chunk of the source sentence, and z_j is the generated text representation. Here, i and j are the positions in the source and target sentences where the source and target sentences are aligned. The position of the target sentence in the source sentence is measured by the length of the source sentence. This setup is called a position-wise pooling operation. Formal: The length-aware pooling operation basically spreads the source and target sentence alignment probabilities across the source and target sentence embedding spaces. Formal: The alignment relations can be calculated using a classifier. Formal: The network is made up of three main parts: an entity embedding layer, a hidden layer, and a gate that controls how much attention is given to different entities. Formal: To handle the different types of alignment relations we’re dealing with, we use a regularization parameter, let’s call it δ_a. Formal: Let’s say α_a = 1 and β_a = 0. Formal: Here, α_a is a regularization parameter that helps the network figure out the most likely alignment for a given entity. Formal: The network is trained using an attention mechanism. Formal: Following what Zhang and Yang (2015) did, we create a special attention matrix called α_a for each alignment relation. Formal: For each entity-type pair and the alignment relation we’re looking at, we calculate the entity embedding, f_a_i, by mixing the embedding of the entity itself with the embedding of the alignment relation. Formal: Finally, we combine the entity embedding and the alignment relation embedding using the softmax function. Formal: Formal: The pooling operation we’re using can take a sequence of entity-type pairs and turn them into a sequence of entity-type embeddings, f_a. Formal: Formal: Finally, we combine the entity embedding and the alignment relation embedding using the softmax function. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C14",
        "filename": "C14-1063.json"
    },
    {
        "casual_text": "For each part of a word’s meaning, we group together all the characters that include that part. As you can see in Table 1, not all characters in these groups have meanings that are closely related enough to be useful examples in teaching materials. So, for any group, the goal is to pick out a smaller set of characters that have meanings that are really similar. When creating an algorithm for this, we focus on two main areas of research:",
        "formal_text": "For each semantic component, we define its \"character family\" to consist of all characters that contain the component. As shown in Table 1, not all family members have sufficiently related meaning to serve as good examples in pedagogical materials. Given a family, the character set construction task is to identify a subset of its characters that are semantically close. In designing an algorithm for this task, we address two research topics:",
        "GPT2_formal_text": "= arg max u, arg min v, arg max w, arg min b) --c. Formal: The output function is the same as the input function but with the logarithmic rank of each feature vector. Formal: The latent variable is calculated using the weighted average of the BNN parameters, with the logarithmic rank of each latent vector added as a covariance term. Formal: You can find more details in Section 3. Formal: In Figure 2, we’ve included the batch size we used for the first experiment. Formal: When running the BNN, we tweak the training parameters using cross-validation. Formal: For the first batch, we set α to 0.1 and λ to 10. Formal: For the second batch, we set α to 0.2 and λ to 100. Formal: In the third batch, we set α to 0.5 and λ to 10. Formal: Lastly, we keep going with the same training process for the next batch. Formal: We ran the second and third batches using the same parameters as the first one. Formal: If you want more details, you can check out the original papers by Ling and Weld (2015), Huang et al. (2018), and Zhang and Dredze (2018). Formal: Lastly, we apply the finetuning algorithm from Tsuchiya et al. (2016). Formal: We’ve also included the results from the training phase in Section 3. Formal: Formal: Finally, we’ve included the results from the validation phase in Section 4. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "bea",
        "filename": "2021.bea-1.6.json"
    },
    {
        "casual_text": "* Portability: SABLE started with French and English, then they added Spanish and English, and later Korean and English too. They've got the whole process down and even wrote it all up (Melamed, 1996c).",
        "formal_text": "* Portability: SABLE was initially implemented for French/English, then ported to Spanish/English and to Korean/English. The porting process has been standardized and documented (Melamed, 1996c).",
        "GPT2_formal_text": "_tgt: Just a regular word, nothing fancy. After that, we feed it through a ReLU activation function to get a final representation of the token's features, which we call f_m. Formal: We use a simple linear function on this token representation to get the semantic representation f_s. Formal: After that, we calculate the attention weights, W_m, by taking the dot product of the attention vector f_m with the target token's feature vector, f_t. Formal: After that, we update the model's parameters using the loss function from equation (1). Formal: Lastly, we use the combined representation of the token and the target token to predict the label y_t. Formal: Here, E_u is the final prediction for the token. Formal: We also came up with a version of the model that uses regular pooling operations. Formal: The parameters for the regular pooling are: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "A97",
        "filename": "A97-1050.json"
    },
    {
        "casual_text": "8. If you type \"J'espère les avocats\" into the test interface, it translates to \"I hope The lawyers.\" Why is \"The\" capitalized?",
        "formal_text": "8. If you write the phrase J'espère les avocats in the test interface, you will get the translation I hope The lawyers. Why is The capitalized?",
        "GPT2_formal_text": "' list is made up of a list of instances (like \"s1, s2, ..., sT\") and is tied to the bag of words, which is just the input sentence. Formal: In this part, we'll talk about how we figure out the labels for the instance tuples. Formal: We use a basic nltk pipeline to do this. Formal: For the instance tuples, we look for the longest common subsequence (LCS) of the source text. Formal: We use the nltk library (https://www.nltk.org/) to process the input sentence. Formal: The input sentence goes through two nltk parsers: one for the source language and one for the target language. The source parser is trained using the model from the last section, and the target parser is trained using the model from the first section. Formal: For each English sentence and its target translation, we calculate the score for each English instance and the score for its translated version. Formal: To make sure the scores match up correctly, we also use a loss function to make sure the results are consistent. Formal: For the English word-based model, we calculate the final score for the English instance by running it through the lexical reordering model (Holtzman and Manning, 2004). Formal: Finally, we normalize the scores to get the final normalized scores. Formal: Formal: The rest of this paper goes like this: In Section 2, we introduce the new model we're proposing. Section 3 explains our method for calculating the model score. Then, in Section 4, we show the results for both the target language and the source language. Finally, in Section 5, we talk about the results for the English sentence-based model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2020.eamt-1.40.json"
    },
    {
        "casual_text": "We're breaking down the whole challenge into three main parts: 1) finding the right documents, 2) figuring out the important bits in those documents that relate to what we're working on and should be summarized, and 3) creating a final summary that focuses on the specific topic we're dealing with. It's obvious that current NLP tech isn't advanced enough to solve this whole thing in one go, but we think working on each part separately will help us get closer to a complete solution eventually.",
        "formal_text": "We dissect the full challenge as bringing together work of disparate interests; 1) in finding relevant documents; 2) in identifying the salient aspects of these documents in relation to the current work worth summarizing; and 3) in generating the final topic-biased summary. While it is clear that current NLP technology does not let us build a complete solution for this task, we believe that tackling the individual components will help bring us towards an eventual solution.",
        "GPT2_formal_text": "ed_predict(x, y) = (x + a_t) / 2, where a_t is the attention mask for the output word x. Formal: In this paper, we're looking at how to predict the next word in a sentence by combining both word embeddings. We do this by using a selective attention approach, which was introduced by Bahdanau et al. (2014). For this, we're using a mix of two types of attention: (1) A linear combination of word embeddings, which we call the selective attention, and (2) An attention mechanism that uses a weighted sum of all the attention masks for the entire sentence. Formal: In this part, we'll explain how to use the selective attention and the attention mechanism in a straightforward way. We'll also describe a method to combine the selective attention and the attention mechanism for improving sentence prediction. Formal: The selective attention and the attention mechanism we use are based on the GRU network, which is an attention-based neural network with a non-linearity. Formal: The selective attention and the attention mechanism we use are based on the GRU network, which is an attention-based neural network with a non-linearity. Formal: We'll start by explaining the selective attention and the attention mechanism in more detail. After that, we'll introduce our new method for reducing sentence prediction error and show that it works well. Formal: To see how our new method for reducing sentence prediction error affects things, we'll compare it to a model that uses supervised learning. Formal: Following the approach of Liu et al. (2015), we'll use a weighted sum of all the attention masks for the entire sentence, which we call the selective attention. Formal: We'll also introduce a method for combining the selective attention and the attention mechanism. Formal: We'll show that our method for reducing sentence prediction error and improving sentence prediction can be applied to other NLP tasks, like text classification or sentiment analysis. Formal: This paper introduces a new way to make phrase embeddings more focused by combining word embeddings. This approach uses a selective attention mechanism and a weighted sum of all the attention masks for the entire sentence, which we call the selective attention. Formal: We'll show that our method for reducing sentence prediction error and improving sentence prediction can be applied to other NLP tasks, like text classification or sentiment analysis. Formal: We'll introduce a method for combining the selective",
        "directory": "C10",
        "filename": "C10-2049.json"
    },
    {
        "casual_text": "To make our questions more believable, we create three wrong answers for each simile. These wrong answers follow two rules (based on Haladyna et al., 2002, and Ren and Zhu, 2020): they should be true-negative and challenging. True-negative means the wrong answers don’t make sense when used in the question, but they’re still related to the correct answer in a tricky way. Our process for making these wrong answers has three steps: 1) coming up with options, 2) picking the best ones, and 3) double-checking with people to make sure they’re good.",
        "formal_text": "To make our probes convincing, three distractors are designed against the original property in each simile with two criteria (Haladyna et al. , 2002; Ren and Zhu, 2020): true-negative and challenging. We argue that well-designed distractors should be illogical when filled into the questions (true-negative) while being semantically related to the correct answer (challenging). Our distractor design mainly involves three phases: 1) distractor generation; 2) distractor selection; 3) Human Confirmation.",
        "GPT2_formal_text": "\": [ ] }, { \"type\": \"labels\", \"text\": \"NP NP NP NP NP NP 不 这 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不 这 不",
        "directory": "acl",
        "filename": "2022.acl-long.543.json"
    },
    {
        "casual_text": "Check out Table 4 for the results. Both knowledge distillation and POSPD help lower the repetition rate in NAG models across four datasets. They work especially well on XSUM, which has longer sentences. When it comes to the number of tokens, knowledge distillation really cuts down on the tokens generated by NAG models, especially on XSUM. On the other hand, POSPD keeps the length of the generated sentences pretty close to the reference without making the repetition rate worse. Table 4 breaks it all down: it shows how the NAG models perform. \"Reference\" is the target sentence we're aiming for. \"Repetition\" and \"Tokens\" show the difference in repetition rate and the number of tokens between the reference and what the model actually outputs.",
        "formal_text": "The results are shown in Table 4, from which we can see both knowledge distillation and POSPD can reduce the repetition rate in NAG models on four datasets, and they are more effective on XSUM datasets with longer sentences. While in token numbers, using knowledge distillation significantly reduces the number of tokens generated by NAG models on XSUM. In contrast, using POSPD remarkably make the length of generated sentences by NAG models close to the reference without increasing the repetition rate. Table 4: Statistical analysis of NAG models' generations. \"Reference\" denotes the target sentence's reference. \"Repetition\" and \"Tokens\" represent the repetition rate and tokens number gap between reference and model outputs, respectively.",
        "GPT2_formal_text": "ed, and word_masked. The rest are treated as a bunch of word embeddings (word1, word2, ..., wordK) and a hidden layer (h_vec). Formal: Let’s break down what these hidden layers mean for our model. After that, we’ll dive into the nitty-gritty of the optimization for the LSTM-based model. Formal: Formal: For the hidden layers, we’re using the transpose of h_vec. To make sure the model can handle different forms of words, we’ve added an LSTM decoder to handle the input word sequence, as shown in Figure 1. Formal: The baseline model, called LSTM-BiLSTM, has an LSTM hidden layer and an LSTM decoder, with a transformer decoder output layer. Formal: The current top model, LSTM-BiGRU, has a hidden layer with a dimension of d_h. Formal: After those layers, there’s an LSTM decoder layer with a dimension of d_d. Formal: Now, let’s talk about the optimization for the LSTM-based model. Formal: The baseline model has a linear, non-linear, log-linear, and dynamic-time part-of-speech tagger. The hidden layer for this tagger is h_vec. The dynamic-time part-of-speech tagger calculates the probability of the next word (y_t) based on the previous word (y_1), the current word (y_t+1), and the word (y_t−1). Formal: In the future, we’ll try out models using LSTM-Regularized and LSTM-BiGRU with a transformer decoder. Formal: The LSTM model we’re using has the same setup as the baseline model but has some extra stuff like the hidden layer dimension and the decoder's output dimension. Formal: We’ve included the full details of the model setup in Appendix A. Formal: For the training process, we’ve set up two types of epochs:\n\n- **Initialization**: We start with a batch size of 50.\n- **Training**: We train for 100 epochs. Formal: After each epoch, we check how well the",
        "directory": "acl",
        "filename": "2021.acl-long.467.json"
    },
    {
        "casual_text": "where m is the length of the question and n is the length of the document.",
        "formal_text": ", where m and n represent the sequence length of a question and a document, respectively.",
        "GPT2_formal_text": "_t and the full answer_id_t):\n         Casual: We use a multi-scale scaling factor, α, to adjust how much weight we give to different parts of the question. We pick α based on what works best for our specific question and setup. Formal: The training goal is to make the loss as small as possible. We use a modified version of the regular loss function from equation (4) to train this multi-scale scaling factor, α. Formal: The loss function L_mult(x) is defined like this: Formal: L_mult(x) = log p(x_i | x) + log p(x_j | x_i) + log p(x_k | x_i) + log p(x_l | x_j) Formal: The main goal for training is to minimize the cross-entropy loss. Formal: So, we take the weighted sum from (x_i, x) for each point x_j, and then we average these values across all the points. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1237.json"
    },
    {
        "casual_text": "AG News. This dataset is for classifying news articles into four different categories. It's split into a balanced training set with 120,000 samples and a test set with 7,600 samples.",
        "formal_text": "AG News. The dataset for the news articles classification partitioned into four categories. The balanced train and test set consist of 120, 000 and 7, 600 samples, respectively.",
        "GPT2_formal_text": "ing_m is basically the same as the attention-masking_m in the original multi-view setup. We're using a linear layer for calculating the attention coefficients. Formal: If we have N different embedding spaces, and N is the total number of labels, then the score for a possible label y in a group of N is calculated like this: Formal: Here, P_i is a positive number that shows how much we care about the label y, while λ is just a special number we use to decide how much we care about different labels. Formal: So, the attention scores for the possible label y and the other labels in the group can be written like this: Formal: y = (q_i, x_i, w_i, λ) Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.94.json"
    },
    {
        "casual_text": "For Indo-European and Mayan, the CNNs actually work better than the baseline system. But for Austronesian, the CNNs don't do better than the baseline. The results for Indo-European and Mayan (check out table 5) are pretty much the same as the ones in table 4. Basically, the CharCNN system is the best for the Mayan language family, and the PhoneticCNN system is the best for the Indo-European family. \n\nInterestingly, for the Austronesian family, the baseline system actually does better in terms of F-score compared to the top-performing system from table 4, which is the CharCNN with language features. The baseline system also has a higher Accuracy score, but the difference isn't big enough to be considered statistically significant. \n\nOne possible reason for this is that there just isn't enough info in those 20 meanings to figure out phonological similarities across 100 languages.",
        "formal_text": "In the case of Indo-European and Mayan, the CNNs perform better than the baseline whereas for Austronesian the CNNs do not outperform the baseline system. The results for Indo-European and Mayan (cf. table 5) are similar to that of the results reported in table 4. That is, the CharCNN system performs the best for Mayan language family, while the PhoneticCNN system performs the best for the Indo-European language family. Surprisingly, for the Austronesian family, the baseline system performs better at F-score than the top-performing system for this language family in table 4, namely the CharCNN (with language features); the Accuracy measure of the Baseline system is also higher, but the difference is not statistically significant. The reason for this could be that there is not enough information in the 20 meanings to learn phonological similarity for 100 languages.",
        "GPT2_formal_text": "` (where `{id}` can be either 0 or 1), we can create a list of 300 word sequences from a question using a greedy search with a beam size of 3. This list is then used to generate answers for the next question. Formal: The hierarchical attention network we came up with works really well for question answering. It also supports other deep learning models, like CNNs or GRUs. This means we can easily adapt it to different types of question-answering tasks without any extra effort. Formal: Next, we’ll check how our method can work for other neural network models. Formal: Lastly, we’ll share the results of a survey that shows how people feel about our approach for question answering. Formal: Formal: We also think about making our answer generation method work for other natural language processing tasks. Formal: Formal: Also, we think about using our knowledge about what people think can help us predict their answer choices. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1097.json"
    },
    {
        "casual_text": "Let’s say you have a text with T sentences and a vocabulary of L unique words (like words that aren’t in a stop list, otherwise most sentences would look pretty similar). You can represent this text using a T x L matrix called F. Here’s how it works: for each sentence (t = 1, 2, ..., T) and each word (1 = 1, 2, ..., L), you put a 1 in the matrix if the 1-th word is in the t-th sentence, and a 0 if it’s not.",
        "formal_text": "Suppose that a text contains T sentences and its vocabulary contains L distinct words (e. g words that are not included in the stop list, other wise most sentences would be similar to most others). This text can be represented by aTxL matrix F defined as follows: for t = 1, 2. . . . , T and 1 = 1, 2, . . . , L we set { 1 iff 1-th word is in t-th sentence F1 = 0 else.",
        "GPT2_formal_text": "ed = (v_t, h_t, w_t) v_t = (v_t, h_t) = TransE (v_t, h_t) - P (v_t, h_t) + U (v_t, h_t) v_t = (v_t, h_t) + P (v_t, h_t) + U (v_t, h_t)\n                   Casual: The correlation between the predictor variables and the generated responses is calculated as (r_s * r_t). Formal: The correlation between the predictor variables and the generated responses is calculated as (r_s * r_t). Formal: The correlation between the predictor variables and the generated responses is calculated as (r_s * r_t). Formal: For the generated responses, we'll define the parameters by looking at a specific example (like a tweet) that has a specific meaning. Formal: For the generated responses, we'll define the parameters by looking at a specific example (like a tweet) that has a specific meaning. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E03",
        "filename": "E03-1058.json"
    },
    {
        "casual_text": "It’s clear that both aligners did a pretty good job, but CATS-Align worked better with hand-aligned data. When we looked into the mistakes Bleualign made, we noticed that most of the errors happened because it grabbed a few extra sentences from the next paragraph. This only seemed to happen with adapted paragraphs. One reason for this could be that the original and adapted texts don’t match up perfectly in size. But when it comes to aligning longer books or novels, this might actually be helpful. Sometimes, longer paragraphs in the original text are split into smaller ones in the adaptation, so grabbing a few extra adapted paragraphs could help match the longer original ones better.",
        "formal_text": "As can be seen, although both aligners showed good results, CATS-Align performed better on hand-aligned data. When investigating the errors of Bleualign, we found out that most of the erroneous alignments happen because the aligner takes some additional sentences from the next paragraphs. This seems to only happen with adapted paragraphs. One possible reason is the size mismatch between original and adapted texts. However, when aligning longer books and novels, this might be appropriate, since sometimes longer paragraphs from the original texts are split into multiple paragraphs during adaptation, so it can be good to grab additional adapted paragraphs during alignment to match a longer original one.",
        "GPT2_formal_text": "\". Formal: The text can be a bit messy, with characters and punctuation mixed together, and some words might be spelled incorrectly. This makes it tricky to check for typos. To fix this, we use a list of common typos to spot possible mistakes. Formal: We’re using a German-English dictionary that’s been tweaked to fix errors, which has around 10 million entries (Buchholz and Marsi, 2006). The entries come from a big multilingual dictionary that’s been tweaked for German, English, and Italian, all done by experts. Formal: We also made a parallel text, which we’ve named the parallel corpus. This corpus is around 3 million German words in total, with around 30 million words in the original German and 3 million in the translated version. We’re using this corpus to train our machine translation system, which was built using the data from the parallel corpus. Formal: Here’s the data we used for training: Formal: The input for our translation system, like an English sentence, a German sentence, and the translated German sentence, is shown in Figure 1. We’ve also included some examples from the original German to English training set. Formal: For testing, we’ve got three reference translations. Formal: We’ve split the training set into 20,000 separate parts using the same method as in the NMT-Base system. We train each part individually using the training set, using a technique called \"split and fill\" (Och, 2003). The results are in Figures 1 and 2. Formal: The translation results are in Figure 3. Formal: For comparison, we’ve got a synthetic example in Figure 4. Formal: Our translation system can handle different levels of word order. Formal: In Figure 3, the Chinese sentence from the training set is given a different word order compared to the original. Formal: We’ve also included a reference translation to show that the system can handle both word order and word order variations. Formal: The main difference between the original and translated sentences in Figure 3 is that the translated sentence comes after the original sentence. Formal: Figure 4 shows the synthetic example in Figure 3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "bsnlp",
        "filename": "2021.bsnlp-1.8.json"
    },
    {
        "casual_text": "[Answer] five [Knowledge] You ride the subway to and from work five days a week.",
        "formal_text": "[Answer] five [Knowledge] You take the subway back and forth to work five days a week.",
        "GPT2_formal_text": "_set_dist\" module. This module can only be used on queries where the given sequence is part of the query. But if the input isn't in the query, it can't be used to calculate attention weights, so it's basically useless. The same goes for the \"KD_query_set\" module. Formal: The query is a sequence of words or phrases that have already been broken down into smaller pieces. For example, the query \"how much\" has a word phrase \"how much\" as its main word. Formal: When the model gets a question, it first breaks it down into a bunch of smaller phrases. Then, it uses a specific method to figure out the weights for these smaller phrases. Formal: To get the attention weights for the query, the model uses something called a \"KD query set.\" This set has pairs of input phrases, like \"how much\" and \"much,\" and each pair has its own weight. Formal: The KD query set is used to estimate the attention weights for each phrase pair in the query. Formal: To find the KD query set, you start by using a specific algorithm on the input phrases. Formal: Since the query set is made up of pairs of input phrases, we use a special function to turn each pair into a \"KD query set.\" Formal: For the sake of simplicity, we’re not going to explain how to compute the query set. Formal: The KD query set for \"how much\" is shown in Fig. 1. Formal: The attention weights for each query phrase are calculated using something called a \"KD query set aggregation.\" Formal: The input phrase pair \"how much\" in the query is broken down into smaller phrases using a specific algorithm. Formal: Formal: After that, we use a specific function to combine the attention weights for each phrase pair in the query set. Formal: Formal: Finally, we calculate the KD query set by combining the input phrases and the query set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.225.json"
    },
    {
        "casual_text": "Another key rule to stop things from going on forever is the attenuation factor. This is like a discount that lowers the score for each level as you move away from the starting point where the original pair entered the matching process. The farther you get from that starting level (called level 0), the lower the highest possible score becomes. Eventually, the top score drops below a certain limit, and that's when SWESIL says, \"Okay, enough matching for now.\"",
        "formal_text": "A third boundary condition, which has an important function in preventing endless loops, is the attenuation factor. This is a factor by which the score for a certain level gets reduced for each step this level is away from the 'entry level' at which the original IST pair entered the matching cycle. The further removed from the entry level (level 0), the lower the maximum possible score will be; and at some point the maximally obtainable score will fall below a certain threshold level, at which point SWESIL stops matching.",
        "GPT2_formal_text": ".\n       Casual: The relevance of a phrase is basically how much more important the phrase is compared to other phrases or things mentioned earlier in the document. Formal: So, for each mention, we figure out how much more important it is compared to the others, and that's how we calculate the relevance. Formal: We figure out how much the mention is connected to the others in a document. This connection is based on how often they show up together in the whole collection of documents, and we use that to figure out how much the mention is more connected to those others compared to the rest of the document. Formal: We use a formula called f(•) to do this. Formal: In a document, the connection between a mention and the others is based on how often they appear together in the whole collection of documents. Formal: We combine the two parts—the relevance and the connection—and use them to calculate the relevance. Formal: For every mention, we estimate how much more important the mention is compared to the others. Formal: We use a formula called f(•) to do this. Formal: We combine the two parts—the relevance and the connection—and use them to calculate the relevance. Formal: We also use a vector called z(•) that helps us rank the mentions in a document, starting with the most important ones. Formal: Formal: Lastly, we use a big list of words (like the one from Iyer et al., 2016) to rank the mentions. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1102.json"
    },
    {
        "casual_text": "Check out https://www.xbrl.org/the-standard/what/an-introduction-to-xbrl/ for a quick intro to xbrl.",
        "formal_text": "See https: //www. xbrl. org/the-standard/what/ an-introduction-to-xbrl/ for an introduction to xbrl.",
        "GPT2_formal_text": "ed_tagger (like what Joshi et al. did in 2020). It lets you use the model's contextualized embeddings as inputs for the token embeddings. Formal: We use unigram and bigram features to figure out the L1 score for each token in a sentence, which helps us create a token-level summary for the whole document. Formal: The context embeddings are just the embeddings of the tokens themselves, not their part of speech tags. Formal: The hyperparameters for the token embedding and the word embedding are the same as the ones you see in the experiments in this paper. Formal: The graph attention is based on the graph convolutional neural network (GCN), which was introduced by Vaswani et al. in 2017. Formal: The HATM is a type of multi-attention mechanism that was introduced by Yang et al. in 2018. Formal: For token embedding, we take the average of the word embeddings and the embedding of the token, then add that to the embedding of the token to get its context embedding. Formal: Formal: Lastly, we use a GCN to create the context embeddings for each token. Formal: Since the token embeddings are calculated using the gradient descent with a fixed learning rate, the model gets better as the gradient gets smaller. Formal: So, the token embeddings are trained by training the HATM, which is a graph attention mechanism. Formal: Formal: Formal: We tested our model on three different NER datasets, which you can check out in Table 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.303.json"
    },
    {
        "casual_text": "We offer support for three different acquisition models, depending on the language and the availability of human resources for the task. In all cases, there needs to be a line supervisor on-site who oversees the language acquisition process. This supervisor doesn’t necessarily need to know the language being worked on, but they should be able to communicate with the people doing the acquisition, no matter where they are or who they are. It’s also expected that this supervisor will spend some time getting to know the language, and if possible, learning to speak it. Their main job is to make sure the acquisition happens on time and meets the required quality standards. Additionally, the supervisor should have some background in computational linguistics to help with building, using, and evaluating the morphological analyzer.",
        "formal_text": "We provide support for three different models of acquisition, depending on the language involved and thus the availability of human resources for acquisition. All models involve having a line su pervisor in place in house for the language. This person need not know the language involved, but must be able to converse with the acquirers, wher ever and whoever they may be. It is also expected that this person spend some time familiarizing themselves with the language, learning to speak it if at all possible. This person has overall responsi bility for seeing that the acquisition takes place in a timely manner and is of acceptable quality. The supervisor also should have some computational linguistics background in order to assist in the con struction, use, and evaluation of the morphological analyzer.",
        "GPT2_formal_text": "_t\" = {0, 1, 2, 3}):\n          Casual: First, let's figure out the probability P(t | s) for all the entity pairs in the query s. Formal: To get the probability P(s | t), we use a greedy search through the graph G(s, t). This method is kind of like the one described in (Ebner et al., 2018). The hidden state h_t at time t is calculated by adding up all the entity pairs from the query s up to time t and the target entity e_t. Formal: Here, g_t and g_s represent the graph parameters and topic distributions for the topic distribution g_t and the topic distribution g_s, respectively. Formal: At the end of the query s, we calculate the expected number of events for each entity pair e_t in the query. Formal: For the query s, we use the average of the count(s) for all the entities in the query. Formal: Formal: For the query s, the context distribution for the entity pair e_t, which includes the entity e_t and its related entities, is calculated using the estimated counts for the entity e_t and its related entities. Formal: Formal: We use a Gibbs sampler (like Gibbs sampling in (Blei et al., 2003) and Gibbs sampler in (Settles, 1999)) on the Gibbs sampler output of the Gibbs sampler (as mentioned by Garnelo and Uszkoreit, 2014) to optimize the query probability P(s | t). Formal: Formal: After running this optimization, we get the expected counts from the query distribution P(s | t) for each entity pair e_t in the query s. Formal: Formal: Formal: Finally, the target entity e_t is calculated by the loss function L_t * L(s). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "amta",
        "filename": "2006.amta-papers.1.json"
    },
    {
        "casual_text": "Following up on Zipf's (1949) ideas, Piantadosi and his colleagues in 2012 argued that, when you look at communication from an information theory perspective, ambiguity is actually a key factor in making a communication system work efficiently. They focused on the idea of using words in a way that saves effort. According to them, having words with multiple meanings can be useful, especially when the context helps clarify what's being said—it lets us reuse simpler words. To back up their claim, they showed a connection between how many meanings a word has in WordNet (Miller, 1995) and how easy it is for speakers to use those words. They looked at things like how easy the word is to pronounce, how long it is, and how common it is in everyday language (based on data from a big collection of text).",
        "formal_text": "Building on Zipf's (1949) theories, Piantadosi et al. 2012posit that, when viewed informationtheoretically, ambiguity is in fact a requirement for a communication system to be efficient. Focusing on economy of expression, Piantadosi et al. suggest that lexical ambiguity serves a purpose when the context allows for disambiguation-it allows the re-use of simpler word forms. 4 They support their hypothesis by demonstrating a correlation between the number of senses for a word listed in WordNet (Miller, 1995) and a number of measures of speaker effort-phonotactic well-formedness, word length and the word's log unigram probability (based on a maximum-likelihood estimate from a large corpus).",
        "GPT2_formal_text": "ed_layers (with attention masking) that work with the masked text. After that, we calculate the average attention value for each token. Formal: The average attention score for each token in the sequence is calculated like this: Formal: The attention values for the tokens in the input sequence are calculated like this: Formal: Similarly, the attention values for the tokens in the masked sequence are calculated like this: Formal: Formal: The attention weights for the tokens in the masked sequence are calculated like this: Formal: Formal: The attention weights for the tokens in the input sequence are also calculated like this: Formal: Formal: Formal: Table 3 shows the structure of the attention weights for the token sequence shown in Figure 1. Formal: The token sequence representation is made by combining the outputs of the masked and input sequences. Formal: The hidden representation for the token sequence is also combined based on the input sequences, and then combined with the output representations of the input sequence. Formal: After the output representation of the input sequence is created, we calculate the attention weights for the tokens in the output sequence. Formal: The attention weights for the token sequence representations are calculated using the input sequence representations. Formal: Each token representation, including the hidden representations, is then combined with the output representations of the input sequence, and the attention weights for that token sequence are calculated. Formal: Formal: Lastly, the output representations are then combined with the output representations of the input sequence. Formal: Formal: Formal: Formal: The output representations for the token sequence are also combined with the output representations of the input sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.328.json"
    },
    {
        "casual_text": "We compare models that were trained on different amounts of similar and different WikiMatrix samples. We start with the examples marked as different in section 3.2. Then, we pick the most detailed differences by using the bicleaner score (Ramírez-Sánchez et al., 2020) set at 0.5, 0.7, and 0.8. Check out section A for more info.",
        "formal_text": "Divergences We conduct an extensive comparison of models exposed to different amounts of equivalent and divergent WikiMatrix samples. Starting from the pool of examples identified as divergent at §3. 2, we rank and select the most fine-grained divergences by thresholding the bicleaner score (Ramírez-Sánchez et al. , 2020) at 0. 5, 0. 7 and 0. 8. For details, see A.",
        "GPT2_formal_text": "\" and \"filter_ids\" fields. These are basically the unique IDs for the characters, and they're figured out using a formula that looks like this: Formal: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 Formal: The first three formulas use the \"i + n + 1\" notation, and the last two use \"i - n + 1\" notation. For example, in the first formula, the first spot has a value of \"2\", while the second and third spots have values of \"1\" and \"0\". Formal: Formal: The first formula uses a \"2\" placeholder, and the second and third formulas use a \"1\" placeholder. Formal: The third formula uses a \"0\" placeholder, and the second and third formulas use a \"1\" placeholder. Formal: Formal: Each formula starts with a \"0\" placeholder. Formal: Formal: Formal: The formula has a \"1\" placeholder. Formal: Formal: The formula has a \"0\" placeholder. Formal: Formal: The formula has a \"0\" placeholder. Formal: Formal: The formula has a \"0\" placeholder. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.562.json"
    },
    {
        "casual_text": "It might look like the super simple TURN architecture isn't cut out for anything more than basic tasks. But actually, that's not true at all. Our first experiment used a TURN for a natural language agreement task from Linzen et al. (2016). This task involves predicting whether third person verbs in English text should be singular or plural, with some supervised training. For example, in the sentence \"The keys to the cabinet are on the table\", the RNN is trained to pick the plural \"are\" instead of the singular \"is\".",
        "formal_text": "It may seem that the extreme simplicity of the TURN architecture renders it unsuitable for any non-trivial processing task. In fact, this is not at all the case. Our first experiment applies a TURN to a natural language agreement task proposed by Linzen et al. (2016). This task is to predict the number of third person verbs in English text, with supervised training. In the phrase \"The keys to the cabinet are on the table\", the RNN is trained to predict the plural \"are\" rather than the singular \"is\".",
        "GPT2_formal_text": "ed = ReLU(i) ∝ ReLU(D). Formal: So, c is just a number that tells us how many dimensions are in the embedding vector, and A is just the number of A layers, like in equation 2. Formal: The number of attention heads, which we call m, and the number of convolutional layers, m, are both the same as in equation 2. Formal: If you have a series of attention heads and a sequence of attention vectors, like in equation 1, the attention weight, a(t), is calculated using a function called an attention weight decay. Formal: We use this attention weight decay function to tweak the number of dimensions of the embedding vector. Formal: We also calculate the attention embedding for the sequence of attention vectors by averaging the attention embeddings of all the attention heads. Formal: We use this weighted average attention embedding to create a weighted average attention vector for the sequence of attention vectors. Formal: Formal: Once we have the attention embedding for a sequence of attention vectors, we can calculate the attention weights for the sequence of attention vectors by averaging the attention weights of all the attention heads, like in equation 3. Formal: Formal: Formal: Lastly, the attention weights for a sequence of attention vectors are calculated using this weighted average attention embedding. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "cmcl",
        "filename": "2022.cmcl-1.2.json"
    },
    {
        "casual_text": "In the Generation Track, each model creates its own set of KPs, so there isn't a single standard set to follow. Along with the official evaluation metrics, two extra scores are included: the average of the strict and relaxed mAP values, and p@50% for the strict view. Submissions that included descriptions are marked with (*). \n\nFigure 2 shows:\n(a) How KPs are spread across arguments for the pro stance on the topic \"Social media platforms should be regulated by the government.\" This is for the expert ground truth (strict view) and selected models, normalized by the number of arguments that match a KP. The JSd compared to the expert distribution is in brackets.\n(b) JSd rank vs. AP rank for all models in the Matching Track, per topic and stance.\n\nThe topic id and topic text mapping isn't relevant for comparing KP distributions, so we can't directly compare them like we did above. But, we can still do a qualitative, anecdotal analysis. Table 8 in the appendix lists the KPs generated for one topic and stance by the expert and different models. The expert made their KPs beforehand without seeing the actual arguments, while the models generated KPs based on the input arguments. Looking at the generated KPs, it seems like the expert might have missed some.",
        "formal_text": "In the Generation Track, there is no single set of KPs, since each model generates its own set. It is In addition to the official evaluation metrics, two more scores are provided: the average value of the strict and relaxed mAP values, and p@50% for the strict view. Submissions that provided a descriptions are marked with (*). Figure 2: (a): The distribution of KPs among the arguments, in the pro stance on the topic Social media platforms should be regulated by the government, for the expert ground truth (strict view) and selected models, normalized to the number of arguments that match to a KP. The JSd vs. the expert distribution is given in brackets. (b) JSd rank vs. AP rank for all models in the Matching Track, per topic and stance. The map between topic id, and topic text can be found at therefore not relevant to compare the distributions over KPs as done above. However, it is possible to perform a qualitative anecdotal analysis. Table 8 in the appendix lists the KPs generated for one topic and stance, by the expert and the different models. Importantly, the expert produced KPs a priori, without seeing the actual set of arguments; while the models generated KPs based on the input arguments. Inspection of the generated KPs suggests that correspondingly, some KPs were perhaps overlooked by the expert.",
        "GPT2_formal_text": "= { schema_name, schema_value } n = { 1, 2, ..., n_K } n_K is the length of the k-th value in the schema. We then use the softmax function to figure out the predicted probability P(s_k | schema_name_p_k) for the k-th value in the schema. Formal: For the second round of experiments, we use the output from the first round of MERT to predict the distribution for each query. Formal: The formula is now like this: Formal: So, the probability P(s_1 | schema_1_p_1) is calculated using this equation. Formal: The output from the first round of MERT is then used to predict the final distribution for the query. Formal: Lastly, we calculate the softmax probability P(s_1 | schema_1_p_p) for the new query by taking the weighted average of the previous (s_1 | schema_1_p_1) and this new (s_1 | schema_1_p_p) distribution. Formal: The formula is basically the same as the first round of MERT, but we added the output from the first round of MERT to the final result. Formal: For the third round, we use the output from the second round of MERT to predict the final distribution for each query. Formal: In each round of MERT, we use a query with schema labels { schema_name, schema_value } n = { 1, 2, ..., n_K } n_K = 1 Formal: After the first round of MERT, we calculate the probability P(s_1 | schema_1_p_1) for each query. This gives us the final probability P(s_1 | schema_1_p_p). Formal: Formal: We use the final predictions from each round of MERT to predict the distribution for each query. Formal: Formal: For the fourth round of MERT, we use the output from the second round of MERT to predict the final distribution for each query. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "argmining",
        "filename": "2021.argmining-1.16.json"
    },
    {
        "casual_text": "D is just a way to measure how different two probability distributions are, and we're using something called Kullback-Leibler divergence for that. The main goal, L sup, for supervised learning is basically the combination of two things: L ce and L com.",
        "formal_text": "where D is a distance function between probability distributions (we use the Kullback-Leibler divergence). The final objective L sup for supervised learning is the sum of L ce and L com.",
        "GPT2_formal_text": "ed_vec2d is basically a feed-forward network that's been adjusted to focus on the mention embedding vectors, but it doesn't include any attention to the actual content words. Formal: Our approach can handle abstract entities (like \"woman\" or \"lion\") in the embedding space and figure out the entailment relation between them. Formal: For example, in Figure 2, the \"woman\" can be understood as the focal point of a lion. Formal: But here's the catch: while we can't directly use knowledge about the specific content words to figure out the entailment relation between the mention and the target entity, we can still pull out the content words as features to help our model predict the entailment relation. Formal: We've got the embedding of the \"woman\" in the text shown in Figure 2. Formal: Using these embeddings, we can create a hierarchical attention-based multimodal representation for the mention and the target entity, which is then combined with the embedding vectors for the words in the text. Formal: So, the main idea behind our method is to extract the embedding vectors from the text and use them to guide the embedding of the mention and the target entity. Formal: We use a bidirectional GRU with a GRU gate with a learning rate of 0.1 to learn the embeddings for the mention and the target entity. We train this model using cross-entropy loss. Formal: Formal: The embedding vectors are generated through cross-entropy loss, and we calculate the loss using the average of the embedding vectors for the mention and the target entity. Formal: Our method can also pick up on the context around the mention and the target entity to make predictions. Formal: Formal: Formal: Here, P_T(e, f) and P_T(f, e) are the conditional probabilities, which are specific to the target entity and the mention, respectively. Formal: Lastly, we use a softmax function to combine the embedding vectors for the mention and the target entity, which gives us a probability distribution for the entailment relation. Formal: We use a bidirectional GRU with a GRU gate with a learning rate of 0.1 to learn the embeddings for the mention and the target entity. Formal: Formal: The embedding vectors are generated through cross-entropy loss, and we calculate",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.319.json"
    },
    {
        "casual_text": "Tables 9 and 10 show how our algorithm boosts the performance of our QuALiM system, like you can see in (Kaisser et al., 2006). In Section 6 of this paper, we explain how answer candidates are ranked using formulas 2 and 3. This ranking is added to the existing QA system's ranking as an extra feature, helping to boost candidates based on their confidence score. \n\nThe difference between the two tables is that Table 9 uses all 1658 questions in our test sets for evaluation, while Table 10 only looks at the 1122 questions where our system could learn a pattern. So, for Table 10, questions that the system couldn’t answer due to limited training data are left out. As you can see, accuracy@1 goes up by 4.9% on the full test set and by 11.5% on the smaller set.\n\nKeep in mind that the baseline QA system we’re comparing to has a couple of advantages: a) It has web-based components, so it can access way more text information. b) The algorithm we’re talking about in this paper is just for answer extraction. For paragraph retrieval, we use the same method as for evaluation set 1, which is explained in Section 7.1. But, in over 20% of cases, this method doesn’t return a single paragraph that has both the answer and at least one question keyword. In those cases, the basic paragraph retrieval makes it really hard for our algorithm to give the right answer.",
        "formal_text": "Tables 9 and 10 show how our algorithm increases performance of our QuALiM system, see e. g. (Kaisser et al. , 2006). Section 6 in this paper describes via formulas 2 and 3 how answer candidates are ranked. This ranking is combined with the existing QA system's candidate ranking by simply using it as an additional feature that boosts candidates proportionally to their confidence score. The difference between both tables is that the first uses all 1658 questions in our test sets for the evaluation, whereas the second considers only those 1122 questions for which our system was able to learn a pattern. Thus for Table 10 questions which the system had no chance of answering due to limited training data are omitted. As can be seen, accuracy@1 increases by 4. 9% on the complete test set and by 11. 5% on the partial set. Note that the QA system used as a baseline is at an advantage in at least two respects: a) It has important web-based components and as such has access to a much larger body of textual information. b) The algorithm described in this paper is an answer extraction approach only. For paragraph retrieval we use the same approach as for evaluation set 1, see Section 7. 1. However, in more than 20% of the cases, this method returns not a single paragraph that contains both the answer and at least one question keyword. In such cases, the simple paragraph retrieval makes it close to impossible for our algorithm to return the correct answer.",
        "GPT2_formal_text": "= {}, y_i = y_i + n - 1, ..., y_M = y_M + n + 1, y_C = y_C - n - 1, ..., y_M = y_M + n + 1, y_M = (y_M + n - 1)^2 y_{M + n - 1}, which basically means that we're looking at the log probability of y_M given y_C. Formal: For each positive x_i and each negative x_i in X, the log probability of y_M for an x_i and an x_i is calculated as: Formal: For the negative samples, the log probability is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E12",
        "filename": "E12-1010.json"
    },
    {
        "casual_text": "DRG is a new way of representing DRS graphs, created specifically for MRP 2020. The goal was to make it as similar as possible to other frameworks (Abzianidze et al., 2020). But there are already a few different ways to encode DRS data. For example, Liu et al. (2018) focused more on labeling edges than nodes in their DRG format. On the other hand, van Noord et al. (2018) worked with DRS in a clausal form, using sets of triples and quadruples. This latter approach is more common among DRS parsers, especially since it was the official format for the DRS parsing shared task (Abzianidze et al., 2019).\n\nThat shared task led to several DRS parsers, including ones by Evang (2019), van Noord (2019), and Fancellu et al. (2019). The best results (with an F1 score of 0.85) came from a word-level sequence-to-sequence model using a Transformer. Just a heads-up: the F1 score for the DRS shared task was based on clausal forms, which isn't directly comparable to the MRP F1 score calculated over DRGs.",
        "formal_text": "DRG is a novel graph representation format for DRS that was specially designed for MRP 2020 to make it structurally as close as possible to other frameworks (Abzianidze et al. , 2020). However, several semantic parsers exist for DRS, which employ different encodings. Liu et al. (2018) used a DRG format that dominantly labels edges compared to nodes. van Noord et al. (2018) process DRSs in a clausal form, sets of triples and quadruples. The latter format is more common among DRS parsers, as it was officially used by the shared task on DRS parsing (Abzianidze et al. , 2019). The shared task gave rise to several DRS parsers: Evang (2019); ; van Noord (2019); Fancellu et al. (2019), among which the best results (F 1 = 0. 85) were achieved by the word-level sequence-to-sequence model with Tranformer. Note that the DRS shared task used F 1 calculated based on the DRS clausal forms, which is not comparable to MRP F 1 over DRGs.",
        "GPT2_formal_text": "_tgt, decoder_att, and decoder_att_s. They're used as input for the encoder, decoder, and attention layers of the Transformer model, as mentioned in Vaswani et al. (2017). Formal: The main goal of DISCO is to spot and pull out important info from text. We train DISCO with the goal of pulling out more relevant info from the text. DISCO works by: (1) pulling out information, (2) measuring how relevant it is, (3) ranking different types of info, and (4) showing how related different parts of the text are. Formal: In the setup shown in Figure 1, the nodes labeled H (text) and D (content) are the entities in the text, and the edges labeled E (entailment) and I (inference) are the connections between these nodes. The red parts on the x-axis represent the sets of labels that show which pairs of entities are important for the selected entities. The colors on the y-axis show how important each entity pair is. Formal: We use the predefined set of labels from DISCO to define the content of the text and how important the entities are. For the entities, we pick the ones that appear in the most sentences. The entities highlighted in red are the ones that we consider as important. Formal: To make DISCO work better with different datasets, we tweak the labels for each entity pair by using both what it means in the text and what other studies have said in the text. Formal: DISCO has five main parts: (1) the main entity, (2) its relations, (3) the relation types that are important for it, (4) the relationship types that are tied to the relation types, and (5) the relation types that are also tied to the relation types. Formal: Here, the main entity is the entity we're looking at, the relations are the relationships, and the relation types are the relationship types. Formal: DISCO can predict how important entities are, both within and across different datasets. Formal: To make DISCO work better with different datasets, we tweak the labels for each entity pair. Formal: DISCO's goal is to find entities with high relevance and extract useful info that's related to the selected entity. Formal: Figure 1: Our model. Formal: Each node in DISCO gets a label that tells us which relation (",
        "directory": "conll",
        "filename": "2020.conll-shared.1.json"
    },
    {
        "casual_text": "For data augmentation, we used synonym replacement on the original dataset, doubling its size. Table 2 shows the results of using this approach on a generation task. We noticed that applying data augmentation to the generation task actually made things worse. But when we used the augmented data for the grounding task, the model scored a 40.55 F1 score and a 23.49 exact match score. This is better than our baseline model, which was trained only on the original grounding task data, by +0.5 F1 score and +0.64 exact match score. These findings suggest that using synonym data augmentation on the generation task's answers doesn't really help the model learn anything useful for the generation task. So, we decided to only use the augmented data for the grounding task during multi-task learning.",
        "formal_text": "Data Augmentation For data augmentation, we apply synonym transformation to the original dataset, attaining twice the baseline size. Table 2 presents the result for data augmentation on generation task. We have observed that applying data augmentation to the generation task degraded the performance. However, by utilizing augmented data on the grounding task, the model achieves a 40. 55 F1 score and a 23. 49 exact match score. Compared to our baseline model implementation trained with the original grounding task data, training with augmented data improved +0. 5 F1 score and +0. 64 exact match score. These results demonstrate that synonym data augmentation on the generation task's gold answers does not provide the model with any informative knowledge for the generation task. Therefore, we include augmented data only on grounding task during multi-task learning.",
        "GPT2_formal_text": "\" [, , ], and \"subjects\" [, ], where each subject is a sequence of words. We group the generated document D using this method, where the total length of the documents is L. Formal: We set the n-gram overlap to n. Formal: The document with the lowest n-gram overlap (let's call this one the document with the lowest n-gram overlap). Formal: The document with the highest n-gram overlap. Formal: The document with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: The two documents with the highest n-gram overlap. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "dialdoc",
        "filename": "2022.dialdoc-1.15.json"
    },
    {
        "casual_text": "Basically, this means that something is considered recognized if we can get the empty string starting from S(0, n, n + 1, n + 1 + m). According to Theorem 1.2 from Boullier's 2000 paper, figuring out if a bottom-up non-erasing k-RCG can be recognized can be done in time O(|G|n).",
        "formal_text": "is recognized iff the empty string can be derived from S( 0, n, n + 1, n + 1 + m ). Theorem 1. 2 ( (Boullier, 2000) ). The recognition problem of bottom-up non-erasing k-RCG can be solved in time O(|G|n",
        "GPT2_formal_text": "ing.py\" from this GitHub repo: https://github.com/adamperezapirom/pretrained-sentence-attention-masking. Formal: The model creates a sentence-level attention mask for each document, which can be adjusted based on the document's features. We train it using the code provided in Section 2.1. The weight for this is figured out using a gradient descent method. The representations it produces are just a series of hidden units, each representing a single sentence. Formal: We set the window size to (n - 2) tokens, with the number of tokens being the number of sentences in the document. This setup ensures that the sentences in the document are roughly the same length. Formal: The model can use the document's embedding E_d as a hidden representation for the hidden layer. Formal: We also use a word embedding layer to predict the attention mask. Formal: The full model setup is shown in Figure 1. The code is available on GitHub: https://github.com/adamperezapirom/pretrained-sentence-attention-masking. Formal: In this setup, the model looks at the document's embedding E_d to figure out the attention mask, which is created by combining the masked sentence embedding E_d with the hidden representation of the target sentence, H_t_d. Formal: This is the default setup. Formal: Here, H_t_d is the embedding of the sentence, and H_t_d is the hidden representation of the target sentence. Formal: We train the model using the code provided in Section 2.1. Formal: Formal: The model can use the document's embedding E_d as a hidden representation for the hidden layer. Formal: Formal: We also use a word embedding layer to predict the attention mask. Formal: Formal: The full model setup is shown in Figure 1. Formal: We train the model using the code provided in Section 2.1. Formal: The model can use the document's embedding E_d as a hidden representation for the hidden layer. Formal: Formal: Formal: The document's embedding E_d is the hidden representation of the target sentence, H_t_d. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C08",
        "filename": "C08-2026.json"
    },
    {
        "casual_text": "1. SyntheticQE-MT: Basically, the MT models are used to create the target side of the synthetic data.",
        "formal_text": "1. SyntheticQE-MT: The target side of the synthetic data is produced using MT models.",
        "GPT2_formal_text": "= [ e1:p-1, e2:p-2, ..., ek:p-k ], where p stands for the length of the input text. These models are built using a RNNG with attention-based learning, which helps them capture the meaning of a sentence better. Formal: The explanations we get for each aspect are shown in Fig. 1. Formal: For the RNNG model, we combine the encoder-decoder attention from Fig. 1 with the local context representation from Fig. 2. We also add a hidden layer that can adjust the attention weights to make the model better at capturing context. Formal: Basically, the representation of the input text is broken down into a bunch of representations that are related to each aspect. These are then fed into an encoder-decoder model to create the overall representation of the input text. Formal: Using this combined representation, we can find the best possible hidden states for the attention weights to make the model better at handling context. Formal: The final representation of the input text is the hidden state of the encoder-decoder model, which we call h = (e1, e2, ..., ek). Formal: In this setup, the local context representation helps the model better understand the context of the input text, and the combination of the encoder-decoder model and the local context representation gives us the final representation of the input text. Formal: In this project, we focused on explaining the aspect of food. For the RNNG model, we added a hidden layer to create the representations of the food aspect. This layer can adjust the attention weights to make the model better at capturing the context of the input text. Formal: In this project, we also introduced a new attention mechanism that uses attention to improve the representation of food aspects. Formal: Here, we tested our model on a really tricky task with tons of inputs and outputs—the Dravidian Linguistic Inquiry (DIAL) task. Formal: We used the same setup for our experiments as in the RNNG model. Formal: The DIAL task has 10 types of questions: Formal: The key to successfully answering these questions is to figure out the right answer for each one and make the right predictions. Formal: The only way to do this is by asking questions that are designed to answer the answers and correctly identify the answers. Formal: To test our model on this task,",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.267.json"
    },
    {
        "casual_text": "Okay, so when we looked at the errors in our system, we found that for certain words, it couldn’t make a matching cluster for specific meanings. For example, words like \"yard\" when it refers to a ship, or the \"impound/enclosure\" meaning of \"pound\" didn’t get matched properly. It seems like this happened because these meanings don’t show up much in English Wikipedia. In fact, the \"yard\" meaning as a ship part only appeared twice, and the \"impound/enclosure\" meaning of \"pound\" showed up three times in the top 100 words. That’s why the system didn’t create a cluster for them.",
        "formal_text": "Analysis Analysis of our system's error shows that for some words the system could not create a matching cluster for specific senses (to name a few examples, \"yard\" as a ship identifier and \"impound/enclosure\" sense for the word \"pound\"). It appears that a matching cluster was not created due to the low tally of these senses in the English Wikipedia, and indeed the two senses appeared only two and three times respectively in the 100 for all words 17 Computed as",
        "GPT2_formal_text": "ed_tot_predict(x, y) = (1 - alpha_tot_predict) * log p_max(y, x) for all x, y in the X dataset.\n\nIn the original paper by Liu et al. (2019b), we use a special equation to tweak the weights for the attention. Specifically, we add two new terms to the normal form of the cross-entropy loss, like this: Formal: If θ_t is the updated output embedding and α_t is the decoder hidden state vector, then Formal: (α_t | θ_t) * log p_max(y, x) Formal: Formal: And for each input sequence, the updated output embedding, α_t, and decoder hidden state vector, θ_t, together form the final prediction. Formal: Formal: Formal: Formal: Formal: Formal:\n\nBasically, we tweak the weights for attention and attention-based state vectors to work better for our specific task. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.325.json"
    },
    {
        "casual_text": "So, these findings suggest that the window size for rank N across the whole dataset and the 4th and 5th features are pretty important. They seem to stand out and play a significant role in the overall analysis.",
        "formal_text": "These results lead to the conclusion that the window size of the rank N for the entire corpus and the 4 È Ö × ÓÒ AEÙÑ Ö Ó ×ÝÒÓÒÝÑ× ÓÖÖ ØÐÝ ÜØÖ Ø AEÙÑ Ö Ó ×ÝÒÓÒÝÑ× ÜØÖ Ø 5 Ê ÐÐ AEÙÑ Ö Ó ×ÝÒÓÒÝÑ× ÓÖÖ ØÐÝ ÜØÖ Ø AEÙÑ Ö Ó ×ÝÒÓÒÝÑ× Ò Ò×Û Ö × Ø",
        "GPT2_formal_text": "= (x_t, y_t) = argmax(x_t, y_t) log p(w|x_t, y_t)\n         Casual: Sure! Here's the informal version:\n\n- It's easier to type! Check out [TOLEDB92](http://www.toliedb.org/92/TOLEDB92.html)\n- It's faster! It runs in O(N^6) time on 8 CPUs!\n- It's a good way to handle text data! It's faster than doing word-level calculations on the same amount of data. Formal: We use this approach to figure out the attention weights in an NLP model. When we add the text embedding into the attention, it helps the model focus on the important parts of the text. Formal: We do the same thing with the word embedding, but we also check the attention weights to see how much each word contributes. Formal: For the language model (LM) we just add the input sequence to the attention vector. Formal: We use a beam search method, which is the same one used in the TAC KBP task (Mikolov et al., 2013). Formal: The final attention weight for the output vector, Φ_t, is calculated by multiplying the log probability of w_t happening with the log probability of y_t happening. Formal: You can find more info in [TOLEDB92](http://www.toliedb.org/92/TOLEDB92.html). Formal: We also created a small task for generating feedback comments. The models we used are the same ones we talked about earlier. Formal: We tweaked the model by swapping out the log likelihood of w_t happening with the log probability of y_t happening. Formal: Formal: We also took a closer look at how well the model is doing. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C04",
        "filename": "C04-1116.json"
    },
    {
        "casual_text": "Okay, so let’s say we have a pair of sentences, (x, y), where x is a sentence in some input (source) language, and y is its translation in a different output (target) language. X and Y represent the vocabularies for the input and output languages, respectively. The IBM alignment models (from 1 to 5) are basically ways to calculate the probability P(x|y), which tells us how likely it is that x is the original sentence that got translated into y.",
        "formal_text": "Let (x, y) be a pair of input-output sentences; i. e. x is a sentence in a certain input (source) language and y is its corresponding translation in a different output (target) language. Let X and Y denote the input and output vocabularies, respectively. The IBM aligment models (1 to 5) are parametric models for the translation probability P (x | y); i. e. , the probability that x is the source sentence from which we get a given translation y.",
        "GPT2_formal_text": "ed, output_prediction, output_sigmoid, and output_vec2d. The basic setup is: (A ∪ {h, t}) where h and t are the input hidden units, and A_i and A_j are the output vectors for the i-th and j-th elements, respectively. Formal: The encoder takes the input h ∈ Rn×d and spits out a representation of the document, which is represented as (A ∪ {h, t}) ∈ Rn×d and T_i and T_j, respectively. Formal: The decoder takes that representation and spits out the actual outputs, which we call [A ∪ {h, t} ∈ Rn×d ∩ T_i and T_j] ∈ Rn×d. Formal: This process of creating outputs, where the input and output vectors are transformed using a specific transform, is what we call the distillation process. Formal: The final representation of the document is the result of the decoder's operation, which we call [A ∪ {h, t} ∈ Rn×d ∩ T_i and T_j] ∈ Rn×d. Formal: Formal: There are two types of outputs: the more detailed ones (like the correct or wrong answers) and the simpler ones (like the entire document). Formal: We’ll go into more detail about how to generate these outputs in the next sections. Formal: In the next sections, we’ll talk about the three key parts of the distillation process: (1) the transformer encoder, (2) the transformer decoder, and (3) the transformer global transformer. Formal: The transformer encoder is the main component that creates the final representations of the document. It uses a transformer model with a hidden state size of 768 and a hidden layer dimension of 1024. Formal: The transformer decoder is a smaller, simpler version of the transformer model. It only has a hidden layer dimension of 2048 and a layer dimension of 1024. Formal: In the transformer global transformer, the global transformer layer has 768 hidden units and a dimension of 2048. Formal: The transformer global transformer uses a transformer model with a hidden state size of 768 and a dimension of 2048. Formal: The transformer global transformer uses a transformer model with a hidden state size of 768 and a dimension of 2048",
        "directory": "eamt",
        "filename": "2006.eamt-1.20.json"
    },
    {
        "casual_text": "This method assigns a weight to each paragraph, showing how likely it is to be related to other paragraphs. The weight takes into account things like the paragraph's length, how big and how often certain word groups (n-grams) show up in it, and how those n-grams are spread out across the whole text.",
        "formal_text": "This procedme associates each paragraph with a weight representing its probability of correlating with other paragraphs. The weight considers factors such as the size of the paragraph, the size and frequency of n-grams existing within that paragraph, and the distribution of the n-grams throughout the corpus.",
        "GPT2_formal_text": "= [q_k, q_m, q_s]            Casual: To figure out the probability of the leftmost child in the pair (q_k, q_m), we use the leftmost child embedding from the MLE model. This is shown in Equation 8. Formal: We use the knowledge from the submodular self-attention model to predict the child's embedding. Formal: Using the submodular self-attention model to predict the embedding for the leftmost child in the pair. Formal: To predict the embedding for the pair, we use the submodular self-attention model to figure out the probability of the child's embedding. Formal: We also train a classifier to predict the child embedding embedding embedding. Formal: Formal: We train the classifier to predict the embedding embedding embedding. Formal: Formal: To predict the embedding embedding embedding for the leftmost child, we use the submodular self-attention model to learn the probability of the child's embedding embedding embedding. Formal: Formal: Formal: Finally, we calculate the probability of the child's embedding embedding embedding by using the submodular self-attention model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C94",
        "filename": "C94-2171.json"
    },
    {
        "casual_text": "Alright, so the second extra dataset we made came from pulling out all the split-antecedent examples from the raw annotations in PD to make sure we got as many as possible. After grabbing all those split-antecedent annotations, we used majority voting to sort things out when people didn’t agree. This gave us 47.7k split-antecedent annotations tied to 6.2k mentions (check Table 1 for the details). \n\nWe tested how good this method was using the gold part of the PD corpus, and it turned out pretty well—91.7% recall, which is exactly what we wanted. But, as you’d expect, the data’s a bit messy, with only 11.1% precision and an F1 score of 19.7%. We went through the false positives manually and found most of the errors were because of three things: single-antecedent coreference (where the chains were marked as split-antecedents), bridging references (which didn’t need to be annotated), and other annotation mistakes. \n\nNow, the first two types of mistakes aren’t a big deal for our task because our third and fourth datasets are built using those kinds of relationships. \n\nOh, and another thing—ARRAU also has bridging references, and one of the bridging relations they annotated, element-of (and its inverse), is super relevant to figuring out split-antecedent plurals.",
        "formal_text": "Raw Crowd Annotations (PD-CROWD) The second auxiliary corpus was created by extracting all split-antecedent examples from the raw annotations in PD to maximise recall. After extracting all splitantecedent annotations, we used majority voting as our aggregation method when players did not agree on split-antecedent annotations. In this way, we extracted 47. 7k split-antecedent annotations associated with 6. 2k mentions ( Table 1 ). The quality of this extraction method was evaluated on the gold portion of the PD corpus as well; the resulting dataset has a recall of 91. 7%, which fulfils the goal of this setting. As expected, the corpus is noisy, with a precision of 11. 1% and an F1 of 19. 7%. We manually checked the false-positive examples, finding they are mainly due to three types of mistakes: single-antecedent coreference (the coreference chains were annotated as the split-antecedent), bridging reference (not required to be annotated), and other annotation mistakes. The first two types of mistakes are not harmful to our task as our third and fourth auxiliary corpora are created using those types of relations. Element-of Bridging References (ELEMENT-OF) ARRAU is also annotated with bridging references, and one of the bridging relations covered by the annotation, element-of (and its inverse) are very closely related to the task of resolving split-antecedent plurals.",
        "GPT2_formal_text": "_len\" to set the maximum length of the attention vector. The value of k is determined by the specific task you're working on and the model's parameters. If you're dealing with an image classification task, you can set k to any value that works for that. Formal: The attention vectors are calculated using a two-layer feedforward neural network (FFNN). Formal: The model learns to predict the next word based on the last hidden state it received. Formal: We combine the last hidden state and the embedding of the word's position into one, which then helps the model predict the next word. Formal: We calculate the hidden state and the embedding of the word's position at the same time, and then use that combined info to predict the next word. Formal: Finally, we use this combined representation to predict the next word's embedding. Formal: After that, we do some more calculations to combine the output of the model's output layer with the output of the embedding layer and the hidden state layer. Formal: This process happens over and over again, each time focusing on the hidden states and embeddings of the current word and the embedding of the next word. Formal: We calculate the hidden state and the embedding of the word's position at the same time, and then use that combined representation to predict the next word's embedding. Formal: Formal: Formal: Here's a more casual version of the explanation: Formal: First, let's look at an example of a neural network model learning to predict the next word. Formal: This network helps predict the next word's embedding by combining the hidden states and position from the last hidden state layer with the hidden states and position from the embedding layer and the last hidden state layer. Formal: The network calculates the hidden state and the embedding at the same time, using the combination of these two vectors. Formal: Finally, the network uses this combined representation to predict the next word's embedding. Formal: The hidden state and the embedding at the same time, along with the word's position and the last hidden state layer, give the model the probability to predict the next word's embedding. Formal: To make this work, we add a special placeholder for the word's position and embedding at the last hidden state layer, giving it the same probability as the word itself. Formal: Formal: Let's say we have a sentence",
        "directory": "coling",
        "filename": "2020.coling-main.538.json"
    },
    {
        "casual_text": "Wow, this text is completely scrambled and looks like a random mix of symbols and letters. It seems like it's been encrypted or encoded in some way. If you're trying to decode it or make sense of it, you might need to use a specific key or method to unlock the original message. It could be a code, a cipher, or even a mistake in formatting. \n\nIf you have any additional context or know what this is supposed to be, I can try to help you figure it out! Otherwise, it’s a bit like trying to read a foreign language without a dictionary. Let me know if you need help with anything else!",
        "formal_text": "¦ ¢ ¥ F B þ ¥ 3 © þ ¥ ¤ w ¤ ¦ § 4 V þ X q ¥ ¢ ÿ l ¤ 0 ¤ ¦ § A ¢ ± ü ¦ A ¢ ¤ z ü ¦ § © ÿ H ü ¦ A ¢ Y 4 7 © ü ¦ I 3 ¤ H ¥! R © A ¢ \" 9 & Ê þ b ü ¦ I 3 ¤ H! ¦ ¦ ý Í (! ¦ B % z 1 ¢ 4 n ü ¦ I 9 2 Ê ü i ÿ R § \" ¢ 4 0 b U ) r t © A ¢ ¤ ¦ 4 7! 0 I ( t ü ¦ I H ü ¦ A ¢ ü i ÿ 2 ( £ ¢ ¤ ) H © A ¢ \" 9 & Ê þ b ü ¦ I ¤ R 4 þ ¥ § ¤ 1 ¢ © s ÿ i p & r H # # I P ¤ \" U v C E ¦ Q ¢ Y 4 7 3 4 7 ¢ & % ü 1! C þ b ü 1 ¢ a e © A ¢ 4 \" # é þ B! C þ b ü ¦ I 9 ¥ ¢ $ ¤ ¦ ¢ g ü 1 ¢ 4 7 ¢ ¤ F $ ü ¦ A ¢ 0 ý þ ¥ t! 0 ¢ º þ ¥ ¤ 1 £ ¢ ' ü ¦ þ b ü Á ü ¦ A ¢ ¡ H ¥! C ý r þ b ü ¦ I ¤ z ü 1! 0 § 4 n ü ¦ § A! 0 ¢ l H 2 § A ¢ ¤ % ü ¦ I ¤ e þ ¥ ¤ t A ü ÿ ¥ ¢ ü e @ ¢ \" ¢ °¤ ¦ § A G 4 \" I ¢ o ü ¦ #ÿ ¢ # Ê þ B £ ¥! C þ b ü 1 ¢ © § A ( £ b U S b x 0 a E ¤ ¤ ¡ A E g W Y § A! ) 4 \" # Ê þ ¥ ¤ ¦ ¤ ¦ I h g 4 þ b ü ¦ I H ü ¦ A ¢ 8 © A ¢ \" 9 & Ê þ b ü ¦ I 3 ¤ H! 0 ¦ ý (! ¦ z 1 ¢ 4 T % ü ¦ I 9 & ü i ÿ t s ¤ 8 t þ ¥ ¤ ¦ ¢ © { ý r þ ¥ #ÿ X ü ¦ A ¢ ý ¥! ¦ ( 3 A B % q ¤ z ÿ & o ü þ ¥ 4 n ü ¦ 4 H¢ º þ b ü ¦ § A! ¦ ¢ ¤ ) H A 2 © ¢ ¤ ) 4 7 ¢ 4 n ü 1 ¢ © s ÿ þ $ & % f (! ¦ z 1 ¢ 4 n ü ¦ I 9 ¥ ¢ © A ¢ \" ( £ ¢ © A ¢ 3 4 n ÿ ¢ © ' ¢ ¥ U b x 3 0 $ g c ¤ \" B ¥ c ¡ ¥ E g c ¤ ¡ C D A B g 5 ¥ } 2 ¤ i P R I ü © ÿ ( £ ¢ ¤ H © A ¢ \" 9 & Ê þ b ü ¦ I ¤ H! 0 ¦ ý (! 0 z ¢ 4 n ü ¦ I 9 & ü © ÿ P 8 ü ¦ þ X A ¦ ý t þ ¥ # c A & © A ¢ ¦ þ ¥ © j ü ¦ ¤ Î þ b ü z ü 1! 0 I 3 § ü 1 ¢ a 4 7 ¢ 4 n ü 1 ¢ © Q o ÿ þ A A % f (! ¦ z 1 ¢ 4 n ü ¦ I 9 ¥ ¢ u ¢ © ' ¢ $ 4 þ ¥ t @ ¢ w © 3 ¤ z ü ¦ ' § 3 ¤ ¦ A ¢ © b ¾ \" ü ¦ A ¢ þ b ü z ü 1! 0 I g § x ü 1 ¢ v ¤ w A A % f (!",
        "GPT2_formal_text": "_t)) {\n         Casual: A few other systems have been suggested before, like the ones from (Koch et al., 2005) and (Guu et al., 2012). But the thing is, they’re not fully connected to each other. The main goal here is to figure out how the system decides whether a sentence is helpful or not. We also need to decide what counts as helpful. For example, if you’ve got this system, it might decide that a sentence doesn’t help because it mentions the wrong thing. On the other hand, if the system says something is helpful, it’s because it talks about something that’s important. Formal: The system can pick one or more of the three types of info. Let’s call this list of possible info F = {f1, ..., fn}, which is basically the rules of the system for deciding if something is helpful. Formal: For the first type, the system doesn’t decide if the sentence is helpful or not until after it’s made up of all the possible info. Formal: For the second type, the system does decide whether to use the info it finds for a specific reason before it’s even complete. Formal: And for the third type, the system doesn’t have a specific decision in place for a specific reason until after it’s all done. Formal: Formal: The question is, how do we figure out which info is helpful and which isn’t? Formal: The main idea here is to find a special part of the system that can decide which info is helpful. Formal: Here, we’re using the system from (Koch et al., 2005) to show how this works. Formal: We’ve come up with a way to handle this: Formal: We’ve set up a system that uses a bunch of rules to decide which info is helpful or not. Formal: The system can pick one or more of the three types. Formal: This is basically the rules for the system for deciding if something is helpful or not, but it’s not super clear what those rules actually mean or how they’re set up. Formal: Formal: Formal: Formal: We’ve also talked about how the system handles a sentence with a specific problem. Formal: Form",
        "directory": "C04",
        "filename": "C04-1042.json"
    },
    {
        "casual_text": "• Thinking about the ability to get a basic grasp of the language stuff.",
        "formal_text": "• Considering the capability to acquire basic understanding of the linguistic-level information.",
        "GPT2_formal_text": "_t and attention_mask_t are the outputs of the attention part and attention part, respectively. It's super important to keep these outputs in the same format when combining them with the model's parameters. Formal: This part deals with the loss, which is basically the negative log of the loss function. We'll use the negative log as the loss for the combined model. Formal: The final output of the model is the normalized attention value, which is calculated using the formula: Formal: When we're dealing with a model, the attention mask is calculated by averaging the attention scores from the words in the input text, which are t_i, t_i+1, ..., t_i+n. Formal: Formal: Here, t_i, t_i+1, ..., t_i+n represent the attention score from the word in the input text, and t_i+1, ..., t_i+n are the attention score from the word in the output text, respectively. Formal: Formal: Once we get the attention mask and the normalized attention value, we can use them to calculate the loss. Formal: Formal: Formal: Finally, we get the final loss by applying the softmax function to the normalized attention value, which gives us the loss as a result. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eacl",
        "filename": "2021.eacl-main.137.json"
    },
    {
        "casual_text": "To check how well our optimization process works, we ran all three steps of it on all 13 datasets from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). Table 1 shows the labeled attachment scores with the default settings and after each of the three optimization phases, along with the difference between the final setup and the default. \n\nFirst off, the optimization boosts parsing accuracy for every language, no exceptions. The improvement ranges from about 1 percentage point for languages like Chinese, Japanese, and Swedish to 8-9 points for Dutch, Czech, and Turkish. For most languages, the biggest jump comes from feature selection in phase 3. However, for languages with lots of non-projective dependencies—like Czech, Dutch, and Slovene—phase 2 also makes a big difference, especially when it comes to choosing the right parsing algorithm.\n\nThe time it takes to run the optimization depends on the dataset size. It’s about half an hour for smaller sets but can take up to a day for really big ones, like the Czech dataset.",
        "formal_text": "In order to assess the usefulness and validity of the optimization procedure, we have run all three phases of the optimization on all the 13 data sets from the CoNLL-X shared task on multilingual dependency parsing (Buchholz and Marsi, 2006). Table 1 shows the labeled attachment scores with default settings and after each of the three optimization phases, as well as the difference between the final configuration and the default. 5 The first thing to note is that the optimization improves parsing accuracy for all languages without exception, although the amount of improvement varies considerably from about 1 percentage point for Chinese, Japanese and Swedish to 8-9 points for Dutch, Czech and Turkish. For most languages, the greatest improvement comes from feature selection in phase 3, but we also see sig-nificant improvement from phase 2 for languages with a substantial amount of non-projective dependencies, such as Czech, Dutch and Slovene, where the selection of parsing algorithm can be very important. The time needed to run the optimization varies from about half an hour for the smaller data sets to about one day for very large data sets like the one for Czech.",
        "GPT2_formal_text": "= {y_i, t} for all i from 1 to n. Formal: A linear projection Φ(h_t, f_t, h_1, ..., h_n) takes a matrix H, which is a matrix of size m × n × d, and turns it into a vector v that looks like this: Formal: The final output is calculated by averaging the vectors from the three different directions, just like in equation (6). Formal: The discriminator for a single projection Φ(h_t, f_t, h_1, ..., h_n) is a specific linear function g(h_t, f_t, h_1, ..., h_n) that's defined using equation (6). Formal: To handle the \"not-a-projection\" problem, you can add an extra linear function, g(h_t, f_t, h_1, ..., h_n), that uses the knowledge of the projected point x. Formal: So, basically, you're adding a linear function g(h_t, f_t, h_1, ..., h_n) that works with the projection matrix. Formal: So, you're adding a linear function that uses a projection matrix, and you're doing this in a specific way. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E12",
        "filename": "E12-2012.json"
    },
    {
        "casual_text": "In this paper, we talk about how context plays a big role in figuring out if emotions and their causes are actually connected. To tackle this, we came up with a new task: figuring out if an emotion and its cause are linked in a given context. We made a dataset for this by manually labeling stuff and using negative sampling from the ECPE dataset. Plus, we created a prediction aggregation module (PAM) that’s not too heavy on the computer, so models can tweak their final answer based on the type of emotion-cause pair in a document. The experiments show that our PAM works well and can be used in different setups.",
        "formal_text": "In this paper, we articulate the importance of context in determining the causal relationships between emotions and their causes. To address this problem, we define a new task of determining whether or not an input emotion-cause pair has a causal relationship under a specific context. We construct a dataset for our task through manual annotation and negative sampling based on the ECPE dataset. Furthermore, we propose a prediction aggregation module (PAM) with low computational complexity, to enable the models to dynamically adjust the final prediction according to the type of emotion-cause pair contained in a document. Experiments demonstrate the effectiveness and generality of our proposed PAM.",
        "GPT2_formal_text": "ed = [x_mask, y_mask], c = [], d = [], y_d = [], c_d = [], g_d = [],\n          Casual: In the first step, the input embeddings h_i and h_j come from a pretrained neural network called G_NN. These embeddings are then fed into an attention mechanism, which is a recurrent neural network. The output from this mechanism gets combined with the input embedding h_i to create the context embedding y_i. This combined embedding is then fed into another attention mechanism, which is a fully connected layer in the network. After that, the output from the second attention mechanism, which is a grid cell, is combined with the input embedding h_j to create the output embedding y_j. This combined embedding is then fed into the final output layer of the network. Formal: Alright, so to get the attention weights for the input and output sequences, the network figures out how to combine the embeddings of the output and input. Formal: After that, the network decides on the attention weights for all the tokens in the input and output. Formal: Lastly, the network spits out the final output. Formal: Formal: Also, the network uses a softmax function to figure out the probability for the output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.252.json"
    },
    {
        "casual_text": "So, take sentence (2a), where we use the comparative form \"taller\" to compare how tall two people are. Then there's (2b), which mentions a specific height, like 5 feet. And (2c) uses the basic form of the adjective \"tall,\" which kind of implies a comparison with some hidden standard. In degree-based semantics, these gradable adjectives are seen as two-part things that involve an entity and a degree (Cresswell, 1976). For example, (2b) would be analyzed as \"tall(john, 5 feet).\"",
        "formal_text": "For example, the sentence (2a), in which the comparative form taller of the gradable adjective tall is used, compares the degree of height between two persons. (2b) is an expression that includes a specific height, which is the numerical expression 5 feet. (2c) is a sentence using the positive form of the adjective, which can be regarded as representing a comparison with some implicit standard value. In degree-based semantics, such gradable adjectives are treated as two-place predicates that have entity and degree (Cresswell, 1976). For instance, (2b) is analyzed as tall(john, 5 feet),",
        "GPT2_formal_text": "= [s_i, r_i, s_j]\n          Casual: To get the attention weights, we just grab the top-k representations of each word and call it h_wt. Formal: In this part, we're assuming that word w_i is already connected to the query t_i in a way that we can figure out based on how the two words are connected in the query. This connection can be represented by an attention score, which we write as h_att_i = h_wt_i + e_wt_i. The similarity between the query and query-w_i is just the average of all these attention scores for each word. Formal: So, we start with a really simple version of this. We just pick the k most similar words to get the attention score, and we use a linear function that measures how similar they are. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-srw.35.json"
    },
    {
        "casual_text": "Alright, so the experts gave us some tips on how to do this task really well. Here's what they said:\n\n1. **True Counter Speech**: This one:\n   - Uses a line from the speech it's responding to.\n   - Talks about a specific point or argument from that speech.\n   - Covers more stuff from the speech than the other options.\n   - Hits the points in the same order they come up in the speech.\n   - Talks about similar topics.\n   - Focuses on the main issue in the speech.\n\n2. **Elimination**: This means getting rid of the other options because they talked about things that weren't even in the original speech.\n\n3. **Guess and Other**: These are for when you have to just wing it and write something in your own words.\n\nSo, that's the gist of what the experts said. Pretty straightforward, right?",
        "formal_text": "Reasons analysis The explanations provided by the experts revealed several best-practices for this task, which we categorized as follows: The true counter speech quotes a phrase from the supporting speech; mentions a specific case or argument from the supporting speech; is more comprehensive and addresses more issues raised in the supporting speech than the other candidates; addresses those issues in the same order as they appear in the supporting speech; discusses similar issues; deals with the main issue raised in the supporting speech. Another reason was elimination -discarding the other candidates since they responded to issues or arguments which were not raised in the supporting speech. The last two categories were guess and other (which required writing a reason in free form language).",
        "GPT2_formal_text": "is a part of the masked token representation that the attention mechanism has to deal with. Formal: In the second step, the token representation is turned into a dense vector using the input mask. Formal: After that, the attention part of the token representation is figured out by using a weighted average of the attention for each token. Formal: The weighted average is a function that takes the token representation and multiplies it by the hidden state vector of the target token. Formal: In the last step, the token representation is turned into a single number by using the same weighted average function, but this time, it also adds up all the attention vectors that have been calculated for the target token. Formal: Finally, the final representation of the target token is computed by summing up all the tokens in the sequence that have been processed so far. Formal: We show how our method for detecting target-side headings (1, 2) works in the figure below. Formal: The three attention modules are shown, along with the matrix multiplication and weighted average function they use to calculate the final token representation. Formal: Formal: The attention mechanism uses the input mask as the key to understand the context. Formal: The normalized attention matrix for the target token is calculated based on the attention weights. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "We also trained the model on the 960-hour LibriSpeech dataset, as mentioned in Table 11. The tests show that our SpeechT5 model gets a big boost in performance, even without adding a language model (LM) on top. It works just as well, or even better, than wav2vec 2.0 when they do use an LM. To check how good the TTS results are, we used the NISQA-TTS tool for automatic evaluation. It's way easier and cheaper than using MOS or CMOS, which require people to listen and rate the audio. As you can see in Table 13, the version of...",
        "formal_text": "We also fine-tune the model on the 960 hours set of LibriSpeech, as reported in Table 11. Experiments show that the proposed SpeechT5 model achieves significant improvement even without LM fusion, and it performs comparable or even better than wav2vec 2. 0 with LM fusion. We use the automatic evaluation tool NISQA-TTS to verify the performance of TTS results here, because it is convenient and cheap compared with MOS and CMOS, which need to be evaluated by humans. As shown in Table 13, the variant of",
        "GPT2_formal_text": "= ([CLS], [ID], [SEP])         Casual: The input sentence for the semantic role compressor is basically a bunch of tokens, where each token has its own semantic role label. Formal: (4) You can also use the compressor to create the semantic role representations for the main parts of a sentence. Formal: (5) And for the semantic role compressor, you can also create a specific sub-tree that covers the whole sentence. Formal: (6) Formal: (7) Here's how the compressor works in a simpler way: Formal: (8) A semantic role compressor works similarly to a sentence compressor, but instead of dealing with tokens, it looks at each word individually. Formal: (9) Formal: (10) Formal: (11) We use two compression methods: syntax and role-specific. Formal: (12) The compression is based on how the sentence is structured. Formal: (13) For example, the compressor for the sentence \"John looked for Peter\" looks at the syntactic structure of the sentence. Formal: (14) Formal: (15) The compressor for the sentence \"John brought Peter to Peter's house\" works the same way as the sentence compressor. Formal: (16) Formal: (17) Formal: (18) The compressor for the sentence \"John sent Peter to Peter's house\" doesn't need to see the syntactic structure of the sentence, but it still needs to pick the right word to do the job. Formal: (19) Formal: (20) Formal: (21) Formal: (22) And for the sentence \"John took Peter to Peter's house\", the compressor can't just look at the word \"to\", but it can also look at other words in the sentence, like \"to Peter's\", \"Peter\", \"house\", and so on. Formal: (23) Formal: (24) Formal: (25) Formal: (26) Formal: (27) Formal: (28) We use a constrained language model to decide how much to compress the input. Formal: (29) Formal: (30) The constrained language model is a simpler version of the usual constrained treebank. Formal: (31) Formal: (32) So, in simpler terms, the compressor decides how much to compress based on how the input is",
        "directory": "acl",
        "filename": "2022.acl-long.393.json"
    },
    {
        "casual_text": "So, entity linking has its limits and can be kind of random. For instance, systems by Ferragina and Scaiella (2010) and Ratinov et al. (2011) both correctly link \"vitamin C\" but mess up with \"pineapple juice,\" linking it to just \"pineapple.\" The thing is, \"pineapple juice\" isn’t recognized as a beverage because it’s not important enough to have its own Wikipedia page. As Table 1 shows, Wikipedia tends to focus on well-known entities but misses out on less common or newer ones. For example, Wang et al. (2012) found there are over 900 active shoe brands, but only 82 are listed on Wikipedia. In areas like intelligence analysis or local search, non-Wikipedia entities are often super important. That’s why we’re looking at the problem of unlinkable noun phrases: when a noun phrase doesn’t link to Wikipedia, we need to figure out if it’s an entity and what its specific type is. It’s tricky because not all noun phrases are entities—like \"Some people,\" \"an addition,\" or \"nearly half.\" Plus, predicting semantic types is hard because there are so many different kinds of entities in regular text. In our experiments, we used the Freebase type system, which has over 1,000 different semantic types.",
        "formal_text": "Wikipedia). Thus, entity linking has a limited and somewhat arbitrary range. In our example, systems by (Ferragina and Scaiella, 2010) and (Ratinov et al. , 2011) both link \"vitamin C\" correctly, but link \"pineapple juice\" to \"pineapple. \" \"Pineapple juice\" is not entity linked as a beverage because it is not prominent enough to have its own Wikipedia entry. As Table 1 shows, Wikipedia often has prominent entities, while missing tail and new entities of the same types. 1 (Wang et al. , 2012) notes that there are more than 900 different active shoe brands, but only 82 exist in Wikipedia. In scenarios such as intelligence analysis and local search, non-Wikipedia entities are often the most important. Hence, we introduce the unlinkable noun phrase problem: Given a noun phrase that does not link into Wikipedia, return whether it is an entity, as well its fine-grained semantic types. Deciding if a non-Wikipedia noun phrase is an entity is challenging because many of them are not entities (e. g. , \"Some people, \" \"an addition\" and \"nearly half\"). Predict-ing semantic types is a challenge because of the diversity of entity types in general text. In our experiments, we utilized the Freebase type system, which contains over 1, 000 semantic types.",
        "GPT2_formal_text": "and attention_mask_len from a CNN to figure out which image (h_i) shows the answer. Formal: So, let's say we have a sequence of words, h_0, ..., h_T, and it has a size of T. In the CNN model, we take the word w_t and the word w_0, ..., w_T, and turn them into a vector (h_t, ..., h_T) with a size of T. This vector then goes through a softmax layer and gets a probability of 1 if w_t and w_0, ..., w_T are part of the sequence. Formal: For each word w_t, we have a sequence of words w_0, ..., w_T and a size of T. The model learns to predict if w_t and w_0, ..., w_T are in the sequence or not based on the words in the sequence. Formal: The vector h_t, ..., h_T can be written as: Formal: Each word in the sequence w_t, ..., w_T gets its own vector h_t, ..., h_T. This is done using the softmax layer. Formal: Let's say we have a sentence x = (x_i, ..., x_T_i) with a length of T. Each word x_i in the sentence gets its own vector h_x_i, ..., h_T_i. Formal: After applying the attention layer, we get a probability of 1 if x_i is in the sequence. Formal: The probabilities from different attention layers are combined together and we calculate a final probability for the sequence. Formal: Formal: The model decides if a word x_i is in the sequence based on its label h_x_i, ..., h_T_i, which is calculated using the attention layer. Formal: Formal: Formal: Formal: The model learns how to represent the response x_i by looking at the words in the context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D12",
        "filename": "D12-1082.json"
    },
    {
        "casual_text": "Our method works with any neural encoder-decoder setup. For this project, we used the big pre-trained BART model (Lewis et al., 2019) and tweaked it with our own technique. Testing it on real news articles showed that our approach improves on what's already out there. When we applied it to a synthetic domain, the BART model, after being fine-tuned with our weak supervision, became way more efficient with data. It even beat the best systems before by a lot, using just 0.4% of the training examples.",
        "formal_text": "Our approach is compatible with any neural encoder-decoder architectures. In this work, we use the large pre-trained BART model (Lewis et al. , 2019) and fine-tune with the proposed method. Experiments on real news articles show our approach achieves performance boosts over existing methods. When adapting to the previous synthetic domain, the BART model after fine-tuning with our weak supervisions becomes substantially more data efficient, and outperforms previous best-performing systems greatly using only 0. 4% training examples.",
        "GPT2_formal_text": ". Each label might have multiple possible translations, but we need to pick one that works best. The reason for this is that our system uses a matrix A, which is like a tree structure, and each node in the tree has a bunch of possible translations, which can be represented using a vector v. So, to make sure we get the best translations, we pick the one with the highest probability. Formal: We take the query, the answer, and the model's output d, which is like the info from the text, and we combine them using a matrix A. This A combines the info from the text (u) with the info from the knowledge base (B). Formal: We use this A to calculate the probability P(t|A) for the t-th translation t in the knowledge base. Formal: This probability is based on the input and the model's output, which we call the query q and the answer a. Formal: Also, there's this thing called \"knowledge span,\" which is like a tree structure with t child nodes, and each node has a bunch of possible translations, which we call v. Formal: To get the probability P(t|A) for the t-th translation t, we calculate it like this: Formal: In this process, the hidden state of the knowledge span is determined by the current query, the knowledge base, and the output of the decoder. Formal: Formal: We calculate the query vector P(t|A) like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.510.json"
    },
    {
        "casual_text": "Lately, there's been some cool progress in understanding procedural text, like the work by Gupta and Durrett (2019b), Du et al. (2019), and Das et al. (2019). But, the current methods don't really handle the relationships between entities, actions, and locations in a structured way. Instead, most of them rely on basic rules about how entities change or use extra knowledge to make guesses. For instance, Gupta and Durrett (2019b) came up with a fancy neural network that keeps track of each entity's state and uses a CRF model to sum up how everything changes overall. They also added some common-sense rules to their model to help it out.\n\nWhile Das et al. (2019) did look at how entities and locations connect, there isn't a standard way to deal with these relationships yet. Plus, some key connections, like how entities interact with actions or with each other, are totally overlooked.",
        "formal_text": "Recent works on procedural text understanding have achieved remarkable progress Gupta and Durrett, 2019b; Du et al. , 2019; Das et al. , 2019; Gupta and Durrett, 2019a). However, the existing methods do not systematically model the relations among entities, actions, and locations. Instead, most methods either leverage inherent constraints on entity states or exploit external knowledge to make predictions. For example, Gupta and Durrett (2019b) propose a structural neural network to track each entity's hidden state and summarize the global state transitions with a CRF model. inject commonsense knowledge into a neural model with soft and hard constraints. Although Das et al. (2019) model the relation between entities and locations, there is no general framework to model the relations, and some important relations, such as entityaction and entity-entity relations, are ignored.",
        "GPT2_formal_text": ", which is a matrix of size d_t^2, and the embedding layer is made up of L_i, L_1, and so on, up to L_T. The weight of the word embedding is given by W_e, and the vocabulary size is d_v. The size of the word embedding is also d_t^2, and the size of the embedding layer is L_i. The final hidden layer is the one in the last column of the Transformer. Formal: The algorithm works by minimizing the cross-entropy loss for a specific range, but it doesn't care what the input is. Formal: The whole thing is basically an endless loop where the loss grows in a straight line, eventually making the whole thing zero. Formal: The original paper was about translating from German to English. Formal: This paper also talked about translating from English to German. Formal: Both versions are different from the original work, though. Formal: There's an example of a parser that can handle both English to German and English to Russian. Formal: The paper also talked about translating from English to Russian. Formal: This paper also mentioned a parser that can handle both English to Russian and English to Russian. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.396.json"
    },
    {
        "casual_text": "(iii) Using dialogue control functions and looking at syntactic features like tri-grams (which include an auxiliary verb, the word right after it, and the one after that) as (DCF, SS2).",
        "formal_text": "(iii) dialogue control functions and syntactic feature (tri−grams-auxiliary verb, right neighbour1, right neighbour2) as (DCF, SS2).",
        "GPT2_formal_text": "\" => [], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], ], }, }, }, }, }, ], }, }, }, }, }, ], }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, }, },",
        "directory": "codi",
        "filename": "2020.codi-1.2.json"
    },
    {
        "casual_text": "Check out our multi-view response selection model in Figure 2. Basically, the context and response are turned into semantic embeddings in two different views. Both views share the same word embeddings, which helps them exchange info. In the utterance sequence view, we use a TCNN to handle the utterance embeddings. For the word sequence view and the utterance sequence view, we have two separate Gated Recurrent Units (GRUs)—one deals with word embeddings and the other with utterance embeddings. The word-level GRU focuses on word dependencies, while the utterance-level GRU looks at semantic and discourse stuff. We calculate confidence scores for picking the response in both views separately. Finally, we tweak the model by minimizing this loss:",
        "formal_text": "Our multi-view response selection model is designed as shown in Figure 2. As we can see, the context and response are jointly represented as semantic embeddings in these two views. The underlying word embeddings are shared across the context and response in these two views. The complementary information of these two views is exchanged via the shared word embeddings. The utterance embeddings are modeled through a TCNN in the utterance sequence view. Two independent Gated Recurrent Units are used to model the word embeddings and utterance embeddings separately on word sequence view and utterance sequence view, the former of which captures dependencies in word level and the latter captures utterance-level semantic and discourse information. Confidences for selecting the response in these two views are calculated separately. We optimize the multi-view model by minimizing the following loss:",
        "GPT2_formal_text": "ing_level = p_masking_level_1...p_masking_level-n.  \n\nAlso, if you want to learn how well you can predict the next word based on the attention, you can use self-attention_att. Formal: Alright, let's break this down in a simpler way.\n\nFirst, we have a matrix called M_d. It's a linear matrix with d rows and d columns. Each row in M_d represents the attention weight for a specific word or token. The 0 means we're ignoring it.\n\nThen, we have a vector called P_d. It's a vector with a size of d_p_d. It represents the predicted word or token based on the attention. The 0 here is just a placeholder, and d_p_d is a representation of the word or token itself.\n\nTo do this, we add up the attention weights for the tokens in M_d and P_d. Formal: If you want to check out the full process for learning word embeddings using this setup, you can check out Appendix C. Formal: Here's how we calculate attention weights for a specific token in a document. Formal: So, we're dealing with the token [CLS] and its document representation, which is d_p_d.\n\nNext, we have a set of weights, W_d. This is a matrix with d rows and d columns. It's a linear matrix with d rows and d columns. We're using this set of weights to figure out how likely it is for the token to be the next word in a document. Formal: Finally, we calculate the token prediction based on the set of weights W_d. Formal: You can find all the nitty-gritty details about how we calculate attention weights in Appendix C. Formal: Formal: Here's how we calculate attention weights for a specific token in a document. Formal: We're looking at the token [CLS] and its document representation, which is d_p_d. Formal: To keep things simple, we only focus on the top-k tokens. Formal: We've got a set of weights, W_d, which is a linear matrix with d rows and d columns. It's a linear matrix with d rows and d columns. Formal: Finally, we calculate the token prediction based on the set of weights W",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "Alright, here's the deal: the best settings for these parameters are: the longest question can have 15 words, the CNN filter sizes are 2 and 3, there's 1 shared CNN layer, 2 shared Bi-LSTM layers, a hidden dimension of 1000, 4 attention heads, and both image and object feature dimensions are set to 2048. The image level feature has 100 spatial locations, while the object level feature has 36 objects. The bilinear pooling rank is 3, there are 8 bilinear attention maps, the model trains for 100 epochs, and the starting learning rate is 0.002. The Adamax optimizer, which was introduced by Kingma and Ba in 2014, is used to tweak the weights during training.",
        "formal_text": "Optimal hyper-parameters are set to: maximum no. of words in a question=15, CNN filter size={2, 3}, # of shared CNN layers=1, # of shared Bi-LSTM layers=2, hidden dimension =1000, # of attention heads=4, image level and object level feature dimension =2048, # of spatial location in image level feature =100, # of objects in object level feature=36, # of rank in bi-linear pooling=3, # of bilinear attention maps=8, # of epochs=100, initial learning rate=0. 002. dataset. Adamax optimizer (Kingma and Ba, 2014) is used to optimize the weights during training.",
        "GPT2_formal_text": "= {r_q, r_s, ..., r_t}          Casual: The whole speech transcript, including the extra stuff like speech recognition stuff and the MP3 files, gets saved as a text file. You can find more info about this in the paper by Faruqui et al. (2010). Formal: We’ve put together a dataset of 250,000 audio clips, called C. It’s been set up to match up with the SpeechT5 format, which was created by Mikolov et al. (2013). Formal: The SpeechT5 dataset, which has 6.2 million speech samples, is a big deal for the research community. It’s one of the main sources of high-quality, multilingual speech transcripts. Formal: We also have a smaller, but more interesting dataset of 23,000 audio samples, called D. Formal: This smaller dataset is actually the speech transcript we used for our experiments. Formal: We picked the SpeechT5 dataset to create a more flexible benchmark for comparing different models in an open-source way. Formal: We use the MP3 audio files that come with the SpeechT5 dataset as our training data. Formal: When we fine-tune a model using these MP3s, we add a bit of extra audio by using the SpeechT5 corpus as a source for training. Formal: To train our models, we use the SpeechT5 corpus as the source and fine-tune them using the MP3s. Formal: We look at how they perform on the SpeechT5 corpus. Formal: In this part, we’ll talk about how we used the SpeechT5 corpus for our experiments. Formal: We’re focusing on the SpeechT5 corpus for the experiments because it’s a big, publicly available dataset with lots of examples. Formal: Our approach is pretty similar to some earlier work on multilingual NMT (like the one by Bahdanau et al., 2015). Formal: Formal: As a quick comparison, we’ve included results for the BERT-base model, which is the current top model, in Table 4. Formal: Formal: For the BERT-base model, the SpeechT5 corpus is the source and we fine-tune with the MP3s. Formal: Formal: Formal: Form",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "Here's the informal version:\n\n\"Figure 4 shows the simulation error rates for each condition (remember, higher is better here). Polyjuice-surprise has the highest error rate, which means showing these counterfactuals would give users the most extra info if they were displayed.\"",
        "formal_text": "Figure 4: Simulation error rates per condition (higher the better). Polyjuice-surprise has the highest error rate, indicating these counterfactuals would add the most information to users if displayed.",
        "GPT2_formal_text": "ed. Formal: Figuring out how different parts of the text relate to each other is super important for understanding text. The idea here is that if the text has a ton of different word choices, it’s likely the same concept shows up in different sentences. But if the text is just a bunch of random words, it’s hard to tell which concept is actually being talked about. To deal with this, we’re suggesting a method that uses something called attention. Basically, this method looks at the context around each word in the text and decides if it’s talking about the same thing. Formal: The idea here is that if the text has a ton of different word choices, it’s likely the same concept shows up in different sentences. But if the text is just a bunch of random words, it’s hard to tell which concept is actually being talked about. To handle this, we’re proposing a method that uses something called attention. Basically, this method looks at the context around each word in the text and decides if it’s talking about the same thing. Formal: So, the text you’re reading is made up of sentences and each sentence has a bunch of words that are linked to a specific concept. These words are like little pieces in the text that point to what’s happening in the concept’s details. Each of these pieces can be thought of as a \"pocket\" of info. This info is like a little guide that tells you where to look in the text to find the concept you’re looking for. Formal: We’ll call this info \"cognitive attributes\" for short, and we’ll talk more about it in the next section. Formal: In this paper, we’re mainly focusing on the two main things we mentioned earlier: how words connect to concepts and how they’re related to the words in the text. To do this, we’re treating the text like a puzzle where each part (like a \"pocket\") of the puzzle (the key) helps solve another part (the \"solutions\"). Formal: So, the text you’re reading is made up of sentences and each sentence has a bunch of words that are linked to a specific concept. These words are like little pieces in the text that point to what’s happening in the concept’s details. Each of these pieces can be thought of as",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "From Figure 1a, we can see that the block n-gram repeats algorithm takes up a whopping 25% of the generation time. To cut down on this cost, we came up with a new GPU-based kernel (check out Algorithm 1) that uses parallel computing to make things more efficient. Here's what it does:\n\n1. It avoids moving data between the GPU and CPU, which helps get around the bottleneck caused by the PCIe bus interface.\n2. It scans n-grams in parallel. Instead of going through tokens one by one to find repeated n-grams, it can do this all at once using threads equal to the number of n-grams generated up to time step t. Plus, each sample in a batch can be handled at the same time using multiple thread-blocks.\n3. It uses GPU shared memory for quicker access to data.",
        "formal_text": "As observed in Figure 1a, the cost of block n-gram repeats algorithm is as high as 25% of generation time. To reduce the cost, a new GPU-based kernel (see Algorithm 1) is developed to leverage the power of parallel compute and achieves the following benefits: 1) avoiding data movement between GPU and CPU to alleviate the throughput bottleneck of PCIe bus interface. 2) scanning n-grams in parallel. Instead of sequentially scanning tokens for detecting repeated n-grams, they can be scanned in parallel using threads equal to the number of n-grams generated till the time step t. Furthermore, each sample in a batch can be processed in parallel using multiple thread-blocks. 3) using GPU shared memory for faster memory access.",
        "GPT2_formal_text": ", f_ent, m_ent). Formal: A key part of the update process is making sure the model isn't too focused on the current event mention x. Formal: After looking at a bunch of documents, we gather the main points (like key points) and then figure out the main topics (like topics) for all the mentions in those documents. Formal: To get a sense of how well the model is doing, we check its performance on the development set. Formal: We take the normalized performance (ROI) and then compare it to the expected performance (EPR) of a regular supervised model. Formal: Check out Figure 1 for an example. Formal: We also use a few examples from the test set to make sure the model can handle new topics without getting confused. Formal: Formal: So, these examples show how we handle a new topic by using something called multi-document summarization. Formal: The final summary for this example looks like this: Formal: So, we're training a CNN to figure out the topic for the new event mention x. Formal: After the summary is done, we use a softmax function to calculate the probability p(z|x) for the whole group of possible event mentions in the summary. Formal: The results we got after using these summarization methods are in Table 1. Formal: Formal: The number of events is N, while the number of event mentions is M. Formal: Formal: Formal: Formal: The average ROI from our experiments is 39%. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-demo.26.json"
    },
    {
        "casual_text": "Alright, so here's the deal: You've got a question in English, Hindi, or even a mix of both, and there's an image that goes with it. The challenge is to use the image to figure out the right answer. Basically, you need to do some deep thinking about the picture to pick the correct response from all the options. In short:",
        "formal_text": "Problem Statement: Given a natural language question Q in English, Hindi or code-mixed and a correlative image I, the task is to perform a complex reasoning over the visual element of the image to provide an accurate natural language answerÂ from all the possible answers A. Mathematically:",
        "GPT2_formal_text": "_type nl = (nl + 1) - 1. . . nl ) wd = (1 - wi) * log(wi) * log(nl) Formal: The final embedding for a word w is defined as: Formal: Let’s say y = wi is the embedding for the word w. In real-world use, the embedding is also known as the \"initial word embedding.\" Formal: If the first word in a sequence, w1, is a noun phrase (NP), the second word w2, which is a verb phrase (VP), is also a noun phrase (NP). This means the embedding for the NP word in w1 and the VP word in w2 are both zp and zv, respectively. Formal: Formal: We’re assuming that the embedding for the word w is defined using the EM-formula (Eq. 3) for a specific input X. Formal: The softmax function we’re using to predict the embedding for a word is called the \"sentence embedding,\" which is basically the average of all the embedding vectors. Formal: In real-world use, we assume that the embedding for the word w is defined using the EM-formula (Eq. 3) for a specific input X. Formal: We’re assuming that the embedding for the word w is defined using the EM-formula (Eq. 3) for a specific input X. Formal: In real-world use, we assume that the embedding for the word w is defined using the EM-formula (Eq. 3) for a specific input X. Formal: The EM-formula works by taking the average of the embedding vectors, which gives us the EM-formula. Formal: So, the equation Eq. (3) is just the EM-formula for a specific input X, which we’ll call x. Formal: Here, N is the total number of words in the sequence. Formal: The embedding matrix for a sentence is defined as: Formal: Let’s say the embedding matrix for a sentence is defined as: Formal: A sentence can have up to n words, where each word has at least one embedding. Formal: If the embedding matrix for a word w is defined using the EM-formula (",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "The graph-enhanced bi-directional attention layer is designed to capture the intricate connections between sentences and relation instances. It creates a more refined representation of relation instances by combining information from within a sentence and across different sentences. This setup includes three main parts: the S2R layer, the GCN layer, and the R2S layer.",
        "formal_text": "The graph-enhanced bi-directional attention layer aims to model the complex interactions between sentences and relation instances, which generates refined representation of relation instance by synthesizing both intra-sentence and inter-sentence information. This component consists of the S2R layer, the GCN layer, and the R2S layer.",
        "GPT2_formal_text": "_m is a tuple of type_id, n, where n can be 0, 1, 2, or 3. Formal: We also show that there are four different setups that can use the types from the PBF. These setups are: Formal: We make sure the types are passed down properly, following the approach in Section 4. Formal: We use the types as input for the finite state automaton (FSA) to figure out the values for the different parts of the function. Formal: We use a list of types to set the parameters for the finite state automaton. Formal: We use type variables for the values of the type variables. Formal: We use the type variable to set the starting point for the type constraints, which can be anything from 0 to n. Formal: We also use some other features that make the type constraints (check out Section 4 for more on that). Formal: We use different weighting functions to balance the different constraints, which you can see in Equations (5) and (6). Formal: We tweak the type constraints to make sure they apply to the input data. Formal: We build the type constraints using a list of type variables, like T, and these variables are used to set up the input for the FSA. Formal: We also use the value of the type variable to set the initial value for the type constraints, which you can see in Equations (7) and (8). Formal: We use the input for the FSA to check if the output is valid. Formal: We also use another list of type variables for the data to keep track of the values, and these variables are used to set up the output. Formal: We use the output as input to check if the output is valid. Formal: Finally, we use the input for the FSA to figure out the values for the different parts of the function. Formal: For our experiments, we use the BNNJM (from Mikolov et al., 2013b) for training. Formal: We also make sure that the input is valid by using the output to check if the output is valid. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.136.json"
    },
    {
        "casual_text": "We're using the imSitu dataset to create our image-to-verb model. This dataset is pretty cool—it has a bunch of images, each showing one of 504 different verbs. The images cover all sorts of semantic roles, as described by Yatskar et al. in 2016. Check out Figure 4 for some examples from the dataset. We've split the data into different parts: 379 classes for training, 29 for validation, and 96 for testing.",
        "formal_text": "Zero-shot with the imSitu dataset We build our image-to-verb model on the newly introduced imSitu dataset, which contains a diverse collection of images depicting one of 504 verbs. The images represent a variety of different semantic role labels (Yatskar et al. , 2016). Figure 4 shows examples from the dataset. We apply our attribute split to the dataset and are left with 379 training classes, 29 validation classes, and 96 test classes.",
        "GPT2_formal_text": ". But we don’t want to keep adding more layers until we’re sure we’ve got everything right. In our system, the weights for the attention and position embeddings are learned in stages, and after each stage, we plug these weights into the final classifier. This way, the final classifier gives us the final output. Formal: We also use a weighted decoding method for these outputs, which is explained in section 3.2.2. This method uses a multi-head attention thingy that picks up the relationships between tokens in a sequence and adjusts the attention weights based on those relationships. Formal: We’ve got a function f that takes an output y and a hidden vector h into account. The first part of f gives the hidden vector h the weight of the token that’s closest to the input y. The second part gives the hidden vector h the weight of the token that’s farthest from the input y. Formal: To break it down, for a specific token y, we calculate the attention weight for that token, like this: Formal: The attention weight for a specific token y is calculated by using this function: Formal: We’ve also come up with a new loss function that takes into account the whole sequence, no matter how long it is. It’s calculated as: Formal: And that’s how we train the classifier. Formal: Here’s how our model works: Formal: We’ve got a bunch of training data for the specific token y and some outputs from a classifier. For each token, we calculate a loss function using these outputs. Formal: For the classifier, we have a bunch of input tokens and some outputs. We train a model using all of these inputs and outputs. Formal: Finally, we plug the weighted outputs from the classifier into the final classifier. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1099.json"
    },
    {
        "casual_text": "Okay, so, in addition to the usual basic rules and glue rules, we’ve got this swap rule that lets us switch around nearby chunks of words, but not whole phrases that are nested inside each other. Instead of using XH as the starting point for these rules, we went with XP. This choice lets us stick the results of the swap rule into bigger, hierarchical phrases, which you can’t do with the results of the regular hierarchical rules in a shallow grammar. To keep things balanced, we also added a rule that lets us combine stuff in a straightforward way. \n\nNow, we can put a limit on how many words these XP and XH chunks can cover. With that length limit in place, you can’t just keep applying the rules from Equation (4) to build super-long sub-derivations.",
        "formal_text": "supplementary to the standard shallow initial rules and glue rules. The swap rule allows adjacent lexical phrases to be transposed, but not hierarchical phrases. Here, we could as well have used XH as the left-hand side of the rules. As we chose XP and thus allow for embedding of subderivations resulting from applications of the swap rule into hierarchical phrases, which is not possible with sub-derivations resulting from applications of hierarchical rules in a shallow grammar, we also include the monotonic concatenation rule for symmetry reasons. A constraint can again be applied to the number of terminals spanned by both XP and XH. With a length constraint, building sub-derivations of arbitrary length by applying the rules from Equation (4) is impossible.",
        "GPT2_formal_text": ") = list(shape(m))      Casual: If the output isn't aligned, the read would be blocked. Formal: In the ACD task, we use attention to get the most important info from the input and then use some smart learning methods to figure out the best solutions for the model. Formal: ACD can be seen as a classification problem, where we have a set of labeled examples called a set C, which includes k classes (like noun, verb, etc.). This set is also called a domain, and it has a set of features, a class label, and a set of classes, X = {x1, ..., xn}. Formal: The ACD framework works by using a sequence of model outputs, M = {m1, ..., mk}, to represent the input. Each output is made up of a sequence of features, which are pairs of feature vectors, like (x1, ..., xm) = (v1, ..., vn). The model then uses these vectors to predict an output, y. Formal: If we have a model that can generate text with these features, it can be called a generative model (or generative-only model for short). Formal: So, if the model can generate text using the features, it can also generate text with the features. Formal: But, the generative model can't create text with the features if the output vector isn't aligned. Formal: So, if the model can generate text using the features, it can also generate text with the features. Formal: Formal: To make sure the generator can't generate text that's not aligned, we define it as a generative model that can generate text using the features, even if the output vector isn't aligned. Formal: Formal: Here, x is the generated text, x is the generated text, and y is the generated text. Formal: Formal: So, if the model can generate text using the features, it can also generate text with the features. Formal: Formal: Formal: We also need to consider how the model generates text, like how it generates words or phrases. Formal: Formal: Formal: Formal: Formal: So, if the generator can generate text with the features, it can also generate text with the features. Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2012.eamt-1.66.json"
    },
    {
        "casual_text": "Every entry has some text attached to it, which is usually the main part of the entry. We think of this text as the way to break down the noun phrase. If the noun phrase has more than one word, those words will show up in the text on their own or as part of other noun phrases.",
        "formal_text": "Each entry is associated with text, which is usually the main text of the entry. 2 We assume the text as the key to segmenting the noun phrase. If the noun phrase in question consists of more than one word, its constituents would appear in the text either freely or as part of other noun phrases.",
        "GPT2_formal_text": "ed_list, f) = [s, t] y_i = [w_i, t] p = δ(Σ Φ(y_i)) p_i = Σ Φ(y_i) p_i + α Σ Φ(y_i) p_i. Formal: In the baseline setup, we also use an attention module. The attention score for the mask is calculated based on the attention weight vector p_i. Formal: To get the attention weight vector p_i, we first calculate the attention score for each token y_i. Then, we combine the attention scores from the mask and the token-level attention. The combined attention score for a token y_i is calculated by multiplying the attention scores of all its tokens. Formal: To get the attention weight vector p_i, we use a method similar to the one shown in equation (7). Formal: We train this attention module using three different data sets: the original dataset D_s, a new dataset D_t, and a mix of the two datasets D_s + D_t. Formal: Formal: Figure 1 shows how the feedback sampling model works. Formal: After training the model for a set number of epochs, the learned model is pretty much set. Formal: The model uses a series of attention updates to focus on the most important word-level stuff, and the feedback it generates is a mix of these. Formal: In the end, the learned model generates a feedback tweet. Formal: We also put the model into production mode to see how it handles different types of feedback. Formal: We train the model using different datasets D_s, D_t, and D_s + D_t. Formal: After training for a set number of epochs, the learned model is pretty much set. Formal: In the final model, we blend the feedback words with the token-level attention to create feedback tokens. Formal: The model then uses a process of iterative back-propagation to refine its feedback. Formal: Figure 1 also shows the output of the model. Formal: We tested this model on a new dataset D_s + D_t. Formal: We trained the model using two datasets D_s and D_t. Formal: We tested the model on a new dataset D_s + D_t. Formal: Formal",
        "directory": "D11",
        "filename": "D11-1056.json"
    },
    {
        "casual_text": "There are a few ways to handle domain generalization. One approach is to train a separate model for each in-domain dataset. Then, when you're testing on out-of-domain data, you pick the in-domain dataset that’s most similar and use its model for predictions. Other methods, like the ones by Ghifary et al. (2015) and Muandet et al. (2013), focus on training a model to learn features that stay the same across different domains. They do this using stuff like multi-view autoencoders and mean map embedding techniques.",
        "formal_text": "Several methods for domain generalization exist. One of them is to train a model for each indomain dataset. When testing on out-of-domain, select the most correlated in-domain dataset and use that model for inference. Other works such as (Ghifary et al. , 2015; Muandet et al. , 2013), model is trained to learn a domain-invariant feature by using multi-view autoencoders and mean map embedding-based techniques.",
        "GPT2_formal_text": "\" (like \"c_t\") in the set T). We also use the similarity between the question and answer sentences to calculate the similarity score between the questions and the answers. Formal: The connection between the question and the answer sentence can vary a lot depending on how the question is structured. A good question structure would include the main idea and at least two of the three parts. In this project, we focus on the main idea and only the three parts. Formal: In Table 3, you'll find a comparison of the four different question structures. The \"Text\" column shows how many words in the question and the answer sentence are the same. The \"QA\" column shows how many words are in the question and the answer sentence. The \"Text+QA\" column shows how many words are in both the question and the answer sentence. Formal: We measure the similarity using the cosine similarity between the word embeddings of the question and the answer sentence. This similarity is calculated using a method called Levenshtein edit distance. The embeddings are created using BERT (Devlin et al., 2019). Formal: The similarity between the question and the answer sentence is calculated by looking at the distance between the embeddings of the words in the question and the answer sentence, and then taking the cosine similarity between these embeddings. Formal: The scores for these four question structures are averaged over all the training questions and answers to get a final score. Formal: The highest similarity score is 0.999, and the lowest is -0.0006. Formal: The model accuracy and perplexity for the whole dataset are listed in Table 4. Formal: The model accuracy and perplexity are averages across all the training questions and answers. Formal: The model accuracy and perplexity for the whole dataset are listed in Table 5. Formal: The model accuracy and perplexity for the whole dataset are listed in Table 6. Formal: The model accuracy and perplexity for the whole dataset are listed in Table 7. Formal: The model accuracy and perplexity for the whole dataset are listed in Table 8. Formal: The model accuracy and perplexity for the whole dataset are listed in Table 9. Formal: The model accuracy and perplexity for the whole dataset are listed in Table 10. Formal: The model accuracy and perplexity for the whole dataset are listed in Table 11. Formal: The model accuracy and perplexity for the whole dataset are",
        "directory": "D19",
        "filename": "D19-5826.json"
    },
    {
        "casual_text": "CNBC is gonna create and make these programs, and the folks from U.S. News are helping out with background info and research.",
        "formal_text": "The programs will be written and produced by CNBC, with background and research provided by staff from U. S. News",
        "GPT2_formal_text": "ed_attributes.json. Formal: The model can handle natural language questions in a straightforward way, so it can answer questions that are the same in both languages. Formal: The model can also answer questions that are different in both languages. Formal: It uses a Transformer encoder to process the questions and the answer keyphrases, then uses a multi-head attention system to process the answer keyphrases. Formal: In this part, we’ll first explain what kind of info is in the question and keyphrase, and then we’ll show how the model uses these features to answer the question in both languages. Formal: Lastly, we’ll dive into how the model picks the right answer keyphrases. Formal: The task here is to look up a question in a database, find the answer keyphrase in the text, and connect the two through a conversation. Formal: The approach we’re proposing is different from other methods because it’s focused on understanding the meaning of questions and keysphrases, not just how they’re put together. Formal: But the goal of our approach is to use all the information from both the question and keyphrase together, not just one at a time. Formal: To figure out the answer keyphrases for a question, you start by using a tool called a graph query, like the one shown in Figure 1, to gather information about the question. Formal: You can use these keyphrases as input for a model that generates text. Formal: The model gets an answer keyphrase based on the question and keyphrase, and then it uses a multi-head attention system to connect the answer keyphrase to the question. Formal: We’ll call this model a question generation model. Formal: We’ll call the output we get the question representation. Formal: To generate the answer keyphrases, you can either use the same multi-head attention system that was used to generate the question keyphrases or use different versions of the model that generate multiple keyphrases. Formal: For this example, the model is using a Transformer encoder to process the question and answer keyphrases, which has been fine-tuned to generate text using the same attention mechanism. Formal: Lastly, we’ll explain how the model learns the answer keyphrases and the answer",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "From the results we got, it's clear that our segmentation algorithm works way better on Choi's text collection compared to what was reported before (like Choi, 2000; Choi et al., 2001; Utiyama and Isahara, 2001). The time it takes to run our algorithm is about the same as other methods (basically O(T^2), where T is the number of sentences). Plus, our algorithm has a cool feature: it automatically figures out the best number of segments without needing us to decide it manually.",
        "formal_text": "From all the results obtained, we can conclude that our segmentation algorithm on Choi's text collection achieves significantly better results than the ones previously reported (Choi, 2000; Choi et al. , 2001; Utiyama and Isahara, 2001 ). The computational complexity of our algorithm is comparable to that of the other methods (namely 0 (1 2 ) where T is the number of sentences) 4. Finally, our algorithm has the advantage of automatically determining the optimal number of segments.",
        "GPT2_formal_text": "= {q_t, q_t+1, ..., q_t+n} y_i = {y_i_t, y_i_t+1, ..., y_i_t+n}. Formal: (a) To handle the non-continuous distribution, we use a log-linear function called f. Formal: (b) We take the log of the probability P(y_t, q_t | q_t+1, ..., q_t+n) for each topic q_t, which we set to 1. Formal: (c) We also have a binary variable called y_i, which is 1 if the question t_i is relevant to the topic q_t, 0 otherwise. Formal: (d) Formal: (e) Formal: (f) Formal: (g) We figure out the probability P(y_t, q_t | q_t+1, ..., q_t+n) for each topic q_t, which we set to 1. Formal: (h) Formal: Formal: Formal: (i) We also calculate the log of the probability P(y_t, q_t | q_t+1, ..., q_t+n) for each thread t_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E03",
        "filename": "E03-1058.json"
    },
    {
        "casual_text": "In this paper, we use some efficient algorithms for figuring out Probabilistic Context Free Grammar (PCFG) from a huge amount of speech transcripts—like millions of words. We show that you can actually learn grammar just from words and it works well, no matter how you start. To make sure our results are solid, we use two different algorithms for Variational Bayesian PCFG inference and tweak two algorithms that were originally made for Latent Dirichlet Allocation (LDA) topic models. The cool part is that all three algorithms handle big datasets and get better over time, ending up with similar accuracy in prediction and parsing.",
        "formal_text": "In this paper, we use scalable algorithms for Probabilistic Context Free Grammar (PCFG) inference to perform grammar induction from millions of words of speech transcripts, and show that grammar induction from words alone is both feasible and insensitive to initialization. To ensure the robustness of our results, we use two algorithms for Variational Bayesian PCFG inference, and adapt two algorithms that have been proposed for Latent Dirichlet Allocation (LDA) topic models. Most importantly, we find that the three algorithms that scale to large datasets improve steadily over training to about the same predictive probability and parsing performance.",
        "GPT2_formal_text": "ed_modifier = input_modifier + self.attn_mask_len_modifier\n          Casual: So, for the cross-modal attention, we start by using the input vector X_i as the input for the cross-modal attention layer. Then, the cross-modal attention layer uses this input to figure out the attention weights for different aspect-related modalities. Formal: We'll use the cross-modal attention layer for the cross-modal attention (like we did in Figure 2) and the aspect-related attention layer for the aspect-related attention (like in Figure 3). Formal: The cross-modal attention layer's parameters, δ_f and δ_b, are calculated using these two embeddings. Formal: Lastly, the aspect-related attention layer's parameters, δ_a and δ_b, are calculated using the cross-modal attention layers. Formal: Alright, let's dive into the specifics of the cross-modal attention (X_i) and the aspect-related attention (X_3). Formal: The cross-modal attention layer has three parts: the first two are based on a combined representation of the input and the aspect, the third part is based on the input and the aspect-modality, and the last part is based on the input and the aspect-modality-modality. Formal: Here, τ and γ are the learned attention parameters, which are learned as an average, and δ_a and δ_b are the attention weights, which are learned as a mean, as you can see in Figure 3. Formal: The cross-modal attention layer performs really well. Formal: Check out Figure 4 to see how the cross-modal attention layer works. Formal: Formal: Formal: We also tried out two versions of the cross-modal attention. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1003.json"
    },
    {
        "casual_text": "Tax diya gaya hai match ka.",
        "formal_text": "maetʃ kā kər diyā gəyā hai match of tax give-past has been 'Tax has been given/paid for the match'",
        "GPT2_formal_text": "= (i, j) for i from 1 to K and j from 1 to K, where i and j are the output embeddings of the word and entity. The whole thing is then turned into a single, fixed-size vector using a linear projection. Formal: Here, π is a non-linear activation function, and p_z is the total number of words in the prediction. Formal: In the context of ICD coding, the vector p_z is the predicted word embedding. Formal: The words in the word embedding x are the words from the data distribution d_w, and the embedding z is the actual text embedding. Formal: The output vector p_z is trained by the bi-directional GRU, which uses something called a beam search to get better at its job. Formal: Each word embedding y_t is calculated like this: Formal: The first word embedding y_t is made up of the input x_t, the token embedding z_t, and a bias term π(t_i) = s_i + b_i. Formal: The second word embedding y_t is made up of the output of the GRU's bi-directional GRU, the token embedding z_t, and a bias term π(t_i) = s_i + b_i. Formal: The third word embedding y_t is calculated as the combination of the output of the GRU's bi-directional GRU, the token embedding z_t, and a bias term π(t_i) = s_i + b_i. Formal: The fourth word embedding y_t is calculated as the combination of the output of the GRU's bi-directional GRU, the output from the GRU's CRF (specifically, the sum of the results from the CRF's output layer), and a bias term π(t_i) = s_i + b_i. Formal: The final word embedding y_t is then combined with the GRU's bi-directional GRU to give us the final word embedding y_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C12",
        "filename": "C12-1152.json"
    },
    {
        "casual_text": "When we're looking at how well a topic model is doing, we usually try to figure out how connected the words are within a topic. Once we have a way to measure how related two words are, we can define topic coherence as the average of these measurements for all the word pairs in the most likely words for that topic. Newman and his team (2010) used Pointwise Mutual Information (PMI) for this, based on how often words appear together in external text collections. Later, Mimno and his colleagues (2011) found that a tweaked version of PMI worked better when compared to what experts thought. AlSumait and his team (2009) figured out which topics were useless by seeing how different they were from the overall word distribution in the text. Fang and his team (2016a) decided to use the cosine similarity between word vectors to measure topic coherence. We liked this last method because it can be used in many different situations, so we used it to check how well tweet groups fit together. We looked at GloVe (Pennington and his team, 2014) and BERTweet (Nguyen and his team, 2020) word vectors, which were made using big Twitter text collections. To make things better and less jumpy, we followed Lau and Baldwin's (2016) idea of averaging the coherence scores for different numbers of top words (like 5, 10, 15, and 20).",
        "formal_text": "Measures of topic model coherence: The most common approach to evaluating topic model coherence is to identify the latent connection between topic words representing the topic. Once a function between two words is established, topic coherence can be defined as the (average) sum of the function values over all word pairs in the set of most probable words. Newman et al. (2010) use Pointwise Mutual Information (PMI) as the function of choice, employing co-occurrence statistics derived from external corpora. Mimno et al. (2011) subsequently showed that a modified version of PMI correlates better with expert annotators. AlSumait et al. (2009) identified junk topics by measuring the distance between topic distribution and corpus-wide distribution of words. Fang et al. (2016a) model topic coherence by setting the distance between two topic words to be the cosine similarity of their respective embedded vectors. Due to its generalisability potential we follow this latter approach to topic coherence to measure thematic coherence in tweet clusters. We consider GloVe (Pennington et al. , 2014) and BERTweet (Nguyen et al. , 2020) embeddings, derived from language models pretrained on large external Twitter corpora. To improve performance and reduce sensitivity to noise, we followed the work of Lau and Baldwin (2016), who consider the mean topic coherence over several topic cardinalities |W | ∈ {5, 10, 15, 20}.",
        "GPT2_formal_text": ", value, and key info.      Casual: Alright, so here's how we set up the model: we train it using the ILP loss function (which is the gradient of the log-likelihood), the Cross Entropy loss (which is the entropy of the cross-entropy function), and the logarithm function. This gives us a probability distribution for the attention mask. Formal: To figure out the logarithm for the cross-entropy loss, we use the empirical distribution of the log-likelihood across the training data. Formal: For the cross-entropy loss, we use the empirical distribution of the cross-entropy function. Formal: Finally, for the logarithm function, we use the empirical distribution of the logarithm across the training data. Formal: We optimize this setup using stochastic gradient ascent, following the method from Elman (2002). Formal: After each training round, we check how much each token contributes to the total contribution for the next round. Formal: Next, we take the token that's closest to the one before it in the list and add it to the contribution list, while leaving the other tokens the same. Formal: If there's a tie, we just pick the token with the highest contribution. Formal: We also calculate a total contribution score for each token in the list. Formal: Finally, we calculate the contribution score for each token in the list based on the sum of their contributions. Formal: We also use some parameter α to adjust how much importance we give to the value of δ(α) and the key info, based on the multi-channel context model. Formal: We tested this method on the test set using the TAC 2008 dataset, which has 170k sentences with 9,153,676 tokens. Formal: The results are in Table 4. Formal: In this paper, we've tested the multi-channel context model against four other models that use different attention methods. Formal: We found that the multi-channel model is more effective than the others for three of these tasks: understanding sentences better, predicting the token distribution, and summarizing documents. Formal: We also showed how the multi-channel model can be used to predict key info for a specific document. Formal: Figure 1 shows the text and document contexts from the TAC 2008 dataset. Formal: We picked the best model based on how well",
        "directory": "acl",
        "filename": "2021.acl-long.530.json"
    },
    {
        "casual_text": "We use the gated model and train it just like Schick and Schütze did in 2018. We also stick to the same character n-gram lengths as FastText.",
        "formal_text": "We make use of the gated model, training it just like Schick and Schütze (2018). The same character n-gram lengths are used as for FastText.",
        "GPT2_formal_text": "= [src_id] for src_id in D_s * |src_id|)              Casual: So, the main idea here is that the way the model's outputs are generated needs to match the current output type. Basically, the model needs to pick the right output. Formal: If a sentence has two sentence embeddings, e_s and e_t, and the model outputs a document, then the output must also have two sentence embeddings, e_s and e_t. Formal: The output output embedding for a document D_s should be an embedding of the sentence embedding e_s. Formal: The output embedding for a document D_t should be an embedding of the sentence embedding e_t. Formal: Finally, the output embedding for a document D_s should be the combination of the sentence embedding e_s, the document embedding e_t, and the document embedding e_s. Formal: Formal: Basically, the outputs in a document are described by the embeddings of the sentences it produces and the document embedding e_s. Formal: The main goal of the model is to figure out the right output for a sentence s by comparing its output with the outputs of all the sentences in a document D. Formal: The output embedding for a document D_s is defined as the combination of the sentence embedding e_s, the document embedding e_t, and the document embedding e_s. Formal: Formal: The output embedding for a document D_t is defined as the combination of the sentence embedding e_t, the document embedding e_s, and the document embedding e_s. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-6104.json"
    },
    {
        "casual_text": "Constantly giving safe answers can make a chatbot really boring, so it's important to try and avoid that when creating the learning algorithms. To fix this, we need a more creative model that can handle both relevance and variety—basically, it should understand how to mix things up while still staying on topic when dealing with the way people actually talk.",
        "formal_text": "Frequent occurrences of safe responses can dramatically reduce the attractiveness of a chat agent, which therefore should be avoided to the best extent possible when designing the learning algorithms. The pathway to achieve this purpose is to seek a more expressive model with better capacity that can take relevance and diversity (or informativeness) into account simultaneously when modelling the underlying distribution of human conversations.",
        "GPT2_formal_text": "_n_t is the attention mask for the i-th input word, and the output from the final-attention layer is the character representation h_i. Formal: The figure shows the hierarchical attention network setup for the SRL task. Formal: In this setup, the entity and word representations are represented as vectors. The entity embedding is created using cosine similarity, and the word embedding is built from a word embedding. Both vectors are normalized to a common size, and the attention weights are learned together. Formal: The attention weight vector for entity and word embeddings. Formal: The final attention vector for the entity and word embeddings. Formal: The entity and word representations are calculated based on the input sequence x_i. Formal: The entity and word representations are combined to form the representation h_i, which is based on the entity and word embeddings. Formal: Formal: Figure 2 shows how to train the model with a task-specific loss, which is explained in Algorithm 1. Formal: Here’s the detailed version of the hyperparameters for the multi-attention model. Formal: The loss function for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The loss function for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The loss function for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The loss function for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The loss function for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The loss function for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The loss function for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The loss function for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The loss function for the multi-attention model. Formal: The model parameters for the multi-attention model. Formal: The loss",
        "directory": "D17",
        "filename": "D17-1065.json"
    },
    {
        "casual_text": "Let's break down what MACHAMP can do. For regular tasks where you need to predict things at the token level, like part-of-speech tagging, MACHAMP uses a straightforward approach. It applies greedy decoding and pairs it with a softmax output layer, all based on the contextual embeddings it generates.",
        "formal_text": "We here describe the tasks MACHAMP supports. SEQ For traditional token-level sequence prediction tasks, like part-of-speech tagging. MACHAMP uses greedy decoding with a softmax output layer on the output of the contextual embeddings.",
        "GPT2_formal_text": "= {s1, ..., sm}. It uses a few types of features: POS tags, word shape tokens, character n-grams, and character embeddings. Formal: Since the model is super complex, we add a few extra features: POS tags, word shape tokens, and character embeddings. We also pull out word segmentation labels to get word shape tokens, which helps us tell apart words from other characters. Formal: For the final round of training, we use the source set and a set of word shape tokens as the test set. The model learns from these examples and scores its outputs using ROUGE, which is a common evaluation metric. Formal: The final model we use is built using a single-layer GCN with a hidden layer size of 200, and it has 32 attention heads. Formal: The main downside of our model is that it struggles to recognize words that have multiple meanings. For instance, in the sentence \"Radiologists said the baby had pneumonia,\" the model predicts \"pneumonia\" as the correct answer, but the actual answer is \"pneumonia and the baby.\" Formal: We also have to deal with the issue of word ambiguity. For instance, in the sentence \"The baby had pneumonia and the doctor said the baby had pneumonia,\" the model could mistakenly say \"pneumonia\" as the answer, which would make the model decide \"pneumonia and the doctor\" as the correct answer. Formal: To fix this, we add a special token to help the model deal with the issue of word ambiguity. Formal: Lastly, since all the features we use are compositional, we also use a beam search to pick the best model. Formal: The goal here is to maximize the log probability of the output. Formal: We're using the Yelp Restaurant Review dataset, which is a big collection of Yelp reviews from 3,489 restaurant reviews. Formal: We use BERT as the encoder-decoder model and feed it the review text. Formal: We start by using the BARTbase model to turn the review text into text. Formal: For the final round of training, we train the model using the Yelp Restaurant Review dataset. Formal: The final model we use is a single-layer GCN with a hidden layer size of 200, and it has 32 attention heads. Formal: Formal: Formal: The next section will show that our model performs really well on a test set",
        "directory": "eacl",
        "filename": "2021.eacl-demos.22.json"
    },
    {
        "casual_text": "We use a partial block adjacency matrix to simplify calculations and cut down on noise when dealing with extreme multi-label classification. The tests we ran show that our approach can hold its own against the best methods out there.",
        "formal_text": "• We utilize a partial block adjacency matrix to reduce calculation and noise for extreme multi-label classification. The experimental results show that our method is competitive with the state-of-the-art method.",
        "GPT2_formal_text": "ed_characters.json\": { \"title\": \"Chen-Xiaognai (战纯句)\" }          Casual: Since our model relies on text, we want it to include all the important details from the original text. To do this, we take the entire input text and use a standard NER (Neural Network Recognition) model to generate the masked words. Formal: To create the mask tokens, we use a fancy sequence-to-sequence model with attention (Bahdanau et al., 2015). We use three types of tokens: word type tokens (e.g., [T, U, P]), character type tokens (e.g., [C, V, S]), and character special token tokens (e.g., [C, V, S]). Basically, the mask token is made up of these three tokens. Formal: We train our model with two different setups: one where we skip the character-type tokens and another where we use the character-type tokens for the mask token. Formal: We use our model to generate a bunch of masked words based on the input, with the final number of words being the average of the mask count. Formal: Check out Figure 2—it shows a model setup where we skip the character-type tokens, which is the main idea behind our model. Formal: We use our model to generate a bunch of masked words, with the final number of words being the average of the mask count. Formal: To make sure we can use the character-type tokens for the mask token, we add them to the input too. Formal: In Table 3, you can see how our model performs on the test set. Formal: We use our model to generate a bunch of masked words. Formal: The first column has the results for the type-based mask model, which is the non-zero version. Formal: The second column has the results for the character-based mask model, which is the non-zero version. Formal: Table 4 shows the results for the type-based mask model, which is the non-zero version. Formal: We use our model to generate a bunch of masked words. Formal: Table 5 shows the results for the character-based mask model, which is the non-zero version. Formal: We use our model to generate a bunch of masked words. Formal: Table 6 shows",
        "directory": "acl",
        "filename": "2020.acl-srw.4.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version of the text:\n\nMTP is similar to MTPC, MTS is similar to MTSC, and TT is similar to TTC. Making sure these were comparable was super important for this study—if they weren’t, any comparison of style or syntax would’ve been messed up. The corpora were put together carefully to make sure they were comparable. The design criteria included things like diatopic, diachronic, diasystematic, and domain stuff. All the translated texts are in British or American English as the source language and Spanish (from Spain) as the target language. Both the translated and non-translated corpora are about the same size.\n\nMTP is made up of biomedical translations done by professional translators—either in-house or freelancers working for certified companies in Europe. It’s a specialized reference corpus because it doesn’t include full documents, just bits and pieces from translation memory (TM) segments. The text types vary a lot, from research papers and clinical essays to textbooks, product descriptions, PILs, user guides, and instructions for surgical equipment.\n\nThe comparable non-translated biomedical Spanish corpus has a similar mix of text types and topics. It’s a bit of a mixed bag, including fragments and whole documents: TM segments from different sources than the ones used for MTP, a small diabetes-related corpus, and a custom virtual corpus that was put together to match MTP in terms of sub-domains, topics, level of specialization, and text types.",
        "formal_text": "• MTP is comparable to MTPC, MTS is comparable to MTSC and TT is comparable to TTC. Comparability was a crucial consideration for this study as otherwise any style or syntax comparison would have been compromised. Corpora were compiled in such a way that comparability was ensured. Design criteria comprise diatopic, diachronic, diasystematic and domain constraints. All translated texts have British or American English as the source language and peninsular Spanish as the target language. Both corpora of translated and nontranslated texts have roughly the same size. MTP is composed of biomedical translations performed by professional translators (in-house or freelancers working for certified translation companies in Europe). It is a specialised reference corpus as it does not contain whole documents, but fragments composed of the target language segments of translation memories (TMs). Text types range from research papers in journals to clinical essays, textbooks, product description and PILs, users' guides and instructions for surgical equipment. Its comparable corpus of non-translated biomedical Spanish includes a similar selection of text types and topics. It is a mixed corpus, as it contains fragments and whole documents: source language segments of TMs different from the ones used to compile the MTP, a small corpus of diabetes and an ad-hoc virtual corpus compiled to match MTP as regards sub-domains, topics, level of communicative specialisation and text types.",
        "GPT2_formal_text": "= [tgt]. Where tgt is a named entity (NE) and tgt_i,j is the combination of tgt_i and tgt_j. Formal: The combination of (tgt_i,tgt_j) is labeled as c_tgt_i,tgt_j. Formal: We use the BERT base model (from Devlin et al., 2019) to figure out the probability of the sequence tgt_i,tgt_j happening based on the input document. Then, we calculate the sum from i=0 to n of the probabilities for all the possible combinations of tgt_i,tgt_j. Formal: Formal: We use the BERT base model (from Devlin et al., 2019) to predict the probability of the sequence tgt_i,tgt_j happening based on the input document. Formal: In Table 2, we show the average accuracy (Acc) and the macro-F1 score for the top 3,000 most common words across both datasets. Formal: The table shows the average accuracy (Acc) and the macro-F1 score (CF) for the top 3,000 most common words across both datasets. Formal: The table shows the average accuracy (Acc) and the macro-F1 score (CF) for the top 3,000 most common words across both datasets. Formal:\n\nFigure 1 shows how the human evaluation of our model compares to the scores for the top-3,000 most common words from the test set. Formal: We take the model parameters and run it on the test set to see how it performs, which is the same as what was done in the human evaluation. Formal: We take the model parameters and run it on the test set to see how it performs, which is the same as what was done in the human evaluation. Formal: We also take the model parameters and run it on the test set to see how it performs, but we don’t do the human evaluation. Formal: We take the model parameters and run it on the test set to see how it performs, but we don’t do the human evaluation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "2008.amta-papers.5.json"
    },
    {
        "casual_text": "We take a close look at how the latest unsupervised NMT systems perform on a bunch of real and made-up translation tasks.",
        "formal_text": "• We thoroughly evaluate the performance of state-of-the-art unsupervised NMT in numerous real and artificial translation tasks.",
        "GPT2_formal_text": "* = softmax(s_s_t), ..., s_m_t)          Casual: The new set of labels D_new is made up of: Formal: For the zero pronoun, the new label is 0, but for all the other examples, it gets a probability of 1. Formal: In the zero pronoun, the new label is zp. Formal: Formal: The new label for the pronoun zp is 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2020.eamt-1.5.json"
    },
    {
        "casual_text": "Text-image matching is definitely the hardest part of our system. Even though we’re using a really advanced method to pair text with images, the results just aren’t great. To get a better idea of how well we could do if everything was perfect, we picked five topics for each language and manually matched the text and images ourselves. The results for these topics are in Tables 7 and 8. The tests show that if we had the perfect text-image matches, the quality of the summaries would improve a lot. This proves that visual info is super important for our system. In Figures 4 and 5, you can see an image along with the text descriptions we got using different methods. From this, we can say that the image captions really matter.",
        "formal_text": "Text-image matching is the toughest module for our framework. Although we use a state-of-the-art approach to match the text and images, the performance is far from satisfactory. To find a somewhat strong upper-bound of the task, we choose five topics for each language to manually label the text-image matching pairs. The MMS results on these topics are shown in Table 7 and Table 8. The experiments show that with the ground truth textimage matching result, the summary quality can be promoted to a considerable extent, which indicates visual information is crucial for MMS. An image and the corresponding texts obtained using different methods are given in Figure 4 an d Figure 5. We can conclude that the image caption",
        "GPT2_formal_text": "\") where the keys (v_k) and values (v_k) come from the same sentence, and the label of the idiom is written as label_idiom. Formal: In the example, when we look at the word \"the\", it's followed by a phrase like \"on the /site/on/the/site/of/the\", which matches the main idea of the idiom. This \"site\" part comes from the specific IDiom we're looking at, and the \"the\" part is just an extra word that doesn't really show the main idea. For more info, check out the appendix. Formal: For each piece of text, we grab the first two words that could be part of the idiom (in the first two positions). If we can't find any of these two words, we just skip that part. Formal: We think that, since we can't always predict exactly where the idiom will be placed, we should use this idea to figure out the next word. Formal: We call this whole sequence of the words we can't find in the idiom \"unmatched words\". Formal: To handle this, we use a bunch of rules that look at the whole sequence of words (check out section 3.4.1 for more details). Formal: After that, we use another set of rules to decide where to put the idiom in the sentence. Formal: Finally, we add a special rule that marks the end of the idiom, which is a bit different from how we handle normal sentences. Formal: Formal: For example, when we see \"the site on the /site/of/the site\", it matches the main idea of the idiom, but it doesn't fit the \"on\" part. So, we skip that part. Formal: Formal: The list of unmatched words in the first two positions is shown in Figure 1. Formal: The list of words we can't find in the first two positions is also shown in Figure 1. Formal: We're going to show a more detailed version of this in the appendix, so you can check that out too. Formal: In Figure 1, the word \"site\" is the first word in the first two positions. Formal: In Figure 2, the word \"the\" is the first word in the first two positions. Formal: In Figure 3, the word \"the\" is the first word in the",
        "directory": "D17",
        "filename": "D17-1114.json"
    },
    {
        "casual_text": "Got it! Let's break this down in simpler terms:\n\nWe need to tweak the main part of the L T definition. So, let’s say M is a tree with some extra features, like (O, D, d), and u is just any random node in that tree. Now, for any well-formed formulas (wffs)…",
        "formal_text": "SThis is worth spelling out in detail to do is alter the base clause of the L T definition. So, let M = (O, D, d) be a feature structure decorated tree, and u be any node in O. Then for all wffs",
        "GPT2_formal_text": "ed = [masked_tok_tok], label_masked = [masked_label], and output_masked = [masked_output]. Formal: We've got a gazetteer for the input data in Table 3. But here's the thing: we also have a gazetteer for the output data, which you can check out in Table 4. The gazetteer is just a list of all the words in the input that are part of the output label (l_0). Formal: We take the entire input, turn it into a fixed-length vector, and use it to calculate the output label. Formal: So, the final output label (l_0) for a vector in L_A is just the sum of the output vectors from each of the input words. Formal: If the label is 'U', the output is 'U', and if it's 'O', it's 'O'. Formal: The output label for a vector in L_A is a vector that includes the sum of the output vectors from all the words in the input. Formal: Formal: So, the final output label (l_0) for a vector in L_A is a vector that includes the sum of the output vectors from each of the input words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E93",
        "filename": "E93-1004.json"
    },
    {
        "casual_text": "Since the dictionary is pretty small and the system is straightforward, it’s not too hard to set up or keep running, and it’s super fast. That’s why JICST decided to use it for their huge database of over 400,000 citations each year, saving them money too. Thanks to some improvements, the system can now handle both Japanese and Western languages. We even got an award for this work—the Japan Association of Information and Documentation’s Prize of Learning in 1980—and we’ve got a patent for it (Japan Patent Kokai Sho 55 (1980) -102074).",
        "formal_text": "As the dlotionary is small enough and lo~Io is simple, implementation and meintenanoe are relatively easy and the speed ls very hlgho JICST adopted this system for its information file production and services of more than 400, 000 citations per ye~T and save cOStS° By the development of the techniques, p~ocessiug of Japanese has become to be able to cope with western lan6uages, We were awarded for the work The Prize of Learning of Japan Association of Information and Documentation in 1980, and have applied patent (Japan Patent Koks£ Shows 55 (1980) -102074). ~K002 016038(~2~ ~K00~ 000. ~01® ~KO0. ~ O028S2®d2@ ~l(005 00. 1281~03@ ~K000 0138~@01@ ~K007 000437~01@ ~K008 03~90@01® ~1(000 025003~02~",
        "GPT2_formal_text": "= {1, 2, ..., M} s_i = {s_i1, s_i2, ..., s_iL}. Formal: To get the attention vector (a_t), we can use this formula: Formal: The dot product is a matrix that shows the logit for two entities, x_i and y_i. Formal: A_t is a weight matrix that shows how important a mention (x_i) is compared to an entity (y_i). Formal: (a_t) = sum of (a_i-1)/a_i for all the mentions in the mention graph. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C82",
        "filename": "C82-2004.json"
    },
    {
        "casual_text": "Step 3: Cross-project split. Check out the middle part of Figure 4. Projects are randomly assigned to training, validation, and test sets. This is separate from the time segments and in-project splits we did in steps 1 and 2.",
        "formal_text": "Step 3: cross-project split. See Figure 4 middle part. Projects are assigned to training, validation, and test sets randomly, which is orthogonal to the time segments and in-project splits in step 1 and 2.",
        "GPT2_formal_text": "ing, and the other is the C set in the second layer. After that, we pick the best w_j for the current layer, which is y_j. Formal: If the next word w_j is in the current layer, we grab the (K + 1)-th word from the (K + 1)-th layer. If not, we just use the highest-ranked word. Formal: We also add two more layers to the model. The first layer starts with a special [CLS] token, which acts as the next word. The second layer looks at the top-k word candidates (those that are likely to be the next word), then combines them using a non-linear function. Formal: The model's internal structure is shown in Figure 2. Formal: We start by using the model's output to decide the next word. If it's not one of the chosen ones, we just use the highest-ranked word. Formal: This process happens for a while—the model tries to match the highest-ranked word with the previous word, but each time, it focuses on a different word from the previous layer. Formal: At the very end, we use a linear function to combine the word candidates. Formal: The whole thing is based on a dataset of 1.4 billion words from the CoNLL-2009 shared task. Formal: The total time it takes to train this model is O(l*|V), and it takes about 1 to 2 seconds per epoch. Formal: The model is trained using a beam size of 5 and a learning rate of 0.001. Formal: The best performance we got is on the CoNLL-2010 shared task dataset. Formal: Formal: We also tried two different ways of updating the model. Formal: We use the FastBERT model with a beam size of 6 and a learning rate of 0.001. Formal: We also tried two different methods to update the model. Formal: First, we use the FastBERT model with a beam size of 6 and a learning rate of 0.001. Formal: Second, we use the FastBERT model with a beam size of 6 and a learning rate of 0.001. Formal: Formal: Formal: The model's performance on the same dataset is shown in Figure 3. Formal: We tested the model on both the full dataset and the smaller one. Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.339.json"
    },
    {
        "casual_text": "A bunch of awesome people helped out with designing the task, getting the data ready, and setting up the software. We want to give a shoutout to Dotan Dvir from Hebrew University of Jerusalem for taking charge of the UCCA annotation work. Dan Flickinger at Stanford University did some fresh gold-standard annotations for around 1,000 WSJ strings, which are part of the EDS evaluation graphs from 2020. Sebastian Schuster from Stanford gave us tips on how to turn the old-school PTB and OntoNotes treebank annotations into Universal Dependencies, version 2.x, using more up-to-date tokenization methods. Anna Nedoluzhko and Jiří Mírovský from Charles University in Prague improved the PTG annotation for LPPS data by adding stuff that was missing, like coreference. Milan Straka from the same university shared an updated version of his UDPipe parser and helped train morpho-syntactic parsing models for Czech, English, and German (for the MRP companion trees). Jayeol Chun from Brandeis University was super helpful with converting Chinese AMR annotations, preparing the Chinese morpho-syntactic companion trees, and getting companion alignments ready for the English AMR graphs.",
        "formal_text": "Several colleagues have assisted in designing the task and preparing its data and software resources. We thank Dotan Dvir (Hebrew University of Jerusalem) for leading the annotation efforts on UCCA. Dan Flickinger (Stanford University) created fresh gold-standard annotations of some 1, 000 WSJ strings, which form part of the EDS evaluation graphs in 2020. Sebastian Schuster (Stanford University) advised on how to convert the goldstandard syntactic annotations from the venerable PTB and OntoNotes treebanks to Universal Dependencies, version 2. x, using 'modern' tokenization. Anna Nedoluzhko and Jiří Mírovský (Charles University in Prague) enhanced the PTG annotation of LPPS data with previously missing items, most notably coreference. Milan Straka (Charles University in Prague) made available an enhanced version of his UDPipe parser and assisted in training Czech, English, and German morpho-syntacic parsing models (for the MRP companion trees). Jayeol Chun (Brandeis University) provided invaluable assistance in conversion of the Chinese AMR annotations, preparation of the Chinese morpho-syntactic companion trees, and provisioning of companion alignments for the English AMR graphs.",
        "GPT2_formal_text": "`):\n       Casual: • 'All': Each document gets a label too. For instance, if you look at the questions in (6), you'll see that most of them are labeled as 'All'. Formal: The documents aren't grouped by category—they're all labeled as 'All'. Formal: But there are some questions that are grouped together in the training data. Formal: For instance, in (3), 'Her' is labeled as both 'Herself' and 'Her Campaign', while 'Donald' is tagged as 'Donald' and 'Trump'. Formal: The groupings aren't always straight-forward. For example, (5) is labeled as 'Barack' and 'Barack Obama', but (6) is labeled as 'Barack' and 'Donald'. Formal: Formal: The documents are organized in a specific way, with topics and questions on the left and the rest of the documents on the right. Formal: You can also use the IE dataset to train the model using the traditional method, but the training process is way faster. Formal: Formal: Lastly, there's a special dataset that's tailored for specific types of questions. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "conll",
        "filename": "2020.conll-shared.1.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way:\n\nWe compared our approach to a few different methods, which you can check out in Table 1. At the top of the table, you'll find the Lead baseline and Oracle. For CNN/DM, the Lead baseline is Lead-3, and for XSum, it's Lead-1.\n\nIn the middle part of Table 1, there's a basic transformer-based summarizer that takes \"sentence representation + position encoding\" as input. There are also two variations of this: one without positional encoding and another that only uses positional encoding.\n\nAt the bottom, you'll see Shuffling (Grenander et al., 2019), which is a recent method aimed at reducing the lead bias in summarization. There's also Learned-Mixin (Clark et al., 2019), a general debiasing method designed for NLP tasks when you know the type of bias in your training data and have a bias-only model. In our case, the bias is lead bias, and the bias-only model is a transformer trained with just positional encoding.",
        "formal_text": "Baselines: We compare our proposal with various baselines (see Table 1 ). The top section of Table 1 presents Lead baseline and Oracle. For CNN/DM, lead baseline refers to Lead-3 and for XSum, it refers to Lead-1. The middle section of Table 1 contains the basic transfomer-based summarizer accepting \"sentence representation + position encoding\" as input, and its two variants, one without positional encoding, while the other with only positional encoding as input. The bottom section contains Shuffling (Grenander et al. , 2019), which is a method proposed lately for summarization lead bias demoting, and Learned-Mixin (Clark et al. , 2019), which is a general debiasing method proposed to deal with NLP tasks when the type of data bias in the training set is known and bias-only model is available. In our case, the data bias is lead bias and the bias-only model is the transformer trained with only positional encoding as input.",
        "GPT2_formal_text": "and attention_pooling_mask are set to the same size. Also, the hyperparameters α, β, and γ are kept the same. The final hidden state vectors are passed through an MLP layer to get the final representation of the image. Formal: To make the semantic space more complete and wider, we add a function called Mean-Shift (M_z) to adjust the space to include more entities. We use the formula Mean-Shift (M_z) = K * cos(θ_t) for this. Formal: We also want the semantic space to include more features. So, we add a function called Count-Shift (C_z) to change the count of entities in the space. Formal: Lastly, we create a function called Seq2Seq, which is basically the sequence-to-sequence model, to help the network get better at its job. Formal: Formal: We train the model by optimizing it and getting its final state vector, which we call its output. Formal: Formal: Formal: We use the Yule2Seq loss (from Yin et al., 2015) to get the loss, and we set it to 1e-5. Formal: Formal: Formal: Formal: Finally, the final semantic space we get is calculated using this process: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-short.119.json"
    },
    {
        "casual_text": "Step 4: Get rid of sentences that are less than 10 words long, any duplicates, and ones that start and end with quotation marks.",
        "formal_text": "Step 4: prune sentences shorter than 10 words, duplicate sentences and those that begin and end with a quotation mark.",
        "GPT2_formal_text": ", label_ids, and their attention weights in a hierarchical attention layer (check out Figure 1a). We also noticed that if we can find the best attention weights for the whole sequence, the model performs way better than the other models in this task. Formal: We tested our attention-based model against two other models. One was a basic beam search model that used a fixed pre-trained model to generate feedback. The other one had an attention-based model that didn’t rely on a pre-trained model. Formal: Here’s how we did it: we randomly picked 100 random words from the input and made it into feedback with different attention weights, using the same pre-trained model that the feedback model was trained on. Formal: We ran the training for a set number of epochs and then used the results to train a more advanced model. Formal: For the extra model, we tested it with just one sentence instead of 100 and tried two different attention weights: one for the word embedding and one for the sentence embedding. Formal: To see how the model reacts to different attention weights, we also ran it for 100 epochs and used the results to train a different model. Formal: We used this sentence-level model to generate feedback for the input. Formal: For all the models, we added a linear layer after the pre-trained model. Formal: We also compared the results from using just one sentence versus the full set of feedback. Formal: We also compared the results from using just one sentence versus the full set of feedback. Formal: We picked the best model based on the final evaluation results, using the top results from the dev set of feedback. Formal: We included the performance from the dev set in our final evaluation because it gave us the best results. Formal: Table 2 shows the evaluation scores for the baseline and attention-based models. Formal: We’re also sharing the results from the attention-based model, which uses the beam model, plus the best results from the baseline model and the attention-based model, all combined in one table. Formal: Table 1 shows the evaluation scores for the baseline and attention-based models. Formal: Table 2 shows the evaluation scores for the baseline and attention-based models. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D15",
        "filename": "D15-1220.json"
    },
    {
        "casual_text": "Plus, our setup has some cool new features: The phrases people use to point things out come from conversations between two people, and they might mention something more than once.",
        "formal_text": "In addition, our setup has several novel aspects: The referring utterances are the result of interactive dialogue between two participants and include subsequent references.",
        "GPT2_formal_text": "ed: It's all about the image data and the attention that goes with it. Formal: We're creating a model that uses a single-layer BiLSTM for both the image and the text. When the text is processed, we add a layer of attention to it, so the word-level attention gets mixed with the word-level attention from the image. In the end, it's the combined hidden states of the text and the image that help create a summary representation. Formal: In this paper, we dive into a model that uses a BiLSTM network for both the image and the text. The goal is to predict the hidden vector for the text. Formal: We use a bi-LSTM model to predict the hidden vector for the text. For the image, we use a CNN to get the image-level representation, which is then fed into a bi-LSTM model. Formal: We trained a CNN using a bunch of labeled images and a single sentence, then trained a BiLSTM model using those labeled images and the word embedding vectors from the text. Formal: We also used the word embedding vectors from the text to train a BiLSTM model, which uses the image embedding vectors to predict the text vector. Formal: We trained a CNN with a bunch of labeled images and a single sentence, and we also used the word embedding vectors from the text to train a BiLSTM model. Formal: We used a CNN with a bunch of labeled images and a single sentence, and we also used the word embedding vectors from the text to train a BiLSTM model. Formal: We also trained a CNN with a bunch of labeled images and a single sentence, and we also used the word embedding vectors from the text to train a BiLSTM model. Formal: Lastly, we built a BiLSTM model with a CNN to predict the hidden vector for the text. Formal: We used a CNN with a bunch of labeled images and a single sentence, and we also used the word embedding vectors from the text to predict the word vector. Formal: We used a CNN with a bunch of labeled images and a single sentence, and we also used the word embedding vectors from the text to predict the word vector. Formal: We used a CNN with a bunch of labeled images and a single sentence, and we also used the word embedding vectors from the text to predict",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.353.json"
    },
    {
        "casual_text": "We can look at this connection in a few different ways. For every statement, can we figure out the right events it's talking about? That's what we call utterance-level alignment.",
        "formal_text": "We can evaluate such a correspondence in several ways. For each utterance, can we predict the correct events to which this utterance refers? This is the problem of utterance-level alignment.",
        "GPT2_formal_text": "= [i, j], where i and j are in R^n. The weight matrix W is then used to adjust the size of the local context vector to make it more like the model's response. Formal: The first layer of the memory network uses the output from the first convolutional layer. The network is trained to create a local context vector like this: Formal: The memory network has a few types of connections. The first one, called the network top-layer, takes the output from the first convolutional layer. This output is fed into a single-layer feed-forward network and then into a three-layer feed-forward network that processes the whole memory network. After this, a two-layer feed-forward network helps the model figure out the context. Formal: The second connection, the network bottom-layer, uses the output from the second convolutional layer to create a local context vector like this: Formal: Finally, the third connection is used to create the context vector again. Formal: Finally, we add the feature vector v_t, which is the last hidden state from the memory network's last layer. Formal: The network is trained to spit out the embedding for the local context. Formal: The graph network uses the output from the last convolutional layer of the memory network to create a local context vector like this: Formal: The graph network's final hidden state vector, h_t, is calculated like this: Formal: The graph network's final hidden state vector is calculated like this: Formal: Finally, the network is trained to spit out the embedding for the local context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1043.json"
    },
    {
        "casual_text": "One way to mix things up is by shuffling around phrases in a translated sentence. To get a rough idea of how many different ways you can rearrange these phrases, you can look at places where two-word sequences don’t match up. If a phrase is stuck between two of these mismatches, you can move it around however you want because doing so won’t mess up the matching ngrams, and that means it won’t lower the Bleu score.",
        "formal_text": "One way in which variation can be introduced is by permuting phrases within a hypothesis translation. A simple way of estimating a lower bound on the number of ways that phrases in a hypothesis translation can be reordered is to examine bigram mismatches. Phrases that are bracketed by these bigram mismatch sites can be freely permuted because reordering a hypothesis translation at these points will not reduce the number of matching ngrams and thus will not reduce the overall Bleu score.",
        "GPT2_formal_text": "_tokens = (src_tokens) for d in src_tokens: if not d: return (src_tokens) - 1 else: return (d) + (src_tokens)\n                     Casual: There’s no specific sequence labeling method. The model learns by looking at the input and the context around the input, without any supervision. This is different from supervised methods, which need to label a sequence to be able to predict the next token, like in dependency parsing (Klein and Manning, 2003). Formal: We’ve come up with a way to use knowledge distillation to improve how well we can predict tokens, especially for sentences with long tokens. Here’s what we’ve contributed: Formal: We show that knowledge distillation can make a big difference in how well a model can predict tokens and how much it costs to train. Formal: We’ve developed a solid method for learning hierarchical attention for sequences and showed how to measure the performance of this model. Formal: We’ve also introduced a new way to train a model that can learn to generate text. Formal: We’ve introduced a way to improve how well a model can predict tokens using hidden representations, which is a key step in making text generation more reliable. Formal: We’ve developed a method to learn a hierarchical attention network for sequence labeling, which can create really detailed text representations that work well for understanding sequence information. Formal: We’ve also created a training method that can learn text representations that are way more detailed than just the traditional way of doing sequence labeling. Formal: We’ve integrated a hierarchical attention network into a neural network model for text generation, and we’ve shown that this approach can significantly boost the model’s performance. Formal: We’ve introduced a way to train a model that can learn text representations that are way more detailed than just the traditional way of doing sequence labeling. Formal: We’ve also introduced a method to learn text representations that are way more detailed than just the traditional way of doing sequence labeling. Formal: We’ve integrated a hierarchical attention network into a neural network model for text generation, and we’ve shown that this approach can significantly boost the model’s performance. Formal: We’",
        "directory": "E06",
        "filename": "E06-1032.json"
    },
    {
        "casual_text": "When training neural networks, words are usually turned into \"word vectors,\" which are kind of like spread-out representations of words. These word vectors can either be made beforehand or created specifically for a certain task. Even if you use a pre-trained word vector model, which is also made with a neural network, the training process can be super quick. Some recent cool work shows that you can process over 100 billion tokens in just one day on a single computer (Mikolov et al., 2013c). \n\nAnother perk of using a pre-trained word vector model is that it’s super flexible. You can use it later for all sorts of different tasks. Plus, there’s no real difference between a pre-trained word vector model and one made specifically for a task. So, it makes sense to use them together. \n\nIn this paper, we suggest doing domain adaptation using big pre-trained word vector models instead of just raw text. Basically, we’re adapting from a big pre-trained word vector model into one that’s specific to the task at hand. This way, we can use massive general domain (GD) corpora without slowing things down too much. We can also tweak richer GD word representations to fit into in-domain (ID) training.",
        "formal_text": "In neural network training, words are represented as distributed representations, so-called \"word vectors\", which can be pre-trained or trained with a specific task in mind. Although a pre-trained word vector model is also learned with a neural network, the training can be very fast. Recent optimized work shows learning word vectors can process more than 100 billion tokens in one day on a single machine (Mikolov et al. , 2013c). Another advantage of a pre-trained word vector model is its flexibility, as it can be used later for different task-specific models. Furthermore, the pre-trained and the task-specific word vector models have no functional difference. Accordingly, we think it is very natural to use them together. In this paper, we propose to perform domain adaptation from the large pre-trained word vector models instead of the raw text, i. e. adapting from large pre-trained word vector into the task-specific one. In this approach, we can make use of huge GD corpora with little speed overhead. We can also adapt the richer GD word representations into ID training.",
        "GPT2_formal_text": "= { ... } ) - set the learning rate θ to 0.00001. - return the final representation of the node          Casual: In this part, we’ll explain the steps for our method. Formal: We’ve also got an example of a cross-modal graph showing what the context is for the one labeled \"a.\" Formal: In the second step, the cross-modal graph we made for the \"a\" node is combined with the \"b\" node, so now, the graph has two nodes. Formal: The third step means we’ve combined the nodes from the \"b\" node with the nodes in the \"a\" node. Formal: Finally, the final graph for the node \"a\" is created by combining the nodes from the \"b\" node with the nodes in the \"a\" node. Formal: A cross-modal graph, g, is just a bunch of nodes (or edges) that show how two things (or concepts) are related, and the connections between these nodes help show how they’re connected. Formal: In this graph, there are two types of nodes: 1) the nodes that describe how two things are related, and 2) the edges that connect the nodes. Formal: These nodes are labeled with labels (or \"tags\") to show how these relationships work. Formal: When creating the graph, the nodes in the graph can have more than one label, depending on how the relationships are structured. Formal: Finally, the final graph for the node \"a\" is created by combining the nodes from the \"b\" node with the nodes in the \"a\" node. Formal: Formal: Formal: We’ve used the open-source OpenSubtitles dataset (thanks to Titov et al., 2014) to train our model. Formal: The dataset is based on the movie-related news domain, which is a mix of articles from news agencies and movie reviews. Formal: We’ve also labeled the different relationships between the nodes in the graph. Formal: We’ve included the training, validation, and test sets for the MultiNLI dataset in Table 5. Formal: \n\nIn this example, the word \"air\" is paired with the word \"breathe\" in the first sentence and \"sniff\" in the second sentence. Formal: We",
        "directory": "C16",
        "filename": "C16-1131.json"
    },
    {
        "casual_text": "Sure! Here's the informal version:\n\nThe annotation files shared in the paper by Lee et al. (2021) were originally in Brat Annotation standoff format, which are the .ann files, along with their matching news articles in .txt format. These were converted into json format. Then, each sentence in the dataset was processed using the Stanford CoreNLP toolkit. This included breaking the text into sentences, splitting words, tagging parts of speech, identifying named entities, and doing dependency parsing to create dependency parse trees. \n\nFor the model input, we used a \"multichannel\" approach (as seen in \"1\" in Figure 5), which combines three parts. Let’s say W = w1, w2, ..., wn represents a sentence with n tokens, where wi is the i-th token.",
        "formal_text": "Input. The annotation files made public in (Lee et al. , 2021) were first converted from Brat Annotation standoff format (. ann files) along with their corresponding news articles (. txt files) to json format. Each sentence in the dataset was parsed using Stanford CoreNLP toolkit, including sentence splitting, tokenization, POS-tagging, NER-tagging, and dependency parsing to generate dependency parse trees. For input to the model, we adopt the \"multichannel\" strategy (shown in \"1\" in Figure 5 ) by concatenating three components listed below. Let W = w 1, w 2, . . . . . w n be a sentence of length n where w i is the i-th token:",
        "GPT2_formal_text": ". To make things simpler, we’ve got a single output layer, which is just a list of numbers with a length of 0.1. The numbers from the input layer are fed into this output layer and get turned into their respective values. After that, these values are sent to the last hidden layer, which has a length of 0.1. \n\nIn the example, we’ve got three input hidden layers. The first one gets the value 0.1, the second one gets 0.5, and the third one gets 0.75. The output vector for the first layer is 0.1, the one for the second is 0.5, and the one for the third is 0.75.\n\nTo keep things simple, we’re not mentioning the inner product or the dot product here. Formal: The input and output vectors from the first and third hidden layers are 0.1, 0.5, and 0.75, respectively. Formal: The outputs from the first and third hidden layers are 0.1, 0.5, and 0.75. Formal: This representation is calculated using a specific formula that we’ll explain later. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "econlp",
        "filename": "2021.econlp-1.10.json"
    },
    {
        "casual_text": "The backward search works by coming up with the N best guesses one after the other, and you don’t have to decide on N beforehand. It’s way less complicated than the forward search.",
        "formal_text": "The backward search generates the N best hypotheses sequentially and there is no need to preset N. The complexity of the backward search is significantly less than that of the forward search.",
        "GPT2_formal_text": "' and the attention weights. It was first used in the CNNDM dataset (Yang et al., 2017). In the latest version, it can also be used to calculate the cosine similarity between images. Formal: The part of equation (6) that’s in the target language (LM) is created by using a beam search method. We can tweak this method to maximize the objective function and use the first-order partial derivatives to find the final vector. Formal: Using these two methods, we can figure out the attention weights using equation (7). Formal: The output is the vector that’s part of the target language (LM) and the final output vector. Formal: We use beam search for optimizing the objective. For learning the attention weights, we use two methods:\n\n1. **Target-side attention (TSA)**: We start with the source-side embedding (src_s) and grab the attention weights from the target-side embedding (tp_s).\n2. **Target-side attention weight (TAW)**: Same idea but for the target-side embedding (tp_t). Formal: We also use the cross-entropy loss to optimize the objective. Formal: We’ve tried out some different ways to optimize the objective and the cross-entropy loss. We tested different combinations of these optimization techniques and also tried out different attention weights. Formal: We tested a few combinations of these optimization methods to see which one works best. Formal: For the attention weights, we tried out different combinations of these methods:\n\n1. **F-scores (from Yang et al., 2017)**: For the target-side attention weights, we set the first-order partial derivative to 0.5 and set the second-order partial derivative to 0.1.\n2. **Input-side attention weight (SSA)**: For the source-side attention weights, we set the first-order partial derivative to 0.1 and set the second-order partial derivative to 0.2. Formal: For the target-side attention weights, we set the first-order partial derivative to 0.5 and set the second-order partial derivative to 0.1. Formal: We also tried out different attention weights for the source-side embedding. Formal: We tested a few different combinations of these methods. Formal: Formal:\n\nTo find the output",
        "directory": "C94",
        "filename": "C94-1032.json"
    },
    {
        "casual_text": "Given how tricky Thai spaces can be for the SBD task, we’re suggesting a word-based labeling method. Instead of trying to classify spaces, we treat Thai SBD as a word labeling problem. Basically, we see spaces as regular words and label each word as either SB (Sentence Boundary) or nSB (non-Sentence Boundary). Figure 2 shows a comparison between the space-based classification method and our word-based labeling approach. \n\nFor reference, Figure 1 gives an example of Thai text where the first sentence has two spaces, but there’s no space at the end of the sentence, marked by <eol>. \"eol\" just means end-of-line.",
        "formal_text": "In view of this complexity of spaces in Thai in light of the SBD task, we propose a word-based labelling approach which regards Thai SBD as a word labelling problem instead of a space classification problem. The approach treats the space as a normal word and labels each word as SB or nSB (non-Sentence Boundary). Figure 2 illustrates the space-based classification approach versus the wordbased labelling approach. Figure 1. Example of a written Thai text in which there are two space characters within the first sentence, but there is no space character at the end of the sentence, i. e. , at highlighted <eol>\". \"eol\" refers to end-of-line.",
        "GPT2_formal_text": "= {d_t} 1.\n           Casual: In this paper, we'll dive into a single-task model that can predict the main object's attributes without needing any labels. The main idea is to pick a label to label a new example. Each example should have one or more words with attribute labels. Formal: We’ve defined two types of attribute labels: type-1 and type-2. The first one tells us what kind of thing is being talked about (attribute-1), and the second tells us which word or phrase is being talked about (attribute-2). Formal: To figure out which words or phrases are being talked about, we need to train a system that can handle both type-1 and type-2 labels. We’ve come up with a simple model called Attribute Attention. The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: The Attribute Attention module picks a word or phrase as the main attribute label, which gives us a new instance for training. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1031.json"
    },
    {
        "casual_text": "We adjust the final score so it always stays between 0 and 1. To handle nodes that don't match in both the alignment area and the alignment range, we tweak Equation (2) by dividing it by:",
        "formal_text": "We normalize the resulting score so that it always falls within the interval [0, 1]. To take into account unaligned nodes in both the alignment domain and the alignment range, we divide Equation (2) by:",
        "GPT2_formal_text": "(r) = {r} * argmax(D_k, h_1, h_2, ..., h_L) + argmax(D_k, h_1, h_2, ..., h_L) * argmax(D_k, h_1, h_2, ..., h_L) + argmax(D_k, h_1, h_2, ..., h_L) * argmax(D_k, h_1, h_2, ..., h_L) + argmax(D_k, h_1, h_2, ..., h_L) * argmax(D_k, h_1, h_2, ..., h_L). Formal: To check if the input is correct, we use a function that takes the argument and gives a probability score. This score helps us decide if the prediction is correct. For example, if we have a sentence like \"The dog barked\" with an argument like \"The dog barked\" and its context \"The dog barked\", the function would give this probability: Formal: For a new input x, we look at all the arguments and give them probabilities. This helps us decide if the new argument x fits the argument and the context. Formal: If the new argument is a proper noun phrase (NP), we add another probability to the set called P(D_k, h_1, h_2, ..., h_L) to make sure the new argument x fits the argument and the context. Formal: Finally, if the new argument is a proper verb phrase (VP), we add another probability to P(D_k, h_1, h_2, ..., h_L) to make sure the new argument x fits the verb phrase and the argument and context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D09",
        "filename": "D09-1002.json"
    },
    {
        "casual_text": "To handle labels that aren't perfect because of distant supervision, we’re stepping away from the usual sequence labeling approach and coming up with a fresh prediction model. Instead of trying to label each individual token, we’re focusing on figuring out if two tokens next to each other belong to the same entity (or if they’re “broken”). The main idea here is that, even if distant supervision messes up the boundaries of an entity, the connections inside the entity usually stay intact and are less affected by noise. So, we’ve created a new tagging system called “Tie or Break” to make better use of this noisy data. \n\nOur new model, called AutoNER, works by first finding all possible entity spans by looking for these ties, and then figuring out the type of each entity. We’ve built a cool neural network architecture to do this. In our tests, AutoNER performed better than the Fuzzy CRF model, so it seems to be working well.",
        "formal_text": "To adapt to imperfect labels generated by distant supervision, we go beyond the traditional sequence labeling framework and propose a new prediction model. Specifically, instead of predicting the label of each single token, we propose to predict whether two adjacent tokens are tied in the same entity mention or not (i. e. , broken). The key motivation is that, even the boundaries of an entity mention are mismatched by distant supervision, most of its inner ties are not affected, and thus more robust to noise. Therefore, we design a new Tie or Break tagging scheme to better exploit the noisy distant supervision. Accordingly, we design a novel neural architecture that first forms all possible entity spans by detecting such ties, then identifies the entity type for each span. The new scheme and neural architecture form our new model, AutoNER, which proves to work better than the Fuzzy CRF model in our experiments.",
        "GPT2_formal_text": "ed takes the output from the last-level BiLSTM and turns it into a hidden representation, h t. Formal: We start with a starting window size of K = 2 and let the input take up the rest of the input space. Formal: We pick the token with the lowest probability p t (w i, w i+1 ) and the one that’s most similar to the last-level BiLSTM. We do the same thing for each token in the input, but we’re looking at the last-level BiLSTM’s output instead of the current one. Formal: For the token with the lowest probability, we just pick its closest neighbor, f c k (w i, w i+1 ) from a uniform distribution p c k (w i, w i+1 ). Then, we average all the neighborhood vectors across all tokens to get the final representation, f t. Formal: Let’s say r is the probability distribution for the token w i, and c k (w i ) is the distribution for the neighbors of w i. Formal: If p t (w i, w i+1 ) is zero, we just stick to the last-level BiLSTM’s output and don’t update its parameters. Formal: After that, we average the neighborhood vectors across all tokens to get the final representation, f t. Formal: The final representation of the token w i, w i+1 is calculated like this: Formal: Let’s say r is the probability distribution for the token w i, and c k (w i ) is the distribution for the neighbors of w i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1230.json"
    },
    {
        "casual_text": "It's calculated by adding up the info you get from each item. If you have two things, X and Y, you can use the probability of their score differences to figure it out.",
        "formal_text": "is computed by accumulating the information gained from each item. Given two subjects X and Y, one can use the probability distribution of score differences",
        "GPT2_formal_text": "' for each query, and we’re using the matrix A in Equation (6) to represent the distributional vectors of the query. The main thing we’re focusing on here is how likely it is for the model to return a specific query s_i. The model’s chances of making the query s_i depend on how often it’s actually used in the data, which is based on how often different query types show up (like how often \"What do you mean?\" and \"What do you mean?\" appear together in the text). To keep things simple, we’re just assuming that the language model works the same across all languages, like a bilingual model. Formal: The uncertainty set for a query q_i, written as U_q_i, is the probability of a specific query p_q_i, written as p_q_i. Formal: We’re using a Gibbs sampling algorithm (from Gibbs, 1965) to figure out the uncertainty set for a query q_i. Formal: The probability of a specific query p_q_i, written as p_q_i, is calculated as: Formal: We’re using the CKY algorithm (from Firth, 1957) to find the best k-dimensional embeddings q_i from a uniform distribution. Formal: We’re using the pseudo-linear function f(•) to represent the probabilities of the embedding of the query q_i based on the embedding of the whole query. Formal: We’re using the Dirichlet distribution for the probability of a query q_i, written as q_i. Formal: We’re using a Gibbs sampler (from Gibbs, 1965) to find the best embeddings q_i from a uniform distribution. Formal: The log likelihood of a query q_i, written as q_i, is just the log of the probability of the embedding of the query q_i given the embedding of the whole query. Formal: Formal: For a query q_i, we calculate the log probability of embedding the query q_i, written as q_i, given the embedding of the whole query. Formal: Formal: Finally, we calculate the probability of embedding the query q_i given the embedding of the whole query. Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.346.json"
    },
    {
        "casual_text": "We took three of our test sets—Text+Berg, Twitter, and Combilex—and split them up based on the length of the input strings. Then, we tested two types of models, PCRF-Seq2Seq and encoder-decoder neural models, on these smaller groups of data. \n\nAs you can see in Figures 2 and 3, there’s a clear pattern: PCRF-Seq2Seq holds up pretty well no matter how long the input strings are. On the other hand, the encoder-decoder models start to struggle a lot more as the sequences get longer, especially the ones without an attention mechanism. \n\nFor shorter sequences, though, things look a bit different. Standard encoder-decoder models actually do a little better than both their attention-based versions and PCRF-Seq2Seq in tasks like Twitter spelling correction (check out Figure 3) and G2P conversion. But this isn’t the case when we look at the full datasets, where they don’t perform as well. \n\nOn the Text+Berg data, all the models are pretty much on par when it comes to short sequences, as shown in Figure 2.",
        "formal_text": "We divided three of our test sets (Text+Berg, Twitter, and Combilex) by input string lengths and evaluated PCRF-Seq2Seq and encoder-decoder neural models on these subsets of the test data. As illustrated in Figures 2 and 3, we observe a consistent tendency: PCRF-Seq2Seq performs relatively robustly over input strings of different lengths, while the performance of the encoder-decoder models plummets more drastically with sequences becoming longer, in particular those without attention-mechanism. For shorter sequences, we observe that standard encoder-decoder models even slightly outperform their attention-based counterparts as well as PCRF-Seq2Seq on both the Twitter spelling correction task (Figure 3 ) and on G2P conversion, in contrast to their rather low performance on the full datasets. On the Text+Berg data, all systems achieve approximately equal WAC for short sequences (Figure 2).",
        "GPT2_formal_text": "ing_1d = Convolutional_Attention(input, size = d_mask, nonlinearity = N). Then, we use this decoder to generate the response y_t. Formal: Here, L stands for the size of the response. The decoder is trained to make the response as long as possible. To make sure the response is relevant to the question, we add a linear layer. Formal: To keep things fair, we train the model with just one question-answer pair to make sure it gets better at generating responses. Formal: For each word in the input, we get a vector representation of its context, using a non-linear function. After that, we run it through a linear layer to get the attention distribution for the whole input. Formal: We also keep track of how much attention the decoder gives to different parts of the input, like the beginning and end of the word. Formal: Finally, we use an attention mechanism on the attention vector to create a vector representation for the context of the word. Formal: Formal: We’re using the trainable attention loss to figure out the attention distribution for the whole input. Formal: We train the model using a training set with a fixed size, let’s call it S_train. Formal: Once the model’s response is generated, we check if it’s relevant to the question and pick the best answer from the options. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1160.json"
    },
    {
        "casual_text": "The main difference between our method and the old ones is that we don’t rely on guessing or testing the results of the rule compilation by hand. Instead, we use a more solid approach: we create finite-state transducers to model phonological rules that mimic how loanwords are adapted, and then we automatically calculate how well these rules work and tweak them to improve their coverage. The cool thing about using a computer is that we can instantly see how accurate the system is during each step of developing and tweaking the rules, which would be almost impossible to do with old-school, manual methods.",
        "formal_text": "What is different between our approach and previous ones is that we do not test or intuit the result of the rule compilation manually; rather, we utilize a more rigorous method: composing finite-state transducers to model phonological rules for mimicking the loanword adaptation processes, and automatically calculating coverage and adjusting the rules to maximize coverage. The benefit of the computational approach is obvious: we can immediately obtain the accuracy of the system output during each round of the rule development and modification process, which is nearly impossible when resorting to paper-andpencil approaches.",
        "GPT2_formal_text": "= [word.split(',',') for word in word_list] t = input_list_t.split(',') for i in 1: t = t.split(',')                     Casual: Sure! Here's the informal version:\n\nInput: a_t = [Word.split(',',') for word in word_list] Formal: A_t = [Word.split(',',') for word in word_list] Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1081.json"
    },
    {
        "casual_text": "Based on the research by Yin and colleagues (Yin et al., 2018b), we're using a BiLSTM network along with a self-attention mechanism to process the text before and after the zero pronoun. The self-attention mechanism helps us figure out how much importance each word in the surrounding text should get. This way, we can create a more detailed and effective representation of the zero pronoun.",
        "formal_text": "According to the work of Yin et al (Yin et al. , 2018b), we use BiLSTM network and self-attention mechanism to encode the preceding and following text of the zero pronoun. The purpose of using the self-attention mechanism is to obtain the attention weight distribution of the preceding and following word sequence. In this way, we can get the more powerful zero pronoun representations.",
        "GPT2_formal_text": "is the input feature vector. Then, the softmax function is used to create a probability distribution across the words in the target vocabulary. Formal: In this part, we're going to tweak the input feature vector so it works better when we use it for the target-side decoder. Formal: The target-side decoder, f_t, looks at the source-side attention vector h_s as it's being generated. It decides whether to give attention to the tokens within the [CLS] token (which is 1 or 0) or the ones beyond it (which is 0 or 1). Formal: Since the source-side attention doesn't depend on the [CLS] token, it can use the knowledge stored in the hidden layer to adjust the attention. Formal: The target-side decoder also relies on the source-side attention, but it adds some special attention weights based on the target-side hidden state, called h_t, and the token embedding vector. Formal: The model gets a reward for predicting the correct tokens for the [CLS] token, h_s, and then calculates the cross-entropy loss. Formal: This cross-entropy loss is calculated using the cross-entropy function. Formal: You can see how this approach compares to the usual model in Figure 1. Formal: We've also included a simple example of a greedy attention-based decoding algorithm for text. Formal: Formal: This approach can be seen as a non-augmented version of the self-attention-based attention mechanism, which was introduced by Bahdanau et al. in 2015. Formal: To see how the dynamic attention mechanism affects the model's performance, we've set up a greedy attention-based decoder to work with this dynamic attention mechanism. Formal: In the second and third scenarios, the method is the same as in the first one. Formal: The last example shows that the dynamic attention mechanism can be used on its own for the target-side decoder. Formal: Formal: We're releasing our code and model, along with the model architecture, publicly to encourage research on creating better text-to-speech systems. Formal: We're sharing the dataset with the authors so they can check how the model performs on different datasets. Formal: Formal: We're also sharing the model setup and the model architecture with the authors so they can easily replicate it. Formal: Form",
        "directory": "ccl",
        "filename": "2020.ccl-1.77.json"
    },
    {
        "casual_text": "This feature hits its max value of 1 when all the content words are translated with a perfect probability of 1. The \"functional words translation strength\" feature is what we're talking about here. The idea is that if the functional words around content words are aligned correctly, they'll likely stay aligned in parallel sentences too. This is because, from a dependency-syntactic perspective, functional words (like prepositions, determiners, articles, particles, etc.) usually connect with or influence nearby content words.\n\nTo get the dictionaries mentioned in this section, we ran GIZA++ on the JRC Acquis corpus. For pairs of source and target words, if they're not in the dictionary, we use a 0 to 1 normalized Levenshtein distance to assign a \"translation probability\" based on how similar the strings are. If the source and target words are similar enough (we set the threshold at 0.7 through experiments), we consider them translations.\n\nMathematically, if 〈 〉 is the highest-scoring pair of aligned functional words near (within ±3 words) the aligned pair of content words 〈 〉, | | is the count of the best alignment found by ( ), and (〈 〉) is the probability of the functional word pair from the dictionary, then...",
        "formal_text": "This feature has a maximum value of 1 if all content words from are translated in with the maximum probability of 1. ( ) is the \"functional words translation strength\" feature. The intuition is that functional words around content words aligned as in ture ( ), will also align for parallel and because of the fact that, from a dependencysyntactic point of view, functional words (prepositions, determiners, articles, particles, etc. ) are 7 To obtain the dictionaries mentioned throughout this section, we have applied GIZA++ on the JRC Acquis corpus (http: //langtech. jrc. it/JRC-Acquis. html). 8 For two source and target words, if the pair is not in the dictionary, we use a 0 to 1 normalized version of the Levenshtein distance in order to assign a \"translation probability\" based on string similarity alone. If the source and target words are similar above a certain threshold (experimentally set to 0. 7), we consider them to be translations. usually governed by or govern nearby content words. Mathematically, if 〈 〉 is the highest scored pair of aligned functional words near (in a window of ±3 words) the aligned pair of content words 〈 〉 from, | | is the cardinal of the best alignment as found by ( ) and (〈 〉) is the probability of the functional word pair from the dictionary, then",
        "GPT2_formal_text": "of entity pairs in a pair. Formal: We're using the same method as in Equation 1. Formal: The updated path model is created using the path encoder, which is shown in Equation 6. Formal: The updated path model is trained using a self-supervised method, as explained in Equations 7 and 8. Formal: The updated path model is then used to predict the output embedding for the entity pair. Formal: The updated path model can also be trained using the same self-supervised method. Formal: Formal: Here, E_p_i represents the embedding for the entity pair, and P_e_i is the probability distribution for the entity pair based on the entity p. Formal: Lastly, we combine the outputs from the decoder and the entity pair using a weighted sum, which is shown in Equation 9. Formal: Formal: The final result of our updated path model is the updated embedding for the entity pair. Formal: Formal: Formal: We also use the ensemble method to create the updated embedding. Formal: Formal: We figure out the probability distribution for the entity pair by combining the predictions from the decoder and the entity pair using a weighted sum. Formal: Formal: Formal: The updated embedding for the entity pair can be found using Equations 10, 11, 12, 13. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2012.eamt-1.37.json"
    },
    {
        "casual_text": "Okay, so for APPLY(α, j), let's say i is the active node. Since we added an edge to j during the transition, W c + 1 is just the same as W c. Now, we'll break down O c again.",
        "formal_text": "APPLY(α, j) Let i be the active node. Since an edge to j was created in the transition, W c + 1 = W c. We decompose O c again:",
        "GPT2_formal_text": "= [ ] s_0 = [ ] for c in x_0: ..., x_n: s_0 = softmax(i_0 + e_0 * f_0) s_1 = [ ] for c in x_1: ..., x_n: s_1 = softmax(i_1 + e_1 * f_1) s_2 = [ ] for c in x_2: ..., x_n: s_2 = softmax(i_2 + e_2 * f_2) s_3 = [ ] for c in x_3: ..., x_n: s_3 = softmax(i_3 + e_3 * f_3) s_4 = [ ] for c in x_4: ..., x_n: s_4 = softmax(i_4 + e_4 * f_4) s_5 = [ ] for c in x_5: ..., x_n: s_5 = softmax(i_5 + e_5 * f_5) s_6 = [ ] for c in x_6: ..., x_n: s_6 = softmax(i_6 + e_6 * f_6) s_7 = [ ] for c in x_7: ..., x_n: s_7 = softmax(i_7 + e_7 * f_7) s_8 = [ ] for c in x_8: ..., x_n: s_8 = softmax(i_8 + e_8 * f_8) s_9 = [ ] for c in x_9: ..., x_n: s_9 = softmax(i_9 + e_9 * f_9) s_10 = [ ] for c in x_10: ..., x_n: s_10 = softmax(i_10 + e_10 * f_10) s_11 = [ ] for c in x_11: ..., x_n: s_11 = softmax(i_11 + e_11 * f_11) s_12 = [ ] for c in x_12: ..., x_n: s_12 = softmax(i_12 + e_12 * f_12) s_13 = [ ] for c in x_13: ...,",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "To get a better understanding of this, we got two bilingual folks to re-label 100 examples each in both English and French, following our usual validation process. We picked these examples from two separate random groups of development data, so the annotators didn’t see the original English text for any of the French translations they worked on. Without any special training or warm-up, these annotators matched the original English labels 85% of the time on the English data and 83% of the time on the French translations. This shows that the general meaning stayed pretty consistent between the two languages. Since most sentences are pretty straightforward to translate, especially the ones made by the workers, it doesn’t seem like the translation process added much confusion.",
        "formal_text": "To quantify this observation, we recruit two bilingual annotators to re-annotate 100 examples each in both English and French following our standard validation procedure. The examples are drawn from two non-overlapping random subsets of the development data to prevent the annotators from seeing the source English text for any translated text they annotate. With no training or burn-in period, these annotators recover the English consensus label 85% of the time on the original English data and 83% of the time on the translated French, suggesting that the overall semantic relationship between the two languages has been preserved. As most sentences are relatively easy to translate, in particular the hypotheses generated by the workers, there seems to be little ambiguity added by the translator.",
        "GPT2_formal_text": "_t = (id, v) in the responses. We use a bunch of different values for this seq2seq model, including one for the words in the query. Formal: Some earlier studies (like Barzilay and Lapata, 2008) tried to generate responses using different language models. For example, Kim (2009) used a two-layer LSTM to do this. They found that using a multiple-choice model for dialog acts made the responses more meaningful and coherent. However, they didn’t use any structure-sharing techniques. Formal: Another method is to use a multi-head attention model, which combines word embeddings (MWE) and hidden state embeddings (HWE), to create responses. This approach has been used in previous studies (like Luong et al., 2009; Pei et al., 2010). Formal: Another approach is to use a structured perceptron, which is a newer model, to generate responses. It builds a probability distribution for each response, using a linear setup. Formal: This approach doesn’t need any structure-sharing techniques, but it’s way slower than the simpler multi-head attention. Formal: Formal: Another approach is to use a multi-head attention model, which is a newer model, to generate responses. It builds a probability distribution for each response, using a linear setup. Formal: Another approach is to use a structured perceptron, which is a newer model, to generate responses. It builds a probability distribution for each response, using a linear setup. Formal: Formal: Another approach is to use a multi-head attention model, which is a newer model, to generate responses. It builds a probability distribution for each response, using a linear setup. Formal: Another approach is to use a structured perceptron, which is a newer model, to generate responses. It builds a probability distribution for each response, using a linear setup. Formal: Formal: Another approach is to use a multi-head attention model, which is a newer model, to generate responses. It builds a probability distribution for each response, using a linear setup. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1269.json"
    },
    {
        "casual_text": "In our initial experiments, we wanted to see if the model could put together separate concepts it learned during training. We used a selection of adjective-noun and verb-noun pairs from Nikolaus et al. (2019) and tweaked their train, validation, and test sets. The pairs we looked at are listed in Table 7.",
        "formal_text": "In the first set of experiments, we investigate whether or not the model can combine known concepts disjointly observed during training. We adopt a subset of the pairs of adjectives and nouns, verbs and nouns defined by Nikolaus et al. (2019), and modify the proposed train, validation, and test sets. The examined pairs are presented in Table 7.",
        "GPT2_formal_text": "ed = d_masked * tanh(M_s_0 + M_i) * tanh(M_e_0 + M_i) * tanh(M_s_0 + M_i) + tanh(M_e_0 + M_i) * tanh(M_s_1 + M_i) * tanh(M_e_1 + M_i) * tanh(M_s_1 + M_i) + tanh(M_e_1 + M_i) * tanh(M_s_1 + M_i).\n\nBasically, each feature function is weighted based on some setup. Formal: Equation (8) is the same as Equation (1), but we’ve added a twist: we’ve added a multiplication step with weights that focus on the average attention values for each node, instead of just the average of the source attention vectors. Formal: A_i and a_j are the source attention vectors for node i and node j, respectively. Formal: M_s_0 and M_i represent the average attention values for each node in the sentence. Formal: M_s_0 and M_i are the average attention values for each node, and the sum of their attention values across all nodes in the sentence. Formal: The sum of (a_i, a_j) from node i to node j equals the average attention value for node i. Formal: The sum of (a_i, a_j) from node i to node j equals the average attention value for node j. Formal: The sum of (a_i, a_j) from node i to node j equals the average attention value for node j. Formal: Here, a_i and a_j are the source attention vectors for node i and node j, respectively. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-srw.11.json"
    },
    {
        "casual_text": "Sure! Here's the informal version:\n\n1. https://github.com/yikangshen/PRPN  \n2. https://github.com/harvardnlp/urnng  \n3. https://github.com/davidswelt/dmvccm  \n4. https://github.com/DrDub/cclparser  \n5. https://github.com/iesl/diora",
        "formal_text": "https: //github. com/yikangshen/PRPN 2 https: //github. com/harvardnlp/urnng 3 https: //github. com/davidswelt/dmvccm 4 https: //github. com/DrDub/cclparser 5 https: //github. com/iesl/diora",
        "GPT2_formal_text": "\" (rows 9-16), which basically means the words in the query and how they're organized in a table. This table is a list of the top k most likely word forms (or \"phrasal distributions\") for a given query. Formal: Let's use the \"Speech Act\" (SA) data from the 2007 International Congress of the Association for Machine Translation. This dataset includes speech acts for 52 languages, and we're using the \"Speech Acts\" dataset from the 2008 Workshop on Spoken Language Translation. Formal: The \"Speech Acts\" dataset includes speech acts for 46 languages. Each of these acts has two categories: a \"speaker\" category and a \"actor\" category. Formal: The dataset includes 33,000 speech acts from 44 languages. Each act has three categories: speaker, actor, and listener. Formal: The \"Speech Act\" dataset includes speech acts for 42 languages. Each act has three categories: speaker, actor, and listener. Formal: The \"Speech Acts\" dataset has 32,000 speech acts. Each act has three categories: speaker, actor, and listener. Formal: The \"Speech Acts\" dataset includes speech acts for 28 languages. Each act has three categories: speaker, actor, and listener. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.300.json"
    },
    {
        "casual_text": "So, if we take a sentence like \"John looked it up,\" it leads to this specific way of understanding it, where we focus on the particle meaning alone.",
        "formal_text": "An input sentence such as =John looked it up = thus produced the following unique derivation, which was interpreted with the particle reading only.",
        "GPT2_formal_text": "is a bit different because it decides how much the next word should be predicted based on the context, not just the whole sentence. This makes it important to figure out a way to combine both the sentence and the context in a way that works well together. We tried a bunch of different ways to combine the sentence and context, and ended up using a mix of beam search and backpropagation. Check out Table 3 for the results of our experiments. Formal: We tested our model against a bunch of models, and the results are in Table 3. We used the top model from the MultiNLI dataset, which used a dataset of 150k examples, and it scored 94.8. Our model was the top performer, and it beat all the other models by an average of 11.9%. Formal: We also looked at other methods, and the results are in Table 3. Formal: We ran our model on the MultiNLI dataset to see how the model would perform on different parts of the sentence. Formal: We focused on the parts of the sentence that are hard to predict, like the parts that get the most attention. We tested different techniques like beam search and backpropagation, which gave us different results. Formal: Our model does a great job with the least-predictable parts of the sentence, but the model still struggles with the more predictive parts. Formal: When we combine the sentence and context in the usual way, we get the best results, but it's not as good as our model. Formal: We also tried combining the sentence and context in a different way, using the probability distribution to figure out the correct answer. This method gives us the best results, but it also has the worst accuracy. Formal: Formal: Formal: Table 4 shows the results of our experiments with different ways of combining the sentence and context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "A88",
        "filename": "A88-1034.json"
    },
    {
        "casual_text": "Following the work of Dubey and colleagues in 2013, we're trying to create a model that can handle pronouns in everyday text and also explain how long it takes to read certain parts. Another goal is for our model to make sense of the human preferences found in studies on psycholinguistics. To do this, we're building on pronoun resolution systems from NLP, like the one by Soon and others in 2001. We're using weighted features to represent the different factors that affect pronoun resolution. This helps us figure out how important each factor is and how they might work together.",
        "formal_text": "Inspired by Dubey et al. (2013), we aim for a model of pronoun resolution that can run on natural texts and explain reading times. A second aim for our model is that it can account for human preferences discovered in the psycholinguistic literature. Based on these criteria, we build a model inspired by NLP pronoun resolution systems (Soon et al. , 2001). The factors of influence on pronoun resolution are represented as weighted features. This provides a way to assess their relative importance and allows to study their interaction.",
        "GPT2_formal_text": "', POS_list, prepos_list, and out_pos_list) are used to do the match. The final result for a word is a list of word embeddings. Formal: This is an example of how a parse tree for an English sentence looks. Formal: The length of the parse tree is determined by the length of the sentences in that sentence, plus some extra bits from the dictionary (like a phrase length limit). Formal: A parse tree for a sentence looks like this: NP → VP NP, VP → NP VP, NP → VP NP, NP → VP NP, VP → NP NP VP. Formal: There's a specific rule for how things like prepositions and conjunctions work. Formal: The translation model you get is a parse tree for a sentence, with each node being a word and its two children as its child nodes. Formal: But there's a catch: the parse tree has to be in the form of a dependency tree. Formal: Here's a more casual version of the parse tree for the sentence in Figure 1: NP → VP VP NP VP → NP VP NP NP → VP NP VP NP → VP NP VP NP → VP NP NP Formal: The same setup applies to a dependency tree. Formal: In Chinese, the main word in the sentence is the one that shows up first. Formal: This is one way the word order in Chinese can be described by its Chinese word form. Formal: In Chinese, there are two ways to arrange words in a sentence: one way is the left-to-right order, and the other is the right-to-left order. Formal: To make things easier to understand, Chinese sentences are usually written in both directions, but they're written in the same order as the English sentences. Formal: There's another way to arrange words in Chinese: one way is the left-to-right order, and the other is the right-to-left order. Formal: It's also common to have a word in the middle of a sentence. Formal: The order of words in Chinese can also be described by their word forms (the Chinese word forms are shown by their Chinese word forms). Formal: Formal: You can also combine the left-to-right order with the right-to-left order to make it easier to understand. Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E17",
        "filename": "E17-4006.json"
    },
    {
        "casual_text": "Cells with the same attribute names usually show similar ideas. We use these metrics to check how alike the cells are.",
        "formal_text": "Value cells under the same attribute names demonstrate similar concepts. We employ the following metrics to measure the cell similarity.",
        "GPT2_formal_text": "_3d, key_k, and value_k representations, respectively. Formal: To get a full picture of how the attention weights are distributed across different parts of the document, we use a bi-directional GRU (BiGRU) to encode the attention output and then use those encoded weights to calculate the document-level representation. Formal: We can add an edge to our graph, meaning each node in the graph can have a directed edge if it has a predicted attention value. Formal: Using the embedding weights for the document-level representation (where h_t = (h_c, h_s)) and the entity-level representation (where h_e = (h_f, h_h)) of the entity pair (e_t, e_s), we can calculate the overall representation for the whole document-level graph. Formal: Since the embedding weights for the entity-level representation (h_e) are tied to the entity-level representation (h_t), we can calculate the representation for the entire document by combining the embedding weights for both the entity-level and document-level representations. Formal: The entity-level representation (h_e) is created by combining the entity-level representation (h_t) and the embedding weights for the entity pair (e_t, e_s) from the entity-level graph. Formal: The entity-level representation (h_e) is a multi-dimensional vector that has three main parts: the entity type, the entity type embedding, and the entity type embedding embedding. Formal: This representation is created using the multi-layer perceptron algorithm. Formal: After combining the embedding weights for both the entity-level and document-level representations, the output is a triple (h_e, h_t, h_e). Formal: After combining the entity-level representation (h_e) and the embedding weights for the entity pair (e_t, e_s), the output is a triple (h_e, h_t, h_e). Formal: Formal: Finally, the attention weights from the entity-level representation (h_e) and the document-level representation (h_t) are combined to create the document-level representation (h_e). Formal: Formal: Figure 2(a) shows the final representation for the document-level graph. Formal:",
        "directory": "C00",
        "filename": "C00-1025.json"
    },
    {
        "casual_text": "Basically, when we're figuring out how to index stuff, we start with the big picture—like general categories that don’t get too detailed (think small feature-trees). Then, we move on to the more specific stuff that adds extra details to those general ideas (like expanding those smaller trees through unification). Most of the time, this means we work from the main parts of the sentence to the details, like starting with the verb phrase (VP) and then looking at what comes after it. This is similar to what Proudian and Pollard talked about in HPSG back in 1985.",
        "formal_text": "Our general principle for indexing rules is that we must parse from categories that specify general information (ie. that have small feature-trees) to those that specify particular modifications of that general information (ie. that provide extensions to the smaller trees by unification). This usually means that we parse from syntactic heads to complements, eg indexing sentences on the vp (cf. HPSG [Proudian & Pollard 1985] ).",
        "GPT2_formal_text": "\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [ ] }, { \"type\": \"REGULAR\", \"id\": [ ], \"label\": [ ], \"src\": [ ], \"disj\": [",
        "directory": "A88",
        "filename": "A88-1029.json"
    },
    {
        "casual_text": "The dataset we end up with includes three pieces of information for each term: the original term we observed, one of the \"intended\" terms that the algorithm figured out, and how many times that intended term was meant. For a single term, it's normal to have several possible combinations because the algorithm decides based on the context.",
        "formal_text": "The resulting dataset consists of triples of the original observed term, one of the \"intended\" terms as determined by the above algorithm, and the number of times this term was intended. For a single term, it is possible (and common) to have multiple possible triples, due to the context-based assignment.",
        "GPT2_formal_text": "ing_rand * W_t = [n_t, m_t, W_t]. Formal: Basically, the main idea is to focus on the \"w_s_i\" part of the input (which is the word embedding) and create a simple vector representation, called h_t, for each token in the text. Formal: We use a Bi-LSTM to create an attention matrix for the word embedding \"w_s_i\" in the text. This matrix is created by combining the word embedding with a residual connection. Formal: Since we don't have any special words to work with, we just stick the word embedding and the residual connections together. Formal: We keep track of the attention vector h_t for each token in the text. Formal: The output from the Bi-LSTM is the vector h_t, which is just the sum of all the attention vectors. Formal: The final attention vector, h_t, is the sum of the attention vectors for each token in the text. Formal: Figure 2 shows how we handle the multi-head attention. Formal: We update the representation of the key word embedding \"w_s_i\" using a softmax function, which gives us a new attention vector h_t, which is the sum of all the attention vectors for the \"w_s_i\" token. Formal: We calculate the attention between the key and the value and then update the output of the bi-LSTM. Formal: The key word embedding \"w_s_i\" is updated using a bi-LSTM to create a new vector h_t, the sum of all the attention vectors for the \"w_s_i\" token. Formal: We use the input attention vector h_t to initialize the output of the Bi-LSTM, which is the sum of all the attention vectors for the \"w_s_i\" token. Formal: Finally, we combine h_t with h_s_i to get the attention vector h_t. Formal: Formal: We run the whole process again, but this time using the output from the Bi-LSTM, which is the sum of all the attention vectors for the \"w_s_i\" token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1093.json"
    },
    {
        "casual_text": "We matched English word vectors with six other languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We used 300-dimensional fastText vectors that were trained on Wikipedia and Common Crawl. We made all the words lowercase, kept only the 200,000 most common ones, and did five rounds of Iterative Normalization to clean things up.",
        "formal_text": "We align English embeddings with six target languages: German (DE), Spanish (ES), French (FR), Italian (IT), Japanese (JA), and Chinese (ZH). We use 300-dimensional fastText vectors trained on Wikipedia and Common Crawl. We lowercase all words, only keep the 200K most frequent words, and apply five rounds of Iterative Normalization.",
        "GPT2_formal_text": "= {q i, j} = [s i, j]\n          Casual: A lot of research on dealing with language differences has been focused on a single language. Some examples include studies about using machine translation (like Klein et al., 2003), using local and mixed data (Bunescu and Mooney, 2006), or using a mix of training and testing methods (Lee et al., 2006). We’re looking at using cross-lingual approaches to handle different languages. For instance, Klein et al. (2003) tried using machine translation to translate a German-English glossarial text into English for a glossarial task. But their approach didn’t really take advantage of the benefits of cross-lingual transfer learning. \n\nSimilarly, previous research has looked at translating one language into another. For example, Bunescu and Mooney (2006) built a cross-lingual approach by translating text in the target language into English using a hierarchical phrase-based machine translation system. But this approach didn’t work well because the phrases they translated were different in meaning. Formal: A lot of research on dealing with language differences has been focused on a single language. Some examples include studies about using machine translation (like Klein et al., 2003), using local and mixed data (Bunescu and Mooney, 2006), or using a mix of training and testing methods (Lee et al., 2006). Formal: Formal: We’re using cross-lingual approaches to handle different languages. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.201.json"
    },
    {
        "casual_text": "Formatting relies on how often words appear together in the text and uses medical terms from a computer dictionary.",
        "formal_text": "Formatting is based on cooccurrence patterns found in the text and on the lexical information obtained from the computer dictionary of medical vocabulary.",
        "GPT2_formal_text": "ed is used to figure out the attention weights for the current token x, while attention_masked_max is used to control how much the attention is focused on the token. Formal: The BiLSTM module takes in the hidden state h_t and the attention weights a_t for each token, while the ResNet-152 module also uses its hidden state h_t to figure out the attention weights for the token. Formal: The final hidden state h_t is calculated by taking the average of the attention weights a_t for all the tokens. Formal: This setup lets the model learn from both the input and output at the same time, so it can work with different types of inputs or outputs. Formal: We’ve listed some examples of mixed inputs and outputs in Table 6, which you can check out in Section 3. Formal: We’ve put the results of our experiments in Table 2, along with how each model performed on the test set. Formal: We also included the results for models that didn’t use any special tricks to boost performance. Formal: For the text-based models, we used the simple regularization model (Zhang et al., 2020), which is basically a non-linear function that takes the input into account. Formal: For the image-based models, we tried out three different methods: (1) we used the MaskedImage-Attention (MVA) model (Liu et al., 2020b) that’s designed to handle images, (2) we used the RandomImage-Attention (RVA) model (Liu et al., 2020a) to learn from images, and (3) we used the MaskedImage-ImageAttention (MVA) model (Liu et al., 2020b) that’s designed to focus on images. Formal: Table 3 shows the results of different models on the test set. Formal: Figure 2 shows the visual attention model architecture, and the attention weights are given in rows and columns. Formal: In Section 3.2, we talked about how the model’s performance affects the attention weights, which we’ll explain now. Formal: The attention weights are calculated based on the attention distribution of the token in the input. Formal: The image-based model uses the same model as the text-based model, except the one we used in the text-based model, but it",
        "directory": "C82",
        "filename": "C82-1032.json"
    },
    {
        "casual_text": "2 Weight Poisoning Attack Framework  \n2.1 The \"Pre-train and Fine-tune\" Approach",
        "formal_text": "2 Weight Poisoning Attack Framework 2. 1 The \"Pre-train and Fine-tune\" Paradigm",
        "GPT2_formal_text": "= (pos_i + 1) i=1 for i in range(s_i, s_1_i), where each i is calculated based on the token length (s_i_l). Formal: We use a logistic regression classifier to figure out the chances of each possible response. Formal: To test how well our model can generate responses, we measure the mean reciprocal rank (MRR) of the response using the F1 score (from Gimpel et al., 2014). We also check how well it does on the validation set using the validation error rate (VAR_E) and the average score (avg_score). Formal: When generating responses, we don’t use any pre-trained language model. This means the model can produce responses that are personalized, making them easier to understand, based on the context. Formal: The model learns how to generate responses by generating responses on its own. Formal: To make sure the model is generating responses that are informative and relevant to the context, we use a masked language model (MLM) to generate responses. Formal: We use a masked language model (MLM) to generate responses. We also create a multi-scale, multi-head attention network (M-HAN) for this task. Formal: We randomly start with 10,000 randomly selected tokens from the dataset. To make sure the model is generating responses that are relevant to the context, we randomly select a token from the dataset for each token and calculate the average interaction score (AL_r) between the token and the context token. Formal: We randomly pick a token from the dataset. We also calculate the average interaction score (AL_r) between the token and the context token. Formal: We randomly pick a token from the dataset. We also calculate the average interaction score (AL_r) between the token and the context token. Formal: We randomly pick a token from the dataset. We also calculate the average interaction score (AL_r) between the token and the context token. Formal: Formal: We randomly choose a token from the dataset. We also calculate the average interaction score (AL_r) between the token and the context token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.249.json"
    },
    {
        "casual_text": "Himanshu Aggarwal and his team came up with a system in 2006 that uses Conditional Random Fields for Hindi. They used a morph analyzer to figure out root words and possible parts of speech (POS tags) for training. The system was tested on a corpus of 21,000 words with 27 different POS tags, and it got an accuracy of 82.67%.",
        "formal_text": "Himanshu Aggarwal et al. , 2006 developed a system using Conditional Random Fields for Hindi. A morph analyzer is used to provide information like root words and possible POS tags for training. The system was evaluated over a corpus of 21000 words with 27 different POS tags and system achieved the accuracy of 82. 67%.",
        "GPT2_formal_text": "_type: Set the output dimension to n_s. If the input dimension is greater than n_s, we use the source embedding to create the output embedding for the next word. For the mask vector, we stick with the last hidden state x_t_l_i as the output. Formal: This setup gives us the output from the source teacher model, which we call h_src. Formal: For our evaluation, we use a validation set for training, a dev set for testing, and an untuned teacher model for validation. We create the target embedding h_t by using the last hidden state x_t_l_i, which comes from the source model. Formal: Since the input and output embeddings are the same, the same kind of contextual embedding works fine. But we think it's better to use the target embedding to create a solid representation that keeps the information about the token intact, which we call h_t. Formal: We're checking out different options for the set of embeddings in Fig. 1 to figure out what works best. Formal: Here's the formal definition of the function g(s, t, x): Formal: We use the convolutional neural network (CNN) model to get the contextual representation h_t. Formal: Formal: G is the weight vector for the convolutional layer, and θ_t is the weight for the fully connected layer. Formal: Formal: The gate g is used for the pooling layer, and θ_b is the weight for the bichar layer. Formal: For the second convolutional layer, g(s, t, x) is the final output that comes from the pooling layer. Formal: Formal: Formal: To figure out the best parameter vector, we can either pick the one that minimizes the loss or ignore the non-linearity (like Och or ReLU). Formal: We set the weights for the second convolutional layer to be zero in the zero-shot setup. Formal: Formal: Lastly, we optimize the function g(s, t, x) using the cross-entropy loss. Formal: Formal: We start by looking at the input and output embeddings, then calculate the regularization term. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C12",
        "filename": "C12-3021.json"
    },
    {
        "casual_text": "The method for finding the right parts of the text involves flipping the usual way we create sentences and checking from the end to the beginning.",
        "formal_text": "The analysis procedure leading to the detection of acceptable segments was formulated as a reversal of the generative grammar and is performed by a right =o left scanning.",
        "GPT2_formal_text": ") = {u, v} in S. Formal: For a given pair (u, v) in S, the ground-truth target segment (u) is picked based on the alignment info from the hidden states. Formal: You can grab the data from the training set directly. Formal: Alright, so we can say that: Formal: We pick the best target segment based on the alignment info from the hidden states. Formal: We also add the target segment to the training set. Formal: Formal: Lastly, we calculate a score for the source segment by using the entire training set. Formal: Formal: So, basically, the alignment info gives us a guide for the alignment, which gives us the best segment for the source. Formal: And the final score is calculated using the entire training set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "A83",
        "filename": "A83-1023.json"
    },
    {
        "casual_text": "When it comes to data structure, the connection between OC and OQ is kind of like the difference between paired data and two-sample data in statistical tests. With OC, we look at how the system sorts items into classes and create a confusion matrix using the actual and predicted classes. On the other hand, OQ is more about comparing how the system's distribution of items matches up with the actual distribution, without worrying about which specific item in one matches up with an item in the other.",
        "formal_text": "In terms of data structure, we observe that the relationship between OC and OQ are similar to that between paired data and two-sample data in statistical significance testing. In OC, we examine which item is classified by the system into which class, and build a confusion matrix of gold and estimated classes. In contrast, in OQ, we compare the system's distribution of items with the gold distribution, but we do not concern ourselves with which item in one distribution corresponds to which item in the other.",
        "GPT2_formal_text": "ing. Def. (1), and the attention mechanism uses the standard multi-head attention architecture from Vaswani et al. (2017). Formal: To get the model's predictions, we start by putting the input sentence and the output sentence together. Formal: We use a straightforward multi-head attention setup with an encoder, a decoder, and a bias for each entity's prediction. Formal: Instead of using a fully connected layer, we suggest using a cross-attention layer that connects the predictions from the encoder and decoder. This way, we can use the entity's predictions as extra context to help the decoder focus on specific parts of the sentence. Formal: We suggest using cross-attention for entity prediction to make things more efficient. Formal: Instead of relying on extra input info, we suggest using a joint decoder and an entity prediction module. The entity prediction module uses a cross-attention layer to focus on the important parts of the input. Formal: We use a multi-head attention network to connect the encoder and decoder to predict the entity type. This setup lets the model learn both the entity type and the entity embedding separately. Formal: We use a cross-attention layer to improve the overall predictions and a weighting module to adjust the entity embedding vector. Formal: The entity prediction module calculates the entity embedding vector for a given entity, kind of like a supervised classifier. Formal: The entity prediction module calculates the entity embedding vector for a given entity, kind of like a supervised classifier. Formal: We add entity prediction loss to the loss from the entity prediction module to keep the model on track. Formal: Formal: We use an entity prediction loss to keep the model on track. Formal: Formal: We also use cross-attention to deal with the entity embedding. Formal: The entity prediction module calculates the entity embedding vector for a given entity, kind of like a supervised classifier. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.214.json"
    },
    {
        "casual_text": "We used the Kyoto Text Corpus as our annotated dataset. It had a total of 1,675,188 characters.",
        "formal_text": "As an annotated corpus, we used Kyoto Text Corpus. 6 It contained 1, 675, 188 characters.",
        "GPT2_formal_text": ". Formal: To prevent issues with duplicate tokens, we use the same token-level methods as in earlier work (Han et al., 2020). We only add edges from the top k results to the list of candidates for the next token. Formal: In the second layer, we calculate the negative log likelihood, which is the same as the one in equation (1). Formal: We put all the candidates back into the top k results. This gives us the candidate-level list in equation (4). Formal: After that, we use the global ranking model to grab the best candidate token. The global ranking model learns to rank the candidates in a way that makes sense, based on some prior knowledge. Formal: This process is pretty similar to the one in equation (6), except that instead of adding a new edge, we just return the top k results. Formal: After that, we update the label for the token from equation (6) by using the global ranking model to rank the candidates. Formal: Formal: For the final layer, we combine the token-level candidate list, which is the result from equation (6), with the global ranking model's results to get the final result for the token. Formal: Formal: We run the model on the candidate-level list to get the token-level list. Finally, we use the global ranking model to rank the candidates in the right order to get the final label. Formal: Formal: We test our model on two datasets to see how well it works. Formal: We follow the same steps to create the token-level list as in (6). Formal: We also tried out different models that perform differently. Formal: Table 2 shows the performance of the best model on two datasets, with the results based on the best results in the last layer. Formal: Table 2 also shows the performance of different models. Formal: Formal: Table 3 has the results for different models on two datasets, with the results based on the best results in the last layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D11",
        "filename": "D11-1056.json"
    },
    {
        "casual_text": "Okay, so we're working with the ACE 2004-NWIRE dataset, which is basically the newswire part of the ACE 2004 corpus. This dataset has been used by researchers like Poon and Domingos (2008) and Haghighi and Klein (2009). For our development phase, we focused on the first corpus, called ACE2004-ROTH-DEV. The rest of the corpora are kept aside for testing later.\n\nWe used the Stanford parser (developed by Klein and Manning in 2003) to analyze all the documents. This parser helps us figure out the syntactic structure, which we then use to pinpoint the main words in mentions and to decide the order of these mentions within a sentence. We'll go into more detail about this in the next section.\n\nTo make sure our results are comparable to earlier work, we didn't use the perfect, hand-labeled named entity tags or mention types. Instead, we relied on the labels given by the Stanford NER tool (created by Finkel et al. in 2005).",
        "formal_text": "• ACE2004-NWIRE -the newswire subset of the ACE 2004 corpus, utilized by Poon and Domingos (2008) and Haghighi and Klein (2009) We used the first corpus (ACE2004-ROTH-DEV) for development. The other corpora are reserved for testing. We parse all documents using the Stanford parser (Klein and Manning, 2003). The syntactic information is used to identify the mention head words and to define the ordering of mentions in a given sentence (detailed in the next section). For a fair comparison with previous work, we do not use gold named entity labels or mention types but, instead, take the labels provided by the Stanford named entity recognizer (NER) (Finkel et al. , 2005).",
        "GPT2_formal_text": "ing = tanh(E s (X ) + 1e-4) * exp(u s (X ) + e s (X )) + tanh(E s (Y ) + 1e-4) * exp(u s (Y ) + e s (Y ))). Formal: To find the logit for the word x s t, you can use the formula below. Formal: φ(•) is just the log probability of the word x s t happening given the context. Formal: The logit for the word x s t is basically the difference between the log probability of x s t happening and the log probability of the context. Formal: So, when you're dealing with a log probability distribution, you need to check how much of that probability is coming from the context or the word itself. Formal: To get the logit for the word x s t, you just multiply the log probability of the word x s t with the log probability of the context c s t. Formal: For the words in a sequence (X, Y), the logits for each word are calculated based on their context (c s t), their position in the sequence (X/Y), and the average length of the sequence (a s t). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D10",
        "filename": "D10-1048.json"
    },
    {
        "casual_text": "Alright, let's talk about child labor in Africa, specifically in South Africa. It's a big issue where kids are forced to work instead of going to school. This affects a lot of children and is a major problem in the labor force there.",
        "formal_text": "non#$ development$of$ child$labor$ child$$$$$$$$$$$$$$$$$$$$$$$$children$$$$$$$$$$$$$$$$$$$$child$ labor$$$$$$$$$$$$$$$$$$$$$$$$$labor$$$$$$$$$$$$$$$$$$$$$$$$labor$force$ Africa$ South$Africa$",
        "GPT2_formal_text": "_f. These are the name and type of entity pairs (e1, e2) that the query gives you, along with the contextual embeddings (e1, e2, t). Formal: (2) For every entity pair (e1, e2), the embedding for the entity pair is made by combining the contextual embeddings for the entity pair with those from the question. Formal: (3) This process is repeated for every pair (e1, e2) in the question. Formal: (4) We then use the entity pair embedding from the query to guess the type for the entity pair. Formal: (5) Finally, the type prediction is based on the final entity pair embedding. Formal: (6) Formal: (7) We mix the entity pair embedding from the query with the entity type prediction for each entity pair in the query. Formal: (8) Formal: (9) Next, we use the final entity pair embedding from the query along with the type prediction for each entity pair in the query. Formal: (10) Formal: (11) Finally, we sum all the entity pair embeddings to get the overall entity pair embedding. Formal: (12) Formal: (13) So, for each entity pair (e1, e2), the final entity pair embedding is based on the entity type prediction for each entity pair in the query. Formal: (14) Formal: (15) Lastly, we use the final entity pair embedding to guess the type for the entity pair. Formal: (16) Formal: (17) Formal: (18) Formal: (19) Formal: (20) Formal: (21) Formal: (22) Formal: (23) Formal: (24) Formal: (25) Formal: (26) Formal: (27) Formal: (28) Formal: (29) Formal: (30) Formal: (31) Formal: (32) Formal: (33) Formal: (34) Formal: (35) Formal: (36) Formal: (37) Formal: (38) Formal: (39) Formal: (40) Formal: (41) Formal: (42) Formal: (43) Formal: (44) Form",
        "directory": "D16",
        "filename": "D16-1055.json"
    },
    {
        "casual_text": "It seems like there's a lot of hype around neural architectures these days, especially when it comes to encoder-decoder models for Seq2Seq tasks. For instance, Rao et al. (2015) got some great results with their encoder-decoder G2P model, hitting a really low error rate on the CMUdict dataset (Kominek and Black, 2004). But, when you look closer, the neural architecture itself isn't all that impressive on its own. It only really shines when paired with a weighted finite state transducer, where it edges out traditional models. \n\nSimilarly, Faruqui et al. (2016) claimed their inflection generation neural architecture performed \"par or better\" compared to traditional methods. But digging into their results, it turns out their system isn't consistently better—sometimes it's worse, and other times it's better.",
        "formal_text": "We have noticed that, given the enormous interest currently surrounding neural architectures, recent research appears to somewhat over-enthusiastically praise the performance of encoder-decoder approaches for Seq2Seq tasks. For example, while the encoder-decoder G2P model by Rao et al. (2015) achieves an extremely low error rate on the CMUdict dataset (Kominek and Black, 2004), the neural architecture itself has a mediocre performance and only outperforms traditional models in combination with a weighted finite state transducer. Similarly, Faruqui et al. (2016) report on \"par or better\" performance of their inflection generation neural architecture. However, a closer inspection of their results suggests that their system is sometimes worse and sometimes better than traditional approaches.",
        "GPT2_formal_text": "_token_len, layer_num, and layer_shape) of the output. Formal: This part is about the decoder part, where we figure out the sequence of hidden states. In this part, we'll use the template from the input to represent the output. Formal: Here, the encoder takes the input and turns it into a sequence of hidden states, which are then fed into the decoder as its output. Formal: Finally, the decoder uses a transformer to make the final output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1160.json"
    },
    {
        "casual_text": "Apart from TAG-TAG, TAG-WORD, and TAG-REASON, we also added the method from (Heymann and Garcia-Molina, 2006) as a baseline, which we call HEYMANN. The HEYMANN method is meant to identify \"similar-to\" relationships rather than \"subsumption\" relationships. \"Similar-to\" relationships are symmetrical, while \"subsumption\" relationships are stricter and not symmetrical. In our tests, we used the same evaluation process for TAG-TAG, TAG-WORD, TAG-REASON, and HEYMANN, and only subsumption relationships were counted as correct.",
        "formal_text": "Besides TAG-TAG, TAG-WORD and TAG-REASON, we also include the method described in (Heymann and Garcia-Molina, 2006) as a baseline, denoted as HEYMANN. HEYMANN method was designed to find similar-to relation rather than subsumption relation. The similar-to relation is symmetric, while subsumption relation is more strict and asymmetric. In our experiments, we use the same evaluation process to evaluate TAG-TAG, TAG-WORD, TAG-REASON and HEYMANN, in which only subsumption relations will be marked as correct.",
        "GPT2_formal_text": "= [s1, ..., sn, tp, ..., tk] | {sj, ..., sk}, where {sj, ..., skj} are the labels for the i-th row in T. Formal: The RNNG (from RNNG by Sagae and Lavie, 2008) is a state-of-the-art model for generating text. It has an embedding layer, a memory layer, a search layer, and a hidden state layer. Formal: We use a TextRank model that was trained on a huge dataset of 1.5 billion words to figure out the most likely sequence. Formal: We take a list of words {w1, ..., wn} and find the best sequence for them. We use the exact phrase length, which is n-1, as the length of the shortest phrase we can find. We just stick with that for all our experiments. Formal: This part talks about the model and how we trained it. Formal: We take the sentences that have the highest probability and feed them into a sentence-level BiLSTM. The hidden state of the LSTM is the probability of the word being the next word in the sentence. Formal: We run the model on a batch size of 128 and fine-tune it for 50 epochs. Formal: We also train a linear classifier using the same dataset and have it predict the probability of the next word in the sentence. Formal: We use the WordPiece dataset (Klein and Manning, 2003) to train a linear SVM classifier for a text. Formal: We use the same set of parameters for the SVM as we did for the Linear SVM, and we set the learning rate to 0.1 for both. Formal: We also train a second linear classifier using the same dataset. Formal: For the non-linear version, we just skip the linear part. Formal: We use the Naive Bayes classifier (from Kingma and Ba, 2014) with the weights from the original linear SVM. Formal: For the Naive Bayes classifier, we also train a linear SVM classifier. Formal: We use the training set for the linear SVM and fine-tune it for 100 epochs. Formal: We take a batch size of 128 and use the parameter λ as the learning rate. Formal: We run the model",
        "directory": "C10",
        "filename": "C10-1114.json"
    },
    {
        "casual_text": "We haven't really looked into GGP's parsing complexity broadly yet, but some key factors that limit chart parsing are pretty well understood, thanks to studies on context-free parsing by Sheil back in 1976. Basically, the number of steps it takes is around O(nD), and the space needed is O(n²), no matter how you approach the parsing. The length of the input sentence (n) plays a big role here. Interestingly, the size of the grammar doesn't affect the complexity, but the branching factor—which kinda tells you how uncertain or unpredictable the grammar is—acts like a multiplier.",
        "formal_text": "Although we have not yet analyzed GGP's parsing complexity in general, some limiting factors for chart parsing are well known by investigations on the context free case by Sheil (1976): The number of steps is o~ O (nD), the space requirements of 0 (n 2) independent of the parsing strategy, where n is the length of the input sentence. The size of the grammar does not influence complexity, but its branching factor, which is a measure for its degree of nondeterminism, acts as a proportionality factor.",
        "GPT2_formal_text": "_len = 1 # W, N = 2 D = {[CLS], T}, B = {[BST], T}, C = {[CST], T}, D = {[DST], T} T = {[CLS], T} = {[TST], T} x = (D, T, B, C, T, B, C, T, D) Formal: We're suggesting a new loss function called MINER. It works by minimizing the following thing: Formal: You can check out these definitions in section 4.1. Formal: We're also suggesting a new model called AFINN. It takes a sequence of event tokens and figures out the sentiment distribution for each token based on the context around the event. Formal: Our model is built on the AFINN model (Wang et al., 2017), and we've added a few new things to it. Formal: We've got a joint model (FFNN-FFNN), a token-level attention layer (α), and some context-aware weights to handle the dependencies in the input (α). Formal: We've also figured out a way to learn attention vectors for each token. Formal: Basically, it's like a probabilistic model that's been trained using the attention vector from the token-level attention layer. Formal: We've created a simple model to measure how related two events are by looking at how much attention each token pays to the event-related words around it. Formal: In our model, the attention vector helps guide the token's attention and learns the event-specific sentiment vector. Formal: We also used the idea of orthogonal regularization to prevent the model from just memorizing the entire event vector. Formal: The final loss function we end up with is the one that minimizes the cross-entropy loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E83",
        "filename": "E83-1019.json"
    },
    {
        "casual_text": "Lexical weighting helps deal with the issue of unreliable phrase probabilities, especially for longer phrases, but it doesn't fix the problem of breaking down context. A lot of the work in picking the right translation still falls on the language model (LM), which is really good at what it does but can only use context from the target language. Plus, decisions made early on—like only keeping the top n translation options for each part of the source text—are based only on the translation models and the target context within that phrase.",
        "formal_text": "Lexical weighting tackles the problem of unreliable phrase probabilities, typically associated with long phrases, but does not alleviate the problem of context segmentation. An important share of the translation selection task is then left to the language model (LM), which is certainly very effective but can only leverage target language context. Moreover, decisions that are taken at early decoding stages-such as the common practice of retaining only top n translation options for each source span-depend only on the translation models and on the target context available in the phrase.",
        "GPT2_formal_text": "\" (where i is from 1 to 6, where 1 is the starting token, 2 is the next token, 3 is the first word, and so on). The S-list is set up like this: A → (1, . . . , n). So, the total size of the S-list is O(N²). Formal: In this paper, we use the S-list approach, but we also use some extra features for our structure-sharing method. Specifically, we've created a special rule for converting the space between parenthesis and abbreviation tokens to a space within parentheses. The idea is that in a full sentence, the space between the parenthesis and abbreviation tokens should match the space between the parenthesis and the abbreviation tokens. Formal: A full sentence can be broken down into smaller parts, with each of these parts being a parenthesis or abbreviation token. Each parenthesis token can then be converted into its abbreviation token by following the parenthesis and abbreviation rules in the S-list. Formal: The space between the parenthesis and abbreviation tokens can be matched with the space between the parenthesis and the parenthesis tokens. This means that in a full sentence, the space between the parenthesis and abbreviation tokens can be converted to the parenthesis and abbreviation spaces. Formal: Formal: Formal: This is similar to the S-list method, but instead of following the parenthesis and abbreviation rules in the S-list, we follow the parenthesis and abbreviation rules in the S-list. Formal: We're using a gazetteer approach to connect these parenthesis and abbreviation spaces. Formal: The gazetteer uses the parenthesis and abbreviation spaces along with the parenthesis and abbreviation tokens. Formal: Formal: This gazetteer is set up to find the abbreviation and parenthesis spaces based on the full sentence's structure. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1175.json"
    },
    {
        "casual_text": "So, when we look at how well the Punjabi text summarization system works for Punjabi news articles, it does pretty well at a 10% compression rate. This is because, at that level, the system usually picks the headline and the next line, which is often enough to give a good idea of the whole article. But for Punjabi stories, the system doesn't do as well at 10% compression. That's because stories don't have headlines, and the few lines that get picked for the summary aren't enough to capture the full meaning of the story.\n\nWe also tested the system by doing question answering and keyword association tasks at different compression levels—10%, 30%, and 50%—for both Punjabi news articles and stories. For news articles, the question answering accuracy was 78.95% at 10%, 81.38% at 30%, and 88.75% at 50%. The system struggled a bit at 10% because news articles are usually short, and at that compression level, it mainly picks the headline and the next line, which isn't enough to answer all the questions.\n\nFor Punjabi stories, the question answering accuracy was 80.65% at 10%, 84.26% at 30%, and 90.72% at 50%. And for news articles, the keyword association accuracy was 80.13% at 10%, and then it goes up to 92% at 30%.",
        "formal_text": "The results of intrinsic summary evaluation show that for Punjabi news documents, Punjabi text summarization system performs very well at 10% compression ratio, because at 10% compression ratio usually headlines and next lines are extracted which are enough to describe the whole text but for Punjabi stories, performance of Punjabi text summarization system is not good at 10% compression ratio, because headlines are not present in stories and only few lines are extracted in summary which are not enough to describe the sense of complete story. We have performed question answering task and keywords association task as extrinsic measures of summary evaluation at compression ratios 10%, 30% and 50% respectively for Punjabi news documents and Punjabi stories. For Punjabi news documents, the accuracy of question answering task is 78. 95%, 81. 38% and 88. 75% at 10%, 30% and 50% compression ratios respectively. The results of question answering task show that for Punjabi news documents, performance of Punjabi text summarization system is low at 10% compression ratio because news documents are usually short and at 10% compression ratio, mainly headlines and next lines are extracted which are not sufficient to give all answers of question-answering task. For Punjabi stories, the accuracy of question answering task is 80. 65%, 84. 26% and 90. 72% at 10%, 30% and 50% compression ratios respectively. For Punjabi news documents, the accuracy of keywords association task is 80. 13%, 92.",
        "GPT2_formal_text": ". Formal: The process for creating a knowledge graph that matches a given query Q_i is basically the same as the one for creating a neural network structure that translates Q_i into a special vector q_i. Formal: We'll use the definition of T from section 4 to create a knowledge graph for the input sentence x. This graph will help us analyze the sentence by looking at the relations between the entities in it. Formal: For each relation r_i in the graph, we have a list of entities in the sentence x, which includes the entity's name and some details about the relation r_i. Formal: We can write this down as E(r_i) = [e_i, x_i, r_i] ∈ T where e_i and x_i are the names and their details, respectively. Formal: The relation r_i for the entity in x is a combination of the entities in the sentence x, and we also have a list of relation r_i for the relation r_i. Formal: In this section, we'll use a neural network to handle the relation extraction part. Formal: Using the relation extraction network, we can predict the relation r_i for the entity in x. Formal: We'll give each relation r_i in the knowledge graph a weight, r_i, which tells us how important it is. Formal: We'll define the relation r_i for the entity in x using this relation vector q_i. Formal: To see how the relation extraction network helps us understand the sentence, we'll use an attention mechanism to pull out the relations from the knowledge graph. Formal: We'll explain how we use the relation extraction network to extract the relations from the knowledge graph. Formal: The knowledge graph created by the relation extraction network for the entity in x is shown in figure 3. Formal: The relation vector q_i for the relation r_i for the entity in x is calculated using an attention mechanism. Formal: We'll also share how we create the knowledge graph by using the neural network. Formal: This knowledge graph is based on the one from section 4. Formal: The relation vectors q_i for the relation r_i for the entity in x are calculated using an attention mechanism. Formal: Formal: We'll explain how to use the knowledge graph generated by the relation extraction network for the entity in x. Formal: The",
        "directory": "C12",
        "filename": "C12-3024.json"
    },
    {
        "casual_text": "Okay, so basically, this formula is like a way to figure out how related a document (d) is to a query (q). It takes into account all the words (w) in the vocabulary (V). Here's what each part means:\n\n- `C * (w | d)` is how common the word is in the document.\n- `C(w | q)` is how common the word is in the query.\n- `idf(w)` is a measure of how rare the word is across all documents.\n- `p` is a number that helps balance things out.\n- `avdl` is the average length of documents.\n- `|d|` is the length of the specific document we're looking at.\n\nThe formula mixes these things together, giving more weight (0.8) to the average document length and a bit less (0.2) to the actual length of the document we're checking.",
        "formal_text": "rel(d, q) = P w∈V C * (w | d) • C(w | q) • idf (w) p 0. 8 • avdl + 0. 2 • |d|",
        "GPT2_formal_text": "_for(s_q, s_t, q, t) = max(v_q * (1 + b_q_t))\n            Casual: Alright, let’s break this down in simpler terms. First, we’re talking about the Q and T parts of the QA dataset. We’ve already got the question and the answer. Now, we’re looking at the pairs of possible answers (s_q and s_t). We’re using the max-pooling method to figure out the attention vectors for these pairs. \n\nThe q_t and q_q in these pairs are related, but q_t is way more important than q_q. This is why we’re focusing on the attention vectors for these pairs instead of just looking at how important each answer is individually. \n\nHere’s how this all works: Formal: There are k different questions, each with a set of possible answers (s_q, s_t) and a vector (w_q, w_t). There’s also some sentence embedding stuff happening, like the average of word embeddings for both q_q and q_q_t. Formal: For each pair of questions, we calculate the attention vectors for their questions. Then, we combine these vectors into one fixed-size vector to figure out the attention for that pair. Formal: Finally, we combine the attention vectors for each pair to get the final answer attention vector. Formal: Here, α is a non-linear activation function that helps adjust how much attention is given to each answer. Formal: Also, the question representation h_q is drawn from a log-linear function. Formal: Finally, the attention weights for each question are figured out based on the query representation h_q. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D07",
        "filename": "D07-1085.json"
    },
    {
        "casual_text": "Once all the skeleton nodes are ready, we build a graph using those nodes, but we leave out the right parenthesis, as shown in Figure 3. To get the global structure info, we slap a GAT network on top of the hidden states.",
        "formal_text": "Specifically, after all the skeleton nodes are generated, we construct a graph based on the nodes except the right parenthesis as shown in Figure 3. We use a GAT network on top of the hidden states to capture global structure information:",
        "GPT2_formal_text": ", y_mask, and h_mask for the data set. To get the final sequence, we do a max pooling with a window size of [1, 2, 4, 8, 16, 32, 64, 128] across all the tokens in the data set. We also add a window size of [1, 2, 4, 8, 16, 32, 64, 128] to the input sequence. Formal: We use the input sequence x,y as the answer to the question q. Formal: Q_W_i and Q_D_i represent the first and second tokens of the input sequence, and the answer is the first token from both of them. Formal: We use the attention mechanism with a window size [1, 2, 4, 8, 16, 32, 64, 128] across all the tokens in the input sequence. Formal: Next, we calculate the loss. Formal: We use a standard regression loss to tweak the model to minimize this loss. Formal: Finally, we calculate the gradient using gradient ascent (with α set to 0.2) and tanh. Formal: We also compute the cross-entropy between the predicted question and answer using a linear model. Formal: Lastly, we train the model using this validation set, but only if the confidence score is over 0.5. Formal: We train the model with a batch size of 64. Formal: For the extraction task, we randomly pick one question as the answer. Formal: To train the model, we randomly pick one sentence as the answer. Formal: We use the validation set for validation, but only if the confidence score is over 0.5. Formal: We train the model with a batch size of 64. Formal: Formal: We train the model with a batch size of 128. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.609.json"
    },
    {
        "casual_text": "TTK has this cool visualization thing called TBox, which Verhagen came up with back in 2007. It uses arrows pointing from left to right, boxes inside other boxes, and stacked boxes to show stuff like what happens before, what's part of something bigger, and what happens at the same time. Check out figure 3 for an example!",
        "formal_text": "TTK uses a visualization scheme named TBox (Verhagen, 2007). It uses left-to-right arrows, box inclusion and stacking to encode temporal precedence, inclusion, and simultaneity respectively (see figure 3 ).",
        "GPT2_formal_text": "\" method. The results can be found in the dataset's DATR schema. You can find more about the datasets, including details on the numbers of models and parameters, in the supplementary material. Formal: Our study shows that fully fine-tuned model architectures can lead to better performance than traditional models, especially when dealing with really complex examples. But, if we want to get the best results, we'll need to carefully balance how complex the examples are and how well the model performs. Formal: In Table 4, we show the performance for the English dataset we looked at. The BLEU scores are the results from translating the examples into English. The results were calculated using the same BLEU model we used to train the GPT-2 model. We didn’t include results from the Chinese dataset in this table because, according to the method we explained in Section 4.2, the BLEU scores for the BLEU-2 model aren’t very reliable. Formal: The second column in Table 4 shows the BLEU scores for the translation of the example shown in the first column, with the BLEU model and the number of parameters (k) shown in bold. We didn’t include results from the translation of the second example, \"The starfish are swimming,\" because the BLEU model didn’t perform well there either. Formal: For the English dataset, we tested the Transformer model with k = 10. The results are in the third column in Table 4. Formal: We compared the performance of the GPT-2 model (with parameters from K = 10) to the BLEU-2 model with k = 10 in the fourth column in Table 4. Formal: In the fourth column of Table 4, the results from translating the example \"The starfish are swimming\" are highlighted in bold. Formal: For the Chinese dataset, we tested the Transformer model with k = 10. The results are in the fifth column in Table 4. Formal: We compared the performance of the GPT-2 model (with parameters from K = 10) to the BLEU-2 model with k = 10 in the fifth column in Table 4. Formal: For the Chinese dataset, we tested the Transformer model with k = 10. The results are in the sixth column in Table 4. Formal: Formal: Table 5 has the results for the English dataset we looked at. Formal: Form",
        "directory": "C08",
        "filename": "C08-3012.json"
    },
    {
        "casual_text": "We fine-tune a bi-directional language model to recognize slang words and understand how they’re used by focusing on the masked language modeling task. To do this, we use a dataset from the Urban Dictionary that was scraped and collected by Wilson et al. (2020). After cleaning and trimming the data (check out Appendix A.1 for the specifics), we end up with a training set of 200,000 text sequences packed with slang. \n\nFor our bi-directional model, we go with RoBERTa (Liu et al., 2019). We pick it over the original BERT (Devlin et al., 2019) because it’s better and allows for more subword units. We think this could be helpful for slang since some of the smaller parts of these words might not have been recognized by BERT. Plus, we go with the smaller 125M parameter version to save on computational resources.\n\nWe train the model using the Adam optimizer (Kingma and Ba, 2015) with different learning rates. After testing, we found that the lowest loss on the test set came with a learning rate of γ = 10⁻⁶, so that’s what we use for scoring semantic change. If you want more details on the training setup, check out Appendix A.2.",
        "formal_text": "We familiarize the bi-directional language model with slang words and the contexts in which they are used by fine-tuning it on the masked language modeling task. For this purpose we use a web-scraped dataset from the Urban Dictionary, previously collected by Wilson et al. (2020). After preprocessing and subsampling, the details of which can be found in Appendix A. 1, we are left with a training set of 200, 000 slang-dense text sequences. As our bi-directional language model we select RoBERTa (Liu et al. , 2019). Beyond performance gains compared to the original BERT (Devlin et al. , 2019), we select this model since it allows for more subword units. We reason, that this could be useful in the context of slang words since potentially some of the sub-units used in these words would not have been recognized by BERT. We choose the smaller 125M parameter base version for computational reasons. We train the model using the Adam optimizer (Kingma and Ba, 2015) with different learning rates γ. The lowest loss on the test set was found with γ = 10 −6, which we proceed with for scoring semantic change. For more details on training configurations, we refer to Appendix A. 2.",
        "GPT2_formal_text": "ing_kb in the set of K facets. We also tested how well the model can predict the MLE loss, which is defined as the loss function. Basically, the higher the value of D, the better the model can predict MLE. We tested this using the development set and the test set to see how well the model can handle real-world data. The results are in Figure 6. We looked at how well the model can predict the MLE loss for different values of D. The results show that the model does better at predicting the loss when D is between 0.5 and 1.0, and it gets even better as D increases. So, we think this model is pretty good at dealing with noisy data. Formal: We ran some experiments on a new dataset called MIMIC-III, created by Schick and Schütze in 2020. This dataset has 2,964 passages (with 2,964 positive and 2,964 negative examples) and is labeled for MLE loss, meaning the model can predict the loss using the gold labels. Formal: We trained the model using the same split as our earlier work and tested it on the MIMIC-III development set. We used a set of 300 examples for MIMIC-III, and the model was trained using the same fine-tuning approach used in the previous work. Formal: We also checked how well the model can predict the loss on the test set by calculating the performance loss (shown in Figure 7). Formal: We tested the model on the MIMIC-III test set using the development set, which has 2,964 passages and is labeled for the MLE loss. Formal: We noticed that the model performs better when the training dataset has fewer examples (less than 300). Formal: The loss on MIMIC-III is shown in Figure 8. Formal: We also tested the model on a different dataset, called SCIMID, created by Kocisky and others in 2020. Formal: We used the training set for the first experiment, and the test set for the second. Formal: The results on SCIMID are shown in Figure 9. Formal: We also used the test set for both experiments. Formal: We used the MIMIC-III development set for the first experiment and the test set for the second. Formal: We also trained the model on the test set for the first experiment, using the same fine-tun",
        "directory": "acl",
        "filename": "2022.acl-long.101.json"
    },
    {
        "casual_text": "Later on, we'll talk about how some methods add an extra step to figure out the best groups of mentions for a document. But even with that, they're still relying on confidence scores that come from looking at things locally.",
        "formal_text": "As we will discuss below, some approaches use an additional component to infer the overall best mention clusters for a document, but this is still based on confidence scores assigned using local information.",
        "GPT2_formal_text": "= [u * i, f * j] for (i, j) in enumerate(c). Formal: Also, we've got a feature extractor called f, which gives us a list of these features, like {u1, ..., uk}, from each group, and a feature extractor called g, which gives us a list of these features, like {f1, ..., fn}. Formal: We use a maximum entropy classifier to figure out the probability P(x, y|e), which we write as P(x, y|e). Formal: For each feature f, the classifier classifies the set of possible output values {u1, ..., uk} as either y or x. Formal: The score s for a prediction y is the average of the predictions made for all the feature values in that prediction y. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D10",
        "filename": "D10-1048.json"
    },
    {
        "casual_text": "If the community wants to go over those maximum amounts we just talked about, they can, but only if they figure out what those extra amounts should be... and then decide on them.",
        "formal_text": "die Gemeinschaft, dab to these quantitative limits die vorgenannten HOchstmengen being increased, Qberschritten werden, on the understanding that sofern the additional quantities die zusatzlichen Mengen shall be determined. . . .",
        "GPT2_formal_text": "ing_type. Similarly, the thing we're looking at here is basically the attention representation for the aspect vector. But the tricky part is that, since the attention mechanism is based on the sub-space representations of each aspect, it's not exactly a simple one-to-one mapping. So, to get the attention vector for a specific aspect, we need to figure out how to map the attention to it. Formal: (2) Formal: (3) Formal: (4) Formal: (5) Formal: (6) Formal: (7) Formal: (8) Formal: (9) Formal: (10) Formal: (11) Formal: (12) Formal: (13) Formal: (14) Formal: (15) Formal: (16) Formal: (17) Formal: (18) Formal: (19) Formal: (20) Formal: (21) Formal: (22) Formal: (23) Formal: (24) Formal: (25) Formal: (26) Formal: (27) Formal: (28) Formal: (29) Formal: (30) Formal: (31) Formal: (32) Formal: (33) Formal: (34) Formal: (35) Formal: (36) Formal: (37) Formal: (38) Formal: (39) Formal: (40) Formal: (41) Formal: (42) Formal: (43) Formal: (44) Formal: (45) Formal: (46) Formal: (47) Formal: (48) Formal: (49) Formal: (50) Formal: (51) Formal: (52) Formal: (53) Formal: (54) Formal: (55) Formal: (56) Formal: (57) Formal: (58) Formal: (59) Formal: (60) Formal: (61) Formal: (62) Formal: (63) Formal: (64) Formal: (65) Formal: (66) Formal: (67) Formal: (68) Formal: (69) Formal: (70) Formal: (71) Formal: (72) Formal",
        "directory": "C86",
        "filename": "C86-1078.json"
    },
    {
        "casual_text": "• Delete(k): Pick a spot, k, and then randomly choose a number, m, from 1 to 6. Starting from that spot, delete m words.",
        "formal_text": "• Delete(k): for selected position k, m (randomly selected from one to six) words starting from this position are deleted.",
        "GPT2_formal_text": "(1) and (2) are the sentence-level aspect-level attention scores, which are calculated using the attention weights from the last layer of the transformer model. Formal: The final attention weight vector for the aspect-level sentence encoder, w_s, is calculated by integrating the encoder matrix W_s with the attention weights. Formal: The final attention weight vector for the sentence-level aspect-level attention model, w_s, is calculated by integrating the encoder matrix W_s with the attention weights. Formal: We’ve got the attention weights and the attention matrix for the different aspect-level sentence encoders in Figure 1. Formal: We also use the same attention weights for the aspect-level sentence encoder in our experiments. Formal: The attention weights for the sentence encoder can be thought of as the weighted average of the attention weights for the aspect-level encoder, w_s. Formal: The attention weights for the sentence-level attention model are basically the weighted average of the attention weights for the aspect-level model, w_s. Formal: Lastly, we use the attention weights for the sentence encoder to estimate the attention score of the aspect-level model, w_s, which is calculated as the weighted average of the attention weights for the aspect-level model, w_s. Formal: Figure 1 shows the attention weights for different aspects in the sentence encoder. Formal: We’ve got the attention weights and the attention matrix for the different aspect-level sentence encoders in Figure 1. Formal: We also use the same attention weights for the aspect-level sentence encoder in our experiments. Formal: Lastly, we use the attention weights for the sentence encoder to estimate the attention score of the aspect-level model, w_s, which is calculated as the weighted average of the attention weights for the aspect-level model, w_s. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.142.json"
    },
    {
        "casual_text": "There are two main types of global algorithms: generative probabilistic models and similarity-based models. Eisenstein and Barzilay (2008) think of a document as a series of parts created by hidden topic variables. Misra et al. (2011) and Du et al. (2013) have similar ideas. On the other hand, Malioutov and Barzilay (2006) and Kazantseva and Szpakowicz (2011) use similarity-based methods. Both of these approaches start with a matrix showing how similar the sentences in a document are to each other. The first one uses graph cuts to identify groups of sentences that fit together well, while the second one tweaks a clustering algorithm to do the same thing.",
        "formal_text": "Among global algorithms, we can distinguish generative probabilistic models and similarity-based models. Eisenstein and Barzilay (2008) model a document as a sequence of segments generated by latent topic variables. Misra et al. (2011) and Du et al. (2013) have similar models. Malioutov and Barzilay (2006) and (Kazantseva and Szpakowicz, 2011) use similarity-based representations. Both algorithms take as input a matrix of similarities between sentences of the input document; the former uses graph cuts to find cohesive segments, while the latter modifies a clustering algorithm to perform segmentation.",
        "GPT2_formal_text": "ed_pos is just the output of the output layer, and h_t is the hidden state of the prediction at the t-th time step. Formal: The system looks for the word w_i that gets the highest score among all the words in the sentence, based on a function f_i, from the word list. Formal: The system uses a modified version of the max-margin perceptron (Dyer et al., 2012) to figure out the scores for words w_i and its related words. Formal: We'll explain how the sentence-level model works in more detail in Section 4.2. Formal: The output layer (where the word w_i is) gets updated using a projection matrix called w_s_t, which is a linear projection of the hidden state h_t, as shown in equation (6). The output layer then predicts the vector for each word in the sentence, based on the attention mechanism. Formal: The output layer also creates a multi-layer perceptron (MLP) (Kipf and Welling, 2017) to figure out the attention scores. Formal: After the MLP makes the predictions, we pass the result through a fully connected layer, which is a linear layer that has three nonlinear parts: ReLU (Wang et al., 2018b), a ReLU activation, and tanh (Wang et al., 2018a). Formal: The MLP also predicts the attention weights for each word in the sentence, which we call h_t_i. Formal: Finally, the updated vector h_t is passed through the output layer (where the word w_i is) and the final prediction is made. Formal: Formal: This process happens in steps, following this structure: Formal: Each input word gets processed by a softmax layer, then the output layer's prediction gets passed through a linear layer, and finally a fully connected layer adds the result. Formal: Formal: This structure is pretty much the same as what you'd see in a sequence modeling approach, except instead of a simple linear layer, we use a fully connected layer with ReLU activation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C14",
        "filename": "C14-1005.json"
    },
    {
        "casual_text": "The training texts had 6580 unique words and 6945 unique tag trigram types. In the open test sentences, there were 247 words and 213 tag trigrams that weren’t in the training data. So, we needed to smooth both the part-of-speech trigram probabilities and the word output probabilities to handle texts with unknown stuff. We tested our system, which uses smoothed part-of-speech trigrams along with a word model, on these open test sentences. Table 4 shows how many words were correctly segmented and tagged. In Table 4, \"label consistency 2\" shows the accuracy of segmentation and tagging, but it doesn’t count differences in conjugation forms.",
        "formal_text": "The training texts contained 6580 word types and 6945 tag trigram types. There were 247 unknown word types and 213 unknown tag trigram types in tim open test senteuces. Thus, both part of speech trigralrl l)robabilities alld word output probabilities must be snioothed to handle open texts. We then tested the proposed system, which uses smoothed part of speech trigram with word model, on the open test sentences. Table 4 shows tile percentages of words correctly segmented and tagged. In Table 4, label consistency 2 represents the accuracy of segmentation and tagging ignoring the difference in conjugation form.",
        "GPT2_formal_text": "= {s_i, ..., s_n} to learn new entity embeddings, and this process is repeated for each token in the query. Formal: The encoder and decoder take the query and run it through a BiLSTM layer to get the entities, and the entity embeddings are then combined with the query vectors to get the final results. Formal: In the process mentioned in (1), the encoder and decoder work together to figure out the query and entity embeddings, along with the query vectors. This process is repeated for each token in the query. Formal: In the same method, the entities and entities' embeddings get combined with the query vectors. Formal: Following (6), we use a combined loss function to tweak the weights for the encoder and decoder during training. Formal: In (5), we can see that the entity embedding and the query vector embeddings are combined. Formal: In (4), the entities' and entities' embeddings get combined. Formal: In (3), the entity embedding and the query vector embeddings are combined. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C94",
        "filename": "C94-1032.json"
    },
    {
        "casual_text": "Wikipedia version: enwiki-20190301. The spaCy model is called \"en_core_web_md\" and it's version 2.1.8. It was trained using the OntoNotes dataset.",
        "formal_text": "Wikipedia version: enwiki-20190301. 5 The spaCy model is \"en core web md\" in version: 2. 1. 8, which trained on OntoNotes dataset.",
        "GPT2_formal_text": "is a vector that represents the attention mask. The final attention weight vector for the source sequence is written as a sum of the attention weights, which we call A_s(f, a) = (a_s(f, f_1) + ... + a_s(f, f_M)) for each source word f_i, where M is the maximum number of words in the source sentence. Formal: A_s(f, a) = a_s(f, f_1) * a_s(f, f_2) ... a_s(f, f_M) Formal: Basically, it's the sum of the attention weights that go with each word in the target sentence, and each attention weight is a vector that represents its attention mask. Formal: Formal: Formal: Formal: Formal: We need to know the most likely sequence of words that are the most likely to change the meaning of the target word. This can be done using a maximum likelihood method. Formal: Formal: Formal: Formal: In this paper, we're mainly focusing on permutation-based models. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eacl",
        "filename": "2021.eacl-main.251.json"
    },
    {
        "casual_text": "Alright, so W(s) and b(s) are the parameters of this feed-forward neural network. To make sure we really understand the syntax, position, and other connections between a zero pronoun and its possible candidates, we add some extra features (v(f e)) that we handpick and feed into our neural network model. We take these features from previous research on zero anaphora resolution (like Chen and Ng, 2013; Chen and Ng, 2016) and turn them into vectors to calculate a score that helps us figure out the best match between the zero pronoun and its candidate.",
        "formal_text": "], W (s) and b (s) are the parameters of this feed-forward neural network. In addition, to better capture the syntactics, position and other relations between an anaphoric zero pronoun and its candidates, we encode hand crafted features (v (f e) ) as inputs to our neural network model. We utilize the features from exsiting work on zero anaphora resolution (Chen and Ng, 2013; Chen and Ng, 2016), map them into vectors to estimate the resolution score for the zero pronoun-candidate mention pair as:",
        "GPT2_formal_text": "\", \"table_rows\", \"table_cols\", \"table_rows\", \"table_cols\", \"col_ids\", \"col_rows\", \"col_cols\", \"labels\", \"labels\", \"bias_ids\", \"bias_rows\", \"bias_cols\", \"bias_cols\", \"min_id\", \"max_id\", \"min_label\", \"max_label\", \"cluster_id\", \"cluster_rows\", \"cluster_cols\", \"cluster_cols\", \"cluster_cols\", \"dist_id\", \"dist_rows\", \"dist_cols\", \"dist_cols\", \"parent_ids\", \"parent_rows\", \"parent_cols\", \"parent_cols\", \"attributes\", \"attributes\", \"relation\", \"relation\", \"precision\", \"precision\", \"recall\", \"recall\", \"quality\", \"quality\", \"weighted\", \"weighted\", \"mean\", \"mean\", \"score\", \"score\", \"predict\", \"predict\", \"predict_cols\", \"predict_cols\", \"predict_cols\", \"max_predict\", \"max_predict\", \"max_pred_predict\", \"mean_predict\", \"mean\", \"score_predict\", \"recall_predict\", \"recall\", \"averages\", \"averages\", \"mean\", \"score\", \"predict\", \"predict_cols\", \"predict_cols\", \"predict_cols\", \"max_predict\", \"max_pred_predict\", \"mean_predict\", \"mean\", \"score_predict\", \"recall\", \"average\", \"average\", \"average\", \"avg_predict\", \"avg_predict\", \"avg_pred_predict\", \"avg_pred_predict\", \"avg_pred_pred_predict\", \"avg_pred_pred_predict\", \"avg_pred_pred_predict\", \"avg_pred_predict\", \"avg_pred_predict\", \"avg_pred_predict\", \"avg_pred_predict\", \"avg_pred_predict\", \"avg_pred_predict\", \"avg_pred_predict\", \"av",
        "directory": "C18",
        "filename": "C18-1002.json"
    },
    {
        "casual_text": "We also checked out DialoGPT, which is a model based on GPT-2 and was trained on a huge collection of dialog data. We went with the small version 4 for our evaluation.",
        "formal_text": "DialoGPT We additionally evaluated DialoGPT, which is a GPT-2-based model pre-trained on large-scale dialog corpora. We used the small version 4.",
        "GPT2_formal_text": "_x, which is like a key-value store for the text, is written like this:\n             Casual: The key and value are basically numbers that represent the important parts of the text (like its start and end points). We take the average of these numbers, which gives us the similarity score between the text and the image. Formal: To create the representation for each piece of text, we figure out how similar it is to the image using the formula: Formal: Check out Figure 1 for the setup of our attention model. Formal: The input is a sequence of vectors, where each one represents a part of the text. We use a hidden layer to grab the hidden info from the whole sequence, and then we pass this to a softmax function to get the final representation. Formal: We calculate the similarity score between the text and the image by averaging the scores from the vector embeddings, which gives us a similarity score between 0 and 1. Formal: We use a logistic model to figure out the probability p(t|e) of picking the right answer token t at each step t. Formal: The embeddings for the tokens t and e are fed into a softmax layer to get the probability. Formal: To get the prediction p(t|e), we use a Logistic Regression (LR) with a linear layer on the token embeddings. Formal: Finally, we add the final prediction to the total score, which gives us the final score for the whole text or image. Formal: The formula for attention is: Formal: Figure 1 shows how we calculate the similarity score between the input text and the image. Formal: We use a distance-based softmax layer to calculate the similarity score between the tokens t and e. Formal: We add the final prediction to the overall score, which gives us the final score for the whole text or image. Formal: The model is built using PyTorch (Paszke et al., 2019). Formal: We tested it on the Transformer-based encoder-decoder setup (Vaswani et al., 2017), which means we trained it on the original Transformer model. Formal: We ran it on the full version of the dataset. Formal: For each token t, we calculate the similarity score by averaging the scores from the embeddings of the tokens t and e. Form",
        "directory": "acl",
        "filename": "2021.acl-long.269.json"
    },
    {
        "casual_text": "If an entity in the dev/test data has already appeared in the training data, we call it a \"seen entity.\" If it hasn’t, it’s an \"unseen entity.\" To figure out how much of the dev/test data’s entities have been seen in the training data, we use something called the \"entity coverage ratio.\" This ratio for a dataset D te is written as r(D te ), and it’s calculated using the equation below.",
        "formal_text": "If an entity in dev/test data has been covered by the training data, we refer it as a seen entity. Otherwise, it is an unseen entity. To quantify the degree to which entities in the dev/test data have been seen in the training data, we define a measurement called entity coverage ratio. The entity coverage ratio of data D te is denoted by r(D te ), which is calculated using the below equation.",
        "GPT2_formal_text": ". We're using the same setup as in HATM and FastBERT, which has a base model size of 64 and a batch size of 128. If you want to check out the different models, they're all available in Appendix A. Formal: We're using the trainable parameters α_t from the Moses toolkit (Koehn et al., 2007), with a learning rate of 5e-5. We've set the batch size to 128, and the learning rate to 5e-4. To save time, we're testing on 10-fold cross validation on the development set, which has the same size as the validation set. Formal: The BERT model is the best performer so far, scoring over 0.9 BLEU points in the evaluation for both English and German. Formal: We've tested our framework on a few popular NER datasets and found that it works great with just a few fine-tuned models. Formal: To see how well a single model can handle different languages, we've built a multilingual model using BERT and other models. Formal: We've also shared results from testing on a few different NER datasets, and it turns out that our framework performs better on the English dataset compared to the other three. Formal: We've set up and tested our framework on three different NER tasks and reported results. Formal: We've also shared results for three different NER tasks and reported results. Formal: We've also shared results for three different NER tasks and reported results. Formal: We've set up and tested our framework on three different NER tasks and reported results. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-short.125.json"
    },
    {
        "casual_text": "In Section 5, we’ll find out that our exact algorithm is usually way too heavy on the computer when dealing with a lot of sentences or a lot of features. So, now we’re going to introduce two",
        "formal_text": "We will see in Section 5 that our exact algorithm is often too computationally expensive in practice to be used with either a large number of sentences or a large number of features. We now present two",
        "GPT2_formal_text": "ed_u and attention_masked_v are set to 1.0 and 0.9, respectively. Formal: • F = {r}^(D * G)^2 | D * G = {D}^n. This means the attention vectors in a sentence are usually somewhere between r and r^2. Formal: • r is the attention output in the DATR grammar, and r^2 is the attention output in the GAT grammar. Formal: • If r = r^2, then: • If the sentence is full, that means the sentence embedding is already made using the attention mechanism. Formal: • Otherwise, the sentence embedding is made using the GAT grammar, and the attention weight vector is calculated using a linear function. Formal: • We assume the sentence is monotone, meaning there's only one attention input for the sentence. Formal: • We also assume the sentence is multi-turn, meaning there are two attention inputs for each turn. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D11",
        "filename": "D11-1004.json"
    },
    {
        "casual_text": "We've expanded this architecture to work in a multi-view way, where we handle both the context and the response from two different angles. Here, we'll start by giving a quick overview of the word sequence model. After that, we'll dive into the details of the utterance sequence model and the multi-view response selection model.",
        "formal_text": "We extend this architecture in a multi-view manner, which jointly models the context and response in two views. In this section, we first briefly describe the word sequence model. Then we introduce the utterance sequence model and multi-view response selection model in details.",
        "GPT2_formal_text": "= {q, t} | l = (1, 2, ..., n-1)           Casual: Alright, so q_t_i represents the index of the t-th word in the i-th row. We use the 1-best training data to create the final model. Formal: For training, we grab the best model based on the cross-entropy loss. Formal: We calculate the cross-entropy loss for each training example using the cross-entropy loss for a specific dataset, where i goes from 1 to n. Formal: We only consider the top-k examples, which are the top-k results from the official test set (with a 5% margin). Formal: We set the training time step to 2. Formal: We pick the best model based on the cross-entropy loss and the validation loss, and the performance metric we calculated. Formal: After training, we get the model's embedding vector, which we call f_i, by averaging the cross-entropy loss across the whole validation set. Formal: After validation, we calculate the cross-entropy loss for each training example using the cross-entropy loss for the validation set, where i goes from 1 to m. Formal: Formal: We use the γ-loss to optimize the training objective, and the weight λ_t for the loss. Formal: Formal: We calculate the cross-entropy loss for each training example using the cross-entropy loss for the validation set, where i goes from 1 to m. Formal: Formal: Formal: We can simplify things by using a uniform distribution with a probability λ_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "We can use the idea that when you multiply two matrices, P and Q, the rank of the resulting matrix is no bigger than the smallest rank of P or Q. So, rank(P Q) ≤ min rank(P), rank(Q). This means we can figure out the upper limit for rank(T) in equation (4) based on that.",
        "formal_text": "We utilize the fact that the rank of product of two matrices P and Q is upper bounded by the minimum of rank(P) and rank(Q), i. e. , rank(P Q) ≤ min rank(P), rank(Q). Thus, the upper bound on rank(T) in eq. (4) can be determined by",
        "GPT2_formal_text": "_len is the number of word embedding units in the first layer, and the Transformer output layer is d_T_1 * n_T_1. Formal: For the word embedding layer, we use the dot product of the hidden state from the Bi-LSTM and the output from the RNN. This gives us the word representation h_w_1_t. Formal: Lastly, we combine the hidden state of the Bi-LSTM with the output from the RNN and the vector representation h_w_2_t to get the final hidden state representation h_t_1. Formal: We can easily extend this process to any multi-head attention mechanism by using the same setup. Formal: Here, we’ve outlined the process for creating the representation h_w_1_t for the word embedding layer in Figure 2. Formal: After generating h_w_1_t, we map it to the hidden state vectors of the RNN using the Bi-LSTM hidden state. The Bi-LSTM then uses its output from the RNN to calculate the attention weights for the word embedding layer. Formal: The Bi-LSTM hidden state vector h_w_1_t is just a combination of the hidden state vector from the Bi-LSTM and the word embedding vector h_w_2_t. We’ve also included an example in Figure 3 to show how this works. Formal: In this project, we’re taking inspiration from the multi-head attention mechanism used in image captioning (like the one by Wang et al., 2015) and tweaking it for text. Formal: The Bi-LSTM hidden state vector h_w_1_t is based on the representation of the word embedding h_w_2_t, which is the representation of the Bi-LSTM itself. Formal: The output of the RNN, which is the vector representation h_t_1, is calculated by combining the hidden state vectors from the Bi-LSTM and the word embedding vectors h_w_2_t. Formal: Lastly, we use the output from the RNN to calculate the attention weights for the word embedding layer. Formal: The final hidden state representation h_t_1 is the result of combining the hidden state vectors of the Bi-LSTM and the word embed",
        "directory": "acl",
        "filename": "2021.acl-long.94.json"
    },
    {
        "casual_text": "The main goal of D-MILN's final objective function is a mix of document-level loss and some extra regularizations to keep things diverse. To keep it simple, let's just talk about the objective function for one document:",
        "formal_text": "The final objective function of D-MILN is a combination of document-level loss and diversified regularizations. To minimize clutter, we describe the objective function for a single document:",
        "GPT2_formal_text": "= { (t_i, t_j) in t} from i=1 to n-1. Formal: The model tweaks based on the input tokens, using the embedding vector {(x_i, y_i)} and the output from the CRF layer. Formal: The model calculates the embedding for each token by averaging the embeddings from the CRF layer. Formal: To keep things simple and clear, we’ll just call these embeddings \"E_t.\" Formal: The model starts with a sequence of tokens that has the label \"e_1.\" Formal: The model starts with a sequence of tokens labeled \"e_2.\" Formal: The model starts with a sequence of tokens labeled \"e_3.\" Formal: The model starts with a sequence of tokens labeled \"e_4.\" Formal: The model starts with a sequence of tokens labeled \"e_5.\" Formal: The model starts with a sequence of tokens labeled \"e_6.\" Formal: The model starts with a sequence of tokens labeled \"e_7.\" Formal: The model starts with a sequence of tokens labeled \"e_8.\" Formal: The model starts with a sequence of tokens labeled \"e_9.\" Formal: The model starts with a sequence of tokens labeled \"e_10.\" Formal: The model starts with a sequence of tokens labeled \"e_11.\" Formal: The model starts with a sequence of tokens labeled \"e_12.\" Formal: The model starts with a sequence of tokens labeled \"e_13.\" Formal: The model starts with a sequence of tokens labeled \"e_14.\" Formal: The model starts with a sequence of tokens labeled \"e_15.\" Formal: The model starts with a sequence of tokens labeled \"e_16.\" Formal: The model starts with a sequence of tokens labeled \"e_17.\" Formal: The model starts with a sequence of tokens labeled \"e_18.\" Formal: The model starts with a sequence of tokens labeled \"e_19.\" Formal: The model starts with a sequence of tokens labeled \"e_20.\" Formal: The model starts with a sequence of tokens labeled \"e_21.\" Formal: The model starts with a sequence of tokens labeled \"e_22.\" Formal: The model starts with a sequence of tokens labeled \"e_23.\" Formal: The model starts with a sequence of",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.570.json"
    },
    {
        "casual_text": "We can use the hidden representations H, which are based on individual tokens, to make predictions for the entire document.",
        "formal_text": "The token-wise hidden representations H can then be used to make prediction based on the whole document.",
        "GPT2_formal_text": "_rnn * [h_i, y_i] = ReLU (y_i) + tanh(x_i * h_i) * [x_i, y_i]^2 - ReLU(h_i) + tanh(x_i * h_i). Formal: The length-2RNN model works pretty much the same as the regular attention-based model, but it doesn't pay attention to the length of the input. Instead, it just focuses on the first word of the input. The length-1RNN model, on the other hand, pays attention to both the length of the input and the length of each character in the input. Formal: To find the attention weights, we use the IOBES tagging algorithm to measure how important each word in the input is. Formal: The attention weights in our model are calculated by looking at the character-level attention from the input. Formal: For encoding the word representations, we take the average of the character embeddings, and the final representation is a combination of all these embeddings. Formal: The attention weights for the attention-based model can also be calculated using the IOBES tagging algorithm, but it doesn't take character embeddings into account. Formal: The attention weights for the attention-based model are calculated using the IOBES tagging algorithm, but it doesn't take character embeddings into account. Formal: We set the number of dimensions for the attention-based model to 10 because we want to avoid really long word embeddings from messing things up, especially if we're decoding the text. Formal: The attention weights for the attention-based model are calculated by looking at the character-level attention from the input. Formal: To get the final representation for the text, we use the combination of the character embeddings, and the final representation is a combination of all these embeddings. Formal: The final attention weights for the attention-based model are calculated using the IOBES tagging algorithm. Formal: Finally, the attention weights for the attention-based model are calculated using the IOBES tagging algorithm. Formal: Formal: In Figure 1, you can see an example of a text embedding. Formal: The attention weights for the attention-based model are calculated using the IOBES tagging algorithm. Formal: The final attention weights for the attention-based model are calculated using the",
        "directory": "clinicalnlp",
        "filename": "2022.clinicalnlp-1.2.json"
    },
    {
        "casual_text": "Summarizing opinions and arguments has been a hot topic lately. People are really into breaking down what’s being said in reviews (like Amplayo and Lapata, 2021, or Elsahar et al., 2021) or in argumentative texts (Wang and Ling, 2016; Syed et al., 2020). KPA is part of this trend, but it brings in a more numerical approach to show how opinions are spread out in the data being analyzed. If you want to dive deeper into how KPA connects to argument clustering and summarization, check out Bar-Haim et al. (2020a, b).",
        "formal_text": "Opinion and argument summarization. Recently there has been an increasing interest in summarizing opinions expressed in various reviews (Amplayo and Lapata, 2021; Elsahar et al. , 2021) or argumentative text (Wang and Ling, 2016; Syed et al. , 2020). KPA contributes to this line of work by adding a quantitative dimension which reflects the distribution of opinions in the examined data. For a detailed discussion of the relation between KPA and argument clustering and summarization see (Bar-Haim et al. , 2020a, b).",
        "GPT2_formal_text": "(s_d) in the target text. This is shown in equation (5). Formal: Basically, we're looking at how words in the target text are grouped together based on their word embedding vector. To do this, we calculate the softmax average of these word embeddings for each word in the target text. Formal: We use a way to figure out how similar two words are in terms of meaning. We do this by doing a convolutional neural network (CNN) on the word embeddings. Formal: We use a way to measure how related two words are, based on cosine similarity. We do this by doing a convolutional neural network (CNN) on the word embeddings. Formal: We use a CNN to figure out how related two words are. Formal: The word embedding vectors are created using two CNNs. Formal: The word embedding vectors for the target word w_i are calculated using two CNNs and then averaged across the whole target vocabulary. Formal: The word embedding vectors for the target word w_i are calculated using two CNNs and then averaged across the whole target vocabulary. Formal: The original English target vocabulary is pulled from a big collection of text, which we call C. Formal: We use a CNN to figure out how related two words are. Formal: The word embedding vectors for the target word w_i are calculated using two CNNs and then averaged across the whole target vocabulary. Formal: We use a CNN to figure out how related two words are. Formal: The word embedding vectors for the target word w_i are calculated using two CNNs and then averaged across the whole target vocabulary. Formal: We use a CNN to figure out how related two words are. Formal: The original English target vocabulary is pulled from a big collection of text, which we call C. Formal: We use a CNN to figure out how related two words are. Formal: The word embedding vectors for the target word w_i are calculated using two CNNs and then averaged across the whole target vocabulary. Formal: We use a CNN to figure out how related two words are. Formal: The original English target vocabulary is pulled from a big collection of text, which we call C. Formal: We use a CNN to figure out how related two words are. Formal: The word embedding vectors for the target word w_i are calculated using",
        "directory": "argmining",
        "filename": "2021.argmining-1.16.json"
    },
    {
        "casual_text": "GPU was first used in topic modeling back in 2011 by Mimno et al. They used it to focus on words that appeared together in the same documents a lot, based on how often they showed up together in the whole dataset. Later, in 2013, Chen et al. used GPU to handle a problem in topic modeling where prior knowledge about a domain could mess things up. They did this by boosting the importance of rare words in the knowledge sets. But even with these improvements, they were still just working with individual words.\n\nMost topic models, like LDA, treat topics as a mix of single words and assume that the order of words doesn’t really matter. However, some researchers have tried to include word order by using n-gram language models. For example, Wallach in 2006 came up with the Bigram Topic Model (BTM), which combines bigram statistics with topic modeling to better represent documents. Then, Wang et al. in 2007 took it a step further with the Topical N-gram Model (TNG), which is like a more advanced version of BTM. It generates words in order by first picking a topic, then deciding if it’s a unigram or bigram, and finally selecting the word from a distribution that’s specific to the topic and whether it’s a unigram or bigram.\n\nEven though the \"bag-of-words\" assumption, which means ignoring word order, isn’t always perfect in real-world applications, it’s still much faster and easier to work with than more complex models that try to account for word order when finding important n-grams.\n\nOur approach is a bit different from these earlier works in two main ways. First, we stick with the \"bag-of-words\" or more accurately, the \"bag-of-terms\" assumption.",
        "formal_text": "GPU was first introduced to topic modelling in (Mimno et al. , 2011), in which GPU is used to concentrate words with high co-document frequency based on corpus-specific co-occurrence statistics. Chen et al. (2013) applied GPU to deal with the adverse effect of using prior domain knowledge in topic modeling by increasing the counts of rare words in the knowledge sets. However, these works still use only individual words. Topics in most topic models like LDA are unigram distributions over words and assume words to be exchangeable at the word level. However, there exists some work that tries to take word order into consideration by including n-gram language models. Wallach 2006proposed the Bigram Topic Model (BTM) which integrates bigram statistics with topic-based approaches to document modeling. Wang et al. (2007) proposed the Topical N-gram Model (TNG), which is a generalization of the BTM. It generates words in their textual order by first sampling a topic, then sampling its status as a unigram or bigram, and then sampling the word from a topic-specific unigram or bigram distribution. Although the \"bag-of-words\" assumption does not always hold in real-life applications, it offers a great computational advantage over more complex models taking word order into account for discovering significant ngrams. Our approach is different from these works in two ways. First, we still follow the \"bag-of-words\" or rather \"bag-of-terms\" assumption.",
        "GPT2_formal_text": "er_t: We pick the k-th word from the target language (LM) that has the smallest contribution to the generation. The other words that are close to the translated word are considered as potential candidates. The prediction is made based on how the model behaves during decoding. Formal: We use a transformer model, trained on the English Wikipedia, to predict the probability of a word from the source language (SL). The probability is calculated by multiplying the log likelihood of the word with the probability of the target language (TL), which is calculated by multiplying the log likelihood of the source language (SL) with the log likelihood of the target language (TL). Formal: The translation probability is calculated by adding up the probabilities of the target language (TL) and the source language (SL), and then multiplying them by the log likelihood of the source language (SL) and the log likelihood of the target language (TL). The probability for the target language (TL) is determined by the source language (SL) probabilities, which are based on the target language's probability distributions. Formal: To train the model, we train it using a cross-entropy loss and a translation error rate, which we got from the development set. Formal: The training process helps the model learn how to predict the probability of a word in the source language (SL). Formal: During training, we use stochastic gradient descent to make the model better at figuring out the probability of a word and its translation. Formal: We train the model on an MLP generator, which gives us a probability distribution for the target language (TL), and we also calculate the cross-entropy loss to get the probability distribution for the source language (SL). Formal: For each word in the source language (SL) and the target language (TL), we calculate the cross-entropy loss to get the probability distributions for the source and target languages. Formal: The source and target language probabilities are adjusted by a linear mapping θ_S that takes into account the combined probability of the source and target language distributions. Formal: After training, the model is tested on a supervised task to predict how likely a word is to be translated. Formal: Finally, we show how this predictive model works in practice by using it to predict how likely a sentence is to be translated. Formal: We use a parameter-free version of the Softmax function (softmax(x)) that works with vectors. Formal: Formal: We",
        "directory": "C14",
        "filename": "C14-1063.json"
    },
    {
        "casual_text": "When we look at the bilingual transfer learning models (check Table3 (2)) compared to the Arabic-only models (Table3 (1)), we see that using MBERT gives us an average F1 improvement of 3, and GIGABERT gives us 2.5. This proves that bilingual transfer learning works pretty well. Both the PIPELINE and JOINT approaches show similar improvements, which means it's the extra training data, not the model itself, that makes the difference. \n\nWhat's cool is that MBERT and GIGA-BERT give us pretty much the same gains, showing that both models transfer knowledge from English to Arabic equally well. Even though MBERT starts a bit behind in Arabic performance (by 15.29 F1), it catches up. It's kind of unexpected since GIGABERT is a bilingual model made just for English-Arabic. We're definitely going to look into this next.",
        "formal_text": "On bilingual transfer learning: Comparing the bilingual transfer learning models (Table3 (2)) to the Arabic-only models (Table3 (1)), we observe 3 and 2. 5 average F1 improvements when using MBERT and GIGABERT respectively. This shows the effectiveness of the bilingual transfer learning. The PIPELINE and JOINT approach shows similar gains. This indicates that it is the additional training data rather than the learning model that leads to the gain. Interestingly, using MBERT and GIGA-BERT gives similar gains, which shows that the transferability of the these two models in English to Arabic is about the same (though with MBERT lagging in baseline Arabic performance by 15. 29 F1). It is surprising given that GIGABERT is a bilingual model trained specifically for English-Arabic. We plan to investigate this as an immediate next step.",
        "GPT2_formal_text": ".py C = {u1, ..., uk, i} i=1:n where i is from 1 to n. Formal: You can tweak the learning rate λ with some smart stuff, like the logarithm, when training the model. Formal: A lot of neural networks are set up to figure out the best sequence of label sequences, so they use something called maximum likelihood estimation. This usually involves sampling from a uniform distribution p(y|x, θ) from a Dirac delta distribution, which was introduced by Dirac in 1948. Formal: To get a better understanding of the model's parameters, you can look at its mean squared error (MSE) or the variance of the probability p(y|x). Formal: For two unknown numbers q and r, you can check if they're part of a uniform distribution p(q|x), which was introduced by Sundararajan and his team in 2009. Formal: There's also a way to check if a number is part of a uniform distribution p(q|x), which was introduced by Vijayakumar and his team in 2012. Formal: Also, remember that the expectation p(q, y|x) is the same as the standard expectation p(y, x), where p(q, y) is the expected probability of the sequence y given the given sequence x. Formal: Formal: There's also a way to check if a number is part of a uniform distribution p(q|x), which was introduced by Sundararajan and his team in 2009. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "crac",
        "filename": "2021.crac-1.10.json"
    },
    {
        "casual_text": "To figure out how premises connect to the conclusion, we first need to pinpoint the targets in the premises. This whole idea of finding target phrases in argumentative text was brought up by Bar-Haim et al. (2017). Here, we're dealing with it using BIO sequence labeling, which means we classify each word as the start, part of, or not part of a target. Since identifying premise targets isn't our main goal, we just grab a top-notch neural sequence tagger (from Akbik et al., 2018) and train it on a claim stance dataset. Then, we use it to automatically tag targets in all the input premises.",
        "formal_text": "To model the relation between premises and conclusion target, we first identify the premises' targets. The task of identifying target phrases in argumentative text has been introduced by Bar-Haim et al. (2017). We here tackle it as BIO sequence labeling, classifying each token as being the beginning, inside, or outside of a target. Since premise target identification is not our main focus, we simply train a state-of-the-art neural sequence tagger (Akbik et al. , 2018) on the claim stance dataset and then use it to automatically annotate targets in all input premises. 2",
        "GPT2_formal_text": "_predict(h_t) = [P_mask(t_1) | P_mask(t_2) | P_mask(t_3)]. Formal: This part just gives a summary of the task, showing what each model is doing and how they're related to each other. Formal: Here, we're using the model parameters and a learning rate to find the best solution. Formal: Once we've got the model's parameters and a learning rate, we stick them together to create the final output. Formal: To make sure the model works well with different datasets, we train it on all of them. Formal: We tweak the model's parameters by feeding them into a feed-forward neural network (FFNN) to create a parameter vector for each dataset. Formal: After that, we tweak the model's parameters by giving it values for both the feature maps and the feature weights in a discriminative way. Formal: To make sure the model's parameters stay consistent, we train it with a mix of different types of data. Formal: The final output is a mix of the model's parameters, the features it's using, and the weights it's learned for each dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.399.json"
    },
    {
        "casual_text": "For the pseudo-disambiguation task, the test set is made up of tuples like (R, t, r, r'). To create this test set, we followed the same approach that Rooth et al. (1999) and Erk et al. (2010) used.",
        "formal_text": "A test set for the pseudo-disambiguation task task consists of tuples of the form (R, t, r, r'). To construct a test set, we adapted the process used by Rooth et al. (1999) and Erk et al. (2010).",
        "GPT2_formal_text": "\": [ \"E_p_t\", \"E_p_s_t\", \"E_p_e_t\", \"E_p_p_t\", \"E_p_p_s_t\", \"E_p_p_e_t\", \"E_p_s_p\" ], function(e, s, t) { return arg(m) - arg(n) + arg(q) + 1; } )\n               Casual: The final result is a bunch of values that are all the same (like in (7)), but they're encoded differently (like in (6)). Formal: We'll break down the whole process into two steps: first, we'll figure out how to represent all the possible entity pairs using methods like HAPS (Hierarchical Attention Separate Representation) and Seq2Seq (Seq2Seq), and then we'll connect each pair to a feature vector to figure out which combination works best for each pair. Formal: This part is a bit more tricky, but it's about how to pick the right features to make the final prediction. Formal: After we've found all the entity pairs, we'll combine all their feature vectors to get a final output. Formal: This part is super important, and it's all about choosing the right set of features and the right combination. Formal: You can find more details about the whole process and examples of how it all works in (7). Formal: In the rest of this paper, we'll talk about the models we used, the data we used, and how we set up our experiments. Formal: Let's quickly go over the important stuff we've covered so far. Formal: We used the TAC KBP dataset (Liu et al., 2013), which is a big collection of biomedical papers with titles and abstracts pulled from PubMed. Formal: We took the best entity embedding from the CNN/Daily Mail dataset (Chen et al., 2016) and used it to calculate two scores: one called Equation 1, which is for embedding the entity pair, and another called Equation 2, which is for embedding the entity and its related entity. Formal: After that, we used the entropy-based E2E model (Bordes et al., 2017) to find the best model for the entity pair in the pair.",
        "directory": "E12",
        "filename": "E12-1038.json"
    },
    {
        "casual_text": "In this part, we’ll dive deeper into the topic with both numbers and descriptions, plus take a closer look at the mistakes and the stuff that’s still tricky.",
        "formal_text": "In this section, we further provide more insights with quantitative and qualitative analysis, as well as error analysis for the remaining challenges.",
        "GPT2_formal_text": "_t and the attention function H(t). The whole attention operation is laid out in Figure 1. Formal: For each word w_i, there's a bunch of possible options: Formal: We start by getting the parameters h_i, h_j, and h_k from the input G, using the standard REINFORCE algorithm. Formal: We use a bidirectional GRU (BiGRU) to figure out the attention value: Formal: We also use a non-linear attention mechanism (H(t)) to figure out the attention weight, H(t). Formal: Finally, we add a linear layer to tweak the whole thing using a linear transformation. Formal: The main thing we're doing is a reinforcement learning (RL) thing, with a learning rate decay set at 0.01. Formal: Our main goal is to maximize the log likelihood, which basically means we want to maximize the log likelihood of the correct label y_i. Formal: We also use a loss function to improve the performance of our model. Formal: The whole process of handling lexical ambiguity is explained in more detail in Section 4.3. Formal: We've got a binary classifier that learns the true label y_i, which is the one we're looking for. Formal: For the first layer of our model, we take the hidden representation h_1 and multiply it by the log probability of the correct label y_1. Then, we use the negative log probability of y_1 to get the loss, which we call L_y_i. Formal: The loss is calculated using equation (5), with the gradient calculated using equation (3). Formal: After that, the second layer in our model adds some linear layers to create the final output, which we call the attention representation H(t). Formal: So, H(t) = (h_1 + h_2 + ... + h_n) * log(P_y_i / P_y_i). Formal: For this process, we call the loss L_y_i. Formal: Finally, we get the attention representation H(t) by using the log probability of the correct label, which we call L_y_i. Formal: In our experiments, we set the learning rate to 0.01. Formal: Finally, we combine all this together into a single process that creates a final output,",
        "directory": "acl",
        "filename": "2022.acl-long.361.json"
    },
    {
        "casual_text": "In this part, we're testing a language model (LM) trained with a large-margin criterion alongside a strong Chinese-to-English neural machine translation (NMT) system. The NMT model was trained on 2 million parallel sentence pairs. Following the approach of Shen et al. (2016), we used the NIST 06 newswire section (616 sentences) for development and the NIST 08 newswire section (691 sentences) for testing. We trained the model using the OpenNMT-py 2 package with its default settings: batch size is 64, word embedding size is 500, dropout rate is 0.3, target vocabulary size is 50K, and we trained for 20 epochs. After that, we got a minimum development perplexity of 7.72.",
        "formal_text": "In this section, we experiment the large-margin criterion trained LM with a competitive Chineseto-English NMT system. The NMT model is trained from 2M parallel sentence pairs. Following Shen et al. (2016), we use NIST 06 newswire portion (616 sentences) for development and NIST 08 newswire portion (691 sentences) for testing. We use OpenNMT-py 2 package with the default configuration to train the model: batch size is 64; word embedding size is 500; dropout rate is 0. 3; target vocabulary size is 50K; number of epochs is 20, after which a minimum dev perplexity of 7. 72 is achieved.",
        "GPT2_formal_text": "= { q, c, a } ∈ Rn×d and λ_c = 0.9. To get a better understanding of how noisy the dialog acts are, we use the \"noisy acts\" label on the dialog acts. This label helps us understand how often dialog acts in different domains are similar. If the number of noisy acts is more than 1/2 of the total, the labeled dialog acts are basically the same. Figure 1 has a plot showing the percentage of dialog acts labeled \"noisy\" on the test set (basically, the ones that can be different depending on the domain) for domains like NewsQA, DailyDialog, and DialogueBank. Formal: We’ve picked some examples of dialog acts from the test set (check out Figure 1) and labeled them with the \"noisy acts\" label. The figure shows how often dialog acts in different domains are similar to each other. Formal: Using these examples, we’ve come up with a new way to measure how similar dialog acts are. Formal: For each domain, we’ve tagged the dialog acts with the \"noisy acts\" label, which means these acts are basically different in every domain. Formal: We’ve checked the \"noisy acts\" label on the test set and compared it to the labels for each domain. The figure shows how often dialog acts are similar in each domain. Formal: In this section, we’re focusing on domain-specific dialog acts that aren’t related to any specific domain. Formal: We’ve added these dialog acts to the dataset D_h. Formal: The dataset D_h has dialog acts for each domain. Formal: We’ve collected dialog acts for this dataset. Formal: The number of dialog acts in D_h includes all dialog acts across all domains. Formal: Formal: Formal: We’ve also listed the dialog acts we’ve labeled in Table 1. Formal: We’ve included the new dataset D_h in the list of datasets we’ve added to this research. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1150.json"
    },
    {
        "casual_text": "Hey, just a heads-up: in most SENSEVAL datasets, they usually give you separate training and test data. But for some target words in SENSEVAL1, they didn't have both. So, we decided to mix the training and test data together into one big dataset and run 10-fold cross validation instead.",
        "formal_text": "Note that, although generally in the SENSEVAL datasets, training and test data are provided separately, a few target words from SENSEVAL1 do not have both training and testing data. Thus, we opted to combine the training and test data into one dataset, and then perform 10-fold cross validation experiments.",
        "GPT2_formal_text": "ed |~ T = [t i, t j, ..., t m ] T = [t i+1, ..., t j+m ] t i = [t i+1, ..., t j ] | w i, ..., w j, ..., w k ] = [t i+1, ..., t j ] + [t i+1, ..., t j+m ] + [t i+1, ..., t j+m ] i = [t i+1, ..., t j+m ] = [t i+1, ..., t j+m ] (2) Formal: Also, the attention weight vector [t i, t j, ..., t m ] T = [t i+1, ..., t j ] + [t i+1, ..., t j+m ] T = [t i+1, ..., t j+m ] + [t i+1, ..., t j+m ] Formal: The attention weight vector [t i, t j, ..., t m ] T = [t i+1, ..., t j ] + [t i+1, ..., t j+m ] Formal: The attention weight vector [t i, t j, ..., t m ] T = [t i+1, ..., t j ] + [t i+1, ..., t j+m ] Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D09",
        "filename": "D09-1020.json"
    },
    {
        "casual_text": "We worked with the data from the shared task on identifying offensive language in Dravidian languages, organized by Chakravarthi and others in 2021 (and also mentioned in their earlier works in 2020). The data was nicely annotated at the comment or post level. The training data included 35,139 samples for Tamil, 16,010 for Malayalam, and 6,217 for Kannada. You can check out the details in Table 1, Table 2, and Table 3.",
        "formal_text": "We used the data provided by the organizers of shared task on Offensive Language Identification in Dravidian Languages (Chakravarthi et al. , 2021 (Chakravarthi et al. , , 2020b Hande et al. , 2020; Chakravarthi et al. , 2020a), which have been annotated well at comment or post level. The numbers of Tamil, Malayalam and Kannada training data are 35139, 16010 and 6217, respectively. The statistics of data are shown in Table 1, Table 2 and Table 3.",
        "GPT2_formal_text": "= {r_p_1, ..., r_p_i} and the embedding for the word w_i, which is just the input word embedding. Formal: Using the same set of embeddings, we calculate a score for a document d_i by averaging the scores of its sentences. We do this for all the documents in the dataset D. Formal: The difference between the sentence-level MLE and the word-level MLE can be calculated like this: Formal: The MLE score for the document d_i is calculated using the MLE score for the sentence s_i, which is also calculated as the MLE score for the word w_i, which is also the output word embedding. Formal: The MLE score for a document d_i is determined by the MLE score for its sentences, which are the embedding values of the sentences in the document. Formal: The MLE score for a document d_i can be found using the MLE score for the sentence s_i. Formal: The MLE score for a document d_i is found by using the MLE score for the word w_i, which is also the output word embedding. Formal: In our experiments, we set the document embedding size to 100, which is the same as what's used in the LSTM-based model by Wang et al. (2015). Formal: We use an attention mechanism to create embeddings for each sentence in the document d_i. Formal: We combine the embeddings from the sentence-level MLE and the word-level MLE to get the final embedding for the document. Formal: We use a multi-head attention mechanism to calculate the MLE score for the document d_i. Formal: The MLE score for the document d_i can be found using the MLE score for the sentence s_i. Formal: The MLE score for the document d_i can be found using the MLE score for the word w_i, which is also the output word embedding. Formal: We use a multi-head attention mechanism to calculate the MLE score for the document d_i. Formal: We use a multi-head attention mechanism to calculate the MLE score for the document d_i. Formal: We use a multi-head attention mechanism to calculate the MLE score for the document d",
        "directory": "dravidianlangtech",
        "filename": "2021.dravidianlangtech-1.21.json"
    },
    {
        "casual_text": "For each word, we use word embedding to capture its grammar and meaning, and position embedding to show where it is in the sentence. Then, we combine both the word embedding and position embedding to create the input vector for that word, which goes into the CNN. (Check out Figure 2 for more details.)",
        "formal_text": "For each word w i, we use word embedding to encode its syntactic and semantic meanings, and use position embedding to encode its position information. We then concatenate both word embedding and position embedding to form the input vector of w i for CNN. (See Figure 2. )",
        "GPT2_formal_text": "ed_x is the weighted average of the attention masks we get from the x-th layer.\n\nThe function l is calculated using this equation. Formal: So, l(0) = max(0, (1 + [α * α] + β * β]) / (1 + [α * α] + β * β) for the first layer, and l(0) = max(0, [α * α] + β * β) / (1 + [α * α] + β * β) for the second layer.\n\nBasically, l(0) is like the max of 0 and the sum of all the other values in the first layer, plus the sum of the other values in the second layer. Formal: The whole thing is laid out like this: l(0) = max(0, [α * α] + β * β) / (1 + [α * α] + β * β). Formal: The weight α is determined by how much each layer contributes to the overall label. Formal: If we take the sum of α * α, we get the final weight, which is the sum of all the values in both the first and second layers. Formal: The whole formula is just a way to sum up all the values in both the first and second layers. Formal: The model doesn't have any separate attention layers, so we just use the same weights to calculate both the first and second layers' weights. Formal: The final result is the normalized attention weight, which is a normalized version of the attention weights from the first layer. Formal: We also have a quick and easy way to estimate the weight for each layer. Formal: Formal: The model figures out the attention weights for all the layers in a sequence, and then uses the attention weight for each layer as the value for that layer's input. Formal: Formal: The final model weights are calculated using the normalized attention weights from the first layer. Formal: So, the attention weights are the attention weights for the first layer, and the final attention weight is the normalized attention weights for the final layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D17",
        "filename": "D17-1186.json"
    },
    {
        "casual_text": "Hey, just a heads-up: SKT and SKT † are basically the same network, but they're tested with different types of knowledge—prior knowledge for SKT and posterior knowledge for SKT †. That's why the KL divergence stays the same.",
        "formal_text": "Note that SKT and SKT † are the same network tested with prior and posterior knowledge selection, respectively. Hence, the KL divergence is same.",
        "GPT2_formal_text": ") , the type of entity e_t, and the entity type e_l. This is done using three specific methods. Formal: For the first type of entity, the value of δ is calculated using the entity type δ, the type of entity e_t, and the current entity type e_l. Formal: The second type of entity pair is the value of δ is calculated using the entity type δ, the type of entity e_t, and the current entity type e_l. Formal: Finally, the final value of δ is calculated based on the current entity type e_l and the entity type δ. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.275.json"
    },
    {
        "casual_text": "To make sure that the sounds at the edges of words can actually connect to each other, we use a special table called the phoneme connectivity table. This table lists all the pairs of sounds that can connect in Korean, based on how the letters at the ends of words change. The table helps us figure out which sound combinations are allowed in the language.\n\nWhen we convert a word into its possible sound sequences, we often end up with a bunch of options. We then organize these sequences into something called a phoneme graph. This graph helps us find the right sequence of sounds for a given sentence. The connectivity check goes through this graph and weeds out any sequences that don’t follow the rules of Korean grammar.",
        "formal_text": "To verify the boundary phonemes' connectablity to one another, the separate phoneme connectivity table encodes the phonologically connectable pair of each morpheme which has phonologically changed boundary graphemes. This phoneme connectivity table indicates the grammatical sound combinations in Korean phonology using the defined left and right connectivity information. The morpheme-to-phoneme conversion can generate a lot of phoneme sequence candidates tbr single morpheme. We put the whole phoneme, ~equence candidates in a phoneme graph where a correct phoneme sequence path can be selected for input sentence. The phoneme connectivity check performs this selection and prunes the ungrammatical phoneme sequences in the graph.",
        "GPT2_formal_text": "is basically the attention score, which is the number of times the word w_i gets turned into the word w_j. It’s calculated using the softmax function on the input sequence x_i. Here, x_i is the input sequence, and x_j is the output sequence. Formal: • The attention mask can be written as (•, •) i=1; j=1. Formal: If you’ve got a reference sequence X_i, it’s just the output of the LSTM or GRU in the encoder. Formal: For the first step of translating a sentence, we make a dictionary called W_a by randomly picking word pairs from W_a. Formal: For the second step of translating a sentence, we pick one word at a time from the source sentence. Formal: Using these two steps, we can get the final translation for the target sentence. Formal: Formal: The last step of translating a sentence is to pick the best output from the current source sentence, using the methods from Section 3.2. Formal: This process, where we select the best translation, is repeated for all the different steps in the process. Formal: Lastly, to make sure the model learns the best sequence of output words (as mentioned in Section 3.1), we add a dropout probability α to the output layer. Formal: The same kind of dropout probability is also used in the attention layer, which helps the model focus on the words that really matter. Formal: Formal: Finally, we use this process to translate the target sentence by using the translations learned by the model. Formal: Formal: The final output of the decoder is calculated by multiplying the attention scores and the output layer outputs, as explained in Section 3.2. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C98",
        "filename": "C98-1107.json"
    },
    {
        "casual_text": "Besides creating phrase alignments, the annotator also needs to give labels to these alignments. We’ve got four labels, which are based on two things: whether there’s a word order difference or not, and whether there are unaligned function words or not. Here’s what each label means and an example for each, shown in Figure 2:\n\n- **REO**: This stands for reordering without any unaligned function words (see Figure 2a).\n- **UFW**: This one is for alignments with unaligned function words (see Figure 2b).\n- **REU**: This label is for reordering that also includes unaligned function words (see Figure 2c).\n- **STD**: This is for structural divergence caused by differences between languages (see Figure 2d).\n\nLet’s break it down with the examples:\n\n- **Figure 2a**: This shows a reordering of the words under the aligned VP nodes. It’s pretty common to see this kind of word order difference between Chinese and English. In Chinese, the PP modifier comes before the verb, but in English, it comes after.\n- **Figure 2b**: Here, there’s an unaligned function word—the English infinitive marker \"to,\" which doesn’t have a match in Chinese.\n- **Figure 2c**: This one has both reordering (like the difference in the order of \"powerhouse\" and \"economy\") and unaligned function words (like \"的\" in Chinese and \"of\" in English).\n- **Figure 2d**: This example shows structural divergence caused by differences between Chinese and English. We’ll talk more about this in Section 4.",
        "formal_text": "In addition to making phrase alignments, the annotator needs to assign labels to phrase alignments. We have four labels that are designed along two dimensions: the presence/absence of word order difference and the presence/absence of unaligned function words. The name and definition of each of the four labels are listed below, and an example for each label is given in Figure 2: a REO, reordering that does not involve unaligned function words (Figure 2a ) b UFW, unaligned function words (Figure 2b) c REU, reordering that also involves unaligned function words (Figure 2c) d STD, structural divergence due to cross-linguistic differences (Figure 2d). . . . Figure 2a is an example where there is a reordering of the immediate children of the aligned VP nodes. This is a very typical word order difference between Chinese and English. In Chinese, the PP modifier is before the verb while in English the PP modifier is after the verb. The phrase alignment illustrated by Figure 2b has an unaligned function word, namely the English infinitive marker to, which has no counterpart in Chinese. There are both reordering (difference in the relative order of powerhouse and economy) and unaligned function words (Chinese 的 and English of ) in the phrase alignment in Figure 2c. Figure 2d provides an example where the aligned phrases have structural divergence caused by crosslinguistic differences between Chinese and English, which we will discuss in some detail in Section 4.",
        "GPT2_formal_text": "= {att_1, att_2, ..., att_n} n) = {att_1, ..., att_n | (v_att_1, v_att_2, ..., v_att_n)|} λ_k, which is the KL term for the k-th classifier λ_k. Formal: For each sentence, we figure out the probability of the correct answer and the k-th classifier that gets it right. Formal: For each sentence, we calculate the probability of the correct answer and the k-th classifier that gets it right. Formal: To put it more formally, the k-th classifier λ_k for a pair of sentences is calculated based on the correct answers for each pair and the correct classifier λ_k for the correct answers. Formal: These probability distributions are then put together to create the final probability distribution for the k-th classifier. Formal: Formal: The final probability distribution for the k-th classifier is calculated using the correct answers for each sentence and the correct classifier λ_k for the correct answers. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C14",
        "filename": "C14-1143.json"
    },
    {
        "casual_text": "Okay, so I equals 1 if h(e_i) matches y_i, and I equals 0 if it doesn't. Basically, example-based precision is just 1.",
        "formal_text": "i=1 I, with I = 1 if h(e i ) = y i and I = 0 otherwise. Example-based precision is defined as 1",
        "GPT2_formal_text": "layer, where the prediction is made based on the attention distribution \\( a_t \\) and the size \\( d_t \\). The first layer of the model, called the self-attention layer, then does its thing using a softmax function to figure out the final prediction \\( a_t(s) \\). Formal: For the output layer, the output vector \\( h_t \\) is made up of the output from the attention layer \\( a_t \\) and the input from the last hidden layer \\( h_{t-1} \\). Formal: The output layer takes the output \\( h_{t-1} \\) from the self-attention layer and sends it to the output of a bidirectional GRU (BiGRU). Formal: If you want to know more about the model, check out [Joshi89]. Formal: We created a dataset for the task of identifying antecedents by training on English text. Formal: The dataset has 60,220 examples in total, with a total of 1,214,332 unique antecedents. Formal: We also created a dataset for recognizing antecedents in parallel texts. Formal: We found 12,964 parallel examples in 1,200 sentences of parallel text. Formal: We trained an SMT system using parallel texts and tested it on the task of recognizing antecedents. Formal: We used a bag-of-words approach to assign labels to each pair of antecedent candidates. Formal: We included some extra text to make our test set even bigger. Formal: Here's what we made: Formal: We used the Stanford Parser (Klein and Manning, 2003) to process the sentence and figure out all the lemmas. Formal: We ran the Stanford parser on the data to convert the words to their base form and calculate the POS tags for each word. Formal: We used the BTS (Wang and Manning, 2014) to find all the potential antecedent candidates. Formal: We also used a common TTS system, called VocalTTS (Joshi et al., 2018), to create an audio file for each pair of antecedent candidates. Formal: We used the Stanford TTS system to convert the text into written forms. Formal: We focused on words that could be the antecedent and picked the most frequent ones as the antecedent. Formal: We took the top 5",
        "directory": "D13",
        "filename": "D13-1055.json"
    },
    {
        "casual_text": "Tweets are turned into TF-IDF representations and placed in a vector space that matches their cluster.",
        "formal_text": "Tweets are embedded into a vector space of TF-IDF representations within their corresponding cluster.",
        "GPT2_formal_text": "(1), but the other two are just the surface forms of the ID. We use the same setup for the WFSI data as our method. Check out Figure 2 for the structure of the data set. Formal: From the example, it's clear that the way the words are arranged doesn't really depend on the ID. Formal: In the first case, the order doesn't change much. But in the second case, the order changes a lot. Formal: The order stays the same whether the ID is \"first\" or \"second.\" Formal: In the first case, \"first\" and \"second\" both follow the same basic order, and in the second case, they're flipped. Formal: The second case is like the first one but with \"first\" and \"second.\" Formal: The third case is more interesting. This time, \"first\" and \"second\" follow different orders, and the ID doesn't influence the order much. Formal: In the first case, the order doesn't change much. But in the second case, the order changes a lot. Formal: The third case has the same order as the first, but the ID doesn't play a role. Formal: Since the sentence has a clear subject/object structure, the ID doesn't really help either. Formal: So, the \"first\" case is the one with the sentence's first word, and the \"second\" case is the one with the sentence's second word. Formal: Finally, the \"first\" case is the one with the sentence's first word, and the \"second\" case is the one with the sentence's second word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.530.json"
    },
    {
        "casual_text": "A big earthquake hit Aceh province in Indonesia on Tuesday. Lots of houses got messed up, and a bunch of villagers got hurt. The word \"damaged\" might sound vague on its own, but when you add the info about when and where it happened, it makes more sense. We tested our system by hiding these kinds of details from the local conversation, and the results are in Table 5.",
        "formal_text": "A strong earthquake struck Indonesia's Aceh province on Tuesday. Many houses were damaged and dozens of villagers were injured. While the event \"damaged\" is ambiguous with only the context of a single sentence, it becomes much more specific when contextualized with the previous sentence which contains both a time and a location for the event. We evaluate our system with tokens of these types masked from the local discourse with results reported in Table 5.",
        "GPT2_formal_text": "_num is the number of attention heads that the attention mechanism has. This number can be different for different languages. Formal: Okay, let’s break this down in simpler terms. We’ve got two sets of inputs:\n\n1. **h_i**: This is the [CLS] token itself.\n2. **e_i**: This is the word embedding for the [CLS] token itself.\n\nThe goal is to figure out the attention weights \\( w_i (x) \\) for each token \\( x \\).\n\n**Inputs**:\n- **h_i**: The [CLS] token itself.\n- **e_i**: The word embedding for the [CLS] token itself.\n- **w_i (x)**: The weight vector for the [CLS] token itself.\n- **s_i**: The [CLS] sentence.\n\n**Outputs**:\n- **h_i**: The [CLS] token itself.\n- **e_i**: The word embedding for the [CLS] token itself.\n- **w_i (x)**: The weight vector for the [CLS] token itself.\n\n**Mean Squared Error (MSE)**:\n- **e_i**: The [CLS] sentence.\n- **s_i**: The [CLS] sentence.\n\n**Calculation of MSE**:\n- MSE = MSE_i / MSE_i * e_i\n\n**Enumeration of tokens**:\n- For the [CLS] token itself:\n                                                        \n                                          \n\n**Equation (1)**:\n\nThe input is [CLS] and the output is the attention weights \\( w_i (x) \\). Formal:\n\nThis equation is a bit tricky, but the main",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.106.json"
    },
    {
        "casual_text": "Basically, our method splits pairs of parallel sentences into two parts, connects the two target parts with their correct translation from the two source parts (whether it's direct or flipped), and then keeps doing this over and over with the new pairs we get.",
        "formal_text": "In a nutshell, out method segments pairs of parallel sentences in two parts, linking the two resulting target segments with their proper translation amongst the two source segments (monotonous or inverted translation), and repeats this process recursively on the segment pairs thus obtained.",
        "GPT2_formal_text": "_t) ) = { h(s_t) } | o_s = (s_t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(t, e_t) = argmax_t Σ(s_t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t | (p_s_t, e_s) = argmax_t Σ(t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(s_t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(s_t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(s_t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(s_t, e_t) = argmax_t Σ(t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t Σ(t, e_t) = argmax_t | (p_s_t, e_s) = argmax_t | (p_s_t, e_s) = argmax_t Σ(t, e_t) = argmax_t | (p_s",
        "directory": "eamt",
        "filename": "2012.eamt-1.62.json"
    },
    {
        "casual_text": "We're aiming to find a group of instructions that work together across different recipes. To do this, we turn the graph (which has loops) into a collection of trees by using a maximum spanning tree algorithm. Check out Figure 4 for an example of one of these trees, made for a specific dish. A path in this tree, with no more than one node from each recipe, gives us a set of instructions that align well together. For instance, in the zoomed-in part of the tree in Figure 4, all the differently colored nodes along the path from the yellow node to the green node make up a set of instructions that align nicely across recipes.",
        "formal_text": "Our goal is to find a set of jointly-alignable instructions across different recipes. We therefore convert the graph (with cycles) into a forest by running the maximum spanning tree algorithm on the graph. Figure 4 shows an example tree derived for one of the dishes. A path in this tree, that has at most one node from each recipe, constitutes a set of jointly-alignable instructions. For example, in the magnified section of the tree in Figure 4, all unique colored nodes in the path from the yellow node to the green node constitute a set of jointly-alignable instructions.",
        "GPT2_formal_text": "ed = { 0.1, 0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.2, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.3, 0.4, 0.4, 0.4, 0.4, 0.4, 0.4, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.5, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.6, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.7, 0.8, 0.8, 0.8, 0.8, 0.8, 0.8, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.9, 0.10, 0.10, 0.10, 0.10, 0.10 }             Casual: In Figure 1, the horizontal axis shows how often the first sentence of each caption matches the target sentence. The vertical axis shows the word count, and the top row shows the number of words in the caption. The color of the sentence shows how many words are in the caption. The word-based score is the average of the word-based scores across all sentences. For example, in Figure 1, the caption \"ImplantHead 1.\" has a word-based score of 4.1 (lower is better). Formal: For every word in the caption, we calculate a score like this: Formal: We treat all captions equally, so the final score for a caption is the sum of its word-based scores across all words. Formal: Each word gets its own score, so we use the average of the word-based scores across all sentences. Formal: So, Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.440.json"
    },
    {
        "casual_text": "We split the data into a training set, a development set, and a test set. We checked how well the model did on all three by looking at how accurately it could identify the antecedents of third person singular pronouns in the data. You can find the accuracy and size of each part in Table 2. \n\nNow, the big question is: are these results good enough? It's tricky to compare our results to the best in the field because we’re only focusing on third person singular pronouns. This means our system doesn’t handle coreference chains, so we can’t really measure its performance the usual way (as Luo, 2005 pointed out). Another thing that’s different from the standard approach is that we don’t have a mention detection module. Instead, we used the gold mention annotations and the singleton mentions we pulled out (check out section 3.2 for more on that).\n\nThat being said, we still want to get an idea of how well our classifier is doing. The closest study we found is Yang et al. 2004, even though they did use a mention detection module. They trained different systems to handle third person pronoun resolution and reported their accuracy using a success metric. When they tested on the MUC-6 corpus, their metric ranged from 70.0 to 74.7 for the different systems they built. On the MUC-7 corpus, the metric was between 53.8 and 62.5.",
        "formal_text": "We divided the corpus into a training set, a development set and a test set. We tested the model's performance on all three of the sets by measuring the accuracy of the identification of antecedents of the third person singular personal pronouns in the corpus. The accuracy and size for each subcorpus can be found in Table 2. An important question is whether these results are satisfactory. Our results are difficult to compare against state-of-the-art work in coreference resolution, because we concentrate on third person personal singular pronouns only. This means that our system does not form coreference chains and that its performance cannot be measured us- (Luo, 2005). A second difference with a more standard approach is that we do not have a module of mention detection. Instead, we use the gold mention annotation and the singleton mentions we extracted (see section 3. 2). This said, we still want to have an indication about the performance of our classifier. The study of Yang et al. 2004is the most comparable we found to ours, although they used a module for mention detection. Yang et al. 2004trained different types of systems to perform third person pronoun resolution and reported accuracy, in their paper indicated by the metric of success. When they tested on the MUC-6 corpus this metric was between 70. 0 and 74. 7 for the different systems they developed. When tested on the MUC-7 corpus the metric laid between 53. 8 and 62. 5.",
        "GPT2_formal_text": "= { r, t } from the text, where t is the idiom t_i. Formal: The phrase table K_s has all the phrase pairs, which are basically the m-best translations (from Brown et al., 1993). We think these phrases are worth more than the ones in the reference translations. This approach has two main parts:\n\n1. First, we use a dynamic programming algorithm to pick the best phrase pair from the phrase table.\n2. Second, we train a language model using this selected phrase pairs to improve the translation quality. Formal: The word alignment accuracy (w_a) is calculated like this: Formal: Using the dynamic programming approach, the phrase pair (p_1, p_2) is created, and this pair is then used as a starting point for the full translation of the phrase pair p_i. Formal: In this setup, p_1, p_2 are chosen based on the phrase table, and p_i is selected based on the language model. This is the most efficient way to find a translation p_i for the phrase pair p_i, which is also the best translation for p_i. Formal: You can find more details about this process in Section 4. Formal: The reordering model (r) is trained using the translated phrase pairs p_1, p_2 and the translation of p_i. Formal: The translation model is trained using the phrase pairs p_1, p_2 and the language model. Formal: We also use a distortion model (u) to handle the distortion caused by words that are split into multiple words. Formal: After running the dynamic programming algorithm, the best translation p_i is chosen. Formal: Finally, we use a translation model with a set probability (λ) to choose the best translation for p_i. Formal: Formal: To make sure the reordering model accurately matches the translation, we use a translation model with a parameterized reordering model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E17",
        "filename": "E17-4006.json"
    },
    {
        "casual_text": "The variables s and r, which are part of the Beta distribution we use as a prior for our decision variables, are random in our model. Unfortunately, there isn’t a straightforward conjugate distribution for these parameters, so we can’t solve for them analytically. Instead, we’ll estimate the integral by repeatedly sampling these variables. Since both s and r are positive real numbers, we’ll use a Gamma prior for them, as explained in Section 3.",
        "formal_text": "The parameters s and r of the Beta distribution which serves as a prior on the decision variables are random variables in our model. Since there is no easily computable conjugate distribution for these parameters, we can not integrate them out analytically. Instead, we choose to approximate the integral through repeated sampling of these variables. Since both these variables take on values in the positive reals, we impose a Gamma prior on them as described in Section 3.",
        "GPT2_formal_text": ", and it's set to 0. Also, in the example from figure 1, the word \"have\" gets a special attention value of 1. But if we only have one reference translation, like \"has\" in figure 1, we don't need to adjust it. In that case, the attention value would be 0. Formal: We're introducing a new unsupervised method called MTST, which aims to make the model perform better by focusing on the source context. We're using the same model we talked about in section 2 to show how it works. To make sure the system can learn from data that's different from what we trained it on, we use a special lookup table called the \"source context lookup table\" (CS LT), which we'll explain next. Formal: The lookup table for CS LT looks like this: Q = {Q i, ..., Q |V i }. Formal: Each part of the lookup table gets updated as it goes through the translation process, based on the source context. Formal: The source context lookup table is used to figure out what words to add to the translation and where to insert them. It uses a lookup table for the source language. Formal: We've got three main parts to the lookup table: first, we look up the source word in the dictionary. After that, we make a list of the closest matches for each word in the source sentence. Then, we go through each word in the source sentence and update the lookup table to include the matches for all the words in the translation. Formal: Next, we calculate the attention value for each word in the target sentence. After that, we add the target word to the current output. Formal: After we've done all that, we get the best result by applying the softmax function to the attention values for the words in the target sentence. Formal: Lastly, we use the best possible attention value (0.2) for the lookup table lookup to figure out the source context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1296.json"
    },
    {
        "casual_text": "Alright, so each word has these specific features, and we use a bunch of yes-or-no variables to represent them.",
        "formal_text": "Then, each word is characterized by the following features, and is encoded by a set of boolean variables.",
        "GPT2_formal_text": "is updated. We set the maximum limit for entity mentions to 10,000. For example, the event \"lastname_year\" has a max length of 10,000.\n\nFor our evaluation, we used the SNLI dataset (Rosenfeld et al., 2014) and got a recall of 0.8. For the biomedical event dataset, we trained an LSTM model with BERT to predict if a mention is positive or negative. We used the same settings as in the text generation part of the event model to make sure the results were fair and consistent. Formal: We're using the same cross-validation setup as before and the same preprocessing steps as the text generation model. Formal: The SNLI dataset is the same as the one used by Pascanu et al. (2014). Formal: To keep things consistent with the text generation model, we're running the text with the same settings as the text generator. Formal: We're also reporting the F1 score on the biomedical event dataset, which has a recall of 0.65, along with an F1 score on the SNLI dataset, which has a recall of 0.41, to get a fair comparison. Formal: We've set the value of n to 10,000 and the number of entities in the list to 100. Formal: We trained the model using the development set for 30 epochs, using a batch size of 32. Formal: The text generation model uses the same data structure as the text generation model, with the same number of hidden layers and the same number of parameters as the text generator. Formal: For text generation, we used the model described in (Hessel et al., 2019). Formal: We created a text generation model for a single medical event using the dataset created by Klementiev et al. (2016). Formal: The text generation model is trained using the publicly available version of the SNLI dataset. Formal: We've set the text generation model to the one described in (Klementiev et al., 2018). Formal: The model is trained for 30 epochs, with a batch size of 32. Formal: We've picked the best settings from the test set for text generation, including the default settings for the encoder-decoder encoder model (e_enc). Formal: We've updated the parameters of the text generation model based on the SNLI dataset. Formal: The model is trained using the development set from",
        "directory": "C04",
        "filename": "C04-1040.json"
    },
    {
        "casual_text": "In this paper, we dive into two main topics about classifying troll farms. First, we check out how three different types of features affect how well our classifier works. We're talking about content, behavioral, and stylistic features. It makes sense that troll farms pushing a certain political agenda would have common words or phrases (like #fakenews). But we also think that how they write could help improve how well we can predict things. If we assume that some people at these troll farms manage multiple accounts, even if the topics they talk about are different, their writing style should still be pretty similar. So, we're guessing that features that work for figuring out who wrote something (like Sari et al., 2018 did) could also be useful for identifying troll farms.",
        "formal_text": "In this paper, we study two specific issues related to troll farm classification. First, we analyze how three different sets of features impacts our classifier's performance. Specifically, we look at content, behavioral, and stylistic features. Based on the political agenda a troll farm is pushing, it is intuitive that there will be common tokens associated with the organization (e. g. , #fakenews). However, it is possible that writing style can improve predictive performance. Intuitively, if we assume that certain employees at a troll organization control multiple accounts, then even if the topical information (i. e. , content) varies across the accounts, the writing style should be similar. Thus, we hypothesize that features that are predictive for authorship attribution (Sari et al. , 2018), can be applied to the troll farm domain.",
        "GPT2_formal_text": "_len (T, y_t, t) = max(T - y_t, 1) + i_t_m. Here, i_t_m is the number of tokens that aren't related to t, and T is the total number of tokens in the training set. Similarly, A_t_m is the number of tokens related to t, and A_t is the total number of tokens in the validation set. Formal: To see how the attention weight vector changes for different embedding types and other parameters, we calculate attention weights for each of the 80,000 training examples. Then, we calculate the softmax similarity of these attention vectors, like this: Formal: So, the attention weight vector for an example is calculated like this: Formal: After the pre-training process, the model gets trained to give us more general sentence representations, which help us better understand the input sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-5003.json"
    },
    {
        "casual_text": "Since we worked with datasets from various fields, we fine-tuned multilingual BERT (from Devlin et al., 2019) on the target side of the parallel corpora to create the MLMs we used.",
        "formal_text": "Since our experiments were conducted on datasets in different domains, the MLMs we used were obtained by fine-tuning the multilingual BERT (Devlin et al. , 2019) on the target side of the parallel corpora.",
        "GPT2_formal_text": "= { 'a' : { id: 1, label: 'a', value: { id: 2, label: 'b' }, }, 'b' : { id: 1, label: 'b', value: { id: 2, label: 'c' } }, 'c' : { id: 1, label: 'c', value: { id: 2, label: 'd' } } }), (4) Formal: Here's how you set up the original code: Formal: Basically, we're treating the output as a sequence of words and then making a summary based on that. Formal: Basically, we're treating the output as a sequence of words and then making a summary based on that. Formal: Basically, we're treating the output as a sequence of words and then making a summary based on that. Formal: Also, the next time we run the code, we'll set a variable to show the output we got. Formal: Also, the next time we run the code, we'll set a variable to show the output we got. Formal: This part deals with how to handle different scenarios. Formal: For each scenario, we use a function called `range(M)`. Formal: For each scenario, we use a function called `range(M)`. Formal: For each scenario, we use a function called `range(M)`. Formal: We pick the best solution based on how confident we feel. Formal: We pick the best solution based on how confident we feel. Formal: For each scenario, we use a function called `range(M)`. Formal: For each scenario, we use a function called `range(M)`. Formal: For each scenario, we use a function called `range(M)`. Formal: For each scenario, we use a function called `range(M)`. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.267.json"
    },
    {
        "casual_text": "A compositional distributional model is all about creating a vector that captures the meaning of a whole phrase or sentence by mixing together the vectors of the individual words. The easiest way to do this is by using basic math operations on the word vectors (Mitchell and Lapata, 2010). Basically, the vector for a bunch of words w1, ..., wn is made like this:",
        "formal_text": "The purpose of a compositional distributional model is to produce a vector representing the meaning of a phrase or a sentence by combining the vectors of its words. In the simplest case, this is done by element-wise operations on the vectors of the words (Mitchell and Lapata, 2010). Specifically, the vector representation − → w of a sequence of words w 1, . . . , w n is defined to be:",
        "GPT2_formal_text": "ed_fMRI and response_masked_fMRI, which are the same as the masked language model (MLM) and response language model (RLM) models, respectively. The labeled data for these models is available for everyone to see. Formal: The word embeddings used for the task and the model are based on the Google NER corpus (thanks to Mikolov et al., 2013). The embeddings are normalized to a fixed size of 300 dimensions. Formal: The MLM and RLM models use the pre-trained word embedding language model from Mikolov et al. (2013) and a set of 9 word embeddings that were trained on Wikipedia and BookCorpus. Formal: The model is trained using cross-entropy loss, with a learning rate of 0.0001, on the English Wikipedia part and a set of 9 word embeddings that were trained on BookCorpus. Formal: We tested the model using a small part of the English BookCorpus (20K sentences) from this GitHub repo: https://github.com/Emilia-Buchholz/test_eval. Formal: The BLAN model is built with an 8-layer transformer model with a hidden size of 128. Formal: The model is fine-tuned with the RLM model. Formal: You can find more details about the training setup and how it was trained in the supplementary material. Formal: The first two hidden layers of the BLAN model are swapped with a stack of 12 word embeddings, each with a size of 300. Formal: Lastly, the last hidden layer is added to the stack. Formal: The BLAN model is trained on the Wikipedia part of the BookCorpus. Formal: The output for the BLAN model is an English sentence with the same word count as the original sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1268.json"
    },
    {
        "casual_text": "In this paper, we take a look at and expand some transfer learning methods for identifying new entities. We use both the usual datasets and some new ones that haven’t been used for this before. As far as we know, this is the first time these transfer learning methods have been directly compared. \n\nWe also run experiments on seven new pairs of source and target domain corpora, which almost doubles the total number of pairs studied in all previous work. Plus, we’re sharing our code so that others can check our results and use our benchmarks to compare their own transfer learning techniques in the future.",
        "formal_text": "In this paper we replicate and extend several transfer learning methods for recognizing novel entities, using both standard corpora and datasets that have not been used for this task before. To the best of our knowledge, this is the first direct comparison of these transfer learning methods. In addition, we perform experiments on seven new source/target domain corpus pairs, nearly doubling the total number of corpus pairs that have been studied in all past work combined. We also share our code 2 so that others may verify our results and compare future transfer learning techniques against our benchmarks.",
        "GPT2_formal_text": "= (sum_1, ..., sum_M, sum_N, sum_Q, sum_R, sum_S, ..., sum_T) p_val = argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmax(p_val, argmin(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmin(p_val, argmin(p_val, argmax(p_val, argmax(p_val, argmin(p_val, argmin(p_val, argmax(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val, argmin(p_val, argmax(p_val))))))))))))))) + sum(y(t_1, t_2), sum(y(t_3, t_4), ..., sum(y(t_k, t_k+1), sum(y(t_k, t_k+n)))))). Formal: In this example, the logic is represented as a path from the antecedent q_j to the argument a_j. The formulas are marked with ⊕ to show they're linked. Formal: In this example, the logical path is shown as a path from the antecedent q_j to the argument a_j. The formulas are marked",
        "directory": "C18",
        "filename": "C18-1168.json"
    },
    {
        "casual_text": "Our approach to improving the reading step by adding more detailed descriptions of relationships is kind of like what Weissenborn et al. (2017) and Mihaylov and Frank (2018) did. They both added background commonsense knowledge to reading comprehension systems. They used structured knowledge bases to pull out info about the semantic relationships between entities. In our case, instead of that, we grab text snippets that mention pairs of entities and turn them into vector representations to show the relationships between those entities.",
        "formal_text": "The idea of our baseline to improving the reading step by incorporating additional relation description spans is similar as Weissenborn et al. (2017) and Mihaylov and Frank (2018), who integrate background commonsense knowledge into readingcomprehension systems. Both rely on structured knowledge bases to extract information about semantic relations that hold between entities. On the other hand, we extract text spans that mention each pair of entities and encoded them into vector representations of the relations between entities.",
        "GPT2_formal_text": "_predict (s, t) = max (\n               Casual: The batch size is set to 4, which is the same as in MMS (Seo et al., 2015). We use max-pooling (Mulcaire et al., 2009) to figure out the parameters. The dropout rate is 0.1, and we train the model using a batch size of 64. The training is done for 10 epochs. The classification accuracy is 0.9 for MMS and 0.95 for all the other experiments. Formal: We have a batch size of 4, which is the same as in MMS (Seo et al., 2015). We also used max-pooling (Mulcaire et al., 2009) to get the parameters. The dropout rate is 0.1, and we train the model using a batch size of 64. Formal: We tested our model on the CoNLL-2015 dataset, which has an order of 4 conversations for both MMS and MSRP. Formal: We used SVM and LSTM as the classifiers in the experiments. Formal: For the sequence classification part, we split the data into training, validation, and testing sets with 2, 4, and 8 conversations. Formal: The batch size is 64. Formal: We trained the model using a batch size of 16. Formal: We tested our model on the CoNLL-2014 dataset, which has an order of 4 conversations. Formal: For the sequence classification part, we split the data into training, validation, and testing sets with 4, 6, and 8 conversations. Formal: Formal: The batch size is 16. Formal: We trained the model using a batch size of 16. Formal: Formal: We tested our model on the CoNLL-2013 dataset, which has an order of 3 conversations. Formal: Formal: Formal: For the sequence classification part, we split the data into training, validation, and testing sets with 3, 4, and 6 conversations. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1134.json"
    },
    {
        "casual_text": "Maximum entropy (ME) models, sometimes called log-linear or exponential learning models, are a versatile machine learning method for tasks like classification and prediction. They've been pretty successful in natural language processing, such as part of speech tagging and named entity recognition. These models can handle features from all sorts of different sources to help with classification. Each feature acts like a rule for the model. For example, in question classification, a feature might be a specific word that’s linked to a certain type of question. The maximum entropy model is the one that has the highest entropy while still following all the rules. In our work, we’re using the Stanford Maximum Entropy implementation, as described by Manning and Klein in 2003.",
        "formal_text": "Maximum entropy (ME) models (Berger et al. , 1996; Manning and Klein, 2003), also known as log-linear and exponential learning models, provide a general purpose machine learning technique for classification and prediction which has been successfully applied to natural language processing including part of speech tagging, named entity recognition etc. Maximum entropy models can integrate features from many heterogeneous information sources for classification. Each feature corresponds to a constraint on the model. In the context of question classification, a sample feature could be the presence of a particular word associated with a particular question type. The maximum entropy model is the model with maximum entropy of all models that satisfy the constraints. In this paper, we adopt Stanford Maximum Entropy (Manning and Klein, 2003) implementation in our experiments.",
        "GPT2_formal_text": "= {}, w) = h * (1 - δ), which is basically a weighted sum of the log probabilities for each possible relation r_i, j. Formal: In this part, we'll give you a quick rundown of the context-free grammar (CFG), where we'll walk you through a step-by-step explanation of the inner product and its signature. We'll also explain how you can use this CFG to figure out the log probabilities for a bunch of relation types. Formal: The inner product is a bit different from the log probability because it involves a weighted sum of probabilities for each pair of things. Basically, we're trying to get a better idea of how likely a pair of things is based on how they interact with each other. Formal: Lastly, we'll talk about how to use the inner product with a fancier approach that uses a more complicated model. Formal: Here's the informal version:\n\nThe inner product is a bit different from the log probability because it involves a weighted sum of probabilities for each pair of things. Basically, we're trying to get a better idea of how likely a pair of things is based on how they interact with each other. Formal: Formal: Lastly, we'll talk about how to use the inner product with a fancier approach that uses a more complicated model. Formal: Formal: We'll also give you the details about how the log probability for a pair of things is figured out. Formal: Finally, we'll explain how to use the inner product with a fancier approach that uses a more complex model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D08",
        "filename": "D08-1097.json"
    },
    {
        "casual_text": "In 1986, she started up Hamilton Technologies, Inc. in Cambridge, Massachusetts, and became the CEO.",
        "formal_text": "In 1986, she became the founder and CEO of Hamilton Technologies, Inc. in Cambridge, Massachusetts.",
        "GPT2_formal_text": ", s = [s1, s2, ..., sk], for s = 1, ..., L + 1, and d = d_s. Formal: We train our model using the help of the GuessWhat?! model, which is basically a big computer that can really guess things. This helps us learn from all the annotations in the database. Formal: Formal: From this, we can guess that the idea behind \"if x\" is that if something is true, then it's not true for everything else. Formal: Formal: Lastly, we see that the Questioner can also handle other modalities by giving them labels and giving the answers that go with them. Formal: Formal: So, we can say that: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1077.json"
    },
    {
        "casual_text": "Alright, let's keep the generator G fixed and update the discriminators DDP and DKB by tweaking them to minimize the stuff in Equation (9) using gradient descent.",
        "formal_text": "Fix generator G and update discriminators DDP and DKB by minimizing Eq. (9) via gradient descent.",
        "GPT2_formal_text": "= (s1, ..., sm) < 0.001. These numbers were calculated using the same method as in the system by Wang and his team (2016b). We found that using these values gives us a small boost over the default settings. When we tested it on the SST dataset, using tf-idf values between 0.0001 and 0.01 gave us a tiny improvement over the baseline. But, these numbers can vary a lot depending on the dataset. For example, for SST-2, which has 20 topics, the average tf-idf value is between 0.1 and 0.2. On the other hand, when we tested it on the MNLI dataset, which has 10 topics, the values were between 0.001 and 0.002, which actually makes a big difference in terms of ROUGE scores. Formal: In Figure 2, you can see how the BERT model performs with different values of the sentence embedding embedding type. It also shows the average scores for all the scores in the top 10 lists, ranked by the highest average score for each list. Formal: Formal: In Table 4, you can see the results for the BERT model with different sentence embedding types, including the one chosen by the student model. Formal: We also looked at how the model performed when it was trained on the original DSTC-2 dataset, without any extra knowledge. Formal: We used the test dataset as the validation set to see how much better the student model can be compared to the original model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.281.json"
    },
    {
        "casual_text": "You can grab the word embedding initialization from this link: https://github.com/npow/ubottu.",
        "formal_text": "Initialization of word embedding can be obtained on https: //github. com/npow/ubottu",
        "GPT2_formal_text": "_len = 0.5 # α is a hyperparameter that decides how much we care about the attention loss. Formal: Using gradient descent on a negative log-likelihood loss would mean we don't get to see how well our method works, so we stick with the simpler, more efficient approach. Formal: We train the model using the maximum likelihood estimation method and use the empirical distribution of the target features. Formal: We also ran experiments using different values of α, like 0.1, 0.5, and 0.9. Formal: To see how well our model performs on different tasks and datasets, we tested it on three datasets that focus on aspect-based sentiment classification (check out Table 3). We trained a single model on these datasets, using a dataset size of 100, and then compared its performance to a bunch of other models trained on other datasets too. Formal: We ran experiments on all three datasets we mentioned in Section 3. Formal: All the models we tested in Section 3 used the same architecture and trained on the same dataset. Formal: Formal: For each dataset, we trained the model with a different amount of labeled data (around 200k sentences) using supervised methods (check out Table 2). Formal: Formal: We also did some experiments comparing the best model (the top one) to the others (the top three), using the same setup and dataset. Formal: We did the same setup as in Section 3 and tested the results using the same three datasets. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "2) In \"He paid his bill,\" the person (the subject) is giving up something valuable, like money. The \"bill\" (the object) means there's a specific amount owed, and that amount is supposed to go to some faraway person or thing (that we could figure out if we wanted to) as the one who should get it.",
        "formal_text": "2) in \"He paid his bill\" the subject gives up something of econ. value; the object implies a specific amount and that this amount is due to some not further specified (but specifiable) remote entity as the rightful receive~;",
        "GPT2_formal_text": "\": [ \"1\", \"2\", ..., \"M\" ], type = [ \"v_in\", \"v_out\", \"v_out_2\", ..., \"v_out_m\" ], ctype = [ \"v_in_i\" ], attn = [ \"v_out_i\" ], gtype = [ \"v_in\" ], attn_size = [ \"v_out_i\" ], attn_len = [ \"v_out_i\" ], attn_perms = [ \"v_in_i\" ], attn_info = [ \"v_in_i\" ], attn_type = [ \"v_in\" ], attn_values = [ \"v_in_i\" ], attn_comps = [ \"v_in_i\" ], attn_arg = [ \"v_in_i\" ], attn_num = [ \"v_in_i\" ], attn_type = [ \"v_in\" ], attn_max = [ \"v_in\" ], attn_probs = [ \"v_in_i\" ], attn_arg_max = [ \"v_in\" ], attn_arg_num = [ \"v_in_i\" ], attn_type = [ \"v_in\" ], attn_valid = [ \"v_in\" ], attn_val = [ \"v_in\" ], attn_val_max = [ \"v_in\" ], attn_val_num = [ \"v_in\" ], attn_num = [ \"v_in\" ] ], attn_type = [ \"v_out\" ], attn_logits = [ \"v_out_i\" ], attn_arg_logits = [ \"v_out_i\" ], attn_arg_logits = [ \"v_out_i\" ], attn_logits = [ \"v_out_i\" ], attn_logits = [ \"v_out_i\" ], attn_arg_logits = [ \"v_out_i\" ], attn_arg_logits = [ \"v_out_i\" ], attn_arg_logits = [ \"v_out_i\" ], attn_logits = [ \"v_out_i\" ], attn_arg_",
        "directory": "C69",
        "filename": "C69-2201.json"
    },
    {
        "casual_text": "The stuff I just mentioned also applies to another type of reduction called strong reduction or 11-reduction, but I don't have room to explain it right now. You can check out Morrill et al. (1990) for more details.",
        "formal_text": "2The above remarks also extend to a second form of reduction, strong reduction/11-reduction, which we have not space to describe here. See Morrill et aL (1990).",
        "GPT2_formal_text": "ed for classification tasks, like POS tagging or named entity recognition (NER). In this paper, we're sharing the results of an NER task (Uyghur Restaurant, Uyghur Restaurant in the Name) using both the pre-trained BERT model and the attention masking framework from Wang et al. (2019a). Formal: In Section 2, we introduce the BERT model, which is trained using the existing Wikipedia dump. Then, in Section 3, we explain the attention mechanism that helps the model generate text relevant to the NER question. We then test this approach on four different NER tasks, including the POS tagging task. We evaluate the model using the IWSLT 2010 Chinese to English (En-En) dataset. We noticed that the model can't really pull out NER knowledge from other languages like English. Formal: In Section 4, we show how the model performs on the POS tagging task. Formal: In Section 5, we evaluate the model on the named entity recognition task. Formal: Section 6 covers the results of an experiment to see how the model performs on other NER tasks, as well as a comparison with a supervised model. Formal: Section 7 wraps up the discussion. Formal: Formal: Table 1: List of NER tasks we tested on four NER datasets in the POS tagging task, including the POS tagging task. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E91",
        "filename": "E91-1035.json"
    },
    {
        "casual_text": "We also tweaked the sequence generation setup to work with the training pairs we’re proposing. In each layer, the attention module figures out how much weight to give to the token representations from the encoder, based on what’s already been generated in the decoder, and then spits out the context c_h. To factor in the future conversation X_f, we added another encoder to create contextualized token representations of X_f, which the attention module then pulls out as the context c_f. This new encoder uses the same parameters as the original one. The attention module’s output is basically the combination of the past context c_h and the future context c_f. Lastly, the training criterion we’re using is the negative log-likelihood, like this:",
        "formal_text": "We also redesign the sequence generation architecture to handle the proposed training pair. The attention module in each layer calculates the weight of the contextualized token representations from the encoder based on the information that has been generated in the decoder, and then returns the context c h. In order to consider the future conversation X f, we apply another encoder to produce the contextualized token representations of X f, which will be further extracted as the context c f by the attention module. The new encoder shares the parameters with the original encoder. Meanwhile, the output of the attention module is the concatenation of the past context c h and the future context c f. Finally, the training criterion is formulated as the following negative log-likelihood:",
        "GPT2_formal_text": "= {x1, ..., xn} | {s1, ..., sm}, | | x1 + n, = 1, 1, ..., | | xn + n | = 1, 1 | = {x1 + 1, ..., xn + 1} Formal: So, s = s_i, and r = r_i, and x_i = x_i, y_i = y_i. The query and the response are just vectors in R^(n×d). Formal: But here's the thing: for the first time, these QA pairs are actually created during the prediction step, as you can see in Figure 1. Formal: We're also adding some extra features to the model, like Q, A, and R. These help capture the reasoning behind the answer. Formal: Let's call the new QA model {Qp, Qa, ..., Qr}. Formal: So, Qp = {qp, qa, ..., qr}, R = {rp, ra}, and Qa = {qa, ..., qr}. Formal: The model decides if the question-answer pair is good or not, based on the answer and the sentence it's in. Formal: So, Qp = {qp, qa, ..., qr}, R = {rp, ra}, and Qa = {qa, ..., qr}. Formal: Each row in the matrix A_t represents the pair of question-answer pairs that the model is looking at. Formal: Also, we're checking how well the model can pick the right answer by checking its performance on the question-answer pair test set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.534.json"
    },
    {
        "casual_text": "Compared to the usual decoding approach (like the one by Zens and Ney in 2008), the method we're suggesting can skip a bunch of complicated calculations, such as figuring out rest costs and dealing with reorderings during decoding. This is because each path in the reordering lattice already has all the info about word order. So, our method ends up making the decoding process simpler than the old way.",
        "formal_text": "Compared with the conventional decoding method (Zens and Ney, 2008), the proposed method can eliminate some complex score calculations, e. g. , rest cost estimation and decoding-time reorderings, because each path in the reordering lattice holds complete information of the word order. As a result, the proposed method makes the decoding algorithm simpler than the conventional method.",
        "GPT2_formal_text": "_attn_tokens = min(attn_tokens, d) # define attention_mask_tokens as the vector representation of the attention mask for a token_tokens. Formal: Basically, if a token is allowed, it gets the mask token. If not, it gets ignored. Formal: In Figure 2, the attention mask vector for a token t_i is calculated by adding up all the attention mask tokens. Formal: We also use a linear attention layer called L_ATT, which adds an attention value to each token. Formal: For instance, if a token is allowed, it gets the attention mask token, which in turn gets the value of the linear layer. Formal: To see how different attention layers work, we train the model using either the official dataset or a simplified version of it called D_Sim. Formal: Formal: A token with token_tokens = 1 gets the first attention layer and the attention mask value. Formal: We also use a linear layer with two values: one for the token's position in the sequence (0 = 0) and another for the position of its token in the sequence (1 = 1). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1134.json"
    },
    {
        "casual_text": "We use these representations to figure out how similar each part of the question is to each part of the document. This is done by creating a matrix S ∈ R m×n that shows the similarities between the tokens (words or pieces) in the question and the document. We do this using a tri-linear function, as described by Seo and others in 2017.",
        "formal_text": "These representations are used to calculate the pairwise similarity matrix S ∈ R m×n between tokens in the question and those in the document by a tri-linear function (Seo et al. , 2017), i. e. ,",
        "GPT2_formal_text": ". This is a way to find the real translations of a word by looking at all the possible options it could have. It's basically a regular search method, but with some cool features: Formal: We use the usual methods for finding the best translation of a word (in this case, the word in the dictionary), using a beam search with a beam size of k = 1 and an initial search step of k = 2. Basically, we’re dealing with a fixed-size dictionary that’s been tweaked to include the target word. Formal: In our approach, we use the target word’s original form to calculate the translation probabilities. Formal: The translation probabilities for the translated word are basically the chances of the word being the right translation for the target word. Formal: The probability of the translation of a given word is calculated using the maximum common subsequence of the target word and the translated word. Formal: We don’t limit how long the common subsequence can be, though. Formal: We use a vector to represent the translation probability. Formal: We take the cross-entropy loss from Equation (2) and apply it to the vector. Formal: To get the cross-entropy loss, we take the target word’s common subsequence and use it to calculate the cross-entropy loss. Formal: The model is trained to find the best translation using this cross-entropy loss. Formal: The translation probability is calculated using the cross-entropy loss, which we use to calculate the translation probability. Formal: We also calculate the loss using the cross-entropy loss to get the cross-entropy loss. Formal: Finally, the cross-entropy loss is the combination of all the cross-entropy losses. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1237.json"
    },
    {
        "casual_text": "The dataset has a couple of cases where the answer is marked as uncertain. However, de Marneffe et al. (2010) didn't include those in their results, and we're following their lead by leaving them out too.",
        "formal_text": "The original dataset contains two additional examples where the answer is annotated as uncertain, but de Marneffe et al. (2010) exclude them from the results and so do we.",
        "GPT2_formal_text": ".hbs is basically a mix of the attention mask and the context vector. Formal: The type of attribute, e, for a word w i, j in the user's context c i, j is determined by the user's intent, s i, j. For example, if we want to figure out the intent for the word \"get directions,\" we need to know the type of attribute, e i, j, and the intent, s i, j. So, for the word \"get directions,\" we take the intent and the type of attribute and combine them to create the query, q i, j, which is the intent and the type of attribute combined. Formal: Our model decides the intent and the type of attribute by combining the query and the intent. For instance, \"get directions\" gets the type \"GET\" and the intent \"GET directions.\" Formal: Figure 1 shows the basic setup of our model, where the attention weights are the weights for the attention nodes. Formal: The attention weights are the weights for the attention nodes. We use a model called attention-based neural network (Bojanowski et al., 2017) to learn these weights. Formal: The weights for the nodes are learned based on the user's intent, s i, j, and the context vector c i, j. The attention weights, w i, j, are learned using the attention mechanism. The attention weights, w i, j, are calculated using the user's intent, s i, j, and the context vector c i, j. Formal: We also use an output layer to calculate the attention weights, w i, j. This layer helps create the attention vector, q i, j. Formal: Lastly, the final output layer combines the attention weights, w i, j, to get the final representation for the word. Formal: We use the model to generate the final word using a sequence-to-sequence model, inspired by attention mechanisms (Mikolov et al., 2013). Formal: The model learns the weight for each attention node and the next word by combining the input from the previous word and the current word. Formal: We also use an output layer to calculate the attention weights, w i, j. We use this layer to generate the attention vector, q i, j. Formal: Lastly, the final output layer combines the attention weights, w i, j, to get the final representation for the word. Formal:",
        "directory": "D18",
        "filename": "D18-1202.json"
    },
    {
        "casual_text": "When you're going through a tagged sentence T = (w 1 /p 1, . . . , w n /p n ) from the end to the beginning, you categorize each word w i into one of three groups: Left, Right, or Shift.",
        "formal_text": "While scanning a tagged sentence T = (w 1 /p 1, . . . , w n /p n ) backward from the end of the sentence, each word w i is classified into three categories: Left, Right, and Shift. 1",
        "GPT2_formal_text": "= {q_i, q_j} from i=1 to m, where q_i is the original query, and q_j is the response generated by the summarizer. Formal: The summarizer's goal is to create a summary that meets the following conditions: Formal: We can swap out the [a_i, a_j] values in equation (2) and (3) with different values, like [a_i, a_j, ..., a_j] (check out Figure 2 for an example). Here's how we do it: Formal: For each query p_i, we look for the query q_i that has the highest score (the highest negative log-likelihood score) from the query relevance set. Formal: The relevance set Q_p is created by combining the query relevance set Q_q with the query relevance set Q_p, which comes from equation (3). Formal: For each key-value pair (k_i, k_j) in Q_p, we calculate the relevance score for the key k_i by adding up the relevance scores for the corresponding key k_j. Formal: Using this evaluation method, we can train the summarizer to produce a query relevance set Q_p that includes all the queries p_i, ..., p_j. Formal: We calculate Q_p using equation (3) and then calculate Q_p_ij using equation (4), which gives us the relevance scores for the query relevance set Q_p. Formal: Finally, we update the relevance set Q_p by combining it with the relevance set Q_p_ij. Formal: The reward function we're looking at is based on this summation-based reward function. Formal: Let's say Q_p and Q_p_ij are the reward functions we calculate for a query relevance set, and the current query relevance set, P_i, is the set of all queries p_i, ..., p_j in P_p. Formal: We tweak the reward function by minimizing the sum of P_i and P_p_ij. Formal: To find Q_p_ij, we update the query relevance set by combining the query relevance set with the query relevance set Q_p_ij. Formal: In this paper, we use a reinforcement learning (RL) framework to train our summarizer. Formal: In our",
        "directory": "C04",
        "filename": "C04-1040.json"
    },
    {
        "casual_text": "How batch size and dropout affect things. The batch size impacts how quickly the model learns and its final accuracy, while the dropout rate also plays a big role in performance. We looked at how these two settings influence things by using a 1:1 ratio for our data (all CTB data and an equal amount of randomly selected PD data). We tracked the accuracy of our neural multi-view learning model over different training epochs, trying out different combinations of dropout rate (d) and batch size (b). The results are available here: [link 1] and [link 2].\n\nFor the stacking model, we set the batch size to 100 for the PD sub-model. The results are in Figure 5. The top two dashed lines at epoch 30 show a 20% dropout rate, the middle solid lines show no dropout, and the bottom dotted lines show a 50% dropout rate. Without dropout, the model starts off doing better but then gets worse after 10 epochs, which suggests it’s overfitting the training data. On the other hand, with a 50% dropout rate, the model starts off way worse, meaning the dropout might be too high and causing underfitting.",
        "formal_text": "Effect of batch size and dropout. The batch size affects the speed of training convergence and the final accuracies of the neural models, and the dropout rate has been shown to significantly influence the performance. We investigate the effects of these two hyper-parameters by adopting a corpus weight ratio of 1: 1 (All the CTB training data is used, while the same amount of PD is sampled randomly), drawing the accuracies of the neural multi-view learning model against the number of training epochs with various combinations of the dropout rate d and batch size b. The results are 1 https: //code. google. com/p/word2vec 2 https: //github. com/SUTDNLP/ZPar shown for the multi-view learning model. For the stacking model, we use b=100 for the PD sub model. The results are shown in Figure 5, where the two dashed lines on the top at epoch 30 represent the dropout rate of 20%, the two solid lines in the middle represent zero dropout rate, and the two dotted lines in the bottom represent a dropout rate 50%. Without using dropout, the performance increases in the beginning, but then decreases as the number of training epochs increases beyond 10. This indicates that the NN models can overfit the training data without dropout. However, when a 50% dropout rate is used, the initial performances are significantly worse, which implies that the 50% dropout rate can be too large and leads to underfitting.",
        "GPT2_formal_text": "function. Formal: • The text and the candidate summary are tagged with the matching entity tags from the Knowledge Graphs (KG). This means they’re connected through a graph structure, and we’ll call these \"Dependency Link Networks (D-LINKs).\" Formal: • We take all the entities from the KG and mash them together using the methods we talked about earlier, like the entity-to-relation (E2R) or relation-to-entity (R2E) relations. This gives us D-LINKs. Formal: • We use a bi-directional RNN to do a sequence prediction, and the output from the bi-directional RNN (called \"h\") is passed through a cross-attention layer to create the summary representation h. Formal: • The model uses a self-attention mechanism to predict the summary label r. This gives us D-LINKs to show how the candidate summary connects to the KG. Formal: • We pick the RNN that gives the highest probability for the predictions and we'll call this model the \"word-based attention mechanism.\" Formal: • We tweak the Bi-GRU network by adding a beam size of 1, which helps it focus on important words. The result is a more focused summary. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1070.json"
    },
    {
        "casual_text": "Basically, if you have a document d with a bunch of tokens like {t1, t2, ..., t|d|}, we can break it down into smaller chunks called segments. Each segment si will have c tokens, and there will be |s| of these segments in total.",
        "formal_text": "More formally, given a document d = {t 1, t 2, . . . , t |d| } of |d| tokens, we split it into |s| consecutive segments s i of c tokens:",
        "GPT2_formal_text": ", and the ones with the highest weights. Formal: This part talks about the feature structures we’ve come up with for Japanese sentence segmentation. Formal: We’ve noticed that when sentences are broken down into smaller parts, words that show up more often tend to have more specific meanings. So, we’ve put together a list of words that usually go with certain parts of the sentence. Formal: We used this list to create some rules that help us find the right way to break down the sentence. Formal: We also made a separate list of words that make sense in the context of the sentence. Formal: We tested this out using a big Japanese text collection. Since we didn’t have any real-world examples of sentence segmentation, we used the features we defined to create these rules. Formal: Here’s how the features work in practice: Formal: We made two lists for each of the three types of Japanese segmentation (2-4-2, 2-4-3, and 2-4-4). The first list is called the \"sentence-context-based\" list, and the second one is the \"sentence-context-agnostic\" list. Formal: The features are based on a part of speech tag (s_a_t) and the word class (s_c_t), plus some rules that keep track of how these parts of speech and words are related. Formal: The features are calculated using a fancier version of the IBM model 1. Formal: We ran experiments with different values for the word-frequency limits, ranging from 0.001 to 10^-6. Formal: The results show that these features work well for Japanese sentence segmentation. Formal: We also tested out different types of prefixes for the feature structures. Formal: We also tested out a few other things, like how the word segmentation affects things like how words are expressed in sentences (word-pos_m and word-pos_o), how words are formed (word-form_m and word-form_o), and how sentences are structured (sentence-structure_m and sentence-structure_o). Formal: The word-structure-based features we used were made using IBM model 1. Formal: We’ve also included a few other features that we thought of as potential features for a Japanese segmentation model, but didn’",
        "directory": "clinicalnlp",
        "filename": "2022.clinicalnlp-1.2.json"
    },
    {
        "casual_text": "1a) The storm wiped out 120,000 people in Jamaica and five in the Dominican Republic, then headed west toward Mexico.",
        "formal_text": "1a) The storm killed 120, 000 people in Jamaica and five in the Dominican Republic before moving west to Mexico.",
        "GPT2_formal_text": "_tgt_tgt (src, *, size, *, source) = h_src_tgt_tgt (src, *, size, *, source). Formal: Formal: In this part, we’ll break down the relation extraction process in a simpler way. We’ll mainly focus on the relation extraction part to understand it better. Formal: So, the relation extraction part is all about extracting the relationships between different parts (the input and output). We’ll talk about how we do this in section 2.1. Formal: For the dependency part, we’ll explain how we connect the relation extraction to the dependency parsing process. This will help us understand how to model the relationship between two things. Formal: Finally, in the relation extraction part, we’ll explain how we use the relation extraction to figure out the relationships between words and sentences. Formal: So, it’s all about extracting relationships between different types of text data and connecting them to the dependency parsing process. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C10",
        "filename": "C10-2170.json"
    },
    {
        "casual_text": "Before, we used these measures to figure out how two argumentative parts were connected. But since our speeches have multiple arguments and we don’t know where they are in the text, we tweaked the method to work at the speech level. This means we looked at every sentence in the supporting speech and every possible sentence in the counter speech. \n\nFor each measure, we compared one supporting speech sentence to all the potential counter speech sentences and combined the similarities using a function f. This gave us a \"sentence-to-speech\" similarity. Then, we took all those similarities and combined them again using another function g, which gave us a \"speech-to-speech\" similarity. \n\nWe call these speech-to-speech measures w f g for word-based similarities and e f g for embedding-based similarities. When combining things, we considered using the max (↑), min (↓), average (+), or product (×). For example, w ↑+ means we took the highest word-based similarity for each supporting sentence compared to all counter sentences, and then averaged those numbers.",
        "formal_text": "Previously, these measures were used to predict the relations between a pair of argumentative units. Since our speeches may contain multiple arguments, and their location within the text is unknown, we defined this method at the speech level by considering every supporting speech sentence and every candidate counter speech sentence. For each measure, the similarities of one supporting speech sentence to all candidate counter speech sentences were aggregated by applying a function f, yielding a sentence-to-speech similarity. These sentence-to-speech similarities were aggregated using another function g, yielding a speechto-speech similarity. We denote these speech-tospeech measures by w f g for word-based similarities and e f g for embedding-based similarities. As aggregation functions, the maximum (↑), minimum (↓), average (+) and product (×) were considered. For example, w ↑+ denotes taking the maximal word-based similarity of each supporting speech sentence to all candidate counter speech sentences, and averaging those values.",
        "GPT2_formal_text": "= { \"a\" : 1, \"b\" : 2, \"c\" : 3 } end for. Formal: If the current sentence is a question, you can figure out the answer by checking the question/answer pair. This method is pretty flexible—you can use it even if the current sentence isn't a question. Formal: Another approach is to start with a question-answer pair and then create a question-answer pair. This method works really well if the question is already set in stone and the answer is already known. Formal: This method is basically the same as the second one, but the system can handle any kind of question. Formal: When figuring out the answer, the system looks at the question-answer pair. Formal: The second method is more like the first, where the system can handle any kind of question. Formal: We tested this method on an unsupervised dataset and got a Precision (P@1) of 0.94, which is pretty good. Formal: We also made a small, unsupervised dataset using our method and got a P@1 of 0.9. Formal: Our method can figure out the answer to a question in just a few steps, even when it’s a long one. Formal: We set the length of a question to four words to make sure we’re not missing any important info. Formal: The first method can handle questions with lots of words. Formal: The second method works on any kind of question. Formal: Our method can handle questions that have a lot of words, even if they’re short. Formal: The third method is simpler than the second one, but it can handle questions that have a lot of words. Formal: The fourth method is also simpler than the first, but it can handle questions that have a lot of words. Formal: Formal: Here’s a breakdown of how we did on the unsupervised data we used in our experiments. Formal: Table 1 shows some examples of questions, their answers, and the generated questions we got. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "You can check out our PCRF-Seq2Seq implementation here: https://github.com/UKPLab/coling2016-pcrf-seq2seq",
        "formal_text": "Our implementation of PCRF-Seq2Seq is available at: https: //github. com/UKPLab/coling2016-pcrf-seq2seq",
        "GPT2_formal_text": "ing_mean = ReLU(h, t) self.attention_masking_mean = softmax(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = softmax(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_mean = tanh(Att(h, t)) * tanh(Att(h, t)) self.attention_masking_",
        "directory": "C16",
        "filename": "C16-1160.json"
    },
    {
        "casual_text": "It's just a mix between the one-hot target distribution \\( p_v \\) and the smoothing prior \\( 1_V \\), where \\( m \\) (which is between 0 and 1) decides how much of each to use. You can figure this out using either the divergence inequality or the Lagrange multiplier method. Check out Appendix A for more details.",
        "formal_text": "which is simply a linear interpolation between the one-hot target distribution p v and the smoothing prior 1 V, with m ∈ [0, 1] being the interpolation weight. One can use either the divergence inequality or the Lagrange multiplier method to obtain this result (see Appendix A).",
        "GPT2_formal_text": "= {(p_i, f_i)}, ..., (p_m, f_m)}, where p_i is the predicted label for a pair of input tokens and f_i is the correct label. Formal: • The parse tree that has the most tokens, as shown in Figure 2. Formal: • The highest number of tokens in the parse tree, as shown in Figure 2. Formal: • The number of tokens in the parse tree. Formal: • The longest common subsequence (LCS) in the parse tree. Formal: • The longest common subsequence length. Formal: • The longest common subsequence length in the input sentence. Formal: • The maximum number of characters in the input sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "aacl",
        "filename": "2020.aacl-main.25.json"
    },
    {
        "casual_text": "A bold result means it's way better (like, p < 0.01 in a t-test) compared to the other stuff in the same box.",
        "formal_text": "A result in boldface indicates that it is statistically significantly better (p < 0. 01 in pairwise t−test) than the others in the same box.",
        "GPT2_formal_text": "ing (a masking operation) is basically the sum of the input embeddings (h x i, z) for each of the entity e i. Formal: The attention is calculated as the sum of the attention functions for each entity e i. Formal: If h x i is 0, it means the entity is unidentifiable, and the sum of attention functions for its embeddings is zero. Formal: In situations where the entity e i doesn't have any context, the embeddings of the entity are represented as vectors in R^d. Formal: We take the average of the entity embeddings (h x i, z) for each entity e i, using them to calculate the attention score for the entity e i. Formal: The attention for entity e i is calculated based on the average of the entity embeddings (h x i, z) for each entity e i. Formal: Formal: We update the embeddings (h x i, z) and the entity embeddings (h x i, z) to get the updated embeddings (h x i, z). Formal: We do this by multiplying h x i, z by the attention score of e i. Formal: Lastly, we get the final embeddings for entity e i by adding up the entity embeddings (h x i, z). Formal: Formal: We update the entity embeddings (h x i, z) and the entity embeddings (h x i, z) to get the updated embeddings (h x i, z). Formal: Formal: Finally, we get the final embeddings for entity e i by adding up the entity embeddings (h x i, z). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "So, we've got 450 training examples, which is way less than 1% of the whole TyDi QA training set. If we double that to 100 examples per language, we see a boost in performance, with an average F1 score of 71.7. Adding even more data—like 500 examples per language, which totals 4,500 examples—makes things even better, bringing the average F1 score up to 76.7. Basically, with less than 10% of the training data, we've closed the performance gap by over 82%. And for some languages, like Finnish, the improvement is even bigger—like over 92%.",
        "formal_text": "Results ( We note that the total 450 training instances represent less than 1% of the full TyDi QA training set! Doubling that amount of data to 100 examples per language further increases downstream performance to an average overall F1 score of 71. 7. Going further to the point of adding 500 training instances per language (for a total of 4500 examples) leads to even larger improvements for an average F1 score of 76. 7. That is, using less than 10% of the available training data we can reduce the average F1 score performance gap by more than 82%. For a few languages the gap reduction is even more notable, e. g. , more than 92% for Finnish.",
        "GPT2_formal_text": "_t = [t1, t2, ..., tk] q, and we're setting the weights for these embeddings to a matrix W. Formal: The k-th word embedding in the input sequence x. Formal: This setup has two parts: the embedding layer and the output layer. The embedding layer maps the input x into a fixed-size vector called e_in. The output layer then spits out the embedding for each word in the input sequence. Formal: This setup is similar to the Transformer model (Vaswani et al., 2017), which also uses a copy mechanism. Formal: In the BERT model (Devlin et al., 2019), the input sequence x is turned into a embedding vector by combining the output layer from the last layer and the output layer from the first layer. Formal: We're skipping the setup where we just use the input sequence x for embeddings. Formal: The input layer takes the input x and turns it into a matrix W, which represents the embedding for each word in the input sequence x. Formal: We’re assuming that the outputs from both the embedding layer and the output layer form a continuous vector. Formal: The embedding layer then maps the input sequence x back into a fixed-size vector, so it can be easily compared to the embedding layer. Formal: Formal: The output layer then spits out the embedding for each word in the output sequence. Formal: The same idea applies to GPT-2. Formal: The output layer only has two parts: the embedding layer and the output layer. Formal: This setup is similar to GPT-2, which uses a copy mechanism. Formal: Formal: Formal: We’re skipping the Transformer model, GPT-2, and GPT-3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-short.79.json"
    },
    {
        "casual_text": "Alright, let’s break this down in simpler terms. So, in the last section, we talked about how the real world—which is super complex and never-ending—is described using languages that have a limited number of words. Because of this, words in any language don’t really have super clear or exact meanings. They kind of cover a range of things or events. \n\nThis fuzziness in word meanings makes translating tricky. In one language, a single word might cover a bunch of similar things or events, but in another language, you might need different words to describe each of those things. It’s like one word doing the job of several in another language (check out Figure 6 for an example).",
        "formal_text": "6 Implicit i[nformation The discussion in the last section can be summarized thus; Because a continuously infinite physical-/mental world is described by a natural language which has only finite words, words in individual languages are used to describe certain ranges o[ events/objects. That is, 'meanings' of words a~: t quite vague. This vagueness causes difficulties of lexical choice in translation by the fact that certain families of events/objects which can be described by the same words in one language should be described by several different words in other languages (Fig. 6 ).",
        "GPT2_formal_text": "_dg is made up of different parts that help the model figure out which attribute belongs to what word. Each part of the attention layer gets a little attention value, like a random variable called h_m for a specific attribute. Formal: The attention for an attribute is calculated by multiplying a bunch of scalar values, like h_m, together. Formal: Here, m is the attention value for a specific attribute. Formal: A simple way to think about how attributes are represented is like saying that each token in a sentence is made up of one attribute, called a_t, and one or more words, like w_1 to w_n, that describe that attribute. Formal: You can figure out the attention values for any attribute by running it through a softmax function. Formal: Once the model gets the attention values for each attribute, it uses them to combine the attention values from all the words in the sentence. Formal: Lastly, we set up a function f to adjust the attention values for each attribute. Formal: For each word w_t, the word w_j, and the attribute a_t, the value of f for each attribute is calculated like this: Formal: Formal: This whole setup is what we call the attention layer for a sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C86",
        "filename": "C86-1155.json"
    },
    {
        "casual_text": "The whole point of using DCGs is that they make things clearer and more organized.",
        "formal_text": "The rule based nature of a DCG result %n systems of greater clarity and modularity.",
        "GPT2_formal_text": ". It's like a way to list out possible ideas and pick the right one by looking at all the possible answers it can find. The scores it gives are based on how well it matches the list of possible answers, called E. The system uses something called answer-to-question attention, which was introduced by Vaswani et al. in 2017. This method helps with both finding the right answer and figuring out if it’s related to the current question. Formal: We’re using the same setup as the original SQuAD 1.1 by Lai et al. (2016). Formal: Basically, we’re checking the answer, the possible answer, and the question to see if they’re related. Formal: Also, we’re looking at the answer and the question to see if they’re related. Formal: This makes sense because the answer is like the key to figuring out what the question is about. Formal: In this setup, there’s a clear connection between the answer and the question. Formal: So, if the answer and the question are related, the answer is the correct answer, and the question is wrong. Formal: Since SQuAD doesn’t have clear answers to questions, we need a similar approach for SQuAD. Formal: Formal: Basically, if the answer and the question are related, the answer is the correct answer, and the question is wrong. Formal: Formal: If the answer and the question are unrelated, the answer is the wrong answer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "A83",
        "filename": "A83-1010.json"
    },
    {
        "casual_text": "Discourse relations usually have a direction. Our QA format shows this by putting discourse units either in the question or the answer. In some question types, the order is fixed by the question itself. For example, in Table 4, ex. 1, since the question asks about the condition, the condition will always be in the answer. \n\nThere’s also a pattern for symmetric relations, where the meaning stays the same no matter how you arrange the question and answer, like in ex. 2 in Table 4. \n\nLastly, some relation types, like cause-effect (reason vs. result) or certain time-related ones (before vs. after), are considered reversed. In these cases, two QA pairs with different question types can actually mean the same thing if you swap the discourse units, as seen in ex. 3 in Table 4. \n\nThese directionality patterns affect how we annotate and evaluate things, which we’ll talk about later.",
        "formal_text": "Relation Directionality Discourse relations are often directional. Our QA format introduces directionality by placing discourse units into either the question or answer. In some question prefixes, a single order is dictated by the question. As seen in ex. 1 of Table 4, because the question asks for the condition, the condition itself will always be in the answer. Another ordering pattern occurs for symmetric relations, meaning that the relation's assertion remains the same no matter how the arguments are placed into the question and answer, as in ex. 2 in Table 4. Finally, certain pairs of relation senses are considered reversed, such as for causal (reason vs. result) and some of the temporal (before vs. after) question prefixes. In this case, two QA pairs with different question prefixes can denote the same assertion when the target discourse units are reversed, as shown in ex. 3 in Table 4. These patterns of directionality impact annotation and evaluation, as would be described later on.",
        "GPT2_formal_text": "layer. The weights for all the LSTM cells are calculated using a dropout rate of 0.1. Formal: When it comes to predicting the probability p(a|b), we start by updating the hidden states H(a) and H(b) by combining their previous hidden states H(a) and H(b). For the LSTM layer, we define the probability p(a|b) by adding up the probabilities p(a) and p(b) for a pair of words a and b, as shown in equation (1). Formal: From equations (1) and (2), we can figure out the probabilities p(a|b) by taking the average of the probabilities. Formal: In the end, we get the probability of a pair of words a and b. Formal: Alright, so for a pair of words a and b, the probability p(a|b) is calculated based on the values from equation (1) and (2). Formal: In equation (1), we don’t use the probability of the first word in the pair, but we do use the probability of the last word in the pair, which comes from the distribution p(a|b). The LSTM layer then uses this combined probability to predict the probability of a pair of words a and b. Formal: So, the final probabilities for a and b in the pair are calculated like this: Formal: In (4), we’re using the conditional probability p(a|b) = p(a) + p(b). Formal: So, the total conditional probability is the sum of all the conditional probabilities from equation (1) and (2). Formal: Since the first word in the pair (a, b) is the most likely one, the conditional probability p(a|b) becomes the negative conditional probability p(a | b). Formal: Formal: In equation (5), we’re adding the conditional probability p(a|b) to the conditional probability of the last word in the pair, which is the distribution p(a | b). Formal: Formal: Formal: This is a bit more complicated because there’s a third option: Formal: Formal: Formal: We’re also considering the probability p(a|b) for a pair of words a and b if they show up together in sentences. Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "The decor could use a little improvement, and having a small bar would make the place feel more welcoming.\n\nFigure 6 shows some examples of how BART (a machine learning model) handles different sentences. Let me break it down for you:\n\n- **(a)**: In this case, the category \"miscellaneous\" isn't mentioned in the sentence, so BART can't figure it out. But our method can still combine different sentiments from different aspects to get the right answer.\n\n- **(b)**: The sentence says, \"the value on the kids menu is good.\" BART might get confused because \"good\" is talking about the value, not the menu itself. But our method gets it right and doesn't get distracted by other parts of the sentence.\n\n- **(c)**: This one's a bit tricky because it involves \"if\" statements, which are hard for BART to handle. But our method can still figure out the negative sentiment in \"if there was... would be a bit more inviting.\"\n\nSo, our method does a better job than BART in these tricky situations.",
        "formal_text": "< menu: neutral > < incorrect output: positive > The decor could be a bit better, and if there was a small bar the overall atmosphere would be a bit more inviting. < place: negative > < incorrect output: neutral > Figure 6: Examples of BART classification. (a) is an instance with category do not occur as term in sentence. (b) represents that our method is not affected by the surrounding interference information. (c) needs conditional reasoning for analysis. Our method can obtain correct sentiment polarity. Figure 6 shows typical examples from the test set which cannot be inferred by the BART classification model. In sentence (a), the given category miscellaneous does not occur as a term in the given sentence. Our method can synthesize different sentiment polarities with different aspects to obtain correct polarity. In sentence (b), \"the value on the kids menu is good\", good modifies the value, rather than the given category menu. Our method gives the correct polarity, not being affected by the surrounding other aspect sentiments. The last instance (c) has conditional reasoning which is difficult for BART classification. In contrast, BART generation gives the correct label by correctly recognizing the negativity in \"if there was. . . would be a bit more inviting\".",
        "GPT2_formal_text": "\": [ -1, -2, -1, 0 ] }, { \"type\": \"trigram\", \"data\": { \"trigrams\": [ 1, 2, 3 ], \"mean\": [ 2.2, 2.5, 2.7 ], \"standard deviation\": [ 2.6, 2.9, 2.8 ], \"p-value\": [ 0.4, 0.7, 0.5 ] }, \"n\": [ 0, 1, 2 ] }, { \"type\": \"whole-word\", \"data\": { \"whole-words\": [ 1, 2, 3 ], \"mean\": [ 2.2, 2.5, 2.7 ], \"standard deviation\": [ 2.6, 2.9, 2.8 ], \"p-value\": [ 0.4, 0.7, 0.5 ] }, \"n\": [ 1, 2, 3 ] }, { \"type\": \"num_of_words\", \"data\": { \"num_of_words\": [ 1, 2, 3 ], \"mean\": [ 2.2, 2.5, 2.7 ], \"standard deviation\": [ 2.6, 2.9, 2.8 ], \"p-value\": [ 0.4, 0.7, 0.5 ] }, \"n\": [ 1, 2, 3 ] }, { \"type\": \"word\", \"data\": { \"word\": [ 1, 2, 3 ], \"mean\": [ 2.2, 2.5, 2.7 ], \"standard deviation\": [ 2.6, 2.9, 2.8 ], \"p-value\": [ 0.4, 0.7, 0.5 ] }, \"n\": [ 1, 2, 3 ] }, { \"type\": \"string\", \"data\": { \"string\": [ 1, 2, 3 ], \"mean\": [ 2.2, 2.5, 2.7 ], \"standard deviation\": [ 2.6, 2.9, 2.8 ], \"p-value\": [ 0.4, 0.7, 0.5 ] }, \"n\": [ 1, 2, 3 ] }, { \"type\": \"datetime\", \"data\": { \"datetime\": [ -1, -2, -1, -2, -1, -2, -1, -2 ] }, \"n\": [ 0, 1, 2 ] }, { \"type\": \"multip",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.361.json"
    },
    {
        "casual_text": "After that, we sifted out 95.5 million paper clusters using the criteria we mentioned earlier and took them out of our dataset. You can check out the breakdown of these filtered papers in Table 2. Turns out, a whole bunch of these clusters got the boot—80 million of them didn’t have any publisher-provided abstract or PDF, and 13 more came from the Unpaywall 2019-04-19 data dump. We used the cld2 tool to check the language, setting a threshold of 0.9 for the English language score. These papers without much text aren’t super helpful for our dataset right now. While they might still be useful as citation points in S2ORC, they’re generally not as high-quality, so we filtered them out to make the dataset better overall.",
        "formal_text": "Subsequently, 95. 5M paper clusters are filtered out based on the aforementioned criteria and removed from the corpus. The distribution of filtered papers is given in Table 2. We note that a large number of paper clusters are filtered out; 80. 0M of these filtered clusters have no associated publisher-provided abstract or associated PDF and 13 Unpaywall 2019-04-19 data dump 14 We use the cld2 tool for language detection with a threshold of 0. 9 over the English language score. do not provide significant value to our dataset in their current state. Although these papers that lack text may be useful as cite-able nodes in S2ORC, they are generally of lower quality and are filtered out of the corpus to improve corpus quality.",
        "GPT2_formal_text": "ed, ..., attention_masked_max) and (att_max_1, att_max_2, ..., att_max_k) are the contextual representations for the k-th token in the input sentence. Formal: We also need to check the current attention weights for the (att_a_i, att_b_i) pairs, where (a_i) is the attention score for the i-th token and (b_i) is the attention score for the i-th word. Formal: This process makes sure that the attention weights for the attention token (att_a_i, att_b_i) are as close as possible to (att_a_i, att_b_i) by minimizing the difference. Formal: We treat the attention weights as the output variables. Formal: Using this setup, the model can learn the relationship between attention score and the attribute, like (att_a_i, att_b_i) ∈ R d s ×d w. Formal: When it comes to the attribute, the model's weights (att_a_i, att_b_i) are calculated based on the attribute embedding space. Formal: The model's weights (att_a_i, att_b_i) are calculated based on the attribute embedding space. Formal: Formal: This process also helps improve how well the model handles new types of attributes. Formal: Lastly, we can figure out the attention weights for the attribute embedding space using the knowledge graph's embedding space. Formal: We can use the attention weights (att_a_i, att_b_i) from the knowledge graph to get the attention score (att_a_i, att_b_i). Formal: We can learn the attention weights (att_a_i, att_b_i) for the attribute embedding space. Formal: Lastly, we can learn the attention weights (att_a_i, att_b_i) for the attribute embedding space. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.447.json"
    },
    {
        "casual_text": "In these patterns, a sentence shows how something is part of something else. They use certain verbs. The \"part\" and \"whole\" words are usually in noun phrases or prepositional phrases with specific prepositions. We use these patterns in our app.",
        "formal_text": "In these patterns, the part-whole relation is indicated in a sentence. The patterns contain specific verbs. The part word and the whole word can be found inside noun phrases or prepositional phrases which contain specific prepositions. We utilize the following patterns in our application.",
        "GPT2_formal_text": "= {u_i, u_j} * p(x_i:x_j) + f_i,j. Formal: This equation represents a long short-term memory (LSTM) model that takes two inputs, x_i and x_j, as inputs. The goal is to predict the next word that will be generated based on the current state of the LSTM. Formal: The probability of a word w_t at time t is calculated by combining the probability of the current word x_i given the history x and the previous word w_t. Formal: Lastly, the model calculates the log likelihood of the next word x_t given the history x and the current state of the LSTM. Formal: Formal: Formal:\n\nThe linear state tracking model is a type of neural network that relies on the loss function (which is a probability distribution) and the previous hidden state. The loss function helps calculate the attention scores that help the model learn to predict the next word. Formal: Formal: Formal:\n\nAlso, in the sequence-to-sequence model, the next hidden state h_t is updated based on the input sequence x_i and the sequence history. Formal: Formal: Formal:\n\nTo understand how the model learns, we use a Bayesian network to model the hidden state history. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C10",
        "filename": "C10-2167.json"
    },
    {
        "casual_text": "1. Input: T is the set of MeSH terms (the topic we're searching for); n + is the number of positive examples we have; IR and θ T are a black-box machine learning-based IR system and its query settings.",
        "formal_text": "1. Input: T ← set of MeSH terms (the retrieval topic); n + ← number of labeled positive data; IR, θ T ← a black-box MLT IR engine, along with query parameters.",
        "GPT2_formal_text": "_m is the hidden vector for the mask. The output layer gets a hidden vector with one element: s_mask_m. We use a multi-head attention mechanism to get a better understanding of the input and output, which we call MF_attn. Formal: We refer to a latent representation as h_wt. Formal: In our model, we use an attention mechanism (like the one by Bahdanau et al., 2015) to handle the masked entity embedding and the latent representation. Basically, MF_attn(m) = h_wt * tanh(m_attn(m)) for the attention mechanism. The attention mechanism takes the target-side embedding, which we call x_t, as input and spits out a vector, e_attn, which takes the input embedding x_t and turns it into a binary value, α_t. This vector is then fed into a fully connected layer to get the final hidden representation, h_wt_m. Formal: In the encoder layer, we use a multi-head attention mechanism (MF_attn) to predict the embedding for each entity pair. Formal: In the decoder layer, we use a multi-head attention mechanism (MF_attn) to predict the hidden vector for each entity pair. Formal: Formal: We also use a BERT encoder and a BERT decoder to figure out the hidden vector for each entity pair. Formal: The result of these calculations is the hidden representation, h_wt_m. Formal: We use a hierarchical attention network (HAN) (Jiang et al., 2017) to estimate the feature representation. The encoder layers use attention mechanisms (like MF_attn) to predict the embedding for each entity pair. We take the multi-head attention mechanism (M_attn) and feed it into the decoder to calculate the hidden representation, h_wt_m. Formal: Finally, we combine all the entity pairs into one output by averaging their embeddings. Formal: Formal: To sum it up, we show that our model can effectively encode and represent different types of entities and their embeddings in a clean and structured way. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.47.json"
    },
    {
        "casual_text": "DISAANA keeps an eye on tweets as they happen, spotting info related to disasters and showing it in a neat, organized way. It works in two ways: QA and problem-listing. In the QA mode, you can just ask something like, \"What’s running low in Kumamoto?\" and it’ll give you a list of stuff that’s in short supply (check out Figure 2a). The answers are sorted into categories like medical supplies to make them easier to understand. You can also see the shortages on a map (Figure 2c). In the problem-listing mode, you don’t even need to ask—it just gives you a list of issues happening in a certain area, like \"people trapped under rubble,\" using a method from Varga et al. (2013) (Figure 2b).",
        "formal_text": "DISAANA analyzes tweets in real time, discovers disaster-related information, and presents it in organized formats. It has two modes: QA and problem-listing. In the QA mode, for example, a user can enumerate goods in short supply in Kumamoto merely by asking, \"What is in short supply in Kumamoto? \" (Figure 2a ). The answers are classified by such semantic categories as medical supplies for readability. A user can also enumerate them on a map (Figure 2c ). In the problem-listing mode, a user can obtain a list of problems, such as \"people were buried alive, \" which are occurring in a specified area (prefecture, city or town) without questions by using Varga et al. (2013) 's method (Figure 2b).",
        "GPT2_formal_text": "= [i, j, k, t, h], where j is the start token and k is the end token. The backward seq2seq model uses a bidirectional RNN to figure out the hidden representation, which is s = (y, h). Formal: In this model, each word gets its own hidden state that acts like a sentence's input. The word's representation is made up of its word embedding (x) and its character embedding (c). Formal: To make sure the model can handle multiple words at once, it uses a context vector that's based on the word embeddings. Formal: The forward seq2seq model takes a word and its context embedding as input. Formal: The forward model breaks down the words in the input sentence into individual words using a bidirectional RNN, and the output word is its hidden state. Formal: The backward model works the same way, but it combines the input word with its word embedding (x) and its character embedding (c) to get the hidden state. Formal: Formal: In this paper, the word embedding is the character embedding. Formal: This model uses a Context Vector Encoder (CVE) (Conneau and Lavie, 2017) to represent the context. Formal: Formal: We made a prediction to figure out the hidden representation (h_t) by applying the forward model's bidirectional RNN to the word embedding (x). Formal: Formal: This model uses a Context Vector Encoder (CVE) (Conneau and Lavie, 2017) to represent the context. Formal: Formal: This model uses a Context Vector Encoder (CVE) (Conneau and Lavie, 2017) to represent the context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-2055.json"
    },
    {
        "casual_text": "Psychological theories about irony, like echoic reminder theory (Kreuz and Glucksberg, 1989) and implicit display theory (Utsumi, 2000b), haven’t really been fully applied to analyzing text yet. Neuropsychology researchers who’ve looked into how the brain reacts to sarcasm say that understanding it depends a lot on not just the context of what’s being said, but also the speaker’s mood, personality, and things like facial expressions and tone of voice (Shamay-Tsoory et al., 2005). Without those non-verbal cues, figuring out sarcasm from just text has to rely mostly on the words and the situation, though you can still guess a bit about the speaker’s personality and mood from the text.\n\nThere are some fancy models that use things like the contrast between positive words and negative situations (Riloff et al., 2013), specific words, punctuation, and even emojis (González-Ibánez et al., 2011) to detect sarcasm. They work pretty well, but they usually miss out on understanding the speaker’s mindset and the broader context of what’s being said. Kreuz and Link (2002) point out that sarcasm is more likely when the speaker and listener share a lot of knowledge—like understanding the situation and knowing each other well.",
        "formal_text": "Psychological theories of irony, such as echoic reminder theory (Kreuz and Glucksberg, 1989) and implicit display theory (Utsumi, 2000b ) have yet to fully translate into text-analytic methods. Neuropsychology researchers who have sought patterns of brain activity to identify the neural correlates of sarcasm note that an understanding of sarcasm is highly dependent not just on the context of an utterance but on the state-of-mind and personality of the speaker, as well as on facial expressions and prosody (Shamay-Tsoory et al. , 2005). Without the latter markers, purely textual detection must depend largely on the content and context of an utterance, though speaker personality and state-of-mind can also be approximated via text-analytic means. Probabilistic classification models that exploit textual cues -such as the juxtaposition of positive sentiment and negative situations (Riloff et al. , 2013), discriminative words and punctuation marks, and emoticon usage (González-Ibánez et al. , 2011) have achieved good performance across domains, yet these models typically suffer from an absence of psychological insight into a speaker and topical insight into the context of utterance production. Kreuz and Link (2002) argue that the likelihood of sarcasm is proportional to the amount of knowledge shared by speaker and audience, which includes knowledge of the world and knowledge of the speaker and audience.",
        "GPT2_formal_text": "ed_categories = { { $k, $t } } → { { $k, $t } | head(S) => { $t, $k } }\n\nThen, we figure out the probability of the target word's type using the model's parameters. Formal: We'll use the model's parameters to figure out the probability of the target word's type, like this: Formal: So, let's say the final probability is p(t|s), which is the probability we get from the model's parameters. Formal: So, to get the probability of the target word's type given the source sentence and all its possible types, we can do the following: Formal: So, to get the probability of the target word's type given the source sentence and all its possible types, we can do the following: Formal: Formal: The probability of the target word's type given the source sentence and all its possible types is given by the model's parameters. Formal: The probability of the target word's type given the source sentence and all its possible types is given by the model's parameters. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D17",
        "filename": "D17-1050.json"
    },
    {
        "casual_text": "During semantic analysis, we take each word or \"leaf\" from the CCG tree we got from the syntactic analysis and give it a meaning based on some semantic templates. Then, we put these words together following the CCG tree structure to create a logical formula that shows the overall meaning of the whole sentence. This process is done using ccg2lambda for Japanese sentences.",
        "formal_text": "In semantic analysis, each leaf (lexical item) of the CCG tree obtained in the syntactic analysis is assigned a meaning from the semantic templates. The lexical items are then combined according to the CCG derivation tree to derive a logical formula that expresses the meaning of the entire sentence. The composition is performed using ccg2lambda in Japanese.",
        "GPT2_formal_text": "= { u i, j } = { u i, j } ∈ R n×d. Formal: If the prediction isn't about A_i, but it's about A_j, then there's a chance of an entity appearing at position i. Formal: For the H2 model, the chances of each entity being at position i are calculated by... Formal: You can check out the whole setup in Figure 3. Formal: To build the generation model, you can start with a simple entity mention (e_i) and some keyphrases (e_j) that are already in the knowledge base. Formal: To keep track of all the mentions, we use a multi-layer perceptron (MLP) for this. Formal: We're focusing on the entity mention embedding model, which we'll talk about in Section 3.2. Formal: In this setup, the mention embedding model is a linear mixed-projective MLP with a linear layer, with a hidden dimension of D_m. Formal: Each mention embedding vector is also a mixed-projective MLP with a linear layer, with a hidden dimension of D_m. Formal: To use the embedding vectors for entity mention embeddings, we use a BiLSTM with a hidden dimension of D_m and a position embedding dimension of D_p. Formal: Finally, the entities mention embedding model (EM) is a linear mixed-projective MLP with a linear layer, with a hidden dimension of D_m, which is D_p. Formal: We'll also look at the entity mention representation model (EM) in Section 3.2. Formal: To create the knowledge graph representation, we can start with a knowledge base entity mention embedding model (e_i) and some keyphrases (e_j) from the knowledge base. Formal: Once we have the entity mention embedding vector (e_i), the entity mention representation model (EM) is a linear mixed-projective MLP with a linear layer, with a hidden dimension of D_m, which is D_p. Formal: Finally, we calculate the knowledge graph representation using the entity mention embedding model, e_i, along with the keyphrases, e_j. Formal: To calculate the entity mention embedding vectors, we use a BiLSTM with a hidden dimension of",
        "directory": "acl",
        "filename": "2022.acl-srw.10.json"
    },
    {
        "casual_text": "The idea that S and I are independent when we know U lets us pick any speech dataset to train P(S|U). This means we can control speaker traits and other sound features separately from the I2U system (Henter et al., 2018; Akuzawa et al., 2018). Table 1 lists the five datasets we used to train the S2U, I2U, and U2S models. We intentionally used different datasets for each part to see how well the units hold up when moving between different areas, like changes in speakers, speaking styles (scripted or spontaneous), and types of content (books, newspapers, or image descriptions). Out of the three datasets with both images and speech pairs—Places, Flickr8k, and MSCOCO—we picked the last two for training I2U models. They have five captions per image, which works better for caption evaluation metrics like SPICE (Anderson et al., 2016). Plus, they're popular image captioning datasets with lots of text-based comparisons in the research. Places only has one spoken caption per image and hasn’t been used for captioning before.",
        "formal_text": "The conditional independence assumption between S and I given the U enables us to choose any arbitrary speech dataset for training P (S | U ), therefore enabling the speaker characteristics and other acoustic properties to be independently controllable from the I2U system Henter et al. , 2018; Akuzawa et al. , 2018). Table 1 summarizes the five datasets used for training S2U, I2U, and U2S models. Note that we deliberately choose different datasets for training each module, which aims to examine the robustness of the units when transferring across domains, including shift in speaker demography, speaking style (scripted/spontaneous), and linguistic content (book/newspaper/image description). Among the three datasets with image and speech pairs: Places, Flickr8k, MSCOCO, we chose the latter two for training I2U models, because they include five captions per image, which is more suitable for caption metrics such as SPICE (Anderson et al. , 2016); moreover, they are commonly used image captioning datasets with many text-based baselines in the literature. Places only contains one spoken caption per image and has not been used for captioning.",
        "GPT2_formal_text": "ing_final layer. This layer gives the model more information about the actions it's trying to take, which helps it spot the tricky ones. Formal: Our model uses a self-attention mechanism, which is pretty cool because it's been proven to work really well (Vaswani et al., 2017) for improving how well models understand language (Han et al., 2017; Devlin et al., 2018). Basically, we use a bunch of attention heads to focus on different parts of the source text, and these heads are connected using a non-linear function. Formal: Once the model spots a tricky action, it calculates an action type score, which helps it pick the right response, like a fix, based on how the text is structured. Formal: The model also gives each action type a weight, and if a response type doesn't have a matching action type, it just picks a random action type instead. Formal: In Section 4, we talk about a task where the model has to figure out which action type is better for a specific pair of actions. Formal: We show this task on two datasets, and here's what we found out: Formal: We think the same approach works for more complex tasks. The model might pick the wrong action type for an action if it's unclear. Formal: We're planning to explore ways to deal with this in future research. Formal: Our model works really well and is pretty flexible, so we plan to keep exploring new actions and the options for actions that can fit them. Formal: We want to look into how the model picks the right action type for an action and how we can figure out the best response type from the options. Formal: Lastly, we're also interested in how the model handles different types of actions, like fixing things, and how different actions work together. Formal: Formal: Formal: In Section 5, we suggest using a reward function that's less sensitive to how well the model's predictions match up with the data. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.411.json"
    },
    {
        "casual_text": "After the session, the system grouped together papers with titles that had words like \"nature,\" \"development,\" \"transfer,\" \"fablizalion,\" or \"generation.\" The user's query, which was in Japanese, was saved in this specific group. This setup means that next time, the system can quickly answer the same query and also add any new papers about the same topic to this group.",
        "formal_text": "The selected papers which included in tile titles words such as nature, development, transfer, fablizalion, or generation were gathered into one subspace by the system after the session, and the user's query was registered above tile subspace. This means that, next time, the system can give the answers to the (*)The query was given in Japanese. This is a tranMation made by the authors same query at once, and can register, in the proper subspace, new papers on tile same notion.",
        "GPT2_formal_text": ") = w. create_unique ( ) .\n          Casual: We're using a retrieval-based model to get the final summary. Basically, we're working with a source document D_s and a summary G_s. Here, D_s is the query document, G_s is the summary's summary, and the set of documents D_s is the set of documents in D_s. Formal: The task of summarizing questions and answers is kind of like summarizing stories—it has a ton of different parts, like the questions and the answers themselves. Formal: To help the model better understand these complex relationships, we use a fancier version of the SciBERT model (check out Section 3 for more details). Formal: We use a Bi-LSTM-based model to create a full-sentence summary. This model grabs the sentence representations from the document's representation layer. Formal: Each token in the query is turned into a single-layer Bi-LSTM, using its embedding from the query. Formal: We compare the original query and summary using cross-entropy loss, which is a standard algorithm for evaluating summarization methods. Formal: We also throw in some loss functions to see how well we're doing. Formal: The final summary, w_s, is basically the sum of the summaries from all the question-answer pairs. Formal: The score s(q) is calculated as the ratio of the query's probability p_q over the summaries' probability p_s, which gives us a score for the query. Formal: Formal: We also use a convolutional neural network (CNN) to get the embeddings of the question and answer tokens. Formal: Formal: Formal: Formal: The answer span is a fixed-size vector and it's calculated using a max pooling operation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C86",
        "filename": "C86-1089.json"
    },
    {
        "casual_text": "While testing, we notice that people tend to focus more on the words at the beginning of the source text. To see how this \"position bias\" affects things, we picked a group of samples with the same source length (77 sentences) from the WMT15 De→En test set. We then figured out the average attention each source position got within this group. \n\nSince different source positions might be paid attention to different numbers of times in SiMT, we made sure to average the attention weights based on how many times each position was looked at. This way, every source position gets a fair evaluation. \n\nTo break it down: if α ij is the attention weight between a target word y i and a source word x j, the average attention weight A j for source position j is calculated like this:",
        "formal_text": "During testing, position bias is reflected in the preference of paying more attention to the source words in front positions. To explore the specific impact of position bias, we select the samples with the same source length (77 sentences) in WMT15 De→En test set as a bucket, and then calculated the average attention weight obtained by each source position in the bucket. Since the times of each source position being paid attention to may be different in SiMT, the average attention weight is averaged on the times of being attended, so the evaluation is fair for each source position. Specifically, give the attention weight α ij between target word y i and source word x j, the average attention weight A j at source position j is calculated as:",
        "GPT2_formal_text": "_len = len/2 + sum from i=0 to n-1 of the sum from j=0 to n of the sum from k=0 to n of the sum from s=0 to n of the sum from t=0 to n of the sum from x=0 to n. Formal: Basically, if you have a query, q, you can figure out the attention value for a specific word in a sentence, by multiplying the attention values from the last k words of the sentence by the attention values for the words in the previous k words of the sentence. Formal: Using the embeddings for word embeddings and sentence embeddings, we can figure out the attention values for the word embeddings and sentence embeddings. Formal: Formal: In Figure 3, you can see an example of how to calculate the attention for a query, q, along with its query and its related sentences. Formal: We use a special attention module called A_q_t, which takes in both the query and the sentences in the current sentence. Formal: Lastly, we add the attention value A_q_t for the query and the current sentence to the query's attention. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.467.json"
    },
    {
        "casual_text": "2. The candidate people look at a lot, but isn't the right one, usually comes right after the correct candidate in the list.",
        "formal_text": "2. The most frequently fixated candidate other than the correct one follows the correct candidate in the ranking.",
        "GPT2_formal_text": "= (h1, ..., hK, ..., hK_P, ..., hL). Similarly, we can create a permutation θ by tweaking the loss function L_π. Formal: Sure, I’ll break it down for you:\n\nBasically, we have the following equation:\n\nf(k) = δ_ij * α_ij + α_ij + γ_ij * α_ij + γ_ij * α_ij + γ_ij.\n\nHere, δ_ij and α_ij are the parameters for how we pick k and h, respectively. And α_ij is the conditional probability of picking k and h, given the other options. Formal: To get a good estimate of f(k) based on these probabilities, we use something called the conditional probability, which we can write as:\n\nf(k) = γ_ij * f(k) + γ_ij * α_ij + γ_ij * α_ij + γ_ij * α_ij + γ_ij * α_ij. Formal: Here, γ_ij is the conditional probability of picking k and h given the other options, while γ_ij is the probability of choosing k and h based on the data, and α_ij is the probability of picking k and h given the data, but adjusted for the other options. Formal: Finally, to get the actual conditional probability of picking k and h, we calculate it as: Formal: So, f(k) is calculated by adding up all the conditional probabilities, plus the actual conditional probability. Formal: Finally, f(k) is calculated by adding up all the conditional probabilities, plus the actual conditional probability. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1269.json"
    },
    {
        "casual_text": "Customized engines are usually worth the cost when there's a strong case for a good return on investment and you're only dealing with a few language pairs.",
        "formal_text": "Because of the expense of customized engines, they are more easily justified in cases where there is a clear ROI argument, and where the number of language pairs is limited.",
        "GPT2_formal_text": "ed_tokens = [mask_tokens] for a in c. tokens. for each token in a: self. image_decoder(a, c) = self. extract_attention_masked_tokens(a, c)                    Casual: We're suggesting using a joint model with two versions: one with the masked tokens and another without. We'll also use a linear layer to make the model better at handling long-term dependencies. The downside is that the model we end up with won't be great for generalization. Formal: Basically, instead of just predicting the token labels y t, we want to create an image embedding D t using the masked token sequence x t. Formal: We add a linear layer to the output layer of a model to make it better at handling long-term dependencies. Formal: The second version we're suggesting doesn't use any external resources and only focuses on the masked tokens. Formal: The final layer has some linear layers for the hidden states. Formal: Lastly, we use the average of all the token outputs to figure out the image embedding. Formal: Formal: To make sure the model doesn't make any mistakes, we've also included some histogram plots showing the BLEU scores. Formal: Formal: Lastly, we've included some histogram plots showing the BLEU scores. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "amta",
        "filename": "2010.amta-commercial.5.json"
    },
    {
        "casual_text": "The JULIUS open-source speech recognition engine, which is designed for Large Vocabulary Continuous Speech Recognition (LVCSR), uses n-grams and context-dependent Hidden Markov Models (HMM) to turn spoken words into written text (Lee et al., 2008). How well it works depends on having the right language-specific stuff, like acoustic models, language models, and dictionaries. We’ve got some basic resources set up for English, German, Italian, and French, but they’re still pretty limited. To make things work better, we’ve also added online speech recognition for these languages using Google’s Speech API. This lets us run experiments with users and at the same time gather the data we need to improve and expand JULIUS’s language resources.",
        "formal_text": "The JULIUS open-source Large Vocabulary Continuous Speech Recognition engine (LVCSR) uses n-grams and context-dependent Hidden Markov Models (HMM) to transform acoustic input into text output (Lee et al. , 2008). Its recognition performance depends on the availability of language dependent resources i. e. acoustic models, language models, and language dictionaries. Our framework includes basic language resources for English, German, Italian and French. As those resources are still very limited we have also integrated online speech recognition for these four languages using the Google Speech API. This allows for conducting experiments with users while at the same time collecting the necessary data for augmenting and filling in JULIUS language resources.",
        "GPT2_formal_text": "are not always evenly spread out, which can lead to issues with discrimination when dealing with multi-class classification. To get around this, we use a mixed-effects model with random intercepts, which is shown in Eq. 8. This model includes a bunch of random variables (h_ij) that are related to the i-th class, which we randomly assign to the class space based on the class label. Formal: You can check out the model code here: https://github.com/llew/multilevel_label_feature_selection_pruning_modeling. Formal: Lastly, we calculate the marginal log-likelihood (η_lj) for the class-based distribution using a weighted linear regression (WDR). Formal: The predicted log-likelihood for the i-th class is calculated like this: Formal: In our experiments, we used standard logistic regression models to predict the label y_i, but since the predictions are multi-class, we adjusted the weights to minimize the cross-entropy loss (check out Appendix A for more on that). Formal: Formal: For more info, you can also take a look at the development and test set results in Appendix A. Formal: Formal: Lastly, let’s talk about how our method compares to the baseline model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E14",
        "filename": "E14-2022.json"
    },
    {
        "casual_text": "Basically, we’re saying that people from ASEAN countries can go study in other places, learn about how things work, and see what other countries have done to succeed. Then, they can bring that knowledge back home to help their own country grow. In the end, this helps some of the poorer people in ASEAN countries because they get better access to development. So, we shouldn’t get rid of ASEAN.\n\nOur last point is about keeping the peace in the region. Southeast Asia has a lot of potential for conflict—some countries are buddy-buddy with China and might like the idea of China being super powerful, while others are closer to the U.S. Some countries are communist, others are capitalist. There’s been trouble in the past, like the conflict over East Timor, and there are still tensions simmering under the surface in many places.\n\nBut one of the best ways to avoid fights between countries is to make sure everyone’s interests are tied together through trade. If everyone benefits from peace and loses out during conflict, it’s way less likely that a war will happen. That’s why we think ASEAN is a great tool for keeping the peace in Southeast Asia.\n\nSo, because ASEAN fights against colonialism, helps with development, and promotes peace in the region, we shouldn’t disband it. Thanks!",
        "formal_text": "So we tell you that people in ASEAN countries can study in other countries and learn about development and industry, and how other countries have been successful in the past, and use this in order to help their own home country. So at the end of the day, we help the people who are worse off in the world, some of them, some of these very poor people who live in ASEAN countries because we better access development so we shouldn't disband ASEAN. Our last argument is about peace in the region. Recognize that there are many potential sources of conflict within the southeast asia region. Some countries are more closely aligned with china so they see an advantage in china becoming more hegemonic, some countries are more aligned with the united states. Some countries are communists, some countries are capitalist. There's been conflict in the past over east timor, and there are other ethnic tensions boiling beneath the surface in many southeast asian countries. But one of the surest ways to prevent international conflict, is to tie everyone's interests together through trade. If everyone stands to get richer through peace and poorer through conflict, then it's much less likely that a war will break out in the region. So for that reason we think ASEAN is a tremendous tool for peace in southeast asia in the future. So because it's an anti colonial institution, because it promotes development, and because it will lead to peace in the region, we should not disband ASEAN thank you. \"",
        "GPT2_formal_text": "ed_gaussian_softmax (M = 0.5) is a fancier version of the transformer model. It focuses on the idea that a word's position in a sentence can tell you about its meaning, especially if that word is the main one or one of the main characters. The tricky part is that, as we mentioned, the results in this experiment aren't directly comparable to those in earlier research. The word embeddings in the model we're talking about here aren't directly connected to the positions in the sentence, so they're not directly comparable to the embeddings in the LSTM. We call these vectors \"position embeddings\" because they can be used to figure out the meaning of a word. Formal: The (N, O) part is just a way to show the total number of embeddings in a pre-trained word embedding matrix, which we call N_w. Formal: For each token x_i in the embedding matrix N_w, we take the dot product of the word embedding vector y_i and the position embedding vector x_i. Formal: You can find more details about the dataset, model, and embedding vectors in Appendix B. Formal: We ran some experiments to see how each of the four transformer models performed. The results are in Table 1. Formal: To calculate the hidden representation for a token x_i, we use the same setup as in the Transformer model. Formal: You can check out the model details in Appendix B. Formal: We tested the word embedding models using the BERT embeddings, which come from the pretrained pre-trained word embedding models, as mentioned in Section 3.3. Formal: The embedding vector for a word w_i is calculated using the e_w embedding vector for the word. Formal: Formal: If the word w_i is part of a character sequence (like in a word form) and its position embedding, x_i, is in the sequence x_i, then the score for the sequence x_i equals the score for the character sequence x_i. Formal: In this case, x_i is in the sequence x_i and x_i is part of a character sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "We mix our structure-aware uncertainty sampling with the bachelor recognizer to create the final acquisition function.",
        "formal_text": "We combine our structure-aware uncertainty sampling with the bachelor recognizer to form the final acquisition function:",
        "GPT2_formal_text": "= {h i } for all the entity mentions i in the chain. Formal: So, a_i is the mention itself, and c_i is the mention context. The set E_c(C) includes all the mentions that come before the mention in the chain, like m_i - 1 up to m_i + 1. Formal: If a_i has a context c_i, then: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.270.json"
    },
    {
        "casual_text": "In the parametric methods, the first approach gives all the hidden states in a layer the same window size to look at the sequence's context. The second approach, though, figures out a different window size for each hidden state.",
        "formal_text": "Among the parametric methods, the first strategy assigns a unified window size to all the hidden states in a layer, so as to consider the context of the sequence, while the second one calculates a distinct window size for each hidden state.",
        "GPT2_formal_text": "er. This setup means that the model learns to focus on the important parts of the input (like the main word) instead of just looking at the bigger picture. This approach works better for unsupervised dependency parsing. For instance, Zhang et al. (2018) suggested using a different attention-based sequence labeling model and used the attention-masking setup in their experiments. Formal: This paper talks about a few different ways to use attention and how to calculate the attention score in NLP. Formal: Formal: We tested different attention models and their variations in three different setups. Formal: The results show that using different attention methods can really help improve the performance of dependency parsing models. Formal: We also looked into different attention methods in relation to dependency parsing, including self-attention, edge-attention, and hybrid attention. Formal: Lastly, we looked at some recent research on how to learn features for dependency parsing, including sequential, label-aware, and hierarchical methods, along with attention mechanisms. Formal: Our work represents the first time anyone has tried to learn graph representations for dependency parsing using sequential methods. Formal: In this paper, we introduced a new attention-based sequence labeling model that uses hierarchical attention and the attention-masking approach. Formal: This model helps the model generate the most useful information by looking at the input graph. Formal: We also introduced a new graph neural network (GNN) with a label-aware setup, which can learn features for dependency parsing. Formal: Lastly, we tested a few graph neural network models and their variations, including the self-attention, edge-attention, and hybrid attention. Formal: These models work by looking at the input graph and applying different attention methods to it. Formal: These models learn different features by combining different attention methods in a hierarchical way. Formal: For the final results, we compared these graph neural network models and their variations to our basic model. Formal: For more details, you can check out the original papers. Formal: We also reported on some recent research on dependency parsing, including hierarchical attention, sequential, label-aware, and label-aware version of linear algebra. Formal: We also ran some experiments on the dependency-aware graph neural network. Formal: Lastly, we compared these graph neural networks to ours. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1475.json"
    },
    {
        "casual_text": "For a deeper dive, we zeroed in on three specific features: (11) case marker, (15) intra/inter, and (18) dependency. We then calculated precision (P), recall (R), and F-measure (F) for each combination of these features in our test cases. Table 4 compares the BiReg+F syn model and the FixRank+F syn model, which had the highest overall accuracy according to Table 3. \n\nOne key difference between these two models is that the FixRank model performs better in terms of F-measure for directly dependent arguments, especially for ga arguments. When it comes to directly dependent ga arguments, the FixRank model has a better recall value while maintaining a similar precision value. On the flip side, for directly dependent arguments with other case markers, the BiReg model has better precision.\n\nWe also looked at how many directly dependent arguments each model picked as the answer. The FixRank model selected 19,612 directly dependent arguments, with 13,751 of those being ga arguments. Meanwhile, the BiReg model picked 12,520 directly dependent arguments, including 7,208 ga arguments. These numbers indicate that the FixRank model is more likely to choose directly dependent arguments without much regard for their case markers.",
        "formal_text": "For further analysis, we focused on the specific three features: (11) case marker, (15) intra/inter, and (18) dependency, and calculated precision (P), recall (R), and F-measure (F) for the test cases with each combination of these features. Table 4 shows the comparison between the BiReg+F syn model and the FixRank+F syn model that show the highest accuracy in total in Table 3. The notable difference between these two model is that the FixRank model shows the better Fmeasures for directly dependent arguments, particularly in ga arguments. Concerning the directly dependent ga argument cases, the FixRank shows the better recall value with the similar precision value. In contrast, in the cases of directly dependent 'other' case markers, the BiReg shows the better precision value. We investigated the number of directly dependent arguments that each model selected as the answer. The FixRank model selected 19, 612 directly dependent arguments, and 13, 751 ga argument among them, while the BiReg model selected 12, 520 directly dependent arguments with 7, 208 ga arguments. These numbers suggest that the FixRank model tends to choose directly dependent arguments regardless of their case markers.",
        "GPT2_formal_text": ", tokens, and tags for a document, respectively. This paper focuses on a specific scenario: a single-language document with limited resources. However, our system could work for any language pair with limited resources. We evaluate how well it does using different metrics, including accuracy, macro F1, micro F1, and micro-F1. We also test the system by evaluating the results on a dataset with 50,000 samples. Formal: The authors' work was partly funded by the European Union through the Horizon 2020 project (project no. 187509), the Marie Skłodowska-Curie grant (project no. 2014-23211), and the grant from the Institute of Electrical and Electronics Engineering (IEE) to Prof. Marek Szpakowicz, Prof. Marek Tędziarz and Prof. Marcin Szymanski. Formal: The pre-processing steps include tokenization, normalization, lemmatization, and fixing spelling issues. Formal: We created a multi-lingual document similarity model using a CKY algorithm (Baker et al., 1993). This method uses a matrix S to figure out how related documents are across languages. Formal: To make our system work for languages with limited resources, we used two sets of data: the 1-best and 5-best lists from the SST-2 dataset (Oepen et al., 2006) and a smaller set from the MSCOCO dataset (Kilgarriff et al., 2014). Formal: We tested our system on the MSCOCO dataset (Kilgarriff et al., 2014). Formal: We built a multi-lingual document similarity model using a CKY algorithm. We made the matching matrix S equal to the number of documents in each language, using a matrix S = M = N ×d. Formal: We also checked how well the system worked on the MSCOCO dataset, using the 2-best lists. Formal: Formal: We used the MSCOCO dataset to train our model. Formal: We built a multi-lingual document similarity model using a CKY algorithm. We made the matching matrix S equal to the number of documents in each language, using a matrix S = M = N ×d. Formal: Formal: Formal: We tested the system on the MSCOCO dataset. Formal: Formal: Formal: We also tested the system on the",
        "directory": "C16",
        "filename": "C16-1269.json"
    },
    {
        "casual_text": "Okay, so softmax(•) is just a function that normalizes stuff along the last dimension, and c(0) is the relation-aware representation we get from the sentence s. After that, we combine this relation-aware representation c(0) with the original sentence representation s using a gate mechanism (like the one from He et al., 2016) and add a residual connection. We also throw in layer normalization (thanks to Ba et al., 2016) for good measure.",
        "formal_text": "where softmax(•) denotes a normalization function along last dimension and c (0) is the resulting relationaware representation corresponding to the sentence s. Then we merge the relation-aware representation c (0) into original sentence representation s by an element-wise gate mechanism with residual connection (He et al. , 2016) and layer normalization (Ba et al. , 2016), i. e. ,",
        "GPT2_formal_text": "_len is set to -1. The feed-forward network is trained to give each token in a sentence an attention score. Formal: The encoder has a hidden size of 128, and the decoder is 128. The word embeddings, h_w and h_d, are set to 128 and 64, respectively. Both the encoder and decoder have been pre-trained using the word2vec library (Mikolov et al., 2013). Formal: We use the LSTM-based language model with a hidden size of 128, a hidden state dimension of 128, and a multi-head attention mechanism with a dimension of 64. We use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 1e-5, with a mini-batch size of 64. We tune the parameters using the development set, and the batch size is set to 256. For inference, we use the Adam optimizer with a learning rate of 1e-4 and a mini-batch size of 32. Formal: Our experiments show that our hierarchical attention-based model performs better than the latest neural network models, especially when it comes to text and speech tasks. Formal: Our model is a combination of a Bi-LSTM-based neural network and a regular transformer-based model. We're also using the BERT-base architecture, which was introduced by Devlin et al. (2019). Formal: We're using the feed-forward network to create the contextual representations for each token. We're using the Adam optimizer with a learning rate of 1e-4, a mini-batch size of 128, and a batch size of 32 for inference. Formal: We're using the Bi-LSTM-based model for speech and text tasks, and we're also using the Adam optimizer with a learning rate of 1e-4, a mini-batch size of 128, and a batch size of 32 for inference. Formal: The final results are in Table 1. Formal: Formal: Table 2 shows how the model performs in terms of average sequence length on the dev set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "coling",
        "filename": "2020.coling-main.145.json"
    },
    {
        "casual_text": "We discovered that CHARAGRAM performs better on word similarity tasks when we use more character n-grams. This is covered in Section 4. Our top result came from the biggest model, SL999 Hill et al. 201452 Schwartz et al. (2015) 56 Faruqui and Dyer (2015) 58 Wieting et al. (2015) 66. 7 CHARAGRAM (large) 70. 6 (Luong et al., 2013), with SL999 for model selection. We got a Spearman's ρ of 47.1, which beats the 41.8 from Soricut and Och (2015) and is close to the 47.8 reported by Pennington et al. (2014), who trained on a 42 billion token corpus.",
        "formal_text": "We found that performance of CHARAGRAM on word similarity tasks can be improved by using more character n-grams. This is explored in Section 4. 4. Our best result from these experiments was obtained with the largest Model SL999 Hill et al. 201452 Schwartz et al. (2015) 56 Faruqui and Dyer (2015) 58 Wieting et al. (2015) 66. 7 CHARAGRAM (large) 70. 6 (Luong et al. , 2013), using SL999 for model selection. We obtained a Spearman's ρ of 47. 1, which outperforms the 41. 8 result from Soricut and Och (2015) and is competitive with the 47. 8 reported by Pennington et al. (2014), which used a 42B-token corpus for training.",
        "GPT2_formal_text": "= {'g', 'l', 'q', 'e', 'r', 's', 't', 'a', 't', 'r', 's', 'b', 'c', 'e', 'c', 'p', 't', 'a', 'r', 'l', 'b', 'e', 'r', 't', 'c', 'c', 'p', 'e', 'r', 's', 't', 'c', 'r', 'l', 'b', 'e', 's', 'c', 'p', 't', 'a', 'r', 'c', 'e', 's', 'b', 'e', 'g', 'e', 'c', 't', 'a', 'g', 'r', 's', 'b', 'e', 'c', 'q', 't', 'a', 't', 'r', 'l', 'b', 'e', 't', 'b', 'e', 's', 't', 'c', 'p', 'g', 'c', 'e', 's', 'c', 'e', 'b', 'e', 's', 't', 'c'}, {'q', 'g', 'l', 'r', 's', 't', 'a', 't', 'r', 's', 'b', 'e', 'c', 'e', 'p', 't', 'a', 'r', 'l', 'b', 'e', 's', 't', 'c', 'p', 't', 'a', 'r', 'l', 'b', 'e', 's', 't', 'c', 'q', 't', 'a', 'g', 'e', 'c', 'p', 't', 'a', 'g', 'r', 's', 'b', 'e', 'c', 'p', 'e', 'r', 'l', 'b', 'e', 's', 't', 'c', 'p', 'g', 'e', 'c', 's', 'b', 'e', 'c', 'p', 's', 't', 'c'}, {'s', 't', 'a', 't', 'r', 's', 'b', 'e', 'c', 'e', 'p', 't', 'a', 'r', 'l",
        "directory": "D16",
        "filename": "D16-1157.json"
    },
    {
        "casual_text": "In Hebrew, when a verb has a sentential object, it's like \"n9~9.\" In German, it's \"kennen,\" and in French, it's \"conaitre.\"",
        "formal_text": "or \"n9~9\" in I lebrew, when the verb takes a sentential object) know -> kennen (ill German; or \"conaitre\" in French;",
        "GPT2_formal_text": "_max and attention_mask_max are the attention weights and attention mask, respectively. Formal: The model uses a bidirectional RNN with a hidden dimension of size d_s and a hidden layer dimension of d_h. Each word w_i in the input is represented by a vector x_i, which is a 2D tensor with dimensions d_s×d_h. The hidden layer size is d_s. The encoder is a GRU with a dimension of d_s, and the decoder is a CNN with a dimension of d_h. The word embedding vectors x_i are then passed through a fully connected layer, which gives us the final representations x_i for each word in the input. Formal: The model calculates the cosine similarity between the embedding vectors x_i and the target vectors x_t. Then, the word embedding vectors are combined by the softmax layer, which gives us the final representation for the target word w_i. Formal: The whole process is laid out in equations (2), (3), and (4). Formal: The model can calculate the cosine similarity between embedding vectors x_i and the target vectors x_t. This is done using an LSTM cell with a hidden dimension of size d_s and a hidden layer dimension of d_h. Formal: Finally, the final word embedding vector x_i is calculated using a fully connected layer, which gives us the final representation x_i for each word in the input. Formal: The word embedding vectors x_i are then combined using a fully connected layer, which gives us the final representation x_i for each word in the input. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C88",
        "filename": "C88-1042.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way. We're looking at how the Transformer model does in two different situations: the \"common-practice\" setting and the \"wug test\"-like setting.\n\nIn the \"common practice\" setting, we're basically using data from past shared tasks and related research (like the work by Cotterell et al. from 2016, 2017, McCarthy et al. in 2019, and Vylomova et al. in 2020). This data usually includes a good number of lemmas, and there's some overlap between the lemmas used in training and those in the test sets. We're using this shared task data to represent the common-practice setting.\n\nNow, in the \"wug test\" setting, things are a bit different. Here, we control the number of lemmas for training, but not the inflected forms (as explained in Section 2). The lemmas we ask the model to inflect are always new, never seen before.\n\nSurprisingly, the Transformer model performs really badly in the \"wug test\"-like setting, even though there's a lot of training data for languages like those from 2018 or the Niger-Congo languages, which have very regular and simple inflections. The performance is way worse than in the common-practice setting, even when there are seven times more training triples for languages like Finnish, Spanish, and Turkish (check out Figure 1 for details).",
        "formal_text": "Common-practice test and \"wug test\" We first compare the performance of the Transformer in the common-practice setting and the \"wug test\"-like setting. The \"common practice\" is represented by (2) hallucination (1) copy previous years' shared tasks and related work (Cotterell et al. , 2016 (Cotterell et al. , , 2017 McCarthy et al. , 2019; Vylomova et al. , 2020); here the training data usually covers a fair number of lemmata and there is overlap between lemmata in the training and test sets. We use the shared task data to represent the common-practice setting. In the \"wug test\" setting, we control the number of lemmata for training but not inflected forms (as explained in Section 2) and the lemmata to be inflected are always previously unseen. To our surprise, the performance of the Transformer at the \"wug test\"-like setting is very poor despite the large amount of training triples for 2018-languages or the very regular and straightforward inflection for Niger-Congo languages. The performance is dramatically inferior to the common-practice setting, even when the number of training triples is seven times larger for Finnish, Spanish and Turkish (see Figure 1 ).",
        "GPT2_formal_text": "_before(f, d) = m(f) - r(d) + c(f, d) - r(d). Formal: To figure out the noise distribution P(f | d), we check how close the actual labels are to the noise distribution P(d). This helps us tell if the actual labels are too close to the noise. Formal: (4) We use the log-likelihood function to estimate the actual labels. Formal: (5) We then use this estimated label distribution to figure out the noise distribution P(d). Formal: (6) Finally, we combine the two distributions to get the final label distribution P(f | d). Formal: (7) Finally, we use the log-likelihood function to estimate the actual labels. Formal: Formal: (8) We also estimate the noise distribution P(d) by looking at the label distribution P(f | d). Formal: (9) Finally, we use the log-likelihood function to estimate the noise distribution. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-short.84.json"
    },
    {
        "casual_text": "To get a better understanding of how things are going at the segment level, Figure 4 shows a comparison between our method and Thot for the first 300 segments of the Autodesk test set used in the multi-domain experiment. The plot highlights the differences in TER (Translation Edit Rate) between the MT output and our approach, as well as between MT and Thot for automatically post-edited segments. \n\nWhat we see is that our approach changes fewer segments compared to Thot. This is because our method only builds a model if it finds useful data in the knowledge base; otherwise, it leaves the MT segment as is. When Thot modifies these untouched MT segments, it often makes things worse instead of better (as you can see from the many negative peaks for Thot in Figure 4). \n\nThis suggests that, compared to other online approaches, the output from our solution is more likely to be helpful for human translators. This benefit isn’t just about making post-editing easier or more enjoyable—it also saves time by providing better suggestions overall.",
        "formal_text": "To gain further insights about the performance at the segment level, the plot in Figure 4 compares our approach against Thot for the first 300 segments of the Autodesk test set used in the multi-domain experiment. It shows the differences between the segment-level TER of the MT (T ER MT ) and our approach (T ER Our approach ), and MT and Thot (T ER T hot ) automatically post-edited segments. We notice that our approach modifies less segments compared with Thot, because it builds a model only if it finds relevant data in the knowledge base, otherwise it leaves the MT segment untouched. These untouched MT segments, when modified by Thot, often lead to deterioration rather than to improvements (as seen by many negative peaks for Thot in the Figure 4). This suggests that, compared with the other online approaches, the output obtained with our solution has a higher potential for being useful to human translators. Such usefulness comes not only in terms of a more pleasant post-editing activity, but also in terms of time savings yield by overall better suggestions.",
        "GPT2_formal_text": "_len is the max number of bits the attention mask gets to keep, while attention_mask_1 is the max number of bits the current attention mask is set to. This setup can be done in one go, without needing a second pass. The output is a matrix with the size of v. Formal: Here, w_m is the weight for the word m in the first attention mask, and it's in the range from 1 to the max value of the first mask. This setup helps prevent the attention mask from getting too focused on a particular word. Formal: In the second attention mask, the attention is spread out over all the words, but the attention mask still picks up on the important words. Formal: The last three attention masks are the same as the first three, but instead of focusing on the words in the first three masks, they're checking out the words in the second and third masks. Formal: The attention mask for a word s_i in a document d_i is calculated like this: Formal: We'll explain more about the model's parameters in section 4. Formal: Each attention mask is like a random variable in the formula, but it’s also part of the multimodal hidden layer. Formal: We’ll call the hidden representation of the vector h_m for the word m in the second mask, h_m_i, and also the hidden representation for the word m in the first mask, h_m_1. Formal: Here, m_i represents the hidden representation of the word m in the second mask, and h_m_i_i_1 is the hidden representation for the word m in the first mask, h_m_1_i_i_1. Formal: The results from this set of simulations are shown in the last two rows of Table 1. Formal: The second and third masks are similar to the first two, but instead of the words in the first two masks, they’re checking out the words in the second and third masks. Formal: For the word m in the second mask, the attention mask gives it a weight of 0. Formal: The second and third masks use the same setup as the first three. Formal: Formal: Formal: The fourth and fifth masks use the same setup as the first three. Formal: Formal: Formal: The last three masks use the same setup as the first three. Form",
        "directory": "amta",
        "filename": "2016.amta-researchers.1.json"
    },
    {
        "casual_text": "CSCs cover general cases and syntactic rules. What sets our model apart is that it uses cases for generation, which isn't something most other generators do—they mostly just rely on syntactic rules.",
        "formal_text": "CSCs represent generalized cases and syntactic rules. Use of cases for generation is one of the unique features of our model while most generators solely depend upon syntactic rules.",
        "GPT2_formal_text": "= [sum from i=0 to m-1 of int(w) for w in wp] Formal: The stuff from the outputs is fed into the BiLSTM, and it takes the hidden state for each word in the input. We do this by figuring out the hidden state h t and doing a softmax operation. The hidden state h t is calculated by dividing the input word x t by the output word y t, and then adding the sum over all the words in the input. Formal: The final hidden state h t for the biLSTM is calculated like this: Formal: At the end, the combined hidden states h t for each word in the input are put together into a single hidden state h t ∈ [h t−1 for the one after the last word, h t+1 for the one before, and so on, up to h t]. Formal: Formal: Formal: The LSTM process can be explained like this: Formal: So, the final hidden state h t is calculated as h t ∈ [h t−1 for the one after the last word, h t+1 for the one before, and so on, up to h t]. Formal: Formal: After that, the output word y t gets added to the final hidden state h t. Formal: Formal: Formal: After that, we can plug h t into the final LSTM to get the final output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C90",
        "filename": "C90-2038.json"
    },
    {
        "casual_text": "It's clear that our vanilla depLCNN+NS, which doesn't use any extra lexical features, still beats the best systems out there—MVRNN+ and CNN+—by a big margin. Those systems actually use extra lexical features, but our approach with dependency paths works even better. When we add similar lexical features to our model, it gets even stronger, improving by 1.6%. This makes it way better than any other system we've seen.",
        "formal_text": "We can see that our vanilla depLCNN+NS, without extra lexical features, still outperforms, by a large margin, previously reported best systems, MVRNN+ and CNN+, both of which have taken extra lexical features into account, showing that our treatment to dependency path can learn a robust and effective relation representation. When augmented with similar lexical features, our de-pLCNN+NS further improves by 1. 6%, significantly better than any other systems.",
        "GPT2_formal_text": "ing=0.1        Casual: We also had the idea of using the final layer's hidden state as the label for the attention vector in the sequence. But, we didn't try that because it didn't work well. Turns out, the hidden state in the last hidden layer is the most crucial part for figuring out attention values. We didn't even mention that in our original paper. Formal: We tested this approach on the Switchboard test set, which has pairs of utterances. The results are in Table 1. Here's what we noticed: Formal: We noticed that the baseline model (B) didn't perform as well as the label-aware model (D). Plus, the model that uses attention (A) is just as good as the one that doesn't use it. Formal: Even with the attention masking, the baseline model still does pretty well. Even if the focus for the last hidden layer isn't being used, the model can still grab the attention vector and use it to figure out the attention values. Formal: The focus is still on the last hidden layer, even if the last hidden layer isn't being used. Formal: The final hidden layer is the key for the final attention vector. Formal: The attention vector is calculated based on the last hidden layer's output, which is like the last sentence's output. Formal: We also checked the accuracy and macro F1 scores for the two models. Formal: The results show that the model that uses attention (A) is just as good as the baseline model. Formal: The model that doesn't use attention (B) is better than the baseline model. Formal: When we used the final hidden layer's output as the label for the attention vector, the performance was basically the same as the model that doesn't use it. Formal: The score for the attention masking model (D) was pretty good too, so it's likely the model using attention (A) is doing a good job too. Formal: We also tested the model using attention (D) with attention masking. The results are in Table 2. Formal: The results show that the model using attention (A) is better than the baseline model. Formal: In Table 3, we also included the F1 score for the model using attention (D). Formal: Finally, the final hidden layer's output is the key for the final attention vector. Formal: The final hidden",
        "directory": "D15",
        "filename": "D15-1062.json"
    },
    {
        "casual_text": "You can grab predicates one at a time using this interface, or you can just download them all at once from the tagging lexicon tool.",
        "formal_text": "With this interface predicates can be either acquired one by one or automatically downloaded from the tagging lexicon acquisition tool.",
        "GPT2_formal_text": "\": [], \"contexts\": [], \"template\": [], \"embeddings\": [], \"next\": [], \"prev\": [ ], \"value\": [ ], \"conditions\": [], \"type\": \"None\" }, { \"type\": \"None\" } ], \"scheme\": { \"type\": \"Int\" }, \"attributes\": { \"key\": [], \"value\": [ ], \"conditions\": [ ], \"type\": \"None\" }, { \"type\": \"Int\" } ], \"embeddings\": [], \"next\": [ ], \"prev\": [ ], \"value\": [ ], \"conditions\": [ ], \"type\": \"None\" }, { \"type\": \"Int\" } ], \"template\": [], \"embeddings\": [], \"next\": [ ], \"prev\": [ ], \"value\": [ ], \"conditions\": [ ], \"type\": \"None\" }, { \"type\": \"Int\" } ], \"scheme\": { \"type\": \"Int\" } ], \"attributes\": { \"key\": [], \"value\": [ ], \"conditions\": [ ], \"type\": \"None\" }, { \"type\": \"Int\" } ], \"embeddings\": [], \"next\": [ ], \"prev\": [ ], \"value\": [ ], \"conditions\": [ ], \"type\": \"None\" }, { \"type\": \"Int\" } ], \"template\": [], \"embeddings\": [], \"next\": [ ], \"prev\": [ ], \"value\": [ ], \"conditions\": [ ], \"type\": \"None\" } ], \"scheme\": { \"type\": \"Int\" } ], \"attributes\": { \"key\": [], \"value\": [ ], \"conditions\": [ ], \"type\": \"None\" }, { \"type\": \"Int\" } ], \"embeddings\": [], \"next\": [ ], \"prev\": [ ], \"value\": [ ], \"conditions\": [ ], \"type\": \"None\" }, { \"type\": \"Int\" } ], \"template\": [], \"embeddings\": [], \"next\": [ ], \"prev\": [ ], \"value\": [ ], \"conditions\": [ ], \"type\": \"None\" }, { \"type\": \"Int\" } ], \"scheme\": { \"type\": \"Int\" } ], \"attributes\": { \"key\": [], \"value\": [ ], \"conditions\": [ ],",
        "directory": "eamt",
        "filename": "2002.eamt-1.9.json"
    },
    {
        "casual_text": "This setup needs an interface that can handle a few specific tasks. First, any queries the grammar writer explicitly mentions need to be sent to DATR. Then, each of these queries, along with their \"resulthag\" values, has to be converted into a PATR path equation (which is kind of like a description of a feature structure) and sent to the PATR system. \n\nThe tricky part about this whole process is that for every unique PATR path, you need to know the exact DATR query that goes with it. It might sound easy to just figure out which paths are defined for a particular node, but that doesn't work because of inheritance—basically, the whole network could be involved. So, in practice, all the PATR structures (except for the basic values) have to be defined twice: once in the DATR statements and again in the queries. \n\nThis duplication is a pain, and it can't be avoided unless you set up types for the feature structures that can be used when writing the queries.",
        "formal_text": "Such an approach requires an interface with the following functions: Queries that the grammar writer has stated explicitly have to be passed on to DATR. Every query together with the resulthag value has to be transformed into a PATR path equation (that partially describes a feature structure) and passed on to the PATR system. What is most disturbing about this strategy is the fact that for every distinct PATR path you have to know the corresponding DATR query. It is tempting to think one could simply check which paths are defined for a given node, but this doesn't work because of inheritance: the entire network is potentially relevant. So in effect all the PATR structures except the atomic values have to be defined twice: once in the DATR statements and once in the queries. This redundancy cannot be eliminated unless types for the feature structure are declared which are consulted in formulating the queries.",
        "GPT2_formal_text": "_vec. The size of the hidden layer in the last layer is calculated using the formula λ_len = (1 − α_len)^2, where α_len is the weight for the last hidden layer in the model. To get the attention vector p_0_t, we take the average of the input word vectors from the first two layers, then average them again for the last hidden layer. This gives us p_0_t = a_0_t + a_0_t^2. After that, we get the output representation for the word x_0_t, which is p_0_t = (a_0_t + a_0_t^2). Formal: In this setup, we have a bunch of word representations. To get the attention vector, we calculate the average of the attention scores from the first two layers of the model. Formal: After the convolution operation, we combine the hidden representations of all the words to get the final representation of the word. Formal: For each word, we grab its embedding h_t from the last hidden layer. Formal: Each vector in the output vector p_0_t gets updated with the word embedding h_t. Formal: Finally, the attention weights get updated to reflect the values of the words, which in this case is the vector p_0_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E91",
        "filename": "E91-1024.json"
    },
    {
        "casual_text": "Here, len refers to the output sequence length we're aiming for. LRPE and LDPE should be able to create sentences of any length, even if the exact length wasn't part of the training data. Takase and Okazaki (2019) took a different approach, focusing on character-based length constraints for summarization, meaning they limited the number of characters in the summary.",
        "formal_text": "where len is the given output sequence length. LRPE and LDPE are expected to generate sentences of any length even if sentences of an exact length are not included in the training data. Takase and Okazaki (2019) used character-based lengths for summarization constraints in the number of characters.",
        "GPT2_formal_text": "_sent:\n          Casual: In this paper, we looked into the concept of \"sentiment drift\" and how it could affect Twitter sentiment analysis. We came up with a way to calculate something called \"sentiment drift\" using sentiment data that we found in tweets. Using this method, we got a new Twitter dataset with around 2.3 million tweets, which is similar to what we had before. Then, we ran a bunch of experiments to see how these changes affect the results. Formal: To make things easier to understand, we’ll start by explaining what \"sentiment drift\" is. After that, we’ll explain how we calculated \"sentiment drift\" using this new dataset. Formal: Lastly, we’ll look at the results of different methods to see how they affect things. Formal: The main idea behind \"sentiment drift\" is that tweets with similar sentiments tend to be more similar to each other. So, if two tweets are alike, they’re probably related. To break down the method we used, we’ll give you the specifics. Formal: When we calculated \"sentiment drift,\" we focused on the percentage of times two tweets share the same sentiment. We didn’t include a \"sentiment drift\" for each tweet. Formal: We tested our method on two well-known datasets for sentiment analysis: TwitterEval (Nguyen and Grishman, 2018) and NYTimes800k (Reddy et al., 2019). In the next section, we’ll explain our method for creating this new dataset, which is called \"Eval-2016.\" Formal: After calculating the \"sentiment drift\" for each tweet, we averaged the results and adjusted the number of examples to match the size of the Twitter dataset. Formal: To make things more fair, we also averaged the results for each tweet to get the average. Formal: In the first round of experiments, we looked at how well our method could predict tweets based on the sentiment of the tweets. The results were pretty good, with a precision of 0.81 and a recall of 0.77 for our method. Formal: In the second round, we focused on how well the model could predict tweets based on the sentiment of the tweets. The results were pretty good, with a precision of 0.78 and a recall of 0.73 for our method. Formal: Lastly, we tweaked the",
        "directory": "coling",
        "filename": "2020.coling-main.319.json"
    },
    {
        "casual_text": "Even though models like BERT have done really well in a lot of tasks, they have a couple of issues when it comes to joint entity relation extraction. First, the training methods used for these models aren’t really designed for this specific task. For example, BERT and similar models don’t focus on the kind of entity-related knowledge that’s super important for finding entities and their relationships. Second, these models only give you ready-made representations for words and sentences, but not for entities or pairs of entities. To get those, you have to add extra parameters during the fine-tuning process, which can actually make things worse and affect how well the model performs on the joint extraction task.",
        "formal_text": "Although pre-trained models, like BERT, have shown impressive performance in many downstream tasks, they have mainly two limitations when applied in the joint entity relation extraction task. One is that currently pre-trained objectives are insufficient for this task. Specifically, these commonly used universal pre-trained model (e. g. , BERT) do not consider the entity-related knowledge that is crucial for better extracting entities and relations. The other is that these models only provide pre-trained representations for tokens and sentences, but not entities and entity pairs. To obtain the representations for entities and entity pairs, additional parameters that are not pre-trained are introduced in the fine-tuning stage, which may futher impair the joint extraction performance.",
        "GPT2_formal_text": ". Attention is a big part of how deep learning systems work, and it's been super important for the task we're working on. So, we made some tweaks to the code to better handle this, and you can check them out in the Appendix. Formal: In this paper, we talk about the structure of the teacher model, its embedding layer, and how we use an attention mechanism. We also explain how we use the generated knowledge to improve the student model. Formal: The model is built using PyTorch (Paszke et al., 2019) and its own setup. Formal: For the student model, the embedding layer is trained on a dataset with labeled examples. Formal: We use a learning rate of 0.001 to prevent overfitting. Formal: The knowledge generation module is created by combining the embedding layer, the output layer, and the attention layer. Formal: We tested the student model on the MNLI dataset (Bird et al., 2018), and the results showed that our model does a good job of training the student model while also being useful for tasks like dialogue generation. Formal: Lastly, we also looked at how to combine the teacher model with a student model. Formal: For future work, we plan to train more model components and add some non-linear operations to make our approach more scalable. Formal: Lastly, we mentioned that the model is trained on a dataset with labeled examples. Formal: Since the model is non-linear, we tried out different random seed seeds and different layers to see how they affect the performance. Formal: We tested the student model on the MNLI dataset (Bird et al., 2018), and the results showed that our model does a good job of training the student model while also being useful for tasks like dialogue generation. Formal: Formal: We also looked at how to combine the teacher model with a student model. Formal: Formal: Finally, we tested the student model on the MNLI dataset, and the results showed that our model does a good job of training the student model while also being useful for tasks like dialogue generation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eacl",
        "filename": "2021.eacl-main.251.json"
    },
    {
        "casual_text": "Pereira (from [Percira, 1985]) came up with a way to do structure-sharing by letting the result graph share info with the original graph by storing stuff in something called the 'environment'. But this method has a log(d) overhead, where d is the number of nodes in the graph. This overhead happens when you try to rebuild the whole graph from the 'skeleton' and the updates in the 'environment'. \n\nIn the new system, though, since the arcs point directly to the original graph structures, there's no extra overhead when accessing nodes. Plus, during unification, charges are called way more times than needed—like 3299 times for sentence 9 when unify-dg was only called 480 times.",
        "formal_text": "Pereira ([Percira, 1985] ) attaills structure-sharing I)y having the rcsult graph share information with the origiw, d gr~tphs by storing charges to tile 'enviroltmerit'. There will be tlle log(d) overhead (where d is the nuM)er of nodes ill a graph) associated with Percira's ntetho<l that is rcqttircd during node access to, ~ssenfl>le the whole graph from the 'skeleton' and the updates ill thc 'environment'. Ill the proposed sdtcmc, since tim arcs dircetly lloint to the original graph structures there will be no overhead till\" node accesses. Also, during unificatioli, siltce citaltges art! called 3299 times for sentence 9 when Itnify-dg was called 480 tillies.",
        "GPT2_formal_text": "= {u_i} | {u_j} = {u_i: j}. Formal: We're focusing on a specific word shape for this example. Formal: To keep the text from getting too long, we're using a pre-trained language model (LM) (Devlin et al., 2019) to generate the responses. Formal: In this case, the response doesn't use any special formatting. Formal: We're using a pre-trained LSTM (Hochreiter and Schmidhuber, 1997) to figure out the hidden representation for the word. Formal: In this case, the response is structured using something called a stack, which acts as a buffer for the response. Formal: Formal: Formal: Basically, the stack helps keep track of what's been said before. Formal: Formal: Formal: Formal: Formal: Formal: The LSTM layer then takes the hidden representation of the current word and gives it a score based on how much the token's representation has changed. Formal: Formal: Formal: Lastly, the final output layer (which is the output vector in the log-linear model) gives the final prediction. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C92",
        "filename": "C92-2068.json"
    },
    {
        "casual_text": "A recent study looked at historical English letters and compared different LSTM setups. It found that bi-directional recurrent neural networks (BRNN) work better than one-directional RNNs. But using different attention models or making the architecture deeper didn’t help improve the results. Adding extra data, like social metadata or century info, actually made the accuracy worse. The study suggests that post-processing is the best way to improve a character-level NMT normalization model. This same approach has also been used successfully for OCR post-correction (Hämäläinen and Hengchen, 2019).",
        "formal_text": "A recent study on historical English letters compares different LSTM architectures finding that bi-directional recurrent neural networks (BRNN) work better than one-directional RNNs, however different attention models or deeper architecture do not have a positive effect on the results. Also providing additional data such as social metadata or century information makes the accuracy worse. Their findings suggest that post-processing is the most effective way of improving a character level NMT normalization model. The same method has been successfully applied in OCR post-correction as well (Hämäläinen and Hengchen, 2019).",
        "GPT2_formal_text": "_2d_t = max(x_t, y_t) s_mask = [s_mask_1d_t; s_mask_2d_t] forward_mask = max(x_t, y_t) Figure 1: The model setup. The main thing we're focusing on is the window size. The vector representation for the hidden state h_t_i is built using the average of the output vectors x_i_t and y_i_t, as shown in Equation 7. Formal: Since the input is a tuple (x_t, y_t), the output value from the output layer of the BiLSTM has to be a tuple too. To make this happen, we use a BiLSTM with an attention mechanism to process the input. Formal: The BiLSTM's hidden state h_t_i is calculated by combining the output value x_i_t with the input vector y_i_t, as shown in Equation 8. Formal: In the top layer of the BiLSTM, the attention mechanism uses a window size of 1d. The output vector for the top layer gets passed through a fully connected layer before it's sent to the model. Formal: To keep the attention mechanism from focusing too much on specific parts of the input, we use a linear layer to adjust the attention vector. Formal: Formal: After the model finishes its post-to-post attention, the hidden state h_t_i is passed back through the BiLSTM to get the final output value. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-5519.json"
    },
    {
        "casual_text": "In both situations, the noun phrase (NP) either decides or has to match the overall structure. Similarly, when we're trying to find a verb phrase, we need to figure out which token (like a variable name or constant) stands for the subject (if the verb phrase is part of a sentence, or S) or the main noun (if the verb phrase is acting as a relative clause). This is done by using the SENDR function to set the subJvar register in the sub-computation to the right value.",
        "formal_text": "[n either case, the NP also determines, or must agree with, the overall Similarly, when we are lookzn8 for a verb phrase, we must know what token (i. e. variable name or constant) represents the subject (if the verb phrase is dominated by a S) or the head noun (if the verb phrase acts as a relative clause). This is done by sanding the subJvar register in the sub-computation the appropriate value via the SENDR function.",
        "GPT2_formal_text": "= {q in qi, t in ti} for i from 1 to n. Formal: We're introducing a new model called the Memory Network to handle this multi-modal info. Formal: We've tweaked the memory network in a cool new way that works with an encoder-decoder setup. We've shown that this updated model can better spot the keyphrases in question-answer pairs and handle multi-modal knowledge. Formal: We're also introducing a way to combine the answer representation and the question representation to get the best of both worlds. Formal: For future work, we're thinking about creating a QA model that can understand both text and images, and we could also explore using the same multi-modal knowledge for multiple question answering tasks. Formal: We're also thinking about using this multi-modal knowledge for other tasks. Formal: Lastly, we're thinking of testing out new QA models that can take into account these multi-modal features. Formal: We're using a simple Bi-LSTM-based model that's already been trained on a huge dataset to tackle multi-modal questions. Formal: We’ve come up with a new way to connect the answer text and the question text to create the most relevant answer for a given question, using both text and images. Formal: We also created a multi-modal question answering model that uses an image-to-text encoder-decoder setup. Formal: We’ve done some experiments to see how well the model works. Formal: We’ve come up with a new way to connect the answer text and the question text to create the most relevant answer for a given question, using both text and images. Formal: Lastly, we’ve come up with a new way to connect the answer text and the question text to create the most relevant answer for a given question, using both text and images. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "A83",
        "filename": "A83-1010.json"
    },
    {
        "casual_text": "(6) Another classic by Grisberg, I’m a huge fan of Stevie—she’s one of the best R&B singers I know. Darwin Halstead introduced me to her, so if you’re a fan too, you should definitely grab this DVD. It’s awesome and totally amazing. This woman really brings a heartfelt vibe to her R&B music.  \n\nContradiction: There’s one review (7) that seems a bit mixed up. The person says their husband isn’t a big fan of the product, but then also says he loves it. Kind of confusing, right?",
        "formal_text": "(6) another classic by grisberg, i love stevie she was one of the greatest r&b singers I know darwin halstead ment her so be a big fan please do yo self a favor and buy this dvd, its nice and it absolutly amazing this woman has a very yorfelt approch to r&b music Contradiction: We find one review (7) containing contradictions, where the subject (husband) is claimed to be not a big fan of a product but also as loving the same product.",
        "GPT2_formal_text": "ed embeddings for both directions. We use the \"i\" stands for the input embedding, and \"o\" is the output embedding. For the first two steps, we train the models using the cross-entropy loss function, which helps us predict the outputs. After that, we generate the target entity embeddings to apply the attention to the target entities. Formal: For the first step, we randomly set the attention vector to zero. Formal: We train a GCBiA model with the cross-entropy loss function and set the output embedding to the output from the BiLSTM. Formal: After the second step, we train the model with the cross-entropy loss function and set the output embedding to the output from the LSTM. Formal: Finally, we generate the target entity embeddings using a linear projection of the sentence embedding vector. Formal: We train a GCBiA model with the cross-entropy loss function and set the output embedding to the output from the BiLSTM. We also use the \"i\" stands for the input embedding, and \"o\" is the output embedding. Formal: We use the linear projection of the sentence embedding vector to generate the target entity embeddings. Formal: For the third step, we train the model with the cross-entropy loss function and set the output embedding to the output from the LSTM. Formal: After the fourth step, we train the model with the cross-entropy loss function and set the output embedding to the output from the LSTM. Formal: Finally, we generate the target entity embeddings using a linear projection of the sentence embedding vector. Formal: After the last two steps, we train the model with the cross-entropy loss function and set the output embedding to the output from the LSTM. Formal: We train the model with the cross-entropy loss function and set the output embedding to the output from the LSTM. Formal: Finally, we generate the target entity embeddings using a linear projection of the sentence embedding vector. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "coling",
        "filename": "2020.coling-main.208.json"
    },
    {
        "casual_text": "The numbers are: 5.96 ± 0.21 (0.00 ± 0.00), 52.68 ± 0.43 (0.00 ± 0.00), 68.95 ± 0.47 (0.00 ± 0.00) for LAV N, and 30.11 ± 1.27 (2.00 ± 1.37), 85.41 ± 1.07 (48.50 ± 4.00), 94.83 ± 0.53 (83.40 ± 4.37). The blue bars on the left show how well our system did, and the green bars on the right show the baseline system's performance. The graphs break down the accuracy for tables with 1, 2, 3, 4, and more than 4 forms.",
        "formal_text": "5. 96 ± 0. 21 (0. 00 ± 0. 00) 52. 68 ± 0. 43 ( 0. 00 ± 0. 00) 68. 95 ± 0. 47 ( 0. 00 ± 0. 00) LAV N 30. 11 ± 1. 27 (2. 00 ± 1. 37) 85. 41 ± 1. 07 (48. 50 ± 4. 00) 94. 83 ± 0. 53 (83. 40 ± 4. 37) The blue bars (on the left) denote accuracy for our system and green bars (on the right) accuracy for the baseline system. The graphs show accuracy separately for tables where 1, 2, 3, 4, and > 4 forms are given.",
        "GPT2_formal_text": "= [u, v]        Casual: The model was trained using the Yule+SMI toolkit (Koehn, 2004), with a learning rate of 0.0001 for the best part of two epochs. It was fine-tuned using a momentum of 0.9 and a warm-up period of 5 epochs. As for the rest of the hyperparameters, we looked at them in the results section, but we didn’t bother with the batch size. Formal: The test set is the same as the training set, but we’ve skipped it to keep things simple. Formal: The test set has the same number of sentences as the training set. Formal: The test set has sentences from the same source, but they’re slightly longer. Formal: The test set has sentences from different sources. Formal: The test set has sentences from the same source, but they’re shorter. Formal: The test set has sentences from different sources. Formal: The test set has sentences from the same source, but they’re longer. Formal: The test set has sentences from different sources. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1315.json"
    },
    {
        "casual_text": "So, like, the stuff we talked about earlier? It can be explained using the example in Figure 1. Let's say we have a sentence in Chinese that says \"在",
        "formal_text": "As stated above, our motivations may be explained by the example shown in Figure 1. Given a source sentence in Chinese \"在",
        "GPT2_formal_text": "ed to a random sample from the bag of words. After that, we figure out the model's parameters using the Adam optimizer, which is a popular algorithm for learning complex models (Kingma and Ba, 2015). Formal: We keep going with the process outlined in section 3.1. The word embeddings (s_w and s_e) stay the same. The model is trained to handle each word's context and outputs. Formal: The model gets a score (s_o) for each word and turns into a probability distribution (p_w) for the whole sequence. Formal: The model also learns to estimate the probabilities for the entire sequence, like the whole bag of words. In this part, we call the learning process \"learning the bag of words.\" For more details on how we calculate these probabilities, you can check out section 4. Formal: After the learning process, we calculate the word representations using the word2vec tool (Mikolov et al., 2013). Formal: After the word representations are calculated, we feed them into the model using the word2vec library (Mikolov et al., 2013). Formal: Finally, we use the softmax function on the probabilities to get the final prediction. Formal: For our experiments, we used the Adam optimizer (Kingma and Ba, 2015) to optimize the model. Formal: We tested our method on two datasets: MNLI (Zhang and Lapata, 2015) and CoNLL-2014 (Liu et al., 2014). Formal: We ran the experiments on a GeForce GTX 1080 Ti GPU, which is a pretty standard setup for deep learning methods. Formal: The results were pretty good, and our model performs better than the current top-of-the-line methods. Formal: To make sure our results weren't just due to random luck, we used the development set to retrain our model. Formal: The code for our experiments is available on GitHub at https://github.com/singhkep/bagofwords. Formal: We took the BERT model from the Hugging Face Transformers library (Wolf et al., 2019) and tweaked it a bit by adding a residual connection, which helped it handle the context better (Raffel et al., 2020). Formal: We also found out that using this residual connection during training gives us a tiny boost in performance. Formal: We tested our method on two different datasets",
        "directory": "amta",
        "filename": "2008.amta-papers.10.json"
    },
    {
        "casual_text": "Sorry, we can’t go into more examples because we’re running out of room.",
        "formal_text": "Due to space limitations, we will not present additional examples in detail.",
        "GPT2_formal_text": "\": { ... }          Casual: The vocabulary is basically a collection of nouns that have been given a different meaning. It's made up of combinations of these nouns, each with a capital letter in the original language (like \"W\" or \"V\"), plus a special placeholder for any missing nouns or pronouns. Formal: This part is like a dictionary where each word in the sentence gets its own number and a special placeholder. Formal: We’ve got some examples of what this vocabulary looks like in the appendix. Formal: This part is like a dictionary where each word in the sentence gets its own number and a special placeholder. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of what this vocabulary looks like in the appendix. Formal: We’ve got examples of",
        "directory": "coling",
        "filename": "2020.coling-main.241.json"
    },
    {
        "casual_text": "Alright, so we've got a target and a relative, and we need to figure out how their parts-of-speech (POS) N-grams and positions are distributed. Using a labeled corpus would be way too limited for this, so instead, we turn to the much bigger, unlabeled Google Ngrams corpus (shoutout to Franz and Brants, 2006).",
        "formal_text": "Given a target and relative, we need to estimate their distribution of POS N-grams and positions. A labeled corpus is too sparse for this purpose, so we use the much larger unlabeled Google Ngrams corpus (Franz and Brants, 2006).",
        "GPT2_formal_text": "ed_cov(X, y_t) = (2 * h_t + 1) * attention_masked_vec(X) + (1 - h_t) * attention_masked_vec(Y) where h_t is the output dimension from the attention mechanism, and h_t = h_t + 1. Formal: For each neuron i and its label l_i, we figure out the hidden state representations, h_i, and the attention-based attention vector, a_i, for all the tokens in the text. We do this using a logistic regression classifier (LM) and a linear layer on a sequence of word embeddings (from Lample et al., 2016) to do it. Formal: Since the text is an LSTM, we only need the hidden state and attention vector. To get the hidden state and attention vector, we just sample the hidden state with a softmax function from the attention mechanism and the text embedding. Formal: We've got two vectors, a_i and a_i, for each word w_i in the text. The words in the text are connected through neural networks to form a vector. Formal: Formal: Here's how the neural network setup works: Formal: We use a graph neural network (NGN) to create the attention-based vectors. We train this network using a feed-forward neural network (FFNN) to figure out how important each word in the text is. Formal: We train the network using two convolutional neural network (CNN) models that have been fine-tuned on the Yelp review dataset (Liu et al., 2015). Formal: Finally, we calculate the relation embeddings, h_i and a_i, by combining the attention vector a_i and the hidden state h_i. Formal: To learn the attention vector a_i, we just use the linear layer on the sequence of word embeddings. Formal: Formal: We train the CNN model using the Amazon Review dataset, which includes all the reviews from the same review. Formal: Formal: Formal: We take the average of the hidden states and attention vectors for each word in the text. Formal: We train the network using two CNN models that have been fine-tuned on the Yelp review dataset. Formal: Formal: Formal: We train the network using two CNN models that",
        "directory": "E12",
        "filename": "E12-1038.json"
    },
    {
        "casual_text": "Swimming is super beneficial, right? It’s got loads of benefits. You know, infinitival verbs and verbal nouns look the same, so sometimes it’s tricky to tell them apart. But when a verbal noun is in the oblique form and followed by a postposition, like in example 6, it’s easier to figure out. The real challenge is when it’s in the direct form. In example 7, the verb \"jānā\" is part of a verb group (VG), so it’s tagged as an infinitival verb. But in example 8, it’s used as a verbal noun and even has a possessive pronoun, \"merā,\" modifying it. This makes it confusing—is it a noun or a verb? Well, if an infinitival verb is right before a possessive pronoun or a genitive postposition, like in 8, it should be tagged as a verbal noun. And this info can help the POS tagger do its job better.",
        "formal_text": "tair-nā bəhut lābhkārī hai swim-Inf very beneficial be-pres 'Swimming is very beneficial' 6) tair-n-e ke bəhut lābh haῖ swim-Inf-obl Poss many benefits be-pres, pl 'Swimming has many benefits' Infinitival verbs as either main verbs or verbal nouns have identical forms. The POS ambiguity is easy to resolve when the verbal noun is in the oblique and is followed by a postposition as shown in 6 above. More difficult are sentences where it appears in the direct form. In 7 the verb jānā appears inside a VG and should be tagged as an infinitival verb. While in 8, it appears as a verbal noun and is also modified by a possessive pronoun merā. These two occurrences of jānā as a noun and an infinitival verb yields POS ambiguity (N or V). However, when an infinitival verb is immediately preceded by a possessive pronoun or a genitive postposition, as in 8, it should be tagged as a verbal noun, and the NG information can be successfully exploited by the POS tagger.",
        "GPT2_formal_text": "= {% i j %} + (lambda x: S j ) + (lambda y: S j + S j ). Formal: All the info from all the tokens in the pair gets added up. Formal: The matrix m_i shows how important each token is for relation extraction. Formal: The type α_t is a matrix that can be any number from 1 to L. Formal: The type α_i is a matrix that represents the emotion distribution, which is basically the average of the emotion vectors in the pair. Formal: Lastly, the attention weights β_t for the pair can be any number from 1 to L. Formal: The model tries to figure out the emotion distribution for the pair based on the emotion vectors in the pair. Formal: It also learns how important each emotion vector is for relation extraction. Formal: Finally, the model learns how important each emotion vector is for relation extraction. Formal: Formal: The model's job is to figure out the emotion distribution for the pair based on the emotion vectors in the pair. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C12",
        "filename": "C12-1152.json"
    },
    {
        "casual_text": "We’ve got two main systems in our experiments: the regular HPB SMT baseline and the CCG-augmented HPB SMT baseline. The CCG-augmented one uses single-category CCG labels and adds some strict syntactic rules (shoutout to Almaghout et al., 2010). For the regular HPB SMT baseline, we used the Moses Chart Decoder. We also used the GIZA++ toolkit to handle word and phrase alignment, and we went with the \"grow-diag-final\" method for refinement (thanks, Koehn et al., 2003). We set both the maximum phrase length and rule span to 12 words. For decoding, the chart maxes out at 20 words, and anything beyond that only uses glue grammar rules. The hierarchical rules we extracted can have up to 2 nonterminals. To fine-tune everything, we did minimum error rate training (props to Och, 2003). For the language model, we used a 5-gram trained on the target side of the parallel corpus with the SRILM toolkit and modified Kneser-Ney smoothing.\n\nThe CCG-augmented HPB system also runs on the Moses Chart Decoder, which has a feature to pull syntax-augmented rules from annotated data. We kept the same rule extraction and decoding settings as the regular HPB baseline. For the CCG-augmented experiments, we used the CCG parser from the C&C tools to handle the training data and combine CCG categories during glue grammar rule application.",
        "formal_text": "We have two baseline systems in our experiments: the HPB SMT baseline system and the CCGaugmented HPB SMT baseline system which uses single-category CCG labels and applies strong syntactic constraints (Almaghout et al. , 2010). We built our HPB SMT baseline system using the Moses Chart Decoder. 3 The GIZA++ toolkit 4 is used to perform word and phrase alignment and the \"grow-diag-final\" refinement method is adopted (Koehn et al. , 2003). Maximum phrase length and maximum rule span are both set to 12 words. The maximum span for the chart during decoding is set to 20 words, above which only glue grammar rules are applied. Hierarchical rules extracted contain up to 2 nonterminals. Minimum error rate training (Och, 2003) is performed to tune all our SMT systems. The 5-gram language model in all experiments was trained on the target side of the parallel corpus using the SRILM toolkit 5 with modified Kneser-Ney smoothing. Our CCGaugmented HPB system was also built using the Moses Chart Decoder, which has an option to extract syntax-augmented rules from an annotated corpus. We used the same rule extraction and decoding settings as for the HPB baseline system. We use the CCG parser from C&C tools 6 to parse the training data for our CCG-augmented HPB system experiments and to combine CCG categories during glue grammar rule application.",
        "GPT2_formal_text": "ing, and gazetteer. The results were pretty good, with an F1-score of 0.89 on the development set and 0.93 on the test set. We ran a bunch of tests to figure out how different methods did in terms of identifying named entities. For each entity, we got three lists of possible types (let's call them a_1, ..., a_N). Then, we ranked the top N names, based on their confidence scores, and picked the top 50 for testing. Table 5 has the results for the Top-1 ranking, and Table 6 has the results for the Top-50 ranking. The methods that did the best in this ranking are highlighted in bold. Formal: The results for the top-1 ranking are in bold, and the results for the top-50 ranking are underlined. Formal: We used a ranked list of entity types (a_1 to a_N) to rank the top N names. Formal: We used a ranked list of entity types (a_1 to a_N) to rank the top N names. Formal: The top-50 ranking is underlined, and the top-1 ranking is in bold. Formal: Formal: The top-1 ranking and the top-50 ranking are underlined. Formal: Formal: The top-1 ranking and the top-50 ranking are underlined. Formal: The top-1 ranking and the top-50 ranking are underlined. Formal: Formal: We only included results from the top-50 ranking if they had a confidence score above 0.5. Formal: The top-1 ranking and the top-50 ranking are underlined. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2012.eamt-1.44.json"
    },
    {
        "casual_text": "The supervised model we got wasn't as great as the fully-lexicalized DMV, but it was still over five points more accurate than using gold part-of-speech tags (check out Table 1: flat). Unsupervised accuracy was lower compared to gold tags, and the main perk of manually created categories might be that they can tag a word differently depending on the context.",
        "formal_text": "The resulting supervised model, though not as good as the fully-lexicalized DMV, was more than five points more accurate than with gold part-ofspeech tags (see Table 1: flat). Unsupervised accuracy was lower than with gold tags (see also tag a word differently in context) may be the primary advantage of manually constructed categorizations.",
        "GPT2_formal_text": "_list = {[CLS], [SEP], and [SEP+1]}. Formal: The first part is the score of the input sentence. It's calculated using the f1-score from the last layer. Formal: The second part is the log likelihood, which is basically the chance of the output being correct or not. Formal: Finally, we have the logit score, which is just the log of the probability of the output sequence. Formal: The probability P(y|x) for the output sequence y given the input sequence x is calculated using this equation: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D11",
        "filename": "D11-1118.json"
    },
    {
        "casual_text": "The Computational Linguistics model seems to be doing pretty well. For instance, it can correctly generate \"a big rabbit\" when it focuses on the object \"rabbit\". Similarly, it can accurately recognize actions like \"crouching\" and the surroundings like \"grass and flowers\" when it pays attention to the different parts of the sketchy scene. However, it doesn’t mention anything about the weather, like \"on a cloudy day\", which might be because the \"cloud\" is too small and not very noticeable. \n\nIt’s also interesting that the model doesn’t describe the \"trees\" even though they take up a big part of the scene. As mentioned earlier, the model tends to focus on the most noticeable parts of a sketchy scene. In this case, the \"rabbit\" was considered the main object, and since \"rabbit\" often appears with \"grass\" and \"flowers\" in the dataset, the \"trees\" didn’t get much attention.\n\nThere are a few more examples in Figure 5 that show how the model works. Overall, it does a good job identifying the main objects in the scenes, except for the \"woman\" and \"house\" in the first example. The reason for this might be that the \"woman\" is hidden behind a \"tree\" in the sketch, making it hard for the model to recognize her properly.",
        "formal_text": "Computational Linguistics scene quite well. Specifically, \"a big rabbit\" can be generated correctly when the model focuses on the object \"rabbit\". Meanwhile, the action \"crouching\" and the surroundings \"grass and flowers\" can be correctly recognized as well when the model focuses on the two sides of the sketchy scene. In addition, the model does not generate the description about the weather, such as \"on a cloudy day\", which may be due to the reason that the \"cloud\" is too small to be salient enough. It is worth noting that the model does not generate the description about the \"trees\" which occupy a large area of the sketchy scene. As mentioned before, a sentence-level caption only describes the most salient parts of a sketchy scene. In the example, the \"rabbit\" has been treated as the salient object and the word \"rabbit\" may co-occur with \"grass\" and \"flowers\" more frequently in the dataset, and thus the \"trees\" are not treated as the salient objects by the captioning model. Another three representative examples of the sentence-level captioning are given in Figure 5. Generally, the salient objects in the selected sketchy scenes can be well identified except the objects \"woman\" and \"house\" in the first one, and this bad result may be caused by the imprecise visual representation of the sketchy scene. That is, the \"woman\" is occluded by the \"tree\" in front of her, making the captioning model fail to recognize the \"woman\" correctly.",
        "GPT2_formal_text": "ed_LSTM: The model's input gets processed in two parts: first, it takes the input text x t and predicts the next word y t using a hidden layer with ReLU as the activation function. Then, it uses a feed-forward layer to get the representation for the next word, y t+1, which is h t. This h t is used to create the context vector for the masked word, x t+1, which we call c t. Formal: To make a masked word, we stick together the first character of the masked word x t and the hidden representation h t to get a vector representation c t. Formal: So, if you have a word y t in the vocabulary V W, it gets sent to the classifier as h t, which then predicts the masked word x t. Formal: To make a context vector for the masked word x t, you just add up the vectors of the words around it. Formal: Formal: So, the prediction for the masked word x t depends on the hidden representation h t for the masked word x t+1. Formal: Formal: In the example shown in Figure 1, the output vector for the masked word x t is a 3D feature matrix that's one size, with three parts. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "ccl",
        "filename": "2021.ccl-1.104.json"
    },
    {
        "casual_text": "In Section 3, we told the annotators to pick the word that most reminded them of an experiment and label it as EX-PERIMENT. They also had to connect any related details to that word. Plus, the EXPERIMENT labels could be linked together if they were about the same experiment or a variation of it. Table 10 gives some numbers on how many EXPERIMENT labels were in each sentence and how often the main annotator actually connected them. \n\nIn the training data, there were 703 sentences about experiments, and 135 of those had more than one word that felt like an experiment. Specifically, 114 sentences had two experiment-related words, 18 had three, and 3 had four (check Table 10 for the details). Out of those 114 sentences with two experiment labels, only 2 didn’t link the labels together. When we showed these cases to our main annotator, they thought one of them should have been linked after all.",
        "formal_text": "As stated in Section 3, we instructed annotators to mark the closest experiment-evoking word as EX-PERIMENT and link the respective slot arguments to this mention. In addition, the EXPERIMENT annotations could then be linked either by same exp or exp variation links. Table 10 shows some statistics on the number of EXPERIMENT annotations per sentence and how often the primary annotator actually made use of the possibility to link experiments. In the training data, out of 703 sentences describing experiments, 135 contain more than one experiment-evoking word, with 114 sentences containing two, 18 sentences containing three, and 3 sentences containing four EXPERIMENT annotations (see Table 10 ). In the 114 sentences containing two experiment annotations, only in 2 sentences, the EXPERIMENTs were not linked to any others. Upon being shown these cases, our primary annotator judged that one of them should actually have been linked.",
        "GPT2_formal_text": "= [e_i, ..., e_j] from the dictionary. We also need a binary classifier called C_i, which we get from the dictionary. Formal: A t = {e_i, ..., e_j} that gives us the combination of document-level features, which we call f_i, for each t. Formal: For each pair (e_i, e_j) that's part of the document-level feature set, we calculate the log likelihood p_t for the given pair. Formal: We take the normalized document-level log probability p_t and run it through a logistic classifier. Formal: The document-level log probability p_t is calculated based on the logistic classifier's predictions and the document-level log probability p_t. Formal: The normalized logistic probability p_t is calculated using the document-level log probability p_t and the normalized logistic probability p_t. Formal: Finally, we toss in a binary classification classifier C_i to predict the document-level log probability p_t for each pair. Formal: Formal: Formal: We pick the document-level log probability p_t for a pair (e_i, e_j) by running the logistic classifier C_i through its predictions and the document-level log probability p_t. Formal: The document-level log probability p_t is calculated by the logistic classifier's predictions and the document-level log probability p_t. Formal: Formal: Finally, we toss in a binary classifier C_i to predict the document-level log probability p_t for each pair. Formal: Formal: Formal: Finally, we pick the document-level log probability p_t for a pair (e_i, e_j) by running the logistic classifier C_i through its predictions and the document-level log probability p_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.116.json"
    },
    {
        "casual_text": "Hey, so for all the questions I just answered, I think I’m pretty good at getting, understanding, and using basic health info to make smart decisions. I mean, I trust doctors a lot (TD), but I also like to stay informed and make choices that make sense for me. So yeah, I feel confident in my ability to handle that stuff.",
        "formal_text": "Regarding all the questions you just answered, to what degree do you feel you have capacity to obtain, process, and understand basic health information and services needed to make appropriate health decisions? Please explain you answer in a few sentences. Trust in physicians (TD)",
        "GPT2_formal_text": "_tf2 = max(0, A_t * T) + sum of (A_t - A_h) from t=1 to m.\n         Casual: We tested the model against a bunch of other methods that use word embeddings and word length (Liu and Lapata, 2018; Li et al., 2019b; Xu et al., 2020). To see how well it works, we ran the models 10 times with different random seeds. Table 2 shows the results of the best-performing model. The model that uses character embeddings scored the best, with a F1 score of 0.96. Formal: We also tried using other resources like sentence embeddings, language model embeddings, and character-to-word embeddings to make the model better at understanding the context. Formal: We found that combining these resources really helps, improving the model's performance by up to 0.86. Formal: We also tried using general-purpose word embeddings, POS tags, and character-to-word embeddings for sentence classification, and the results are in Table 3. Formal: We tried using different neural networks to create context representations. Formal: We built a model to predict the type of word (e.g., noun, verb, etc.) and calculated the average similarity between each character embedding and the context embedding. Formal: The biggest improvement came when we used a 3-layer neural network with 300 hidden units, which gave us an average similarity score of 0.711. Formal: For token prediction, we set the output dimension to 4. Formal: We also checked how much a word embedding affects its accuracy. Formal: We trained the model with a learning rate of 0.001 and a batch size of 32. Formal: Our model did better than the other methods in terms of F1 scores, improving by 0.27. Formal: We also used a multiloss objective to avoid overfitting and improved the performance by 0.48. Formal: We used the BERT model (Devlin et al., 2019) with a layer dimension of 32 and a learning rate of 0.01. Formal: We also tested our model with character embeddings, POS tags, and character-to-word embeddings. Formal: Formal: Lastly, we compared our model to the model that uses character embeddings. Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.304.json"
    },
    {
        "casual_text": "Lately, a lot of normalization techniques have been using neural machine translation (NMT) on a character level, kind of like how older SMT-based methods did it. Bollmann and Søgaard (2016) used a bidirectional long short-term memory (bi-LSTM) deep neural network to normalize historical German text at the character level. They also checked how well the model worked when extra data was added during training (like multi-task learning). Their results showed that neural network-based normalizations were better than those done by conditional random fields (CRF) and Norma, especially when the models were trained with extra data.\n\nTursun and Cakici (2017) tried out LSTM and the noisy channel model (NCM), which is often used for spell-checking, to normalize Uyghur text. They used a base dataset of around 200 sentences from social media, both automatically and manually normalized. They also created synthetic data by scraping news websites and adding noise to it by randomly swapping characters with corrupted ones. Both methods did a great job normalizing the text, showing their effectiveness.\n\nSimilarly, Mandal and Nanmaran (2018) used an LSTM network to successfully normalize code-mixed data with an accuracy of 90.27%.",
        "formal_text": "Recently, many normalization methods utilized neural machine translation (NMT) analogously to the previous SMT based approaches on a character level due to its considerable ability in addressing the task. Bollmann and Søgaard (2016) have used a bidirectional long short-term memory (bi-LSTM) deep neural network to normalize historical German on a character level. The authors have also tested the efficiency of the model when additional auxiliary data is used during the training phase (i. e. multi-task learning). Based on their benchmarks, normalizations using the neural network approach outperformed the ones by conditional random fields (CRF) and Norma, where models trained with the auxiliary data generally had the best accuracy. Tursun and Cakici (2017) test out LSTM and noisy channel model (NCM), a method commonly used for spell-checking text, to normalize Uyghur text. In addition to the base dataset (≈ 200 sentences obtained from social networks, automatically and manually normalized), the authors have generated synthetic data by crawling news websites and introducing noise in it by substituting characters with their corresponding corrupted characters at random. Both of the methods have normalized the text with high accuracy which illustrates the their effectiveness. Similarly, Mandal and Nanmaran (2018) had employed an LSTM network and successfully normalized code-mixed data with an accuracy of 90. 27%.",
        "GPT2_formal_text": "(0.1). The formula (1.1) was originally suggested by Shen et al. in 2018. Formal: Theorem G.6 (EQ. 7) and Equation (1.1) are the same as those in (EQ. 6), but they use different scales. Formal: Check out Table 1 for the results of our experiments. The results are shown for the LSTM-based model trained on WikiData, which is the full dataset for the GREEDY benchmark. Formal: To better understand how the model is performing, we did an ablation study. We compared the results from the regular model, which is the one we trained on WikiData, to those from the model that was trained on the full dataset, which is the GREEDY benchmark. Formal: Next, we'll check out how the model performs when we train it on the full dataset. Formal: Lastly, we'll check how well the model holds up when it's trained on both WikiData and the full dataset. Formal: You can find more details about the training, testing, and results in Appendix D. Formal: Lastly, we'll update the proof from (EQ. 6) to (EQ. 7) using equation (1.1) and give a more formal explanation in Appendix E. Formal: So, in short, our main goal is to create a model that can learn representations for different kinds of text, no matter the dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-5519.json"
    },
    {
        "casual_text": "Alright, big thanks to our sponsors, the Japanese Society for the Promotion of Sciences and the Laboratoire Parole et Langage. Their awesome support helped us give fee waivers to PhD students who were first authors of accepted papers and cover the costs for our invited speakers.\n\nHere’s a quick rundown of some of the teams and their work:\n\n- **LangResearchLab_NC at CMCL2021 Shared Task**: Predicting Gaze Behaviour Using Linguistic Features and Tree Regressors by Raksha Agarwal and Niladri Chatterjee.\n- **TorontoCL at CMCL 2021 Shared Task**: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction by Bai Li and Frank Rudzicz.\n- **LAST at CMCL 2021 Shared Task**: Predicting Gaze Data During Reading with a Gradient Boosting Decision Tree Approach by Yves Bestgen.\n\nAnd that’s a wrap for the teams!",
        "formal_text": "Finally, thanks again to our sponsors: the Japanese Society for the Promotion of Sciences and the Laboratoire Parole et Langage. Through their generous support, we have been able to offer fee waivers to PhD students who were first authors of accepted papers, and to offset the participation costs of the invited speakers. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 72 LangResearchLab_NC at CMCL2021 Shared Task: Predicting Gaze Behaviour Using Linguistic Features and Tree Regressors Raksha Agarwal and Niladri Chatterjee. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 79 TorontoCL at CMCL 2021 Shared Task: RoBERTa with Multi-Stage Fine-Tuning for Eye-Tracking Prediction Bai Li and Frank Rudzicz. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 85 LAST at CMCL 2021 Shared Task: Predicting Gaze Data During Reading with a Gradient Boosting Decision Tree Approach Yves Bestgen. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 90 Team",
        "GPT2_formal_text": "_1 = (i, j) at the end of the sentence, which is basically the hidden state from the LSTM. Formal: To get the final output, we use the embeddings from the transformer layer of the hidden state from the last sentence (t_s) as the final score. Formal: The first sentence is considered the context and the second sentence is the response. Formal: The hidden state from the last sentence is used to figure out the probability p_t_i, j for the current sentence (t_s). Formal: We set the mean of the output to 1. Formal: For the responses, we can use either a maximum entropy approach (from See et al., 2015) or a logistic regression approach (by van der Maaten et al., 2016). Formal: In this paper, we use the logistic regression approach. Formal: The response from the first sentence is just the token embeddings for the first word in the response. Formal: The final output from the LSTM is the embeddings from the first sentence. Formal: Finally, the output from the transformer layer is the token embeddings. Formal: In the next section, we’ll talk about the model setup for generating the responses in Section 4.2. Formal: We tested the model on five different datasets, which include four IIT-Bombay datasets, the UFAL dataset, and the UFAL-SQuAD dataset. Formal: Check out Table 1 for the evaluation results. Formal: For the IIT-Bombay dataset, the model performed pretty well in terms of precision, recall, and F1 score. In the UFAL dataset, it’s pretty close to the best model from Kipf and Welling (2016). But for the UFAL-SQuAD dataset, the best model, which is based on a transformer model, doesn’t really match up well with the rest of the results. Formal: For the IIT-Bombay dataset, the best model from Kipf and Welling (2016) does better than the other models. Formal: For UFAL-SQuAD, the best model from Kipf and Welling (2016) is still a bit behind the other models. Formal: To see how the transformer model performs, we compared the results from the transformer model to the gold standard. Formal:",
        "directory": "cmcl",
        "filename": "2021.cmcl-1.0.json"
    },
    {
        "casual_text": "We came up with two types of features for splitting sentences: the phrase-embedding feature and the segmentation-specific feature. The final feature for each segmentation is just a mix of these two. Oh, and the phrase-embedding feature is also used for sentiment classification, not just sentence segmentation.",
        "formal_text": "We design two kinds of features for sentence segmentation, namely the phrase-embedding feature and the segmentation-specific feature. The final feature representation of each segmentation is the concatenation of these two features. It is worth noting that, the phrase-embedding feature is used in both sentence segmentation and sentiment classification.",
        "GPT2_formal_text": "= [query_id, document_id]          Casual: Here, e_i represents the i-th element in the attention matrix. Formal: Also, the number of tokens in the document can be figured out using equation (8). Formal: The similarity function S(X, Y) for two documents X and Y is calculated using equation (7). Formal: The vector representation of the query sentence y_q in Y is just a combination of its query and the responses in X. Formal: For the query's representation in X and the responses' vector, we use the softmax function to combine them. Formal: The similarity function S(X, Y) for the query sentence y_q in Y is calculated using equation (7). Formal: Formal: The sentence representations h_s in Y are calculated using equation (6). Formal: To get the attention matrix, we first calculate the attention weights for each query and response using equation (5). Formal: For this, we use the cross-entropy loss function. Formal: Formal: We then apply this loss to the attention matrix to get the final attention vector H. Formal: To avoid overfitting, we can set a batch size of 8 and train for 2 epochs. Formal: In the final step, we calculate the normalized attention score for the query and response. Formal: Formal: Finally, we use the softmax function to get the attention vector H_s. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D14",
        "filename": "D14-1054.json"
    },
    {
        "casual_text": "In this paper, we treat both the translation and generation methods as ways to boost the amount of data. Our main goal is to deal with the issue of noise in these augmented datasets. We come up with a structured way to learn from multiple noisy augmented datasets for cross-lingual SLU, especially when there's no perfectly labeled data in the target language. Our key technical contribution is a set of methods to clean up the noise, like relabeling instances, co-training, and re-weighting instances.",
        "formal_text": "In this paper, we regard both the translation approach and the generation approach as data augmentation methods. We tackle the problem of reducing the impact of noise in augmented data sets. We develop a principled method to learn from multiple noisy augmented data sets for cross-lingual SLU, where no golden-labeled target language data exists. Our major technical contribution consists of a series of denoising methods including instance relabeling, co-training and instance re-weighting.",
        "GPT2_formal_text": "ing() function. It uses the same approach as in BART, so we can use the same pre-trained language model (LM) for both tasks. Formal: So, we're looking for a translation model that helps us understand the relationship between the source and target languages, specifically by understanding the source language's word order, which we call the source-side syntax. Formal: We're also dealing with a bunch of other stuff, like the target language's grammar, words, and syntax, as well as the source-side word order. We're trying to use these models to compare and combine different types of features, like word embeddings, phrase embeddings, and POS tag embeddings. Formal: The models we're using are transformers, which are trained using cross-lingual embeddings and their respective representations. Formal: We're using a task-specific translation model called bert-trans. Formal: To make things clearer, we’ve included some examples of the relation spaces we’re looking at in Figure 2. Formal: The cross-lingual embeddings we’re using are basically part of a bigger word embedding, which is labeled as w. Formal: We’re using a multi-layer perceptron (MLP) to predict the target-side word order, and then using a reverse LSTM (specifically, a bidirectional LSTM) to pull out context-related features. Formal: Lastly, we’re thinking about using a self-attention mechanism to improve the overall representation of the relation spaces. Formal: This whole setup is built using the HuggingFace Transformers library, which is open-source and freely available. Formal: We’ve included some examples of the relation spaces we’re using in Figure 2. Formal: We’ve included some examples of the relation spaces we’re using in Figure 2. Formal: We’ve included some examples of the relation spaces we’re using in Figure 2. Formal: We’ve included some examples of the relation spaces we’re using in Figure 2. Formal: To make things clearer, we’ve included some examples of the relation spaces we’re using in Figure 2. Formal: Formal: We’ve included some examples of the relation spaces we’re using in Figure 2. Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.259.json"
    },
    {
        "casual_text": "We figure that for a perfect system (or what we call the human gold standard), the correlation index should be 1, meaning it gets 100% accuracy on both the monothematic dataset and the original RTE dataset. So, we think of CI = 1 as the ideal situation. To see how close a system S is to being perfect, we just subtract its actual correlation from this ideal CI.",
        "formal_text": "We expect the correlation index of an optimal ideal system (or the human goldstandard) to be equal to 1, i. e. 100% accuracy on the monothematic dataset should correspond to 100% accuracy on the original RTE dataset. For this reason, we consider CI = 1 as the ideal correlation, and we calculate the difference between such ideal CI and the correlation obtained for a system S.",
        "GPT2_formal_text": "= (t) i=1 for each i from 1 to N. Formal: For the multi-view learning method, the model takes as input a bunch of triplets, and it spits out a single prediction for each triplet. Formal: When it comes to predicting a single label, the model focuses on the parts of the triplet where the labels are all the same. Formal: So, the final score for the triplet is just the sum of all the scores from the four views. Formal: The model can handle multiple labels at once by considering the relationships between them and also between the triplet and each view. Formal: Basically, the goal is to maximize the loglikelihood of the predicted triplet, which is the same as the loglikelihood of the prediction. Formal: Formal: Since the actual labels can be all over the place, the model also tries to minimize the cross-entropy loss, which is what the model learns from the unlabeled data. Formal: In this paper, we don’t include any loss function λ for the model’s prediction. Formal: The multi-view learning method does a good job of creating a clear and distinct prediction for each triplet. Formal: We’ve come up with a tweak to the multi-view learning method called multi-view ensemble (MVLA). This version tweaks the multi-view learning method by using different views for the same triplet. Formal: We’ve also introduced a new loss function that focuses on the probabilities of different label sequences. Formal: We’ve added two more losses: one that focuses on the probabilities of all labels, and another that takes into account the probabilities of individual label sequences. Formal: For more details, you can check out the original papers. Formal: We’ve also included some visualizations of the model’s output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C10",
        "filename": "C10-2012.json"
    },
    {
        "casual_text": "Sure, when we use a model trained on this kind of transformed data for new inputs, it’ll produce transformed outputs. So, the transformation needs to be reversible, meaning we can convert the model’s output back to the original, un-transformed value.",
        "formal_text": "Of course, when we apply a model trained on this type of transformed output to new inputs, it will generate transformed output values. Thus, the transformation must be reversible, so that we can map the output of the model back to an un-transformed output value.",
        "GPT2_formal_text": "ed_cad. The attention layer is calculated based on the attention weights. When using multiple layers for the same instance, the score for the combined representation is the average of the scores for the individual layers. Formal: LSTM networks handle sequence generation by generating tokens one at a time. Each time, the output vector is a sum of a bunch of input vectors, which include the sequence itself, the previous token, and the attention mask (which is the number of tokens in the next token). We use the truncated CKY algorithm (from Cao et al., 2015) to make sure the sequence is unique. Formal: To make sure the model doesn't lose track of the sequence, we add a soft margin to the output vectors. This means that even if the output is different from the input, the model still treats it as a single unit. Formal: We tested a few different methods to train our model: truncated CKY (from Cao et al., 2015), CNN+BiLSTM (Zeng et al., 2015), LSTM+CNN (Chang and Lin, 2015), LSTM+CNN (Zeng et al., 2015), and LSTM+CNN with Softmax (Hochreiter and Schmidhuber, 1997). The experiments are summarized in Table 1. Formal: We tested two key things: how well the model can identify important parts of the input sequence, and how well it can handle different types of relationships between tokens in the input. Formal: In our experiments, we used the BiLSTM model with a dropout rate of 0.1. This setup works really well, both in terms of accuracy and how much the model handles relationships between tokens. Formal: To check how well the model can handle ambiguity and change, we tried different dropout rates: 0.1, 0.2, and 0.3. Formal: We also tried a mix of these dropout rates: 0.1, 0.2, and 0.3. Formal: The results are in Table 1. Formal: We used a transformer model with an initial learning rate of 0.1. Formal: We used the BiLSTM model with a dropout rate of 0.1. Formal: We used the CNN+BiLSTM model with a dropout rate of 0.1. Formal: The results are in Table 1. Formal: We also tried a mix of these dropout rates",
        "directory": "D07",
        "filename": "D07-1084.json"
    },
    {
        "casual_text": "Li et al. (2020) had a similar idea to ours, which is to make word representations better by using context. But their work isn't open source, and they didn't use the same dataset, so we can't really compare it to MINER. Instead, we compared our method to some baselines like this:",
        "formal_text": "Li et al. (2020) share the same intuition as us, enriching word representations with context. However, the work is neither open source nor reported on the same dataset, so this method cannot be compared with MINER. We compare our method with baselines as follows:",
        "GPT2_formal_text": "= [q_i, ..., q_j, ..., q_k] for i=1, ..., k=1, where each q_i is a vector that represents the query. For the sample, the vector z_s is made up of two parts: z_s_s and z_s_t, which are vectors that represent the sample's words in both the target language and the source language. Formal: • In this setup, the word embeddings in each sentence are combined into a single hidden representation using a linear transformation. Formal: • The model calculates the likelihood P(s, a_t) by taking the log probability of a word w_t, a_t, given the context w. Formal: • This project started with the idea of creating a word embedding model based on a model of context sentences. Formal: • The model learns to generate dialogue responses. Formal: • The model picks the response that has the highest probability among all the possible ones. Formal: • The model can decide to use the response it generates instead of the current one based on a yes/no decision made by the user. Formal: • The model takes the probability of a word w_t, a_t, and the context w and averages them to get the likelihood P(w_t, a_t) for the current response. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.383.json"
    },
    {
        "casual_text": "The interface stacks the source and target language segments, with the source on top and the target below for editing. In Figure 2, you can see the PosEdiOn interface: the upper box shows the source segment, and the lower one lets the translator make changes. There are toolbar buttons at the bottom that let you scroll through the whole document to see more context.",
        "formal_text": "The interface displays the source and target language segments one on top of the other. Figure 2 shows the PosEdiOn interface, where the upper window contains the source segment and the lower window enables the translator to edit the text. Translators can see a wider context using the toolbar buttons located on the lower part, which can be used to move along the whole document.",
        "GPT2_formal_text": "_feature_tgt. Formal: 1. For each word, check if the closest match is in the vocabulary. If it is, grab the feature vector for that word. Formal: 2. For each word in the document, find the closest match in the vocabulary. If both words are in the vocabulary, grab the feature vector for that word. Formal: 3. For each word in the document, find the nearest match in the vocabulary. If both words are in the vocabulary, grab the feature vector for that word. Formal: 4. For each word in the document, find the closest match in the vocabulary. If both words are in the vocabulary, grab the feature vector for that word. Formal: 5. For each word in the document, find the closest match in the vocabulary. If both words are in the vocabulary, grab the feature vector for that word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "Okay, so here's the deal: the monitor keeps an eye on the whole reconstruction process. If it thinks things are taking too long, it steps in by raising the threshold to stop the process when it detects inconsistency. Figure 3 gives an example of how this space reconstruction works.\n\nIn Fig. 3(a), the distance between entities A and B is 10. Now, let's say a new estimate puts that distance at 5. The reconstructor checks out the neighbors of both A and B. It decides to move entity B because B's neighbors are less crowded than A's. The reconstructor picks one spot out of eight possible positions around A where B can go with the least amount of new inconsistency.\n\nIn Fig. 3(b), B is placed to the left of A. Any new inconsistencies, like those involving B and G, are checked and noted in *inconsistent. After a few rounds of trial and error to reduce inconsistency, the space settles into the final configuration shown in Fig. 3(c), which is all good and consistent.",
        "formal_text": "in *inconsistent. 6. Go to step 1. The monitor monitors the whole reconstruction process and stops the process by raising the threshold to judge the inconsistency when it judges that the reconstrnetion takes too much time. Fig. 3 shows an example of the process of space reconstruction. Ill Fig. 3(a), the distance between the entities A and B was 10. Let's assume that a new estimation for tile distance is 5. The reconstructor looks around the neighbors of both entities, and decides to move tile entity B because the neighbors of B are less dense than those of A. The reconstructor selects one position that causes the least new inconsistency, from among eight positions around A, for B to be placed in. In Fig. g(b), ]: 3 is placed to the left of A. New inconsistencies ill the scope of *attention such as inconsistency about B and G are checked and registered in *inconsistent. After a few 3\"77 trial loops to decrease inconsistency, the space settles in the configuration shown in Fig. 3(c), which includes no inconsistency.",
        "GPT2_formal_text": ", and V_a is just the average of all the attention scores across all the token embeddings, which is 2 × d_a. Formal: We use the same train, validation, and test splits we used for BERT's pretraining, development, and test sets, with a batch size of 32. Formal: We fine-tune a single BERT model using the same settings as the one in the SQuAD 1.1 model. Formal: For these tests, we ran the dev set from HuggingFace Transformers on a single Nvidia GeForce GTX TITAN Xp GPU. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We used the same train, validation, and test splits as in SQuAD 1.1, with a batch size of 32. Formal: We fine-tune on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same. Formal: We fine-tune BERT on the same NVIDIA GTX TITAN Xp GPU, so the model's parameters stay the same",
        "directory": "C86",
        "filename": "C86-1089.json"
    },
    {
        "casual_text": "Ukrainian President says it's not true when @cnnbrk shared that Russia claims their records show a Ukrainian warplane was within 5 km of #MH17 on the day it crashed.",
        "formal_text": "Ukrainian Pres. says it's false \"@cnnbrk: Russia says records indicate Ukrainian warplane was flying within 5 km of #MH17 on day of crash.",
        "GPT2_formal_text": "ed/links/3 https://huggingface.co/en/make-up-masking-with-details.html https://huggingface.co/en/face-masking-with-details-with-gpt-2.html https://huggingface.co/en/tasks/16 https://huggingface.co/en/tasks/20 https://huggingface.co/en/tasks/25 https://huggingface.co/en/tasks/26 Formal: This part talks about the main concepts of the compositional approach, like the way we organize and combine words or sentences. Formal: We use a special kind of attention called composition-based attention, which is pretty similar to the attention mechanism in Transformers, and we add this to the word embedding layer. This helps us figure out how related a word is to the meaning of another word. Formal: The idea behind compositional attention is that each word should focus on the most important parts of the sentence to get the most accurate prediction. Formal: We add this extra layer of attention to the word embedding layer, and it works really well for predicting the meaning of the whole sentence. Formal: The final prediction for each word is made by combining the predictions from the hidden layers of the word embedding layer with the words in the sentence. Formal: We also tested this with a different attention mechanism, which is all about creating a vector for each word, like a mix of the hidden states from the different layers. Formal: Since we have a lot of different words in our training data, we decided to combine these vectors to form a representation for each word, which we call a vector for each word. Formal: We use these vectors to create a representation for each word, which we call a vector for each word. Formal: You can check out the code and all the hyperparameters for our experiments here: Formal: We trained our model using an NVIDIA GTX 1080 GPU and set the learning rate to 0.00005. Formal: For the evaluation part, we used BERTScore (shoutout to Devlin et al., 2019). Formal: We ran the model on a few different setups, but we stuck with the same settings for the main model that we talked about earlier. Formal: For the evaluation results, we did a random baseline without attention and a basic model without attention. Form",
        "directory": "D19",
        "filename": "D19-5006.json"
    },
    {
        "casual_text": "When you plug this IF into the mapper, it spits out the FS structure you see in Figure 6.",
        "formal_text": "When this IF is fed into the mapper, the FS structure shown in Figure 6 is produced.",
        "GPT2_formal_text": "is a group of sentences that includes one or more pairs of sentences, like S = (s1, s2, ..., smt). Here, smt is the best possible translation from a phrase-based translation model, using the phrase-based translation model's accuracy as a score. Formal: To find these best translation pairs, we use a method called maximum likelihood estimation on a big pile of manually annotated parallel data. We use this method to filter out any sentences that are too short or too long for the phrase-based translation model to work well with. Formal: We train the phrase-based translation model using data from one of the available language pairs, like English-German, to get the best possible translation of each sentence. We then pick the best translation from this model to create the sentence pair. Formal: We use the Maximum Likelihood Estimation (MLE) algorithm (from Schumann, 2002) to do this filtering. Formal: Here, n is the total number of sentence pairs we have, and c is the length of each sentence. Formal: We use a log-linear model for this. Formal: For every pair of sentences, we grab the sentence pairs with the highest MLE scores and align them to the phrase table. Formal: For each alignment, we try to match up the phrase pairs from the phrase table with the aligned sentences. Formal: We use a method called maximum likelihood estimation (MLE) to filter out sentences that don’t align well with the phrase-based translation model. Formal: We do this by estimating the probability P(s1|t) for each alignment. Formal: Formal: For each alignment, we calculate the probability P(s2|t) for each pair of sentences, using the alignment’s probability as a score. Formal: Formal: Formal: The main thing we’re focusing on here is finding the best possible translation for each sentence pair. Formal: We use maximum likelihood estimation on a big pile of manually annotated parallel data to find the best translation for each pair of sentences. Formal: Formal: Formal: For each alignment, we try to match up the phrase pairs from the phrase table with the aligned sentences. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "bcs",
        "filename": "2006.bcs-1.12.json"
    },
    {
        "casual_text": "So, the tasks in CLUES-Real have explanations from different teachers and students for each teacher. This gives us a lot of info about how teachers and students perform differently, showing how well different tasks can be learned through language. In section 4, we share some details about how teachers and students did in our setup.",
        "formal_text": "Thus, the tasks in CLUES-Real contain explanations from multiple teachers and multiple students corresponding to a teacher. This provides rich information about variance in teacher and student performance indicating how amenable different tasks are for learning via language. We provide insights into the performance of teachers and students of our setup in §4.",
        "GPT2_formal_text": "_rnn(T, T_i, T_i, v, y) = 1/p + (1/p + b). This is all done using the Adam optimizer with a learning rate of 0.0001. Formal: L_pn = max(α_pn, L_pn) ∈ R^T (1/p) + δ_pn. The final state vector is the sum of all the attention vectors. Formal: L_pn = max(α_pn) ∈ R^T (1/p) + δ_pn. Formal: A_pn = α_pn ∈ R^T (1/p) + δ_pn. Formal: Formal: L_pn = max(α_pn) ∈ R^T (1/p) + δ_pn. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.451.json"
    },
    {
        "casual_text": "We think the main reason DIORA struggles with supervised parsing is because it can't really fix mistakes once they happen. DIORA is great at finding the best tree based on the numbers it gets for each part of the sentence, but the problem is in how it assigns those numbers. It only uses local information, so it might give a low score to a part of the tree that should actually be higher if it had more context. This can be a big deal when a sentence is ambiguous and needs more context to make sense. For example, take the sentence \"We saw the dog with my phone.\" The way it's parsed depends on the context. In the next part, we'll show how we fixed this issue in DIORA.",
        "formal_text": "We posit the low performance in supervised parsing is due to DIORA's inability to effectively recover from local errors. Predicting trees in DIORA is exact -you are guaranteed to find the highest scoring tree given the scalar values associated with each span, but there is a weakness when assigning the scalar values. Specifically, the scalar values are assigned using local information, and may assign a low weight to a subtree which, when given more information, deserves to be given higher weight. Said plainly, this might occur when the sentence has structural ambiguity that requires context to resolve. For instance, the clause 'We saw the dog with my phone, ' has a more likely parse tree depending on the context. 2, 3 In the next section we present our extension to DIORA that addresses this downside.",
        "GPT2_formal_text": ". find(s) which is basically the source id that's set to s. The loss function for the generation process is defined like this: Formal: With the input sentence, the model generates a bunch of possible translation options. For each of these options, the model figures out the loss using something called a GRU. Formal: After running the model, we get back the translated sentence using the context vector. Formal: Using the context vector, we can figure out the loss for each of the generated sentence pairs. Formal: Finally, the loss function for the final result is calculated based on the model's output. Formal: Formal: Basically, this part shows how the model's output, called w, is calculated, along with some parameters θ and a margin λ. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.392.json"
    },
    {
        "casual_text": "Since there aren't many examples of split-antecedent anaphora in ARRAU, we decided to use four extra corpora to help the system perform better. These extra corpora came from either the Phrase Detectives (PD) corpus, which was annotated by lots of people, or the gold-standard annotated version of ARRAU.",
        "formal_text": "Since the number of examples of split-antecedent anaphora in ARRAU is small, we deployed four auxiliary corpora created from either the crowd annotated Phrase Detectives (PD) corpus or the gold annotated ARRAU corpus to improve the performance of the system.",
        "GPT2_formal_text": "ed_text\" and \"attention_masked_summary\" for the two attention heads, respectively. Formal: We start by encoding the document and its text into a Bi-LSTM. Then, we stick the whole thing together using the Transformer encoder. Formal: Here's what we call the \"text encoder\" in Figure 3. It starts with the document's representation, which we call e_d, and its summary representation, e_s. Here, d_t and e_d_t are the vectors representing the sentence's topic and the summary sentence's topic, respectively. Formal: For each document and summary, the encoder takes the document's representation e_d and the summary's representation e_s to create its context vector e_c, and the hidden state for the topic vector e_s. Formal: Finally, the decoder decodes the document and the summary using the pre-trained Transformer decoder. Formal: Lastly, the final input vector for the query decoder, which is a mix of the topic and topic vector of the query, gets fed into the hidden layer of the cross-modal transformer decoder. Formal: Lastly, the decoder decodes the document and the summary using the pre-trained Transformer decoder. Formal: In Section 4, we show that our method works way better than other methods. Formal: We run our experiments with two different models: the base Transformer model (S) and the one that uses our method, the Transformer model with attention masking (T). Formal: When it comes to multi-modal topic and multi-modal document, the results show that our method outperforms both single-modal and multi-modal models. Formal: To understand why our method works, we use cross-modal attention for the multi-modal model. Formal: To figure out how well our method can handle multi-modal documents, we use cross-modal attention for the multi-modal model. Formal: We tested our method on two datasets: the WMT14 dataset (Yencken et al., 2014) and the WMT2015 dataset (Vijayakumar et al., 2015). Formal: To figure out how well our method can handle multi-modal documents, we tested it on two datasets: the WMT14 dataset (Yencken et al., 2014) and the W",
        "directory": "coling",
        "filename": "2020.coling-main.538.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\n- **w_i**: Just think of this as a single word in a sentence.\n- **w_i w_i+1**: This is called a bigram, which means two words in a row, like \"word one\" and \"word two.\"\n- **S**: This is the whole sentence.\n- **tf_d(w_i)**: This is how many times the word **w_i** shows up in a specific document or text (let's call it **d**).\n- **df_D(w_i)**: This is how many documents in a collection (let's call it **D**) contain the word **w_i**.\n\nSo, in short:\n- **w_i** = a word\n- **w_i w_i+1** = two words together\n- **S** = the full sentence\n- **tf_d(w_i)** = how often a word appears in a specific text\n- **df_D(w_i)** = how many texts have that word in them",
        "formal_text": "w i unigram w i w i+1 bigram S sentence tf d (w i ) the frequency of w i occurring in d df D (w i )",
        "GPT2_formal_text": "' and the graph2tree features (which are just the node-based features). In this project, we’re focusing on the RS + US framework, which is basically the US version of the EURAMIS framework (thanks to Popović, 2012). The main differences between EURAMIS and RS + US are the concept of a 'knowledge pool,' which is a bunch of domain-specific knowledge and a shared feature representation, and a 'value representation' for the class labels. The main difference between EURAMIS and RS + US is how they handle the text segmentation and the tree structure. Formal: So, we’re looking at three different methods for pulling out concept features from text data, each based on the EURAMIS framework. Formal: We’re comparing our approaches to the RS + US framework. Formal: Here’s what we’ve done: Formal: We’ve got a model that can pull out concepts from text using the EURAMIS framework, as shown in Figure 1. Formal: We’ve made a simple dependency parser using the EURAMIS framework. Formal: We’ve created a small, general-purpose framework to get concept features from text. Formal: We’ve set up a system that can pull out features from any kind of text, including news articles. Formal: We’ve created a parser using the EURAMIS framework that can pull out features from any kind of text. Formal: We’ve tested our system and found that it works really well for learning about concepts in text. Formal: We’ve also created a small, general-purpose framework that can pull out features from any kind of text. Formal: We’ve tested this system and found that it works really well for learning about concepts in text. Formal: Formal: We’ve also created a parser using the EURAMIS framework that can pull out features from any kind of text. Formal: We’ve built a small, general-purpose framework that can pull out features from any kind of text. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C10",
        "filename": "C10-2060.json"
    },
    {
        "casual_text": "We're using this version: praw.readthedocs.io/en/latest. You can check out all our code here: http://www.cs.cmu.edu/pmichel1/mtnt/.",
        "formal_text": "In particular, we use this implementation: praw. readthedocs. io/en/latest, and our complete code is available at http: //www. cs. cmu. edu/ pmichel1/mtnt/.",
        "GPT2_formal_text": "_1 and attention_mask_2 are the hidden size and position, respectively. For the base model, we assume that all the words in the input text are in one language. To keep things simple, we just use word embeddings as input for the masked language model. This helps us create the hidden representation for the current word, which we call h_t. Formal: We also do word embeddings using a mix of CNN and LSTM with a recurrent neural network. The goal is to predict the next word based on the words in the input. Formal: For the masked language model, we just use the logit from the hidden layer, which is the same as the sentence embedding vector h_t. Formal: Lastly, we use a classifier (like the one from Devlin et al., 2019) to figure out if the generated text fits the task's domain. Formal: You can find more details about the architecture and the setup of the model in Appendix A. Formal: We also tried using a BiLSTM with a BiLSTM with a multi-head attention mechanism (like the one by Hochreiter and Schmidhuber, 1997) to generate the text. Formal: But since the output didn’t match the task's domain, we had to skip that part. Formal: The model did pretty well on task-related questions, even though it was missing the language modeling part. Formal: Formal: We also tested our model on the IMDb dataset to see how it performed. Formal: Formal: Our model also made some cool predictions for the IMDb dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1050.json"
    },
    {
        "casual_text": "Okay, let's break it down in simpler terms.\n\nSo, let's say τ 0 turns into τ j after a certain number of steps, and that number of steps is at least 1. In each step...",
        "formal_text": "Proof Assume τ 0 factorizes to τ j in a number of steps j ≥ 1. In every step",
        "GPT2_formal_text": "= {0, 1, ..., k}, from k=1 to N. Formal: We used the input_ids from Equation (6) to tweak the learning rate schedule. The learning rate is set to 1e-4 in our setup. Formal: We also added a max-over-time policy to help with the error propagation. Formal: To get the best possible label propagation, we set the learning rate to the maximum value it can go. This max-over-time policy can also be used during inference. Formal: During the initial process of creating the annotation, we try to maximize the log likelihood. After the first round of annotation, we calculate the posterior probability using the updated label set. Formal: For the final round of annotation, the model starts with the updated model score. The model is initialized with a weight vector of 0.1. Formal: Formal: In this paper, we’re focusing on using the latest NLP tech to improve the quality of NER annotations. But, we’re also exploring other cool stuff, like non-local, cross-lingual, and cross-lingual word embedding methods. Formal: Formal: We’re using the same decoding method as in (Zhu et al., 2020a) to get a final sequence of embeddings. Formal: We tested our NER models with various translation engines and datasets. Formal: Formal: We used the latest multi-encoder bidirectional RNN models as the encoders. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1204.json"
    },
    {
        "casual_text": "A6. Did they copy the previous work instead of reproducing it? And did they make sure that all the MT systems used the exact same pre-processed training, validating, and testing data? Just answer yes or no.",
        "formal_text": "A6. If previous work has not been reproduced but copied, whether it has been confirmed that all the compared MT systems used exactly the same pre-processed training, validating, and testing data: yes or no.",
        "GPT2_formal_text": "_p, t = (x_i, y_i, z_i) → ∆t, where t = φt. Formal: We train the neural network using the validation set from a supervised model. Formal: To make things clearer, we use a simple linear model with just a linear layer. We also include some extra linear layers to make things more efficient. Formal: At the start, we use a linear projection to figure out the attention weights for the context representation. Formal: After the fact, we grab the attention weights from the forward and backward hidden layers. We use a linear projection to get the attention weights for the whole sentence and layer. Formal: We use a linear projection to get the attention weights for the context and sentence representations. Formal: To get the attention weights for the whole sentence, we use a linear projection. Formal: For each sentence, we calculate the attention weights using a linear projection. Formal: The encoder layer in the linear model takes the attention weights from the forward and backward hidden layers, and then we use a linear projection to get the attention weights for the whole sentence. Formal: We train the neural network using the validation set from the supervised model. Formal: To make things clearer, we use a simple linear model with just a linear layer. Formal: After the fact, we grab the attention weights from the forward and backward hidden layers. We use a linear projection to get the attention weights for the whole sentence and layer. Formal: To get the attention weights for the whole sentence, we use a linear projection. Formal: For each sentence, we calculate the attention weights using a linear projection. Formal: We train the neural network using the validation set from the supervised model. Formal: To make things clearer, we use a simple linear model with just a linear layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.566.json"
    },
    {
        "casual_text": "The Online Primal Subgradient method, introduced by Ratliff et al. in 2007, calculates the subgradient for the margin objective for each instance by doing a loss-augmented decode. It then uses these subgradients for each instance to optimize the overall objective using Ada-Grad, which was developed by Duchi et al. in 2011. Ada-Grad can be used with either L1 or L2 regularization.\n\nThe basic version of Ada-Grad updates every weight when processing a batch. To speed things up, we split the updates into two types. When the subgradient isn't zero, we do the regular update. But when the subgradient is zero, we delay the update until the weight is needed again. This approach saves time because we only update the weights that have a nonzero subgradient in the current batch, which is usually a smaller number.\n\nAlgorithm 1 provides a pseudocode for our implementation, which was inspired by Dyer's work in 2013.",
        "formal_text": "Online Primal Subgradient (Ratliff et al. , 2007) Computes the subgradient of the margin objective on each instance by performing a loss-augmented decode, then uses these instance-wise subgradients to optimize the global objective using Ada-Grad (Duchi et al. , 2011) with either L 1 or L 2 regularization. The simplest implementation of Ada-Grad touches every weight when doing the update for a batch. To save time, we distinguish between two different types of update. When the subgradient is nonzero, we apply the usual update. When the subgradient is zero, we apply a numerically equivalent update later, at the next time the weight is queried. This saves time, as we only touch the weights corresponding to the (usually sparse) nonzero directions in the current batch's subgradient. Algorithm 1 gives pseudocode for our implementation, which was based on Dyer (2013).",
        "GPT2_formal_text": "= [ q ] for i in range ( 0 , n ) # Keep track of the i-th word in the sentence. This sentence-level attention is created by combining the attention from the word embedding layer with the sentence-level attention, as shown in Equation 1. Formal: So, n is the total number of words in the sentence, and v_i is the embedding vector for the word in the sentence at the i-th position. Formal: For each word in the sentence, we calculate its attention vector using this equation: Formal: The embedding for the i-th word in the sentence is created using the attention from the word embedding layer, as shown in Equation 1. Formal: So, the total number of words in the sentence is just the sum of all the word embeddings, which is v_n. Formal: Then, we combine these attention vectors to get the final attention vector for the i-th word in the sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D15",
        "filename": "D15-1032.json"
    },
    {
        "casual_text": "Okay, so we've got this thing where S_k is calculated by adding up U(k-j+1) {l_j } for j from 1 to k-1. After that, we tweak M_k by combining S_k with S(1) {l_k } and using the task loss L_T to fine-tune it.",
        "formal_text": "S k = k−1 j=1 {U (k−j+1) {l j } }. M k is then fine-tuned by using S k = S (1) {l k } ∪S k with the task loss L T.",
        "GPT2_formal_text": "ed, which gets a reward of 10 for each hidden state h_i. For the decoder, h_l represents the predicted output at position l, with the weights τ set to 0. And the output gate φ_l is just the regular output gate. Formal: The whole process of generating responses is basically this: Formal: Next, a linear layer grabs the \"head\" (h_l) and another linear layer does some more calculations. Then, the final output from both the decoder and the knowledge pool is calculated. Formal: Let’s break this down in a simpler way: Formal: • The input vector x. Formal: The attention mechanism works with a batch size of n, which is the total number of tokens in the response. Formal: The decoder (d) is trained using a cross-entropy loss function, trying to minimize this cross-entropy loss. Formal: In the context of this paper, the cross-entropy loss (δ_d) can be thought of as the cost of doing a single step. Formal: Let’s say the decoder is trained using a binary cross-entropy loss (d_ent) and the knowledge pool has a hidden size of h. The input vector x is also input to this loss function, and the loss is calculated using the cross-entropy loss. Formal: The output gate φ_l is just the regular output gate. Formal: The response generation (g) and knowledge pool (h) are combined in a pool (λ_g). Formal: The output from the pool is passed through a linear layer to create the actual response. Formal: Formal: Finally, the final output from the knowledge pool is used to calculate the reward for the knowledge pool. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-short.106.json"
    },
    {
        "casual_text": "When translating from right to left, the left side of the sentence (which is the later part in this case) is actually more accurate than the right side. This is kind of weird because earlier studies said that mistakes build up over time, making the end of the translation worse. But here, the end is better, which doesn't really match what we thought before.",
        "formal_text": "• When translating from right-to-left, the accuracy of the left half (in this way it's the later part of the generated sentence) is still higher than the right half. Such an observation is contradictory to the previous analyses between error propagation and accuracy drop, which regard that accumulated error brought by exposure bias will deteriorate the quality in later part of translation (i. e. , the left half).",
        "GPT2_formal_text": "_w, which are the names of the query and key, respectively. Formal: We're taking a look at how well named entity recognition (NER) systems work using a dataset that includes relation labels. Formal: The stuff we're testing is a mix of three main things: relation extraction, entity labeling, and relation detection. We're using a dataset called CoNLL-2009 to do this. Formal: For the entity labeler, we're using a dataset from the OntoNotes-1.0 dataset (Ours). Formal: For the relation labeler, we're using a dataset called CoNLL-2009. Formal: For the relation detector, we're using a dataset called CoNLL-2009. Formal: For both systems, we're using the CoNLL-2009 dataset. Formal: This dataset has relation labels from a wide range of types, including entities, relations, and concepts. Formal: For all three systems, we're training and testing them on this dataset. Formal: We're calling the number of samples (N) for each system T and the accuracy of our system's scores (acc) T. Formal: We're reporting the overall F1 score and how well the system does on the whole dataset as the F1 score. Formal: We're using the F1 score as our main metric because it works well with human evaluations. Formal: We're reporting the system's performance on CoNLL-2009 as the accuracy. Formal: We're reporting the F1 score for all three systems on the whole dataset as the F1 score. Formal: We're reporting the F1 score for the systems on the whole dataset as the F1 score. Formal: For the algorithms, we're reporting the accuracy, F1 score, and the number of samples (N) each one did. Formal: We're reporting the system's performance on CoNLL-2009 as the accuracy. Formal: We're reporting the system's performance on CoNLL-2009 as the accuracy. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1396.json"
    },
    {
        "casual_text": "A response with a bigger s mtv (y = 1|c, r) is more likely to get picked. We'll look into other combination models later on.",
        "formal_text": "The response with larger s mtv (y = 1|c, r) is more likely to be selected. We will investigate other combination models in our future work.",
        "GPT2_formal_text": "ing, which helps in understanding the context of text while looking at its structure. Formal: 1. The context of the sentence can be really varied, and you need to consider different parts like how the sentence is put together, the words, and the grammar. So, figuring out the perfect way to summarize the whole text is a tricky issue that needs a deep understanding of the text. Formal: 2. You can also come up with different ways to represent the sentence, like the left-to-right model shown in Figure 2(a). For example, you could use a simple two-token model to summarize the whole text, or you could mix different approaches, like the word2vec model shown in Figure 2(b) or the graph2tree model shown in Figure 2(c). Formal: 3. The main goal is to use different types of information—like what’s already in the sentence—to make the summarization better. Formal: 4. To make sure the summary is accurate and relevant, you need to make sure the information is gathered well. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "In Section 1, we start by using Bleualign for unsupervised alignment. Instead of using length-based methods like Sennrich and Volk (2011) to create an initial training set, we train Monoses and use it to give Bleualign machine translations of the sentences we're trying to align. Monoses is trained by creating crosslingual word embeddings from monolingual corpora using word2vec and Vecmap (Artetxe et al., 2018a), which helps build a phrase table. We then train a Statistical Machine Translation (SMT) system with this data and use it to translate a monolingual corpus from one of the two languages. The translated data is then used to train an SMT system in the other direction. We build a new phrase table and repeat the process three times to get the final model.",
        "formal_text": "As stated in Section 1, we initially employ Bleualign for unsupervised alignment, but instead of bootstrapping an initial training set with lengthbased methods like Sennrich and Volk (2011), we train Monoses and use that to provide Bleualign with machine translations of the sentences being aligned. Monoses is trained by building crosslingual word embeddings from monolingual corpora using word2vec and Vecmap (Artetxe et al. , 2018a), inducing a phrase table. An SMT system is then trained on this data and used to translate the monolingual corpus in one of the two languages. The translated data is then used to train a standard SMT system in the opposite direction. A new phrase table is built and the process iterated three times for a final model.",
        "GPT2_formal_text": "= {e i, ..., e i+m } is basically a way to group the output tokens together. Formal: Alright, let's talk about different types of errors. Formal: We're looking at different types of tokenization issues. Formal: If you want to know more about those, you can check out the papers we talked about in Section 1. Formal: We can also look at problems in the source and target language too, as long as they involve the same number of tokens. Formal: Lastly, we can handle issues that involve multiple tokens. Formal: So, if you have a sentence x i with tokens Y i, ..., Y i+m, which we'll call x i+m, we can figure out a solution by solving a specific optimization problem. Formal: Formal: Formal: In this case, we're dealing with a single sentence and its possible translations. Formal: But, this method can also be used for smaller tasks, like translating a sentence into a new language. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-srw.25.json"
    },
    {
        "casual_text": "The attention weights determine how much each hidden state contributes. In this case, the connection between the words \"sat\" and \"mat\" can be clearly picked up.",
        "formal_text": "The attention weights controls the individual contribution of each hidden states. In this example, the semantic dependency between the word \"sat\" and \"mat\" can be be explicitly captured.",
        "GPT2_formal_text": "(like the ones by Liu et al., 2016; and Xu et al., 2016) that use a special input called a word embedding. The attention is based on the fact that words usually show up together in sentences. Formal: Some research tries to learn an attention vector that connects different parts of a text by looking at both the actual words and the ones that appear before them. But this approach can be tricky because it might not pay much attention to important words that appear before the ones it was trained on. Formal: Another option is to use a single-layer perceptron to figure out the attention score. But this method isn't as straightforward, and it could cause issues if the input text is too long. Formal: Another approach is to train a neural network to create an attention vector. But this method can be a bit tricky because it might treat each word as a special type of data, without considering how words are grouped together. Formal: Lastly, there are fancier methods that use something called multi-head attention, which are kind of like those fancy neural networks but way more complex and hard to learn. Formal: We're not going into all the nitty-gritty details here, though. Formal: Alright, let's break this down in a simpler way.\n\nFirst, we take an input word x and its related vector w. Then, we calculate the attention score α_t(x, w), which is like a fancy way of saying how much attention we should pay to that word's vector. Formal: For each word x in the input text x, we look at all the words before it in the document d_i, where d_i is the document where that word appears. Formal: To make this work, we add a special vector z_i that looks at how the word x is related to other words in the text. Formal: Then, we just apply a simple linear transformation to the attention score α_t(x, w) to get the attention vector for x. Formal: Using this, we can figure out the attention vector for each word in the input text x by taking the average of all its attention vectors. Formal: After that, we calculate the attention score α_t(x, w) for each word in the output text x. Formal: Formal: Next, we take the attention vector α_t(x, w) and add it to the attention score α_t(x,",
        "directory": "C18",
        "filename": "C18-1124.json"
    },
    {
        "casual_text": "Check out Figure 3 for how the joint system is set up. The parser and semantic role labeler are linked in the embedding layer, using the same vector lookup tables for characters, words, and POS tags. Basically, the Bi-LSTM model from Section 2.1 is used for the SRL task, while the Stack-LSTM model from Section 2.2 handles the parsing task. Both the Bi-LSTM labeler and Stack-LSTM parser share the embedding layer. \n\nDuring training, we focus on minimizing the loss from both the semantic role labeler and the parser, which both affect the embedding layer. This helps create better vector representations for each token, making both tasks more accurate. However, since the neural structures are different, they don’t share any other parameters. \n\nThis joint model is a simpler version of shared training (like what Collobert et al. did in 2011), but it doesn’t use shared decoding (like Sutton and McCallum in 2005 or Zhang and Clark in 2008b). Instead, syntax and semantic roles are assigned separately, which avoids messing things up due to errors.",
        "formal_text": "The structure of the joint system is shown in Figure 3. Here the parser and semantic role labeler are coupled in the embedding layer, sharing the vector lookup tables for characters, words and POS. More specifically, the Bi-LSTM model of Section 2. 1 and the Stack-LSTM model of Section 2. 2 are used for the SRL task and the parsing task, respectively. The Bi-LSTM labeler and Stack-LSTM parser share the embedding layer. During training, we maximize the The loss from the semantic role labeler and the parser both propagate to the embedding layer, resulting in a better vector representation of each token, which benefits both tasks at the same time. On the other hand, due to different neural structures, there is no sharing of other parameters. The joint model offers the simplest version of shared training (Collobert et al. , 2011), but does not employ shared decoding (Sutton and McCallum, 2005; Zhang and Clark, 2008b). Syntax and semantic roles are assigned separately, avoiding error propagation.",
        "GPT2_formal_text": "'):\n       Casual: In Appendix A, we talk about how we find these individual words and how they connect with the text. Formal: In section 5.1, we explain how to make a list of keyphrases that can help us figure out the vocabulary. Formal: After that, we use the dictionary to check how the words fit together in the text. Formal: We also test how well the vectors work on their own (using Gibbs sampling) and also check how well they work with other vectors to make the summary better. Formal: In section 5.2, we show how we use these vectors to automatically create better summaries. Formal: In section 5.3, we tweak the vectors to pick the most important sentences from the summary. Formal: In section 5.4, we share our methods for building the vector spaces for sentences and paragraphs, which help us figure out the vocabulary. Formal: We also test how well the vectors work on their own (using Gibbs sampling) and also test how well they work with other vectors to make the summary better. Formal: We use these vectors to automatically create better summaries. Formal: Finally, in section 5.5, we show how we use these vectors to automatically create better summaries. Formal: We use the dictionary to check how well the words fit together in the text. Formal: After that, we use the dictionary to check how well the words fit together in the text. Formal: Lastly, we use these vectors to automatically create better summaries. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1098.json"
    },
    {
        "casual_text": "The integration of existing morphological processing tools has resulted in a really cool CAI tool. This tool offers a dictionary lookup, shows examples from real texts, and displays morphological info—all online. Adding support for other languages would be pretty straightforward since the basic framework is already in place. Even though the prototype is still a work in progress, the early results are looking great. By February, it was advanced enough for some communication students to do a user study. We'll share the full details later, but the study showed that people are interested in using it. In the near future, we're planning to organize the text data by lexemes and also want to add features like a teaching or diagnosing module to make the tool even better and more like real CAL software.",
        "formal_text": "The intergration of existing morphologica. I proeessing tools has led to a powerful CAI, I, tool. The tool provides a dictionary lookup, it, gives examples fl'om corpora and displayes morl~hological information, all on-line. Other languages could be easily imlflemented in the overall skeleton of (; I, ()SS I'; IUI{uC. Although development of the l)rototype ({ I, ()SSI~; l{. -[{, u(l is still ongoing, these first results look very promising. The prototype -( ' w~ts sul~ticiently advanced in li'ehruary ror (, n mgen communications stndellts to conduct an in vestigatiw~ user study. Although we. 'll report on this seperately, it indicated user interest. In the uear fllture we're planning to index the corpora on basis of lexemes, l, al, er we wish to extend tim' software with for example a teaching; rod diagnosting module so that the tool matures to real (: ALl, software.",
        "GPT2_formal_text": "_tgt;\n        Casual: Sure! Here's a more casual version:\n\nWe're looking at the cosine similarity between the hidden state h t and the hidden state h jt. Formal: The convolution-based network has two main parts: the convolutional layer, which has a window size of 1, and a max pooling layer, which has a window size of 3. This max-pooling layer is super important, and its weight can be different depending on the feature you're looking at. Formal: We're using a standard transformer model setup (Vaswani et al., 2017) with a hidden layer dimension of 512, a hidden layer dimension of 128, a batch size of 64, and the dropout rate set to 0.9. Formal: The input feature representation for the attention layer, which is the main goal for this task, is h t = {h jt }. Formal: The hidden state is also encoded using a vector. Formal: For each mention m i in a set M, we calculate the attention weight vector h ijt by averaging the attention weights from all mentions in M. Formal: The final attention weight vector h is calculated based on the score of the mention m i, which is calculated using the convolution-based network. Formal: After that, we pick the top k candidates that have the highest average attention weights and the biggest attention vector. Formal: The probability of a mention m i is calculated based on the weights for all its potential candidates, which are h t. Formal: Lastly, we consider a mention m i as a candidate if it’s at least k words away from m i’s representation in M. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C96",
        "filename": "C96-2140.json"
    },
    {
        "casual_text": "Figuring out what things in a text are referring to and connecting them to the right entries helps us make sense of documents and search queries. A lot of research uses knowledge bases like Freebase (Chiu et al., 2014), YAGO (Yosef et al., 2011), and Dbpedia (Olieman et al., 2014). Wikify (Mihalcea and Csomai, 2007) was one of the first projects to link text to Wikipedia pages. It looks at all possible n-grams that match Wikipedia concepts, like links and titles, and considers them as potential candidates. They use a voting system that combines knowledge-based and data-driven methods to figure out the best match. Cucerzan (2007) uses four types of resources to find candidates: entity pages, redirect pages, disambiguation pages, and list pages. Then, they compare the context of the text with the information on Wikipedia pages, including category tags, to pick the right candidate. They also look at all the n-grams in the document and keep the ones that have a high enough probability. To narrow down the candidates, they use the structure of Wikipedia links, focusing on how common and related the terms are.",
        "formal_text": "Recognizing entity mentions in text and linking them to the corresponding entries helps to understand documents and queries. Most work uses the knowledge base including Freebase (Chiu et al. , 2014), YAGO (Yosef et al. , 2011) and Dbpedia (Olieman et al. , 2014). Wikify (Mihalcea and Csomai, 2007) is the very early work on linking anchor texts to Wikipedia pages. It extracts all ngrams that match Wikipedia concepts such as anchors and titles as candidates. They implement a voting scheme based on the knowledge-based and data-driven method to disambiguate candidates. Cucerzan (2007) uses four recourses to generate candidates, namely entity pages, redirecting pages, disambiguation pages, and list pages. Then they disambiguate candidates by calculating the similarity between the contextual information and the document as well as category tags on Wikipedia pages. generate candidates by gathering all n-grams in the document, and retaining those whose probability exceeds a low threshold. Then they define commonness and relatedness on the hyper-link structure of Wikipedia to disambiguate candidates.",
        "GPT2_formal_text": "= {h1, h2, ..., hm, hn, ...}           Casual: If there's no matching answer, just return 0. Formal: You can find the code for the dataset right here: https://github.com/mikedeb/maxent. Formal: We're also planning to create some datasets in the future that have different types of entities, like entities that are questions or answers. Formal: The dataset and code we used for this paper are available to everyone. Formal: We're sharing our code for both the multi-task objective and the random walk objective, along with some code for the LSTM model. For more info, you can check out the original paper by Miedema et al. from 2016. Formal: To train our model, we use a data augmentation method. Formal: For the multi-task objective, we randomly replace one entity with its matching answer. For the random walk objective, we randomly replace two entities with their matching answers. Formal: We tested our model with a few different datasets: Formal: We picked some datasets based on how well they perform in the four tasks we're looking at. Formal: Our main goal is to see how well different models, including our own, can perform on these datasets. Formal: Formal: We ran our experiments using two NVIDIA Tesla P100 GPUs, each with 8GB of memory, and trained for 10 epochs. Formal: We looked at the results from three different seeds: Formal: We picked a random seed from the first 1,000 seed lists. Formal: We didn't use the same seed for the second set of experiments, which was used for the cross-lingual task. Formal: We trained all our models for 10 epochs on the validation set of the multi-task objective. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1007.json"
    },
    {
        "casual_text": "Pretty much anything that can be thought of or talked about individually can be a discourse object. So, stuff like objects and facts stored in a database, what the user types in, the commands they give, and the system's responses—all of these are potential discourse objects. Terms like \"discourse elements\" (Sidner, 1984) and \"discourse entities\" (Webber, 1984) are used to describe the things that are \"specified\" or brought up in a conversation. These things and how they relate to each other make up the discourse model of the person speaking. Hayes (1984) calls the objects, events, commands, states (and so on) that a system needs to understand \"entities.\" Following that idea, I think of a discourse object as something very broad—basically, the whole universe of discourse is just a bunch of these objects. Even a relationship between discourse objects counts as a discourse object. Sometimes, that relationship might also be part of the description of one or more of the objects involved.",
        "formal_text": "Virtually anything that can be perceived as and talked about as an individual may serve as a discourse object. Thus, objects and facts represented in a database as well as the user's inputs, the commands to be executed and the responses of the system are all (potential) discourse objects. Notions such as discourse elements (Sidner, 1984) and discourse entities (Webber, 1984) have been employed to denote the entities that are =specified\" or evoked by the constituents of a discourse, they and their relations then constituting the discourse model of a speaker. Hayes (1984) refers to the objects, events, commands, states (and so on) that an interface system needs to recognize collectively as \"entitities ~. In the same vein I ta~e the notion of a discourse object to apply in the most general sense; the universe of discourse is in principle just a collection of discourse objects. A relation between discourse objects is also a discourse object although it may also, or alternatively, be attributed to one or more of its constituents as part of their descriptions.",
        "GPT2_formal_text": ". The main thing to notice here is that we're using a standard word embedding matrix to represent the input text, which is e_i. Formal: In the second part of the process, we calculate a hidden representation for each of the m_i words (the ones before and after the one we're looking at). After that, we feed this hidden representation into a feed-forward network to get a new hidden representation for the word we're focusing on. Formal: This part is super simple. First, we figure out the hidden representation for the word we're looking at (the one before and after the one we're focusing on) and feed it into a BiLSTM (a fancy neural network that takes in inputs and output spaces). The network spits out a new hidden representation for the word. Formal: This part just checks if the word we're looking at matches a word in the collection. If it does, we add that word to the collection. Formal: We use the same approach for adding words to the collection as we did for the one before the one we're focusing on, and the only change is in the formula. Formal: We compare the two representations and pick the one that matches the one we're focusing on. Formal: Finally, we keep going like this, checking if the word we're looking at matches any words in the collection. Formal: We also use the same matrix e_i that we used in the first part to train our model. Formal: The whole thing is laid out in Figure 2, and it's all laid out in Figure 3. Formal: We tested our model on three different datasets and the results are in Table 4. Formal: Basically, the thing we're doing here is pretty straightforward. We take the word before the one we're looking at and add it to the collection, and then we use the model to guess if that word matches a word in the collection. Formal: We also tried out a method where we just copy the word before and after the one we're looking at, so it's like we're just copying the word itself. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E87",
        "filename": "E87-1024.json"
    },
    {
        "casual_text": "Basically, when k = 100 and p = 0.9, we used a loss truncation method with c = 0.6 for summarization. This gave us entropies of 18.08, 20.01, and 17.93. On the other hand, with k = 2 and p = 0.4, we used rejection sampling for summarization at c = 0.6 and α = 0.1, which resulted in entropies of 3.71, 4.02, and 3.84.",
        "formal_text": "Specifically, k = 100 and p = 0. 9 matched loss truncation at c = 0. 6 for summarization (entropies of 18. 08, 20. 01, and 17. 93 respectively). k = 2 and p = 0. 4 matched rejection sampling for summarization at c = 0. 6, α = 0. 1 (entropies of 3. 71, 4. 02, and 3. 84 respectively).",
        "GPT2_formal_text": "\" to create text features for an entity mention. This can be done in different ways:\n\n1. **Multilabel**: Instead of doing it with individual labels, you can use multiple labels that work together to create a feature for an entity mention. For instance, you might have multiple labels like (A1, A2, ..., AN), where each AN label matches up with a specific part of the entity mention.\n\n2. **Multi-label**: Instead of making multiple features, you can make one feature for each entity mention. For instance, you might have two labels like (A1, A2, ..., AN), where each AN label matches up with two parts of the mention.\n\n3. **Simultaneous label generation**: In a situation where the input (source) and target (target) entities are different entities, it's important to generate one feature for each pair. This lets you use both the source and target entities as input for the feature.\n\nFor instance, in Figure 2, the source entity is \"Liu Bo,\" and the target entity is \"Tsai Ming.\" The feature is \"Liu Bo,\" and the output includes the label \"Tsai Ming.\" Formal: The NER model creates a bunch of features for each entity mention, which are then combined to create a single feature for \"Tsai Ming.\" Formal: The same model creates multiple features for each entity mention, which are combined to create a single feature for \"Tsai Ming.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.66.json"
    },
    {
        "casual_text": "Okay, so let’s break this down in simpler terms. \n\nFirst, we’re using something called a Siamese dual-encoder (Lowe et al., 2017) to encode both the context and the responses. \n\nNow, let’s talk about the decoder. The decoder starts with the context encoding and generates a response one token at a time. Each new token depends on the ones that came before it. The decoder is trained to make sure it’s generating the right words by minimizing something called log-perplexity for each word in the correct response.\n\nBut here’s the issue: when we train the decoder on real conversation logs, it often ends up giving generic or totally irrelevant responses. To fix this, we add a classifier to the decoder. At each step of decoding, this classifier checks if the response so far is relevant to the conversation context. The classifier is trained to say \"yes\" (relevance of 1) if the response matches the correct answer and \"no\" (relevance of 0) if it’s just some random response. \n\nAt the same time, the decoder is still working on minimizing the word loss, which is the log-perplexity for each word in the correct response. \n\nSo, for any response r, the relevance loss can be written like this:",
        "formal_text": "(3) Using the encoded representations in a Siamese dual-encoder (Lowe et al. , 2017) that also encodes the responses. Vanilla Decoder: The decoder is initialized using the context encoding. The decoder generates the response autoregressively, that is, the token at each time-step is generated conditioned on the previous tokens of the response. The decoder is trained to minimize the log-perplexity of each word in the gold response. Decoder with sub-sequence validation: When trained on actual conversation logs, vanilla decoders often resort to generic responses or responses that are irrelevant to the context. Hence, to enforce relevance, we enhance the decoder with a classifier for each time-step of decoding. At each time-step, the classifier predicts the relevance of the response so-far for the given conversational context. The classifier is trained to predict a relevance of 1 for a prefix of the gold response and 0 for a prefix of any other randomly sampled response at each time-step of decoding. Simultaneously, the decoder is also trained to minimize the word loss, that is, log-perplexity of each word in the gold response. For any response r, the relevance loss can be written as follows:",
        "GPT2_formal_text": "ing_depth=1.0 mixweight_masking_depth=1.0 iteration_time=1.0)\n             Casual: Alright, let's talk about how we combine the attention we get from different layers into a single representation. Formal: We take the log-likelihoods for each input feature and the log probability for the final prediction, which we'll call the log_predict. Then, we normalize those log-likelihoods to get the attention weights. Formal: Next, we calculate these weights using an attention weight matrix, A_t. Formal: Finally, we normalize the attention weights to get the final output, which is the attention representation for the aspect mentioned in the query. Formal: For example, in the sentence \"France was the one who introduced the euro,\" the predicted weight for the aspect of \"introduce\" is called \"introduce.\" Formal: In the example above, the output of the network gets represented as \"France was the one who introduced the euro.\" Formal: The final output of the attention layer is then sent to a fully connected layer. Formal: This fully connected layer gets combined with a feedforward neural network (FFN), which is a type of neural network (like the one by Hochreiter and Schmidhuber, 1997). Formal: The final output of this combined layer, which is the attention representation for the aspect mentioned in the query, is then passed through a fully connected layer. Formal: For instance, if we have a query like \"France was the one who introduced the euro,\" the final output from the network would look like \"France was the one who introduced the euro.\" Formal: Formal: Formal: Formal: The final output from the attention layer is then passed through a fully connected layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-demos.20.json"
    },
    {
        "casual_text": "To make it easier to explain and understand how complicated the algorithm is, we need to introduce a few more terms related to the hierarchy. Besides the sequence length L, the number of hierarchy levels M, and the embedding or feature size d from Eq. (1), we’re adding these new terms: \n\n1) N r: the numerical rank of the off-diagonal blocks (for example, it’s 2 in Eq. (20)). This also happens to be the size of the diagonal blocks at the lowest level (level-0); \n2)",
        "formal_text": "To facilitate the description and the complexity analysis of the algorithm, we define a few more hierarchy-related notations. In addition to sequence length L, number of hierarchy levels M and embedding or feature size d in Eq. (1), the new notations include: 1) N r: numerical rank of the off-diagonal blocks (for instance, 2 in Eq. (20)). This is also the diagonal block size at level-0; 2) N",
        "GPT2_formal_text": "`] = { idx1, idx2, ..., idxk }, where `k` is the number of spans we're dealing with. Formal: The loss function we use is based on a specific setup for how the input text is represented. Formal: We train the model using the regular structured perceptron method to get a nice, steady performance. Formal: We also try out some different approaches to see how they compare. Formal: Table 1 has the results for text generation on our test set. The results in the figure show that our model, BERT (base model), beats all the other models we tested. We also tried using a transformer model to get even better results. But, this approach didn't perform as well as BERT. Formal: BERT and Transformer were trained on the same training data but had different datasets, so we didn't include their results here. Formal: The only exception is when we use the Tri-CNN model to generate a text. Formal: BERT is trained to make a text in a specific format. Formal: For comparison, we also looked at the multi-head attention model, which takes into account the context in the text. Formal: We ran our experiments using the settings from Dathathri et al. (2019). Formal: Our model did way better than the basic model and significantly outperformed the other models we compared it to. Formal: Lastly, we checked how well our model could generate text based on the instructions given to it. Formal: We used the same training set as Dathathri et al. (2019) to train the model. Formal: We didn't add any extra guidance during training. Formal: For reference, we also looked at the Multi-head attention model, which is trained on the same training set. Formal: The output from the model we trained on the 'gold' dataset was the same as the one from the model we trained on the 'gold' dataset. Formal: For comparison, we also did a sub-linear transformation between the output from the model trained on the 'gold' dataset and the one from the 'gold' dataset. Formal: Formal: Lastly, we tried out our model on the 'gold' dataset to see how it performed. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.294.json"
    },
    {
        "casual_text": "In Section 3, we talked about how CDRNN uses distributional regression, which means it has an IRF that shows how predictors affect the spread of the prediction over time. You can visualize the IRF for the variance just like you would for the mean.",
        "formal_text": "As discussed in Section 3, CDRNN implements distributional regression and thus also contains an IRF describing the influence of predictors on the variance of the predictive distribution as a function of time. IRFs of the variance can be visualized identically to IRFs of the mean.",
        "GPT2_formal_text": "ed_tokens = {[mask_tokens, [mask], [], [], [], [], [], [], [], [], []]. Formal: Once we've figured out the tokens in the text, we connect the key and value tokens and the word at the end to the set of tokens in the input text. Formal: We’re going to use the same method to figure out how important each token is, considering the context of the whole input text. We’ll call this process token importance, or EW for short. Formal: In the end, the final list of tokens is what we get from the last step. Formal: We’re using a mix of the old and new methods we talked about, as well as some new ideas we came up with. Formal: We’ve come up with a new way to check how important each token is, using something called attention weighting. Formal: We’ve also set up a new way to combine the old and new methods, using attention weights and attention scores. Formal: Since the results are calculated in a specific way, we’re not sharing the code or any numbers that go with these calculations. Formal: For example, the formula looks like this: Formal: Attention weighting can be written in a more straightforward way as: Formal: For each token, we calculate the average attention weight, a_i, for all the tokens around it, which we call w_t_i. Formal: We’ve also come up with a new way to combine attention weights and attention scores, using a weighted sum, which is called the weighted sum of a_i. Formal: Formal: Formal: We’re also using a weighted sum of the attention weights, which is the weighted sum of a_i. Formal: Formal: Formal: Finally, we’re using a weighted sum of the attention scores, which is the weighted sum of a_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.288.json"
    },
    {
        "casual_text": "In this paper, we introduced a new word-based labelling method for Thai sentence boundary detection (SBD) for the first time. We used a Linear Chain Conditional Random Field (LCRF) for sequence labelling, which worked really well compared to previous results on the ORCHID dataset. We also looked at how POS tagging could help with SBD using the TaLAPi corpus. We compared cascade models and joint models, and came up with a \"2-step\" joint model that combines POS tagging and SBD. This new model was more than twice as fast as the \"1-step\" joint model, but still got almost the same accuracy for SBD when using the same features. With the extra speed, we could add more features to improve SBD while keeping POS tagging performance on par.",
        "formal_text": "In this paper, we demonstrated for the first time a word-based labelling approach to Thai SBD. The word-based labelling approach was proposed to leverage LCRF to do sequence labelling which achieved very good performance compared to reported results on ORCHID data. Furthermore, the performance of SBD with the help of POS tagging was investigated on the corpus TaLAPi. Cascade models and Joint models were compared and the \"2-step\" Joint POS tagging with SB detection was proposed. This proposed model saved more than half of the time, while obtaining almost the same accuracy for SBD as \"1-step\" Joint model, when using the same feature set. With increased speed, more features were therefore used to improve SBD and yields comparable POS tagging performance.",
        "GPT2_formal_text": "= [query, entity_id, entity_type, slot, value] form = Response(Form( self.model.query, self.model.entity_type)) return form return Response(Form( self.model.slot, self.model.value)) Formal: The latest models for event extraction work on the same model structure as the event trigger extraction model. Formal: The type of the input text is specified by the [CLS] token. If the [CLS] token is used, the input text is processed as a single unit, and the type is chosen based on the target type and how it fits into the event. Formal: The context of the input text is basically the whole event, which includes the events and the target types. Formal: The event type can be identified by the [EVENT] token. If the [EVENT] token is present, the event type is chosen based on the target type and how it fits into the event. Formal: The event trigger is matched to the [EVENT] token. If the [EVENT] token is present, the trigger is chosen based on the target type and how it fits into the event. Formal: The argument label is matched to the [EVENT] token. If the [EVENT] token is present, the argument label is chosen based on the target type and how it fits into the event. Formal: The entity type is identified by the [EVENT] token. If the [EVENT] token is present, the entity type is chosen based on the target type and how it fits into the event. Formal: The event type can be identified by the [EVENT] token. Formal: The value type is identified by the [EVENT] token. If the [EVENT] token is present, the value type is chosen based on the target type and how it fits into the event. Formal: The input text is then passed through a single-layer GCN decoder to create a representation for the event. Formal: Finally, the event is represented using the [EVENT] token and the [EVENT] token. Formal: Formal: Formal: The input text is sent to the model through a single-layer GCN decoder to create a representation for the event. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1031.json"
    },
    {
        "casual_text": "So, on the brain cancer test set, the F1 score was 566, which is way lower than what the MTL Model got. Here's a breakdown of the flops/inst and total instances for different models:\n\n- **MTL Model**: 218,767,889 flops/inst, 20k instances, Ratio OP 1\n- **OP+MTL**: 218,783,260 flops/inst, 20k instances, Ratio OP 1\n- **Multi-pass**: 218,724,880 flops/inst, 427k instances, Ratio OP 23\n- **Multi-pass+Silver**: 218,724,880 flops/inst, 497k instances, Ratio OP 25\n\nTable 4 shows the computational complexity in terms of flops per instance and the total number of instances.",
        "formal_text": "566 F1 on the brain cancer test set, significantly lower than the MTL Model flops/inst inst # Ratio OP 218, 767, 889 20k 1 OP+MTL 218, 783, 260 20k 1 Multi-pass 218, 724, 880 427k 23 Multi-pass+Silver 218, 724, 880 497k 25 Table 4: Computational complexity in flops per instance (flops/inst)×total number of instances (inst#).",
        "GPT2_formal_text": "ed is the input representation. After the first convolution, we use a fully connected layer to get the representation for the word token y_t. After that, we pass this representation through a fully connected layer to get the contextual representation for the token, which we call c_t. Formal: Basically, it's about figuring out the context of the current token. Formal: In the token embedding layer, we take the contextual representation of a word (y_t) and use it to represent the whole sequence. Formal: Finally, we use this contextual representation to get the output of the output layer. Formal: This is just a quick summary of how we calculate and combine the contextual representations for the current token, y_t. Formal: Formal: The full source code is available online at https://github.com/diab/DT-Bert. Formal: Also, check out Appendix B.4. Formal: Here's the pre-trained model setup we used: Formal: We trained it on 500,000 tweets. Formal: We trained it on a smaller dataset of 1.5 million tweets. Formal: For each token, we had one of the authors (V) label it, and then we combined the output from all the authors. Formal: For each token, the token embedding layer was used, and then we used the contextual representation to get the output from the output layer. Formal: Formal: This setup helps us understand the context of a token better. Formal: Formal: We calculated the token embedding layer's weight using the logarithm. Formal: Formal: We used the cross-entropy loss function to minimize the cross-entropy. Formal: Formal: We set the dropout rate to 0.4. Formal: Formal: We also trained an entity detection model using the input sentence embedding layer, which is what we used for token embedding layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "bionlp",
        "filename": "2020.bionlp-1.7.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\nWe have a set of sentence pairs: C = {(S₁, T₁), ..., (S_N, T_N)}. We also have a maximum number of iterations we want to do, which is M, and a depth value δ for something called \"seg rev.\" The goal is to get a word alignment A = {A₁, ..., A_N}.\n\nHere's how we do it step by step:\n\n1. Start with an empty alignment, so A is empty.\n2. Loop through the iterations from 1 to M:\n   - Set I and C to empty.\n   - If A is still empty:\n     - Go through each sentence pair from 1 to N.\n     - Use \"seg rev\" with depth δ on the pair (S_n, T_n, A_n) to get RS and RI.\n     - Add (RS, T_n) to the set C.",
        "formal_text": "C = {(S 1, T 1 ) • • •, (S N, T N )}; maximum iteration M; depth δ for seg rev; output: word alignment A = {A 1, • • •, A N }; 1 A ← ∅; 2 for iter ← 1 to M do 3 I ← ∅; C ← ∅; 4 if A = ∅ then 5 for n ← 1 to N do 6 RS, RI ← seg rev δ (S n, T n, A n ); 7 append (RS, T n ) to C;",
        "GPT2_formal_text": "* word_ids) = (1, 2, 3, 4, 5) for (i, j) in list(T) if (i > j) or (j > i): return (T[i, j] + T[i, j] + ... + T[T[i, j]]) * (T[i, j] + T[i, j] + ... + T[T[i, j]]) Formal: There are two ways to handle this stuff: the method using the tree structures (check out Eq. 3) and the method using the word types (Eq. 4). Formal: The tree structures deal with the word types that are way bigger than the word types in the target language. The word types are usually shorter, and they don't connect well with the dependency structure of the target sentence. Formal: Using the dependency structure can help fix this problem, but it means the output is more complicated. Formal: Word types are super important because they can give us the longest source sentence in the target language. Formal: The problem with using the word types as input is that we need to compare each word in the source sentence with every word in the target sentence. This means we need to handle the dependency structure of the target sentence in every step. Formal: Even though words can be very different in meaning, we can still use the dependency structure to figure out the longest source sentence in the target language. Formal: The method we're suggesting, which uses the tree structures, doesn't need to compare every word in the source sentence with every word in the target sentence. Formal: This method works because we can use the shortest source sentence in the target language to help the tree structures match up with the dependency structure of the target sentence. Formal: Formal: This method isn't as accurate because we can't use the source sentence's dependency structure to predict the longest source sentence in the target language. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D15",
        "filename": "D15-1119.json"
    },
    {
        "casual_text": "Alright, so P(f) doesn't depend on the domain D. Also, we're just assuming that P(D) stays the same, like a constant. The formula we're looking at here is all about finding the best domain. It's like trying to figure out the argmax.",
        "formal_text": "Here, P (f ) is independent of domain D. Furthermore, we assume P (D) to be constant. The following formula embodies the search for the optimal domain. argmax",
        "GPT2_formal_text": "ing. This setup is based on the fact that words that share the same stem (masculine or feminine) are often used in similar ways in writing. We think that words that don’t share a stem tend to have similar meanings because they often look similar in spelling and have similar character sets. To create a word type embedding matrix W_u that focuses on the most common word type in a text, we use the word embedding matrix W_u that was made from the words in the texts. Formal: A big part of our semantic embedding model is figuring out the relationships between the words in a sentence. We use a bi-directional LSTM network to pull out all the semantic features from the words. Formal: Once we have the features from the words, we predict the relationships between them using a multi-head attention mechanism to create a representation for the sentence. Formal: In this paper, we’re introducing a new method for creating semantic embeddings that work for both text and images. We’ve come up with a new way to encode word-image relationships using a hierarchical attention network. This network helps us understand how words are related to each other. Formal: We’ve also come up with a new method for predicting the relationships between words in a sentence by using a multi-head attention mechanism. Formal: To show how well our method works, we’ve run a bunch of experiments. Formal: We’re sharing the code, trained models, and pre-trained word embeddings that we used in this project. You can check out the results in Table 1. Formal: We’ve also included the hierarchical attention network that we used in our experiments. Formal: Our model includes a Bi-directional LSTM network, a Bi-LSTM+Attention mechanism, and a multi-head attention mechanism for word-image relations. Formal: We’ve also shared the embeddings that we used for each layer. Formal: Our model includes a Bi-directional LSTM network, a Bi-LSTM+Attention mechanism, and a multi-head attention mechanism for word-image relations. Formal: We’ve also shared the embeddings that we used for each layer. Formal: Our model includes a Bi-directional LSTM network, a Bi-LSTM+Attention mechanism, and a multi-head",
        "directory": "D07",
        "filename": "D07-1054.json"
    },
    {
        "casual_text": "The kappa scores for the petfoodrecall and vtech datasets were pretty consistent among the raters, showing good agreement. However, the spitzer dataset didn't do as well—there was only fair agreement. As for the iPhone dataset, the evaluators had a lot of disagreement on what they thought were subtopics.",
        "formal_text": "The kappa scores for the petfoodrecall and vtech datasets showed good agreement among the raters, while the spitzer data set had only fair agreement. For the iPhone data set, both evaluators had a large amount of disagreement on what they considered subtopics.",
        "GPT2_formal_text": "= (s, e) + 1, ..., (s, e), where i represents the i-th token, are the scores for the prediction in the target language. The score for the s-th token is calculated using a Dirichlet distribution with parameters β and γ, where β and γ are set to 0.5 and 0.1, respectively. The output from the cross-attention mechanism is a single vector that's md by d_simd. Formal: Here, the input vector s, the target language prediction e, the input embedding e_t, the cross-attention matrix, and the output embedding e_t are all in R^n. Formal: Once the prediction is made, the embedding e_t is created by combining the input embedding e_t and the probability distribution P(t|s) across the m-dimensional hidden states H_t and H_simd, respectively. The final output vector e_t is then passed through a multi-layer perceptron (MLP) to get the predicted probability distribution P_t. Formal: The cross-attention mechanism has three main parts: input, embedding, and output. Formal: Formal: To make use of the cross-attention mechanism, we'll use two different methods: one for training a language model (LM) and another for fine-tuning the cross-attention mechanism (CDM). Formal: We'll use the cross-attention mechanism and its parameters as inputs for the language model. Formal: The CDM will use the cross-attention mechanism along with the target language model (LM) to predict the target sentence. Formal: Formal: Lastly, we'll incorporate the cross-attention mechanism into the fine-tuning process. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C08",
        "filename": "C08-1077.json"
    },
    {
        "casual_text": "We tackled this optimization problem using a branch-and-bound algorithm, which was first introduced by Land and Doig back in 1960. This graph alignment problem is generally considered NP-hard, as Klau pointed out in 2009, and is typically solved in an approximate way, kind of like how beam search works. But, the unique structure of constraints 1 to 3, which come from the requirement that the alignment function must be injective (one-to-one), lets us solve this optimization exactly.\n\nNow, our version of the branch-and-bound algorithm doesn’t usually run in polynomial time, but in practice, we found it works pretty well. We were able to compute optimal alignments efficiently in almost all cases—only less than 0.1% of the alignment pairs in our data took too long to solve. This good performance is mainly because we don’t have to deal with aligning entire graphs, and the number of nodes in the subgraphs we’re aligning is limited.",
        "formal_text": "We solve this optimization problem with a version of the branch-and-bound algorithm (Land and Doig, 1960). In general, this graph alignment problem is NP-hard (Klau, 2009) and usually solved approximately following a procedure similar to beam search. However, the special structure of constraints 1 to 3, originating from the required injectivity of the alignment function, allows us to solve the optimization exactly. Our implementation of the branch-and-bound algorithm does not generally run in polynomial time, however, we found that in practice we could efficiently compute optimal alignments in almost all cases (less than 0. 1% of alignment pairs in our data could not be solved in reasonable time). This relatively benign behavior depends crucially on the fact that we do not have to consider alignments between full graphs, and the number of nodes in the aligned subgraphs is limited.",
        "GPT2_formal_text": "ed_s, which is basically the token distribution from the masked dataset Y_m. Since we're using a linear layer to adjust the representations of the masked tokens, we'll call this layer the \"masking layer.\" For example, in Figure 2, the masked token representation we get from this layer is labeled as Y_mask. Formal: Let's use a simple linear mapping to represent the masked token distribution Y_m. Formal: In this linear mapping, the token representation Y_m is just the linear combination of the masked token representation Y_m. Formal: In the masked dataset Y, the mapping π is just the output of the linear mapping. Formal: Here, we're assuming that the masked token representation Y_m is represented in a specific way, which we'll call the \"token representation.\" Formal: Once we have the token representation Y_m, we can plug it into the linear mapping π to get the actual representation of the token Y. Formal: The token representation Y_m has two main parts: a non-linearity and a bias term. The non-linearity is a 1-dimensional linear transformation, while the bias term is a linear projection that takes a vector w_i as its input. Formal: The non-linearity can be written as π(w_i) = 1×n. Formal: The main goal of the mapping is to create a uniform vector w_i that represents the masked token distribution Y_m. Formal: We've come up with a way to calculate the embedding of the masked token representation Y_m, which we'll call the \"masked token embedding,\" y_m. Formal: The masked token embedding is basically a uniform embedding of the masked token representation, y_m. Formal: We've also come up with a way to calculate the position embedding of the masked token representation, which we'll call the \"masked token position embedding,\" y_m. Formal: Finally, we can calculate the hidden representation of the masked token representation, y_m, by taking the dot product of the hidden representation of the masked token representation, y_m, and the output of the linear mapping π. Formal: We're using the same approach as in equation (6), but this time, instead of setting the shape to a uniform, we're setting it to a matrix of size T. Formal: The whole process of mapping",
        "directory": "D09",
        "filename": "D09-1002.json"
    },
    {
        "casual_text": "Back in 1991, Elman pointed out that messing around with artificial systems can help us cut through the noise of real-world language data. But to make sure our model actually learns recursive patterns and not just simple, limited ones, we need to test it on structures that are more deeply nested than what it was trained on. That’s exactly what we did. Generalised Dyck languages are perfect for this kind of testing (Bernardy, 2018). \n\nNow, LSTMs (and GRUs) can handle some deeper nesting, but their performance drops as the nesting gets more complex, just like how they struggle with natural language agreement stuff. Other studies have shown similar results (Hewitt et al., 2020; Sennhauser and Berwick, 2018). The same thing happens with generative self-attention models (Yu et al., 2019), while BERT-like models, which aren’t generative, just can’t handle this task at all (Bernardy et al., 2021).",
        "formal_text": "Dyck Languages Elman (1991) already ob-served that it is useful to experiment with artificial systems to filter out the noise of real world natural language data. However, to ensure that the model actually learns recursive patterns instead of bounded-level ones, it is necessary to test on more deeply nested structures than the ones that the model is trained on, as we did. Generalised Dyck languages are ideal for this purpose (Bernardy, 2018). While LSTMs (and GRUs) exhibit a certain capacity to generalise to deeper nesting their performance declines in proportion to the depth of the nesting, as is the case with their handling of natural language agreement data. Other experimental work has also illustrated this effect (Hewitt et al. , 2020; Sennhauser and Berwick, 2018). Similar conclusions are observed for generative self-attention architectures (Yu et al. , 2019), while BERT-like, non-generative self-attention architectures simply fail at this task (Bernardy et al. , 2021).",
        "GPT2_formal_text": "\": { [0], [1], [2], [3], [4], [5] }, { \"type\": [0, 1], \"id\": [1], \"value\": [2], \"type\": [2, 3], \"value\": [4], \"type\": [5] }, { \"type\": [0, 1], \"id\": [1, 2], \"value\": [2, 3, 4, 5] } ], { \"type\": [0, 1], \"id\": [1, 2], \"value\": [2, 3, 4, 5] } ], { \"type\": [0, 1], \"id\": [1, 2, 3], \"value\": [1, 2, 3, 4, 5] } ], { \"type\": [0, 1, 2], \"id\": [2, 3], \"value\": [2, 3, 4, 5] } ] } Formal: After adding the entity embedding, we make a column vector for each e_i, using the type of e_i as the id for the column vector. This id is fed into an LSTM (Hochreiter and Schmidhuber, 1997) to get a hidden representation of the entity e_i. This representation is then fed into a bi-directional LSTM (Bi-LSTM) to create a target representation of the entity e_i, which we call e_i_t. Formal: For each entity e_i and its possible values v_i, we calculate the entity embedding e_i using the h(e_i) ∈ R t×d. Formal: This setup allows us to use any input we want—like the type of entity e_i, its values, or the entity embedding e_i—to predict the value for a particular value v_i. Formal: For each e_i and its possible values v_i, we calculate the entity embedding e_i using the h(e_i) ∈ R t×d. Formal: This setup allows us to use any input we want—like the type of entity e_i, its values, or the entity embedding e_i—to predict the value for a particular value v_i. Formal: Formal: This means we can tweak the input to match the type of entity e_i, its values, or the embedding e",
        "directory": "cmcl",
        "filename": "2022.cmcl-1.2.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nThis is an interactive argument pair. For instance, in Figure 1, the review argument spans are set X, which is {â v 1, â v 2} = {(3, 5), (6, 9)}, and the rebuttal argument spans are set Ŷ.",
        "formal_text": "is an interactive argument pair. For example, in Figure 1, the review argument spans set X is {â v 1, â v 2 } = {(3, 5), (6, 9)} and the rebuttal argument spans setŶ is",
        "GPT2_formal_text": "ed_cores = self.attn_masked_cores. len() - 1), self.attn_masked_bios = self.attn_masked_bios. len() - 1), self.attn_masked_probabilities = self.attn_masked_probabilities. len() - 1), self.attn_masked_probabilities_cores = self.attn_masked_probabilities_cores. len() - 1), self.attn_masked_probabilities_bios = self.attn_masked_probabilities_bios. len() - 1) - 1), self.attn_masked_probabilities_bios_cores = self.attn_masked_probabilities_bios_cores. len() - 1) - 1). Formal: Check out this link for more info: https://github.com/sshenk/attn_masked_cores. Formal: Lastly, there's also the fact that the length of the generated sequence is N(1), which is basically the length of the input sequence x. Formal: Since the input sequence is O(N^2) and the output sequence is O(N^3), we can easily find the length of the generated sequence in the GCN. Formal: For example, if we have a GCN with N outputs, the length of the output sequence in the GCN is N(1), which is the length of the input sequence x. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.319.json"
    },
    {
        "casual_text": "Creating hand-crafted features is important, but it’s also super time-consuming. It’d be great if we could just automatically learn useful features from text. For figuring out how readable a whole document is, a good approach is to combine the representations of the words in it (Kim, 2014). When it comes to representing words, a cool trick is to turn them into dense, low-dimensional vectors, which is called word embedding. There are already some word embedding models out there (like the ones by Collobert et al., 2011; Mikolov et al., 2013; Pennington et al., 2014) that could help with readability assessment, but they have a downside: they mainly focus on how words relate to each other in terms of syntax or meaning, and they don’t really consider how easy or hard those words are to read. So, words like \"man\" and \"gentleman\" might end up close in the vector space because they’re similar in function or topic, even though they’re not equally easy to understand. This means we need to find a way to include reading difficulty when training these word embeddings.",
        "formal_text": "Designing hand-crafted features are essential but labor-intensive. It is desirable to learn representative features from the texts automatically. For document-level readability assessment, an effective feature learning method is to construct the representation of documents by combining the representation of the words contained (Kim, 2014). For the representation of word, a useful technique is to learn the word representation as a dense and low-dimensional vector, which is called word embedding. Existing word embedding models (Collobert et al. , 2011; Mikolov et al. , 2013; Pennington et al. , 2014) can be used for readability assessment, but the effectiveness is compromised by the fact that these models typically focus on the syntactic or semantic relations of words, while ignoring the reading difficulty. As a result, words with similar functions or topics, such as \"man\" and \"gentleman\", are mapped into close vectors although their reading difficulties are different. It calls for incorporating the knowledge on reading difficulty when training the word embedding.",
        "GPT2_formal_text": "= {\n       Casual: Alright, let’s break this down in a simpler way. In the first part, we’re looking at two different parts of our model: one where we’ve added some soft features (we call this model (1)a) and another where we’ve kept the original feature (1)b). \n\nNow, about those two-part models: one (1)a) uses the character sequence (s) as the input, and the other (1)b) uses the word sequence (w). Both of these models work by creating a hidden vector, h_t, which represents the word sequence, a_t.\n\nIn our experiments, we set h_t to 300. The idea here is that since the input is different (s is different from w), the vectors we create should be different. We measure how similar the vectors are by looking at something called cosine similarity.\n\nFor our experiments, we’ve used the English Gigaword corpus, which has around 6 million words in total. We’ve also included the average word length for all the words in our training data, which we call W. To make sure we’re comparing our model to something, we’re using the BLSTM model from Mikolov et al. (2013a). This model has a hidden layer size of 300.\n\nWe tested our model with different learning rates, starting with 0.1, 0.5, and 0.95, and then looking at the results after each increase in learning rate. For all these experiments, we set the learning rate to 0.01. We also made sure to include a random initialization, which we call θ.\n\nFor all these experiments, we trained the model for 100,000 steps. \n\nFor the second part, we’re checking out how the word sequence (a_t) affects the vector we create (h_t). We measure how similar the vectors are by looking at cosine similarity, which we call S. Formal: For each time step, we create a vector with the word sequence (s) as the input, and the second vector is the word sequence (w). Formal: For a given input, the word sequence (a_t) is represented by the vector (h_t) multiplied by its own vector (s_t), and the word sequence (",
        "directory": "C18",
        "filename": "C18-1031.json"
    },
    {
        "casual_text": "Orthogonal transforms have some pretty cool features. For instance, you can get the inverse matrix by just flipping it around (transposing it). Plus, it keeps the L2 norm of a vector intact after the transformation. In our case, we're mainly interested in that handy trick of finding the inverse by transposing. This helps cut down on the number of model parameters we need to deal with (check out Table 3 for details).",
        "formal_text": "Orthogonal transform has many desired properties, for example, the inverse matrix is obtained by simply transposing itself. It also preserves the L 2 norm of a vector after the transform. For our work, we are just interested in its property to obtain inverse matrix by simple transposing. This saves the number of model parameters (see Table 3 ).",
        "GPT2_formal_text": "ed_vec2 is a matrix, and the softmax function is applied to it. Formal: If we have a set of input embeddings x that doesn't have any parts that are word-related, and we're assuming there's no word-based context to learn, the objective function we get looks like this: Formal: (2) Formal: The full explanation of the attention-based CNN embedding function is in Section 3. Formal: (3) Formal: For example, here's how the CNN and attention-based attention vector look for a sentence in an image caption: Formal: Here, φ is a matrix, and the softmax function is applied to it. Formal: The matrix [h(x)] is the input hidden state, and the output vector y is the attention vector for that sentence. Formal: In this case, the input embedding vector x is a 2D vector, which is like a representation of the input embedding y. Formal: Formal: And the final representation h(x) is the output vector for that sentence. Formal: Formal: Formal: In the equation, W represents the number of layers, and N is the total number of parameters. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.241.json"
    },
    {
        "casual_text": "For the scoring experiment, we found out that scoring is super tough, and we still need to do more work to get better results. But hey, we did get some promising stuff—almost a 0.7 correlation for the advanced class in a task where people often disagree a lot.",
        "formal_text": "In terms of the scoring experiment, we observed that scoring is a very challenging task and that there is still need for further study before we can get good results. Even so, we did get some encouraging results, reaching almost 0. 7 correlation for the advanced class in a task that is known to present high discordance among evaluators.",
        "GPT2_formal_text": "= []. Each letter is a word, so the sum of the normalized tokens is a positive number. We use a modified version of the SVM algorithm, which was introduced by Pennington et al. in 2014. The parameters for this algorithm are: α = 0.5, β = 0.1, and γ = 0.1. Formal: The rest of the text goes like this: The first sentence is made up of the first letter of the word (which we call w_i). Then, the next letter w_j is added to make the next sentence. The rest of the words, w_i−1 to w_j, are added to each of the words in the text. Formal: After that, the last letter of the word (w_k) is used to create the last sentence. Formal: After that, the first letter of each word (w_1 to w_k) is used to create the first sentence of the next word. Formal: Finally, the last letter of the word (w_l) is used to create the last sentence of the previous word. Formal: In the examples we just looked at, the length of the text is the same for each word. But in real-world scenarios, the length can vary depending on the situation. For instance, a sentence might have more than one word, and the last word in it could be a pronoun, an exclamation, or a question. So, the length of the text can be different for each word in the text. Formal: We also calculate the average length of the words in the text, which we call w_i, where w_i = w_1 to w_k, and the average length of the words in the rest of the text, w_i−1 to w_j, is also included. Formal: We set the weights for the parameters by randomly choosing from a set of options, which we call {w_0, w_1, ..., w_k}. Formal: The final result of the algorithm is a vector (w_i, w_k, δ) that includes the last letter of each word in the text. Formal: Formal: For each word in the text, we run the SVM algorithm on the words in the text to get its word embedding. Formal: We also use a dynamic programming method that works with a multinomial distribution to handle the non-",
        "directory": "C18",
        "filename": "C18-1294.json"
    },
    {
        "casual_text": "Using hints while learning, especially when you've got 1000 or fewer data points, makes a big difference and can even work better than just trying to teach yourself.",
        "formal_text": "• Learning with hints, especially for ≤ 1000 training data points, helps significantly, even over self-training.",
        "GPT2_formal_text": "ing_base. Formal: The state vector is updated based on the input token. Since the attention weights for the attention heads in T 3 and T 4 are different (check out equation (6)), they can't overlap. So, we calculate the average attention vector, r, for both the input and output vectors using the multilingual BERT model (Devlin et al., 2019). Formal: Here, V is the vocabulary used by the Transformer, and c is the number of languages. Formal: After we've done the attention calculation for both the input and output vectors, we end up with the final attention state vector, which is in R N U. Formal: After the decoder is done with its decoding, we calculate the attention weights, e_t, for both the input and output vectors. Formal: Finally, the final attention state vector is a normalized attention matrix, A_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "So, our model needs to consider how both predicates and variables affect the sentences in the documents. A good approach is to let the predicates guide the words we use and let the variables decide the sentence structure. For instance, the formula card(x) ∧ freecell(y) ∧ empty(y) has three predicates and two variables. The predicates directly influence the words: sentences based on this formula are more likely to include words related to each predicate. So, a sentence like \"Put the cards on the empty freecells\" makes more sense than something like \"Columns are constructed by playing cards in alternating colors.\"",
        "formal_text": "Thus, our generative model must account for the influence of both predicates and variables on the sentences in the documents. A natural choice is to use the predicates to influence the lexical items, while letting the variables determine the syntactic structure. For example, the formula card(x) ∧ freecell(y) ∧ empty(y) contains three predicates and two variables. The predicates influence the lexical items in a direct way: we expect that sentences generated from this formula will include a member of the gloss set for each predicatethe sentence \"Put the cards on the empty freecells\" should be more likely than \"Columns are constructed by playing cards in alternating colors. \"",
        "GPT2_formal_text": ". For instance, the second example could be rewritten as... Formal: We look at the whole dataset, which includes the target entity, its dependency relations, and their confidence scores, plus the embedding for the ground-truth entity's representation. The system figures out the embedding for this representation using the embedding space. Formal: We use a simple greedy algorithm to build the embedding for the ground-truth entity's representation. For each task, we pick an entity type (like person, location, etc.) based on the confidence scores for each relation pair. Formal: The final embedding for the ground-truth entity's representation is built by combining the embedding for its own representation and the embedding for its relation pairs, following the usual method. Formal: The hidden state for the embedding for the ground-truth entity's representation is learned by randomly picking tokens from the embedding space. Formal: The entity type and the confidence scores for each relation pair are set to 1, 2, or 3. Formal: We test different attention methods to see how they work, like random, softmax, and max pooling. Formal: We use a beam size of 100 for the basic model and a beam size of 200 for our fancy models. Formal: We follow the same setup as in section 3.2. Formal: We treat the embedding for the ground-truth entity's representation as the target entity's representation. Formal: We use a standard attention mechanism to figure out the embedding for the ground-truth entity's representation. Formal: We train both the entity type and the relation types for each relation pair using the same training data. Formal: We compare these relation types and entity types to the ground-truth entity's representation, using the same method. Formal: We set the embedding for the ground-truth entity's representation to 1, 2, or 3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D09",
        "filename": "D09-1100.json"
    },
    {
        "casual_text": "Results. Figure 7 shows the same trend as the BLEU score, but with a smaller improvement of 23% when using the adversarial term.",
        "formal_text": "Results. The results in Figure 7 show the same pattern as for the BLEU score, although with a smaller relative gain of 23% when using the adversarial term.",
        "GPT2_formal_text": "_envelope: Formal: These models use a BiLSTM to take in both the image and the surrounding text as input and spit out the hidden representation for the caption. Formal: This paper introduces a model that takes in multiple input features to generate captions. It’s different from the attention-based model in this part. The main difference is that instead of focusing on the image and text features, it uses the image and text features, which are all input features. Formal: We also introduced a new model called attention-based BiLSTM-Attn, which uses an attention mechanism to pull out multi-feature representations from the surrounding text. This model also incorporates multi-task learning, which means it can handle multiple tasks at once. Formal: One of the challenges in creating captions with multiple aspects is dealing with the huge number of possible aspects. While people have looked into abstractive summarization for multiple aspects before (like Qiu et al., 2019), there’s still a lot of research needed to create a single framework that can handle this kind of multi-aspect summarization. Formal: Aspect-level summarization, as proposed by Banerjee and Lavie (2011), uses a hierarchical attention mechanism to group aspects together based on their different attributes and then figures out the most effective way to summarize those grouped aspects. Formal: This paper introduces a new model called Attention-based BiLSTM-Attn, which uses an attention mechanism to pull out multi-feature representations from the surrounding text. It’s different from the attention-based BiLSTM-Attn in this part. Formal: We also introduced a new model called attention-based BiLSTM-Attn, which uses an attention mechanism to pull out multi-feature representations from the surrounding text. This model also incorporates multi-task learning, which means it can handle multiple tasks at once. Formal: Lastly, we compared our model with another approach that combines multiple aspects. Formal: We used two common features from two popular extractive summarization datasets: CNN/Daily Mail (CNNDM) and DailyDialog (DailyDialog). Formal: We tested our model against two others: one called attention-based BiLSTM-Attn and the other called attention-based BiLSTM-Attn + Multi-task Learning. Formal: We trained our model using the validation set of CNNDM and DailyDialog, and the validation set",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.491.json"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. We're talking about a pre-trained word vector model called *look-up pre−trained*, which is fixed and doesn't change. Then, we have two word vectors for the input s: *x t* and *x * t*. These vectors represent the input s, and *h * t* is the current hidden state, while *h * t−1* is the hidden state from the previous step.\n\nFor instance, if we take an input s from the ID training data, we can get these two word vectors. After that, we can feed these vectors into two different hidden layers (RNN and RNN * 3).\n\nNow, here's something interesting: the language models (LMs) we're using are trained from scratch using the ID training data, but they also borrow some knowledge from a different word vector model called GD. So, the hidden state in Equation 6 isn't exactly the \"GD hidden state\"; it uses the word embeddings from the GD model, but the actual data being fed in is still from the ID dataset.\n\nIn Figure 1, you can see the process we're proposing in this paper. The domain-adapted training is explained in Sections 3.1, 3.2, and 3.3.",
        "formal_text": "where look-up * pre−trained is a pre-trained word vector model and static. 2 x * t is the word vector representation of input s, h * t is the hidden state and h * t−1 is the previous hidden state of word s. For example, given input s from the ID training data, we can obtain two word vector representations (x t and x * t ). In addition, the two representations can then be fed into the corresponding hidden layer (RNN and RNN * 3 ). It is also worth mentioning that the proposed LMs are trained from scratch on ID training data, but adapting knowledge from the GD word vector model. Thus, the hidden state in Equation 6is not strictly the \"GD hidden state\"; it uses the word embeddings from the GD pre-trained word vector model, but the inputs are still ID data. 4 Figure 1 shows the adaptation flow proposed in this paper, where the domain-adapted training is presented in Section 3. 1, 3. 2 and 3. 3.",
        "GPT2_formal_text": "' to test the models. The rest of the algorithm follows the same structure as in Eq. 2. Formal: The algorithm checks to see if the extracted relation instances are unique and doesn't bother with adding any extra labels. Formal: Each relation instance gets a random label, but if there are multiple relations, each one gets a unique label. Formal: The count of unique relations (c(0)) is calculated by adding up the counts of all the relations in the dataset. Formal: If there are more than N unique relations, we just throw in an extra probability of 1 to account for the rare ones. Formal: After checking all the relations in the dataset and getting rid of any duplicate ones, we get a probability distribution for the instances. Formal: If two relations have the same instances but are different, we use a formula to figure out the probability of each relation. Formal: Formal: If the entity type of the relation instance is \"person,\" the probability of a relation instance is calculated like this: Formal: In the \"person\" case, the probability of a relation instance is calculated like this: Formal: The probabilities for different relation instances are calculated by adding up the probabilities for all relations in the dataset. Formal: Formal: The model for the relation instance's label, a(0), is figured out based on the dataset's relation instances. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1131.json"
    },
    {
        "casual_text": "In this part, we're sharing the results of our experiments for spotting sentences that describe experiments, pulling out mentions of entities, and identifying specific details about the experiments. For breaking down the text into smaller pieces (tokenization), we used ChemDataExtractor, which is really good at handling chemical formulas and units.",
        "formal_text": "In this section, we present the experimental results for detecting experiment-describing sentences, entity mention extraction and experiment slot identification. For tokenization, we employ ChemDataExtractor, 8 which is optimized for dealing with chemical formulas and unit mentions.",
        "GPT2_formal_text": ", attention_mask_r, and attention_mask_e are just the input, output, and attention parts for the attention mechanism. Formal: Alright, let's break down how the model learns. We'll talk about how it learns representations, how the attention mechanism works, and how the model learns representations for specific features. Formal: So, the model learns to predict the attention for each token in the input. This prediction is based on the attention mechanism's parameters δ(c_i) for the token. Figure 2 shows how the model learns the representation δ(c_i) for a token c_i. Formal: We start with the input X = (x_1, ..., x_n), the output Y = (y_1, ..., y_n), and the attention parameters δ(c_i) for the token c_i. After that, we use this updated representation to predict the attention weights for the token's embedding. Formal: The model gets a prediction for each token c_i using the input X. Then, we calculate the attention weights for the token's embedding using the updated representation δ(c_i). The model then learns the weights δ(c_i) for the token's embedding. Formal: Finally, we use this updated representation to predict the attention weights for the token's embedding. Formal: In this paper, we're looking at an unsupervised method that can learn the representation δ(c_i) for a token c_i using the input X. Formal: Here's the link to the code and model data: https://github.com/johngren/HAN Formal: Also, here's the dataset we used: https://github.com/johngren/HAN Formal: Here's the link to the dataset we used: https://github.com/johngren/HAN Formal: Figure 2 gives an overview of how the model learns representations for each token. Formal: We start with a training set of tokens x_t and a final representation δ(c_i) for that token. Formal: We train the model using the validation set, the validation set again, and the whole dataset we used. Formal: For each token, we evaluate the model on three datasets: Y_t, Y_t+1, and Y_t+n. Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.116.json"
    },
    {
        "casual_text": "For the training data, we grabbed a parallel corpus from the web using our own crawler. This corpus has around 9.5 million sentences and 160 million words on average, with at least 8 million sentences and 140 million words for each language pair. For the development and test data, we picked and manually translated 3,000 and 5,000 sentences, respectively, from other web sources for each language pair. We tweaked all the hyperparameters for each method using the development data, and then did the final evaluation with the test data. \n\nFor word alignment, we used IBM Model 1 (from Brown et al., 1993) and HMM alignment (Vogel et al., 1996) on the best-ranked source sentences and their corresponding target sentences. Based on these alignment results, we built the phrase table, which was shared across all decoding methods. For the English language model, we used a 4-gram model with stupid backoff smoothing (Brants et al., 2007), which was the same for all setups. We decided on each configuration for word alignment and the language model based on some preliminary experiments with the baseline system.\n\nFor the baseline system, we used a standard PBMT system, similar to what Och and Ney (2004) did, but with a lexical reordering model (Zens and Ney, 2006) boosted by a cutting-edge preordering method using bracketing transduction grammar (Nakagawa, 2015). We also used a similar decoding strategy and other basic feature functions as in Moses (Koehn et al.).",
        "formal_text": "For the training data, we used a parallel corpus by mining from the Web using an in-house crawler. The corpus contains 9. 5M sentences and 160M words on average, at least 8. 0M sentences and 140M words for each language pair. For the development/test data, we separately sampled and manually translated 3, 000/5, 000 sentences from other data sources on the Web for each language pair. All hyperparameters for each method are optimized using the development data and final evaluation is performed using the test data. During word alignment, IBM Model 1 (Brown et al. , 1993) and HMM alignment (Vogel et al. , 1996) were performed using one-best preordered source sentences and corresponding target sentences. The phrase table was built according to the alignment results, and shared with all decoding methods. For the English language model, a 4-gram model with stupid backoff smoothing (Brants et al. , 2007) was built and commonly used for all settings. Each configuration of the word alignment and the language model was decided according to the preliminary experiments on the baseline system. For the baseline system, we employed a standard PBMT system, similar to that of (Och and Ney, 2004) with a lexical reordering model (Zens and Ney, 2006) enhanced by a state-of-the-art preordering method based on bracketing transduction grammar (Nakagawa, 2015). We used similar decoding strategy and other basic feature functions to Moses (Koehn et al.",
        "GPT2_formal_text": "= list(c) for i in range(n-1):            Casual: Alright, so for each comment c_i in the collection C, we figure out the value for the attention vector h_i_t by adding up the values of all the attention heads in the comment. Then, we average these values to get the attention score h_i_t. Formal: Each comment is a pair of word embeddings, and they're fed into a softmax layer to predict the attention distribution. Formal: Let’s say the input to the softmax layer is a tweet in the target language (e_t), and the output from the attention head is the attention vector h_i_t. Formal: Similarly, the attention distribution for the predicted comment can be written like this: Formal: Here’s the formula for the attention vector h_i_t for a comment c_i in the collection C: Formal: Basically, this formula is the result of calculating the attention vector h_i_t for a specific comment c_i, multiplied by a set of weights that are adjustable. Formal: Formal: We can see that the attention distribution is defined as h_i_t = h_i * h_i_t + ... Formal: We also calculate a perplexity value for the attention distribution. Formal: Formal: The difference between the predicted attention vector h_i_t and the actual attention vector h_i_t is what we call the loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1134.json"
    },
    {
        "casual_text": "We're focusing on finding how ideas connect in a text, not just how often words repeat. Instead of looking at how words link together, we're looking at how the overall meaning ties everything together. We’re suggesting a way to measure how similar different parts of a text are by considering how words relate to each other across those parts. This method kind of acts like those \"lexical chains\" people talk about, but we’re doing it by averaging how similar a part of the text is to a few parts that came before it. We also handle synonyms by using a mix of different ways to index the text. Our tests on breaking up text into segments show that this approach works way better than the basic method we started with.",
        "formal_text": "Our approach aims at discovering semantic relatedness beyond word repetition. It is based on the notion of semantic cohesion rather than lexical cohesion. We propose to use a similarity metric between segments of text that takes into account semantic associations between words spanning a number of segments. This method approximates lexical chains by averaging the similarity to a number of previous text segments and accounts for synonymy by using a hybrid document indexing scheme. Our text segmentation experiments show a significant performance improvement over the baseline.",
        "GPT2_formal_text": "ing_types is the max pooling operation for the attention masking feature. Similarly, the embedding of the input sentence x is encoded using the learned embedding and also with the attention masking feature. Formal: (10) Formal: The output of the convolutional layer is treated as the contextual representation of the input sentence. Formal: (11) Formal: The output of the attention layer is the contextual representation of the input sentence. Formal: (12) Formal: Basically, if the input sentence x is in a sentence, the output of the attention layer is the hidden representation of the sentence. Formal: (13) Formal: In the third model, the embedding of the input sentence x is created by combining the attention masking feature with the hidden representation of the sentence. Formal: (14) Formal: In the fourth model, the output of the attention layer is the hidden representation of the sentence. Formal: (15) Formal: The word embedding for the input sentence x is created by combining the attention masking feature with the word embedding of the sentence. Formal: (16) Formal: The input sequence x, the embedding for the hidden representation of the sentence x, the word embedding for the sentence x, and the attention masking feature for the sentence x are all combined in a convolutional layer. Formal: (17) Formal: The output from the convolutional layer is the contextual representation of the sentence. Formal: (18) Formal: Formal: This approach is pretty similar to the first model, but instead of using a fixed embedding, the model learns one that fits the context of the sentence. Formal: (19) Formal: Formal: The output from the attention layer is the contextual representation of the sentence. Formal: (20) Formal: Formal: This model uses a self-attention mechanism (like the one by Bahdanau et al., 2015) to learn the embedding for the hidden representation of the sentence. Formal: (21) Formal: The output from the attention layer is the contextual representation of the sentence. Formal: (22) Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D07",
        "filename": "D07-1037.json"
    },
    {
        "casual_text": "A simple way to do this would be to just use noun phrases as they are. For instance, ipadic 1 treats a bunch of long proper nouns as single words, like \" \" (which means Kansai International Airport Company Connecting Bridge). But this method has some issues. For example, if you're searching for \"Kansai International Airport,\" it won't match the single word for the bridge. That's why we need to break it down into smaller parts.",
        "formal_text": "A naïve implementation would be to use noun phrases as they are. In fact, ipadic 1 regards as single words a large number of long proper nouns like \" \" (literally, Kansai International Airport Company Connecting Bridge). However, this approach has various drawbacks. For example, in information retrieval, the query \"Kansai International Airport\" does not match the \"single\" word for the bridge. So we apply segmentation.",
        "GPT2_formal_text": "= (r, t) ∈ R | (s, t) ∈ S | R | r, t = (r, s, t) ∈ R | r, s, t = (r, t, s) ∈ R | (s, t) ∈ S | R | r, t = (r, s, t) ∈ R | r, s, t = (r, t, s) ∈ R | R | Here, p_t and p_s are the outputs for the state s_t and the transition transition between them, respectively. Formal: The key concept behind these neural network setups is that the probability distribution P(h, t) doesn’t depend on the input sequence H, but it does depend on the hidden state h, which can have different hidden states. The goal is to find a way to combine these hidden states to learn the probability distribution P(h, t). Formal: We’ve come up with a way to optimize this, inspired by the Gibbs sampler, and it’s worked really well so far. We’ve also come up with a more efficient way to calculate the soft probability, which we’ll explain in the next section. Formal: In this paper, we’re trying to figure out the probability for a transition from s_t to s_t that leads to s_t. Formal: We also suggest a method to optimize this using the Cross-Entropy loss. Formal: We’ve developed a toolkit that uses a cross-entropy loss to optimize the probability of a transition from s_t to s_t, starting from a model that generates the target sequence. Formal: This toolkit can also be used for more complex tasks involving multiple generations. Formal: We’ve developed a toolkit that uses a cross-entropy loss to optimize the probability of a transition from s_t to s_t, starting from a model that generates the target sequence. Formal: We’ve developed a toolkit that uses a cross-entropy loss to optimize the probability of a transition from s_t to s_t, starting from a model that generates the target sequence. Formal: Formal: We’ve developed a toolkit that uses a cross-entropy loss to optimize the probability of a transition from s_t to s_t, starting from a model that generates",
        "directory": "D11",
        "filename": "D11-1056.json"
    },
    {
        "casual_text": "In this part, we're going to talk about our new method for classifying sentiments in questions and answers. Check out Figure 2 for a quick look at how this method works.",
        "formal_text": "In this section, we introduce the proposed hierarchical matching network approach for QAstyle sentiment classification. Figure 2 depicts the overview of the proposed approach.",
        "GPT2_formal_text": "ed to the input and it's done. Formal: The initial hidden state of the decoder is h, which is made up of four components. Let's call the vector h for the character-level attention layer. The first component, β1, is a multi-head attention matrix with a size of d_k × d_n, which is a matrix with n elements. Here, n is the size of the embedding vector, and d_k is the size of the hidden state, which in this case is d_k = 2. The second component, β2, is another multi-head attention matrix with a size of d_k × d_n, but it also has d_k elements. The third component, β3, is a bi-directional long short-term memory (LSTM) attention matrix with a size of d_k × d_n, and it's also d_k × d_n. The fourth component, β4, is a fully connected layer with a size of d_k. Formal: Now that the sentence embeddings have been pre-trained using the Bi-LSTM decoder, the decoder can use a hidden variable h for the character-level attention. Formal: We use a loss function called L_s_t, which is a linear function, to figure out how well the decoder can capture the meaning of the sentence s. Formal: To create the attention weights for the attention layer, we use the softmax function applied to the hidden state of the Bi-LSTM decoder. Formal: We combine the character-level and attention layer representations in the Bi-LSTM decoder to get the final hidden states h_t and h_t. Formal: To make sure the attention weight for the character-level attention layer is learned to focus on the important characters in the sentence, we apply a non-linear function to the hidden states of the character-level attention layer. Formal: Finally, we calculate the best possible loss L_s to get the final hidden states h_t and h_t. Formal: To train the model, we set the learning rate η to 0.1 and the batch size to 32. Formal: To prevent overfitting, we apply dropout with a probability of 0.2. Formal: We apply a dropout of 0.2 for the character-level attention layer and a dropout of",
        "directory": "D18",
        "filename": "D18-1401.json"
    },
    {
        "casual_text": "The team has a few key roles: the operators (UGV-1, UGV-2, UAV) who actually control the robots, a team leader (TL), and sometimes a mission commander (MC) depending on the mission. The MC is responsible for the whole mission and assigns tasks to the teams. The TL takes those tasks and splits them up among the operators, making sure everyone is on the same page and keeping the MC in the loop if they're around. The operators use the robots to do the tasks they're given and let the TL know how it's going, including any issues that come up.\n\nEveryone on the team uses a shared system that shows a digital map with points of interest (POIs) and where the robots are located. There's also a collection of photos taken by the robot cameras, and since 2017, a task list that the TL can update manually.",
        "formal_text": "The team consists of operators (UGV-1, UGV-2, UAV) who control the robots, a team leader (TL) and in some missions also a mission commander (MC). A MC is in charge of the whole mission and gives tasks to teams. The TL distributes the tasks between the operators, coordinates their actions and reports to the MC (if present). The operators use robots to perform the tasks assigned to them and report to the TL about the results or possible difficulties. The team members use a shared situation awareness interface, consisting of a digital map on which POIs are marked and robots' positions are displayed; a repository of shared photos made with the robot camera; and in 2017 also a task list which the TL can manually edit.",
        "GPT2_formal_text": "= [ (i, j, t) for i, j in ipairs(t) if i is in t], kind = 'SILVER'): g = tanh(W * tanh(α + β + γ + δ)) return W * W + E * W + U * W + O * U + C * W + P * W + Y * U + Z * U. Formal: Since the model is only trained using the full training set, all the parameters are set to zero. Formal: So, θ is just a weight matrix that's not zero. The loss function we use is called L. Formal: The loss function L is defined as: Formal: In real-world scenarios, there's usually not much data to begin with. So, when training, we focus on optimizing a specific goal called θ. Formal: To make sure the model gets better over time, we add some uncertainty to the loss function. Formal: We want the loss function to adjust itself to fit the data distribution of the data points we're trying to optimize. Formal: We don't create any word embeddings, so we don't need to worry about how similar words are to each other. Formal: Instead, we use a model that can adjust itself to fit the data distribution when it's needed. Formal: Formal: We also keep an eye on the loss function as we're training. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "crac",
        "filename": "2020.crac-1.13.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nDiplomacy (DM) is a dataset based on conversation logs from the online text-based game Diplomacy. It was put together by Peskov et al. in 2020. The dataset includes stuff like whether someone is lying or not during the game. For example:\n\n- DM 13, 132: Lies, True\n- MS 9, 676: Liar, True\n- OD 7, 168: Lies, False\n- LIAR 4, 560: Lies, False\n- BOL 502: Lies, True\n- MU3D 320: Lies, True\n- RLT 121: Liar, True\n\nBasically, it’s all about figuring out who’s being honest or not in the game.",
        "formal_text": "• Diplomacy (DM) (Peskov et al. , 2020): conversation logs collected from Diplomacy, an online text-based board game. Statement Context? DM 13, 132 Lies T Yes MS 9, 676 Liar T Yes OD 7, 168 Lies T No LIAR 4, 560 Lies A, T No BOL 502 Lies A Yes MU3D 320 Lies A Yes RLT 121 Liar A Yes",
        "GPT2_formal_text": "ing_c (s, t) = attention_masking_c(s, t) * h_attention_masking_c(s, t) The attention-based sequence labeling model is trained using a maximum likelihood estimation method. This method is based on the work of Xia and others (2015). The training process is shown in Figure 1. The model parameters are learned by optimizing the log probability of the correct label, and then they are adjusted using a scoring function called G_log(Y_t * X). We decided to use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 0.00001 to train this model. Formal: In this paper, we use the conditional probability to model the likelihood. Basically, we start with the predicted label y and the reference label X, and then we estimate the probability P(y | X) using an average of the current prediction y and the predicted label y plus the actual label Y. Formal: Next, we calculate the conditional probability P(x_t | y_t) for the input sequence x_t and the output sequence y_t. Formal: To use the attention-based model, we add the attention-based probability to the current prediction y_t. Formal: To calculate the attention-based probability P(x_t | y_t), we calculate the attention-based probability P(x_t | y_t) using the current prediction y_t plus the predicted label y_t. Formal: The final score y_t is calculated as the sum of the attention-based probabilities P(x_t | y_t) for the input sequence x_t and the output sequence y_t. Formal: We can also calculate the probability P(x_t | y_t) from the output sequence x_t using the output probability y_t. Formal: Formal: In the next section, we'll quickly explain the framework and results from our experiments. Formal: Alright, so here's the deal: Formal: • The conditional probability P(x_t | y_t) is estimated using a maximum-likelihood estimation with a variational inference model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.370.json"
    },
    {
        "casual_text": "Comparing the proposed method to two reranking systems (DL > 0 or DL = 0), the version without distortion (DL = 0) usually gets a better BLEU score than the one with DL > 0. We think this might be because it uses a smarter way of preordering multiple options instead of just doing reordering during decoding based on distortion. These results make it pretty clear that you don’t really need decoding-time reordering if you already have a good preordering setup in place.",
        "formal_text": "In comparison between the proposed method and two reranking systems (DL> 0 or = 0), the proposed method without distortion (DL= 0) often achieves higher BLEU score than DL> 0. We conjecture that the tendencies may come from the use of better preordering among multiple candidates instead of a distortion-wise decoding-time reordering. These results clearly show that the decoding-time reordering is not necessary if better reordering is encoded in a preordering lattice.",
        "GPT2_formal_text": "= [], main = [], output = [], pos = [], neg = [], self = [], p = [], l1 = [], l2 = [], u1 = [], u2 = [], aug = [], tanh = [], g = [], log = [], y = [], l = [], s = [], s1 = [], s2 = [], aw = [], f = [], h = [], wi = [], s = [], ui = [], yi = [], v = [], a = [], n = [], q = [], t = [], qi = [], aj = [], p = [], qj = [], v = [], ajjj = [], q = [], j = [], qj = [], c = [], r = [], j = [], q = [], cj = [], s = [], s1 = [], s2 = [], sj = [],\n\nwhere y[, v] = y[, v] + ui[, yi] + v. Formal: To make sure the dictionary entries are unique, we add a special token with a special code, like \"xx\" for the first time in the input, followed by a space. Formal: We use the context to figure out the type of input, like \"a\" for a aj and \"c\" for cjj. Formal: The first time in the input, we add a special token with a special code, like \"xx\" for the first time in the input, followed by a space. Formal: We also have a dictionary, called D, with a set of tags for each input. Formal: To figure out the type of input, we add a special token with a special code, like \"xx\" for the first time in the input, followed by a space. Formal: Finally, to find the first time in the input, we just add a special token with a special code, like \"xx\" for the first time in the input, followed by a space. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1134.json"
    },
    {
        "casual_text": "Alright, so we're looking at a situation where |S| is greater than 2ε, and there's no chance of misclassifying any training points. Using our definition, F(X) = {f = w, ∆Φ(X ) |w ∈ R d+ }, with ∆Φ being a projection from X to R d, the L1-norm for (f, g f ) is:",
        "formal_text": "provided |S| > 2 ε, and there is no discrete probability on misclassified training points. Based on our definition F (X ) = {f = w, ∆Φ(X ) |w ∈ R d+ } with respect to a given projection ∆Φ: X → R d, the L 1 -norm of (f, g f ) is then given by:",
        "GPT2_formal_text": "ed_tokens = [h_tokens_1, ..., h_tokens_m, ..., h_tokens_n]                Casual: So, h_tokens_i and h_tokens_j are just the attention scores for the tokens i and j. Formal: Here, τ is a special number we use to control how much each token's attention gets shared. Formal: The token attention layer also gets some parameters, which we call h_tokens. For each token w_i in our dataset, h_tokens[w_i] and h_tokens[w_j] are just the attention scores for the token w_i and the token w_j, respectively. Formal: To make sure our token-level attention doesn't get messed up by the surrounding words, we set the token attention layer's parameters to be normalized. Formal: In the PABEE model, when we're looking at the output probability P_t, we take the output from the attention layer as the \"positive\" token and the final output vector for the token. If the token has a hidden state h_t_t, we just add h_t_t to P_t. Formal: The attention layer's parameters, h_tokens, are calculated like this: Formal: In our experiments, we use the method from (Vaswani et al., 2017), which was suggested by (Dhingra et al., 2018). Formal: But, we noticed that even with a simple transformation, the model still struggles to handle tricky cases. Formal: Here, τ is a special number we use to control how much each token's attention gets shared. Formal: The token-level attention layer's parameters, h_tokens, are just the attention scores for the token w_i and the token w_j, respectively. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2009.eamt-smart.9.json"
    },
    {
        "casual_text": "To fix this, we’re tweaking the translation process like this:",
        "formal_text": "As a solution to this issue, we change the translation pipeline as follows:",
        "GPT2_formal_text": "= [id1, id2, ..., idK] for i from 1 to N. We also use a multinomial logistic regression classifier (Lin, 2004) to figure out the probability of each subjectivity. Formal: The language model gives us a probability distribution p(t|s) for the whole text T. The logistic regression classifier gives us the probability p(s|t) for each subjectivity s in T. Formal: Basically, the language model tells us how likely it is that a word s is linked to a subjectivity t. The logistic regression classifier helps us predict the probability for each subjectivity s, which is calculated like this: Formal: The logistic regression classifier gives us the probability p(s|t) for each subjectivity s in T. Formal: Using the logistic regression classifier, we calculate the probability p(s|t) for each subjectivity s in T. Formal: For instance, the logistic regression classifier gives us the probability p(s|t) for the word s in T. Formal: The language model gives us a probability distribution for each subjectivity s in T. Formal: The logistic regression classifier helps us predict the probability of each subjectivity s in T. Formal: Formal: We use the logistic regression classifier to predict the probability of each subjectivity s in T. Formal: We also use a multinomial logistic regression classifier (Lin, 2004) to predict the probability of each subjectivity s in T. Formal: The language model gives us the probability p(s|t) for each subjectivity s in T. Formal: The logistic regression classifier helps us predict the probability of each subjectivity s in T. Formal: Formal: We calculate the probability p(s|t) for each subjectivity s in T. Formal: Formal: For instance, the logistic regression classifier gives us the probability p(s|t) for the word s in T. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2014.eamt-1.37.json"
    },
    {
        "casual_text": "Zhang et al. (2018b) came up with this cool graph RNN called S-LSTM, which looks at how words in a sentence connect to each other. We took that idea and added our own twist by incorporating task-specific stuff, like slots and intents, using something we call collaborative memories. \n\nNow, the S-LSTM already does a good job with local features, but it kind of misses the bigger picture when it comes to the whole sentence's sequence. So, we brought in external BiLSTMs to handle that global sequential information. Our experiments showed that this extra step was super important for both tasks we were working on.",
        "formal_text": "Sentence-State LSTM Zhang et al. 2018b propose a novel graph RNN named S-LSTM, which models sentence between words simultaneously. Inspired by the new perspective of state transition in the S-LSTM, we further extend it with task-specific (i. e. , slots and intents) representations via our collaborative memories. In addition, the global information in S-LSTM is modeled by aggregating the local features with gating mechanisms, which may lose sight of sequential information of the whole sentence. Therefore, We apply external BiLSTMs to supply global sequential features, which is shown highly necessary for both tasks in our experiments.",
        "GPT2_formal_text": "_x, y, t_i=1, t_s=1, . . . , t_n=1) → {1, . . . , D} Formal: The attention weights for the k-th token in the current sequence, which we'll call x_k, are calculated as: Formal: Here, W(k) is the weight matrix, and b_k is the bias for that sequence. Formal: For a sequence S, we calculate the hidden representation for each token x_t by looking at the sentence S, which we'll call h_S. Formal: We pick the token that has the highest attention weight, called h_T, and add h_T to the attention weight matrix, W(S). Formal: The attention weights for the k-th token, x_k, are calculated based on the previous tokens in the sequence. Formal: For a token x_t, the hidden representation for the token is calculated based on the last sequence in the sequence, S. Formal: We pick the token with the highest attention weight, h_T, and add h_T to the attention weight matrix, W(T). Formal: The attention weights for the k-th token, x_k, are calculated based on the next token in the sequence, x_t. Formal: We pick the token with the highest attention weight, h_T, and add h_T to the attention weight matrix, W(T). Formal: So, the final output representation for a token x_t is basically the combination of the last output for x_t and all the previous outputs for the token, S. Formal: Formal: We train a separate classifier, g_T, using the attention weights from the previous token in the sequence, x_t. Formal: Formal: We train a classifier, g_T, using the attention weights from the previous token in the sequence, x_t. Formal: Formal: Finally, we combine the final output vector for the token x_t with the final hidden representation of the whole sequence, h_S. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-1097.json"
    },
    {
        "casual_text": "Okay, let’s break this down in simpler terms. So, there’s a diagram (Figure 4) that shows how predictions work based on Wikipedia data, as mentioned in Table 7. Here’s what the colors mean:\n\n- **Green circles**: These represent the \"facet embeddings\" from something called MFS.\n- **Orange circle**: This is the average of all those green circles (called MFS Avg).\n- **Blue circles**: These are word embeddings that are close to both the green and orange circles.\n\nThe word \"project\" is highlighted because it’s the next word in the actual data we’re looking at.",
        "formal_text": "Wikipedia 2021 Improvement Model S3I9P4 S3I9P4 S3I9P4 S3I9P1 S3I1P1 S3I9P4 S3I9P4 S3I9P4 S3I9P1 S3I1P1 Reference Model S3I9P1 S3I1P1 S1I1P1 S1I9P1 S1I1P1 S3I9P1 S3I1P1 S1I1P1 S1I9P1 S1I1P1 Figure 4: Illustration of the MFS predictions given the Wikipedia context in the second column of Table 7. The green circles mean the facet embeddings from MFS. The orange circle is the average of the facet embeddings (MFS Avg). The blue circles are the word embeddings that are close to the facet embeddings and MFS Avg. The word project is highlighted because it is the next word in our ground truth.",
        "GPT2_formal_text": ", e_i, f_i) = (r_i + 1) divided by the total number of words in the sentence, which is (r_i + M). This process of breaking down the document into smaller parts using a lexicon and then recombining them using some morphological and syntactic analysis is called segmentation (Eisenstein, 1987; Eisenstein, 1989; Eisenstein, 1991). Formal: The point where the order of words in the sentence changes depending on the word in the target word's context changes. Formal: In an NAG setup, this means that for each word in the sentence, you first group the words around it, and then you can rearrange the words around the word to fit the context. Formal: Formal: Another thing about NAG is that it doesn’t limit the number of ways words can be split into. Formal: In Figure 2, you can see an example of how this works with two words: \"nicht\" (not) and \"muss\" (much). Formal: The complexity of NAG can be measured in two ways: (1) By looking at how many word forms (f_i, f_j) and how many ways the words can be split (r_i, r_j), and (2) By looking at how many ways the words can be rearranged (s_i, s_j). Formal: Another way to measure complexity is by counting the number of non-terminals, like 0, 1, 2, or 3. Formal: Here’s a quick summary of some of the related papers: Formal: 1. NAG: A classification problem that deals with multiple meanings of a word (NAG). Formal: 2. NAG: A system that creates an NAG representation using a word’s context (NAG-rec). Formal: 3. NAG-PIPM: A simpler system that uses only the probabilities from the PIPM, ignoring the context (NAG-pipm). Formal: 4. NAG-ILP: A more advanced system that uses both the PIPM and the ILP, combining them. Formal: 5. NAG-SGM: A hybrid system that combines both PIPM and ILP (NAG-sgm). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.554.json"
    },
    {
        "casual_text": "The distributional inclusion hypothesis (DIH), which has been around since the late 90s and has been studied by folks like Dagan, Geffet, and others, basically says that if one word (let's call it u) means something that includes another word (v), then you can swap u for v in a sentence and it still makes sense. For example, \"cat\" includes \"animal,\" so in the sentence \"a cat is asleep,\" you can replace \"cat\" with \"animal\" and get \"an animal is asleep.\" Makes sense, right? But if you try to replace \"cat\" with something it doesn't include, like \"butterfly,\" you end up with \"a butterfly is asleep,\" which doesn't really work.\n\nNow, this idea has its limits. It only really works in situations where the meaning of the sentence goes \"upward,\" like when you're talking about something getting bigger or more general. It doesn't work so well in sentences with negations or words like \"all\" and \"none.\" So, you can't just swap \"cat\" for \"animal\" in sentences like \"all cats are asleep\" or \"a cat is not asleep.\"\n\nEven with these limitations, the DIH has been a pretty big deal in the world of distributional semantics. A lot of research has been done on it, and it's been shown to hold up pretty well in many cases.",
        "formal_text": "The distributional inclusion hypothesis (DIH) (Dagan et al. , 1999; Geffet and Dagan, 2005; Herbelot and Ganesalingam, 2013) is based on the fact that whenever a word u entails a word v, then it makes sense to replace instances of u with v. For example, 'cat' entails 'animal', hence in the sentence 'a cat is asleep', it makes sense to replace 'cat' with 'animal' and obtain 'an animal is asleep'. On the other hand, 'cat' does not entail 'butterfly', and indeed it does not make sense to do a similar substitution and obtain the sentence 'a butterfly is asleep'. This hypothesis has limitations, the main one being that it only makes sense in upward monotone contexts. For instance, the substitution of u for v would not work for sentences that have negations or quantifiers such as 'all' and 'none'. As a result, one cannot replace 'cat' with 'animal' in sentences such as 'all cats are asleep' or 'a cat is not asleep'. Despite this and other limitations, the DIH has been subject to a good amount of study in the distributional semantics community and its predictions have been validated (Geffet and Dagan, 2005; Kotlerman et al. , 2010).",
        "GPT2_formal_text": "ed_values = attention_masked_values + attention_masked_normalized_values + attention_masked_values_normalized. Formal: We'll use the attention masked values for the next phase. These masked values will act as a kind of \"filter\" for our chosen output. Formal: We'll pick the attention values based on how similar they are to the input values. Formal: For the next few steps, we'll tweak the attention values by combining the normalized attention value with the original masked value. Formal: For the final round, we'll use this updated attention value along with the original masked value to get the final output. Formal: Finally, we'll combine all the attention values using this combined output to get the final sentence. Formal: Basically, we're combining the original attention value with the current output value to get the final sentence. Formal: We'll show how this whole process works in Figure 3. Formal: To give you an idea of how the sequence generation process works, we’ll show a demo of how it works with the TREC 2000 dataset in Figure 4. Formal: We’ll also explain how we calculate the average attention values across all the tokens in the input sentences. Formal: To get the attention values for all the tokens in the input sentences, we’ll use the attention mechanism we talked about in Section 2. Formal: We’ll use the same vocabulary as in Section 2.1 to calculate the attention weights for the first time. Formal: With all the attention values for the tokens in the input sentences, the average attention weight for each token in the output sentence is calculated using the attention mechanism we talked about in Section 2. Formal: Formal: Now that we’ve calculated the attention weights for all the tokens in the input sentences, we’ll calculate the average attention values for the words in the input sentence. Formal: After that, we’ll use the average attention values for the words in the input sentence to figure out the average attention value for each token in the output sentence. Formal: Lastly, we’ll calculate the attention values for each token in the output sentence by using the attention mechanism we talked about in Section 2. Formal: We’ll show how this whole process works in Figure 4. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1268.json"
    },
    {
        "casual_text": "Here’s the informal version:\n\nSo, in the paper, they tested some models on the Penn Treebank validation set using F1 scores for binarized trees. The results are like this:\n\n- **Kitaev and Klein (2018)**: Binary - 87.5, N-ary - 84.0, Binary N-ary - 85.9, N-ary Binary - 83.6.\n- **DIORA**: Binary - 86.0, N-ary - 73.9, Binary N-ary - 81.7, N-ary Binary - 69.1.\n- **S-DIORA**: Binary - 89.9, N-ary - 77.5, Binary N-ary - 84.8, N-ary Binary - 73.2.\n\nBasically, DIORA didn’t perform that great due to some weaknesses, but S-DIORA did better, especially in the best setup they found (check Table 2 for more details).",
        "formal_text": "Model Binary N-ary Binary N-ary Kitaev and Klein (2018) 87. 5 84. 0 85. 9 83. 6 DIORA 86. 0 73. 9 81. 7 69. 1 S-DIORA 89. 9 77. 5 84. 8 73. 2 Table 1: Supervised parsing results on the validation set of PTB using parsing F1 with binarized trees. DIORA does not do well because of its inherent weakness, and the best setting from S-DIORA (Table 2) is superior.",
        "GPT2_formal_text": "= [s_i, t_j, p_i, ..., p_k] Formal: We noticed that the model trained on the source domain doesn't perform as well as the one trained on the target domain. To fix this, we suggest using domain adaptation during training to help the model better understand the relationships between the source and target domains. Formal: We found that the model performs better when the target domain has fewer mentions of the source domain. So, we think adding more sentence pairs from the source domain might help the model learn better by focusing on less common sentences. Formal: We noticed that the model's performance on the target domain can vary depending on the context. We think this could be because the context can be changed during training. Formal: To see if this holds true for both single-language and multi-language data, we ran some experiments with both English and Spanish sentences. We chose these sentences to have similar word orders across languages and to mimic the text generation process in the target language. We picked 100 sentences for each language, and we used a sentence pair from the source domain as the source sentences for each sentence pair in the target domain. Formal: The results for single-language data are in Table 3. For multi-language data, we used the same sentences from the source domain as in the target domain to get parallel sentences for the target domains. Formal: We also looked at how the model performed across different language pairs. Formal: The model performed better in single-language data when there was less mention of the source domain, and it performed the same in multi-language data when there was more mention of the target domain. Formal: We found that the model's performance drops off when there are more mentions of the target domain. Formal: For both single-language and multi-language data, we ran the experiments with the same source sentences as in the source domain. Formal: For the single-language data, we only used one sentence from the target domain. Formal: The results for the multi-language data are in Table 4. Formal: In both single-language and multi-language data, the model's performance is pretty much the same. Formal: The results for the single-language data are in Table 5. Formal: The results for the multi-language data are in Table 6. Formal: The results for the single-language data are in Table 7. Formal: The results for the multi-language data",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.392.json"
    },
    {
        "casual_text": "We also did some tests to see how different feature sets work together. Turns out, combining WLS with vectors from the MUSE model works really well. We saw improvements for eight out of thirteen language pairs, with the biggest jump being 5% (for Hi-Or) and the smallest being 1% (for Hi-Ko, Hi-Ml, Hi-Ta, Hi-Te). It's worth mentioning that this combo didn't hurt performance for any language pair, which is why we included it in Table 3. Other combinations, like MUSE + VecMap, MUSE + XLM-R, or MUSE + PVS, didn't do as well and hurt performance for at least one language pair.\n\nWhen we compared our best model (MUSE + WLS) to the strongest baseline (from Kanojia et al., 2019b), we saw an average improvement of 9%, with the biggest improvement being 18% (for Hi-Ta). Compared to the weakest baseline (just WLS), our best model showed an average improvement of 50%, with the highest being 61% (for Hi-Or).",
        "formal_text": "Additionally, we perform an ablation test with our feature sets for further experimentation. We observe that the combination of WLS and vectors from the MUSE model performs even better. An improvement is observed for eight language pairs out of thirteen ranging from 1% point (Hi-Ko, Hi-Ml, Hi-Ta, Hi-Te) to 5% points (Hi-Or). It should be noted that this is the only combination where no degradation in performance was observed for any language pair and hence, is reported in Table 3. Any other combination (MUSE + VecMap, MUSE + XLM-R, MUSE + PVS, and so on) degrades the performance of the cognate detection task, on at least one language pair. The average improvement observed by using our best model (MUSE + WLS) over the strongest baseline approach (Kanojia et al. , 2019b) is 9% points with the highest being 18% points (Hi-Ta). Over the weakest baseline approach (WLS), our best model obtains an average improvement of 50%, peaking at 61% points (Hi-Or).",
        "GPT2_formal_text": "ed_positional_positions, which have higher distortion scores but lower accuracy. These errors are usually caused by mistakes in the attention mechanism (like wrong attention weights) or problems with the pipeline. Formal: We also test with different number of examples in our experiments, like 4, 6, 10, and 24. Formal: We use Logistic Regression (LR) as the classification model. Formal: For sentence classification, we try out a few popular models like BERT, SciBERT, LSTM-based models, and XLM-RoBERTa. Formal: We train the model for 200 epochs, which is about 18 epochs per document. For document classification, we pick the best model based on performance on the validation set, just like we mentioned before. Formal: We tested our model on 6 documents and the results are in Table 1. Formal: The best model by our model is XLM-RoBERTa (Zhang et al., 2020), which is the model trained on the ParaNMT dataset. Formal: The Baseline model is a BiLSTM-based model trained on the ParaNMT dataset. Formal: We also made a bunch of other models based on different types of documents: Formal: We tested the other models we mentioned in this paper and the results are in Table 1. Formal: We also made another dataset for training the model called 'Tokenless Baseline', which is a collection of documents without any tokenization. Formal: For the sentence classification task, we used a few models that are commonly used for tasks like sentiment classification (like BERT, SciBERT, LSTM-based models, XLM-RoBERTa, and XLM-RoBERTa-PM) and sentence classification (like BERT, SciBERT, LSTM-based models, and LSTM-PM). Formal: We also used some models that are commonly used for tasks like sentiment classification (like BERT, SciBERT, LSTM-based models, and XLM-RoBERTa-PM). Formal: We picked a baseline model based on performance on the validation set, just like we mentioned before. Formal: The results for the XLM-RoBERTa model are in Table 2. Formal: The results for the SciBERT model are in Table 3. Formal: For the LSTM-based model, we tested the model with and",
        "directory": "coling",
        "filename": "2020.coling-main.119.json"
    },
    {
        "casual_text": "The dataset has around 50,000 documents in three languages: English, French, and Spanish. Out of these, about 35,000 are labeled with useful snippets, which can be used to train and test models for extractive summarization. We tested how well three different models work on this dataset: LEAD4, TextRank (an unsupervised graph-based model from Mihalcea and Tarau, 2004), and NeuSum (a supervised neural model created by Qingyu et al. in 2018).",
        "formal_text": "The dataset consists of approximately 50K documents in three languages: English, French, and Spanish. Among these documents, approximately 35K are annotated with informative snippets and can be used for the training and evaluation of ex-tractive summarization models. We evaluate the performance of LEAD4, TextRank (Mihalcea and Tarau, 2004) -an unsupervised graph-based model -, and NeuSum (Qingyu et al. , 2018 ) -a recently created supervised neural model in the dataset.",
        "GPT2_formal_text": "= {a_i} in {|t| ≤ k|} and the attention weights are calculated as: Formal: The title and the abstract are the same, and the body is the summary. Formal: • Attention weight: The attention weight for the title is calculated like this: Formal: So, for a title, t_i, the attention weight is calculated like this: Formal: For a summary, t_i, the attention weight is calculated like this: Formal: So, for a title, t_i, the attention weight is calculated like this: Formal: Formal: • Title-specific attention weight: This is calculated as: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eacl",
        "filename": "2021.eacl-main.146.json"
    },
    {
        "casual_text": "In this project, we tested our networks at four different depths: 9, 17, 29, and 49 layers. Here, \"depth\" refers to the total number of convolutional layers. To calculate the depth, we add up the number of blocks with 64, 128, 256, and 512 filters, where each block has two convolutional layers. In Figure 1, for example, there are 2 blocks of each type, so the depth is 2 × (2 + 2 + 2 + 2) = 16. If you include the very first convolutional layer, that brings the total to 17 layers. You can adjust the depth by adding or removing blocks with specific filter sizes. Table 2 shows the best setups we found for depths 9, 17, 29, and 49, along with the number of parameters for all the convolutional layers.",
        "formal_text": "In this work, we have explored four depths for our networks: 9, 17, 29 and 49, which we define as being the number of convolutional layers. The depth of a network is obtained by summing the number of blocks with 64, 128, 256 and 512 filters, with each block containing two convolutional layers. In Figure 1, the network has 2 blocks of each type, resulting in a depth of 2 × (2 + 2 + 2 + 2) = 16. Adding the very first convolutional layer, this sums to a depth of 17 convolutional layers. The depth can thus be increased or decreased by adding or removing convolutional blocks with a certain number of filters. The best configurations we observed for depths 9, 17, 29 and 49 are described in Table 2. We also give the number of parameters of all convolutional layers.",
        "GPT2_formal_text": "ing = 0;\n        Casual: The first two rows in Table 4 show how different attention heads handle different types of input. The first row is about the word embeddings for the input words, and the second row focuses on the representation of the words themselves. The second row shows the performance of the autoencoder based on the encoding of the words. In the first row, the two attention heads both focus on the same word embedding for the word n-gram. But in the second row, the first head does a bit better with the word embeddings, while the second one doesn't improve much compared to the first one. Formal: The first row shows the performance of the autoencoder based on the word embeddings, and the second row is about the representation of the words themselves. Formal: The first row shows the performance of the autoencoder based on the word embeddings, and the second row is about the representation of the words themselves. Formal:  \n\nFigure 2: Figure 2 gives an example of how attention heads work at different levels of detail in the first sentence. Formal: The first sentence is a bit longer than the second sentence. Formal: The first sentence is longer than the second sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E17",
        "filename": "E17-1104.json"
    },
    {
        "casual_text": "We tested a neural network setup using graph convolutional networks for relation extraction and it worked well. We also came up with a method called path-centric pruning to make dependency-based models more reliable by cutting out unnecessary stuff without losing important details. After looking into it closely, we found that our model works really well alongside sequence models, and the pruning trick can be used on other dependency-based models too.",
        "formal_text": "We showed the success of a neural architecture based on a graph convolutional network for relation extraction. We also proposed path-centric pruning to improve the robustness of dependencybased models by removing irrelevant content without ignoring crucial information. We showed through detailed analysis that our model has complementary strengths to sequence models, and that the proposed pruning technique can be effectively applied to other dependency-based models.",
        "GPT2_formal_text": "\" and \"method_ids\" respectively, just like in Equation 2. This setup gives us a matrix in R^(m×n) for the final output. Formal: We use a modified version of the LSDA for cross-lingual transfer learning. Basically, we make a matrix (m×n) with m numbers that represent how similar the source and target languages are. This matrix is created by averaging the pairwise scores (⊥) from all the pairs in the source and target languages. The idea is that the words in the source language should match up with similar words in the target language. So, the similarity matrix for a language X is made by combining the similarity matrix for its word embedding (µ_i) with the similarity matrix for its word embedding (µ_1) (as explained in Section 3.1). Formal: We also have a matrix called ⃗_i, which is a similarity matrix for the language X. Formal: The LSDA is a standard machine learning method (thanks, M. Collins, 1995). It uses a weighted log-likelihood (W) to figure out how likely different word embeddings are to appear with words in the source and target languages. Formal: We start by training a language model called L_w_i using all the training data from the target language. Formal: After training, we use an L2-regularized kernel to get the cross-lingual word embeddings, which we feed into a sentence classifier. The loss function L_w_i helps the language model create cross-lingual embeddings that are closer to the target language's word embeddings. Formal: The final cross-lingual embedding for a language X is the one made by L_w_i. Formal: After that, we add a linear layer (λ) to a fully connected layer (FC) to get a new cross-lingual embedding that is learned from the target language data. Formal: Formal: The cross-lingual embedding for a language X is the cross-lingual embedding of its word embedding, but we add a linear layer (λ) to the FC to get a new embedding that is learned from the target language data. Formal: Formal: Lastly, we use a linear layer (λ) to the FC to get a new embedding that is learned from the source language data. Form",
        "directory": "D18",
        "filename": "D18-1244.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nThe connection between these split-antecedent anaphors and their individual parts is kind of like the opposite of \"element-of.\"",
        "formal_text": "https: //www. phrasedetectives. org/ 3 The relation between the split-antecedent anaphors and the individual antecedents is the inverse of element-of.",
        "GPT2_formal_text": "= [h, f, e, r, t, m]. Formal: Alright, so r is a linear activation function, and we're using a ReLU activation function to make it easier to remember. Formal: With this setup, the value of βt is calculated by: Formal: After you've got the attention weights for the query i, the sequence representation h, and the output embedding e, you can use a simple linear function to create a linear matrix called β. Formal: The attention weights α are figured out using this same linear function. Formal: After you've gotten the attention weights for the query and the sequence, you can also calculate the attention vector for the output embedding e. Formal: Using the attention weights αt, you can get a linear matrix called α, which is a scalar with dimensions [e, t, m]. Formal: If you have a query i and a sequence s, you can calculate the attention weight βt using this equation: Formal: For the query and the sequence you're focusing on, the attention weights αt are calculated by taking the dot product of the embedding vector for the query and the sequence. Formal: Formal: Formal: After that, you can adjust the linear vector βt to get the attention vector for the output embedding e. Formal: Finally, you can plug all this into a pre-trained model to get the final output embedding e. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "coling",
        "filename": "2020.coling-main.538.json"
    },
    {
        "casual_text": "The goal here is to boost the score for the correct tags in the sequence, which we write as s θ (x T 1, ŷ T 1 ). One thing to point out is that the first part and the second part of this new sequence score don’t really depend on each other. This means we can train the BLSTM-based tagging model and the LSTM-based language model separately using cross-entropy.",
        "formal_text": "The training objective is to maximize the sequence score of the true tags, i. e. s θ (x T 1, ŷ T 1 ). We note that the first term and the second term of the new sequence score are independent of each other, eventually leading to cross-entropy training of the BLSTM-based tagging model and the LSTM-based LM separately:",
        "GPT2_formal_text": ", m=1, 2) = {0, 1, 2} h t = arg max f s t dt t || (w 1, w 2 )  \n\nFor the q-th sentence: Formal: In Figure 1, you can see that the whole process of creating a response is shown in Figure 2. For the first question, \"How many?\", we use the topic distribution q t for the representation in the top part. Then, we calculate a topic distribution (d t, s t ) for each sentence s t in the response, which is made up of topic vectors (s t, d t ). Formal: To find the best topic vector, we tweak the topic distribution a bit by using the model distribution c t for the same representations (s t, d t ). Formal: Finally, we combine the topic distributions from the questions into a single topic vector (q t, s t ). Formal: Figure 3 shows how the response is created using the model parameters Φ p (w 1, w 2 ), Φ q (w 1, w 2 ), and Φ s (w 1, w 2 ). Formal: In Figure 4, you can see the topic distributions for each of the responses generated by the models. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.612.json"
    },
    {
        "casual_text": "Alright, so we've got an encoder E and a decoder D. Our job is to take a bunch of inputs {x 1. . . x N } and turn them into outputs {ȳ 1. . . ȳ N }, where N is the number of data points. For instance, in machine translation, each x i is a sentence in the source language, made up of words or tokens, and each y i is the translated version. The decoder D works by taking e i = E(x i ) and a partial translation y i as input, and it builds the full translation y i one token at a time.",
        "formal_text": "Given encoder E and decoder D, our task is to convert inputs {x 1. . . x N } into corresponding outputs {ȳ 1. . . ȳ N }, for data size N. For example, in machine translation, each x i is a source sentence consisting of a sequence of tokens and each y i is a translation. We assume D(e i, y i ) receives e i = E(x i ) and a partial y i as input, constructinḡ y i one token at a time.",
        "GPT2_formal_text": "ing/long_text https: //code. google. com/p/word2vec/ Formal: Let’s focus on words that have a POS tag of n, like i = [i1, ..., in]. We use a set of word embeddings called W = (w_1, ..., w_N) and a lookup table that looks like this: w_i = [w_1, ..., w_k], where k is just the length of the word embedding. The POS tag is based on how often the word appears in a set of sentences where the POS tag is labeled. We start with the POS tag set from the first sentence in the training data and randomly add to it as we train. The hidden size of the POS tag set is d_Pt = d_Pt1, ..., d_PtT, where T is the total number of POS tags in the sentence. Formal: To figure out how the POS tag affects the learning rate for the masked language model (MLM), we used the standard linear regression task (thanks to Wang et al., 2016). We use a linear layer with a linear bias that grabs the hidden variable from the feature space. The model is trained using the softmax function on the hidden variable to get its predictions. Formal: Basically, the goal is to minimize the cross-entropy loss when the hidden variable is aligned with the POS tag. Formal: In our setup, the hidden variable i is aligned with POS tags because there aren’t many POS tags to choose from. Formal: For all these things, we tweak the pre-trained language model M_p_t by taking the average of the regularization parameter λ for the vectors d_Pt and d_Pt1, ..., d_PtT. Formal: This was all explained in section 5.2. Formal: Formal: For all the data, we run each sentence 10 times. Formal: Formal: Lastly, the model is trained using stochastic gradient descent (SGD). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.366.json"
    },
    {
        "casual_text": "Alright, so we've got two vectors, and we calculate the cosine similarity between them. We do the same thing for the Question-language Features. Earlier, we talked about translating the sentence (s 1best) and finding the cosine similarity with the original question (q). We use Equation 2 to get v q and v s 1best. Even though we could translate the sentence into English using four different methods, we only went with the one-best translation because it’s less heavy on the computer. So, in the QL view, we just have one lexical similarity feature (let’s call it LexQL). The whole process for these five lexical similarity features is laid out in Table 1. Once we’ve got all the numbers, we use a maximum-entropy model to figure out the feature weights. Oh, and even though it’s not shown in the figure or table, we also include the same set of features from the sentence right before the answer in the forum post to give a better idea of the bigger picture.",
        "formal_text": "Given the two vectors, we compute the cosine similarity. Same process is repeated for the Question-language Features. As mentioned before, we also obtain a similarity value by translating the sentence (s 1best ) and computing the cosine similarity with the original question (q). v q and v s 1best are computed using Equation 2. Although it is possible to translate the sentence into English using the same four methods, we only used the one-best translation due to the computational cost. Hence, we have only one lexical similarity feature in the QL view (call LexQL). The computation process for the five lexical similarity features is summarized in Table 1. After computation, feature weights are learned via a maximum-entropy model. 7 Although not included in the figure or table, we also include the same set of features from the sentence preceding the answer (within the corresponding forum post), in order to represent the larger discourse.",
        "GPT2_formal_text": "= {t_i}_s = h_t_i) represents the attention scores that the model learns for each mention t_i. This attention score is basically the average of the attention scores from all the mentions in the document. Formal: After training the model using the cross-entropy loss, the attention weights φ_w(i, t_i) can be used directly for training the model or can be adjusted based on the performance of the model. Formal: During training, the model gets a lot of input attention, which is what we call a bottleneck. The goal is to make the model more efficient by reducing the cross-entropy loss as much as possible. Formal: We’ve come up with two ways to make the model faster during training. Formal: We’ve come up with two methods to make the model faster during training: Formal: A method that cuts down the cross-entropy loss during training. Formal: Another method that uses the entire document to create weights for the model. Formal: Both methods focus on reducing the cross-entropy loss. Formal: A method that uses the entire document to calculate weights for the model. Formal: We’ve suggested a new loss function called the coreference loss, which we call the abbreviation for coreference loss. Formal: Using all the document to calculate weights for the model. Formal: We’ve also introduced a new loss function called the abbreviation for coreference loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1055.json"
    },
    {
        "casual_text": "- **Usability**: Making it easy for developers and users from all kinds of backgrounds to use the stuff we’ve built in TSNLP for all sorts of diagnosis and evaluation tasks.\n- **Suitability**: Making sure the system can handle storing and managing natural language test data (like in string processing) and giving really flexible tools to work with.",
        "formal_text": "• usability: to facilitate the application of the methodology, technology, and test; data developed in TSNLP to a wide variety of diagnosis and evaluation purposes for ditferent applications by developers or users with varied backgrounds; • suitability: to meet the specific necessities of storing and maintaining natural language t; est data (e. g. in string 1)recessing) and to provide maximally flexible interfaces;",
        "GPT2_formal_text": "ed is basically the attention weight for the key distribution vector h_s, but it’s not dependent on the context vector h_c. So, we can create a separate attention vector for each token in the input sequence, just like in equation (1). Formal: We’re treating the attention distribution, h_s, as a key-value vector. The key-value combo we use is basically the average of all the attention values for the s-th token in the input sequence. Formal: Using this attention vector, the attention can focus on the most important words, like the ones that are important for the current token. This way, the output from the attention can be tailored to the specific context it’s working with. Formal: Using the attention, we can connect the current token with the most important words in the input sequence. Formal: To make sure the attention isn’t influenced by the context, we’ve come up with a method called residual connection loss (RC). This method helps us create a separate attention vector for each token in the input sequence. Formal: To get the attention vector, we use an LSTM (a type of neural network) to process the input sequence. Formal: After that, we combine the attention vector with the hidden state vector h_s to get the attention vector for the token. Formal: Formal: We use the hidden state vector to represent the attention distribution vector h_s in the target model, which we then use to calculate the attention weight. Formal: We use the attention vector to figure out how important the current token is for the current token. Formal: Lastly, we combine the attention vector with the hidden state vector h_s to get the attention weight, which is calculated as h_s = h_s + RC. Formal: Formal: We use this attention vector to get the attention vector for the token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C96",
        "filename": "C96-2120.json"
    },
    {
        "casual_text": "Clearly, these two tasks are connected because the info you need to answer a new question often comes from threads of similar questions. We’re looking at tackling both tasks together, with the help of another related task: figuring out if a comment in a question-comment thread actually answers the main question.",
        "formal_text": "Obviously, the two subtasks are interrelated as the information needed to answer a new question is usually found in the threads of highly related questions. Here, we focus on jointly solving the two subtasks with the help of yet another related subtask, i. e. , determining whether a comment within a question-comment thread is a good answer to the question heading that thread.",
        "GPT2_formal_text": ". For a given document d, the relevance score l_d is the highest similarity score between the reference summary and the generated summary, given the reference summary's length and document length. Formal: The relevance score l_d is calculated like this: Formal: The relevance score l_d = max(1 - a_d_t) / a_d. Formal: The relevance score l_d = max(1 - a_d_t) / a_d_t. Formal: Similarly, the relevance score l_d = max(1 - a_d_t) / a_d_t. Formal: In Figure 2, you can see an example of a document with the attention distribution. The attention weights α_t_n, which are drawn from the distribution p_n_t, are labeled as h_t_n. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1452.json"
    },
    {
        "casual_text": "Figure 2 shows how the different versions of the DeReKo corpus—DeReKo-2018-I to DeReKo-2019-I—are connected, along with three virtual corpora (let’s call them 1, 2, and 3) and some texts. Corpus 1 was first defined based on DeReKo-2018-I and included two texts: HMP17/FEB. 18387 and AZM18/MAI. 11491. When DeReKo-2018-II came out, another text, GAZ18/JAN. 12539, was added to Corpus 1 because it fit the criteria for that corpus. At the same time, based on DeReKo-2018-II, Corpus 2 was created, and it included GAZ18/JAN. 12539. Later, with DeReKo-2019-I, Corpus 3 was added, and it included AZM18/MAI. 1149. The point here is that texts in DeReKo can be part of multiple corpora, which can make things complicated if you start removing texts.",
        "formal_text": "Figure 2 shows the relationships between the DeReKo corpus releases DeReKo-2018-I -DeReKo-2019-I, three persistent virtual corpora 1, …, 3, respectively initially defined on one of the releases, and three texts. 7 Based on DeReKo-2018-I, 1 was intensionally defined, already containing the texts HMP17/FEB. 18387 and AZM18/MAI. 11491. With DeReKo-2018-II, GAZ18/JAN. 12539 was added to 1 because the text matches the intensional definition of 1. In addition, based on DeReKo-2018-II, 2 was defined, containing the text GAZ18/JAN. 12539. Based on DeReKo-2019-I, then 3 was added, containing AZM18/MAI. 1149. You can see here that texts in DeReKo can belong to many different corpora so that the removal of texts can have complex consequences.",
        "GPT2_formal_text": "_tokens) = {mask_tokens}; h_tokens = (1 + sum from k=1 to k+1 of E_tokens) / (1 + (1 - sum from k=1 to k-1 of E_tokens) * tanh(L_T)). Formal: We can use the same approach to figure out the attention weights for the single-modality video captioning model. Formal: We can use the same approach to figure out the attention weights for the single-modality video captioning model. Formal: Formal: Formal: Using the single-modality video captioning model, we can figure out the attention weights for the attention-based model. Formal: We can use the single-modality video captioning model, which is a version of the attention model, to figure out the attention weights for the attention-based model. Formal: This gives us the attention weights for the attention-based model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "cmlc",
        "filename": "2020.cmlc-1.1.json"
    },
    {
        "casual_text": "In Daxenberger and Gurevych's 2012 paper, they broke down a 21-category system into three main types of edits: text-based (which change meaning), surface (edits that don't change meaning), and Wikipedia policy edits (like VANDALISM and RE-VERT). Within the text-based edits, they grouped things like templates, references (both internal and external links), files, and information. Each of these was then split into three subcategories: insertions (I), deletions (D), and modifications (M). \n\nSurface edits include stuff like paraphrasing, fixing spelling and grammar, moving things around, and markup changes. The markup edits cover anything that tweaks markup elements that don’t fit into the other categories, like adding, deleting, or modifying things. An example would be messing with apostrophes in '''bold text'''. They also added an \"OTHER\" category for edits that couldn’t be labeled due to segmentation issues.\n\nFigure 1 gives an example of an edit from WPEC, labeled as REFERENCE-M. WPEC was made through a manual annotation process with three people working on it. The agreement between the annotators, measured using Krippendorf's α, was .67. The experiments in the study used the gold standard annotations from WPEC, which were decided by a majority vote for each edit.",
        "formal_text": "In Daxenberger and Gurevych (2012), we divide the 21-category taxonomy into text-base (meaningchanging edits), surface (non meaning-changing edits) and Wikipedia policy (VANDALISM and RE-VERT) edits. Among the text-base edits, we include categories for templates, references (internal and external links), files and information, each of which is further divided into an insertion (I), deletion (D) and modification (M) category. Surface edits consist of paraphrases, spelling and grammar corrections, relocations and markup edits. The latter category contains all edits which affect markup elements that are not covered by any of the other categories and is divided into insertions, deletions and modifications. This includes, for example, apostrophes in '''bold text'''. We also suggested an OTHER category, which is intended for edits which cannot be labeled due to segmentation errors. Figure 1 shows an example edit from WPEC, labeled with the REFERENCE- M category. WPEC was created in a manual annotation study with three annotators. The overall interannotator agreement measured as Krippendorf's α is. 67 (Daxenberger and Gurevych, 2012). The experiments in this study are based on the gold standard annotations in WPEC, which have been derived by means of a majority vote for each edit.",
        "GPT2_formal_text": ", which are basically just the final representations of the two words. Formal: We take the output vectors from the teacher model and use them to train the final classifier. Formal: The final classifier uses some general knowledge about the data, which we talked about in section 3.1, to figure out the correct label for the pair of words. Formal: Lastly, we use the final representation of the two words to create a class label for the pair. Formal: This whole process is kind of like what other researchers have done before, like Siegel et al. (2009) and Siegel and Hirschberg (2012). Formal: So, in this paper, we’ve come up with a different way to use a pre-trained transformer model to represent the relationship between words. We also introduced a new way to estimate the posterior probability for each word in the pair, which is important for figuring out the right label. Formal: We tested this on a big set of Chinese text data. The results show that our method works really well. Formal: We also built a model that uses all the knowledge from a pre-trained transformer and can correctly predict the labels. Formal: Formal: For the next part, we’ll share the model we used to predict the labels. Formal: We tested this model on a really big set of Chinese text data and got a precision of 90.8%. Formal: Formal: Lastly, we’ll share the prediction from the model’s predicted label for the pair. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D13",
        "filename": "D13-1055.json"
    },
    {
        "casual_text": "To go along with Table 1, we’ve put the correlations for the best baselines in Figure 2. We looked at what happens when we reduce the number of gold references from 11 down to 1. For all four dimensions and all the baselines, we noticed that fewer references lead to lower correlation and more variance. But QU E S TEV A L doesn’t need any references at all. So, as the number of references drops, QU E S TEV A L does better compared to the other metrics. Plus, QU E S TEV A L can still evaluate systems even when there are no gold references available.",
        "formal_text": "To complement Table 1, we report in Figure 2 the correlations for the best baselines as we progressively decrease the number of available gold references from 11 to 1. For all four dimensions and all the baselines, we observe that less references result in decreased correlation and increased variance. However, QU E S TEV A L does not require any reference. Therefore, the improvement over the other metrics grows larger as the number of references used decreases. Furthermore, QU E S TEV A L enables the evaluation of systems even when no gold reference is available.",
        "GPT2_formal_text": "_hidden(m) = m ht + tanh(tanh(L * m)) A = (1, 1) / (N + O) Formal: We start by training the student network using an embedding layer, and then we apply attention to the hidden parts of the sentences. Formal: We also use attention to the hidden parts of the sentence representations, but this time we focus on the direction of the input attention. For our training, we use the knowledge base (KB) for both the source and target domains. Formal: We take the learned representation of the KB and pass it through a Bi-LSTM network to get the new input representation, which we then use to calculate the attention distribution for the target sentence. Formal: The same approach is used for the student network, and we also add a position-wise attention layer to the student network to adjust its representation. Formal: We tested how well our model works on the English dataset (from Section 2.1) and on the Japanese dataset (from Section 2.2). The results are in Table 6. Formal: We also looked at how it performed on the Chinese dataset (from Section 2.3). Formal: We included the BLEU scores for both datasets, which you can find in Table 7. Formal: The results for each dataset are in Table 8. Formal: We also included the BLEU scores for the supervised learning method (from Section 3.1) to see how well it works. Formal: We also trained a model to predict how similar two sentences are, using the context they share in the source domain. Formal: For our tests, we used the same setup as the baseline models mentioned in Section 4. Formal: For the Chinese dataset, we used the full model, which you can find in Table 9. Formal: We also included the BLEU scores for the supervised learning method (from Section 3.1) to see how well it works. Formal: Formal: The results for each dataset are in Table 10. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.529.json"
    },
    {
        "casual_text": "You can use TexSmart in two ways: either by calling the HTTP API directly or by downloading the offline SDK. Just a heads-up, the results from the HTTP API and the SDK might be a bit different for the same input text. This is because the HTTP API uses a bigger knowledge base and supports more text understanding tasks and algorithms. If you want a detailed comparison between the SDK and the HTTP API, you can check it out here: https://ai.tencent.com/ailab/nlp/texsmart/en/instructions.html.\n\n**Offline Toolkit (SDK)**  \nRight now, the SDK works on Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support is coming in version 0.3.0. The SDK supports a bunch of programming languages, including C, C++, Python (both versions 2 and 3), and Java (version 1.6.0 or higher). If you want to see how to use the SDK with different languages, check out the example codes in the `./examples` sub-folder. For instance, the Python example in `./examples/python/en_nlu_example1.py` shows how to process an English sentence using the TexSmart SDK. And the C++ example in `./examples/c_cpp/src/nlu_cpp_example1.cc` demonstrates how to analyze both an English and a Chinese sentence.\n\n**HTTP API**  \nThe HTTP API for TexSmart has two parts: the text understanding API and the text matching API. The text understanding API can be accessed using HTTP-POST, and the URL is available on the webpage. The text matching API is used to calculate how similar two sentences are.",
        "formal_text": "Two ways are available to use TexSmart: Calling the HTTP API directly, or downloading one version of the offline SDK. Note that for the same input text, the results from the HTTP API and the SDK may be slightly different, because the HTTP API employs a larger knowledge base and supports more text understanding tasks and algorithms. The detailed comparison between the SDK and the HTTP API is available in https: //ai. tencent. com/ ailab/nlp/texsmart/en/instructions. html. Offline Toolkit (SDK) So far the SDK supports Linux, Windows, and Windows Subsystem for Linux (WSL). Mac OS support will be added in v0. 3. 0. Programming languages supported include C, C++, Python (both version 2 and version 3) and Java (version ≥ 1. 6. 0). Example codes for using the SDK with different programming languages are in the. /examples sub-folder. For example, the Python codes in. /examples/python/en_nlu_example1. py show how to use the TexSmart SDK to process an English sentence. The C++ codes in. /examples/c_cpp/src/nlu_cpp_example1. cc show how to use the SDK to analyze both an English sentence and a Chinese sentence. HTTP API The HTTP API of TexSmart contains two parts: the text understanding API and the text matching API. The text understanding API can be accessed via HTTP-POST and the URL is available on the web page. 4 The text matching API is used to calculate the similarity between a pair of sentences.",
        "GPT2_formal_text": "= { uidx, username, passwd, home, group, gidx, email, group. . . } Formal: Okay, so here's the deal: we have this thing called τ, which is part of a bigger thing called Φ. The thing is Φ is made up of a bunch of operators, and τ and τ' are just the nodes in that part of Φ. We'll call τ a and τ' i the operators for the i-th element in τ. Formal: Here's how we define Φ(x i, y i ): Formal: Φ(x i, y i ) = Φ(x, y) and Φ' i, which is made up of Φ(x, y) and Φ' i. Formal: Now, we take all the arguments from the data X and pass them through a nonlinear function called φ(x i, y i ) to get the features for each pair of arguments. This gives us a list of features, like {x i, y i }, where x i is the i-th argument and y i is the i-th argument. Formal: Formal: Finally, we use the output of the linear function φ(x i, y i ) to figure out the predicted argument values. Formal: The whole thing is laid out in Figure 1. Formal: So, Φ(x, y) and Φ' i, where x i is the i-th argument and y i is the i-th argument, are just the nodes in Φ that are part of Φ. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-demo.1.json"
    },
    {
        "casual_text": "A popular and straightforward way to measure distance is using the l1 norm. This method just adds up the differences in brightness between corresponding pixels in two images, assuming they're lined up properly. Luckily, all kanji are designed to fit into the same size square, so aligning them is pretty straightforward—it's just a grid that works for all kanji. If we say p_x(i, j) is the brightness of the pixel at position (i, j) in the kanji x, then we calculate the l1 norm like this:",
        "formal_text": "A common and simple distance metric is the l 1 norm, which simply sums the difference in luminance between pixels of the two images for some alignment. Fortunately, all kanji are intended to occupy an identically sized block, so alignment is via a grid, constant across all kanji. Considering p x (i, j) to be the luminance of the pixel at position (i, j) of rendered kanji x, we evaluate the l 1 norm as follows:",
        "GPT2_formal_text": "ed [1] by using the embeddings from the last layer of the encoder, and we only keep the first n tokens of the sequence as the summary for the output layer. Formal: In this paper, we suggest using an attention mechanism with a hidden state size of 2 to get better representation of a sequence. This way, the attention mechanism helps the model understand the relation between two words, like \"by\" or \"because,\" better. Formal: To make sure the attention mechanism is effective, we use a global pooling layer with a dimension of 8 and train it using cross-entropy loss. We also use linear and nonlinear functions to train our model, following the method from Du et al. (2017). Formal: To get a better idea of how much knowledge the model has, we check the representation of each token using a BiLSTM. Formal: We feed the model the tokens from the second and fourth layers of the encoder, using the embeddings from the last layer of the encoder, to get the representation for the summary. Formal: To get the embedding for the query, we use the token representation of the first and second layers of the encoder, to get the representation for the query. Formal: To check how well the model can understand the relationship between two words, we use attention. Formal: We use a regular CNN to combine the query and the summary, and then we average the results. Formal: Following the approach by Kim et al. (2015), we use a contextual GRU to calculate the attention. Formal: We take the latent vectors of the two words \"by\" and \"because\" and average them to get the representation for the query. Formal: To find the embedding for the query, we use the summary representation from the first and second layers of the encoder, and the input word embedding to get the embedding for the query. Formal: Lastly, we use a linear layer to calculate the attention. Formal: We use a CNN to combine the query and the summary, and then we average the results. Formal: We take the latent vectors of the two words \"by\" and \"because\" and average them to get the representation for the query. Formal: To find the embedding for the query, we use the summary representation from the first and second layers of the encoder, and the input word embedding to get the embedding for the query. Formal",
        "directory": "C08",
        "filename": "C08-1131.json"
    },
    {
        "casual_text": "The process involves picking from a list of thirteen options that look like this: H (modal) [P true/false], where H stands for the person involved and P is some statement or event. The job of the annotators is to choose the option that best shows the meaning of the modal in a given situation. The agreement rates between different annotators are pretty good—0.82 for triggers and 0.76 for targets.",
        "formal_text": "The procedure relies on a list of thirteen choices of the form of H (modal) [P true/false] where H is a holder and P is a proposition or an event. The annotators' task is then to select the best form to represent the modality meaning of a given trigger. Reported kappa κ inter-annotator agreement rates are 0. 82 for triggers and 0. 76 for targets.",
        "GPT2_formal_text": "= [si, t, sm], where si is the i-th token, t is the i-th sentence, and sm is the i-th paragraph. Formal: Each question qi is made up of words w1, w2, ..., wn, and each word wj is made up of characters from a set C. For a given word x, the question is made up of words wj, wi, ..., wk. Formal: The C set for question answering is made up of words w1, w2, ..., wn. Formal: Here, wj is the i-th word in the answer, and wi and wi+1 are the i-th words in the question. Formal: Formal: Each question qi is made up of words w1, w2, ..., wn and each word wj is made up of characters from a set C. Formal: A question qi is made up of words w1, w2, ..., wn and each word wj is made up of characters from a set C. Formal: The C set for question answering is made up of words w1, w2, ..., wn. Formal: Formal: Each question qi is made up of words w1, w2, ..., wn and each word wj is made up of characters from a set C. Formal: Formal: The C set for question answering is made up of words w1, w2, ..., wn. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C14",
        "filename": "C14-1144.json"
    },
    {
        "casual_text": "Sure, there are algorithms with a = 2.3727, but the ones we actually use have a = 2.807 or a = 3 (thanks to Coppersmith and Winograd, 1990).",
        "formal_text": "Algorithms with a = 2. 3727 are known but practical algorithms have a = 2. 807 or a = 3(Coppersmith and Winograd, 1990)",
        "GPT2_formal_text": "ing is a training approach that learns to produce responses by looking at how similar the words in the question and response are. Formal: We've been building a dictionary of possible answers to help a model figure out how to answer the question. We look at all the words in the question and decide if they're similar to the words in the answer. Formal: This means we take a question and compare it to a dictionary of answers. For each word in the question and every answer in the dictionary, we figure out how similar they are. Then, we use that similarity score to predict the answer. Formal: For each word in the question, we compare it to all the answers in the dictionary and see how similar they are, using something called cosine similarity. This gives us the answer score. Formal: This approach is pretty similar to word2vec, which uses a simpler approach, specifically cosine similarity, to measure how similar two words are. Formal: For each answer, we use a cosine similarity score to figure out how similar it is to the words in the question. Formal: We use this similarity score to predict the answer. Formal: We use the similarity score to predict the answer. Formal: This approach is similar to unigram surprisal, which works by trying to find the closest neighbors of a word in a list of possible answers. Formal: If two words appear in the same word list, they’re probably related. Formal: But, if they don’t appear together, they’re not related. Formal: We use the similarity score to predict the answer. Formal: We also use cosine similarity to measure how similar two words are, but this time we’re looking at the closest neighbors in the dictionary. Formal: Formal: The similarity score (or cosine similarity) is basically a way to measure how similar two words are, and we call this the cosine similarity. Formal: Formal: This is also similar to word2vec, which uses cosine similarity to measure how similar two words are. Formal: This is also similar to unigram surprisal, which measures how similar two words are. Formal: Formal: Formal: So, we can use any method to measure similarity, but we pick the one that gives the highest score. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D13",
        "filename": "D13-1179.json"
    },
    {
        "casual_text": "For our next steps, we're thinking of focusing on three main areas. First, we want to work on improving how we collect data. Right now, we're using screenshots and OCR, which can mess things up with lots of non-text stuff and broken-up text due to weird webpage designs. Second, we'd like to get more people involved in our studies. And third, we're curious to see how our word2vec results stack up against newer models like BERT (Devlin et al., 2019). BERT has been doing really well across a bunch of NLP tasks, better than models with static word embeddings. \n\nUsing BERT for language modeling isn't straightforward because of its bi-directional setup and masking, but Salazar et al. (2020) recently figured out how to get prediction values from BERT and similar models trained with masking loss. BERT's subword representations might also help clean up OCR errors, especially when only a few letters are wrong. \n\nThe downside is, with our current corpus sizes of 300/500K tokens, it's not clear if we have enough data to properly handle all of BERT's parameters. One idea is to use a BERT model that's already been trained on a huge corpus and then fine-tune it with our smaller datasets.",
        "formal_text": "For future work, we would like to proceed in three directions. First, we would like to improve the collection procedure: the corpora collected via screenshots and OCR contain a high number of artifacts stemming from non-textual material, as well as non-contiguous texts as a result from complex webpage layouts. Second, we like to increase the number of participants in future studies. Third, it would be interesting to compare our word2vec results with more recent contextual embeddings such as BERT (Devlin et al. , 2019), which have been shown to achieve better performance across a wide range of natural language processing tasks than language models with static word embeddings. While it is nontrivial to use BERT's bi-directional architecture and its masking mechanism for language modelling tasks, Salazar et al. , (2020) have recently shown how to obtain prediction values for BERT and other architectures trained with masking loss. Subword representations as used in BERT may also help to compensate OCR-based errors, when only a few letters have been falsely recognized. On the downside, it is questionable whether the present corpus sizes of 300/500K token are large enough to obtain reliable estimates for the large number of BERT's parameters. A potential solution is to rely on a BERT model pre-trained by a large corpus, and to use the individual corpora to fine-tune the language model.",
        "GPT2_formal_text": ".py -f ctx.txt\n         Casual: Even though it's not mandatory, it's totally cool to have a collection of documents where each document is connected to the ones before it. Formal: Here's how to generate the text by using the input we mentioned earlier. Formal: We can easily add more documents to the collection if there are any, but there's a catch—there's no way to connect the original document to the ones that come after it. Formal: In the next part, we'll explain how we figure out how to link documents to each other in the current system we're working on. Formal: We use the same approach for matching documents as we did for linking documents to each other, but the connection can be more specific. Formal: The pairwise alignment function we're using for matching documents is pretty straightforward. Formal: When we're generating text, we add a little extra term called the \"next document.\" This tells us what the next document should be. Formal: Once the next document is chosen, the whole collection gets updated. Formal: We can also use a maximum matching term (m), which is just the maximum number of matching documents we want to match to any document in the collection. Formal: But, we still need to set a minimum matching term, m, which is the minimum number of documents we want to match to any document in the collection. Formal: Basically, for any collection C, the matching term m is calculated by adding up the matching documents. Formal: Formal: Formal: In the next part, we'll talk about the ranking score (r) and how to handle different ranking orders when calculating a ranking score. Formal: Formal: Formal: In the next part, we'll also look at a couple of other ranking scores to see how they work. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "cogalex",
        "filename": "2020.cogalex-1.1.json"
    },
    {
        "casual_text": "We showed that our approach works well across different languages and test sets, and it even improved how pronouns were translated. While we mainly focused on pronouns, the fine-tuning method we used isn’t limited to just that—it can also fix other types of errors in machine translations, like named entities or rare words. For future projects, we plan to test out how this method can be applied to other kinds of issues too.",
        "formal_text": "We demonstrated the effectiveness of our methods on different languages and testsets, also reporting improved pronoun translations. Although we focus on pronoun translations, our fine-tuning method is generic and can be used to correct other kinds of errors in machine translations, like named entities or other rare words. In future work, we will explore other such applications of our proposed methods.",
        "GPT2_formal_text": "ing is similar to the previous two models, but it adds some nonlinear effects. Specifically, the x-axis shows the average attention distribution, while the y-axis shows the attention distribution after applying the attention masking. In Figure 2, the figure shows the attention distribution after adding the attention masking layer. The y-axis has the attention distribution after applying the masking, and the x-axis shows the original attention distribution after applying the masking layer. The solid lines are the masking layer's output, which usually has a higher value compared to the attention distribution. So, the solid lines act like a filter that focuses on the important parts and removes the rest. Formal: So, the attention distribution after applying the attention masking layer looks like this: Formal: Using the attention distribution after applying the attention masking layer (a_mask), the model can figure out the attention distribution for each token, like this: Formal: Then, the attention distribution after applying the attention masking layer gives us this: Formal: This is a bit more complicated, but the model can still figure out the attention distribution for the rest. Formal: The attention distribution after applying the attention masking layer (b_mask) can be figured out using this formula: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.177.json"
    },
    {
        "casual_text": "In this case, Lake and Baroni (2018) came up with a straightforward experiment to check how well machine translation handles compositionality. Chen et al. (2020) and Li et al. (2019b) also looked into this. They created a new word called \"dax\" and used a simple pattern of sentence pairs in their training data, like \"I am daxy\" and \"je suis daxiste\". But for the test set, they used different patterns. The problem is, their test set only had 8 sentences, which isn’t much. Raunak et al. (2019) noticed a dip in performance when dealing with concatenated source sentences. Fadaee and Monz (2020b) played around with the source sentences by removing adverbs, swapping numbers, adding words that keep the syntax intact (like \"very\"), and changing the gender, and found some weird translation issues. Unlike these studies, we’re using a compound translation error rate to measure how well NMT handles compositionality.",
        "formal_text": "In this respect, Lake and Baroni 2018propose a simple experiment to analyze compositionality in MT, followed by Chen et al. (2020) and Li et al. (2019b). Specifically, they introduce a novel word \"dax\", and their training data contains a single pattern of sentence pairs (e. g. \"I am daxy\", \"je suis daxiste\") while the test set contains different patterns. However, their work is limited in that there are only 8 sentences in the test set. Raunak et al. (2019) observe a performance drop on a dataset of concatenated source sentences. Fadaee and Monz (2020b) modify source sentences by removing adverbs, substituting numbers, inserting words that tend to keep syntax correct (e. g. \"very\"), and changing the gender, and find unexpected changes in the translation. In contrast to these studies, we quantitatively measure compositionality of NMT under compound translation error rate.",
        "GPT2_formal_text": "\" : \"select_q_t\" , \"select_q_s\" : \"select_q_s\" , \"select_r_t\" : \"select_r_s\" , \"select_r_s\" : \"select_r_t\" , \"select_b_t\" : \"select_b_s\" }, \"dw_idx\" : \"select_dw_idx\" , \"dw_s_idx\" : \"select_dw_s_idx\" }, \"dw_t\" : \"select_dw_t\" , \"dw_s_t\" : \"select_dw_s_t\" }, \"cntrl\" : \"cntrl_p\" }, \"cntrl_s\" : \"cntrl_s\" }, \"cntrl_t\" : \"cntrl_t\" }, \"cntrl_s\" : \"cntrl_s\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" } ], \"selected\" : [ \"q_t\" ], \"selected_p\" : \"select_p\" }, ], \"selected_q\" : \"select_q\" }, ], \"selected\" : [ \"q_s_t\" ], \"selected_q\" : \"select_q\" }, \"q_s_t\" : \"select_q\" }, ], \"selected_r\" : \"select_r\" }, ], \"selected\" : [ \"r_t\" ], \"selected_r\" : \"select_r\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select_q\" }, \"q\" : \"select",
        "directory": "acl",
        "filename": "2021.acl-long.368.json"
    },
    {
        "casual_text": "Here’s everything from EFCAMDAT for C1 and C2 combined, focusing on scores above 90%, after removing the ones used to calculate divergence scores. We split the whole dataset into eleven smaller groups: 0-9, 10-19, and so on, up to 90-99, with 100 as its own separate group. This was done for each of the three levels: beginner, intermediate, and advanced. \n\nEFCAMDAT has a lot of documents with high scores, but not as many with lower ones. For some of the lower score ranges, there weren’t 70 documents available, so we just used whatever was there. None of these documents were part of the production corpus used to create the language profile.",
        "formal_text": "This is the whole of EFCAMDAT for C1 and C2 combined for scores above 90% after excluding those that were used for calculating the divergence scores. 8 We divided the whole corpus in eleven truncated ranges: from 0 to 9, 10 to 19 etc. , up to 90-99, and 100 as a separate range, for each of the three classes (beginner, intermediate, and advanced). The EFCAMDAT contains lots of documents evaluated with higher notes, but not so many on the lower side. For some of the lower score ranges, there was not 70 documents, so we used all those available. None of these documents were present in the production corpus that was used for developing the language profile.",
        "GPT2_formal_text": ", which helps us figure out how important each word in a sentence is. This is all based on what Rennie and Niehren (2019) suggested. Formal: To make sure the representations we get for each word are similar, we make sure each token's representation is only a little different from the average of all its tokens. Formal: We use the standard multilingual BERT model (Devlin et al., 2019) and fine-tune it with our own special masked language model. We call the BERT model we train on our data BERT_mask. Formal: We randomly pick 5% of the tokens in the original input sentence to create new tokens. Formal: We use the same setup as before to train the BERT model. Formal: After fine-tuning, we check how well BERT_mask does on the test set using the original input sentence. Formal: Next, we train a BERT model using the masked dataset D = {d_1, ..., d_M}. Formal: We pick a random sentence from the original dataset D = {d_1, ..., d_M} to use as the teacher and calculate the masked language model's prediction P_T for that sentence. Formal: We compare the original sentence to the masked version using the same BERT model to see how well the model can guess the masked label. Formal: After fine-tuning, we check how well the BERT model, P_T, performs on the test set. Formal: Finally, we use the same BERT model as before to predict the future token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C18",
        "filename": "C18-1294.json"
    },
    {
        "casual_text": "So, the model has a pretty high Self-BLEU score, which means it’s more likely to give the same response to different input reviews compared to models trained on more data. On the other hand, it does better in other areas like chrF-tgt, chrF-src, DIST-1, and Uniq., and the responses are also longer.",
        "formal_text": "That said, the relatively high Self-BLEU score indicates that this model tends to generate the same response to different input reviews to a greater extent than those trained on more data. chrF-tgt ↑ chrF-src ↑ DIST-1 ↑ Self-BLEU ↓ Uniq. ↑ Len ↑",
        "GPT2_formal_text": ", which are basically the right answers. Formal: We also made a separate dataset for the final task where we had to pick the right answer from all the ones we got. This dataset, called Question #2, was created by making sure the correct answer was in the top 20 results. Formal: You can check out Table 1 for the average scores and how often each option appeared in each question. Formal: For the BFP-algorithm, we kept everything else the same as the original dataset. We kept the same number of questions (N), the same number of questions (Q), and the same number of sentences (S) in the original dataset. Formal: Lastly, we collected the answers to these questions to see how accurate the human answers were. Formal: We looked at answers from two different people for every question. Formal: We averaged the precision (P), recall (R), and F-score for each question to get the final answers. Formal: We calculated precision (P), recall (R), and F-score for each question, including the answers. Formal: Each word in the question, including the title, was counted separately and then averaged. Formal: We grouped the answers into three different groups based on how they were broken down. Formal: To figure out how much each question affected the answers, we calculated the average F-score for the three answers to see how they lined up with the answers. Formal: Formal: Lastly, we looked at the questions that had the highest average F-score, which gave us the most detailed answers. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "ecnlp",
        "filename": "2022.ecnlp-1.15.json"
    },
    {
        "casual_text": "The NEL performance on the In Media Res corpus is way worse compared to traditional datasets. This wasn't a surprise since adding creative works to the mix introduces a lot of errors. As expected, the tools struggled to tell the difference between a character and the franchise with the same name, and they didn't perform well on the YouTube influencer part of the corpus either. The influencer section is particularly tricky because some of the stuff mentioned there—like YouTube channels that got shut down or early work from famous influencers—are NIL, meaning they’re not in Wikipedia or related knowledge graphs like DBpedia and Wikidata.",
        "formal_text": "NEL performance on the In Media Res corpus is considerably lower than the results obtained on traditional data sets. This was expected due to the large amount of errors introduced by adding creative works to the corpora. Also as expected, the tools were not able to distinguish well between a character and the franchise that bears its name or offer good results on the YouTube influencer partition of the corpus. The influencer partition is especially difficult due to the fact that some of the works mentioned there (e. g. , YouTube channels that were shut down or early gigs for famous influencers) are NIL (i. e. entities that have not been included in Wikipedia or related KGs such as DBpedia and Wikidata).",
        "GPT2_formal_text": "ed_multi_task_layers. Also, the tokenizer has a tokenization module. A good tokenizer is super important for making text look natural and can be trained end-to-end using supervised methods (Zhang et al., 2019a). Formal: We’ve come up with a tokenizer that can work with all the different types of text we’re dealing with. Plus, it’s flexible enough to handle text with different lengths too.\n\nOne thing to keep in mind is that the tokenizer we’re using has a few key components. First, there’s the tag set, which is a set of features that help keep track of what type of text it is. Second, there’s the multi-task learning module, which helps the tokenizer understand the context of the task better and helps it learn better features. Finally, there’s the input layer, which helps pick out important features for each type of text. Formal: We’ve got two versions of the tokenizer: one with a one-hot encoding to avoid noise and another with a multi-task learning module. Both versions have a pretty basic setup: they just use one tokenizer, no fancy stuff. Formal: Our multi-task learning module is trained using the supervised learning method we talked about in Section 3.1. Formal: For the tokenizer to work well, we use an embedding layer with 1024 dimensions to get the features for each token. Formal: The input layer helps pick out the important features for each type of text. Formal: We’ve got two versions of the tokenizer: one with a one-hot encoding to avoid noise and another with a multi-task learning module. Formal: For the tokenizer to work well, we use an embedding layer with 1024 dimensions to get the features for each token. Formal: The input layer helps pick out the important features for each type of text. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "conll",
        "filename": "2020.conll-1.28.json"
    },
    {
        "casual_text": "• If two arguments don’t have any words in common or similar meanings, it’s tough to pair them up. In this case, our ISRG method, which relies on matching words, just doesn’t work. Plus, even a pre-trained model struggles to connect these kinds of argument pairs.  \n\n• Sometimes, our model only picks out a couple of key sentences instead of the whole argument. Other times, it lumps together multiple arguments into one big chunk. This happens because we treat both AM and APE as sentence-level tasks. Since argument boundaries can be all over the place, the model often gets confused and mixes things up.",
        "formal_text": "• It is difficult to extract argument pairs if there are no co-occurring or semantically similar words in two arguments. In this scenario, our proposed ISRG based on co-occurring words cannot provide valid information. Also, it is hard for the pre-trained model to capture the association between such argument pairs. • In some cases, our model identifies only a few important sentences instead of a complete argument. However, in some other cases, multiple consecutive arguments are identified as one argument. The reason is that we frame both AM and APE as sentence-level sequence tagging tasks. For such a task, the boundaries of arguments are often diverse and difficult to determine, so the model often misidentifies them.",
        "GPT2_formal_text": "\": [i, j] = 1 / len(i) for i, j in T if i == j. Formal: We're using a GCN for multi-attention, and we're using the GNN-LSTM setup with the ReLU activation function. We're also setting the maximum window size to 4 and the attention weight decay to 0.1. Formal: In the second stage, we use an ensemble of these different model parameters to create a final representation for the input sequence. Formal: Here, v_i represents the prediction of the i-th token, while v_j is the prediction of the j-th token. Formal: To get the H-feature representation for the token t_i, we calculate the log probability P(t_i | ... | t_j | y_i | ... | y_{i-1}) using a Gibbs sampler (from Schuster and Schütze, 2002). Formal: Here, ρ is the negative log probability. Formal: Finally, we add the H-feature representation to the current sequence representation t_i. Formal: Formal: This final representation is then sent to the decoder (which was trained on the training set). Formal: Formal: We're doing this in a similar way to the first stage, but instead of predicting the i-th token, we predict the j-th token. Formal: Formal: After that, we use the decoder to generate the actual output token. Formal: Formal: Formal: Finally, we do the same for the remaining tokens to get the final representation for the input sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.319.json"
    },
    {
        "casual_text": "For beginners or regular users, there’s a straightforward interface where you can just type in a text query without needing to specify anything else. The results show a list of matches with small images (thumbnails) of the video clips related to your query, along with the text from those clips. Clicking on a match lets you watch the full video clip. If you want more details—like what an advanced user might see—you can click the \"advance information\" link. \n\nThe way the results are sorted is kind of sneaky. It’s based on how much the words in the transcript or the visual labels connect to your query across different media. The more connected they are, the higher up they appear in the list. The goal of the COSMOROE approach is to make all this happen automatically—processing video and audio files to figure out these connections and make searching through digital video archives smarter.",
        "formal_text": "For the novice or general user, a simple interface is provided that allows the user to submit a text query, with no other specifications. The results consist of a hit list with thumbnails of the video-clips related to the query and the corresponding transcribed utterance. Individual hits lead to full viewing of the video clip. Further details on the hit, i. e. information an advanced user would get, are available following the advanceinformation link. The use of semantic relations in multimedia data, in this case, is hidden in the way results are sorted in the results list. The sorting follows a highly to less informative pattern relying on whether the transcript words or visual labels matched to the query participate in cross-media relations or not, and in which relation. Automating the processing of audiovisual files for the extraction of cross-media semantics, in order to get this type of \"intelligence\" in search and retrieval within digital video archives, is the ultimate objective of the COSMOROE approach.",
        "GPT2_formal_text": "ing, and attention_attention_recall. The final attention representation for the answer s_a_i is then passed into a sigmoid layer to get its attention weight vector a_i. Formal: To tackle this, we add a second hidden layer to the target model to handle the more complex context of the answer span s_a. This helps us not only learn the relevant information but also create an answer span s_a_i_c, which is kind of like the answer span itself. Formal: So, the final attention vector is a_i_c. This approach is all about transferring knowledge from the target model to the source model. We also came up with a sigmoid function to focus on the important parts of the answer. Formal: After that, we calculate the final attention vector using this updated target model. The results look like this: Formal: We ran experiments using a small Chinese dataset, specifically the Baidu Tieba dataset. Formal: Check out Figure 2 for the details. It shows an example of how a model learns by looking at the answers in both the source and target models. Formal: At each step t, the model calculates the attention matrix for the source and target questions. Formal: To make sure the attention for the correct answer stays positive, we add a sigmoid activation to the source model's hidden layer to make sure it keeps an eye on the answer span s_a_i_c and also pays attention to the answer span s_a_i. Formal: If the answer span s_a_i_c is correct, the sigmoid activation will act as a kind of \"focus\" for the answer span s_a_i_c. Formal: The sigmoid activation is calculated by adding the sigmoid of the attention vector a_i_c multiplied by the sigmoid of the attention vector a_i_c for the answer span s_a_i_c. Formal: Finally, we pass this updated attention vector through an attention layer to get the final attention vector a_i_c. Formal: Formal: You can find the full code and model details for the multi-answer sequence classification task in the open-source code we released. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E09",
        "filename": "E09-2014.json"
    },
    {
        "casual_text": "So, let's look at a couple of sentences, S and T, from a parallel text. S is the source sentence with I source words, and T is the target sentence with J target words.",
        "formal_text": "To this end, we consider a pair of sentences (S, T ) from the parallel corpus, where the source sentence S is made up of I source words and the target sentence T is made up of J target words:",
        "GPT2_formal_text": "= {s1, ..., sm}. If no query is found, it just turns into a list of tuples like (s1, ..., sm) with the same keys and values.\n\nThe query is basically the sentence that's most likely to match with the paragraph we're looking at. The length of the query doesn't really matter in this case. In the first example, the query is \"how much,\" and the paragraph is \"how much money.\" Formal: In this example, the query is \"how much money,\" and the paragraph is \"how much money.\" Formal: The length of the query and the paragraph are both set to 10. Formal: The length of the query and the paragraph is also set to 10. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2012.eamt-1.62.json"
    },
    {
        "casual_text": "To get a grasp on how global inheritance works in DATR, we need to talk about the idea of global context. Let's say we're trying to figure out the value of \"Dog: (sing)\" in a DATR theory. At first, the global context is just the pair (Dog, sing). From the theory, we know that \"Dog: (sing)\" gets its value locally from \"Noun: (sing)\", which in turn gets its value globally from the path \"(root)\". To figure out what \"(root)\" means, we look at the global context to find the current global node, which is \"Dog\". Then, we evaluate \"Dog: (root)\" to get \"dog\", which is what we needed.",
        "formal_text": "To understand the way in which global inheritance works, it is necessary to introduce DATR's notion of global contea't. Suppose that we wish to determine the value of Dog: (sing) in the exalnt)le DATRc; theory. Initially, the global context will be the pair (Dog, sing), bS\"om tile theory, the value of Dog: (sing} is to be inherited (locally) fl'om Noun: (sing), which ill turn inherits its value (globally) from the quoted path \"(root)\". rio evaluate the quoted path, the global context is examined to find the current global node (this is Dog) and the vahle of \"(root)\" is then obtained by evaluating Dog: (root), which yields dog as required.",
        "GPT2_formal_text": "ed (E b, E c ) (E b = {a i } | E c = {a j } ) = { {a i } e a j }. The goal is to guess the label for E b by using the attention for E c. Formal: In our experiments, we set the number of tokens to 10. The total number of possible labels is also 10. To keep things simple, we use a simple linear-chain CRF (check out the top part of Figure 1) that's trained with the training data D. Formal: Basically, the goal of this model is to find the label for a sentence based on the token distribution and the attention for that specific token. Formal: Formal: We use a two-layer BiLSTM to process both the sequence of tokens and the attention distribution in the sequence of attention blocks. Formal: This approach works because the attention blocks are basically weighted sequences. Formal: We train the model using cross-entropy loss, which is calculated based on the token and attention distribution. Formal: We also use a CNN to predict the attention distribution for the token representation x c, which is x i = {a i } e a j. Formal: Finally, we use a post-processing loss function to smooth out the attention. Formal: We calculate the cross-entropy loss with the target sequence and attention distribution using a set of development set data. Formal: We use two input words, the token sequence and the attention distribution, as inputs for the model. Formal: The model learns to create a sentence token representation by combining the token and attention distribution. Formal: The model learns to represent the token by combining the word and attention distribution. Formal: We use a CNN to predict the attention distribution for the token representation x c, which is x i = {a i } e a j. Formal: We use a CNN to predict the attention distribution for the token representation x c, which is x i = {a i } e a j. Formal: We also use a post-processing loss function to smooth out the attention. Formal: The model learns to represent the token by combining the word and attention distribution. Formal: Formal: Finally, we use a post-processing loss function to smooth out the attention. Formal: We use a CNN to predict the attention distribution for the token representation x c, which is x i = {a i } e a j. Form",
        "directory": "C96",
        "filename": "C96-2109.json"
    },
    {
        "casual_text": "Check out Table 9 for the results. Basically, the multi-embedding methods keep beating the single-embedding ones when it comes to training, validation, and testing perplexity. The difference isn't as big as with similar words, though. Also, the gap gets wider when the nouns are really different. We think that when the word embeddings for nouns are farther apart, the next word distribution tends to be more complex, and multiple embeddings do a better job of capturing that.",
        "formal_text": "The results are presented in Table 9. In terms of the training, validation, and testing perplexity, multi-embedding approaches consistently outperform the single-embedding baselines, though the margins are smaller than those from the analogous words. Moreover, the improvement gap is larger when the nouns are dissimilar. We hypothesize that as the word embeddings of nouns become further away from each other, the next word distribution is more likely to be multi-mode and thus could be better captured by multiple embeddings.",
        "GPT2_formal_text": "= { id_t, id_m, ..., id_p }, ..., { id_t } is just a list of tuples where the key and value are the same, and each tuple has a tuple (v_k, e_k, a_k). Formal: In this part, we’ll talk about how we’ve figured out how to use the DATR theory to handle the whole setup of a standard DATR program. We’ll use some of these theories to build a grammar for DATR. We’ll also explain how to make sure the DATR lexical entry is always unique. Formal: We’ve come up with a way to handle both the theory and the actual implementation. Formal: We’ve also got a grammar that uses these theories, and we’ve built a grammar that actually works. Formal: We’ve tested this grammar on two real-world text collections, and it’s already got some useful features. Formal: We’ve shown that even when the DATR lexical entry isn’t unique, the DATR lexical entry can still be used to build a DATR program. Formal: In this part, we’ll explain how we use the DATR theory to handle the whole DATR lexical entry setup. Formal: We’ll also talk about how to make sure the DATR lexical entry is unique. Formal: We’ll also show that even when the DATR lexical entry isn’t unique, the DATR lexical entry can still be used to build a DATR program. Formal: Formal: We’ve also tested this grammar on two real-world text collections, and it’s already got some useful features. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.554.json"
    },
    {
        "casual_text": "On the essay datasets, Target Embedding (learning) works really well in the optimistic scenario, especially for most scores. The hybrid approach doesn’t really show much improvement, though. This might be because the dataset is pretty small, so it’s hard to tell if there’s any real difference. In the pessimistic scenario, Premise Target (ranking) seems to be a better fit. The lower scores for Essay Conclusions could be due to having fewer premises (check out Figure 6), which makes it harder to pick a good conclusion target from the available premise targets. Figure 7 shows three examples of premise targets from the datasets, along with the actual conclusion target and the ones our approaches came up with.",
        "formal_text": "In the optimistic scenario on the essay datasets, Target Embedding (learning) is strongest for most scores. The hybrid approach hardly achieves any improvement. Due to the small dataset size, no significance was found, though. In the pessimistic scenario, Premise Target (ranking) seems more suitable. The lower scores on Essay Conclusions can be attributed to the low number of premises (see Figure 6 ), which makes finding an adequate conclusion target among the premise targets less likely. Figure 7: Three examples of premise targets from the datasets, the associated ground-truth conclusion target, and the conclusion targets inferred by our approaches.",
        "GPT2_formal_text": "ing, which we call M_i. Formal: The main idea behind attention mechanisms is to figure out how important each token in a sequence is. We call this importance weight τ_i. Formal: A key part of our attention mechanism is the attention decoder, which takes in a sequence of word embeddings x_i as its input. Formal: We use a bi-directional GRU (BiGRU) model (from Vaswani et al., 2017) to figure out the attention weights τ_i. Formal: Next, we calculate the attention weights, let's call them h_i, for each token x_i by doing a max pooling layer. Formal: This means we take the output h_i and average the attention weights, h_i, for each token x_i. Formal: The final attention weight for the token x_i is calculated like this: Formal: The final attention weight for the token x_i is calculated like this: Formal: To calculate the output hidden representation h_i, we use a BiLSTM (Hochreiter and Schmidhuber, 1997) to get the output representation h_i. Formal: Finally, we combine the output h_i with the output hidden representation h_i to get the final output. Formal: In Section 4.2, we show how we train our model using this framework. Formal: The main idea behind summarization models is to summarize a document or article by focusing on the most important parts. We use a sequence-level BiLSTM (Vaswani et al., 2017) with a BiGRU (BiGRU) as our model. Formal: To improve the summaries, we use an attention-based multi-head attention mechanism (Hochreiter and Schmidhuber, 1997) to make the summarization process better. Formal: In Section 4.3, we dive into the specific details of our model. Formal: The results from our experiments are in Table 5. Formal: Our model outperforms a few other models in summarization tasks, including a sentence-level BiLSTM model, a document-level BiLSTM model, and a graph-based BiLSTM model. Formal: Table 6 shows the average scores for each model and how they compare to a baseline model that only uses the document-level BiGRU network. Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.399.json"
    },
    {
        "casual_text": "InLäubli et al. (2018) used the Sign test instead of the Wilcoxon rank sum, and it turned out to have similar statistical power for the effect size they were looking at. On the other hand, Toral et al. (2018) took a different approach—they only had document context for the source document, not for the MT output.",
        "formal_text": "InLäubli et al. (2018) the Sign test was used as opposed to Wilcoxon rank sum and has similar statistical power for such an effect size. 3 This approach is not that ofToral et al. (2018), where document context was only available in for the source input document as opposed to MT output document.",
        "GPT2_formal_text": "= {h, t} j=1:n for n in range(T): for q in t, {u, v} j=1:n:\n                Casual: As shown in Figure 4, our model does way better than other methods in terms of calculating F1-scores. It does a great job of matching the reference data, which is what we're aiming for. Our model not only performs better than other methods but also does a better job of matching the reference datasets. Formal: With these F1-scores, our model can automatically spot new sources of noise and fix them, all thanks to the complexity of the target text. Formal: Our model can find new sources of noise in the source text, fix them, and even boost the reference F1-score. Formal: Formal: The final results show that our method is pretty versatile. It can find new noise sources and fix them, and even boost the reference F1-score. Formal: Formal: Check out the high F1-score results from our model in Figure 5. Formal: Figure 6 shows the F1-score improvement for the phrase-based model from the WMT2015 shared task. Formal: Formal: We also noticed that our model does a better job of spotting new noise sources compared to the phrase-based model from WMT2015. Formal: Formal: Formal: Figure 7: An example showing how our model looks for noise. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.6.json"
    },
    {
        "casual_text": "These issues come up because LDA’s math doesn’t really focus on capturing how words relate to each other in terms of meaning. A cool next step could be tweaking the parts of the model that use probabilities, training it on a good dataset, and creating a vector-based model that’s tailored specifically for combining words into meaningful phrases. We’re also thinking about exploring more advanced models that consider the structure of sentences. Our experiments so far, where we mixed these models with a parser, show that blending syntax and semantics gives us a lot of potential for improvement.",
        "formal_text": "These shortcomings arise from the mathematical formulation of LDA, which is not directed at modeling the semantic interaction between words. An interesting future direction would be to optimize the vector components of the probabilistic model over a suitable training corpus, in order to derive a vector model of semantics adapted specifically to the task of composition. We also plan to investigate more sophisticated composition models that take syntactic structure into account. Our results on interpolating the compositional models with a parser indicate that there is substantial mileage to be gained by combining syntactic and semantic dependencies.",
        "GPT2_formal_text": "ing) with a stack, and then there's the input, like x, that goes through an output layer and the output layer's attention layer. This whole setup is called the multi-head attention network (MWAN). We’ve got two versions of this network: one that deals with multi-head attention (MWAN-Multi) and another that focuses on attention only on the token that came before the current input token (MWAN-Attn). Formal: Basically, the multi-head attention network takes the input sequence x = (x1, ..., xn) and turns it into an attention vector, p ∈ R n×d. Then, it sends this vector through an output layer and a pre-attention layer. Formal: In the multi-head attention network, the token that comes before the current input token is seen as the target entity, y j. And the token that comes after the current input token is treated as the context for the current input token, z j. Formal: Lastly, the output layer takes the attention vector p ∈ R n×d and passes it through an output layer. Formal: We’re using a pre-trained BERT-Base model (Devlin et al., 2019) with a hidden size of 256 and a hidden layer dimension of 512. We’re fine-tuning it on a dataset called CoNLL-2019, which is a dataset with over 200,000 samples. Formal: We’re also using the word2vec model (Mikolov et al., 2013) that has a dimension of 300 for the input vector p. Formal: The main goal of the attention-based model is to create a representation that includes both the context (y j ) and the token that comes before the current input token (z j ). This representation is fed into an output layer that processes it. Formal: For the example in Figure 2, the attention network for the multi-head attention network helps in finding the entity mention \"John Smith\" that is right before the current input token. Formal: The output layer calculates the attention weight vector for the token that came before the current input token, which helps the model focus on the entity mention \"John Smith.\" Formal: For the sentence \"The woman was ...\", the attention network (MWAN-Attn) focuses on the token that comes after the current input token (z j+1), the entity mention \"woman\", and",
        "directory": "D09",
        "filename": "D09-1045.json"
    },
    {
        "casual_text": "In terms of how much work it takes to get this done, labeling counterfactuals is basically the same as labeling regular examples. So, no extra training for annotators or separate systems are needed. On the other hand, Kaushik et al. (2020) had to set up two different crowdsourcing tasks to create and label counterfactuals. Plus, it’s way less effort for annotators to just evaluate examples instead of making them. Kaushik et al. (2020) say it took about 2 minutes per counterfactual for NLI tasks before checking the quality, but for us, it was only 10 seconds per counterfactual on average. Even after we cleaned up the data (kicked out bad annotators and removed unclear counterfactuals), our time per NLI counterfactual was around 36 seconds (this is what we used in Table 5).",
        "formal_text": "terms of implementation effort, the process of just labeling counterfactuals is the same as labeling original examples, such that no additional annotator training or separate pipelines are required; in contrast, Kaushik et al. (2020) set up two separate crowdsourcing tasks for creating and labeling the counterfactuals. Further, annotator effort is much lower, as evaluating examples is easier than creating them - Kaushik et al. (2020) report an average of ≈2 minutes per NLI counterfactual prior to quality validation, while our median time was 10 seconds per counterfactual. Even after our quality validation (removing noisy annotators, disregarding non-fluent counterfactuals), our rate for NLI is ≈36 seconds per counterfactual (used in Table 5 ).",
        "GPT2_formal_text": "= [] for e in xrange(m): if e.type == 'open': break elif e.type == 'close': return True else: return False return False def process_urllib2_parse(self, path): #**Open**: if self.type == 'open' or self.type == 'close': return self.node(path).get_value(0) elif self.type == 'open' or self.type == 'close': return self.node(path).get_value(1) else: return False return False def process_urllib2_parse(self, path): #**Close**: if self.type == 'close' or self.type == 'open': return False elif self.type == 'open' or self.type == 'close': return True return False def process_urllib2_parse(self, path): #**Open**: return True if self.type == 'open' or self.type == 'close': return False return False def set_ifnot(self, **) if **'*'** == self.type: self.type = **'*'** self.value = **'*'** return True else: return False def set_key(self, **) if **'*'** is not in self.type: self.type = **'*'** self.value = **'*'** return True else: return False def set_value(self, **) if **'*'** is not in self.type: self.type = **'*'** self.value = **'*'** return True else: return False def set_nil(self, **) if **'*'** is not in self.type: self.type = **'*'** self.value = **'*'** return True else: return False def set_key(self, **) if **'*'** is not in self.type: self.type = **'*'** self.value = **'*'** return True else: return False def set_value(self, **) if **'*'** is not in self.type: self.type = **'*'** self.value = **'*'** return True else: return False def get_ifnot(self, **) return self.type == 'open'",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "We tried out a few different versions of our model as a starting point: BERT-SeqWD, which only looks at coreference scores based on entity or event representations without any cross-document linking, and BERT-SeqXdoc, which calculates coreference scores across documents but doesn’t use candidate composition. So, BERT-SeqXdoc just compares the query mention to all antecedent mentions from previous documents, instead of comparing it to clusters created with candidate composition. We also tested our model, SeqXdoc+IC, with and without adaptive pre-training for both event and entity coreference. For entity coreference, we compared our model against a few others:",
        "formal_text": "We experiment with the following baseline variations of our model: BERT-SeqWD -computes coreference scores using only the entity (or event) representations, without any cross-document linking, and BERT-SeqXdoc -computes coreference scores across documents but without candidate composition. This means the baseline BERT-SeqXdoc computes scores between the query mention and all antecedent mentions across all prior documents, rather than between the query and the clusters computed with candidate composition. For both event and entity coreference we experiment with our model, SeqXdoc+IC with (+Adapt) and without adaptive pre-training. For entity coreference we compare against the following models:",
        "GPT2_formal_text": "ing, with d set to 100. We picked a fixed-size response size to avoid any issues with overfitting. We found that the regularization term λ was about the same as the parameters in the RoBERTa model (Conneau et al., 2020), so we just stuck with that. Formal: We trained all the models using a batch size of 64. For SCLM and SDA, we set the learning rate to 0.01 and the batch size to 20. For RoBERTa, we set the learning rate to 0.00002, and the batch size to 1024. For PLMs, we trained them on the same data as the BERT model, but we trained them for 50 epochs. Formal: We ran the experiments 10 times and averaged the results. Formal: For the final classification layer, we used a linear layer with 12 hidden units. The number of nodes in the final layer in PLMs is fixed. Formal: We trained all the models using the training split shown in Figure 2, with d set to 100. We trained the PLMs for 50 epochs on the validation set and checked their performance on the test set after that. Formal: We tested the models using the validation split shown in Figure 2. Formal: We trained all the models on the training split. Formal: We used Adam (Kingma and Ba, 2014) with the learning rate set to 0.00002 and the batch size to 10. Formal: We used the development split. Formal: To keep the training balanced across different types of entities, we used the log-likelihood ratio λ to create the final prediction distribution. Formal: We tested the models using the validation split. Formal: We trained the models using the training split. Formal: We also used the validation split. Formal: We trained all the models on the training split. Formal: We used the development split. Formal: We trained all the models on the training split. Formal: We trained all the models on the training split. Formal: We used the validation split. Formal: We trained all the models on the validation split. Formal: We trained all the models on the validation split. Formal: We also used the development split. Formal: We trained all the models on the validation split. Formal: We used the validation split. Formal: We trained all the models on the validation split. Formal: We used the development",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.382.json"
    },
    {
        "casual_text": "EmotionPush 1: This dataset includes conversations pulled from Facebook Messenger chats, with all the private stuff taken out. The emotion categories are the same as in the Friends dataset.",
        "formal_text": "EmotionPush 1: The conversations in this dataset are from the facebook messenger logs after processing the private information. Emotion categories are the same as Friends dataset.",
        "GPT2_formal_text": "= {q, i, j} = (r, f, g, t, s) for all i, j in the target set. Formal: For the 'RNNLM+NNLM' model, we use the full RNNLM model to learn the target vectors, and we keep the source vectors the same as the 'RNNLM' model. Formal: So, how do we figure out the target vectors for a given input x? Here's what we do: Formal: We take the input x and the output vectors created by the RNNLM model, then mix them up to create new vectors for the input x. Formal: We use a beam search method to train the RNNLM model on this mix of the target vectors and the source vectors, trying to minimize the cross-entropy loss. Formal: Also, we pick the most important source embedding x_i, j and the most important target embedding x_j for our input x. Formal: For each source embedding x_i, j, we pick a target embedding x_j from the target set. Formal: We use the cross-entropy loss to check how well we're doing at predicting the target embedding x_j. Formal: We check the cross-entropy loss for each target embedding x_j and source embedding x_i, j. Formal: Finally, we use the log loss to calculate the cross-entropy loss for each source embedding x_i, j, and target embedding x_j. Formal: Formal: We use the beam search method on the source embedding x_i, j, and the target embedding x_j to train the model on this mix of the source and target vectors. Formal: Formal: We look at the target embedding x_j and the source embedding x_i, j for each source embedding x_i, j. Formal: Finally, we calculate the cross-entropy loss for each source embedding x_i, j, and target embedding x_j. Formal: Formal: We train the model using the cross-entropy loss, using the cross-entropy loss for each target embedding x_j, and the log loss for each source embedding x_i, j. Formal: Formal: We compare the source and target embedding vectors for each source embedding x_",
        "directory": "ccl",
        "filename": "2020.ccl-1.102.json"
    },
    {
        "casual_text": "We used GraphChi, a cool library for processing graphs on a single machine (shoutout to Kyrola et al. in 2012), to handle both the feature selection and feature computation parts of PRA. For the logistic regression, we went with MAL-LET's setup, including both L1 and L2 regularization (props to McCallum in 2002). To figure out what counts as negative evidence, we made a closed world assumption, meaning any (source, target) pair we found during feature computation that wasn’t labeled as positive was treated as negative.\n\nWe tweaked the parameters for our methods by doing a rough, manual grid search with cross-validation on the training data we’ll talk about later. The parameters we messed with included the L1 and L2 regularization settings, how many random walks we did for feature selection and computation in PRA, and some spikiness and restart parameters for vector space walks. Turns out, the results didn’t change much even if we fiddled with these parameters a bit.",
        "formal_text": "We perform both the feature selection step and the feature computation step of PRA using GraphChi, an efficient single-machine graph processing library (Kyrola et al. , 2012). We use MAL-LET's implementation of logistic regression, with both L1 and L2 regularization (McCallum, 2002). To obtain negative evidence, we used a closed world assumption, treating any (source, target) pair found during the feature computation step as a negative example if it was not given as a positive example. We tuned the parameters to our methods using a coarse, manual grid search with cross validation on the training data described below. The parameters we tuned were the L1 and L2 regularization parameters, how many random walks to perform in the feature selection and computation steps of PRA, and spikiness and restart parameters for vector space walks. The results presented were not very sensitive to changes in these parameters.",
        "GPT2_formal_text": "ing_t1 = [mask] [mask_t1] [mask_t2] [mask_t2] [mask] [mask_t3] [mask] [mask_t3] [mask] [mask] [mask] [mask_t4] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask_t5] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] [mask] Formal: To get the attention weight vector p_t, we just add up the attention weight vectors. Formal: For the span prediction, we calculate the probability p_t^2 for each span x_t in the test set using the cosine similarity measure, which is basically the dot product between the log probability p_t^2 and the log probability p_t for each span x_t. Formal: We also calculate the probability p_t^3 for each span x_t in the test set using the cosine similarity measure",
        "directory": "D14",
        "filename": "D14-1044.json"
    },
    {
        "casual_text": "Lastly, we take a look at how this affects search and scoring, and we think there's a lot more to explore about the difference between how things are trained and how they're tested.",
        "formal_text": "• Finally, we examine the implications in search and scoring and motivate further research into the mismatch between training and testing.",
        "GPT2_formal_text": ". Each word has a vector representation called h_t. Formal: The t-th token has a size of {1, ..., n}, and the graph G_t is made up of the label-specific vectors for each token. Formal: The cell state of the token is represented using a matrix called α_t. Formal: So, for any token i in the graph G_t, the state h_t of that token is basically the combination of the states from the last sentence (S_t-1) and the last token (t_s_t-1). Formal: The last state h_t is calculated using this equation: Formal: The final token representation h_t is a matrix of size {n, ..., d}, where d is the length of the token and n is the number of words in the sentence. Formal: Formal: Here, m is the total number of words in the sentence, and e_t_s is the actual label for that token. Formal: Formal: Formal: And finally, the final graph G_t is made up of the label-specific embedding matrix h_t and the self-attention network parameters h_a_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "aacl",
        "filename": "2020.aacl-main.25.json"
    },
    {
        "casual_text": "Okay, let me break this down in a simpler way:\n\n2. s = c (G, i)  \n   G = ⊥ ([i, i + 1], i, τG): s  \n   Init ([i, k], r, τ ): s  \n   s = c (⊥, k) + c 0  \n   IGNORE − −−− → k ([i, k + 1], r, τ ): s + s  \n   Skip-R ([i, k], r, τ ): s  \n   s = c (⊥, i − 1) + c 0  \n   IGNORE − −−− → i − 1  \n   Skip-L ([i − 1, k], r, τ ): s + s  \n   ([i, j], r1, τ1): s1  \n   ([j, k], r2, τ2): s2  \n   τ = (τ1, τ2) defined  \n   s = c r1 − → r2  \n   Arc-R [ ] ([i, k], r1, τ ): s1 + s2 + s  \n   ([i, j], r1, τ1): s1  \n   ([j, k], r2, τ2): s2  \n   τ = (τ2, τ1) defined  \n   s = c r2 − → r1  \n   Arc-L [ ] ([i, k], r2, τ ): s1 + s2 + s  \n   ([1, n + 1], r, [ ]): s = c 0  \n   ROOT − −− → r ([0, n + 1], r, [ ]): s + s\n\nAlright, let’s make this more conversational:\n\n2. s is calculated based on G and i.  \n   G is defined as ⊥ over the range [i, i + 1], with i and τG.  \n   Init sets s over [i, k], r, and τ.  \n   Then, s is updated to include c(⊥, k) and c0.  \n   IGNORE leads to k, updating s over [i, k + 1], r, and τ.  \n   Skip-R adjusts s over [i, k], r, and τ, with s now including c(⊥, i - 1) and c0.  \n   IGNORE then points to i - 1, and Skip-L updates s over [i - 1, k], r, and τ.  \n   For ranges [i, j] and [j, k], we have s1 and s2 with r1, τ1 and r2, τ2 respectively.  \n   τ is defined as (τ1, τ2), and s connects r1 to r2.  \n   Arc-R combines s1 and s2 over [i, k], r1, and τ.  \n   Similarly, for [i, j] and [j, k], s1 and s2 are defined with τ2 and τ1, and s connects r2 to r1.  \n   Arc-L combines s1 and s2 over [i, k], r2, and τ.  \n   For the range [1, n + 1], s is set to c0, and ROOT connects to r.  \n   Finally, s is updated over [0, n + 1], r, and an empty list.\n\nThis should be easier to follow!",
        "formal_text": "2. s = c (G, i) G = ⊥ ([i, i + 1], i, τG): s Init ([i, k], r, τ ): s s = c (⊥, k) + c 0 IGNORE − −−− → k ([i, k + 1], r, τ ): s + s Skip-R ([i, k], r, τ ): s s = c (⊥, i − 1) + c 0 IGNORE − −−− → i − 1 Skip-L ([i − 1, k], r, τ ): s + s ([i, j], r1, τ1): s1 ([j, k], r2, τ2): s2 τ = (τ1, τ2) defined s = c r1 − → r2 Arc-R [ ] ([i, k], r1, τ ): s1 + s2 + s ([i, j], r1, τ1): s1 ([j, k], r2, τ2): s2 τ = (τ2, τ1) defined s = c r2 − → r1 Arc-L [ ] ([i, k], r2, τ ): s1 + s2 + s ([1, n + 1], r, [ ]): s = c 0 ROOT − −− → r ([0, n + 1], r, [ ]): s + s",
        "GPT2_formal_text": "ing(h, m) = 1 + ReLU(d_T * h + m) / (1 + ReLU(d_T * h)) Formal: We trained our model using the OpenNMT-py package (http://opennmt.net/dist/opennmt-py.zip), which is open-source and is available online. We also trained it on a batch of parallel data that was split into training and development sets. Formal: Here's what we did for classification in this paper: First, we check if the word is in the dictionary by looking at its part-of-speech tag. If it is, we grab the word's embedding, d_T. Then, we use this embedding to calculate the attention score, d_T. Formal: We use a \"zero-shot\" approach for training, where we train a model with just the dictionary data and don't touch the corpus or test set. Formal: We tested different settings for the learnable parameters, like β1, β2, and γ. Formal: We ran the model 10 times with different random seeds to get the best results for the test set. Formal: Formal: We picked the best model using the BLEU score (Papineni et al., 2002) on the development set. Formal: We also used the same setup for development and testing in Table 5. Formal: We used the CIDEr and F1 scores (from Callison-Burch et al., 2005) for classification. Formal: We split the test set into training and development sets. Formal: We got the data split by splitting it into training and development sets. Formal: We did a 10-fold cross-validation to figure out the parameters for the learned parameters. Formal: For classification, we used the best model using the BLEU score. Formal: The code is available for anyone to check out at http://nltk.org. Formal: Check out Figure 2 for a visual of how we built our model. Formal: The dataset we used includes 8 million English words with 100 different part-of-speech tags. Formal: We trained the model using the English Penn Treebank (PTB) from the Penn Treebank project (Marcus et al., 1993). Formal: We tested different datasets to see how the learning process affects things. Formal: For classification, we used the C",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "Alright, so we've got W ao, W ad, and W ae i, which are all learnable parameters. Here, W ao and W ad are in R m×b, and W ae i is in R m×d. The variable m represents the hidden size of the attention mechanism, b is the hidden size of BERT, and d is the size of the word embedding. As you can see in Figure 1, the attention process works like this:",
        "formal_text": "where W ao ∈ R m×b, W ad ∈ R m×b, and W ae i ∈ R m×d are learnable parameters; m denotes the hidden size of attention, b denotes the hidden size of BERT, d denotes the size of word embedding. As shown in Figure 1, the attention goes through as:",
        "GPT2_formal_text": "= {ha, de, it}, where `ha` is the human-made evidence label, and `de` is the evidence label created by the model. Formal: In this paper, we start by looking at how people use the word \"evidence\" to describe the stuff they gather. We then talk about what a model can learn from that and how it’s different from other types of knowledge. After that, we explain how we create a knowledge graph (NG) for text collections by using a fancy method called graph neural network (GNN). Our approach is different from older methods because it works with two types of text: documents and documents that have been labeled with some label. We use a two-layer graph neural network to figure out the label for each document in our dataset. Formal: We then go into a detailed comparison of different methods for figuring out the label for documents and documents. We test them using two types of text: documents and document-labeled documents. This gives us the most detailed look at how these methods are being used in real-world situations. Formal: We also talk about some important research questions that haven’t been answered yet. Formal: Lastly, we talk about some ideas for future work that could help us use this new kind of knowledge to tackle new problems. Formal: Our work builds on a few ideas: (1) knowledge graph embedding for text collections, (2) using a graph neural network for text collection classification, and (3) learning from labeled documents. We think these ideas could be super useful for lots of different tasks, especially ones that deal with big collections of text. Formal: We also talked about a few other cool things in this paper: (1) there are already some models using graph neural networks for text collection classification, (2) there are some cool ways to use knowledge graph embeddings, and (3) there are also some approaches that use two types of text: document and document-labeled documents. Formal: We also talked about some cool research questions that haven’t been answered yet: Formal: We also talked about some important research questions that haven’t been answered yet: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "deelio",
        "filename": "2020.deelio-1.1.json"
    },
    {
        "casual_text": "Alright, let's dive into what we found when we used one-sided discriminative labeling with some hints. We grabbed the real syntactic labels from the Penn Treebank—about 9000 sentences worth. Then, we plugged them into the LaSO sequence labeling software (shoutout to Daumé III and Marcu, 2005) with its default features. Our main aim here was to look at two key things:",
        "formal_text": "In this section, we describe the results of one-sided discriminative labeling with hints. We use the true syntactic labels from the Penn Treebank to derive the constraints (this is roughly 9000 sentences). We use the LaSO sequence labeling software (Daumé III and Marcu, 2005), with its built-in feature set. Our goal is to analyze two things:",
        "GPT2_formal_text": "ed. Model 2 is an LSTM-based setup, and its parameters are trained using the cross-entropy loss. Formal: The LSTM-based model's parameters are trained using a cross-entropy loss to make sure the sentiment distribution stays the same. Formal: We take the LSTM-based model's parameters, which are in R f, and plug them into the logistic regression classifier in an FST to get the sentiment prediction for the whole document. Formal: The logistic regression classifier is trained using the cross-entropy loss to predict the sentiment label for the whole document. Formal: We call the logistic regression classifier \"LSTM with loss\" and add a linear layer to it to calculate the loss. Formal: The loss LSL is basically the cross-entropy loss LSL applied to the logistic regression classifier, applied to the whole document. Formal: This loss LSL can be adjusted during training to create different loss functions, each tailored to the specific task of predicting the sentiment label. Formal: The loss LSL can be adjusted during training to create different loss functions, each tailored to the specific task of predicting the sentiment label. Formal: For training the classifier, we use the cross-entropy loss LSL to get the probability of the document's label, p(d|e). Formal: Formal: For training the classifier, we use the cross-entropy loss LSL to get the probability of the document's label, p(d|e). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "The VQA model by Johnson and team (2017) has two main parts: a program generator (G) and an execution engine (E). The program generator takes a question (q) and predicts a program (p) to answer it. The execution engine then uses this program to combine different modules and runs the whole thing on an image to get the answer. Johnson and colleagues (2017) trained the model using a semi-supervised method. They showed that the program generator can work well even when trained on just a small number of possible programs (like 4% or less).\n\nTo test how well the execution engine (E) performs with limited data, we did some experiments using regular supervised training but with smaller training sets. In all these experiments, we fed E the correct program and image pairs. Figure 1 shows the best accuracy we got for each experiment on CLEVR's validation set, where the execution engine was trained on a portion of CLEVR's training set, like 50% (check out Figure 2 for some examples from the CLEVR dataset). The results show that the execution engine doesn't do so well when it's trained on smaller subsets of data.",
        "formal_text": "The VQA model (Johnson et al. , 2017b) includes two main components: a program generator G and an execution engine E. The program generator predicts a program p to address a question q. The execution engine combines the modules according to the program, and executes the obtained network on the image to produce an answer. Johnson et al. (2017b) train the model using a semi-supervised learning approach. They demonstrate that the program generator can produce acceptable programs while training on only a small fraction of possible programs (≤ 4%). To evaluate E's performance in a low data regime, we conducted a number of vanilla supervised training experiments with decreasing sized training sets. Note that we use ground truth program and image pairs as the input to E in all experiments. Figure 1 shows the best accuracy of each experiment on CLEVR's validation set while the execution engine is trained on a subset of the CLEVR's train set e. g. , 50% (See Figure 2 for some examples of the CLEVR dataset). The results verify execution engine's poor performance on the small sized training subsets.",
        "GPT2_formal_text": "/all\" (check out the validation set in Table 2) and \"events_all_ids/sent\" (in the test set in Table 2). These extra features show how each pair of sentences is structured, as shown in the same schema. Formal: For the same schema, we also use a special feature for the first and second sentences. The first sentence is always labeled as \"reference\" and the second sentence as \"sentiment.\" Formal: The third and fourth sentences are labeled as \"reference\" and \"sentiment\" respectively. Formal: The last sentence is always labeled as \"sentiment.\" Formal: This works the same for the test set too. Formal: Formal: We're using the same setup for the features we mentioned earlier. Formal: Formal: We do the same thing for the second and third sentence as shown in the schema. Formal: Formal: The same is done for the fourth and fifth sentence. Formal: Formal: Formal: We use the same setup for the fourth and fifth sentence as shown in the schema. Formal: Formal: Formal: To make sure the validation set is correct, we add the following conditional probability to the schema. Formal: Formal: We've got a new model setup that helps us figure out how sentence structures are related. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "alta",
        "filename": "2021.alta-1.3.json"
    },
    {
        "casual_text": "FB15k-237 #θ MR MRR Hits@10 @3 @1 MR MRR Hits@10 @3 @1 TuckER (Balazevic et al.). We also saw some cool improvements. Plus, our local-cognitive negative sampling method, which combines the best of both worlds, really shines in structured knowledge learning.",
        "formal_text": "FB15k-237 #θ MR MRR Hits@10 @3 @1 MR MRR Hits@10 @3 @1 TuckER (Balazevic et al. , improvement. The results also emphasize the effectiveness of our proposed local-cognitive negative sampling method, a non-trivial integration of the both above, in structured knowledge learning.",
        "GPT2_formal_text": "(d) and max_pooling (c), respectively. The summaries we're looking at are the ones generated by the model, not the ones written by the users. Formal: A summary is basically a bunch of sentences, but we're only focusing on the ones from the user's perspective. Formal: We only update the summary token by token, so it doesn't get changed. Formal: So, we need to find the best way to rank these summaries, which basically means we want to rank the summaries from the user's point of view. Formal: To do this, we're using a method that's pretty popular: maximum entropy optimization (shoutout to Neubig, 2008). Formal: We're using a bunch of features to figure out how good the summaries from the user's perspective are. Formal: In this part, we're just looking at the rank(u) and rank(v) of the summaries from the user's perspective. Formal: Here's how the ranking function looks like for the summaries generated by the model, which are from the user's perspective. Formal: We use the model's output to figure out the ranking(u, v) for the summaries generated by the model. Formal: And here's the formula that calculates the rank(u, v) for the summaries generated by the model, which are from the user's perspective. Formal: We're using a matrix A ∈ R m×d to store the rank(u, v) for the summaries generated by the model. Formal: A ∈ R m×d is a matrix with dimensions m×d×d, where m is the number of sentences, and d is the number of words in each sentence. Formal: Next, we update the summary token by token using the output from the model. Formal: So, we've got the ranking(u, v) of the summaries generated by the model. Formal: To find the best way to rank these summaries, we're using this formula: Formal: For the ranking(u, v) of the summaries generated by the model, we're using this formula: Formal: The formula for the ranking(u, v) of the summaries generated by the model, which are from the user's perspective. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.48.json"
    },
    {
        "casual_text": "In this part, we’re introducing our clickbait detection method called SATC, which stands for style-aware title modeling and co-attention. The whole setup of SATC is shown in Fig. 2. It has four main parts: a content modeling module that learns how titles and bodies are represented based on their content, a style modeling module that picks up on the stylistic patterns in the title, an interaction modeling module that looks at how the title and body connect, and a clickbait prediction module that calculates the clickbait score. Let’s break down each of these modules in more detail. Oh, and Fig. 2 shows the full structure of our SATC approach for detecting clickbait.",
        "formal_text": "In this section, we introduce our proposed clickbait detection approach with style-aware title modeling and co-attention (SATC). The framework of our proposed SATC approach is illustrated in Fig. 2. It consists of four core modules, i. e. , a content modeling module to learn representations of title and body from their content, a style modeling module to capture the stylistic patterns in the title, an interaction modeling module to capture the interactions between the contexts within title and body, and a clickbait prediction module to compute the clickbait score. The details of each module are introduced as follows. Figure 2: The architecture of our SATC approach for clickbait detection.",
        "GPT2_formal_text": "\" }, {}, {}, and {}, where A is the set of individual units. The coefficient α controls the number of aggregation operations, and δ(α) is the smoothed importance score. The sum of all these factors, G, is what we use as the document representation for the query vector. Formal: In this paper, we're looking at the classification task in a multi-document summarization setup. We're focusing on the case where each query has a number of units. In the multi-document summarization task, we figure out the probabilities using the distribution we get from each query. To handle the input sentence, we use a multi-head attention mechanism, where each query has a fixed number of units. This approach helps us get the maximum representation, q, for each query. Formal: To get the summarization probability for a query, we use a weight vector δ(q) that's part of the query vector. We figure out the probability for a query using the multi-head attention mechanism, p θ (q). Formal: The multi-head attention mechanism is a fancy way to handle multi-head attention. It uses a multi-head attention mechanism to combine information from different parts of the query vector. This multi-head attention mechanism gets the probability p θ (q). Formal: We combine the query-specific vector q q with the document vector e, which is part of the query vector, to get the representation for the query query. Formal: Finally, the attention mechanism p θ (q) is used to estimate the importance score for the query. Formal: Formal: The document representation is basically the sum of the representation for each query and the summary, which are also part of the query vector. Formal: Formal: Figure 1 shows a multi-document summarization system that uses a multi-head attention mechanism, p θ (q). Formal: Here, δ(α) is the smoothed importance score, α is the number of aggregation operations, and γ is the number of query-specific units. Formal: The relevance score q is calculated by combining the relevance score e, which is in the query vector, with the relevance score p θ (q). Formal: Formal: The relevance score for a query is calculated by integrating the relevance score e, p θ (q). Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "ccl",
        "filename": "2020.ccl-1.106.json"
    },
    {
        "casual_text": "Alright, so the goal here is to show how to use \"locus\" to break down the interpellation of a phrase into two parts. One part is the interpellation of the addressed item, and the other is something related to it that this can combine with. Let's say, for example, we're looking at the VP \"a peach should be interpreted as.\"",
        "formal_text": "The g, ~mera. I aim of ISis l)a. l~er is to show h()w t, o use locus I; o decotnpose the ii~l, erpreL: . ~t, ioJl of a. phra, se iul; o two) pa. . rl, s, where, oue pa, rt is I, he iittez'prel; . ; : d, ion, . ff I, he ['o, : : t+ssed item a, n(l the or; her is sotn, : ; o1: , ], : : , , : : 1; with which this ca. n comlJine. Sup-pose, ['or exa. ml>le, we t. h(+ughl, l. ha, t; the VP. /. , a pea. oh shoutd Im iul: ; rF, reted ~ts:",
        "GPT2_formal_text": "is a matrix that gives the attention value for the word in the target word's vector. Let's say v_t is the target word's vector. The input to the attention layer is a [CLS] token, and the output from the first layer is a vector with 4d elements, where each d_i is the element from the [CLS] token of v_t. The output vector for the second layer is a vector with 32d elements, and the output from the first layer is a vector with 16d elements. Formal: We can see how all this fits together in our setup in Figure 1. Formal: The first layer's vector is the result of combining the vectors from the [CLS] and [SEP] tokens, following the process outlined in Equation (1). Then, the second layer's vector gets a little boost because the [SEP] token is adjusted by one, so it starts with a lower value. Formal: So, the [CLS] and [SEP] tokens are transformed into vectors, and the [CLS] vector is adjusted by one. This process is repeated for all the tokens in the [CLS] and [SEP] vectors. Formal: After this, the [CLS] and [SEP] vectors are combined into a single vector, and the [CLS] vector is adjusted by one again. This process is repeated for all the tokens in the [CLS] and [SEP] vectors. Formal: After this, the [CLS] and [SEP] vectors are combined into a single vector, and the [CLS] vector is adjusted by one again. Formal: So, the [CLS] and [SEP] vectors are combined into a single vector, and the [CLS] vector is adjusted by one again. Formal: We can make this process more efficient by picking smaller vectors, which helps the model focus on specific parts of the text. Formal: We can keep going with this process, updating the [CLS] and [SEP] vectors using the values from the [CLS] and [SEP] vectors. Formal: We can keep going with this process, updating the [CLS] and [SEP] vectors using the values from the [CLS] and [SEP] vectors. Formal: The [CLS] and [SEP] vectors are combined into a single vector, and the [CL",
        "directory": "C94",
        "filename": "C94-2142.json"
    },
    {
        "casual_text": "It means that in any ordered tree, for any node u, if the feature structure linked to u begins with the point w and inv is true at w, then auz is also true at w. Plus, you can get to the propositional info fin from w by taking a (VFORM) transition to another node w'.",
        "formal_text": "This says that for any ordered tree and any node u in this tree, if the feature structure associated with u starts with the point w and inv is true at w, then auz is also true at w and furthermore, the propositional information fin is reachable from w by making a (VFORM) transition to some other node w'.",
        "GPT2_formal_text": "\" and \"response_ids\" from the documents. The first two are just the query-related info, and the last one is about the response-related info. To make sure the documents have the same number of documents, we use a special token called \"d\" to mark them as documents. This token helps keep track of how many documents there are. Formal: For this, we use \"rand\" to pick the best text sequence (like a random sentence) from all the options. We use a \"rand\" that changes depending on the specific query, based on how often it shows up in the documents. Formal: Formal: For the first query-related random sentence, we pick the one that has the highest chance of appearing in the documents. Formal: We also start with a basic index (like the ones you'd see in a regular text index) and grab the best match from the word embeddings. Formal: We use the same method for the first query-related sentence, just like before. Formal: Now, for the second query-related sentence. Since the probability of a word embedding (like the one in Figure 2) depends on how often it shows up in the documents, we adjust the word embeddings by taking the average of how often they appear in each document. Formal: Here, we pick the sentence with the highest probability of being the best match for each query-related sentence. Formal: Formal: Finally, for the third query-related sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E93",
        "filename": "E93-1004.json"
    },
    {
        "casual_text": "This paper introduces a phrase structure parsing algorithm that’s kind of a mashup of ideas from LR (Knuth, 1965), GLR (Tomita, 1988), and some newer stuff on dependency grammar by Huang and Sagae (2010). It also uses a discriminative weighting system inspired by Collins (2002). Now, discriminative phrase structure parsing can be a real pain when it comes to speed and efficiency, as Turian and Melamed (2006) and Finkel et al. (2008) pointed out. But we’ve figured out some shortcuts that not only make this approach manageable but also super efficient and accurate, especially for a language like French, which is packed with tons of vocabulary.",
        "formal_text": "The paper provides a phrase structure parsing algorithm inspired by LR (Knuth, 1965), GLR (Tomita, 1988) and the recent developments of (Huang and Sagae, 2010) for dependency grammar. The parsing algorithm comes with a discriminative weighting framework inspired by (Collins, 2002). Although discriminative phrase structure parsing has been shown to be challenging when it comes to efficiency issues (Turian and Melamed, 2006; Finkel et al. , 2008), we use here several approximations that make the framework not only tractable but also efficient and accurate on a lexically rich language such as French.",
        "GPT2_formal_text": "_layers is the key to understanding the attention structure. The output layer for the attention layer is basically the output value for the masked word y_i. Here, \"l\" is the number of hidden units, and \"M\" is the number of attention heads. The model parameters α and β are both learned in a linear way. The training process includes some bias term optimization, so the model's parameters can be tweaked during the actual use. Formal: Also, let's note that a bunch of attention layers are combined into one, called the global attention. This helps the model focus on different parts of the input, which can make it better at classifying the input (Bordes et al., 2014). Formal: The attention mechanism in our approach is based on a hierarchical attention mechanism (HATM) from (Bejan and Zuidema, 2014). We started by combining the global attention with the global attention for the first token, and then for each token after that, we added the global attention for the next token, and so on, until the global attention is fully used. This way, the attention mechanism takes in the hidden state info of the previous token, which is the key to understanding the sentence and the target word. Formal: The global attention network is created by adding an extra layer called global_attn, which has a hidden state for the global attention for the word w_i. The global attention layer is built by adding a non-linear function, which helps the model learn to focus on the important parts of the word. Formal: Here's the breakdown of the model's parameters: Formal: We start by training the model using a linear learning rate schedule (also called a stochastic gradient descent, or SGD for short). Formal: We use a learned objective function, which gives us a special number (the value) for each token, which helps the model learn how to focus on that specific part. Formal: We also have a batch size set to 0 to prevent overfitting. Formal: We tested our model on two real-world datasets: the Yelp review dataset (Yu et al., 2015) and the Yelp customer review dataset. Formal: We did a bunch of experiments, including manual evaluation, to see how well it works and how well the model learns. Formal: We set the batch size to 0 to avoid overfitting. Formal: We followed the same setup as in (Bordes et al., 2014) for our experiments.",
        "directory": "C14",
        "filename": "C14-1052.json"
    },
    {
        "casual_text": "2 So, the overall change on the main test set (Total) is basically nothing, but when we look closer, we see that the new data helps reduce the error rate for low-frequency phrases by up to 3%. And, as we hoped, it doesn’t mess things up for the high-frequency stuff. Things get even better for the MARUPA examples we held back, where the baseline had a high error rate on purpose. We noticed that a lot of the collected phrases are so rare that they don’t even show up in the lowest-frequency category of the main test set, so it’s hard to fully understand how much MARUPA is helping. Across different languages, we found that for newer ones like Hindi, the examples we get aren’t as helpful and the quality is lower. This is because there are fewer user interactions and the underlying IC/SL system isn’t as developed yet. Overall, the results show that MARUPA can boost accuracy for those less common phrases.",
        "formal_text": "2 While the overall change on the main test set (Total) is negligible, the break down reveals that for low-frequency utterances the new data leads to error rate reductions of up to 3%. As desired, this comes with no negative effects for high-frequency utterances. Even stronger error reductions can be seen for the heldout MARUPA examples, on which the baseline's error rate is by design high. We observed that many collected utterances are so rare that they are not captured even by the lowest-frequency bin of the main test set, making it difficult to assess the full impact of MARUPA. Across languages, we observe that for newer languages like Hindi, less useful examples can be collected and their quality is lower, which is because fewer user interactions are available and the underlying IC/SL is less mature. Overall, the results demonstrate that MARUPA can improve accuracy on long-tail utterances.",
        "GPT2_formal_text": "\" and \"reverse_adjectives\" in different languages. This project is an example of a hybrid approach, where there are similarities between the two types of data but also some differences. Formal: We built the model using Python 3.7. The data came from Wikipedia dumps from 2007 and 2008, which we used to train the models. Formal: In our system, we use WordNet synsets to connect words with their synset, meaning a word can be linked to multiple synsets, and multiple synsets can be linked to a single word. We also use a lexicalized re-ordering model to handle these connections. We tested our methods on the English and Spanish datasets and found that the model works better than older methods that used super-specific word embeddings. Formal: Check out Figure 1, which shows the setup of the hybrid model with a basic word embedding (w) and a lexicalized re-ordering model (c). Formal: We trained the models for 100 epochs with different random seeds, starting with a learning rate of 0.001, followed by a mini-batch size of 32. The model was built with PyTorch 1.6. Formal: For the NER task, we also used the dependency parse tree that comes with the English dataset (check out Section 5.2 for more details). Formal: The dependency parse tree that comes with the Spanish dataset has extra words in the form of lexical units (LUs) and has been annotated with POS tags for the Spanish data. Formal: Formal: In our experiments, we used the English or Spanish datasets for the experiments in Table 4. Formal: We looked into how different language pairs handle NER by testing the models on the English and Spanish datasets. Formal: Formal: We tested the NER models on the English dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "coling",
        "filename": "2020.coling-industry.3.json"
    },
    {
        "casual_text": "So, we’re talking about a survey by Gao et al. (2018) that gives a big-picture look at neural methods in conversational AI. In this paper, we’re focusing on similar work, specifically semantic parsing approaches for conversations. \n\nLiang et al. (2017) came up with a Neural Symbolic Machine (NSM) that’s enhanced with a key-value memory network. In this setup, the keys and values come from a sequence model during different encoding or decoding stages. The NSM is trained using the REINFORCE algorithm with weak supervision and tested on the WebQuestionsSP dataset (Yih et al., 2016).\n\nThen, Saha et al. (2018) introduced a hybrid model that combines the HRED model (Serban et al., 2016) and the key-value memory network model (Miller et al., 2016). This model has three main parts:\n\n1. The Hierarchical Encoder, which creates a representation for each utterance.\n2. A higher-level encoder that makes a representation for the overall context.\n3. The Key-Value Memory Network. It stores each candidate tuple as a key-value pair. The key is made up of the combined embeddings of the relation and the subject, while the value includes things like:\n   - Finding objects in triples with a specific subject and predicate.\n   - Finding subjects in triples with a specific object and predicate.\n   - Filtering entities based on a given type or multiple types.",
        "formal_text": "We point to the survey by (Gao et al. , 2018) that provides a holistic overview of neural approaches in conversational AI. In this paper, we stick to our closely related work, i. e. , semantic parsing-based approaches in conversations. (Liang et al. , 2017) introduce a neural symbolic machine (NSM) extended with a key-value memory network, where keys and values are the output of the sequence model in different encoding or decoding steps. The NSM model is trained using the REINFORCE algorithm with weak supervision and evaluated on the WebQuestionsSP dataset (Yih et al. , 2016). (Saha et al. , 2018 ) propose a hybrid model of the HRED model (Serban et al. , 2016) and the keyvalue memory network model (Miller et al. , 2016). The model consists of three components. The first one is the Hierarchical Encoder, which computes a representation for each utterance. The next module is a higher-level encoder that computes a representation for the context. The second component is the Key-Value Memory Network. It stores each of the candidate tuples as a key-value pair where the key contains the concatenated embedding of the relation and the subject. In contrast, the value contains Action Description set → find(e, p) set of objects part of the triples with subject e and predicate p set → find reverse(e, p) set of subjects part of the triples with object e and predicate p set → filter type (set, tp) filter the given set of entities based on the given type set → filter multi types(set1, set2)",
        "GPT2_formal_text": "ed. It’s pretty cool that the MAP score doesn’t get worse when we remove words that show up way less often. But hey, the issue with the character-based metric isn’t too big because the term vector is already weighted to avoid overfitting. Formal: • The distance between a word’s representation and its definition vector (we’ll call this vector D_D) is usually less than a constant. Formal: • The distance between a word’s representation and its definition vector (we’ll call this vector D_D) and the distance between the definition vector and the vector for that word (we’ll call this vector D_D) are usually smaller than a constant. Formal: • The loss function for a bunch of these vector spaces is basically the square root of the absolute difference between the vector for the word and its vector. Formal: • The loss function for a bunch of these vector spaces is the square root of the absolute difference between the vector for the word and its vector. Formal: • The loss function for a bunch of these vector spaces is the square root of the absolute difference between the vector for the word and its vector. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.72.json"
    },
    {
        "casual_text": "Most spelling systems these days need some special, language-specific stuff, like dictionaries, lists of common typos, or rule books. Systems that use statistical models need a lot of data with examples of spelling mistakes to learn from. But our system doesn’t need any of that fancy annotated data. Instead, we just use the internet as a big, messy source of information. Here’s how we do it:\n\n1) We figure out stuff about typos by looking at how words are used on the web and use that to create a model of errors.\n2) The words we see the most online become a messy list of possible corrections.\n3) We use n-grams (little chunks of words) to build a language model (LM) that helps us make corrections that sound right in context.\n\nSince our error model works by looking at parts of words, we don’t have a fixed list of \"correct\" words to compare things to. So, whether a word is spelled wrong or right, it’s all good. Plus, when we combine this with our n-gram LM, the system can catch and fix real-word mistakes—like when you use the wrong word or mess up grammar.",
        "formal_text": "Most spelling systems today require some handcrafted language-specific resources, such as lexica, lists of misspellings, or rule bases. Systems using statistical models require large annotated corpora of spelling errors for training. Our statistical models require no annotated data. Instead, we rely on the Web as a large noisy corpus in the following ways. 1) We infer information about misspellings from term usage observed on the Web, and use this to build an error model. 2) The most frequently observed terms are taken as a noisy list of potential candidate corrections. 3) Token n-grams are used to build an LM, which we use to make context-appropriate corrections. Because our error model is based on scoring substrings, there is no fixed lexicon of well-spelled words to determine misspellings. Hence, both novel misspelled or well-spelled words are allowable. Moreover, in combination with an n-gram LM component, our system can detect and correct real-word substitutions, ie, word usage and grammatical errors.",
        "GPT2_formal_text": "= {a1, ..., an, ..., ani}) = {x1, ..., xi} = {y1, ..., yi} where each yi is the result of applying a classifier to each of the ai-th samples. Formal: Let's say the input sequence x1, ..., xI represents the classifier's predictions for a certain sample yi. Similarly, the output sequence yi, ..., yI can be considered the classifier's predictions for a sample yi, where each yi is a prediction from a classifier that was applied to a sample in the sequence xI. Formal: In the case where the classifier's predictions for a sample yi come from a classifier that was applied to the sample in the sequence xI, the result of applying that classifier to the sample yi is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D09",
        "filename": "D09-1093.json"
    },
    {
        "casual_text": "Check out the annotation templates and interfaces we used for the explanation collection and verification stages in Figures 12, 13, 14, 15, and 16.",
        "formal_text": "We present the different annotation templates and interfaces used for our explanation collection and verification stages in Figures 12, 13, 14, 15 and Figure 16 respectively.",
        "GPT2_formal_text": "ed. Formal: We’ve got a bunch of shared tasks in the pipeline, like GPT-2 and GPT-2 Small, plus the OpenWebText shared task (Yang et al., 2019). To make the most of this shared task setup, we’ve decided to create and release two open-source tools: GPT-2 Training (GT) and GPT-2 Small (GPT-2 Small). These tools have been developed on top of the development version of GPT-2. These tools are meant to help researchers build more efficient and effective models. Plus, they’re also pretty handy for teaching and experimenting with the whole model. Formal: GPT-2 Training (GT) has been built to handle both training and inference in one go. It’s been tested with different approaches, and it’s clear that GPT-2 Training is a solid alternative to the usual pipeline methods. Formal: GPT-2 Small has been made to be efficient for both inference and training. It’s been trained on a smaller set of text and is super easy to tweak for new tasks and datasets. Formal: To help researchers explore different architectures and different needs, we’ve also come up with a Python module called GPT-2Attention (GT-Att) (Yang et al., 2019). It’s built on GPT-2 and can handle various tasks, including GPT-2 Small. Formal: Our experiments show that GPT-2 Training and GPT-2 Small are pretty useful for researchers. Formal: We’ve made GPT-2 Small available under the Apache 2.0 license, so anyone can use and improve it. Formal: This project has been made possible thanks to the support from the European Union through the EC-funded project 'GPT-2', which is part of the TRAC project 61314/1. Formal: Formal: We want to give a shoutout to the European Commission for their support and guidance throughout the whole process. Formal: Our code is available on GitHub at github.com/feng-yufei/gpt-2_train. Formal: Formal: We’ve also included some related work on our GitHub page: https://github.com/apingupta/gpt-2_small Formal: For the validation set of the OpenWebText benchmark, you can find it",
        "directory": "acl",
        "filename": "2022.acl-long.451.json"
    },
    {
        "casual_text": "We used Pytorch 1.4.0 (Paszke et al., 2019) along with libraries like numpy (Harris et al., 2020) and scipy (Jones et al., 2001-) to code all the models. For running the experiments, we had two setups: (1) a GeForce RTX 2080 GPU with 12 GB of memory, 256 GB of RAM, and 40 CPU cores, or (2) a Tesla V100-SXM2 GPU with 16 GB of memory, 250 GB of RAM, and 40 CPU cores.",
        "formal_text": "All the models are coded using Pytorch 1. 4. 0 14 (Paszke et al. , 2019) and related libraries like numpy (Harris et al. , 2020), scipy (Jones et al. , 2001-) etc. We run all experiments on one of the following two systems -(1) GeForce RTX 2080 GPU of size 12 GB, 256 GB RAM and 40 CPU cores (2) Tesla V100-SXM2 GPU of size 16GB, 250 GB RAM and 40 CPU cores.",
        "GPT2_formal_text": "\" in the set of training data. The paraphrase model we're using, based on the work by Aspell et al. (2017), can automatically find a bunch of paraphrases and pick the best one out of a set of options. For instance, if we have the phrase \"I am trying to reach my destination by night\" in the training data, it’d get picked as the best paraphrase. Formal: The system assigns each word in a sentence to one of three different paraphrase types: (1) the actual translation of the word, (2) a translation that’s similar to the original word, or (3) a paraphrase that’s not the original word. Formal: The system uses word embedding models that have been trained on parallel corpora. These embedding models are set up to focus on specific parts of the sentence (or sentence pair) that are important for the translation. Formal: Some text encoders are pre-trained on huge corpora to help with learning different aspects of language, which is pretty common in real-world situations. Formal: The translation model we’re using, called the paraphrase model, has four parts. Formal: In this paper, we’re introducing a new way to estimate the model’s parameters. Formal: We’re using two basic embedding models to learn the parameters, which are: Formal: The model gives each word in a sentence a vector with three parts: Formal: The model uses three different embedding models to learn the parameters. Formal: We’re using two basic embedding models to learn the parameters. Formal: We’re working with three different embedding models to learn the parameters. Formal: We’re using a neural network to learn the parameters. Formal: We’re using three different embedding models to learn the parameters. Formal: We’re using a neural network to learn the parameters. Formal: The model calculates the log probability of the output. Formal: The model calculates the log probability of the output. Formal: The model calculates the log probability of the output. Formal: The model calculates the log probability of the output. Formal: The model calculates the log probability of the output. Formal: The model calculates the log probability of the output. Formal: The model calculates the log probability of the output. Formal: The model calculates",
        "directory": "acl",
        "filename": "2022.acl-long.451.json"
    },
    {
        "casual_text": "Okay, let’s say c₁, c₂, and so on represent the sequence of vector embeddings for the parts of the new phrase. The size of these vectors depends on the number of dimensions in the bidirectional LSTM that was used in the original composition function, as shown in Figure 2. We’ll use a semicolon (;) to show when we’re sticking two vectors together, like gluing them end-to-end.",
        "formal_text": "Now let c 1, c 2, . . . denote the sequence of vector embeddings for the constituents of the new phrase. The length of these vectors is defined by the dimensionality of the bidirectional LSTM used in the original composition function (Fig. 2). We use semicolon (; ) to denote vector concatenation operations.",
        "GPT2_formal_text": "ed_u. Formal: Basically, the input word, w_i, is turned into a representation in R^m, where m is the size of the hidden layer in the neural network. The projection matrix P_t is also in R^m. The hidden state h_t for the tth time step is calculated using the matrix P_t and the projection matrix P_t. Formal: So, the attention layer has a hidden state h_t for the time step t in its output h_t. This hidden state is then used to figure out the attention vector a_t. Formal: We've got a simple model for the sequence of words w. The input word is the nth word in the sequence, written as w_n. The hidden state h_t for the nth time step is calculated using P_t and P_n. Formal: The output h_t for the nth time step is also calculated using the matrix P_t and the projection matrix P_t. Formal: To get the attention vector a_t, we use this formula: Formal: We're using the hidden states of the words w_i in the sequence (the sequence is U = (w_1, ..., w_n)). Formal: We're using the projection matrix P_t to calculate the attention vector a_t. Formal: Formal: We're using the hidden states of the words w_i in the sequence (the sequence is U = (w_1, ..., w_n)). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E17",
        "filename": "E17-1117.json"
    },
    {
        "casual_text": "The most common approach for semi-supervised sentiment classification is using some labeled data to help guide the process (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008; Wan, 2009; Li et al., 2011). But in a lot of cases, we don’t have any labeled data, which opens the door to unsupervised methods. The typical way to do unsupervised sentiment classification is by using a sentiment lexicon (Turney, 2002; Taboada et al., 2011) or by figuring out sentiment orientation through matrix factorization and clustering (Li et al., 2009; Hu et al., 2013). \n\nInstead of following those methods, we’ve come up with a different approach that combines word-level and document-level sentiment information (like how nearby words or documents often share the same sentiment) into one framework. Our framework uses geometric information to make up for the lack of labeled data, which is a big deal for sentiment classification. \n\nOn top of that, some researchers have used matrix factorization for other NLP tasks, like relation extraction (Peng and Park, 2013) and question answering (Zhou et al., 2013). There are also a bunch of studies focusing on other parts of sentiment analysis, such as cross-domain sentiment classification (Blitzer et al., 2007; Pan et al., 2010; Hu et al., 2011; Bollegala et al.).",
        "formal_text": "The most representative way to perform semi-supervised paradigm is to employ partial labeled data to guide the sentiment classification (Goldberg and Zhu, 2006; Sindhwani and Melville, 2008; Wan, 2009; Li et al. , 2011). However, we do not have any labeled data at hand in many situations, which makes the unsupervised paradigm possible. The most representative way to perform unsupervised paradigm is to use a sentiment lexicon to guide the sentiment classification (Turney, 2002; Taboada et al. , 2011) or learn sentiment orientation via a matrix factorization clustering framework (Li et al. , 2009; ? ; Hu et al. , 2013). In contrast, we perform sentiment classification with the different model formulation and learning algorithm, which considers both word-level and document-level sentiment-related contextual information (e. g. , the neighboring words or documents tend to share the same sentiment polarity) into a unified framework. The proposed framework makes use of the valuable geometric information to compensate the problem of lack of labeled data for sentiment classification. In addition, some researchers also explored the matrix factorization techniques for other NLP tasks, such as relation extraction (Peng and Park, 2013) and question answering (Zhou et al. , 2013) Besides, many studies address some other aspects of sentiment analysis, such as cross-domain sentiment classification (Blitzer et al. , 2007; Pan et al. , 2010; Hu et al. , 2011; Bollegala et al.",
        "GPT2_formal_text": "\" and \"translation_ids\" respectively. These refer to the actual scores from the translation model, the training data, or both. Formal: In the output part, we use \"norm\" to mean \"normalized.\" Formal: For an input \"X,\" there are two possible output strings: the source sentence \"X\" and its normalized version \"Y.\" Formal: For each sentence \"X,\" there are two possible output strings: the source sentence \"X\" and its normalized version \"Y.\" Formal: We take the normalized versions of these outputs to get the final scores for each \"X.\" Formal: We can solve the problem using a regular, linear programming approach. Formal: The algorithm works by finding the best solution using a linear programming algorithm. Formal: This method is easy to explain, and you can check out the algorithm for more details. Formal: We do the whole process with two main steps: Formal: First, we break down the sentence \"X\" into smaller pieces (parts) using word embeddings. Formal: Then, we calculate the normalized versions of these parts, which we call \"Y.\" Formal: Next, we train a translation model using these normalized versions of the sentence \"X.\" Formal: Once the model is trained, we use these normalized versions of the sentence \"X\" as the main input to calculate the final scores for each \"X.\" Formal: Finally, we use these normalized scores to find the best solution for the language model problem. Formal: The parts of \"X\" that get normalized are shown in Figure 1, and the parts of the sentence that get normalized are in Figure 2. Formal: Formal: We train a language model using the normalized versions of the source sentence \"X.\" Formal: Finally, we use these normalized scores to find the best solution for the language model problem. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C14",
        "filename": "C14-1126.json"
    },
    {
        "casual_text": "Plus, the LSTM-based language model (LM) might do a better job because it captures more context than the CRF layer. To really make the most of the LM, we also tried training the tagging model and the LM together at the sequence level. Sure, this slows things down a bit, but the combined system performs just as well as the top-notch NER models on four different datasets.",
        "formal_text": "In addition, the LSTM-based LM potentially can perform better as it encodes more contextual information than the CRF layer. To unlock the full power of the LM, we also try to train the tagging model and LM jointly at the sequence level. In this case we lose the speed advantage, but the jointly trained system achieves comparable performance as the state-of-the-art NER model on four different corpora.",
        "GPT2_formal_text": "ing_frozen_layers = 3;\n           Casual: Even though the English model has some really good features, we noticed that the model doesn't work as well in French. The reason is that the model struggles to pick up on the differences between French and English sentences. For instance, in Figure 2, the French sentence is \"Vous avez pas besoin de l'arrêt. Noir est à l'arrêt.\" But the English version is \"The boy ran away from home. Noir is at home.\" This makes it tough for the model to figure out where the English sentence is going or what it's actually saying. Formal: We tested our method on the French language using the Pan-German Treebank (PTB) (de Marneffe et al., 2013) and got results like these: 50.37% accuracy on French to English translation, 32.08% accuracy on French to German, and 31.94% on French to English. Formal: We also tried the English-French Treebank (ETB), but it only got an accuracy of 26.78% on French to English translation, which isn't great either. Formal: For the test set, we only looked at two models: one with 3-gram baselines and another with 4-gram baselines. The 4-gram baselines were trained on the PTB, while the 3-gram baselines were trained on the CTB. The results in Table 5 show that the model we trained on the PTB performed the best on the English-French test set, with a 53.22% accuracy. Formal: In Table 5, you can see the accuracy scores for the three models we trained on the PTB and CTB, along with the BLEU score for the test set. Formal: Also, in the first row of Table 5, we show the BLEU score for the test set, which was trained on the PTB and CTB. Formal: On the CTB, the results are pretty similar to the English-French set, with a 34.26% accuracy. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.612.json"
    },
    {
        "casual_text": "Alright, so basically, D' is about recalculating the data from the original dataset (let's call it Dataset 1) by focusing on specific parts of it. These parts are related to certain features or variables that come from a different source, like the Å ÖÓ×Ó Ø Ü Ð ÐÔ Ø ÜØ. The idea is to adjust or refine the original dataset based on this new information. It's kind of like tweaking the data to make it more accurate or relevant by incorporating these extra details.",
        "formal_text": "D' = recalculate ÙÖ ¾ Ë Ø Ó ÌÖ Ò× Ö ÊÙÐ × È Ö× × Ò ÌÖ Ò× Ö ÊÙÐ × ÙÖ ½ × Ô Ö Ó Ö ÙÐ Ö Þ Ô Ö× × ÓÖ ÓÖÖ ×ÔÓÒ Ò Ô Ö Ó ËÔ Ò × Ò Ò Ð × × Ò¹ Ø Ò × ÖÓÑ Å ÖÓ×Ó Ø Ü Ð ÐÔ Ø ÜØº Ì × Ö ¹×ØÖÙ ØÙÖ ¹Ð Ô Ò Ò Ý Ò ÐÝ× × Ó × Ò¹ Ø Ò × Ø Ø Ö",
        "GPT2_formal_text": ", w_p, p_i) = (1, 1, 1, 1) (7) Formal: Alright, so for the binary classification part, we take the predicted probability and add it to the label probability. This probability then becomes part of the final classifier, which we call f_p. Formal: The labels that are most likely to be correct are marked with a \"+1\" next to them. Formal: So, this is basically saying that the labels with the highest probability are the most likely to be right. Formal: Check out Figure 2: This shows an example of predicting a part-of-speech (POS) tag and its correct result, based on the NER result. The POS tag with the highest probability is also the most likely tag. Formal: We have a binary label prediction thing that starts with a probability of 1, and then we add a \"+1\" label, based on the POS tag's probability, to get the final label prediction. Formal: The model then decides whether to tag the part-of-speech as \"NP\" or \"NP NP\" based on the POS tag's probability. Formal: Lastly, the final POS tag with the highest probability is also the most likely tag. Formal: In short, POS tagging is like guessing if a word is a noun, verb, or adjective, and then combining those guesses with a probability for a POS tag to get the final result. Formal: Formal: The whole process of POS tagging is laid out in Algorithm 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C00",
        "filename": "C00-1078.json"
    },
    {
        "casual_text": "Experiment 4: This one compares two different update methods—early update and max violation update. The max violation update gets trained for 12 epochs, but the early update goes through 25 epochs.",
        "formal_text": "Experiment 4 This experiment contrasts early update with max violation update. Max violation update is trained over 12 epochs, while early update is trained over 25 epochs",
        "GPT2_formal_text": "ed()/2). Formal: We're using the LibLinear package, which you can find at https://www.nltk.org/, to calculate the hidden states of the bidirectional LSTM model. Formal: The LSTM model has six hidden layers, and each layer has a size of 768. We're setting the learning rate to 0.001, which works out to a vector size of 768 × 768. Formal: The LSTM hidden state size is 768. Formal: We're using the LibLinear package, which you can find at https://www.nltk.org/, to calculate the hidden states of the bidirectional LSTM model. Formal: The LSTM model has six hidden layers, and each layer has a size of 768. Formal: The LSTM hidden state size is 768. Formal: We're using the LibLinear package, which you can find at https://www.nltk.org/, to calculate the hidden states of the bidirectional LSTM model. Formal: The LSTM model has six hidden layers, and each layer has a size of 768. Formal: The LSTM hidden state size is 768. Formal: We're using the LibLinear package, which you can find at https://www.nltk.org/, to calculate the hidden states of the bidirectional LSTM model. Formal: We're using the LibLinear package, which you can find at https://www.nltk.org/, to calculate the hidden states of the bidirectional LSTM model. Formal: We're using the LibLinear package, which you can find at https://www.nltk.org/, to calculate the hidden states of the bidirectional LSTM model. Formal: We're using the LibLinear package, which you can find at https://www.nltk.org/, to calculate the hidden states of the bidirectional LSTM model. Formal: We're using the LibLinear package, which you can find at https://www.nltk.org/, to calculate the hidden states of the bidirectional LSTM model. Formal: We're using the LibLinear package, which you can find at https://www.nltk.org/, to calculate the hidden states of the bidirectional LSTM model. Form",
        "directory": "C14",
        "filename": "C14-1052.json"
    },
    {
        "casual_text": "We're testing our zero-shot activity recognition against a few other baseline models that also learn from attributes and embeddings. One of them is the \"Embarrassingly Simple Zero-shot Learning\" (ESZL) model by Romera-Paredes and Torr (2015). It's a straightforward linear model that predicts class labels using attributes and includes some regularization techniques.\n\nWe're also comparing our results to a version of the DAP model by Lampert et al. (2014), which we talked about in Section 4.1.1. Plus, we're including DeVISE (Frome et al., 2013) in our comparisons, as mentioned in Section 4.2.\n\nFor all these baselines, we're using visual features from a Resnet-152 CNN that's been fine-tuned on the imSitu V train classes. This is the same setup we discussed in Section 4.1.",
        "formal_text": "Zero-shot activity recognition baselines We compare against several additional baseline models for learning from attributes and embeddings. Romera-Paredes and Torr (2015) propose \"Embarassingly Simple Zero-shot Learning\" (ESZL), a linear model that directly predicts class labels through attributes and incorporates several types of regularization. We compare against a variant of Lampert et al. (2014)'s DAP model discussed in Section 4. 1. 1. We additionally compare against DeVISE (Frome et al. , 2013), as mentioned in Section 4. 2. We use a Resnet-152 CNN finetuned on the imSitu V train classes as the visual features for these baselines (the same as discussed in Section 4. 1).",
        "GPT2_formal_text": "= [], , ] # Get the info from the user's input and use it to fill in the slots for the answer. Each query for the knowledge base needs to include these two things: Formal: We'll take the knowledge base and give it a name, a description, and a special token called [CLS] to show what the user is looking for. Formal: The question needs to have at least one of the predefined slot types: <Type1> <Type2>, <Type3>, or <Type4>. The whole question is turned into a query string, and the info in that query is used to fill in the slots for the answer. Formal: For instance, the query for \"pink\" might look like this: [CLS] pink. We need to figure out what the user wants, like: Formal: We can use the knowledge base to find the right answers for the questions. Here, I will give you the structure of the query to help you out. Formal: For instance, when we're looking for a specific type of answer \"entailment,\" the query might look like this: Formal: We can use the knowledge base to find the right answers for the questions. Formal: The query for \"pink\" has the key words: [CLS] pink. It can also have a description, like: Formal: But if the user wants something specific, like \"pink blue\", the query would look like this: [CLS] pink blue. Formal: A typical example is: Formal: A typical example is: Formal: The answer for \"pink\" is \"pink blue.\" Formal: Formal: This specific query can't be used in a question about \"pink.\" Formal: Formal: It's not possible to use this query in a question about \"pink.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D17",
        "filename": "D17-1099.json"
    },
    {
        "casual_text": "A part-whole pattern shows that one thing is part of another thing. Take the example \"There's a valley on my mattress.\" Here, \"valley\" and \"mattress\" have a part-whole relationship, indicated by the word \"on.\" But in reality, \"valley\" isn't actually part of the mattress—it's more like an effect on the mattress. This is called a pseudo part-whole relation. For our purposes, though, we won't make a big deal out of the difference between this and a real part-whole relationship because they don't matter much for our feature mining task. In this case, \"noun 1 on noun 2\" is a helpful pattern that suggests noun 1 is part of noun 2. So if we know \"mattress\" is a class concept, we can guess that \"valley\" is a feature for \"mattress.\" There are lots of other phrases or sentences that show this kind of relationship, as mentioned in (Girju et al, 2006).\n\nBesides part-whole patterns, the \"no\" pattern is another important feature indicator, especially in opinion documents. We'll talk more about these patterns in Sections 3.2 and 3.3. \n\nNow, let's tackle the first issue: noise. With opinion words, part-whole patterns, and \"no\" patterns, we have three ways to identify features. But all of these are kind of vague, meaning they're not strict rules. This means we'll probably end up with some wrong features (also known as noise) when we use them. Getting rid of noise from our feature candidates is tricky. Instead of trying to prune out the noise directly, we're going to take a different approach: feature ranking.",
        "formal_text": "A part-whole pattern indicates one object is part of another object. For the previous example \"There is a valley on my mattress\", we can find that it contains a part-whole relation between \"valley\" and \"mattress\". \"valley\" belongs to \"mattress\", which is indicated by the preposition \"on\". Note that \"valley\" is not actually a part of mattress, but an effect on the mattress. It is called a pseudo part-whole relation. For simplicity, we will not distinguish it from an actual part-whole relation because for our feature mining task, they have little difference. In this case, \"noun 1 on noun 2 \" is a good indicative pattern which implies noun 1 is part of noun 2. So if we know \"mattress\" is a class concept, we can infer that \"valley\" is a feature for \"mattress\". There are many phrase or sentence patterns representing this type of semantic relation which was studied in (Girju et al, 2006). Beside part-whole patterns, \"no\" pattern is another important and specific feature indicator in opinion documents. We introduce these patterns in detail in Sections 3. 2 and 3. 3. Now let us deal with the first problem: noise. With opinion words, part-whole and \"no\" patterns, we have three feature indicators at hands, but all of them are ambiguous, which means that they are not hard rules. We will inevitably extract wrong features (also called noises) by using them. Pruning noises from feature candidates is a hard task. Instead, we propose a new angle for solving this problem: feature ranking.",
        "GPT2_formal_text": ". Formal: The transformation layer doesn't get too deep or depend too much on the input sentence's structure. Formal: Since the input sentence has a lot of words, it's usually a good idea to keep the input sentence structure as simple as possible. The length of the input sentence usually doesn't matter. Formal: In this setup, the input sentence is made up of a bunch of words, like w_i. Each word in the input sentence has a position vector, p_i. The output from the source embedding layer takes the form of a matrix T_s = (w_i, x_i, ..., x_i−1, ..., x_i−n), where T_s is a collection of vectors representing the words in the sentence. This matrix, T_s, is used to figure out the relevance scores for each word in the input sentence. Formal: Using the sentence's structure is kind of like minimizing the cross-entropy loss, which is what's done in CNNs and RNNs. Formal: The loss function in CNNs or RNNs, which is a natural variation of this loss, focuses more on how the words are spread out compared to how they're organized. Formal: The loss function in Transformer models is similar to the one in RNNs. Formal: One issue with traditional RNNs is that they need to keep track of all the possible output sequences for a given input, which can be a real pain. Formal: In Transformer models, you can take the embedding of a word and toss out the part of the sentence that doesn't have the word. Formal: We take the word embedding from the source embedding layer and throw out any words that don't have the word. Formal: In RNN models, you can predict the output by using a weighted sum of all the words in the input sentence. Formal: In CNN models, the output layer can look at the input sentence to predict the output. Formal: This means you can predict the output using a weighted sum of all the words in the input sentence. Formal: Here, α is a number that decides how much of the input sentence's words can be predicted. Formal: The loss function in RNN models can be described using a simple linear function that looks at how words are spread out in the input sentence. Formal: This formula is a way to estimate the loss function in RNN",
        "directory": "C10",
        "filename": "C10-2167.json"
    },
    {
        "casual_text": "After combining the question with both image-level and object-level features (check out Section 4.3 for more details), we add these features to something called the \"counter feature,\" which was introduced in the paper. This counter feature helps the model keep track of how many objects there are. Then, we use a simple two-layer neural network to pick the answer from a list of options. The list is made up of all the correct answers from the training data that showed up more than 8 times. To figure out the best answer, we calculate something called \"logits\" using this equation:",
        "formal_text": "Given the final joint representation of question with image level and object level features (c. f. Section 4. 3), we augment both of these features to the counter feature (c f ) proposed in. The counter feature helps the model to count the objects. Finally, we employ a two-layer perceptron to predict the answer from a fixed set of candidate answers. It is predetermined from all of the correct answers in the training set that appear more than 8 times. To this end, the logits can be computed by the following equation:",
        "GPT2_formal_text": "= {query_ids1, ..., query_idsL} | i=1, ..., m=1 | . . . | c = {c1, ..., cL}, where c1 is in L and cL is in R. Formal: We don't know the feature values for these question-answer pairs. In this paper, we use a method called latent semantic similarity to figure out the similarity between questions and answers. To do this, we first turn the question and answer into vectors using a method called word2vec (Mikolov et al., 2013). After that, we use this similarity vector to figure out the semantic similarity between questions and their answers. Formal: The method we just talked about calculates the similarity between questions and their answers, while the matrix A, which is related to the semantic similarity, is also used to calculate the similarity between the question and the answer. Formal: The similarity between the question and the answer is determined by the semantic similarity, which is calculated using the A matrix. Formal: We use this similarity vector to combine the question and answer, then use a linear transformation to get the semantic similarity between them. Formal: This similarity vector is then used to combine the two vectors into a single similarity vector that we can use to calculate the similarity between questions and answers. Formal: The similarity between the question and the answer is figured out by the semantic similarity, which is calculated using the A matrix. Formal: Formal: The similarity between the question and the answer is determined by the semantic similarity, which is calculated using the A matrix. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "We suggested using space to represent the meanings of words. We came up with a way to learn how to do this in tile space. We also showed that our methods work well in an information retrieval system.",
        "formal_text": "We proposed to represent the meanings of words in space. We gave a learning mechanism for tile space. We proved tile effectiveness of the proposed methods in an information retrieval system.",
        "GPT2_formal_text": "ing = (1, [T], [MASK]) # To make the model's output more varied and helpful, we toss in an attention mechanism. The model's output is made up of a bunch of word embeddings, called w_i. Formal: We create a matrix called H_ij that holds the input hidden state, where each row represents the word embedding. Formal: Next, we pick a random token from the vocabulary V_ij for the target word y_i. We use this random choice to calculate the attention vector, which we call o_ij. Formal: Next, we feed this o_ij into a Bi-LSTM with a hidden layer, which creates the word representation h_ij for the target word y_i. Formal: Lastly, we use the attention mechanism to update the model's output. Formal: So, in short, this method turns the input hidden state into a sentence vector, then uses the Bi-LSTM to process it, and finally spits out the final word representation h_ij. Formal: So, in short, this method turns the input hidden state into a sentence vector, then uses the Bi-LSTM to process it, and finally spits out the final word representation h_ij. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1089.json"
    },
    {
        "casual_text": "We use model checking to compare the FOL structure and the FOL formula for inference, and we do this with NLTK while also optimizing things (check out Section 3.4 for more details). Basically, we're working with the FOL formula and the FOL structure and making some assumptions.",
        "formal_text": "We apply model checking between the FOL structure and the FOL formula for inference using NLTK with optimization (see Section 3. 4). Under the FOL formula and the FOL structure, we assume",
        "GPT2_formal_text": "= { u1, u2, ..., uN } | P1, P2, ..., Pn. Formal: So, the second part of the constraint is about how to handle the vectors f_i, which are basically the probability distributions for u_i. Formal: For the first part, the constraint is that the vector f_1, f_2, ..., f_K should match up with the vector x_i, which is the distribution for y_i. Formal: The vector u_i, which is the vector for y_i, is just the sum of the vectors for u_1, u_2, ..., u_K. Formal: Lastly, the vector x_i, which is the vector for y_i, is also the sum of the vectors for x_1, x_2, ..., x_K. Formal: The formulas and proofs in this paper are available for anyone to check out. Formal: • http: //github. com/saruu/samd-vec2sim-formula Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-srw.8.json"
    },
    {
        "casual_text": "To make sure a fragment shows up in two parse trees, its labels have to be exactly the same, including all those categories, tags, and features we talked about earlier. The \"h 1 binarization\" thing means that fragments can include parts of bigger chunks; like, a string of kids from a big family of pieces. Figure 1 has an example of a parse tree, but it’s not binarized to keep things simple. The non-terminal labels have a syntactic category (in red) and a function tag (in green). The part-of-speech tags also have extra details (in black) in square brackets. Some labels have extra details that got passed down, and they’re marked with a colon at the beginning.",
        "formal_text": "For a fragment to be attested in a pair of parse trees, its labels need to match exactly, including the aforementioned categories, tags, and features. The h \" 1 binarization implies that fragments may contain partial constituents; i. e. , a contiguous sequence of children from an n-ary constituent. Figure 1 shows an example parse tree; for brevity, this tree is rendered without binarization. The non-terminal labels consist of a syntactic category (shown in red), followed by a function tag (green). The part-of-speech tags additionally have morphological features (black) in square brackets. Some labels contain percolated morphological features, prefixed by a colon.",
        "GPT2_formal_text": "_len, A_mask_len) = ((L_int) - A_int) * N_int) / N_int for each token in h. Formal: Here, \"A_int\" is the attention matrix that gets passed between the model's output and the input, and \"L_int\" is the number of layers in the model. The sum (L_int + A_int) is the total number of attention units for the token in h. Formal: The attention mask for the token in h can be written as A_mask_len = ([L_int] + A_int) * N_int / N_int. Formal: The attention mask for the last token in h is calculated like this: Formal: We also calculate the attention score for the next token in h by adding the attention weights for all the tokens in h. This score is calculated as the sum (A_mask_len + A_int) * N_int / N_int. Formal: This gives us the final attention weight for the token in h. Formal: Finally, we use equation (9) to get the attention vector for the token in h. Formal: For each token in the source sequence, we calculate the attention score for the token in h by adding the attention weights for all the tokens in h. This is done using equation (10). Formal: We also calculate the attention score for the next token in h by adding the attention weights for all the tokens in h. This is done using equation (11). Formal: Formal: The formula for the attention score for the last token in h can be written like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E17",
        "filename": "E17-1115.json"
    },
    {
        "casual_text": "For each phrase in the training file, grab its count (C(f)). Then, go through all the unique phrases.",
        "formal_text": "foreach phrase f in training file do extract C(f ); /* phrase count */ foreach unique phrase",
        "GPT2_formal_text": ". Formal: If all the reordering options are A_i, then A_i = {A_j}. If not, it just means A_i = {A_j} for all the possible combinations. Formal: Using this setup, the model can generate all the reordering options. Formal: In our project, we're focusing on the A_i and A_j cases, and for the rest, we're assuming they're all the same. Formal: Once the translation is done, we take the model's output (y_t) and apply it to the original sentence, which we'll call s_t. Formal: Next, we use the phrase table of the target language's version of s_t to pick the best translation, y_t, for the whole sentence. Formal: Finally, we check how similar the target sentence is to the original in terms of grammar. Formal: The word alignment model is basically a more advanced version of the phrase table model, so it's way more powerful. Formal: Formal: We tested it on three languages: Chinese (zh_en), English (en_en), and Japanese (ja_en). Formal: Formal: We also looked at how well it worked on a language with fewer resources, with English being the main resource. Formal: Formal: For the Chinese and Japanese experiments, we used the latest version of the BLEU score, which comes from the Wall Street Journal corpus (Bollacker et al., 2005). Formal: Formal: Formal: For the Chinese and Japanese experiments, we used the latest version of the BLEU score, which comes from the Wall Street Journal corpus (Bollacker et al., 2005). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D10",
        "filename": "D10-1064.json"
    },
    {
        "casual_text": "Neural networks have been doing really well in various AI areas lately (LeCun et al., 2015), and because they’re so good at modeling stuff, a lot of researchers have been diving into creating new recommendation algorithms using Deep Learning. You can check out some examples like Barkan and Koenigstein (2016), He et al. (2017), Hidasi et al. (2015), and Covington et al. (2016).",
        "formal_text": "Due to the recent success of neural networks in multiple AI domains (LeCun et al. , 2015) and their superior modeling capacity, a number of research efforts have explored new recommendation algorithms based on Deep Learning (see, e. g. , Barkan and Koenigstein, 2016; He et al. , 2017; Hidasi et al. , 2015; Covington et al. , 2016).",
        "GPT2_formal_text": "ing. This process is illustrated in Figure 3. The main goal of our training process is to make the model's predictions as accurate as possible. To do this, we calculate the cross-entropy loss, which is shown in Equation 4. Formal: In Equation 1, we first pick the first token in the sentence, and then we calculate the cross-entropy loss for each token, D t 1. Here, we're dealing with a sequence of words w 1, . . . , w K, where each token has two words, w 1 t and w K t. Formal: In this part, we're just trying to make the model's predictions as accurate as possible. Formal: We start by picking the token w i. Then, for each token, we calculate the cross-entropy loss. Formal: We train the model using cross-entropy loss on the entire dataset D. Formal: Lastly, we can figure out the N-best list of tokens by optimizing the cross-entropy loss using the gold-labeled dataset D. Formal: In this part, we're just trying to make the model's predictions as accurate as possible. Formal: We train the model using cross-entropy loss on the entire dataset D. Formal: Finally, we can figure out the N-best list of tokens by optimizing the cross-entropy loss using the gold-labeled dataset D. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "ecnlp",
        "filename": "2020.ecnlp-1.8.json"
    },
    {
        "casual_text": "At each time step t, we figure out the value using a max-pooling thingy across different window size filters. In simpler terms, q c t = max(q l 1, c t, q l 2, c t, . . . , q l L, c t ). The final output from the CNN layer can be written as {q c t } T t=1. Taking inspiration from how well this has worked in other NLP tasks (like Luong et al., 2015; Yue-Hei Ng et al., 2015), we stack a bunch of Bi-LSTM (Hochreiter and Schmidhuber, 1997) layers to get a better understanding of the whole question's meaning. The first LSTM layer takes in the convoluted representation of the question, which is {q c t } T t=1. Then, q r t = Bi-LSTM(q r t−1, q c t ).",
        "formal_text": "t at a time step t is computed by the max-pooling operation over different window size filters. Mathematically, q c t = max(q l 1, c t, q l 2, c t, . . . , q l L, c t ). The final representation computed by CNN layer can be denoted as {q c t } T t=1. Inspired from the success in other NLP tasks(Luong et al. , 2015; Yue-Hei Ng et al. , 2015), we employ stacking of multiple Bi-LSTM(Hochreiter and Schmidhuber, 1997) layers to capture the semantic representation of an entire question. The input to the first layer of LSTM is the convoluted representation of the question {q c t } T t=1. q r t = Bi-LSTM(q r t−1, q c t )(3)",
        "GPT2_formal_text": "= {'a': [], 'b': [], 'c': [], 'd': [], 'e': [], 'f': [], 'g': [], 'h': [], 'i': [], 'j': [], 'k': [], 'l': [], 'm': [], 'n': [], 'o': [], 'p': [], 'q': [], 'r': [], 's': [], 't': [], 'u': [], 'v': [], 'w': [], 'x': [], 'y': [], 'z': [], 'c': [], 'd': [], 'e': [], 'f': [], 'g': [], 'h': [], 'i': [], 'j': [], 'k': [], 'l': [], 'm': [], 'n': [], 'o': [], 'p': [], 'q': [], 'r': [], 's': [], 't': [], 'u': [], 'v': [], 'w': [], 'x': [], 'y': [], 'z': [], 'c': [], 'd': [], 'e': [], 'f': [], 'g': [], 'h': [], 'i': [], 'j': [], 'k': [], 'l': [], 'm': [], 'n': [], 'o': [], 'p': [], 'q': [], 'r': [], 's': [], 't': [], 'u': [], 'v': [], 'w': [], 'x': [], 'y': [], 'z': [], 'c': [], 'd': [], 'e': [], 'f': [], 'g': [], 'h': [], 'i': [], 'j': [], 'k': [], 'l': [], 'm': [], 'n': [], 'o': [], 'p': [], 'q': [], 'r': [], 's': [], 't': [], 'u': [], 'v': [], 'w': [], 'x': [], 'y': [], 'z': [], 'c': [], 'd': [], 'e': [], 'f': [],",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "Since the model's accuracy is pretty high, you don't see many mistakes in the output. It's also clear that the chunk model can still predict the right form even when it's shortened to just one character, like on line 5.",
        "formal_text": "Since the model accuracy is rather high, the errors are not very common in the output. We can also see clearly that the chunk model is able to predict the right form even when form is reduced to one character, as on line 5.",
        "GPT2_formal_text": ". Formal: • Using a specific decoder (let's call it `decoder_1`). Formal: • In each layer, we randomly pick a random token in the input from the set `s`. Formal: • We'll use a temperature `t` of 0.1 to adjust the output of the model. Formal: • The decoder can be trained to minimize the cross-entropy loss as the loss function. Formal: • To prevent overfitting, we use a penalty term `λ` of 0.01. Formal: • We use a linear function `W(y)` to average the attention and attention weight vectors across the output layers, which gives us a hidden state `H(y)` for each token. Formal: • We use the same setup as the original paper to figure out the hidden state `H(y)` for each token. Formal: • We train this model using a batch size of 32. Formal: • The model is built using PyTorch version 1.9. Formal: • The model is trained using Adam (from Kingma and Ba, 2014). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-5519.json"
    },
    {
        "casual_text": "In this add-on, we’re going to break down how we collected our data (Section A.1), explain the nitty-gritty of our algorithm (Section A.2), and dive into the results of our alignment process (Section A.3).",
        "formal_text": "In this supplementary, we describe the details of our data collection process ( § A. 1), experimental details of our algorithm ( §A. 2) and provide analysis of our alignment outputs ( §A. 3).",
        "GPT2_formal_text": ". Each time you send an update to the model, it gets added to the batch. Formal: Alright, let’s break down each part of the equation we’re talking about here. First, we’ll explain what the embedding layer is doing, and then we’ll explain the hidden state layer. We’ll assume that each input x_i has a positive score, which we’ll call φ_i. In the first part, we’ll call the embedding layer x_1. Then, in the second part, x_2. . . x_m. . . x_n. . . x_m. . . x_n. . . . Formal: So, x_i is the input for the embedding layer, and φ_i is the score we get from that input. \n\nNow, to use the model, we need to build this hidden state layer. Formal: The hidden state layer, which we’ll call the h_s layer, is just a bunch of gates that control the input and output of the model. Formal: The gates we’ll use are G_l_l (l_0 through l_l), G_l_l (l_1 through l_l), and G_l_l (l_m through l_m), which are the gates that handle each dimension in the input and output. Formal: The model does this: Formal: If φ_i is positive, we add φ_i + G_l_l (l_0 through l_l), φ_i + G_l_l (l_1 through l_l), and φ_i + G_l_l (l_m through l_m) to the output. This gives us the final output. Formal: We do the same thing for φ_2, . . . , φ_m, but this time, φ_2 + G_l_l (l_0 through l_l) + φ_2 + G_l_l (l_1 through l_l) + φ_2 + G_l_l (l_m through l_m) is added to the output. Formal: The model does this: Formal: We add G_l_l (l_0 through l",
        "directory": "acl",
        "filename": "2020.acl-main.440.json"
    },
    {
        "casual_text": "First off, we checked out single task models to make sure our setups could hold their own against the best out there. We tested on dependency parsing (EWT), GLUE classification tasks, and machine translation (WMT14 DE-EN (Bojar et al., 2014), IWSLT15 EN-VI (Cettolo et al., 2014)) using mBERT for our embeddings. Check out Table 2 for our results on the test sets compared to other studies. For all the UD tasks, we did a bit better, but for the GLUE tasks, we consistently scored lower than the benchmarks. This is probably because of how we fine-tuned things, since the actual implementations are pretty similar. The biggest dips were in the machine translation tasks, which suggests that we might need to tweak our fine-tuning and pre-processing for these specific tasks.",
        "formal_text": "As a starting point, we evaluate single task models to ensure our implementations are competitive with the state-of-the-art. We report scores on dependency parsing (EWT), the GLUE classification tasks, and machine translation (WMT14 DE-EN (Bojar et al. , 2014), IWSLT15 EN-VI (Cettolo et al. , 2014) ) using mBERT as our embeddings. 10 Table 2 reports our results on the test sets compared to previous work. For all UD tasks, we score slightly higher, whereas for GLUE tasks we score consistently lower compared to the references. This is mostly due to differences in fine-tuning strategies, as implementations themselves are highly similar. Scores on the machine translation tasks show the largest drops, indicating that task-specific finetuning and pre-processing might be necessary.",
        "GPT2_formal_text": "= self.attention.mask.r;\n           Casual: The loss function L_v(•) basically tells us how much we're losing by focusing on the target word instead of the surrounding context. We use a threshold, λ_v, which is set by the training data, to decide how important the target word is compared to the surrounding context. We're using the same setup as the baseline model to make our model better. Formal: We're looking at how well the model can predict the target word, and we're also checking how well it can handle multiple target words at once. Formal: The model learns to pick the right word to focus on when the surrounding context is off. Formal: This model can handle multiple target words at once, so it can tell the difference between the positive context and the negative one. Formal: For each time step t_i, the model learns to pick the right word to focus on, which makes the prediction a bit better. Formal: The model can handle multiple target words at once, so it can pick the right one to focus on. Formal: Formal: We're testing how well the model can learn how to create a proper target word. Formal: The model can handle multiple target words at once, so it can pick the right one to focus on. Formal: Formal: To test how well the model is learning, we're using a model that can predict the target word. Formal: Formal: We're using a model that can predict the target word, which means it can learn to focus on the correct target word. Formal: Formal: We're testing how well the model can learn how to create a proper target word. Formal: Formal: We're using a model that can predict the target word, which means it can learn to focus on the correct target word. Formal: Formal: To test how well the model is learning, we're using a model that can predict the target word. Formal: Formal: Formal: We're testing how well the model is learning, and we're testing how well it can handle multiple target words. Formal: Formal: Formal: We're using a model that can predict the target word, which means it can learn to focus on the correct target word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-demos.22.json"
    },
    {
        "casual_text": "Alright, so let's break this down in simpler terms. We've got:\n\n- `l` which is just the number of the layer we're looking at.\n- `N_i` represents the neighbors of a specific entity `e_i`.\n- `σ` is the activation function we use.\n- `norm(•)` is a way to normalize things.\n- `V(l)` and `b(l)` are the parameters (kind of like settings) for the `l`-th layer.\n\nNow, after we process each entity `e_i` through all the layers of the GCN (Graph Convolutional Network), we take all the different representations we got and smash them together into one big representation for each entity.",
        "formal_text": "where l is the layer index, N i is the neighbouring entities of entity e i, and σ is the activation function, norm(•) is a normalization function, and V (l), b (l) are the parameters in the l-th layer. The representations of each entity e i obtained in all GCN layers are concatenated into a single representation:",
        "GPT2_formal_text": ". * ) * ) * (1, 2) Formal: The results for the verbs in the right headings are also shown in Table 3. Formal: It looks like the synonym relationship is pretty strong, with a high correlation of 0.73. Formal: This is also kind of interesting, since the co-occurrence between right headings is super high, with a correlation of 0.77. Formal: But, the correlation for right subheadings is a bit lower, at 0.65. Formal: The subheadings that show up more often are also linked to higher co-occurrence, with a correlation of 0.65. Formal: This matches what we noticed earlier with the co-occurrence score—the more right headings appear, the higher the correlation. Formal: Formal: So, there’s a strong link between right headings and higher co-occurrence. Formal: Formal: Also, the correlation for right subheadings is pretty low, at 0.65. Formal: Formal: The patterns in Figure 1 suggest that each verb is linked to a specific noun phrase, like \"the chicken.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.270.json"
    },
    {
        "casual_text": "The sentence hypothesis selection module just looks at the final translation results from different systems, including the one from the glass-box combination. For every source sentence, it picks the \"best\" translation by using some feature functions to decide.",
        "formal_text": "The sentence hypothesis selection module only takes the final translation outputs from individual systems, including the output from the glass-box combination. For each input source sentence, it selects the \"optimal\" system output based on certain feature functions.",
        "GPT2_formal_text": "_len) * (u_i + 1) is the number of tokens that the attention-based embeddings in the sentence embedding h_i can't fit. The attention parameters α_i are calculated as a combination of the word embeddings of the entity e_i, the entity type f_i, and the entity type t_i. Formal: To keep things simple, we'll just call these attention vectors α_i. Formal: Here, c_i stands for the number of times the entity e_i appears in the sentence. Formal: For the masked entity-type t_i, we calculate it like this: Formal: For the masked entity-type f_i, we do the same thing but with the position embeddings of f_i. Formal: Lastly, we add some noise to the input to make it harder for the attention to focus on certain parts of the entity type. Formal: We'll call the hidden states h_i, which are the output representations for the masked entity-type t_i, H_i. Formal: To get these hidden states h_i, we calculate them as a combination of the entity type embeddings h_i, the entity type embeddings f_i, and the entity type embeddings t_i. Formal: We use a learning rate of 0.1 for each epoch. Formal: Formal: Using this setup, we can create a list of possible entity types for each entity type. Formal: We also calculate the hidden representation for the masked entity-type f_i using the list of possible types, h_i, as the input. Formal: The final hidden state H(h_i) is the result of the scalar H_i, given the input embeddings. Formal: After running this process a few times (n to n=4), we end up with the final hidden state H(h_i) = H_i. Formal: Formal: We use the softmax function (which is a linear function) to figure out the probability of each entity type. Formal: Formal: Finally, we set the parameters α_i to the same values as the entity type embeddings. Formal: Formal: The result of the softmax is then used to calculate the relation label embeddings. Formal: Formal: Formal: We do this for a",
        "directory": "D07",
        "filename": "D07-1029.json"
    },
    {
        "casual_text": "Using word-level details, like word n-grams and word length, just feels right in a word-based segmenter. On the other hand, character-level details, such as character n-grams, make more sense in a character-based segmenter. Sun (2010) did a deep dive into comparing these two methods, looking at both the theory and real-world results. Word-level info is better at showing how words connect in context, while character-level info shines when it comes to breaking down words and understanding their structure.",
        "formal_text": "It is more natural to use word-level information, such as word n-grams and word length, in a word-based segmenter; while it is more natural to use character-level information, such as character ngrams, in a character-based segmenter. Sun (2010) gives a detailed comparison of the two approaches from both the theoretical and empirical perspectives. Word-level information has greater representational power in terms of contextual dependency, while character-level information is better at morphological analysis in terms of word internal structures.",
        "GPT2_formal_text": "= []: w_s = w_s + cosine(h_s * e_s_t - b_s_t) + tanh(s_t * e_s_t) + e_s_t + cosine(h_s * e_s_t - b_s_t) + e_s_t + tanh(s_t * e_s_t) + e_s_t - tanh(s_t * e_s_t) - e_s_t + e_s_t. Formal: The squared distance between the current sentence and the highest sequence length s_t for a sample with n words is calculated like this: Formal: The absolute value of the log of the distance between the current sentence and the highest sequence length s_t for a sample with n words is calculated like this: Formal: The log of the absolute value of the log of the distance between the current sentence and the highest sequence length s_t for a sample with n words is calculated like this: Formal: The log of the absolute value of the log of the distance between the current sentence and the highest sequence length s_t for a sample with n words is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D13",
        "filename": "D13-1119.json"
    },
    {
        "casual_text": "To make things simpler and less resource-intensive, we suggest picking important representations one by one to create angles, which introduces the hyperparameters k1 and k2. We tried out different values for k1 and k2 by using token-level and sample-level triplet-wise relationships to guide the student model. To cut down on the options we had to consider, we just made k1 equal to k2. We plotted the accuracy for different values of k1 and k2, and you can see that in Figure 2. \n\nFor token-level tasks, we noticed that bumping up k1 and k2 helps boost accuracy when they're small. But once k1 and k2 hit 20 or more, the accuracy starts to fluctuate. So, we decided to go with k1 = k2 = 20 for token-level angle calculations. \n\nOn the other hand, for sample-level features using triplet-wise relationships, the accuracy just keeps going up as we increase k1 and k2. So, we just set k1 and k2 to match the batch size.",
        "formal_text": "To ease the computation and memory complexity, we propose to sequentially select important representations to form angles, leading to the hyperparameters k 1 and k 2. We test different choices of k 1 and k 2 by adopting token-level and sample-level triplet-wise relations to teach the student respectively. To reduce the search space, we simply set k 1 = k 2. We draw the accuracy curve for different choices of k 1, k 2, as shown in Fig. 2. For tokenlevel objectives, we find that increasing k 1, k 2 improves the accuracy when they are small and when k 1, k 2 ≥ 20, the curves begin to vibrate. Therefore we choose k 1 = k 2 = 20 for token-level angle calculation. While for the triplet-wise relations of sample-level features, we observe that the accuracy increases monotonically with k 1, k 2. Therefore we just set k 1, k 2 as the batch size.",
        "GPT2_formal_text": "ing, which is the attention masking operation, is used to adjust how much each word in a sentence is focused on. The function f(x; θ) tells us how much each word in the sentence matters to the model. To show how much each word in the sentence is being considered, we multiply f(x) by the attention masking score: Formal: Alright, let's break this down in simpler terms. Imagine we have two models: the one with input x and the other with input θ. The \"input\" is the whole input x, and the \"input mask\" is a number that tells us how much each word in x matters to the model.\n\nNow, let's say we have a sentence s_i, which is made up of words x_i and y_i. The attention masking operation works by multiplying the attention scores from both models together. The result is a vector that shows which words in s_i are being looked at (check out Fig. 1 for this). Formal: So, the attention score f(x; θ) is like a formula that uses these vectors to compare the attention scores. Formal: In the next part, we'll look at how to get these vectors by using a model that generates text. Formal: The attention scores for the words in s_i are figured out by looking at the output from the model that generates text. Formal: The model creates a sentence s_i that has words x_i and y_i, and then checks how similar each word is to each word in s_i. Formal: The similarity between a word and its closest neighbors is what we use to calculate the attention scores. Formal: The model uses these scores to compare the attention scores. Formal: So, basically, we're looking for the vector that shows how similar the words in s_i are to each other, and that's what we use to get the attention scores. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.71.json"
    },
    {
        "casual_text": "(2) Connecting the arguments in different speeches that disagree with each other.",
        "formal_text": "(2) establishing counterargument relations between such arguments found in different speeches;",
        "GPT2_formal_text": "= {l_e, l_f, l_e, ..., l_f_w}, where l_e, l_f, ..., l_e_w are vectors from R^d and l_e, l_f, ..., l_e_w are vectors from R^d_m. Formal: We pick some random starting points for the different layers and use their cross-entropy loss to train the model. Formal: We create a word embedding for each token x_i at layer l, which is calculated by using a softmax function on the word embedding vectors l_i. Formal: We use a linear kernel to calculate the cross-entropy loss for the whole model, like this: Formal: We use a linear kernel to calculate the cross-entropy loss for the whole model, like this: Formal: We pick a fixed loss term α to control how much the model pays attention to the internal nodes. Formal: We use a linear kernel to calculate the cross-entropy loss for the whole model, like this: Formal: We choose a fixed loss term α to control how much the model pays attention to the internal nodes. Formal: We pick a fixed loss term α to control how much the model pays attention to the internal nodes. Formal: We pick a fixed loss term α to control how much the model pays attention to the internal nodes. Formal: We pick a fixed loss term α to control how much the model pays attention to the internal nodes. Formal: We set a fixed number of word embedding dimensions, which is d_w, for each word embedding. Formal: We set a fixed number of word embedding dimensions, d_w, for each word embedding. Formal: We set a fixed number of word embedding dimensions, d_w, for each word embedding. Formal: We choose a fixed number of word embedding dimensions, d_w, for each word embedding. Formal: We choose a fixed number of word embedding dimensions, d_w, for each word embedding. Formal: We choose a fixed number of word embedding dimensions, d_w, for each word embedding. Formal: We choose a fixed number of word embedding dimensions, d_w, for each word embedding. Formal: We choose a fixed number of word embedding dimensions, d_w, for each word embed",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "Slang is just a casual, offbeat part of language that often pops up in specific settings or follows trends in society (Dumas and Lighter, 1978). It helps people feel connected to a group (González, 1998; Bembe and Beukes, 2007; Carter, 2011) or even to a particular generation (Citera et al., 2020; Earl, 1972; Barbieri, 2008). Mattiello (2005) points out how slang adds new words to the language and follows its own unique way of forming words. Based on this, Kulkarni and Wang (2018) came up with a data-driven model to mimic how slang words are created, as Mattiello described. Some people have talked about how slang words tend to be short-lived (González, 1998; Carter, 2011), but this idea hasn’t really been tested using computational methods before.",
        "formal_text": "Slang is an informal, unconventional part of the language, often used in connection to a certain setting or societal trend (Dumas and Lighter, 1978). It can reflect and establish a sense of belonging to a group (González, 1998; Bembe and Beukes, 2007; Carter, 2011) or to a generation (Citera et al. , 2020; Earl, 1972; Barbieri, 2008). Mattiello (2005) highlights the role slang plays in enriching the language with neologisms, and claims that it follows unique word formation processes. Inspired by this, Kulkarni and Wang (2018) propose a data-driven model for emulating the generation process of slang words that Mattiello (2005) describes. Others have described the ephemerality of slang words (González, 1998; Carter, 2011), although this property has not been previously verified by computational approaches.",
        "GPT2_formal_text": "_tgt is basically the attention score for the target word, and h_tgt(s) is the hidden state for the representation of the target word after the last layer of attention. Formal: Basically, for a given word w in the source language, we figure out its representation h_tgt by using the attention mechanism (basically, the cross-attention) on the embedding vectors in the target language. Formal: To make use of this, we use a target word embedding vector to get the output h_tgt(s), which we then pass through a bidirectional LSTM (like the one shown in Figure 2) to get the representation h_tgt(s). Formal: The embedding h_tgt(s) gets updated based on the hidden state of the last layer of attention from the target language, which we get from the cross-attention mechanism. Formal: Lastly, the attention mechanism takes the last hidden state h_tgt(s) and multiplies it by the sum of h_tgt_tgt and h_tgt_tgt, giving us the final attention score for the target word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.101.json"
    },
    {
        "casual_text": "Share the outcomes of the project at conferences, scientific meetups, and industry events. This will make a big difference not just in the countries involved but also in other places.",
        "formal_text": "• Disseminate the results of the Action through conferences, scientific and industrial gatherings, which will have substantial impact in the participating countries and beyond.",
        "GPT2_formal_text": "\" and \"attributes\" respectively.      Casual: Alright, so the main goal here is to figure out the target length. For each sentence in the input x, we use a Pointer Generator (which is a type of model) to create a target sequence x = {x₁, ..., xₙ}. The model is trained to generate a response y = {y₁, ..., yₙ} at the current time step t, using the same model parameters as the generator. Formal: So, the goal is to figure out the length of the target sequence by looking at the model parameters. The model gives us back a probability p(y|x), where y is the length of the target sequence and x is the length of the source sequence. Formal: We start with the Pointer Generator model p(x₁, ..., xₙ) and train it to generate the target sequence x = {x₁, ..., xₙ}. Formal: After that, we repeat the process for the source sequence x, making sure the model isn't getting stuck on an infinite loop. Formal: Finally, we figure out the length by using the model parameters and the model's probability p(y|x), all from the same model p(x₁, ..., xₙ). Formal: This whole process happens in a way where each part is designed to be different, which helps us make sure the generated responses are unique and still grammatically correct. Formal: To make sure the model is fully trained, we keep it in a loop for N rounds, trying to minimize the loss. Formal: Formal: So, we end up with a valid target length. Formal: Formal: Finally, we use this length to estimate the translation model parameters θ, which are based on the source and target lengths. Formal: Formal: Formal: Formal: This whole process happens in a way where each part is designed to be different, which helps us make sure the generated responses are unique and grammatically correct. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2022.eamt-1.63.json"
    },
    {
        "casual_text": "Some categories have way fewer questions than others—like astronomy, which only has 331 questions. So, we’re just focusing on literature and history, since they make up over 40% of the whole set. That gives us 21,041 history questions and 22,956 literature questions to work with.",
        "formal_text": "Because some categories contain substantially fewer questions than others (e. g. , astronomy has only 331 questions), we consider only literature and history questions, as these two categories account for more than 40% of the corpus. This leaves us with 21, 041 history questions and 22, 956 literature questions.",
        "GPT2_formal_text": ", which is basically the attention component for the argument span. Similarly, in the rest of the paper, we'll call these attention inputs A_s for the span and A_t for the argument span. Formal: The focus span gets a special attention vector called h_t, which is calculated using something called softmax. The argument span gets a similar attention vector, h_a_t, but it's just a linear activation, so it's not exactly the same. Formal: The attention for the span can be represented using something called a graph attention mechanism. This mechanism helps decide how much the span should focus on the argument span, based on the representation it gets from the relation graph H. Formal: This graph attention mechanism is designed to manage the attention distribution in a graph and, when combined with the attention for the argument span, it helps decide the representation for the span. Formal: And finally, the whole network gets updated as the output from a CNN or a recurrent neural network. Formal: Formal: Lastly, we can define the representation for the span as h_s_t = h_t * A_s. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D14",
        "filename": "D14-1070.json"
    },
    {
        "casual_text": "Basically, we think our study is a cool move toward combining computational social science and derivational morphology. In the next phase, we plan to tweak our approach even more to handle MFEP better.",
        "formal_text": "Overall, we see our study as an exciting step in the direction of bringing together computational social science and derivational morphology. In future work, we intend to further fine-tune our methodological apparatus for tackling MFEP.",
        "GPT2_formal_text": ", which are basically the identifier and the search query. This matching process is just like the one we talked about in Section 3.2, but with an extra step to make it faster, like a few milliseconds instead of tens of milliseconds. This way, the model can grab all the relevant info from the document. Formal: To deal with the issues mentioned in Section 3.1, we’re using a deep attention model called M-BERT to process the query and its potential query, which is the answer. Unlike CNNs and RNNs, our model has a second hidden layer that focuses on how similar the query is to the potential answer. This similarity is calculated using something called cosine similarity. Formal: We’ve come up with a new approach called MOD-BERT. This model adds a modal layer on top of a regular Transformer to create a new layer for understanding the context of the query. As a result, MOD-BERT can read the document faster and gets the answer right more often than other models. Formal: In this project, we’re building on the work of (Yang et al., 2019). They suggested using a multi-head attention mechanism (like in ) to improve retrieval. They also used a hierarchical attention mechanism to focus on the key aspects of the document. Unlike our approach, theirs also deals with the complex relationship between the query and answer. Formal: We’re calling MOD-BERT the Multi-Head Attention Network with Hierarchical Attention. It’s built on the Transformer model, which we’ll call M-BERT in this paper. Formal: So, MOD-BERT is a model that uses an attention mechanism to keep track of the relationship between the query and answer. Formal: To show how much better it is, we’ve added a binary classifier to MOD-BERT. It’s a one-vs-all decision, so the answer gets labeled as either correct or incorrect. We’ve tested it out on the IMDB dataset and found that MOD-BERT works way better than other models. Formal: We’re also working on a similar approach to MOD-BERT, but we’re focusing on different aspects of the document. Formal: You can check out the code for our model, MOD-BERT, over here. Formal: Oh, and by the way, we’re using the same",
        "directory": "acl",
        "filename": "2020.acl-main.649.json"
    },
    {
        "casual_text": "So, N + (0, 1) and N − (0, 1) are just the positive and negative parts of the normal distribution, but cut off at 0.",
        "formal_text": "Where N + (0, 1) and N − (0, 1) is the positive and negative truncated normal distribution, truncated at 0, respectively.",
        "GPT2_formal_text": "_len - 1, . . . , -1) Formal: The main issue with the majority voting method is that it doesn't make sure that the final answer is consistent with the input. This inconsistency is a big deal and makes the whole process a lot less efficient. Formal: We came up with a solution that doesn't require any special knowledge about the input or the dataset, but it still manages to get the best of both worlds: it can correctly pick the right answer and also make sure the answer is relevant to the dataset. Formal: For every pair of answers, we use a special knowledge base to figure out the probability of their correct answers. Then, we use a learning-to-rank model that uses this probability to rank the correct answers. We also came up with a cross-entropy loss to make sure the ranking model focuses on getting the right answer. Formal: We’ve created a new dataset with 20,000 examples for training and testing how well the ranking model works. This dataset is a fresh take on the ranking problem, covering a bunch of different tasks and dealing with a lot more data. Formal: We also created a simpler version of the ranking problem that only uses the input, the main input, and the output from the ranking model. Formal: We trained our ranking model using this new dataset and compared its results to the original one, which used the same input, the main input, and the output from the ranking model. Formal: We used this new dataset to test how well the ranking model works and to compare it to the original one. Formal: We ran experiments using two different datasets: SQuAD1.1 and SQuAD2.0. Formal: We’ve put the results in Table 1. Formal: We compared the ranking model to the original SQuAD with the same input, the main input, and the output from the ranking model. Formal: We noticed that the ranking model can boost the performance of the original SQuAD, but it also messes up the ranking model. Formal: The ranking model can be used for scoring the answers, but it gets penalized for being too strict. Formal: Formal: For a full breakdown of the rankings and how they compare to the original SQuAD, check out Appendix A. Formal: We tested how well our ranking model works on two different datasets: SQuAD1.1 and SQuAD2.0. Formal",
        "directory": "D19",
        "filename": "D19-1661.json"
    },
    {
        "casual_text": "But, super high scores might not tell the whole story. There’s some debate about whether these probing tasks actually check if the model truly understands the structure of language or if the results are just being interpreted correctly (Hewitt and Liang, 2019; Zhang and Bowman, 2018; Voita and Titov, 2020; Pimentel et al., 2020b). To figure this out, the next part will look at different probing methods used with language models and how we judge how well a probe works.",
        "formal_text": "However, high performance may have confounding factors; there is uncertainty on whether the probing tasks properly test if representations actually encode linguistic structure and on how to interpret the results of probes (Hewitt and Liang, 2019; Zhang and Bowman, 2018; Voita and Titov, 2020; Pimentel et al. , 2020b). Toward that end, the following section reviews several probing approaches in the context of language models, and the evaluation criteria used to determine the proficiency of a probe.",
        "GPT2_formal_text": "= {query} token. Each document pair gets turned into a vector using a multi-layer perceptron, where the hidden layer uses the contextualized word embeddings. In each layer, the student network learns to predict the label for each token. For instance, the text \"Pang and Lee (2004)\" gets turned into a vector like this: Formal: The work has three main parts: Section 3 talks about the methods used for training, the dataset used to train, and how to evaluate the results. Sections 4 and 5 explain the evaluation methods for the experiments. Section 6 covers how to interpret the results and what to do next. Section 7 covers the results and where they stand, including some thoughts on future work. Finally, Section 8 wraps things up. Formal: We look at a few different types of noise, like unaligned words, named entities, and words with multiple meanings (like \"the\" and \"of\"). But we also include some non-standard words that could cause the model to overfit. Formal: The experiments are done on a single Nvidia Tesla V100 GPU, which is way faster than other models, but still pretty slow when it comes to using high-quality NLP tools. Formal: We think these results can be helpful for tackling NLP challenges, but we also believe that the best solution is for NLP researchers to figure out what works best for their specific tasks. Formal: Since this paper is about noisy data and not directly about improving NLP tools, we decided to focus on a specific type of noise, which is noise related to word confusion. Formal: We built a model using the KingJames dataset, which has texts from English, French, and German, and each document is labeled with its own part of speech (POS). Formal: We used the \"KingJames POS\" corpus, which is from the CoNLL-2009 shared task, to create this data. Formal: Since we didn't have a trained NLP model yet, we used the Penn Treebank (PTB) to get the training set. Formal: We tested how well it works on a real-world dataset, testing it on two datasets: the Yelp dataset (YELP) and the CNN/DailyMail dataset (CNN/DM). Formal: To make sure we're comparing well, we used the Mean Squared Error (MSU) to check the accuracy. Formal: We picked 50 documents from the Yelp dataset, chosen randomly from the",
        "directory": "bigscience",
        "filename": "2022.bigscience-1.11.json"
    },
    {
        "casual_text": "Event argument detection is super important for event extraction. It’s kind of like semantic role labeling (SRL), where the goal is to figure out which words or phrases fit into the roles of an event. But with event arguments, things can get a bit more complicated—they might not just be within the same sentence. Sometimes, they can be in different parts of the document, like when an argument is implied or hidden somewhere else. Take a look at Figure 1 for an example: in the purchase event triggered by the word \"bought,\" the money argument is actually found in the sentence before.",
        "formal_text": "Event argument detection is a key component in the task of event extraction. It resembles semantic role labeling (SRL) in that the main target is to find argument spans to fill the roles of event frames. However, event arguments can go beyond sentence boundaries: there can be non-local or implicit arguments at the document level. Figure 1 shows such an example: for the purchase event, which is triggered by the word \"bought\", its money argument appears in the previous sentence.",
        "GPT2_formal_text": "_polarity(C, pol) is like this: `C = 1 - `polar` if `C` is zero, else `C = pol`. Formal: The prior σ for the given input `a` is basically the same as the context vector `c` plus the prior attention vector `b`. Formal: The posterior σ for the given input `a` is calculated by taking the difference between the prior attention vectors `b` and `c`, and then adding it to the prior attention vector `b`. Formal: And the specific attention component we're looking at is `α` from the end. Formal: There's another version of this model where we ignore the singular value decomposition, which is shown in Equation (1). Formal: For the LSTM-based model, `i_s` is the i-th token in `s_t`, and `i_t` is the i-th token in `s`. Formal: Formal: For the BiLSTM model, `i_h` is the i-th token in `h_t`, and `i_h` is the i-th token in `h`. Formal: And for the BiLSTM model, `i_q` is the i-th token in `q_t`, and `i_q` is the i-th token in `q_t`. Formal: Formal: And for the BiLSTM model, `i_c` is the i-th token in `c_t`, and `i_c` is the i-th token in `c_t`. Formal: Formal: Lastly, we'll note that the attention vector `α` for the given input `a` is calculated based on the individual attention vector `b` and the current hidden state `h_t`, respectively. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.667.json"
    },
    {
        "casual_text": "These findings suggest that mined coreference chains are a pretty handy resource and offer insights that work well alongside other techniques. While adding coreference-based embeddings does boost performance in antonym classification, the experiments also highlight that relying solely on coreference-based embeddings usually doesn’t work as well as sticking with text-based embeddings. This makes sense when you think about it—the amount of training data for the word embeddings is different in both cases. Coreference chains only cover a tiny fraction of the word-word relationships that the word2vec skip-gram model gets when it’s applied to raw text. If both methods had similar amounts of training data, we’d probably see similar results.",
        "formal_text": "These results show that mined coreference chains are a useful resource and provide information that is complementary to other methods. Even though adding coreference-based embeddings improves performance on antonym classification, the experiments also show that using only coreference-based embeddings is almost always worse than using only text-based embeddings. This is not surprising given that the amount of training data for the word embeddings is different in the two cases. Coreference chains provide only a small subset of the word-word relations that are given to the word2vec skip-gram model when applied to raw text. If the sizes of the training data sets were similar in the two cases, we would expect performance to be comparable.",
        "GPT2_formal_text": ") = (num_sentences, len(sent_id)) for i from 1 to N. s = {s1, ..., sM, ..., sT}, where T is the size of the sentence and s1, ..., sT are the total number of sentences in the summary. Formal: The feature functions we're looking at are: Formal: We're using the column vector embedding method (like the one by Lample et al., 2016), which works with both vectors and matrices. Formal: We're using the feature vector projection (from Dyer et al., 2014) for both vectors and matrices, and we're using the dot product function. The feature functions are calculated based on the data, using the same weights as the final classifier. Formal: We're using a linear kernel for the matrix embedding, which includes both the dot product and the kernel weight. Formal: The logistic regression classifier weights are: Formal: We're using the logistic regression classifier weights. Formal: The logistic regression classifier weights are: Formal: For the model parameters, we're using the results from the mini-batch model, which is trained using the BLAN method (from Kim et al., 2016). Formal: Formal: For the model parameters, we're using the BLAN model, which is trained using the BLAN method (from Kim et al., 2016). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D14",
        "filename": "D14-1151.json"
    },
    {
        "casual_text": "We're planning to dive deeper into feature engineering for instance-based SRL. Specifically, we want to look into ways to automatically pick out the best features, especially when it comes to composite features. We're also thinking about testing how well things like word clusters or word embeddings work in instance-based SRL.",
        "formal_text": "We intend to focus more specifically on feature engineering for instance-based SRL. In particular, we plan to explore automatic feature selection methods especially in the context of composite features. We also plan to evaluate generalization features such as word clusters or word embeddings in the context of instance-based SRL.",
        "GPT2_formal_text": "and attention_semantics are all about calculating the attention vector and the attention sequence for each token. The attention weights are calculated based on how often the token shows up in the document. Formal: In Section 3.1, they talk about two ways to handle word embeddings: the log-linear model and the log-linear model with a nonlinear function. This nonlinear function helps create a kind of \"hard negative\" distribution for words, which is similar to how neural networks work. Formal: To make sure the results are actually positive and meaningful, they use something called the hard negative binarization, which looks at how words are used in a specific context and compares them to some norm. Formal: In Section 3.2, they explain how to set up the training to get a better result. Formal: Lastly, in Section 3.3, they throw in a third layer of softmax to get the final attention representation. Formal: We use the Transformer model from Vaswani et al. (2017) as our base model and use the same setup as Vaswani et al. (2018) for our experiments. Formal: We train our model by running it through a sequence of 1,000 different model iterations. Formal: We've got the hyperparameters for all the models, including the hidden layers, in Table 1. We also checked if the model takes more than a second to train. Formal: The best training epoch is 10. We've included the standard hyperparameters for the models in Table 2. Formal: We've included the standard hyperparameters for the models in Table 3. Formal: We've also included the standard hyperparameters for the models in Table 4. Formal: We've included the standard hyperparameters for the models in Table 5. Formal: To save time and effort, we train the model with just one epoch instead of the usual two. Formal: We've included the standard hyperparameters for the models in Table 6. Formal: In Table 7, we show how the models perform in terms of recall (R), precision (P), and F-score (F). Formal: We've included the standard hyperparameters for the models in Table 8. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1058.json"
    },
    {
        "casual_text": "HPSG's type system includes something called parametric types, like the one shown in Figure 1 from (PS94). Unlike regular types and features, we don't really understand how powerful parametric types are. In fact, they've never been properly formalized in a way that could be used for HPSG parsing, so we couldn't even compare them to other types. This paper talks about a formalization of parametric types, based on the typed attribute-value logic from (Car92). This logic is special because it has a strict idea of what's \"appropriate\"—it tells us which features an object of a certain type can have and what types those features can take. It also says that every feature structure must have the right values for all the features that match its type.\n\nBefore, people thought that every parameter in a subtype had to be in all its supertypes and vice versa. This made it impossible to use something like Figure 1, because if _1_ was a list(X) and A_ was parametric, then everything else would have to be too. But this paper changes that (check out Section 2). It gets rid of that rule by saying there should be a simple most general type (which (Car92)'s logic already requires). This general type is then used during type-checking and figuring out new parameters.",
        "formal_text": "HPSG's type system also includes parametric types, e. g. , Figure 1, from (PS94). In contrast to the relative expressive potential of normal typing and features, the expressive potential of parametric types is not at all understood. In fact, parametric types have never been formalized in a feature logic or in a manner general enough to capture their use in HPSG parsing so that a comparison could even be drawn. This paper summarizes such a formalization, 1 based on the typed attribute-value logic of (Car92). This logic is distinguished by its strong interpretation of appropriateness, a set of conditions that tell us which features an object of a given type can have, and which types a feature's value can have. Its interpretation, total well-typedness, says that every feature structure nmst have an appropriate value for all and only the appropriate features of its type. Previous approaches have required that every parameter of a subtype should be a parameter of all of its supertypes, and vice versa; thus, it would not be 1The full version of this paper presents a denotational semantics of the logic described here. possible to encode Figure 1 because _1_ __ list(X), and if A_ were parametric, then all other types would be. 2 The present one eliminates this restriction (Section 2) by requiring the existence of a simple most general type (which (Car92)'s logic requires anyway), which is then used during type-checking and inferencing to interpret new parameters.",
        "GPT2_formal_text": "= { l_i } | l_i is in [0, i+1] = {l_i} Formal: After picking the graph constant, we do something cool: we add the graph constant to the sentences we've already generated. Formal: The memory module works by using an attention mechanism. Basically, it takes the output from the RNNG and uses a binary cross-attention to figure out the attention score for the current sentence. Formal: Then, the graph constant is added to the sentence embedding matrix. This helps the memory module grab the important info from the attention matrix and use it to generate the next sentence. Formal: The final embedding for the next sentence is calculated based on the graph constant. Formal: Finally, the final hidden state for the next sentence is created by combining the hidden state from the current sentence with the hidden state from the previous sentence. Formal: Formal: In this project, we used the same setup as in (Och and Ney, 2003), but we swapped out the attention mechanism for a global one. We did the same experiments as before, but this time, we used a batch size of 64 and set the training set to 100. Formal: In the rest of this paper, we’ll explain how to use the global attention mechanism (like in (Lin et al., 2014)) and the global memory module (like in (Och and Ney, 2003)) for our generation process. Formal: The experiments show that our method works well for generating stories. Formal: We also looked at the attention mechanism and learned from the results. Formal: We tested a few different versions of our method. Formal: We looked at the maximum likelihood estimation (MAP) for the attention network and found that the learning rate (lr) and the batch size (b) played a big role. Formal: We used the best model from (Och and Ney, 2003). Formal: To make the memory module more efficient, we added a memory bottleneck module to the memory module (as suggested by Shen et al., 2014). Formal: In our experiments, we set the learning rate to 0.001 and the batch size to 20. Formal: We ran our experiments 10 times and averaged the results. Formal: In the last part, we compared our method to some existing methods and found that it does better than some of the latest ones. Formal: We also tried a few baseline methods. Formal",
        "directory": "C98",
        "filename": "C98-2164.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. In Table 3, we tested how well a Semantic-aware Model works when combined with BERT. The results show that this combo performs the best. This tells us that BERT and semantic dependency graphs can work together nicely for solving the Chinese zero pronoun problem.\n\nBut when we compare the Semantic-aware Model without BERT to the Baseline Model with BERT, we see that BERT plays a bigger role in improving performance than the semantic dependency graph. Plus, BERT boosts the Baseline Model more than it does the Semantic-aware Model. This suggests that BERT might already be capturing some of the semantic information that the dependency graph would provide.\n\nLooking ahead, we think it would be cool to combine BERT with not just the semantic dependency graph, but also semantic role labeling. This could make BERT even better at handling Chinese zero pronouns in the future.",
        "formal_text": "Semantic-aware Model with BERT 57. 9 Table 3: Integration with BERT According to the experimental results in the Table 3, we can see that the performance of the Semanticaware Model with BERT is the best. This shows that BERT information and semantic dependency graph information have certain complementarity in the Chinese zero pronoun resolution task. But by comparing the performance of the Semantic-aware Model without BERT and Baseline model with BERT, We can see that the BERT information contributes more to the Chinese zero pronoun resolution task than the semantic dependency graph information. In addition, we can also see that BERT information improves the Baseline Model more than the Semantic-aware Model. This shows that the BERT model may encode part of the semantic information of the semantic dependency graph. Based on the above analysis, we hope that in the future research, we can further integrate the semantic dependency graph and even the information of semantic role labeling on the basis of the BERT model, so as to further enhance the ability of the BERT model in the Chinese zero pronoun resolution task.",
        "GPT2_formal_text": "_length, and output_shape, which are the same as the input_shape, are the same as the inputs. Formal: We pick the top k feature vectors with the highest confidence scores, which are [CLS, HGT] for HGT and [SEP, HGT] for HGT. Formal: You can also use the output from the teacher network, along with its hidden representations, to train a student network. This network takes the output from the student network and uses it to predict the next word. Formal: The student network is trained by picking the output from the teacher network and its hidden representation, δ_t, which is calculated using the decoding loss function in Algorithm 1. Formal: We train all these models using mini-batches of text and get the final predictions for the final test set, which we call E_t. Formal: Also, we calculate the perplexity for all possible sequences in our training set using the solution from an EM-HMM model. Formal: For each part of the training process, we use a simple linear SVM with a 10-fold cross-validation method. Formal: To figure out the log-likelihood for a sequence, we take the log of the difference between the model's log probability and the actual probability for the sequence. Formal: We break down the training process into a few steps: Formal: We create a M × N (N − 1) setup for the student network. Formal: We train the student using mini-batches of text and calculate the decoder's loss using the decoding loss function in Algorithm 1. Formal: The student network is trained by generating the output from the student network and its hidden representations, δ_t, using the decoding loss function in Algorithm 1. Formal: We calculate the perplexity for all possible sequences in our training set using the solution from an EM-HMM model. Formal: After all the training is done, we calculate the log-likelihood for a sequence based on the log of the difference between the model's log probability and the actual probability for the sequence. Formal: The student network is trained by generating the output from the teacher network and its hidden representations, δ_t, using the decoding loss function in Algorithm 1. Formal: Finally, we calculate the perplexity for all possible sequences in our training set using the solution from an EM-HMM model. Formal:",
        "directory": "ccl",
        "filename": "2020.ccl-1.77.json"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms.\n\nN is just the number of examples we use to train the model. yᵢ and ŷᵢ are the actual and predicted labels for the i-th instance, respectively. Θ represents all the parameters that the model can adjust during training. λ is the value that controls how much we care about L₂-regularization, which helps prevent overfitting.\n\nNow, for the experimental setup part, we'll set up everything to test how well the model performs.",
        "formal_text": "where N is the training data size. y i andŷ i respectively represent the ground-truth and estimated label distribution of instance i. Θ denotes all trainable parameters of the model, λ represents the coefficient of L 2 -regularization. 4 Experimental Setup",
        "GPT2_formal_text": "= {u1, ..., un}, and the ones that follow are the candidates for the given input. Formal: For the LSTM-based models, the parameters are all set up in a linear way: the input size is fixed at 100, the number of hops is set to 3, and the learning rate is set to 0.01. Formal: For the CNN model, the input size is also fixed at 100, and the number of hops is set to 3. Formal: The loss function for the CNN model is pretty straightforward—it uses the loss term from the discriminator, which is basically the cross-entropy between the inputs and the candidates. Formal: Lastly, for the BiLSTM model, the input size is also fixed at 100, and the number of hops is also 3. Formal: For these models, the learning rate is 0.1 for LSTM, 0.2 for CNN, and 0.1 for BiLSTM. Formal: We also used some tips and tricks from (Nguyen et al., 2013), who use a fixed number of hops for early stopping during testing. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.124.json"
    },
    {
        "casual_text": "Stepwise regression isn't a good fit for this kind of analysis because there just aren't enough language pairs to work with in each setup (for example, PanLex only has 14 pairs for each source-to-target language selection). This small number makes it hard to get reliable results, which is what stepwise regression needs. So, we decided to go with a standard multiple linear regression model instead. In this model, we use the isomorphism measure that has the strongest individual correlation along with some other linguistic measures. Just like with stepwise regression, we’ll share the overall correlation coefficient, r.",
        "formal_text": "Stepwise regression analysis is not suitable for the selection analysis due to the limited number of language pairs in each language selection setup (e. g. , PanLex offers 14 language pairs for each source-or target-language selection analysis). These conditions impede the statistical significance power of the tests which stepwise regression requires. We therefore opt for a standard multiple linear regression model instead; the regressors include the isomorphism measure with the highest individual correlation combined with the linguistic measures. Similarly to the stepwise analysis, we report the unified correlation coefficient, r.",
        "GPT2_formal_text": "_sigmoid\" ). The first part is the output from the attention layer, and the second part is the output from the hidden layers. Formal: The token representation, x t, is made up of a token's embedding, x e t, plus a softmax layer. Formal: The input for the model is a list of token embeddings, x t. Formal: The output from the projection layer, y t, is calculated using the previous hidden state, y t−1. Formal: If the output is zero or a negative number, it means the attention layer didn't learn anything useful. Formal: If the output is non-zero, it means the model got a negative prediction for a specific token. Formal: The output from the projection layer is treated as the negative representation of the whole token, x t. Formal: The output from the attention layer is calculated based on the previous hidden state, y t−1. Formal: A negative token is just a token that's not the target of attention, like in equation (4). Formal: The attention layer does its thing by predicting the output from the first hidden layer, y t−1, and then the next token, x t. Formal: Formal: Formal: The output from the last hidden layer, y t−1+1, is calculated using the output from the first hidden layer, y t−1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.186.json"
    },
    {
        "casual_text": "So, when people try to assign keyphrases without repeating themselves, they usually look back at what they've already said. Inspired by that, we created a \"target side review context set\" that keeps track of the context of the phrases we've generated so far. By using this context along with an attention mechanism, we can better predict the next phrase. We call this the \"review mechanism.\"",
        "formal_text": "Considering human behavior on assigning keyphrases that review previous phrases to avoid duplicate assignment, we construct a target side review context set which contains contextual information of generated phases. The target context with an attention mechanism can make use of contextual information of generated phrases to help predict the next phrase, which we call the review mechanism.",
        "GPT2_formal_text": "= {}, . . . |\n      Casual: We also use a text classification model that uses a sentence-level language model. This model has an LSTM layer that grabs the hidden features and an LSTM layer that predicts the probability for a specific label. Formal: This model can classify sequences of words using both sentence-level and word-level features, which are combined in a single sequence-level LSTM layer. Formal: We take the sentence-level representations and run them through a max-pooling layer to get the probability distribution, which is kind of like the log probability in Equation 2. Formal: The final probability for a label is calculated using the max-pooling layer and a few other layers, like the last one or the last hidden layer. Formal: We keep doing this with different combinations of the target, label, and the sequence-level LSTM layers until the probability hits a steady state. Formal: Figure 1 shows the hidden representation and the prediction probability for a sequence of words w = [w1, ..., wn] using a GRU-based model. Formal: The probability is calculated using the max-pooling layer and a few other layers, like the last one or the last hidden layer. Formal: The classification layer and the last hidden layer can be thought of as the log probabilities from Equation 2. Formal: Formal: The model isn't fully set up yet. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1439.json"
    },
    {
        "casual_text": "Smart To-Do: Two-Step Process\n\nAlright, so here's how we handle creating To-Do items in two steps. In the first step, we...",
        "formal_text": "Smart To-Do: Two Stage GenerationIn this section, we describe our two-stage approach to generate To-Do items. In the first stage, we",
        "GPT2_formal_text": "ed is a trick to deal with the lack of data by making the input vectors more sparse. The idea is that if you have a part of the input that's not labeled, you can figure out how to remove it from the input by making the vector more sparse. The model then learns to fill in the blanks by doing a simple matrix multiplication, which basically means it's pulling out a bunch of numbers from the whole input. Formal: In the next part, we'll explain the different attention methods, how they work, and how they're tweaked to fit the specific needs of different situations. Formal: Alright, let's break this down in simpler terms.\n\nFor the encoder-decoder setup, we think of the input as a bunch of vectors, which can be shown as a set of vectors. Now, we have a set of numbers, φ_i, that represent the label distribution for this input, and we're using something called the Gaussian distribution to measure how much each label has in common with other labels. Formal: φ_i is the total number of labels, and R_i represents the number of unique labels in the vocabulary. Formal: Here's the equation to explain it all: Formal: The first part of the equation (Equation (1)) says we're using a vector x_i to represent the input, and the second part (Equation (2)) says we're calculating the label distribution for this vector using the Gaussian distribution. Formal: These two parts are both related to the input and the label distribution, and we call them the \"input vector\" and \"label distribution.\" Formal: Finally, the final part (Equation (3)) says we're using a matrix w, which is a matrix with dimensions m by n, and we're multiplying it by φ_i to get the label distribution for the input. Formal: These formulas are all about figuring out the label distribution for a certain input vector x_i. Formal: The first part (Equation (1)) says we're using a vector x_i to represent the input, and the second part (Equation (2)) says we're calculating the label distribution for this vector using the Gaussian distribution. Formal: The third part (Equation (3)) says we're using a matrix w, which is a matrix with dimensions m by n, and we're multiplying it by φ_i to get the label distribution for the input",
        "directory": "acl",
        "filename": "2020.acl-main.767.json"
    },
    {
        "casual_text": "We've tested the translation checker on eight different texts: two novels, two software manuals translated by hand, three software manuals translated using a translation memory tool, and one set of dialogue fragments that was machine-translated. The novels aren't super interesting for checking consistency, mainly because they’re way less repetitive than the manuals, and also because consistency isn’t always a big deal in literary translations. For more details, check out the section \"Ange relationer mellan tabeller\" in Chapter 7, \"Grunder för tabeller.\" SOURCE 1:",
        "formal_text": "We have run the translation checker on eight different texts, two novels, two manually translated software manuals, three software manuals translated with the aid of a translation memory tool, and one MT-translated collection of dialogue fragments. The two novels are not really interesting applications for consistency checking, partly because they are not at all as repetitious as the manuals and partly because consistency is not something necessarily aimed for in literary translation. Mer information finns i avsnittet \"Ange relationer mellan tabeller\" i kapitel 7, \"Grunder för tabeller\". SOURCE 1:",
        "GPT2_formal_text": "* (int * argmax) = argmax: argmax = argmax + argmax_s - 1, while argmax_s + 1 is the log of the sigmoid for the logits from both x and y. Formal: Also, in Equation 2, the first term, argmax_1, is the log of argmax(x), the second term, argmax_2, is the log of argmax(y), and the third term, argmax_3, is the log of argmax(z). Formal: But, if we use SVM instead of logits, we get a different result. The second term, argmax_3, is the log of argmax(z), and the third term, argmax_4, is the log of argmax(x). Formal: From Equation 2, we can see that the logits for x and y come from the same distribution, which makes it possible to use the logits from a single class for each class. Formal: Basically, logits, which are based on the logits for the class, are a pretty good way to represent the class's importance. Formal: If we find a way to reduce the logits, we can use the logits to represent the class's importance directly. Formal: This means we can reduce logits without worrying about the other parameters and focus on the class's importance directly. Formal: Formal: We'll show how this works in section 4. Formal: If we have a classifier, e, we can figure out the importance of each word x in the current conversation by taking its logits. Formal: Formal: We can also figure out the importance of each word in the current conversation by using the logits of the words that were said before it. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "amta",
        "filename": "1996.amta-1.16.json"
    },
    {
        "casual_text": "This part takes in a bunch of vector representations, each one matching up with a single token. These vectors are labeled as d t, and they're in a space with a certain dimension, l. Think of it like each token has its own little vector. For instance, this input could come from the co-attention layer we talked about in Section 3. The way this layer works is...",
        "formal_text": "This layer takes input as a sequence of vector representations corresponding to individual tokens, d t ∈ R l, where l is the given vector dimension. For example, such input can be the output of the co-attention layer in Section 3. The operation of this layer is defined as",
        "GPT2_formal_text": "ing is the hidden state representation that helps reduce the bias caused by the attention mechanism. Basically, the input for equation (2) is made up of a bunch of vectors and their related hidden states. The output from equation (2) is a weighted sum of these vectors, with each vector getting a weight based on how much it contributes to the overall representation. Formal: For a specific word w t, the attention mechanism gives the token representation a_t t, which looks like this: Formal: In Figure 1, you can see how we calculate the attention to a token y t by combining its representation a_t t (w t ). Formal: The input vector for equation (2) is represented by its representations. The attention weights for a token y t are calculated using equations (4) to (6). Formal: The token representation x t is calculated using equations (8) to (10). Formal: To get the attention weights for a token, we use equation (9), which is based on equation (10). Formal: Formal: Next, we calculate the attention weights for the entire input vector x t using equations (11) to (14). Formal: Finally, we calculate the attention weights for the final token representation x t by multiplying the attention weights for each token in the input vector x t by the attention weights for each token in x t. Formal: Figure 1 shows the process for calculating the attention weights for a token y t in an image. Formal: Let’s say we have a sentence x t and its representation y t. The attention mechanism for y t is calculated by calculating the attention weights for all the tokens in x t that are the same length as y t. Formal: Using equations (11) to (14), we can calculate the attention weights for all the tokens in x t that are the same length as y t. Formal: Formal: The attention weights for a token y t can be calculated using equations (5) to (7). Formal: Formal: So, the overall token representation y t is calculated using equation (4). Formal: Formal: We can also simplify the input to calculate the attention weights for a token y t, which is just the combination of the attention weights for each token in x t that are the same length as y t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1237.json"
    },
    {
        "casual_text": "Parameter settings. One cool thing about HAPS is that you don’t need to know how many segments you want beforehand. Instead, you just set how much you care about each level. HAPS is pretty flexible with these preferences, so this general setting is a handy way to adjust the level of detail in your segmentation without having to pick the exact number of segments for each part of the tree. In our case, we kept things simple by setting the preferences evenly across the board, but you could definitely add more specific details if you wanted to.",
        "formal_text": "Parameter settings. An important advantage of HAPS is that it does not require the number of segments in advance. Instead, the user needs to set the preference values for each level. However, HAPS is fairly resistant to changes in preferences and this generic parameter is a convenient knob for fine-tuning the desired granularity of segmentation, as opposed to specifying the exact number of segments at each level of the tree. In this work we set preferences uniformly, but it is possible to incorporate additional knowledge through more discriminative settings.",
        "GPT2_formal_text": "= {q_i, q_j} = {u_i, u_j} y = (q_i, q_j) + (1 − α) T s m t (T s m t − 1 ) + (1 − α) T s m t (T s m t − 2 ) (4) Formal: To combine these two types of knowledge, we pick the one that shows up most often in the pair of data points. Formal: The probability p(y | x) is calculated by adding up the log probabilities of the correct class label and the incorrect one. Formal: If the teacher model's confidence or knowledge is uncertain, we adjust the knowledge by multiplying the probabilities of the correct class label and the incorrect one. Formal: Formal: For a given task T s m t, the correct class label y and the incorrect one x are represented by M(y, x) and M(x, y), respectively. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C14",
        "filename": "C14-1005.json"
    },
    {
        "casual_text": "Vocabulary growth models help us see how words change over time in a specific area by looking at the relationship between different types of words and how often they're used. Macau, being a mix of different languages and cultures, can show us what's important in society based on how people use words. But no one has really looked at how Macau's vocabulary has changed over time—until now.\n\nThis paper is the first to create a big collection of texts from Macau Chinese over time. We used three different models to see how the vocabulary changed in these texts and picked the best one, the Heaps model, to dig deeper. It turns out that the way words change in Macau is closely tied to what's trending in the news, government policies, and everyday life.\n\nTo make sure our method works, we also tested it on texts without any time information, and it still gave us good results. This is the first time anyone has studied how Macau's vocabulary has evolved using such a big collection of texts over time. This research helps us understand Macau's language life better and how it's developed over the years.",
        "formal_text": "Vocabulary growth models can reflect the diachronic change of vocabulary in a certain field by fitting the quantitative relationship between word types and tokens. As a place of multi-language and multi-cultural integration, Macau's vocabulary use can reflect the focus of society, but there is no research on Macau's diachronic vocabulary growth. This paper constructed a diachronic corpus of Macau Chinese for the first time, used three vocabulary growth models to fit the vocabulary changes in the corpus, and selected the Heaps model with the best effect to further analyze the relationship between vocabulary change and newspaper content. The results reflect that the changing trend of Macau vocabulary is closely related to hot news, Macau's policy guidelines and people's livelihood. This research also uses the out-of-order text after removing the text timing information to verify the effectiveness of the method. This is the first study to investigate the evolution of Macau vocabulary based on a large-scale diachronic corpus, which is of great significance for the in-depth understanding of the development of Macau's language life.",
        "GPT2_formal_text": "ing layer. It uses a linear kernel, which has a dimension of d_t and a window size of d_t + d_s. Formal: So, the attention in the output layer is... Formal: 1. It's a linear kernel with a dimension of d_t and a window size of d_t + d_s. Formal: 2. It's a Recurrent Neural Network (RNN) with a dimension of d_t and a window size of d_t + d_s. Formal: 3. It's a weight matrix that's the size of d_t + d_s, where each element has d_t elements. Formal: 4. It's a linear non-linear function with a dimension of d_t and a window size of d_t + d_s. Formal: 5. It's a softmax layer. Formal: 6. It's a feed-forward neural network (FFNN) with a dimension of d_t and a window size of d_t + d_s. Formal: 7. It's a fully connected layer with a dimension of d_t and a window size of d_t + d_s. Formal: 8. It's a softmax activation function. Formal: 9. It's a tanh activation function. Formal: 10. It's a ReLU activation function. Formal: 11. It's an attenuation factor, which is calculated by... Formal: 12. It's an attenuation factor, which is calculated by... Formal: 13. It's an attenuation factor, which is calculated by... Formal: 14. It's an attenuation factor, which is calculated by... Formal: 15. It's an attenuation factor, which is calculated by... Formal: 16. It's an attenuation factor, which is calculated by... Formal: 17. It's an attenuation factor, which is calculated by... Formal: 18. It's an attenuation factor, which is calculated by... Formal: 19. It's an attenuation factor, which is calculated by... Formal: 20. It's an attenuation factor, which is calculated by... Formal: 21. It's an attenuation factor, which is calculated by... Formal: 22. It's an attenuation factor, which is calculated by... Formal: 23. It's an attenuation factor, which is calculated",
        "directory": "ccl",
        "filename": "2021.ccl-1.33.json"
    },
    {
        "casual_text": "In Row 5, we focus only on subtask B, but we work with a set of ten related questions, using their connections to help us out. This gives us a small boost in all the measures we're tracking. Even more importantly, this approach is key to getting better results with the joint models.",
        "formal_text": "Row 5 is a special case where we only consider subtask B, but we do the learning and the inference over the set of ten related questions, exploiting their relations. This yields a slight increase in all measures; more importantly, it is crucial for obtaining better results with the joint models.",
        "GPT2_formal_text": "= [ [ arg1, arg2, ..., argn, ..., argT ], ] + [ arg1, arg2, ..., argT ], ]\n\nBasically, it's about how many arguments you want to give each response. Formal: So, we need to figure out this probability P(r|s) for each response s given each question q, based on the question and the question history Q. Formal: We can simplify this probability by just counting how often each argument shows up in a question. Formal: Let's call the unique argument (α|q) the \"question history.\" Formal: We also need to know the \"question,\" which is a group of questions that the model has already seen. Formal: To handle the challenges we'll face when checking how well the model's responses are doing, we'll use something called masked language modeling (MLM), which is a type of autoencoder (thanks to Hochreiter and Schmidhuber, 1997 for that). Formal: We'll use an MLM to figure out the probability P(r|s), and then we'll look at how the model's answers compare to those answers. Formal: The model's answers will be represented as the vector [arg1, arg2, ..., argT] Formal: We'll train the model using the question's history and the history of the question we're checking. Formal: Finally, we'll calculate the accuracy of the model's predictions by averaging the scores from all the responses. Formal: The accuracy of the model's predictions depends on the question's history, but it's also affected by the response. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1452.json"
    },
    {
        "casual_text": "Let's take a look at the noun phrases with conjunctions in the last three rows of Table 5. The way conjunctions work has been a big topic of debate in syntax for a long time (Johannessen, 1998, among others). Our model shows that when dealing with single nouns connected by conjunctions, it might pick either the first noun (8) or the last one (9). But when it comes to conjunctions involving multiple noun phrases (not just single words), the model always chooses the conjunction itself as the main element. These strategies have all been discussed separately in linguistics, and since our model uses all of them, it seems to share the same confusion that linguists have been dealing with.",
        "formal_text": "We analyze the case where the noun phrase contains a conjunction in the last three rows of Table 5. The syntax of conjunction is a long-standing source of controversy in syntactic analysis (Johannessen, 1998, inter alia). Our model suggests that several representational strategies are used, when coordinating single nouns, both the first noun (8) and the last noun (9) may be selected. However, in the case of conjunctions of multiple noun phrases (as opposed to multiple single-word nouns), the model consistently picks the conjunction as the head. All of these representational strategies have been argued for individually on linguistic grounds, and since we see all of them present, RNNGs face the same confusion that linguists do.",
        "GPT2_formal_text": "ing. At each step t, we check how the word w i relates to the attribute h at step t. The vector h_t for the word w i is calculated using the embedding (e_t) of h at step t. This representation is then fed into a softmax function on the attribute vector, which gives us the probability distribution P(e_t|h_t). Formal: To get the hidden state representation h_t, we start by using the word embedding from the last epoch. After that, we apply the softmax function to the embedding of h at step t, which gives us the representation h_t. Formal: For each input word w i, we calculate the relation vector h_t like this: Formal: Using the hidden state from the last epoch, we calculate the relation vector h_t like this: Formal: Finally, we mix these representations together using a softmax function, giving us the final relation vector P(h_t|w_i). Formal: Finally, we calculate the h_t and h_t for each word w i, then average the values to get the final representation h_t and h_t for the whole sentence. Formal: We explain these steps and how we calculate the hidden state representation h_t and the embedding h_t for the word w i in section 3. Formal: Lastly, we combine the representations from all the words in the sentence into the hidden state representation h_t and the embedding h_t for the whole sentence. Formal: We apply the softmax function to the embedding of h at step t, which gives us the representation h_t. Formal: Finally, the loss function L_h is calculated using the hidden state of the last epoch. Formal: Formal: Lastly, the final distribution P(h_t|w_i) is calculated for the whole sentence using the mixed representation h_t and h_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E17",
        "filename": "E17-1117.json"
    },
    {
        "casual_text": "A lot of the current ways we represent word meanings rely on the distributional hypothesis. This idea, which has been around since the 1950s (thanks, Harris!), suggests that words that show up in similar contexts tend to have similar meanings. Researchers like Turney and Pantel, and more recently Clark, have built on this concept. These representations, often called embeddings, can capture how people naturally think about word similarities and connections. They’ve been super useful in all kinds of NLP tasks, like figuring out word meanings across languages (Mikolov et al., 2013b), analyzing emotions in text (Socher et al., 2013), and even identifying important names in text (Turian et al., 2010; Guo et al., 2014).",
        "formal_text": "Most current models of semantic word representation exploit the distributional hypothesis: the idea that words occurring in similar contexts have similar meanings (Harris, 1954; Turney and Pantel, 2010; Clark, 2015). Such representations (or embeddings) can reflect human intuitions about similarity and relatedness (Turney, 2006; Agirre et al. , 2009), and have been applied to a wide variety of NLP tasks, including bilingual lexicon induction (Mikolov et al. , 2013b), sentiment analysis (Socher et al. , 2013) and named entity recognition (Turian et al. , 2010; Guo et al. , 2014).",
        "GPT2_formal_text": "{ w i | w i }       Casual: We start with some background info about the document (like it's from the text collection), the words in the document (like the words in the title), and the starting sentence (the first word of the document). We use a method called phrase embedding (shoutout to Artetxe et al., 2015) to grab a bunch of sentence embeddings. These sentence embeddings are then turned into a fixed-size vector. Formal: For the multi-class classification task, we create a few different sets of word embeddings for the document, which are then fed into the classification part. Formal: For a specific document d, we use a multi-class classifier F M c ∈ R m×d. Formal: We use a dictionary of words from the collection, which we call D, as the training dictionary. We use a Dirac delta function to measure how similar each word is to its neighbors in the dictionary. Formal: The word embedding vectors we get from the training dictionary are used to get sentence embeddings. Formal: We use a sentence embedding vector as the input for our classifier. Formal: The dataset D, which includes the training dictionary, the document collection, the document collection's word embeddings, the starting sentence, and the classification results for the document, is called D. Formal: To make sure we're focusing on the words in the document, we break down the input document into n parts, making the embedding vectors for each part E, as well as the embeddings for the document collection and the document collection's word embeddings. Formal: We do this by creating a multi-dimensional representation of the document using a multi-layer perceptron, as suggested by Bahdanau et al. Formal: Finally, we plug these three embeddings into our classification part to get the final prediction. Formal: We did some experiments and found that using the word embedding vectors (e i | w i ) makes the classification more reliable. Formal: We also checked out how the word embedding vectors (e i | w i ) affect things by comparing the accuracy (Acc) and macro-F1 (Mean) of classification, using the same settings as before. Formal: For the multi-class classification task, we show how the cross-entropy loss (λ) changes with the classifier F M",
        "directory": "D15",
        "filename": "D15-1242.json"
    },
    {
        "casual_text": "Social media data is usually pretty messy since it’s user-generated. Different researchers have used all kinds of methods to clean this data before feeding it into a machine learning algorithm. We found that the steps they choose can change the size of the data, making it harder to compare results fairly between studies. Table 2 shows some examples of papers using three popular hate speech datasets and the different preprocessing techniques they used, which messes up the ability to compare them properly.\n\nSome studies use different train-test splits, like 70:30 or 80:20, while others do a train-test-validation split, such as 70:15:15, 60:20:20, or 80:10:10. Some even go for 10-fold or 5-fold cross-validation. All these different setups make it impossible to compare studies fairly unless every researcher reruns all the studies they want to compare, which is not only impractical but also super expensive.\n\nSo, we’re pointing out what makes a dataset a good benchmark—something everyone can use to compare their results fairly.",
        "formal_text": "Train-Test Splits Social media data is often very noisy since it is a user-generated data. Different researchers have employed varying steps to clean the data in preparation for an ML algorithm. We show that these choice of steps can affect the data size, therefore obstructing an objective comparison between studies even more. Table 2 shows a few papers using three commonly used hate speech datasets and the preprocessing applied which leads to variations that negatively affect a fair comparison. Some of the existing studies select different train-test splits such as 70: 30 or 80: 20, some do a train-test-validation split of 70: 15: 15 or 60: 20: 20 or 80: 10: 10 while some do a 10-fold or 5-fold cross validation. This varying setting means that fair comparison amongst studies is not possible except if every researcher reruns all existing studies they wish to compare with. This is both impractical and costly. Here, we highlight factors that qualifies a dataset to be considered as benchmark.",
        "GPT2_formal_text": "_len is 0. The model decides whether to generate the next word based on the hidden state h_t, which is also calculated based on the input data. Formal: To create the masked token x_t, we need to create a mask m_t using the model's input and output states. After we have the hidden state h_t and the masked token x_t, we use the following two rules to make sure the masked token x_t only gets generated if the hidden state h_t matches x_t. Formal: The model decides whether to generate the next word based on the hidden state h_t, which is calculated based on the input data. Formal: We're checking if the masked token x_t is generated by the model based on the hidden state h_t, which is also calculated based on the input data. Formal: The hidden state h_t and the masked token x_t are created using the model's inputs and output states. Formal: The output states h_t and x_t are created using the model's inputs and output states. Formal: The model generates the next word based on the hidden state h_t and the output states x_t. Formal: Finally, the masked token x_t gets generated by the model based on the hidden state h_t and the output states x_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "alw",
        "filename": "2020.alw-1.18.json"
    },
    {
        "casual_text": "For the Farsi-to-Arabic translation project, we started by testing the impact of adding reordered Farsi sentences along with their Arabic translations to the original training data, as explained in Section 3.6. Table 4 shows how this new system compares to the baseline, which was trained on the original, unreordered data. About 52% of the sentences were reordered using parse-based rules—we skipped POS-based rules for this. This change bumped up the training data size from 289K to 439K. For the translation part, we used a run-based penalty model for this experiment. The results were pretty good: we saw a solid improvement over the baseline, with a 1.2% boost in BLEU scores and a 0.6% improvement in WER. \n\nFunny enough, when we tried the same thing for Farsi-to-English translation, we didn’t notice much of an improvement. We think the main reason for the better results in the Farsi-to-Arabic case is because of better word alignment. In Farsi, the verb usually comes at the end of the sentence, but in Arabic, it’s often at the beginning. Due to how the alignment model works, this can cause issues. By seeing the same verb in both positions, the alignment algorithm might be able to figure it out better.",
        "formal_text": "For the Farsi-to-Arabic translation task, we first show the effect of adding reordered Farsi training sentences with their target language counterparts to the original bilingual training corpus as described in Section 3. 6. Table 4 compares the baseline system trained on the unreordered corpus with this system. About 52% of the sentences were actually reordered with parse-based rules; we did not use POSbased rules for reordering. Thus, the training corpus size increased from 289K to 439K. In translation, we used the run-based penalty model for this experiment. The improvement in MT error measures over the baseline is substantial: 1. 2% absolute in BLEU and 0. 6% absolute in WER. Interestingly, we did not observe any notable improvement when performing the same experiment for Farsi-to-English translation. We speculate that the main reason for the improvement here is better word alignment quality: because of its sentence-final position, the Farsi verb is often not aligned to its Arabic counterpart in the sentence-initial position due to alignment model restrictions 1. Having seen the same verb in two positions, the iterative alignment algorithm may be able to align it correctly.",
        "GPT2_formal_text": "\" : []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []: []:",
        "directory": "amta",
        "filename": "2010.amta-papers.29.json"
    },
    {
        "casual_text": "The other group has rules to help tell the difference between good and bad stuff. For example, if the answer has words like \"disappointed\" or \"terrible,\" we mark it as negative. E5 is a good example of this kind.",
        "formal_text": "The other group contains the guidelines which aim to distinguish the categories of positive and negative, i. e. , (e) If the answer text contains sentimental expressions to question like \"disappointed\", \"terrible\", and so on, we annotate it as negative. E5 is an example of this type.",
        "GPT2_formal_text": "ing = self.attention_masking + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self.attention_type + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self.attention_type + self.attention_type self.attention_masking = self.attention_masking + self.attention_type + self.attention_type + self.attention_type + self",
        "directory": "D18",
        "filename": "D18-1401.json"
    },
    {
        "casual_text": "Lately, a bunch of NLP studies have been diving into lie detection by gathering datasets and using computer models to spot lies (Hirschberg et al., 2005; Pérez-Rosas et al., 2014; Peskov et al., 2020). However, most of these studies don’t really consider the traditional methods and findings in lie detection, and there’s hardly any follow-up research. This makes it tricky to figure out which datasets are actually good for training models.\n\nTo bridge the gap between machine learning and lie detection research in psychology and linguistics, this study is all about analyzing verbal leakage cues. For the sake of simplicity, we’ll just call them leakage cues from now on. We’re looking at how these cues work in terms of how the data is collected and how well the models perform. We’ve got seven lie detection datasets to play with, and we’re analyzing them using word categories from LIWC2015 (Pennebaker et al., 2015).\n\nThrough this research, we’re trying to answer three main questions:\n1. How does the way we collect data affect strong leakage cues?\n2. What’s the deal with the number and type of strong leakage cues in lie detection tasks?\n3. Do strong leakage cues actually help make the models more reliable?\n\nWe’re hoping that by answering these questions, we can figure out how to build and pick the right datasets for lie detection tasks.",
        "formal_text": "Many NLP studies have recently collected lie detection datasets and detected lies using computational models (Hirschberg et al. , 2005; Pérez-Rosas et al. , 2014; Peskov et al. , 2020); most of these ignore traditional lie detection methods and findings, and have no follow-up studies, making it difficult to know which datasets are suitable for model training. To use machine learning approaches together with lie detection research in psychology and linguistics, and to seek a way to evaluate and select proper datasets, this study focuses on analyzing verbal leakage cues within; leakage cues hereafter indicate verbal leakage cues. We study leakage cues in terms of the data collection method and model performance. Seven lie detection datasets are adopted for experiments. We analyze these datasets using word categories defined in LIWC2015 (Pennebaker et al. , 2015). Through this study, we aim to answer three questions: (1) How do data collection methods affect strong leakage cues? (2) What is the role of the quantity and the category of strong leakage cues in lie detection task? (3) Do strong leakage cues contribute to model validity? We expect these answers to help in the construction and selection of appropriate datasets for lie detection tasks.",
        "GPT2_formal_text": ", which are all the possible id values for the query. We use the same attention layer that the model learns during training to figure out the attention weights for all the entity mentions. Formal: This part talks about the fancy supervised models we’ve got and how well they perform compared to supervised methods. We also talk about how we’ve put them to the test in the task we’re working on. Formal: Here’s the dataset we used to test our models. Table 2 shows the NMT models trained with different setups, like masked entity prediction (Masked), masked entity prediction (Mapped), and span prediction (Span). The entities in the entity pair are tagged as either <f-type>, <e-type>, or <m-type>. \"NMT\" stands for neural machine translation. Formal: We tested all the models in the training set to see how well they could handle new inputs. Formal: We ran experiments on the new test set to check how they handle different types of sentences and how they handle complex tasks. Formal: The results are in Figure 1. We noticed that model training (MDMT) really boosts performance in the tricky <e-type> cases, while model fine-tuning (MT) gives the best results in the <m-type> cases. Formal: Our findings back up the idea that models that can generalize well should be able to handle new tasks. Formal: Formal: We’ve included the datasets from the WMT14 shared task and the CoNLL-2014 shared task, which we used for our experiments. Formal: We’ve included the results for the masked entity prediction model in Figure 1. Formal: In Figure 1, the masked entity prediction model is trained with the same setup as in the CoNLL-2014 shared task, using the same dataset as the model from WMT14. Formal: The results for the span prediction model in Figure 1 are in Figure 2. Formal: Formal: This shows how the model performed with different settings for the span prediction model. Formal: We used the same parameters for the model and the span prediction model in Figure 2. Formal: The results for the masked entity prediction model in Figure 2 are in Figure 3. Formal: We also ran experiments on the new test set from WMT14. Formal: We used the same parameters as the model in Figure 2. Formal: The results",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.370.json"
    },
    {
        "casual_text": "In our approach, the POS predictor adds an extra linear layer after the decoder to create the target sentence, which you can see in Figure 3. Once we're done training, we only use that POS predicting linear layer for making predictions, which helps us get better results for POS sequence prediction.",
        "formal_text": "In our method, the POS predictor uses an extra linear layer after the decoder to generate the target sentence, as shown in Figure 3. After training, we only need the POS predicting linear layer for inference, thus enjoying the better performance for the POS sequence prediction.",
        "GPT2_formal_text": "_v1, ..., attention_mask_vn), for a specific token in the sequence. Formal: This part uses the attention mechanism to combine the context from the source document and the output of the decoder, then adds them up to get the final output. Formal: We also use attention to predict the label for a specific token. Formal: Lastly, we use a weighted linear projection (a method from Xu et al., 2017) to combine the context and the output from the decoder, and another weighted linear projection to combine the input and output from the encoder. The probability of the correct label for a token is calculated using a weighted average of these three projections, weighted by the attention mechanism. Formal: Basically, we can combine the context, the output from the decoder, and the attention to get the final output. Formal: Lastly, we use a weighted linear projection (a method from Xu et al., 2017) to combine the context, the output from the decoder, and the attention to get the final output. Formal: Here's a quick breakdown of each part: Formal: • Attention: We use attention to predict the label for a specific token. Formal: • Weighted Linear Projection: We take the output from the encoder and project it onto a weighted linear matrix. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.467.json"
    },
    {
        "casual_text": "Sequence labeling models have been a big deal in basic NLP tasks like POS tagging, chunking, and named entity recognition (NER). Back in the day, people used statistical methods like Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al., 2014; Luo et al., 2015) with manually designed features and task-specific resources. But with the rise of deep learning, neural models have been crushing it on many sequence labeling tasks (Ling et al., 2015; Lample et al., 2016; Ma and Hovy, 2016). Words and characters are now encoded using distributed representations (Mikolov et al., 2013), and sentence-level features are learned automatically during end-to-end training.\n\nA lot of the current top-performing neural sequence labeling models use word-level Long Short-Term Memory (LSTM) structures to handle global sequence information and a CRF layer to deal with dependencies between labels (Lample et al., 2016; Ma and Hovy, 2016; Peters et al., 2017). On the other hand, Convolutional Neural Networks (CNN) (LeCun et al., 1989) have also been used because they can do parallel computing, making training and decoding faster and more efficient.",
        "formal_text": "Sequence labeling models have been used for fundamental NLP tasks such as POS tagging, chunking and named entity recognition (NER). Traditional work uses statistical approaches such as Hidden Markov Models (HMM) and Conditional Random Fields (CRF) (Ratinov and Roth, 2009; Passos et al. , 2014; Luo et al. , 2015 ) with handcrafted features and task-specific resources. With advances in deep learning, neural models have given state-of-the-art results on many sequence labeling tasks (Ling et al. , 2015; Lample et al. , 2016; Ma and Hovy, 2016). Words and characters are encoded in distributed representations (Mikolov et al. , 2013 ) and sentence-level features are learned automatically during end-to-end training. Many existing state-of-the-art neural sequence labeling models utilize word-level Long Short-Term Memory (LSTM) structures to represent global sequence information and a CRF layer to capture dependencies between neighboring labels Lample et al. , 2016; Ma and Hovy, 2016; Peters et al. , 2017). As an alternative, Convolution Neural Network (CNN) (LeCun et al. , 1989) has also been used for its ability of parallel computing, leading to an efficient training and decoding process.",
        "GPT2_formal_text": "_t = arg max id max c i, j) = arg max id max c i, j          Casual: The part about the words in the text and their possible meanings is kind of cool, but it’s super hard to describe. Formal: Alright, let’s break this down in simpler terms. \n\nFirst, we’re dealing with the sentence we’re trying to figure out, which we’ll call s_i. Here’s what’s going on:\n\n1. s_i is the sentence we’re trying to understand, and w_i is some other sentence we know about.\n2. s_i is a part of a bigger chunk of text, called an M_t.\n3. w_i is also part of this M_t, but we don’t know what it’s talking about yet.\n4. The word w_i we’re looking at is also part of this M_t.\n5. If w_i is a word, we can assume it’s related to s_i, but we don’t know what it’s actually saying.\n6. We’re looking at the word w_i, but we don’t have any context to guide us.\n7. We can’t predict the next word w_i, so we just assume it’s related to s_i and use that to guess the right meaning.\n8. We can also assume w_i isn’t related to anything in the M_t, but we don’t know if it’s related to w_i, s_i, or w_i.\n9. We don’t know the context around w_i or w_i, so we assume it’s related to s_i but don’t know if it’s related to w_i, s_i, or w_i.\n10. We don’t know the order of w_i, s_i, or w_i, so we assume they’re related to s_i but don’t know how they’re related.\n\nSo, in short, we’re trying to understand this sentence and the words in it, but we don",
        "directory": "C18",
        "filename": "C18-1327.json"
    },
    {
        "casual_text": "CDSP methods use different types of encoders: one for handling utterances and another for managing MRs (meaning representations). Utterance encoders create neural representations for both the current and past utterances, whereas MR encoders focus on building neural representations using historical MRs.",
        "formal_text": "Context-aware Encoders Encoders of CDSP methods differentiate between utterance encoders and MR encoders. Utterance encoders construct neural representations for both current and historical utterances, while MR encoders build neural representations based on on historical MRs.",
        "GPT2_formal_text": "ed_max: It uses the convolutional neural network (CNN) to grab the hidden representation of the text, like c_i, from the input text x. We use the word embeddings from a pre-trained GloVe word embedding model as the input for this component. After that, we turn the output vector of the attention layer into a single number. Formal: The attention layer takes the attention vector c_i and turns it into a real-valued vector. Formal: We keep doing this for all the words in the input. Formal: After training, we check how well the model can predict the next word in the input by averaging the attention vectors from the word embedding layer. This gives us the word-level prediction. Formal: Finally, we update the word embedding matrix in the output layer by using the model's prediction and the previous prediction. Formal: We also show how the model performs when we only use the word embeddings (like a single-layer model). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.226.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. The findings are all laid out in Table 2. Applying INLP has a mixed effect on how well the main task performs: it boosts BOW by 1.9%, but brings BWV down by 5.1% and BERT by 5.51%. \n\nNow, GAP TPR, RMSg, which measures how close the true positive rates are for male and female classifiers, takes a noticeable hit. It shows that the rates are getting closer on average: \n- In BOW, it drops from 0.203 to 0.124, a 38.91% decrease.\n- In BWV, it goes from 0.184 to 0.089, a 51.6% drop.\n- In BERT, it falls from 0.184 to 0.095, a 48.36% decrease.\n\nWe also looked at the correlation between GAP TPR for each profession and the percentage of women in those professions. Here's what we found:\n- In BOW, the correlation went from 0.894 before INLP to 0.670 after, a 33.4% decrease.\n- In BWV, it dropped from 0.896 to 0.425, a 52.5% decrease.\n- In BERT, it fell from 0.883 to 0.470, a 46.7% decrease (check Figure 4b for more details).\n\nInterestingly, De-Arteaga et al. (2019) found a correlation of 0.71 for BWV when they used a \"scrubbed\" version of the biographies, meaning they removed all pronouns and names. INLP does way better than that baseline, even while keeping all the gender markers intact. \n\nSo, the big question is: How does trying to make things fair affect how much importance the classifier gives to different words in the biographies? That's what we're analyzing here.",
        "formal_text": "The results are summarized in Table 2. INLP moderately changes main-task accuracy, with a 1. 9% increase in BOW, a 5. 1% decrease in performance in BWV and a 5. 51% decrease in BERT. GAP T P R, RM S g is significantly decreased, indicating that on average, the true positive rate of the classifiers for male and female become closer: in BOW representation, from 0. 203 to 0. 124 (a 38. 91% decrease); in BWV, from 0. 184 to 0. 089 (a 51. 6% decrease); and in BERT, from 0. 184 to 0. 095 (a 48. 36% decrease). We measure the correlation between GAP T P R y, f emale for each profession y, and the percentage of biographies of women in that profession. In BOW representation, the correlation decreases from 0. 894 prior to INLP to 0. 670 after it (a 33. 4% decrease). In BWV representation, the correlation decreases from 0. 896 prior to INLP to 0. 425 after it (a 52. 5% decrease). In BERT representation, the correlation decreases from 0. 883 prior to INLP to 0. 470 following it (a 46. 7% decreases; Figure 4b ). De-Arteaga et al. (2019) report a correlation of 0. 71 for BWV representations when using a \"scrubbed\" version of the biographies, with all pronouns and names removed. INLP significantly outperforms this baseline, while maintaining all explicit gender markers in the input. Analysis. How does imposing fairness influence the importance the logistic classifier attribute to different words in the biography?",
        "GPT2_formal_text": ". To keep the model from overfitting, we tweak the loss for each token to maximize the log loss, which is denoted as L. Formal: To train the token representation, we first create the token representation using the BiLSTM network. After that, we update the model with the updated representation. Formal: Finally, the updated model is passed to a softmax classifier to predict the sentiment. Formal: The goal of sentiment classification is to figure out if a sentence is positive or negative. The model learns to predict the sentiment based on the context (the input features) to create a prediction. We use a dataset of sentences from the English movie review dataset (from Lu et al., 2015). The dataset has reviews for a movie, and each review is labeled as either positive or negative. This dataset is part of the CNNDM dataset, created by Chen et al. (2017). Formal: For our experiments, we use the development set of the CNNDM dataset with a batch size of 50. The model has a hidden size of 128, and the learning rate is set to 0.001. Formal: We also use the development set for training the sentiment classifier. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.647.json"
    },
    {
        "casual_text": "Alright, so here's the deal: If both C1 and C2 can be learned in a structured way (PAC-learnable), and h01 and h02 are kinda helpful in predicting f1 and f2 (weakly useful predictors), plus χ is on point with D, f1, f2, h01, and h02, and has a discrimination score that’s high enough—like at least 4(|Y| − 1)² for 0/1 loss or ≥ 4|V|²(|Y| − 1)² for Hamming loss—and h01 and h02 aren’t related to each other (uncorrelated), then C1 and C2 can also be learned with two-sided hints.",
        "formal_text": "Corollary 1. Suppose C 1 and C 2 are both PAClearnable in the structured setting, h 0 1 and h 0 2 are weakly useful predictors of f 1 and f 2, and χ is correct with respect to D, f 1, f 2, h 0 1 and h 0 2, and has discrimination ≥ 4(|Y| − 1) 2 (for 0/1 loss) or ≥ 4 |V | 2 (|Y | − 1) 2 (for Hamming loss), and that h 0 1 and h 0 2 are uncorrelated. Then C 1 and C 2 are also PAC-learnable with two-sided hints.",
        "GPT2_formal_text": "ed_words = [masked_words1, masked_words2, ..., masked_wordsN] h = precomputation(h_t) h_t = h_t + h_masked_words_num. Formal: In the Encoder-decoder setup, the attention mechanism (like the one from Vaswani et al., 2017) is designed to create attention weights for each token in the input sentence. Formal: We think the token attention mechanism, which is basically a simpler version of the sentence attention mechanism (Vaswani et al., 2017), is more efficient than the sentence attention mechanism. Formal: Since the model is made for a specific task and the attention weight distribution is set beforehand, we don't need to tweak the weights much. Formal: Let's break down the different parts of the model. Formal: For each word w_i, the Encoder-decoder uses an attention mechanism to grab attention weights for each token in the input sentence. Formal: For each hidden state h_t and word w_i, the Encoder-decoder adds a word token attention mechanism (like the one from Vaswani et al., 2017) to create attention weights for each token in the input sentence. Formal: The Encoder-decoder also uses a learning rate to adjust the weights. Formal: The final output representation h_t is made by combining the hidden state h_t and the attention weights from the token attention mechanism. Formal: After that, we use a ReLU activation function to normalize the attention weight distribution. Formal: The ResNet model is designed to take the attention weights from the token attention mechanism. Formal: The Transformer model is designed to learn the attention weights from the token attention mechanism. Formal: Formal: Lastly, we apply a cross-attention mechanism that gets the word token attention mechanism's attention weights. Formal: In Table 1, you can see the attention weights for each token in the input sentence (w_i). Formal: Formal: We used the attention weight distribution for each token to optimize the model. Formal: We also apply a cross-attention mechanism to adjust the attention weights. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "Limitations and Future Plans. Our paper mainly focuses on unsupervised summarization because it's super important for situations where we don't have a lot of data. One thing we haven't done yet is get solid results for supervised summarization, even though our model might work there too. The reason for this is that past studies on supervised summarization don't clearly categorize summaries by length (Yang et al., 2020), which makes comparing results tricky and not really fair (Schumann et al., 2020). This issue is also pointed out by Su et al. (2021), who noticed that the same model can score a few ROUGE points differently depending on the length of the summary it generates. Despite this, we did compare our work with Su et al. (2021) under our conditions and showed that our NAUS model comes out on top when the comparison is fair. For future work, we plan to dive into supervised summarization once we set up a proper and rigorous testing environment, but that's something we'll tackle in another paper.",
        "formal_text": "Limitation and Future Work. Our paper focuses on unsupervised summarization due to the importance of low-data applications. One limitation is that we have not obtained rigorous empirical results for supervised summarization, where the developed model may also work. This is because previous supervised summarization studies lack explicit categorization of summary lengths (Yang et al. , 2020; , making comparisons unfair and problematic (Schumann et al. , 2020). Such an observation is also evidenced by Su et al. (2021), where the same model may differ by a few ROUGE points when generating summaries of different lengths. Nevertheless, we have compared with Su et al. (2021) in our setting and show the superiority of the NAUS under fair comparison. We plan to explore supervised summarization in future work after we establish a rigorous experimental setup, which is beyond the scope of this paper.",
        "GPT2_formal_text": "= self.get_repos_ids()         Casual: We start with two parts of the dataset, D1 and D2. The model gets better as we add more examples, but each example only has one relation to learn from. So, to keep the examples in D1 as small as possible, we use a rule: for every pair (f, e) in D1, we only add one relation. This rule ensures that for any pair (e, f) from D1, the relation r is only used when e is a direct child of f. Formal: We also need to include a relation r and a few child relations as a bonus. To make sure we can use relation r in all the relation-pairs, we follow the approach by Drozdov et al. (2019). Formal: For each relation r, we create a special feature called h_r. Formal: When we add a relation r to a relation-pair, we also add the child relation r. Formal: Formal: We end up with a dataset that has two parts, D1 and D2. The model learns to predict the correct relation r using different relations. Formal: Formal: The model we’re suggesting is based on a few key ideas. Formal: Formal: It’s not a super strict or exact rule, but it’s better than the old approach where we only use one relation for each pair. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.545.json"
    },
    {
        "casual_text": "In this paper, we’ve added a straightforward alignment algorithm to Anymalign to help us figure out where it’s currently falling short. These new alignments make Anymalign’s phrase tables much better, enough to get top-notch results. Along the way, we also came up with a quick and easy method to calculate ITG alignments using any word-level association scores.",
        "formal_text": "In this paper, we complement Anymalign with a simple alignment algorithm, so as to better understand its current limitations. The resulting alignments improve Anymalign's phrase tables to a point where they can be used to obtain state-of-the art results. In passing, we also propose a computationally cheap way to compute ITG alignments based on arbitrary word level association scores.",
        "GPT2_formal_text": "_len = 3 * num_tokens_mask + 1. Keep in mind, the depth of the attention mask needs to match the length of the sequence we're trying to mimic. If the attention mask isn't long enough, the sequence-level representation won't work. This makes the classifier's loss function more complicated and harder to understand. Formal: Alright, so the goal of the task is to find the first token that matches the masked sequence. The model takes the input sequence and turns it into a representation using a hidden layer. The input sequence is taken from a flat array of words, and the output is a matrix that represents the masked sequence. Formal: Following what Guo and Zou (2019) did, we use the l2 norm of this matrix to break down the input into smaller pieces. Formal: This means the model uses a flat, two-dimensional vector to represent the masked token. We also use a softmax function to figure out the probability of the output token. Formal: This way of encoding the tokens directly in the hidden state helps the model tell apart the masked tokens from the actual input words. Formal: This way of encoding the tokens directly in the hidden state helps the model tell apart the masked tokens from the actual input words. Formal: This is also done with a softmax function, but the masked tokens are only used as a context to help estimate the probability. Formal: We’ve tested our method on two big datasets, but we didn’t get a lot of feedback from users about how well it works. Formal: We’re showing how our method works in the next section. Formal: There are a bunch of other methods to learn representations for sequences, like sparse matrix matching, convolutional neural networks, and convolution filters. We’re not diving into those here. Formal: Our method is pretty similar to a neural language model (NNLM), but we’ve added a twist to the word embeddings to include the masked tokens, which is based on studies by Yeh (2014) and Hessel et al. (2015). Formal: We’ve outlined our methods in Figure 1 and also included the basic setup of the model. Formal: The hidden size is a function of the size of the masked token. Formal: We’re testing two versions of our method using two types of hidden states. Formal: The input mask can be either a",
        "directory": "eamt",
        "filename": "2012.eamt-1.62.json"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way:\n\nSo, we're dealing with some kind of system or process here. There are different parts labeled with codes like *c12, *c13, and so on. These parts seem to be interacting with each other in a specific order.\n\nFor example, there's something called OBJ2 that's connected to *c12 and *c13. Then, there's another part called FAf~ that's linked to *el3 and *el40BL. It looks like FAf~ is doing something with these parts, maybe controlling or processing them.\n\nNext, there's FA¢~ which is working with *c14, *ell, and *oont. It seems like FA¢~ is handling these parts and might be outputting something (maybe \"nil\" is the result here).\n\nThen, there's FAOJP' which is connected to *clO, *ell, and (*gf *cont). This part seems to be doing something with these connections and then passing it on to *gf.\n\nFinally, there's a part labeled *i0 and *10 that's linked to ~-VP\". This part is working with *cI0, *ell, *cont, and *outpxcomp. It seems like it's processing these parts and then passing them to NP (*el0 *ell *ontpnp).\n\nAt the end, there's a <- lET that's connected to *el0, *cii, and *ontpdet. This part is working with N (*outpdet *outpnp), which seems to be the final output or result of the whole process.\n\nIn short, this is a bunch of interconnected parts doing different tasks, and they all seem to be working together to produce some kind of output.",
        "formal_text": "Ifun£tional FA(~ (*¢12 *c13 OBJ2 ~ *~) controll FAf~=P (*el3 *el40BL ~ *~) FA¢~\" (*¢14 *ell *oont xcem ~ nil) l i~iAst~ FAOJP' (*clO *ell (*gf *cont) *gf ). *i0) *10) ~-VP\" (*¢I0 *ell *cont *outpxcomp) NP (*el0 *ell *ontpnp) <- lET (*el0 *¢ii *ontpdet) N (*outpdet *outpnp)",
        "GPT2_formal_text": "\" ; $query = \"select id, name from users where name is in the input_ids\" ; return $query; } function get_ids_user( $query, $where_id ) { $user = $query; $where_id = $where_id; } function get_user_by_id( $query, $where_id ) { $user = $query; $where_id = $where_id; } function user_by_query( $query, $where_id, $where ) { $user = $query; $where_id = $where_id; }\n\nThe first part is about getting the user info and the second part is about finding the user by its ID. We use a basic similarity measure to figure out the similarity. The second part uses the user info and the user's info to find the matching user. Formal: The dataset is split into training, validation, and testing sets. Each test set has a random sample of 20,000 sentences, and the rest are the training set. We use the LSTM model for all the training steps, and the dropout rate is set to 0.1. Formal: The training data is split into 3 groups of 5,000 sentences each, and the validation and testing sets have each group of 5,000 sentences each. Formal: We train the model with a learning rate of 0.00001, a batch size of 32, and an initial validation loss of 10%. Formal: We evaluate the model on the validation set using the validation set's data. Formal: We then apply dropout to the model. Formal: The model is tested on the validation set's data using the validation set's data. Formal: We evaluate the model on the validation set's data. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E83",
        "filename": "E83-1009.json"
    },
    {
        "casual_text": "To optimize this objective function while considering the constraints, we need to pick vectors a and b in a way that makes a_i * b_j high when C_xy_ij is also high. Basically, each non-zero entry in the crosscovariance matrix limits the options for the projection directions we can choose. This usually isn't a big deal if the training data is clean, but that's pretty rare, especially with high-dimensional data like text documents. Plus, natural languages are inherently ambiguous, so there's a good chance any document will have some noisy words. Every time a noisy word pops up, it adds to the covariance matrix, making it denser and harder to find the right projection directions.",
        "formal_text": "Maximizing this objective function, under the constraints, involves a careful selection of the vectors a and b such that a i b j is high whenever C xy ij is high. So, every non-zero entry of the crosscovariance matrix restricts the choice of the projection directions. While this may not be a severe problem when the training data is clean, but this is very uncommon especially in the case of high dimensional data like text documents. Moreover, the inherent ambiguity of natural languages increases the chances of seeing a noisy word in any document. Every occurrence of a noisy word will have a non-zero contribution towards the covariance matrix making it dense, which in turn prevents the selection of appropriate projection directions.",
        "GPT2_formal_text": ", attn_mask, attn_size, attn_len) = g_attn_len + h_attn_len. . . + h_attn_len. The attention weights are calculated using the cross-entropy loss function L_attn. Formal: A_attn_len is the length of the input sequence before the first attention head in the output layer. A_attn_len is the length of the output sequence after the first attention head in the input layer. This setup is similar to the multihead attention setup in RoBERTa (Liu et al., 2019). Formal: Similarly, R_attn_len is the length of the input sequence before the first attention head in the output layer. R_attn_len is the length of the output sequence after the first attention head in the input layer. Formal: This allows the model to learn a straightforward representation of the input sequence. Formal: This straightforward representation is then fed into the decoder (as mentioned by Xu et al., 2019). Formal: In this case, the model doesn't have to directly calculate the attention weights because the decoder itself only needs to directly calculate the attention weights. Formal: For example, if a sentence is represented as x_s, the attention weights for the attention heads in the output layer are calculated as: Formal: We'll use the attention weight for an input as w_attn_len, and the attention weight for a token as w_attn_len. Formal: Also, here's a rule where the output layer's output is set to zero: Formal: For example, if the output layer's output is 0, the attention weights for the attention heads in the output layer are calculated as: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D11",
        "filename": "D11-1086.json"
    },
    {
        "casual_text": "We add e_t in Eq. 8 to handle cases where N_g(t) is empty, making it possible to compute the context representation. Think of it as a way to smooth things out when calculating context. After that, we figure out the distance between the head-relation context of t and the orthogonal transform representation of a triple (h, r, t), like this.",
        "formal_text": "We use e t in Eq. 8 to make the computation of context representation possible when N g(t) is empty. This can be viewed as a kind of additive smoothing for context representation computation. Then, we compute the distance of the headrelation context of t and the corresponding orthogonal transform based representation of a triple (h, r, t) as follow.",
        "GPT2_formal_text": "is a list of id-tuples. The ngram-based scoring function for the question and the answer is basically a list of ngrams. Formal: The model gets a score for each response if the question and answer are both in the dataset, and the score is greater than or equal to the average of the responses. Formal: If the question and answer are not in the dataset, the model gets a score of 0. Formal: If the question and answer are in the dataset, but the answer is not in the dataset, the model gets a score of 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.241.json"
    },
    {
        "casual_text": "Our approach falls into the third category of studies, but it’s a bit different because we combine two types of features in a unique way. In our method, internal component features are more important. We’ll show that a model using just these features can work really well on its own. The other type of features is like a helpful extra that can slightly improve the results for certain words. \n\nWe connect the two models by giving each part-of-speech (POS) guess from the first model a credibility score. If a result has a lower credibility score, we flag it and run it through a second step that uses global context to reconsider it.",
        "formal_text": "Our method is among the third type of studies, but is different from the rest in the scheme of combining the two types of features together. In our method, internal component features play a more important role. We will prove that a model based on this type of features alone can perform very well. The other type of features acts as a useful supplement and can improve the results of some words in a certain degree. The two models are linked together by assigning a credibility score for each POS guessing result generated by the initial model. The results with a relatively lower credibility score are identified and put through reconsideration by a method based on global context information.",
        "GPT2_formal_text": "ed.\n          Casual: Here, n stands for the length of the hidden vectors. Formal: The student network is trained on a dataset D t, which is created by combining the training data D t (s) with the corresponding weights from a fully connected layer. The goal is to minimize the cross-entropy loss, which is a formula that looks like this: Formal: We're using a bidirectional RNN-based student model called the Bidirectional RNN-Attention Network (BRAN). This network is designed to learn representations for both the input words (x i ) and the output words (x i+1 ). Formal: Following what Luong et al. (2015a) did, we start with a pretrained RNN-based student that was trained on the same dataset D t as the teacher. Formal: For each input word x i+1 in the input sequence, we generate a representation using the RNN-Attention Network. Formal: This process can be repeated for every input word x i+1. Formal: To deal with the issues of word ambiguity, we suggest using a non-linear activation function, which we call φ(w|x), to help the model adjust to different contexts. Formal: We train a specific classifier using the results from the last round of this process. Formal: We calculate the cross-entropy loss using equation (2). Formal: After we've trained the classifier, we test it on the target dataset D t to see how well it performs. Formal: Once we have the scores from the last round, we use them to update the weights of the previous layer in the student network. Formal: If the loss is above the desired threshold, the layer is prepped and then fine-tuned to minimize the cross-entropy loss, using equation (3). Formal: The final representation for the word x i+1 is then passed into a fully connected layer, which gives us the student's output. Formal: We finish up by applying a linear transformation to the student's output. Formal: We use a non-linear activation function, φ(w|x), to help the student learn representations for both the input words (x i ) and the output words (x i+1). Formal: We start by using the teacher's pre-trained RNN-based student to train a bidirectional",
        "directory": "C08",
        "filename": "C08-1089.json"
    },
    {
        "casual_text": "Chinese abbreviations are made by picking out key characters from the full name. Like with \"北京大学\" (Peking University), the abbreviation is \"北大,\" which comes from the first and third characters. You can think of it as a sequence labeling problem. Check out Table 1 for an example: the abbreviation \"北大\" from the full form \"北京大学\" (Peking University).",
        "formal_text": "Chinese abbreviations are generated by selecting representative characters from the full forms. For example, the abbreviation of \"北京大学\" (Peking University) is \"北大\" which is generated by selecting the first and third characters, see TABLE 1. This can be tackled from the sequence labeling point of view. Table 1: The abbreviation \"北大\" of the full form \"北京大学\" (Peking University)",
        "GPT2_formal_text": ") for each attention head to get the hidden states that represent the query and the article. After that, we can use the learned attention to focus on the key parts of the article that actually contain the answer. Formal: Okay, so here's what's going on in simpler terms:\n\nWe're using a sentence-level BiLSTM with a hidden layer size of 512, and it's using a positional encoding setup. Then, there's a linear layer in between, and it's setting the final output to [0, 1] (so, 1 = 0 and 0 = 1).\n\nFor the query, we're taking the average of the query's attention vectors (which are [0, 1] in the case of a positional encoding), and we're doing this in a hidden layer size of 512. The article is a 300-dimensional vector (using the tanh function), and we're using a linear layer with a dimension of 300 to get it.\n\nThe article's representation is calculated using a combination of the attention vectors, the article's representation, and the attention heads. Formal: In short, we're taking the average of the query's attention vectors, the article's representation, and the attention heads to figure out the article's representation. Formal: The article's representation is a 300-dimensional vector, and we're using a linear layer with a dimension of 300 to get it. Formal: After running through a bunch of layers, we finally get the article's representation, which is a 300-dimensional vector. Formal: The article's representation is a 300-dimensional vector, and we're using a linear layer with a dimension of 300 to get it. Formal: Formal: Formal: Formal:\n\nThe result from the last layer is a 300-dimensional vector. Formal: Then, we've also got the attention vectors (which are [0, 1] in the case of a positional encoding), and this combined vector is turned into a 300-dimensional vector. Formal: This is the final output vector for the sentence-level BiLSTM that's part of the answer-level BiLSTM. Formal: Finally, we're using a 300-dimensional vector to encode the article. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D14",
        "filename": "D14-1147.json"
    },
    {
        "casual_text": "For instance, in the top graph in Figure 5, there's an edge connecting a vowel to a Hebrew character with a really high weight. But when you look at the bottom graph, which shows the results after discriminative training, that same edge now has a weight of zero.",
        "formal_text": "We can see for example, in the top graph presented in Figure 5 an edge matching a vowel to a Hebrew character with a high weight, the bottom graph showing the results of the discriminative training process show that this edge is associated with a zero weight score.",
        "GPT2_formal_text": "_t) → ∆L_r ∈ R m×n = N_r. In this setup, the attention vector is created by combining the output of the RNN at each time step with the hidden state from the last time step, which was passed through the LSTM. Formal: The input and output representation for the RNN are combined using an attention mechanism that learns how to maximize the log probability of the output at each time step. This is done by combining the output from the last time step and the hidden state from the last time step. The hidden state is a vector with m elements that have a dimension of d_t. Formal: This setup allows the network to learn how to generate the next word based on the previous ones. Formal: To make the network work better, we use a linear layer with a ReLU activation function. We use the embedding dimension d_t, which is d_t, to represent the hidden state of the last time step. Formal: In the network, the LSTM has two types of parameters: the global and local hidden states. Formal: We use the hybrid attention mechanism in Equations (4) and (6) to learn the attention vector. This means the network can better adjust to the context and generate text that captures the attention of the user. Formal: In Equations (4), (6), and (7), we take the final hidden state vector h_t as the output and use the attention mechanism to create the output representation t. Formal: We also use the position-wise activation function to adjust the attention vector. Formal: To maximize the log probability of the next word, the network tries to maximize the log probability of the prior word, based on the previous words generated so far, and the distribution of attention weight vectors in the latent space. Formal: We use a linear layer to combine the hidden states. Formal: In Equations (8), (9), and (10), we use the attention mechanism to learn the attention vector and the hybrid attention mechanism to adjust the attention vector. Formal: Formal: We evaluate the network by using the formula: Formal: For the LSTM model, we fine-tune the model using the Adam optimizer (from Kingma and Ba, 2014) with a learning rate of 0.001. Formal: For the word embedding model, we train the model using the cross-entropy loss function, with a",
        "directory": "D08",
        "filename": "D08-1037.json"
    },
    {
        "casual_text": "Also, we check out how our method stacks up against CTC beam search (Graves et al., 2006). Usually, a non-autoregressive model trained with CTC can be decoded in two ways: either greedily or using beam search. Greedy decoding just picks the most likely token at each step, like w*i = argmax wi P(wi|x), and then squeezes those tokens into a sentence using Γ.",
        "formal_text": "In addition, we compare our approach with CTC beam search (Graves et al. , 2006). 6 Typically, a CTC-trained non-autoregressive model can be decoded either greedily or by beam search. The greedy decoding finds the most probable token at each step, i. e. , w * i = argmax w i P (w i |x), and reduces the tokens to a sentence by Γ",
        "GPT2_formal_text": ", you can figure out the rest using the max-pooling thing. Formal: In this section, we talk about how we can use a biaffine model to predict the value of a specific word. Formal: In the first round of experiments, we train the model to match the actual value. Formal: After that, we check how well the model handles situations with different values. Formal: The main idea behind this is that the model should be able to predict the values of words in an input sentence. It's better if it can do this even if the values are totally different from what we see in the sentence. Formal: This is something we can use to learn about the values of words by looking at how often they appear. Formal: To make this work, we think we'll use a specific kind of loss function that focuses on the changes in the words, not just the overall value of the whole sentence. Formal: We're not saying this is the only way to go about it, but it's what we're thinking. Formal: The loss function we're using is called LTF, and we'll explain it in more detail later. Formal: The other loss functions we're using are γLTF, γASLTF, and γLTFASLTF. Formal: We'll also talk about how we can adjust the model to work better with values other than the expected ones. Formal: Here's a quick rundown of these loss functions: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.545.json"
    },
    {
        "casual_text": "In Table 3, we’ve got the results from our pairwise alignment algorithm, comparing it to some baseline methods using 200 text-text recipe pairs from Common Crawl that were aligned by humans. Unlike the text-video alignments, we noticed that the uniform alignment baseline didn’t do better than the textual similarity baselines. This seems to be because the different ways text-text recipe pairs are reordered make alignment a bit trickier. \n\nWhen it comes to the textual similarity baselines, RoBERTa came out on top, just like in the text-video alignment. We think this is because text recipes often use similar vocabulary, which makes it easier to spot matching words between two sets of instructions. On the other hand, video narrators usually use more casual language than the folks writing text recipes, so it’s harder to align things based on word similarities.\n\nSomething cool we found is that both BM25 and RoBERTa have higher recall than our best HMM+IBM1 model, but they fall short in precision. This means retrieval models are great at finding more alignments, even if they’re not always super precise. Our unsupervised HMM+IBM1 model, though, still crushed all the baselines when it came to the F1 score (p < 0.001). \n\nWhen we broke down the HMM+IBM1 model, we saw that using all the words to learn alignments gave us the best results.",
        "formal_text": "On text-text alignments Table 3 shows results of our pairwise alignment algorithm compared with baselines on 200 human-aligned text-text recipe pairs from Common Crawl. Unlike text-video alignments, we find that the uniform alignment baseline does not outperform textual similarity baselines, suggesting that the different re-orderings between text-text recipe pairs makes alignment more challenging. Within textual similarity baselines, similar to text-video alignment, RoBERTa outperforms all others. We believe this is because text recipes tend to share similar vocabulary, making it easier to find similar words between two textual instructions. Video narrators tend to use more colloquial language than the authors of text recipes, making it more difficult to learn alignments using word similarities. Interestingly, both BM25 and RoBERTa get higher recall than our best HMM+IBM1 model but they lose out on precision. This suggests that retrieval models are good for identifying more alignments, albeit with lower precision. Our unsupervised HMM+IBM1 model again significantly outperforms (p < 0. 001) all baselines on F1 score. Under ablations of the HMM+IBM1 model, we again find that using all words to learn alignments performs best.",
        "GPT2_formal_text": ". But, if you focus just on the difference between the outputs of the first and last hidden states, the model can't really tell the difference between these two representations. We need to figure out how to put all these different pieces of information together. Formal: We don't need a fancy math model to make the final call on the values, like the one in equation (1). But the fancy math model does help the model create the final value, which is a good thing. Formal: The main idea here is to separate the values that can be seen as positive and those that can't, based on the chance that a sequence of features will produce the output y = y1, ..., yn. Formal: We want to create a score between 0 and 1 that tells us how likely it is for a pair of consecutive features to produce the output y = y1, ..., yn. Formal: To create this score, we start by calculating the negative log-likelihood for all the sequences. Formal: We're looking to figure out how much of the positive log-likelihood we can take out by reducing the value of the log-likelihood. Formal: Formal: To get the negative log-likelihood, we can use a weighted average of all the features that produce the output y = y1, ..., yn. Formal: In equations (3) and (4), we're assuming that for any y, a(y) will be between 0 and 1. Formal: Formal: So, we can write equation (3) as: Formal: Here, ⊥ is a scalar that shows how much the log-likelihood is weighted, and ⊥ is a scalar that tells us how much we're losing by reducing the log-likelihood. Formal: Formal: In equations (4) and (5), the log-likelihood is figured out by looking at the log-likelihood of the entire sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.440.json"
    },
    {
        "casual_text": "• Hansards (200 sentences): This is a set of sentences that aren't related to the main topic. They're just random ones picked from a standard test set of Canadian Hansards.",
        "formal_text": "• Hansards (200 sentences): out-of-domain set consisting of randomly selected sentences from the standard test set of Canadian Hansards.",
        "GPT2_formal_text": "(in a set called r_i). Formal: For any word w_i in a sentence, you can figure out its attention weight vector θ_w_i by just using the dot product between the words in w_i and the embedding vector of the entire sentence. Formal: This equation basically tells you how much attention each word in w_i gets compared to the embedding of the whole sentence. Formal: We’ve got this whole attention thing figured out using a simple linear program that looks at how often words appear together, and we’ve tested it on a bunch of different datasets. We’ve also included some stats for these datasets to show how well our model works. Formal: We also calculated attention weights for the top-n words in our dataset using the same approach as before. Formal: We call the vector of the top-k words in our dataset w_k_i. Formal: Next, we look at the vectors of the next k words in the sentence. Formal: We calculate these vectors for the top-k words in the sentence using the same algorithm as before. Formal: We also calculate attention weights for the top-k words in the sentence using the same method as before. Formal: For this last part, we’re focusing on the top-k words in the sentence, specifically the k-1 word embeddings, which are the embeddings of the first k words in the sentence. Formal: Lastly, we calculate the attention weights for the top-n words in the sentence using the same method as before. Formal: Our model works by giving each word a weight based on how often it appears in the whole sentence and then multiplying those weights together. Formal: The total weight of all the attention vectors in the sentence is the total of all these weights. Formal: For all the attention weights, we use a simple linear program. Formal: Formal: When we check out the attention weights for the top-k words, we look at their vector representations. Formal: Formal: We use the matrices A_k_i, which are the embeddings of the first k words in the sentence, and A_k_1, which are the embeddings of the first k words in the sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "amta",
        "filename": "2012.amta-commercial.11.json"
    },
    {
        "casual_text": "In this part, we’re looking into how each layer figures out what’s “local.” First, we took a look at how the window size changes as we move through the layers. After that, we dug into the behavior of the first word embedding layer, which turned out to be a bit different from the rest.",
        "formal_text": "In this section, we try to answer how does each layer learn the localness. We first investigated how the window size varies across layers. Then we checked the specific behavior of the first word embedding layer, which is inconsistent with the trend of other layers.",
        "GPT2_formal_text": "_x n ct (i) = [mask_y n ct (i)] / [mask_y n ct (i)]. Formal: The predicted probability P(t|s) of token y t at token s, given the input sequence x s, is calculated based on this. Formal: For each token y t in the input sequence x s, the probability of y t is calculated as the sum of the probabilities of its tokens before and after y t, divided by the sum of all probabilities. Formal: The predicted probability P(t|s) of token y t at token s, given the input sequence x s, is calculated based on this. Formal: Here, \"y\" stands for token y t, and \"s\" is the sequence x s. Formal: Finally, the prediction P(t|s) for the target token y t is calculated by adding the predicted probability P(t|s) for token y t at token s, with the correct label s, and the probability P(t|s) for token y t after token s. Formal: Formal: We use the best model (M) for calculating the prediction P(t|s) for the target token y t. Formal: This is done using the cross-entropy loss function L ct = P(M | s). Formal: Formal: To make sure we don't add any noise from the source, we calculate the loss P(M | s) using the log-likelihood of the actual token y t, which is calculated as the log of P(M | s) divided by the log of P(s|T). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1475.json"
    },
    {
        "casual_text": "As more and more video content gets uploaded online every day, learning stuff from videos, especially for tasks like \"How to,\" has become a big part of our daily lives and work. But watching long videos can be super time-consuming. Right now, there are two main ways tech is trying to make videos easier to handle: video summarization, which cuts long videos into shorter ones, and (dense) video captioning, which creates a text summary of the important parts in the video. For really long videos, dense video event captioning breaks things down into smaller, detailed captions for all the key events. This helps people quickly scan through the video and opens up cool possibilities like creating video chapters or making it easier to search within a video.",
        "formal_text": "With the increase of video data uploaded online every day, the acquisition of knowledge from videos especially for Howto tasks is indispensable for people's daily life and work. However, watching a whole long video is time-consuming. Existing technologies focus on two main research directions to compact video information: video summarization to trim long videos to short ones and (dense) video captioning to generate a textual description of the key events in the video. Typically for long untrimmed videos, dense video event captioning generates fine-grained captions for all events to facilitate users quickly skimming the video content and enables various applications e. g. video chaptering and search inside a video. *",
        "GPT2_formal_text": "\" and the edge weights. This kind of approach works well in areas where there's a lot of data, like in languages with lots of text.\n\nNow, it's not so easy to apply this idea to a new language like German, which has way more words per sentence than English. But it's not too hard to make the algorithm work with other languages that have similar sentence structures. Plus, by focusing on the shortest paths in the shortest paths framework, you can get better results for languages that have more complicated sentence structures. Formal: To make sure we have enough data to train the SRL model, we calculate the number of tokens (n) in each input sentence and then use a threshold to decide when we can stop training. Formal: The SRL model takes the input sentences and the graph G = (V, E) and spits out a list of possible semantic labels. We check each label from left to right. The final label is chosen as the semantic label. Formal: The reason this method is different from the usual beam search is that it considers both the longest path in the shortest paths and the shortest path in the path matrix. Formal: To make sure we don't end up with a bunch of really long paths, we set a minimum number of hops between labels, let's call it L. Formal: The parameters α and β are both set to 0. Formal: For the method using the shortest path, we start with the shortest path and then calculate the total number of hops to create the shortest path. Formal: For the method using the path matrix, the shortest path and the path matrix are combined. Formal: We also calculate the average number of hops over a certain time interval, T, and the frequency of hops with a certain probability, p, for each label. Formal: Using the shortest path gives us a probability p(c|a) based on the label c, a, and the total number of hops we've already calculated. Formal: The total number of hops and the probability p(c|a) for each label are both calculated and normalized using this formula: Formal: Formal: So, the total number of hops and the probability p(c|a) for a label c, a, and the total number of hops we've already calculated are: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.156.json"
    },
    {
        "casual_text": "Even though this approach might seem like it’s not following the usual rules of compositionality, it’s actually pretty straightforward to create a function that combines the meanings of smaller parts to give you the meanings of all these sentences. We can just set it up with different rules for different cases: for example, \"departs~from/\" means \"connect,\" \"departs/from/on\" means \"dday,\" and so on. Hirst tweaks the definition of compositionality to say that \"the meaning of a whole is a systematic meaning of the parts\" (from his work, page 27, with our emphasis added), but he doesn’t really explain what \"systematic\" means.",
        "formal_text": "Although this semantics does look like noncompositional, it is easy to create a function that produces the meanings of all these sentences from the meanings of its parts --we can simply define such a function by cases: the meaning of departs~from/ is connect, the meaning of departs/from/on is dday, and so on. Hirst therefore changes the definition of compositionality to \"the meaning of a whole is a Lystematic meaning of the parts\" (op. cit. p. 27. ; tile emphasis is ours), but without defining the meaning of the word \"systematic. \"",
        "GPT2_formal_text": "ing_tables (P2) and attention_masking_tables (P3). Basically, they take the encoder-decoder setup from the Transformer model and apply it to the attention mapping table, which is a subset of all the attention maps. This subset is created by the attention_tables algorithm in Section 2. If you want to know more about how they do that, you can check out the full explanation in Appendix B. Formal: In this part, we’re talking about the token-wise attention masking matrix P, which is part of the encoder-decoder setup. You can see the attention matrix in Figure 1. In this matrix, the attention masking attention values are related to the word embeddings in the input text. So, if you have a text x = x₁, ..., xₙ, you can calculate the attention masking matrix P(x) using these embeddings. Formal: To make sure that all the attention maps work together properly, we’re creating two types of attention maps—one for text and one for image. Formal: In Figure 1, the attention masking matrix P(x) is the same for both text and image attention maps. Formal: To make sure the attention maps work together effectively, we’re using a global attention mechanism called the global attention mechanism in the Transformer model. This global attention mechanism is what’s been described in the past by Wang et al. Formal: We’ve set up the global attention mechanism using a bidirectional LSTM-based sequence-to-sequence model. Formal: To make sure the attention maps are learned properly, we’re also using a multi-layer perceptron model. Formal: Formal: After creating the attention masking matrix P(x), we’re applying the attention masking function on each token in the input text xᵢ, ..., xᵢᵣ. Formal: We’ll call this entire process of creating the attention masking matrix P(x) \"attn,\" or in simpler terms, the attention mapping matrix P_attn for a specific token x. Formal: Here’s the formula for the attention masking function that we’re proposing: Formal: For each token in a sentence: Formal: Formal: We add an extra layer of attention, let’s call",
        "directory": "C92",
        "filename": "C92-1042.json"
    },
    {
        "casual_text": "We’ve noticed that some parts of morphology don’t really fit well with the transformation-based approach we’re using here. Infixation and templatic morphology are good examples. Even agglutinative systems, which seem like they’d work with transformation rules because they add stuff to the ends of words, can get tricky when you start dealing with more complex morphological processes. For example, in Turkish, the suffixes -lar and -ler seem like they’d need two separate transformation rules, but they’re actually just one morpheme that changes depending on vowel harmony. \n\nThis isn’t a huge deal for breaking words into their parts (morphological segmentation) because -lar and -ler show up a lot and we can still recognize them as separate suffixes. But to really cover all the different ways languages work, we’re going to need a more solid way of representing these processes. We’ll save that for later research, though.",
        "formal_text": "We note, however, that certain morphological phenomena do not readily yield to the transformationbased approach here. Infixation and templatic morphology are obvious examples. Even agglutinative systems, which at first glance appear suitable for transformation rules that operate at word edges, may still prove problematic when more global morphological processes are at play. For instance, the Turkish suffixes -lar and -ler will fall under two distinct transformational rules but are in fact one morpheme that is realized differently due to vowel harmony. This problem does not pose insurmountable problems for the purpose of morphological segmentation since both -lar and -ler are relatively frequent and can be identified as genuine (and distinct) suffixes, but clearly a more robust representation of morphological processes will be necessary to account for the full range of languages. We leave this problem for future research.",
        "GPT2_formal_text": ", which are basically the training set and test set, respectively. Formal: The model is trained using a big sample from the test set, specifically the subset from x_train that includes x_test. The training set is made up of the subset from x_train where the scores are aligned (which basically means they're the same for all labels in the test set). The model is trained using a big sample from the test set, specifically the subset from x_train that includes x_test. Formal: The model is trained using a big sample from the test set, specifically the subset from x_train where the scores are aligned (which basically means they're the same for all labels in the test set). The training set is made up of the subset from x_train where the scores are aligned. Formal: Formal: The model is trained using a big sample from the test set, specifically the subset from x_train where the scores are aligned (which basically means they're the same for all labels in the test set). The training set is made up of the subset from x_train where the scores are aligned. Formal: Formal: The model is trained using a big sample from the test set, specifically the subset from x_train where the scores are aligned (which basically means they're the same for all labels in the test set). Formal: Formal: The model is trained using a big sample from the test set, specifically the subset from x_train where the scores are aligned (which basically means they're the same for all labels in the test set). Formal: Formal: The model is trained using a big sample from the test set, specifically the subset from x_train where the scores are aligned (which basically means they're the same for all labels in the test set). Formal: Formal: The model is trained using a big sample from the test set, specifically the subset from x_train where the scores are aligned (which basically means they're the same for all labels in the test set). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C18",
        "filename": "C18-1005.json"
    },
    {
        "casual_text": "We're using the 460-hour LibriTTS clean sets (Zen et al., 2019) for training our text-to-speech (TTS) model. This dataset is a multispeaker collection of English speech from audiobooks on the LibriVox project. We've trimmed the audio waveforms following the ESPnet recipe (Watanabe et al., 2018). To check the Word Error Rate (WER), we're using the open-source ASR model wav2vec 2.0 CTC 6. For evaluating how natural the synthetic speech sounds, we're relying on the NISQA-TTS model (Mittag and Möller, 2020), which is also open-source.",
        "formal_text": "Dataset and Evaluation Metrics We use the 460-hours LibriTTS clean sets (Zen et al. , 2019), a multispeaker corpus of read English speech from the audiobooks of the LibriVox project, as TTS training dataset. We trim the waveform as ESPnet recipe (Watanabe et al. , 2018). The WER is evaluated by using the open-source ASR model wav2vec 2. 0 CTC 6. The naturalness of synthetic speech is estimated by using the open-source TTS naturalness prediction model NISQA-TTS 7 (Mittag and Möller, 2020).",
        "GPT2_formal_text": "_layers = 2; ctx.c = ctx; h = h - W2V; } else if (l_a) { h = h - l_a; } else if (h_a) { h = h - h_a; } if (h_a) { h = h - h_a; } ctx.c = ctx; h = h - W2V; } else { h = h - l_a; } h = h - l_a; } if (l_a) { h = h - l_a; } h = h - l_a; } if (h_a) { h = h - h_a; } h = h - h_a; } return h; }\n\nIn short, this is a way to model how changes happen over time in a network, using different things like attention, position, and matrix vectors to do it. Formal:\n\n1. First, figure out how things change in the network by looking at the hidden states of the vectors that are affected. Formal:\n2. Then, for each time step, you calculate the weight matrix P_t by using the attention weights for the vectors that are being changed. Formal:\n3. Finally, for the next time step, you calculate the attention weights again using the weight matrix P_t. Formal:\n\nAnd that's it! It's basically a step-by-step process to model how changes happen in a network, focusing on different parts of the network at a given moment, using different weights, and using some special matrices to keep track of how everything's changing. Formal:\n\nSo, in short, it's a pretty detailed way to model how things change in a network, showing how attention changes and how the whole thing is connected. Formal:\n\nAnd yeah, there are other ideas too. Formal:\n\nI know there's a lot of research out there, but this is the most basic one out there. Formal:\n\nSo, the model tries to find the best way to handle the change in the network by taking into account all the different parts of it, including the whole structure. Formal:\n\nSo, it's about finding the best way to change the structure in the network, using a weighted attention model and figuring out the best weights to get the best overall outcome. Formal:\n\nThere",
        "directory": "acl",
        "filename": "2022.acl-long.393.json"
    },
    {
        "casual_text": "Okay, let's break this down in simpler terms:\n\nFirst, \"D i, j\" just means there's a connection between words \"w i\" and \"w j\" in the sentence's dependency tree. \"Sim(•)\" is how we measure the similarity between words. If there's no similarity, we just set \"Sim(•)\" to 0. \n\n\"ξ i, j\" is a tweak factor that deals with how the sentiment (like happy or sad) of an image part doesn't match the sentiment of a word in the text. \"ω(w i )\" is the emotional weight of word \"w i,\" which we get from a thing called SenticNet. If the word isn't in SenticNet, we just set its weight to 0. \"|•|\" is just the absolute value, like turning -5 into 5.\n\n\"a j\" and \"o j\" are the attribute and object of a box in the image, kind of like describing what's inside the box.\n\nNow, inspired by some smart people (Kipf and Welling), we made a graph that connects different types of info (like text and images) without any direction, so it's like A i, j = A j, i. Each part of the graph also has a loop to itself, so A i, i = 1.\n\nThe reason we made this graph (see Equations 7 and 9) is two-fold:\n1) Sometimes, sarcasm in text can be spread across multiple words, like \"wonderful weather\" when it's actually bad. So, we added connections based on the sentence's structure to help understand these situations better.\n2) We also made a special factor, \"κ i, j,\" that adjusts how much we care about opposite sentiments. \"γ\" is a number we can change to control how much we focus on these opposite sentiments, and we usually set it to be more than 1.",
        "formal_text": "Where D i, j indicates that there is a relation between w i and w j in the dependency tree of the sentence. Sim(•) represents the computation of word similarity 2. We set Sim(•) = 0 if the return value is N one. ξ i, j is a modulating factor refers to the sentiment relation (sentiment incongruity) between an image region and a text token. ω(w i ) ∈ [−1, 1] represents the affective weight of word w i retrieved from SenticNet (Cambria et al. , 2020). We set ω(w i ) = 0 if w i cannot be found in SenticNet. |•| represents absolute value calculation. a j and o j respectively denote the attribute and the object of the bounding box j. Inspired by Kipf and Welling (2017), we construct the cross-modal graph as an undirected graph, A i, j = A j, i, and set a self-loop for each node, A i, i = 1. The intention of the cross-modal graph construction (Equations 7 and 9) is that: 1) As in the examples shown in Figure 1, the sarcastic information of text-modality may be expressed by multiple words, such as \"wonderful weather\". Therefore, we incorporate the syntax-aware relations over the dependency tree of the sentence into the cross-modal graph to advance the learning of the contextual dependencies 3. 2) We devise a coefficient κ i, j, which is associated with the affective weights, to modulate the influence of contrary sentiment relations. Here, γ > 1 is a tuned hyper-parameter to regulate the bias of inconsistent sentiment relations.",
        "GPT2_formal_text": "is a list of words, and f_i is the dictionary that describes the i-th attribute. To deal with cases where two items might be connected by attributes, we use a mix of global pooling and local pooling. In our setup, global pooling is applied to the results of local pooling, and we also add a dropout probability. We also add two dropout probabilities: one for each attribute. Formal: So, the whole thing is basically a bidirectional matching problem, where we match the attribute with the word. Formal: Okay, so we're looking for the word or phrase that best matches the attribute in the list. Here's how we do it: Formal: We use the LSTM network to pull out features. Formal: For each word in the list, we do something: Formal: For each word in the list, we do something: Formal: We look for the word that matches the attribute in the list. Formal: For each word in the list, we do something: Formal: For each word in the list, we do something: Formal: For each word in the list, we do something: Formal: We apply the global pooling thing on the output of the LSTM network. Formal: We apply the global pooling thing on the output of the LSTM network. Formal: Finally, we use a dropout probability of 0.5 to handle cases where two words or phrases are connected by the attribute. Formal: Finally, we use a dropout probability of 0.5 to handle cases where two words or phrases are connected by the attribute. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.124.json"
    },
    {
        "casual_text": "First, let’s quickly go over Procrustes Analysis (PA), which is a method to find the best orthogonal mapping between two languages. Then, we’ll explain our approach, called Multi-Pairwise Procrustes Analysis (MPPA), which builds on PA and works for multiple languages. \n\nImagine we have d-dimensional word embedding data from two languages, along with a dictionary that pairs up corresponding words. Mikolov et al. (2013) found that there’s a strong linear relationship between the vector spaces of two languages, and surprisingly, a simple linear mapping works just as well as a more complicated nonlinear one. Xing et al. (2015) took this a step further and showed that making sure the linear mappings are orthogonal (like rotation matrices) helps prevent overfitting and improves results. \n\nTo find this orthogonal mapping, T, we can minimize this cost function:",
        "formal_text": "We first briefly review Procrustes Analysis (PA), a procedure to find the best orthogonal mapping between two languages. We then describe our approach, Multi-Pairwise Procrustes Analysis (MPPA), which extends PA to the multilingual case. Assume we are given d-dimensional word embedding data from two languages along with a dictionary consisting of pairs of corresponding words. Mikolov et al. (2013) showed that there is a strong linear correlation between the vector spaces of two languages and that learning a complex nonlinear neural mapping does not yield better results than with a linear mapping. Xing et al. (2015) further showed that enforcing the linear mappings to be orthogonal matrices reduces overfitting and improves performance. We can learn the orthogonal mapping T by minimizing the following cost function:",
        "GPT2_formal_text": "ing is calculated like this: Formal: This method tries to guess the hidden states of the sequence by using something called a softmax function with a parameter λ. Formal: The model does a better job at predicting the correct label compared to the baseline method. Formal: We also looked at the F1 score (a measure of how well the model is doing) and the average accuracy of the top 10 predictions (called perplexity) for the model. As you can see in Table 4, the model's performance is better than the baseline method when both scores are calculated using a linear model. Formal: We also checked how well the model handles new data by using the perplexity of the original data. Formal: We also calculated the log probability of the correct label, p(r|s), which tells us how likely it is to get the label r from s given the input s. Formal: We calculated the log probability using the perplexity of the entire sequence, which is 0.22 for CNN/Daily Mail, 0.10 for BERT-Base, and 0.19 for BERT-Large. Formal: The model's performance on new data is pretty good, especially when comparing it to the baseline model. Formal: We also checked how well the model handles noise. Formal: In Table 5, we included the accuracy for predicting the correct label for CNN/Daily Mail. Formal: In Figure 1, we show how well the model handles sequence-level predictions and how it compares to BERT-Base. Formal: The model's performance is better than the baseline model on the test set. Formal: We also tested how well the model handles sequence-level predictions and how it compares to BERT-Large on the test set. Formal: The model's performance is better than the baseline model on the test set. Formal: We also tested how well the model handles sequence-level predictions and how it compares to BERT-Large on the test set. Formal: We tested how well the model handles sequence-level predictions and how it compares to BERT-Large on the test set. Formal: Figure 2 shows how well the model handles sequence-level predictions and how it compares to BERT-Base on the test set. Formal: We calculated the log probability of the correct label, p(r|s), for each sequence-level prediction, p(r|s) for each test prediction. Formal: We also calculated the",
        "directory": "D19",
        "filename": "D19-1363.json"
    },
    {
        "casual_text": "To make things sound more natural and varied, we have real people manually turn graph queries into regular questions. We’ve got two ways of doing this, which you can check out in Figure 5. Each query gets sent to a few different people to rewrite it in their own words. Plus, we use different names or ways of saying things for the same entity, which we get from FACC1, to mix it up even more. We’ve got a list of common ways to say each entity, along with how often they’re used. For instance, for \"UnitedStatesOfAmerica,\" you’ve got options like \"us\" (108M times), \"united states\" (44M), \"usa\" (22M), and so on. Lastly, we automatically convert these graph queries into SPARQL queries to find the answers.",
        "formal_text": "In order to ensure naturalness and diversity, we employ human annotators to manually convert graph queries into natural language questions. We manage to provide two levels of paraphrasing (Figure 5). Each query is sent to multiple annotators for sentence-level paraphrasing. In addition, we use different lexical forms of an entity mined from FACC1 for entity-level paraphrasing. We provide a ranked list of common lexical forms and the corresponding frequency for each topic entity. For example, the lexical form list for UnitedStatesOfAmerica is \"us\" (108M), \"united states\" (44M), \"usa\" (22M), etc. Finally, graph queries are automatically translated into SPARQL queries to collect answers.",
        "GPT2_formal_text": "= arg_ids) } t, r = arg_t, arg_r if arg_t is not None: if arg_t is None: if not (t, r) is in D: # The user can't enter if the last answer wasn't in D. return False elif arg_t is None: # The user can't enter if the last answer didn't answer any of D. return False else: return True return (r, t) if arg_t is None: return False return arg_t, arg_r if arg_t is None: return False return False return (t, r) if not None is in D: return False return False return arg_t, arg_r else: return True return (t, r) if not None is in D: return False return False return (t, r) else: return True return False return True return False return None\n\nTable 4 shows the error rates and performance results for the top 10 predictions. The accuracy is based on the percentage of correct predictions (C), with a standard deviation of 10%. Formal: If a question is either unclear or has a long answer, it's not likely to be in the top 10. Formal: If a question is unclear or has a long answer, it's probably in the top 10%. Formal: If a question is unclear or has a long answer, it's likely in the top 10%. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1054.json"
    },
    {
        "casual_text": "We’ve added our stuff to our Lee-BERT framework, and here’s the gist of it:",
        "formal_text": "Our contributions are integrated into our Lee-BERT framework, which can be summarized as follows:",
        "GPT2_formal_text": ". ctx[i] = raw_input(u * u) * (1 - tanh(u * log(u * log(u))) + ctx[i] * w_i. EndSim() EndSim() End(Sim(u * w_i)) End(Sim(u * u)) End(Sim(u * q)) End(Sim(u * q))) End(Sim(u * arg)) End(Sim(u * arg)) End(Sim(u * arg)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End(Sim(u * arg)) End(T)) End(T)) End",
        "directory": "acl",
        "filename": "2021.acl-long.231.json"
    },
    {
        "casual_text": "On Wikipedia, \"category\" basically means tags or topics that group together things related to the same subject. For example, Yao Ming is in the category \"Olympic basketball players of China,\" which tells you what he's about, not what kind of thing he is. Categories can be helpful for figuring out what something is if you don't already know.",
        "formal_text": "• Category refers, in Wikipedia, actually to tags/topics that group entities on similar subjects. For instance, Yao Ming in Wikipedia has category Olympic basketball players of China, which is the topic of the entity instead of its type. Category can be a useful information to infer missing type information.",
        "GPT2_formal_text": "= [i] for i from 1 to n - 1.         Casual: Since the source and target sentences are pretty similar, we can just average out the scores of the translation options. Formal: Here's how we use the average score, adjusted for length: Formal: Here's how we use the average score, adjusted for length: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-1502.json"
    },
    {
        "casual_text": "If we don't consider the LM score, we can calculate everything exactly. We can sort the different translation options for each hyperedge based on their costs. For hypernodes without any predecessors (basically, just words), we can generate these in a straightforward, step-by-step manner. This setup lets us, with the right approach, generate all the options for each hypernode in order of increasing cost.",
        "formal_text": "If no LM score is taken into account, this computation can be carried out in an exact way. The different translation alternatives for one hyperedge can be sorted according to their costs. The derivations in the hypernodes with no predecessors (purely lexical rules), can thus be generated in a monotonic way. This allows, with a proper combination strategy, to generate the derivations in every hypernode in a cost-increasing order.",
        "GPT2_formal_text": "ing_vec*_comp* (s_m, s_n) = s_m + (1 - s_n) * attention_masking_vec* (s_m, s_n) # X's representation in the feed-forward layer. Formal: We’ve got two separate embedding matrices for each of the 10,000 closest neighbors, which we call m_10k and m_10k. This setup helps us figure out how similar a word is to the most similar one in the embedding space. Formal: The attention for the target word W_t is calculated by multiplying the attention for all the neighboring words in the embedding space. Formal: The embedding vectors m_m and m_n are randomly initialized based on a specific condition. Formal: After that, the attention for W_t is used to calculate the score s_t for each word, which we call a_t. Formal: Lastly, we add all these combined representations to get the final representation of the word. Formal: We take the last hidden state vector from the attention layer and feed it into a fully connected layer. Formal: Finally, we use a convolutional neural network to create a feature representation for the target word, using the embedding vectors for m_m and m_n. Formal: Formal: This setup gives us a network structure that looks like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2009.eamt-1.33.json"
    },
    {
        "casual_text": "It'll be a basic search tool for regular users, but behind the scenes, it uses fancy semantic annotations to find audiovisual files more accurately and smartly.",
        "formal_text": "• it will serve as simple search interface for general users, taking advantage of the rich semantic annotation -behind the scenes-for more precise and intelligent retrieval of audiovisual files",
        "GPT2_formal_text": "\" attribute, which basically means that the document representation D_t should be the same as the sequence representation D_s for the sentence s in the sequence-level memory network. Formal: If the weight for an entity is 1, it means the entity's representation should be the same as the sentence's representation, and vice versa. If the weight for a sub-entity is 1, it means the entity's representation should match the sentence's representation, and vice versa. Formal: If the weights for a node and a node pair are 1, then the node's representation should be the same as the sentence's representation, and vice versa. Formal: If the weights for a sub-entity and a node are 1, then the entity's representation should match the sentence's representation, and vice versa. Formal: If the weights for a node and a node pair are 1, then the node's representation should match the sentence's representation, and vice versa. Formal: If the weights for a node and a node pair are 1, then the entity's representation should match the sentence's representation, and vice versa. Formal: If the weights for a node and a node pair are 1, then the entity's representation should match the sentence's representation, and vice versa. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E09",
        "filename": "E09-2014.json"
    },
    {
        "casual_text": "Alright, let's break down what Table 7 is showing us about the comment generation dataset. The table has a few key stats:\n\n1. The total number of samples.\n2. The average number of subtokens in the code.\n3. The percentage of samples where the code has fewer than 100, 150, or 200 subtokens.\n4. The average number of subtokens in the comments.\n5. The percentage of samples where the comments have fewer than 20, 30, or 50 subtokens.\n\nFigure 5 gives us a visual representation of how the number of subtokens in the code (on the x-axis) and the number of subtokens in the comments (on the y-axis) are distributed.",
        "formal_text": "Comment generation. Table 7 shows the statistics of the comment generation dataset. The rows, from top to bottom, are: the number of samples; the average number of subtokens in code; the percentage of samples whose number of subtokens in the code is less than 100, 150, 200; the average number of subtokens in comments; the percentage of samples whose number of subtokens in the comment is less than 20, 30, 50. Figure 5 visualizes the distributions of the number of subtokens in code (x-axis) and the number of subtokens in comments (y-axis).",
        "GPT2_formal_text": "_len = stride;\n       Casual: There are a few ways to handle sparse input. One idea is to make the decoder work directly with sparse data, which is basically a way to \"smooth\" out the data. This means the decoder can focus on more important parts of the data and skip over less important stuff. A straightforward example is using a tableau method to shrink the input, which was introduced by Munteanu in 2002. Formal: Instead of using sparse data, we're using a fancy model called a loss truncation. Formal: Instead of using sparse data, we're using a fancy model called a loss truncation. Formal: We ran some experiments and noticed that the loss truncation works better than regular sparse data. Formal: We also ran some experiments and noticed that the loss truncation works better than regular sparse data. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.339.json"
    },
    {
        "casual_text": "When you run two NLP systems on the same data, you’d expect their results to follow certain rules or patterns. This is like a built-in assumption we have about how they should work. We’re suggesting a method where we use this idea to make one of the systems perform way better. The main concept is to focus on improving it only where the outputs match those expected patterns.",
        "formal_text": "When two NLP systems are run on the same data, we expect certain constraints to hold between their outputs. This is a form of prior knowledge. We propose a self-training framework that uses such information to significantly boost the performance of one of the systems. The key idea is to perform self-training only on outputs that obey the constraints.",
        "GPT2_formal_text": "= [u, v] )       Casual: So, basically, we're using this formula where u(l) = L(u(l) + u(l) − u(l)), and v(l) = L(u(l) + u(l) − v(l)). Formal: So, you take the logarithm of a probability p(l) and then multiply it by the probability p(u(l) + u(l) − u(l)). Then, you apply a transformation where you take the product of u(l) and u(l) and multiply it by the product of u(l) and u(l) again. Formal: So, you take the logarithm of a probability p(l) and then multiply it by the probability p(u(l) + u(l) − u(l)). Then, you apply a transformation where you take the product of u(l) and u(l) and multiply it by the product of u(l) and u(l) again. Formal: Formal: After that, you pick a threshold t and a direction D and apply a function f that takes two inputs and multiplies them by the total number of tokens (n). Formal: After that, you pick a threshold t and a direction D and apply a function f that takes two inputs and multiplies them by the total number of tokens (n). Formal: After that, you pick a threshold t and a direction D and apply a function f that takes two inputs and multiplies them by the total number of tokens (n). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "In this paper, we’re suggesting three different ways to automatically break words into syllables using a bunch of text data. The first method is rule-based, the second one uses sequence labeling, and the third one combines the rule-based approach with sequence labeling.",
        "formal_text": "In this paper, we propose three different corpus-based automatic syllabification methods; (i) Rule-based approach, (ii) Sequence labelling approach and (iii) Hybrid of rule-based and sequence labelling approaches.",
        "GPT2_formal_text": ", and h_i is the attention value at position i. The main thing we're looking at here is the attention weight vector and it's calculated as w_a_i = (h_i * h_i_1 + h_i * h_i_2 + ... + h_i * h_i_n). This attention weight vector is learned from the training data, but we don't actually use it in the final model. Formal: We've got this softmax layer that uses the attention vector for a given word, and it's calculated like this: Formal: The attention weight vector for the word w_a_i is calculated like this: Formal: The attention weight vector for the word w_a_i_1 is calculated like this: Formal: The attention weight vector for the word w_a_i_2 is calculated like this: Formal: The attention weight vector for the word w_a_i_n is calculated like this: Formal: And the final output is just the sum of all these attention vectors. Formal: We're assuming that the vector h_i is linearly dependent on the word w_a_i_1 and w_a_i_2, and we're using a linear kernel function to do it. Formal: In this paper, we use a mix of logistic and bi-linear kernel functions for the attention weights. Formal: We use a mix of logistic and bi-linear kernel functions for the attention weights. Formal: We use a linear kernel function to learn the attention weight vector. Formal: We've got a binary cross-entropy loss for the binary cross-entropy of the attention weight vector. Formal: We've got a binary cross-entropy loss for the binary cross-entropy of the attention weight vector. Formal: We've got a binary cross-entropy loss for the binary cross-entropy of the attention weight vector. Formal: We've got a binary cross-entropy loss for the binary cross-entropy of the attention weight vector. Formal: We've got a binary cross-entropy loss for the binary cross-entropy of the attention weight vector. Formal: We've got a binary cross-entropy loss for the binary cross-entropy of the attention weight vector. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1034.json"
    },
    {
        "casual_text": "The usual ways to combine features in neural networks are adding them together, sticking them side by side (concatenation), or projecting them into a new space. These methods all happen after the features are processed. But for tasks like reading comprehension, where we need to think more interactively, we’d rather combine features earlier on. This way, each part can use what the others have figured out. \n\nFor the QCP model, as we talked about earlier, we only use [CLS] to represent the overall meaning (holistic semantic encoding) of the QCP, not the whole input sequence. There are two reasons for this. First, we believe that focusing on the big picture helps with the interaction between questions and paragraphs. Second, we want to steer clear of the attention deconcentration problem we mentioned before.",
        "formal_text": "The feature fusion methods commonly used in neural networks are addition, concatenation, and projection. All of these modalities are post-fusion mechanisms. However, in order to perform more interactive reasoning in reading comprehension tasks, we want to do feature fusion as early as possible. Therefore, these modalities can benefit from the results of the other modalities. In particular, for the QCP model, as we mentioned above, we use only [CLS] as the holistic semantic encoding of h QCP, but not the encoding of the whole input sequence. There are two reasons for this. First, we think that holistic semantics has essential benefits for the interaction of questions and paragraphs. Second, we need to avoid the effects of the attention deconcentration problem we addressed before.",
        "GPT2_formal_text": ". Without any data, the system should perform just as well as when it has data. Formal: For all the models, we use a script that takes the training text and turns it into a sequence of values for a specific query. This script has a few steps. First, it breaks down the text into tokens and tags each one with the correct attribute values. Then, it uses a function to find the best set of values for the query and assigns each query to one of the different attributes. Formal: For the LSTM model, there are two main steps. First, it runs the query through a feed-forward neural network to get the final hidden states. Next, it uses a max-pooling layer to get the final result. Formal: For the BiLSTM model, the first step is to identify the query. The second step is to create the best values for each query and then add them to the final hidden state. Formal: For the BiLSTM-CRF model, the first step is to identify the query. The second step is to create the best values for each query and then add them to the final hidden state. Formal: The main difference between the CRF model and the LSTM model is that the CRF model uses an attribute-value network to connect the query and the attributes. The LSTM model, on the other hand, uses a max-pooling layer to process the input and assign each value to a specific attribute. Formal: Formal: The big question is: How do we make sure the responses from the dialogue are actually helpful for the user? Formal: We think that the user's questions should really help in answering the question. Formal: We’re working on creating a system that can generate responses that are both informative and relevant to the question. Formal: We’re planning to explore different ways to evaluate how well this system performs. Formal: We’re also thinking about how to make the responses more helpful for the user by looking at both the content and the way the questions are structured. Formal: In Table 4, you can see the results from the LSTM model for the AQUAINT dataset, focusing on Question (Q). Formal: Based on these results, we’re proposing a new approach that uses a dialogue-based reinforcement learning (DRL) model to generate responses for questions in the AQUAINT dataset. Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.241.json"
    },
    {
        "casual_text": "We're using a single Bi-LSTM layer for words, with hidden dimensions set to 128 for CONLL and 256 for ONTONOTES. Both models have a character embedding size of 25, and the forward and backward character LSTMs have hidden dimensions of 50. To avoid overfitting, we add a dropout mask (thanks, Srivastava et al., 2014) with a probability of 0.5 on the input and output vectors of the Bi-LSTM layer. For both datasets, we set the capitalization embeddings to 25 dimensions and trained the models for up to 50 epochs.",
        "formal_text": "Our system uses a single Bi-LSTM layer at the word level whose hidden dimensions are set to 128 and 256 for CONLL and ONTONOTES respectively. For both models, the character embedding size was set to 25, and the hidden dimension of the forward and backward character LSTMs are set to 50. To mitigate overfitting, we apply a dropout mask (Srivastava et al. , 2014) with a probability of 0. 5 on the input and output vectors of the Bi-LSTM layer. For both datasets, we set the dimension of capitalization embeddings to 25 and trained the models up to 50 epochs.",
        "GPT2_formal_text": "ing_sup is set to 1. This means that when we evaluate the attribute, we just turn it into a single number instead of putting it into a complex vector. This makes it easier to work with compared to using complex vectors, like the ones mentioned in studies by Zhu et al. (2017) and Sun et al. (2019). Also, we use the logarithm of the probability to figure out the probability of the output being \"neural,\" meaning the probability is calculated using logarithms. Formal: The loss function for this model looks like this: Formal: In this situation, we've got the output sequence and its hidden representation. The loss function for this model is calculated like this: Formal: So, the loss function is calculated like this: Formal: In this paper, we introduced a new loss function called L_c for the neural encoder-decoder model that's designed for the masked-language model. The main idea behind this loss is to make the representation more sparse. Formal: To train this model, we use a supervised model called mBERT. We also came up with a new loss function called L_d for the masked-language model that works with the masked-language sequence. The main idea here is to use the info from the masked-language part and the input sequence to improve the representations of the masked words. Formal: We tested different loss functions and used the methods from Sun et al. (2019) and Sun et al. (2019) to train this model. Formal: After training, the model gets evaluated using a standard model called MUSE. Formal: Here, f is a special number used to control how much we focus on the masked word representations and how we set the parameters for the masked-language model. Formal: For the masked-language model, we ran experiments using different hidden states and learned the weights for the L_c loss. Formal: Following the approach in Sun et al. (2019), we use the softmax function to predict the output probability and calculate the loss for the masked-language model. Formal: Formal: Lastly, we tested how well the model performs on the development set using cross-entropy loss, which is the method we used in the evaluation. Formal: In the experiments in this paper, we used the official benchmark for evaluation (from Tjong Kim Sang and Buchholz, 2005) for testing. Formal: Formal: To compare the",
        "directory": "C18",
        "filename": "C18-1161.json"
    },
    {
        "casual_text": "If a language limits how its possible LGTs can be structured, LGTs become way more efficient and simpler to work with. The issue we talked about earlier doesn't happen with constituent grammars and phrase structure rules because these rules show hierarchy just by the names they give to the parts.",
        "formal_text": "If a language constrains the structures of its possible LGT's, LGT's become computationally much more economical devices. The problem discussed above does not arise with constituent grammars and phrase structure rules because these rules indicate hierarchy implicitly through the naming of the constituents.",
        "GPT2_formal_text": "= (c_i, q_i) for each document c_i. Formal: In this paper, we talk about how to use fancier unsupervised models to make Word Sense Disambiguation (WSD) better. We tested a few models that are now being used in real-world systems. Specifically, we looked at two models: the SemEval-2009 and SemEval-2010 datasets (Andreas et al., 2009 and 2010). The SemEval-2009 dataset, created by Titov and team in 2009, has 57,000 pairs of senses in the target language and 15,000 in the source language. The SemEval-2010 dataset, by Lin et al. in 2010, has a different number of pairs but includes just one sense in the target language and five senses in the source language. We got these two datasets from SemEval 2010, a big WSD dataset that uses WordNet and was released by the European Association for Machine Translation. Formal: When we tried a big model that uses WordNet, it didn’t help improve WSD. But when we compared a model that only uses WordNet to a simpler model that just uses Wikipedia, we saw a big improvement in WSD. Formal: To make things simpler, we took the Wikipedia pair from SemEval 2010 and used it to train a simple model that only uses WordNet. This simple model performs pretty well on the SemEval-2009 dataset and beats a more complex model that uses WordNet. Formal: To better understand how the Wikipedia-based and simple models compare, we ran some experiments with the Wizard of Wikipedia (Wojcicki et al., 2010). This model has 64,000 pairs of senses in the target language and 15,000 in the source language, all using WordNet. Formal: We used a simple model that only uses WordNet to train a simple model that uses WordNet. Formal: The results show that the simpler model consistently does better than the more complex one. Formal: We also tried a simpler model that uses WordNet to train a simpler one that uses Wikipedia. This simpler one beats the simpler one on both datasets. Formal: The results show that the simpler model consistently does better than the more complex one. Formal: We also tried a simpler model that uses WordNet to train a simpler one that uses Wikipedia. This simpler one beats the simpler one on both datasets. Formal: Formal: We also",
        "directory": "C88",
        "filename": "C88-1056.json"
    },
    {
        "casual_text": "Okay, so EWD is the array where English words are stored after looking them up in the dictionary, and ENG is where they're stored during the analysis. The first five spots in the VOC table hold the vowels U, O, I, E, and A. \n\nNow, if the first letter of the noun we're looking at, which is stored in EWD(1, N), starts with a consonant, then in statement 1476, K will be zero, and in statement 1477, K-1 will be negative. If the first letter is U, K-1 will be zero, and if it's any other vowel, K-1 will be positive. Basically, for any noun that doesn't start with U, statement 1477 decides whether to use \"a\" or \"an\". \n\nBut if the noun starts with U, the choice depends on the third letter. Statement 1910 checks that: if the third letter is a vowel, it picks \"a\", and if it's a consonant, it picks \"an\". \n\nOf course, this system still needs some tweaking for cases where the word starts with \"un\" followed by a vowel. \n\nNow, about machine-word parts: \n4.1. CSW0 (which stands for \"compose word\").",
        "formal_text": "EWD is the array where the English words were brought after the dIB~ionary lookup and ENG where they are brought d~ring the analysis. The first five locations of the table VOC contain the vowels U, O, I, E A. If the first letter of the noun in question, stored in EWD(1, N~, starts with a consonant, K in statement 1476 will be zero, and K-1 in statement 1477 will be negative. If the first letter is U, K-1 =0, and if the first letter is any other vowel t K-1 will be positive. That means, that for any first letter of the noun, except U, statement 1477 decides whether \"a\" or \"an\" will be chosen. In the case of U the choice depends on the third letter of the noun, found in statement 1910. \"A\" will be chosen if the third letter is a vowel, and \"an\" will be chosen if it is a consonant. This routine will, of course, still have to be refined for the case when the prefix \"un\" is followed by a vowel. 4. Machine-word parts 4. 1. CSW0 (compose word)",
        "GPT2_formal_text": "ed_tweaks = { pruned = { pruned_sim_mask }, refined = { refined_sim_mask }, oversampled = { oversampled_sim_mask }, mixed = { mixed_sim_mask }, lookup = { lookup_sim_mask }, prefix = { prefix_sim_mask }, weight = { weight_sim_mask }, alignment = { alignment_sim_mask }, penalty = { penalty_sim_mask }, gap = { gap_sim_mask }, label = { label_sim_mask }, confidence = { confidence_sim_mask }, percentage = { percentage_sim_mask }, rate = { rate_sim_mask }, share = { share_sim_mask }, confidence_sim = { confidence_sim_mask }, share = { share_sim_mask },\n              Casual: This approach just picks an aspect-specific subset for the remaining input, which is often the same as what's in the query. Formal: We’ve set up a system where the model does its thing and then uses a classifier to figure out what’s relevant. We’ve tested two versions of this using MRC as the test set. Formal: For each query, we grab a set of terms that are tied to that query. Then, we toss in a bunch of keywords that are related to that query. Formal: A query is a sequence of words like \"what,\" \"when,\" or \"where.\" We’ve got a set of terms that are related to \"what\" or \"where.\" We toss in a bunch of keywords that are related to \"what\" or \"where.\" Formal: We want the model to pick a query that’s related to the query we’re focusing on, and we’re also trying to pick a query that’s not related to any other query but also isn’t related to any of the other terms in the query. Formal: The output from the model can be a list of terms, a bunch of keywords, or a mix of both. Formal: We’re trying to figure out what’s relevant to a specific query by looking at all the words in the sentence. Formal: The word \"which\" is the key term, and the word \"how\" is the head term. Formal: The model might also pick a query from a list of",
        "directory": "C67",
        "filename": "C67-1017.json"
    },
    {
        "casual_text": "The question will be represented by the controller's state at t = 1. To generate the answer, we'll use a softmax function on a fixed answer vocabulary. This approach makes sense for factoid and list questions since the possible values for a given variable are usually known. For Yes/No and Indefinite knowledge questions, we add {Yes, No, Maybe} to the output options. Following the approach in (Weston et al., 2014), a list-task answer is treated as a single item in the answer set, similar to how count questions are handled. \n\nAn alternative could be swapping the softmax activation function at the MemN2N output for a logistic one and using categorical cross-entropy loss. However, this would require cross-validating a decision threshold to pick the right answers, which is a bit of a hassle. For count questions, the numbers found in the training set are included in the vocabulary.",
        "formal_text": "question will be encoded as the controller state at t = 1. The answer will be produced using a softmax operation over the answer vocabulary that is supposed fixed. We consider this hypothesis valid in the case of factoid and list questions because the set of value for a given variable is often considered known. In the cases of Yes/No and Indefinite knowledge question, {Yes, No, Maybe} are added to the output vocabulary. Following (Weston et al. , 2014), a list-task answer will be considered as a single element in the answer set and the count question. A possible alternative would be to change the activation function used at the output of the MemN2N from softmax activation function to a logistic one and to use a categorical cross entropy loss. A drawback of such alternative would be the necessity of cross-validating a decision threshold in order to select a eligible answers. Concerning the individual numbers for the count question set, the numbers founded on the training set are added into the vocabulary.",
        "GPT2_formal_text": "' array. The input and output are separated by '; ', which is a special token used to break down the data. The input includes the words in the sentence and some syntactic info. We use the Stanford CoreNLP toolkit (Manning et al., 2014) to break down the data. We use the standard hypernym/hyponym parser to find the best possible parse tree for the sentence. For this, we use the Stanford Parser (Klein and Manning, 2003). We evaluate the parses on a development set to see how well they perform. For the feature extraction part, we use a CNN with a linear layer and an attention mechanism, which was introduced by Collobert and his team in 2011. To get word embeddings, we train a convolutional neural network using WordPiece (Klein et al., 2013) and get the word embeddings as individual word vectors. We test different values of the -max pooling parameter to see how they affect the results. We also check out Lattice-LSTM for a more advanced version of the CNN. We train the LSTM model for a set number of epochs, λ, and then we pick the best one using a learning rate decay method. Formal: The features we extracted are organized in a hierarchical structure. The top level has sub-families that have a really strong connection to the sentence and its head. Then, there are sub-families that are related but not directly related to the sentence. For example, \"the monitor\" is a sub-family of \"the woman\", which is the head of \"the woman\". There are also sub-families that are related but not directly related to the head. Formal: The feature extractor takes a sequence of word embeddings as input and creates word embeddings for each word in the sentence. Formal: We use the same method to calculate the scores for both the CNN and LSTM model. Formal: Once the feature extractor gives us a list of feature vectors (u), we use an MLP to create the hidden states for each word in the sentence. Formal: We train the LSTM model using max-pooling and a learning rate decay. Formal: We combine the hidden states from the input feature extractor with the LSTM's hidden states to get the hidden states for the final output of the LSTM. Formal: For the word embedding (u",
        "directory": "E17",
        "filename": "E17-1029.json"
    },
    {
        "casual_text": "In this paper, we're looking at how to model zero pronouns using related text. To do this, we’ve added a cool new self-attentive mechanism that helps our model pay attention to different parts of the text, which is super helpful for understanding zero anaphors. Plus, we use an attention-based method to handle candidate modeling, so our model can focus on the most important bits of the mentions. All of this makes our approach really effective for resolving zero pronouns.",
        "formal_text": "In this paper, we focus on exploring an effective way of modeling zero pronoun by using the associated texts. More specifically, we integrate a novel self-attentive mechanism, which provides our model an ability to focus on multi-aspects text, benefiting the encodings of zero anaphoras. In addition, by employing an attention-based technique for modeling candidates, our model learns to encode more informative parts of the mentions. All these bring advantages to the resolution of zero pronouns.",
        "GPT2_formal_text": "ed_pooling_2_t and attention_masked_pooling_2_c. Formal: The same structure shows up in equation (4). Formal: In equation (4), the output word embedding for the updated position embedding π_i (i, j) comes from a mix of the previous positions and the updated embedding π_i (i, j) itself. Formal: Here, ∆(i, j) and ∆(i, j) are the objective functions, and f(i, j) is the connectivity matrix. Formal: The final state embedding for π_i (i, j) is calculated by the last hidden state of the output, which is x_i. Formal: The final state embedding for π_i (i, j) is also calculated by the last hidden state, which is x_i. Formal: A simple way to use this function is by figuring out the cross-entropy loss (L_c(x)) for the updated position embedding π_i (i, j) using equation (2). Formal: Formal: We also use a multi-layer perceptron (MLP) to figure out the cross-entropy loss for the updated position embedding π_i (i, j). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C18",
        "filename": "C18-1002.json"
    },
    {
        "casual_text": "First off, we’ll talk about the dataset and how we’re measuring success. Then, we’ll go over the settings we used for our experiments. After that, we’ll compare how our method stacks up against other approaches, both feature-based and neural-based ones. Oh, and we also did a case study showing that our SHTCNN is pretty good at pulling out better semantic features.",
        "formal_text": "In this section, we first introduce the dataset and evaluation metrics. Then we list our experimental parameter settings. Afterwards, we compare the performance of our method with feature-based and selected neural-based methods. Besides, case study shows our SHTCNN is an effective method to extract better semantic features.",
        "GPT2_formal_text": "= p j, t ) } ) . This setup is pretty much like the one used in (Mohler et al., 2011) and (Mihalcea and Tarau, 2007), but we’ve tweaked it a bit to handle the definition of t using the inductive hypothesis. Formal: We’re looking at a situation where we’re trying to find the best translation for the input sentence x i, t. The model (F (x i, t ) ) is basically a linear top-down program, where the goal is to find the best translation based on the source language, x i, t, using a set of rules (C). Basically, it’s a translation decision problem, like the one described in (Böhm et al., 1998). Formal: In this situation, we have a source sentence x i, t, which has both a source index and a target index. This setup is a bit different from the usual top-down transfer grammar, but we’re keeping it simple to keep things easy to understand. Formal: We’re using the same approach we used in (Yeh and Hirschberg, 1998), which is a top-down translation logic called LFG with two levels. Formal: We’re building the grammar from scratch using the original idea (Böhm et al., 1998) and tweaking it a bit to match the definition of t. Formal: This way, we can figure out the best translation for the source sentence x i, t using just the source and target indexes. Formal: We’re using the same grammar for a left-to-right parsing process as the one described in (Yeh and Hirschberg, 1998). Formal: We’re using a parser for left-to-right parsing in an incremental left-to-right style, which is kind of like (Yeh and Hirschberg, 1998). Formal: We’re using a left-to-right left-corner parser for the target language, x i, t. Formal: For this part, we’re leaving out the definition of t and just focusing on the rules that make up the system. Formal: We’re using LFG with a left-corner parsing approach and a derivation level of 4. Formal: We’re creating the grammar using the original LFG rules, but we’re removing",
        "directory": "D19",
        "filename": "D19-5533.json"
    },
    {
        "casual_text": "Alright, let's start by looking at how the model spreads out its predictions across all the different possible sequences.",
        "formal_text": "First, consider the sequence-level distribution specified by the model over all possible sequences",
        "GPT2_formal_text": "ed = k d_s for d in c where d_s is either the output of the k-th BiLSTM or the input for the c-th layer. Formal: The BiLSTM is an attention network that uses a sequence of BiLSTMs to predict the hidden representations. Formal: To predict the output vector for a specific word w_i, the BiLSTM's output is basically a combination of three things: the attention weights, the hidden representation of w_i, and a label probability vector called p_w_i. Formal: The output of the BiLSTM can be written as an MLP, where p_w_i is calculated using the attention weights and the hidden representation of w_i. Formal: The whole process of predicting the hidden representation for a word w_i is shown in equation (2). Formal: The probability vector for a label y_i can be written as a one-hot vector, where y_i is calculated as the cross-entropy between p_w_i and p_w_i * c_1, ..., c_k. Formal: The cross-entropy between the attention weights for a word w_i and the output vector p_w_i can be written as a one-hot vector, where the vector p_w_i * c_1, ..., c_k is the cross-entropy between the attention weights and the output vectors. Formal: The cross-entropy between the label probability vector p_w_i and the output vector p_w_i can be written as a one-hot vector, where the vector p_w_i * c_1, ..., c_k is the cross-entropy between the label probability vector p_w_i and the output vector p_w_i. Formal: Finally, to get the final prediction vector, we take the softmax function from equation (4) and multiply it by the cross-entropy. Formal: In this paper, we're focusing on figuring out how important different parts of the input sentence (like words, entities, and phrase structure) are, but we're also trying to predict how likely a word w_i is to appear next in the sentence. Formal: We're treating the sentence as a bunch of words, entities, and phrase structures (let's call them w_i, e_i, p",
        "directory": "D16",
        "filename": "D16-1139.json"
    },
    {
        "casual_text": "1. A way to think about how we come up with a conclusion based on some starting ideas.  \n2. Two different but helpful methods that help us figure out what the conclusion should be, using the information we already have.  \n3. Real-world proof that focusing on what we're aiming for when making a conclusion is really important.",
        "formal_text": "1. A conceptual model of the task of generating an argument's conclusion from its premises. 2. Two complementary approaches that infer a conclusion's target from premises effectively. 3. Empirical evidence for the importance of modeling targets in conclusion generation.",
        "GPT2_formal_text": "= {h_i, h_j}, where h_i, h_j can be one of the different types {chat, message, reply} in the set {M}. Formal: For each of the possible values of x_i, h_i, and h_j, we pick the one that makes the most sense based on the possible labels. Formal: For each of the possible values of x_i, h_i, and h_j, we choose the one that works best for the label h_i, the possible value of x_j, and the possible label for h_j. Formal: Formal: Lastly, we figure out the smallest value of x_i, h_i, and h_j that gives us the best result for the label h_i, the value of x_j, and the value of h_j. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.399.json"
    },
    {
        "casual_text": "Detecting clickbait automatically is super important for websites to clean up their content and make things better for users. The usual way people have been doing this is by using specific features they’ve picked out manually to describe webpages. For example, Chen and his team in 2015 came up with a way to describe news articles using things like how the language sounds (like suspenseful words or numbers being overused), how the sentences are structured (like forward references), and even how the images look (like where they’re placed or if they seem emotional). They also looked at how people interact with the news, like how long they read it, if they share it, or if they comment. They used models like Naive Bayes and SVM to figure out if something was clickbait based on these features.\n\nAnother group, Biyani and friends in 2016, took a different approach. They focused on features from the webpage’s content, like patterns in the words (n-grams), how positive or negative the tone is, the parts of speech used, and how many numbers are there. They also checked how similar the title was to the first few sentences of the article using TF-IDF. Plus, they considered other stuff like how casual the title sounds, if it uses forward references, and even the URL as extra clues.",
        "formal_text": "Automatic detection of clickbaits is important for online platforms to purify their web content and improve user experience. Traditional clickbait detection methods usually rely on handcrafted features to build representations of webpages (Chen et al. , 2015; Biyani et al. , 2016; Potthast et al. , 2016; Chakraborty et al. , 2016; Bourgonje et al. , 2017; Cao et al. , 2017; Indurthi and Oota, 2017; Geçkil et al. , 2018). For example, Chen et al. (2015) proposed to represent news articles with semantic features (e. g. , unresolved pronouns, affective words, suspenseful language and overuse numerals), syntax features (e. g. , forward reference and reverse narrative) and image features (e. g. , image placement and emotional content). In addition, they incorporate users' behaviors on news, like reading time, sharing and commenting, to enhance news representation. They use various classification models like Naive Bayes and SVM to identify clickbaits based on the news and user behavior features. Biyani et al. (2016) proposed to represent webpages using content features like n-gram features extracted from title and body, sentiment polarity features, part-of-speech features and numerals features. They also incorporate the similarities between the TF-IDF features of title and the first 5 sentences in the body. Besides, they consider the informality of title, the use of forward reference, and the URL of webpage as complementary information.",
        "GPT2_formal_text": "ed. The detector looks at how much the input changes as the hidden state changes, but it doesn't notice the difference between the original input and the hidden state. Formal: You can break down the loss L T for each batch of sentences into these parts: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "ccl",
        "filename": "2020.ccl-1.106.json"
    },
    {
        "casual_text": "So, we've got two sets of vectors: l and r, each with n elements. For each pair of corresponding vectors ⃗h_il and ⃗h_ir, we combine them to get a final feature vector ⃗h_if. The way we do this is by using the formula: ⃗h_if = tanh(W_l ⃗h_il + W_r ⃗h_ir + ⃗b).",
        "formal_text": "(1: n) l, h (1: n) r, respectively. Each ⃗ h i l is combined with its corresponding ⃗ h i r for final feature vector ⃗ h i f: ⃗ h i f = tanh(W l ⃗ h i l + W r ⃗ h i r + ⃗ b)",
        "GPT2_formal_text": "= {h t } s t = {h t1, . . . , h tK }, where h t1, . . . , h tK stand for the i-th words in the sentence. Formal: We train a Transformer-based NMT model using only the validation set. The model gets trained for three epochs, each time using the same dataset D. Formal: We also train a pairwise classifier using all the training data, but we skip the validation set to save time. Formal: We train a pairwise classifier using all the training data, but we skip the validation set to save time. Formal: To get a better understanding of the proposed model, we train a multi-class classifier using both the validation set and the test set. Formal: We train a multi-class classifier using all the training data, but we skip the validation set to save time. Formal: Lastly, we test our model on the test set using the validation set. Formal: We train a multi-class classifier using all the training data, but we skip the validation set to save time. Formal: We train a multi-class classifier using all the training data, but we skip the validation set to save time. Formal: Formal: Formal: We also use the BLEU-4 and TER scores as evaluation metrics. Formal: We train a multi-class classifier using all the training data, but we skip the validation set to save time. Formal: Formal: Lastly, we test our model on the test set using the validation set. Formal: Formal: Formal: We also use the BLEU-4 and TER scores as evaluation metrics. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1070.json"
    },
    {
        "casual_text": "To check how well we can spot mistakes in captions (like extra or missing info), we picked a random bunch of data and went through it by hand to find the real extra stuff (called E C) and the actual missing bits (called O C) in each caption. We then used a method based on the average co-2 https: //github. com/tylin/coco-caption sine similarity to compare the errors our system found (a C i⊥ &g i⊥) with the actual errors. Figure 3 shows two examples of this process. \n\nIn the examples, red highlights (like \"on a table\") mark extra details that shouldn’t be in the caption, while green highlights (like \"mashed potatoes\") show things that are missing from the caption but are in the image and the reference caption. We noticed that the errors our system found are pretty similar to the real ones in both examples (with a similarity score of 0.65 or higher). This tells us that our method can effectively catch both extra and missing information in image captions.",
        "formal_text": "In order to measure the effectiveness of error identification (i. e. , extraness and omission), we randomly sampled a subset of data, and manually identify the actual extraness (i. e. , E C ) and true omission (i. e. , O C ) of each candidate caption. We conduct validation based on the average co-2 https: //github. com/tylin/coco-caption sine similarity between the machine-identified error (i. e. , a C i⊥ &g i⊥ ) and true error description. Figure 3 provides two illustrative examples of the validation process. Phrases highlighted in red (e. g. , \"on a table\") are extra information (more text in the candidate caption than in the ground truth). Meanwhile, phrases in green (e. g. , \"mashed potatoes\") are missing from the candidate description, but occur in the image and the reference caption. We observe that machine-identified errors are highly similar to the true error information in both cases (≥ 0. 65). This result suggests that our method can capture extraness and omission from an image caption.",
        "GPT2_formal_text": "ing (M_s). Formal: Sure! Here's the informal version:\n\nTo see how a model handles different situations, we trained it with different amounts of examples from the training set:\n\n- **1 million**: Just the first 100,000 examples.\n- **20 million**: The first 2000 examples.\n- **50 million**: The first 5000 examples.\n- **100 million**: The first 2000 examples.\n\nWe used the same threshold for both datasets: 0.0001.\n\nWe also trained the model for a few rounds with different random seeds:\n- **100 million**: 10 rounds with a random seed of 0.0001.\n- **50 million**: 10 rounds with a random seed of 0.00001.\n\nWe kept all the parameters the same except for the batch size.\n\nWe ran the experiments 5 times and reported the average results for all 5 runs. Formal: The model doesn't do as well in low-resource scenarios. But that's okay because it can still figure out a high-level representation of the text. Formal: We also noticed that the model doesn't perform as well in low-resource scenarios. Formal: We tried running the model with a batch size of 50, but it didn't help improve performance. Formal: The model shows some serious issues when it comes to handling low-resource tasks. Formal: These issues make it hard for the model to figure out important details from small chunks of text. Formal: For our experiments, we made sure to include the source text and a short reference summary in the reference summary, so the model can still properly use those details to build a general representation for the text. Formal: Formal: In our experiments, we tried out different sentence-level representations. For each sentence, we looked at: Formal: If the sentence is at least 1 word long, we use a contextualized sentence representation. Formal: If the sentence is at most 1 word long, we use a sentence representation based on the previous sentence. Formal: For sentences that are no more than 5 words, we use a sentence representation based on the source text, not just the sentence itself. Formal: We also tested some sentence-level representations by combining source and reference sentences. Formal: These representations are adjusted based on the length of the source sentence and the reference sentence. Formal: Formal: Lastly, we tested the model using sentence-level",
        "directory": "D19",
        "filename": "D19-1156.json"
    },
    {
        "casual_text": "KWPSI looks pretty similar to the traditional SI, at least when it comes to its overall appearance.",
        "formal_text": "The general appearance of KWPSI is close enough to the appearance of SI of traditional type.",
        "GPT2_formal_text": "= {v_i for i in v} v_j = {v_j for j in v} Formal: The gradient of the cross-entropy loss is calculated using a tanh function with a temperature τ. The temperature τ helps balance how much importance the actual loss should have. This idea has been looked at by folks like Gao et al. (2017), Hu et al. (2017), and Xu et al. (2017). Formal: The gradient of the cross-entropy loss is calculated as: Formal: Here, v_i and v_j are two vectors. v_i is the representation from the i-th layer, while v_j is the representation from the j-th layer. Formal: The cross-entropy loss is also called the cross-entropy variation of the loss. Formal: The cross-entropy loss is basically a variation of the loss, where the actual loss is the one the model learns. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "A83",
        "filename": "A83-1023.json"
    },
    {
        "casual_text": "First, let's talk about how the GTAHS attaches language-specific function words to a main word. This can cause some overlap between word alignments and the way sentences are structured because many of these function words are already connected to a main word within a part of the sentence (like how the English word \"the\" is part of a noun phrase, or NP). \n\nBut here's a bigger problem: the GTAHS can create fake ambiguities. We know that words can have multiple meanings when translating, like how the English word \"bank\" can mean different things and each meaning has a different Chinese word. But the GTAHS adds extra confusion that, in our opinion, could mess up Machine Translation (MT) if it tries to use these as translation rules. \n\nTake this example: the Chinese word 苹果 is linked to six different English phrases (the connected parts are underlined):",
        "formal_text": "First note that, by attaching language-particular function words to a host, the GTAHS creates redundancies between word alignments and syntactic structures since many of these function words have already been associated with a host within a constituent in the parse tree (e. g. , the English determiner the is placed inside the projection of its host, namely an NP). A more serious issue is that the GTAHS creates spurious ambiguities. Lexical ambiguity is inevitable in translation. For instance, the English noun bank has more than one lexical meaning and each of the meanings corresponds to a different Chinese word. That fact aside, the GTAHS creates spurious ambiguities, which, in our view, would be harmful to Machine Translation (MT) if extracted as translation rules. Consider the following example, where the Chinese noun 苹果 is aligned to six English strings (aligned elements are underlined):",
        "GPT2_formal_text": "ed_tokens = [x_tokens1, ..., x_tokensL] embedding_tokens = [E_tokens1, ..., E_tokensL]\n           Casual: So, we can figure out the attention mask by checking if the token \\( o_t \\) gets cut off at a specific token \\( w_t \\) in the sequence \\( {x_t1, ..., x_tL} \\). The attention mask \\( \\hat{M}_t \\) is calculated using something called the attention vector \\( \\hat{M}_t \\), which is a matrix made up of four elements, where each element is a number between 0 and 1 that shows how much attention the token \\( o_t \\) pays to the token \\( w_t \\). Formal: A lot of these attention masks are set up so that the attention vector \\( \\hat{M}_t \\) covers the entire sequence. Formal: In this project, we focus on creating a model that does better than the current top model, BART, at finding keyphrases. Formal: To make sure our model can handle different types of information, we also use a random walk to gather examples from a bunch of different areas. Formal: For the last two rounds, we use the same approach for updating the attention vectors. Formal: Formal: Formal: Lastly, we use a gated unit (like the one by Kipf and Welling, 2017) to keep the vector vector \\( \\hat{M}_t \\) and the token vector \\( \\hat{M}_t \\) fixed, so the model can learn different types of attention at the same time. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C14",
        "filename": "C14-1143.json"
    },
    {
        "casual_text": "We highlight how crucial it is to calculate the CHAIR metric for both models and human-generated text. We then use this metric to do a deeper, more qualitative analysis to really get a handle on what's going on with the results.",
        "formal_text": "• We show the importance of computing the CHAIR metric on models' and humans' text, and use this metric to guide a qualitative analysis to better understand the results.",
        "GPT2_formal_text": "(g_i) = {g_i[i]}; f = argmax(D_P(T)) * argmax(D_Q(T)) * argmax(D_M(T)) * argmax(D_S(T)) + argmin(0, argmax(0, argmin(0, 1), argmin(1, argmax(1, argmin(0, 2))) + argmin(1, argmax(1, argmin(0, 3))) + argmin(2, argmax(2, argmin(0, 4))) + argmin(3, argmax(3, argmin(0, 5))) + argmin(4, argmax(4, argmin(0, 6))) + argmax(5, argmax(5, argmin(0, 7))) + argmax(6, argmax(6, argmin(0, 8))) + argmax(7, argmax(7, argmin(0, 9))) + argmax(8, argmax(8, argmin(0, 10))) + argmax(9, argmax(9, argmin(0, 11))) + argmax(10, argmax(10, argmin(0, 12))) + argmax(11, argmax(11, argmin(0, 13))) + argmax(12, argmax(12, argmin(0, 14))) + argmax(13, argmax(13, argmin(0, 15))) + argmax(14, argmax(14, argmin(0, 16))) + argmax(15, argmax(15, argmin(0, 17))) + argmax(16, argmax(16, argmin(0, 18))) + argmax(17, argmax(17, argmin(0, 19))) + argmax(18, argmax(18, argmin(0, 20))) + argmax(19, argmax(19, argmin(0, 21))) + argmax(20, argmax(20, argmin(0, 22))) + argmax(21, argmax(21, argmin(0, 23))) + argmax(22, argmax(22, argmin(0, 24))) + argmax(23, argmax(23, argmin(0, 25))) + argmax(24, argmax",
        "directory": "acl",
        "filename": "2021.acl-srw.11.json"
    },
    {
        "casual_text": "Deep reinforcement learning (RL) has been used in a bunch of cool stuff, like computer games, text-based games, and controlling robots (Mnih et al., 2015; Narasimhan et al., 2015; Kimura, 2018). But there’s a catch: these methods need a ton of practice runs to figure out the best moves, and even then, the final strategy is super hard for humans to understand. Why? Because all the smart decisions are locked away in a deep neural network, which is kind of like a black box—you know it works, but you can’t see how.\n\nThis becomes a big deal when someone wants to use RL for real-world problems and actually check if the rules make sense. If the rules were clear and easy to tweak, people could adjust them or set limits as needed. Representing the rules in a symbolic or logical way could help with both understanding and faster training, but traditional training methods struggle to learn these logical rules effectively.",
        "formal_text": "Deep reinforcement learning (RL) has been successfully applied to many applications, such as computer games, text-based games, and robot control applications (Mnih et al. , 2015; Narasimhan et al. , 2015; Kimura, 2018; . However, these methods require many training trials for converging to the optimal action policy, and the trained action policy is not understandable for human operators. This is because, although the training results are sufficient, the policy is stored in a black-box deep neural network. These issues become critical problems when the human operator wants to solve a real-world problem and verify the trained rules. If the trained rules are understandable and modifiable, the human operator can control them and design an action restriction. While using a symbolic (logical) format as representation for stored rules is suitable for achieving interpretability and quick training, it is difficult to train the logical rules with a traditional training approach.",
        "GPT2_formal_text": "_tweaks_4)\n     Casual: The first approach uses the input embedding, h_i, to figure out the output hidden states, h_i'. As for the second one, it takes h_i, adds some extra information (like the output word or phrase), and then uses a neural network to figure out the hidden states, h_i'. Formal: The word embedding, h_w_i, for the last layer of the RNNG is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.283.json"
    },
    {
        "casual_text": "We looked at how different paraphrasing methods create diverse paraphrases and shared the results for two key datasets in Table 3. (We’ve put all the other datasets in Appendix B because there wasn’t enough space here.) For each method and dataset, we calculated some metrics using unlabeled sentences and their paraphrases. \n\nNow, when it comes to measuring how diverse the paraphrases are, the usual BLEU score used in Neural Machine Translation isn’t great for this purpose (Bawden et al., 2020). So, we went with the bi-gram diversity (dist-2) metric suggested by Ippolito et al. (2019). This metric counts how many unique 2-grams there are and divides that by the total number of tokens. We also checked the average similarity within each set of sentences using the Universal Sentence Encoder as a separate way to compare sentences.\n\nThe results showed that paraphrases made with back-translation were way too similar to each other, meaning high sentence similarity but low bi-gram diversity. On the flip side, DBS created more diverse sentences with less similarity. And guess what? Our masking strategies made this even better, boosting the diversity even more. The diversity we measured here was closely linked to how well the intent detection task performed on average, as shown in Table 4.",
        "formal_text": "We evaluate the diversity of paraphrases for each method, and report results for two representative datasets in Table 3 (due to space limitations, the report for all datasets is given in appendix B). For each paraphrasing method and each dataset, metrics are computed over unlabeled sentences and their paraphrases. To assess the diversity of paraphrases generated by the different methods, the popular BLEU metric in Neural Machine Translation is a poor choice (Bawden et al. , 2020). We use the bi-gram diversity (dist-2) metric as proposed by (Ippolito et al. , 2019), which computes the number of distinct 2-grams divided by the total amount of tokens. We also report the average similarity (denoted use) within each sentence set, using the Universal Sentence Encoder as an independent sentence encoder. Results show that paraphrases obtained with back-translation are too close to each other, resulting in a high sentence similarity and low bi-gram diversity. On the other hand, DBS generates more diverse sentences with a lower similarity. Our masking strategies strengthen this effect and yield even more diversity. The measured diversity strongly correlates with the average accuracy of the intent detection task (Table 4 ).",
        "GPT2_formal_text": "[j] = {p_i, a_i} from the set P_u(u, j).\n\nIn this setup, each (p_i, a_i) is a part of the distribution P_u(u, j), which has a bunch of possible values for each i and j. For each p_i, a_i, the feature functions can either be the actual label (p_i, a_i) or the combination of all the labels (p_i, a_i, P_u(u, j)). For instance, in the dataset D_test, each p_i, a_i is linked to a vector p_i. Formal: The output from the POS tagging module is figured out using the trigram model. For each word w_i in the input text, the POS tag for w_i is calculated using the word embedding model. Formal: The output from the POS tagging module is calculated using the word embedding model. Formal: The output from the POS tagging module is calculated using the word embedding model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.191.json"
    },
    {
        "casual_text": "We wanted to see how well BERT can predict transitive IS-A relationships, so we tested it on a bunch of cases. Imagine we have three word senses: A, B, and C, where A is a B, and B is a C. We checked how often BERT could correctly predict that A is a C, if it already got A is a B and B is a C right. We looked at 666 of these transitive triples and put the results in Table 2.\n\nIn the table, p(AB) shows the percentage of times BERT correctly predicted A is a B out of the 666 pairs. Same goes for p(BC) and p(AC). p(AC|AB, BC) tells us how often BERT got A is a C right, given that it already predicted A is a B and B is a C correctly. The table shows that when BERT says A is a B and B is a C, it’s right about A is a C 82.4% of the time.\n\nBut since it doesn’t always get A is a C right, even when it’s right about the other two, it seems like BERT isn’t always making logically consistent predictions.",
        "formal_text": "We quantify BERT's prediction ability for transitivity by measuring how often BERT makes logically consistent predictions for IS-A relations. Specifically, suppose word senses (A, B, C) form the following transitive IS-A relations: A is-a B is-a C. We measure how often BERT correctly predicts the IS-A relation (A, C) given that it correctly predicts (A, B) and (B, C). Table 2 shows the accuracy scores for the 666 transitive 3-tuples. In the table, p(AB) denotes the percentage of cor-rectly predicted (A, B) in the 666 (A, B) pairs. Similar definitions apply to p(BC) and p(AC). p(AC|AB, BC) denotes the percentage of correctly predicted (A, C), given that (A, B) and (B, C) are correctly predicted. The conditional probability in Table 2 indicates that when BERT predicts that A is-a B and B is-a C, it correctly predicts that A is-a C 82. 4% of the time. That A is-a C is not always predicted correctly (given that BERT correctly predicts A is-a B and B is-a C) suggests that BERT lacks the ability to make logically consistent predictions.",
        "GPT2_formal_text": "ing_len, which we'll call m_m for short. The hyperparameters for our model are based on the ones you can find in Table 1. Formal: Basically, the model has a hidden state size of m and can be trained using a fixed learning rate, η. Formal: The model learns a softmax function for the output distribution, which is η_softmax. Formal: Each input token x_i has a number l_i, and a sequence of input tokens x_i has a length of l_i. Formal: Formal: Also, the model has an input bias term φ_i, which is the same as the input bias term x_i. Formal: The objective function for the model is a nonlinear function called the forward-backward non-linearity. Formal: Formal: Formal: Finally, we use a linear layer that has a size of d_r and a nonlinearity layer that has a size of d_r. Formal: Formal: Formal: Formal: This setup helps us pull out multi-view features from the input and output distributions. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-short.11.json"
    },
    {
        "casual_text": "This tool creates a short, clear, and useful summary of a scientific paper, covering the main points. You can either ask it to focus on a specific question or just get a general overview. Scientific papers can be tricky—they're usually long, organized in different ways, and often use different language in different parts, like the introduction versus the experiment section. To make sure we give each part the attention it deserves, we decided to make a separate summary for each section. This way, we’re summarizing smaller, more focused pieces of text, and it’s easier for you to follow along since you can see the paper’s structure. Finally, all these section summaries are put together to give you a complete summary of the whole paper.",
        "formal_text": "This module generates a concise, coherent, informative summary for a given scientific paper that covers the main content conveyed in the text. The summary can either be focused around a query, or query agnostic (a generic summary) 10. Scientific papers are complex: they are long, structured, cover various subjects and the language may be quite different between sections, e. g. , the introduction is quite different than the experiments section. To ensure our summarizer assigns sufficient attention to each of these aspects we have opted to generate a standalone summary for each section. This way we summarize a shorter, more focused text, and the users can navigate more easily as they are given the structure of the paper. Each of these section-based summaries are eventually composed together into one paper summary.",
        "GPT2_formal_text": "= {}, where each part is a [CLS] token. The way we figure out how similar two sets of text are depends on the scoring function we use. This function tells us how many common words (like nouns, verbs, adjectives, etc.) are in the text from the two sets. For instance, if we have a text in two different languages (English and Chinese), we would count the word \"apple\" in both sets as one of those common words. The same goes for the word \"tree\" in both sets. Formal: We pick a probability to assign to these common words, based on the context of the text, and then look for pairs of words that match. The probability for \"apple\" is calculated using this matching function. Formal: In our model, the matching function is based on how similar the words are in meaning. This similarity can be measured using something called the cosine similarity, which calculates the cosine similarity between two vectors. Formal: This similarity is calculated using cosine similarity, which calculates the similarity between two vectors. Formal: We can also use something called similarity loss to calculate these similarity scores. Formal: We use a loss function to adjust the similarity scores. Formal: We calculate a similarity score for each word in the two sets, but this score doesn’t depend on the context of the text. Formal: We only use one similarity score for each pair of words to make sure we’re comparing them fairly. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-3036.json"
    },
    {
        "casual_text": "We came up with a way to clean up short social media posts using paraphrasing. We collected a bunch of tweets and their cleaned-up versions from parallel data on microblogs, all using machine translation techniques. After that, we created two models to learn how to normalize these posts—one works on phrases and the other on individual characters. Finally, we made a decoder that uses both models together during the process. The fact that our method improved several machine translation systems shows it actually works.",
        "formal_text": "We introduced a data-driven approach to microblog normalization based on paraphrasing. We build a corpora of tweets and their normalizations using parallel corpora from microblogs using MT techniques. Then, we build two models that learn generalizations of the normalization process, one the phrase level and on the character level. Then, we build a decoder that combines both models during decoding. Improvements on multiple MT systems support the validity of our method.",
        "GPT2_formal_text": "ed = {1, ..., n}, where m is the number of times the input features were used to predict the output at step t. Formal: After the decoder's decoder output, we get the resulting token representation, W_dec, which is just a matrix with dimensions d_dec. Then, we stick W_dec back together and call it W_dec. Formal: After the decoder's decoder output, we get the token representation, W_dec_t, which is a matrix with dimensions d_dec_t. Formal: Lastly, we add the predicted output token, t, to W_dec to get the decoder's output. Formal: At this point, the model's decoder output is the token representation of the token predicted at step t, and the resulting decoder output is the token representation of the token predicted at step t-1. Formal: To combine these outputs, we use a linear layer to make sure the model's outputs always add up to 1. Formal: In this situation, the tokens t and t-1 are already in the token representation of the token predicted at step t, so we just focus on creating the decoder output using the predicted token representation at step t-1. Formal: After combining these outputs, we use a linear layer to make sure the model's outputs always add up to 1. Formal: Formal: If the token prediction at step t is \"John\" and the predicted token is \"l,\" then the predicted output token at step t-1 is \"John.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D13",
        "filename": "D13-1008.json"
    },
    {
        "casual_text": "We tell our annotators to always connect slot fillers to the EXPERIMENT mention that’s closest in the sentence. If an experiment description is spread across multiple clauses, we use the \"same exp\" relation to link the two EXPERIMENTs together. For experiments done on the same cell but with slightly different conditions, we use \"exp variation\" to link them. The \"exp variation\" link can also connect two frame-evoking elements that talk about measurements on different materials or cells but under the same conditions. These elements often suggest a comparison, like \"increase\" or \"reach from...to...\".",
        "formal_text": "We instruct our annotators to always link slot fillers to the syntactically closest EXPERIMENT mention. If the description of an experiment spans more than one clause, we link the two relevant EXPERIMENTs using the relation same exp. We use exp variation to link experiments done on the same cell, but with slightly different operating conditions. The link type exp variation can also relate two frameevoking elements that refer to two measurements performed on different materials/cells, but in the same experimental conditions. In this case, the frame-evoking elements usually convey an idea of comparison, e. g. , \"increase\" or \"reach from. . . to. \"",
        "GPT2_formal_text": "_seq = list(t_i, b_i)         Casual: A big deal for this method is that the output should be different from the input. Instead of just copying stuff from the input, it mixes things up by focusing on how the different parts of the input interact. Formal: Here, p_s_i is a vector of position features, which basically show how the word w_i is generated from the start of the text. Formal: In this setup, p_s_i is a vector of character features, which are similar to what's used for word embeddings. Formal: Next, the word embedding h_i is created by combining the word embeddings in the input. This is done by applying something called Lagrange multiplicative softmax to the matrix H_i. Formal: Finally, we can add a bias term to adjust the probability distribution p_s_i to make it more like the input distribution. Formal: Here's how the whole thing is set up: Formal: The word embedding h_i is a matrix H_i with dimensions d_w_i by d_a_w_i. Formal: For the word embedding h_i, we also use a softmax function to create the output vector, which is H_i with dimensions d_w_i by d_a_w_i. Formal: The output vector h_i can be written as... Formal: The output vector h_i can be written as... Formal: The output vector h_i can be written as... Formal: And the output vector h_i can be written as... Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.116.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way.\n\nOne way to train models is by using extra bits of info about the article, like who wrote it, when it was made, and where it came from. You add this info as extra tokens at the start of the article before training the model. These tokens give the model more context about the article, helping it understand how the extra info connects to the main content. After training, you can control the model by giving it the same kind of extra info that you want.\n\nThe first model that does this is called GROVER. It can make a news article based on details like the headline, author, and date. The tricky part is, GROVER can create really convincing fake news that's harder for people to spot than fake news written by humans. This could be a big problem.\n\nAnother model, called CTRL, works similarly but uses control codes found naturally in the text, like the URL of a news article. These codes help the model adjust the style, content, and even how it performs specific tasks, like answering questions or translating text. For example, it can switch between writing about sports or politics, or even between different sources like FOX Sports or CNN Sports.",
        "formal_text": "Training with control tokens: The first way is to leverage meta-information about the article such as its author, date of creation, source domain and prepend this information as additional token(s) to the input sequence, before training the TGM. These tokens act as additional context for the article, allowing the TGM to learn the relation between the meta-information and the original article. Once trained, the TGM model can be controlled by prompting with the meta-information of users' interest. The first controllable TGM proposed is the GROVER model, which can generate a news article given the meta-information of the news article (such as headline, author, and date). The GROVER model can create trustworthy fake news that is harder for humans to identify than human written fake news and can thus pose a significant threat. Similar to the GROVER model, the CTRL model provides explicit control of particular aspects of the generated text by exploiting naturally occurring control codes (e. g. , the URL for a news article) to condition the text (e. g. , news article body). These control codes govern style (e. g. , sports vs. politics, FOX sports vs. CNN sports), content (e. g. , Wikipedia vs. books), and task-specific behavior (e. g. , question answering vs. machine translation).",
        "GPT2_formal_text": "ed: For each sequence type s in T, a convolutional layer takes the input x and turns it into a kind of summary for each word in s. It then spits out an attention vector, which we can write as a matrix A_s = [x_1, ..., x_n] ×d, where each dimension shows how important each word is. To figure out the attention for each word in a sentence, we use a convolutional layer with a max pooling operation, which gives us this: Formal: The model has a bunch of convolutional layers and max-pooling operations, but we don’t need to explain how the final representation comes out. Formal: So, the attention vector A_s is made up of a bunch of max-pooling operations, and we don’t need to say which one to use. Formal: The model has an input vector, x, which is shaped like a matrix A_s ×d. Formal: The max-pooling operation can be seen as a way to combine all the attention vectors and figure out the final representation. Formal: The model’s output is a vector that looks like A_s ×d, where each dimension shows how much attention each word gets. Formal: The model has a max-pooling operation, max_t, that works with the input. Formal: The model can produce an output that’s shaped like A_s ×d, where each dimension shows how much attention each word gets. Formal: Formal: The output is a vector that’s shaped like A_s ×d, where each dimension shows how much attention each word gets. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.208.json"
    },
    {
        "casual_text": "Figure 4 shows how skipping chunks of Transformer layers affects task performance. The Y-axis and X-axis represent the first and last layers being skipped, and there's no additional fine-tuning. Generally, performance goes down when more layers are skipped, but skipping just one layer usually doesn't hurt much, except for the very first layer. Like the results we saw earlier, skipping some of the higher layers doesn't seem to mess things up too much.",
        "formal_text": "Figure 4: Layer Experiments: Task performance when skipping contiguous spans of Transformer layers, with the Y-axis and X-axis indicating the first and last (inclusive) skipped layers, with no further fine-tuning. Performance tends to drop as more layers are skipped, but in many cases skipping any single layer makes little to no impact to performance, except for the first layer. Consistent with results above, many of the higher layers can be skipped with minimal impact to performance.",
        "GPT2_formal_text": "' (where u_t is the id of the token, and w_t is the word embedding). The relevance scores for the generated sentences are calculated based on these scores. Formal: The sentence relevance score is calculated using a weighted average of the relevance scores for the t-th token in the input sentence (s_t). Formal: The relevance scores for the t-th token in the input sentence are averaged across all the tokens (s_t) in the input sentence. Formal: The relevance score for the token is calculated by averaging the relevance scores for each token in the input sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "blackboxnlp",
        "filename": "2021.blackboxnlp-1.42.json"
    },
    {
        "casual_text": "This subroutine lets you switch \"O\"-bits to \"1\"-bits and the other way around. It works kind of like this: J = LBIT(WORD, N). The result stored in J will be \"1\" if a \"0\" was changed to \"1\", and \"0\" if a \"1\" was changed to \"0\".",
        "formal_text": "This subroutine permits the conversion of \"O\"-bits into \"1\"-bits and vice versa. Its format is similar to that o--~IT, namely J = LBIT(WORD, N) ~he result stored at J will be \"1\" when \"0\" has been replaced by \"1\" and \"0\" when \"1\" has been replaced by \"0\".",
        "GPT2_formal_text": "ing) and attention_retrieval (check out Figure 1). Basically, it's the same as the regular attention masking, but instead of using a value between -1 and 1, you use a list of numbers from 0 to 1. These numbers are adjusted by how the input is structured and the length of the input, following the attention network's setup (Wang et al., 2017). After that, the attention network's output is calculated using the normalized version of the score from the input. Formal: The attention network takes a sequence of words X and turns it into a vector representation, where each element is a combination of the input and some special vector values. Formal: Formal: The matrix of the scores is created by combining the scores of the words that were seen during the training phase, along with the scores of the words that weren't seen (that's the hidden layer). Formal: Formal: These scores are then turned into a vector representation, so they add up to one. Formal: Formal: This vector representation is then fed into a convolutional layer to get a hidden state representation, which is the main part of the model. Formal: Formal: Formal: In this layer, the vector representation for each word is fed into an attention layer. Formal: Formal: This layer then uses a BiLSTM to create a response, which is the final piece of the model. Formal: Formal: Formal: Formal: Formal: Formal: The feedback loop is super simple. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C67",
        "filename": "C67-1017.json"
    },
    {
        "casual_text": "We’ve talked about the LMG framework, how it works in practice, and what we know about its complexity, especially for a specific subset of cases. Take Example 2.9, for instance—it shows how LMG can neatly explain movement patterns. The complexity result in section 3.5 is mostly there to give us an idea of how tough it is to work with LMG compared to regular context-free grammars. Just to be clear, though, the result we’re discussing here only applies to non-combinatorial LMGs, so it doesn’t cover things like the grammar in Example 2.9 as it’s presented.",
        "formal_text": "We have presented the LMG formalism, examples of its application, and a complexity result for a constrained subclass of the formalism. Example 2. 9 shows that an LMG can give an elegant account of movement phenomena. The complexity result 3. 5 is primarily intended to give an indication of how the recognition problem for LMG relates to that for arbitrary context free grammars. It should be noted that the result in this paper only applies to non-combinatorial LMGs, excluding for instance the grammar of example 2. 9 as presented here.",
        "GPT2_formal_text": "= { ..., { ..., { ..., \"δ_0\": [ ..., { \"δ_1\": [ ..., { \"δ_2\": [ ..., { \"δ_3\": [ ..., { \"δ_4\": [ ..., { \"δ_5\": [ ..., { \"δ_6\": [ ..., { \"δ_7\": [ ..., { \"δ_8\": [ ..., { \"δ_9\": [ ..., { \"δ_10\": [ ..., { \"δ_11\": [ ..., { \"δ_12\": [ ..., { \"δ_13\": [ ..., { \"δ_14\": [ ..., { \"δ_15\": [ ..., { \"δ_16\": [ ..., { \"δ_17\": [ ..., { \"δ_18\": [ ..., { \"δ_19\": [ ..., { \"δ_20\": [ ..., { \"δ_21\": [ ..., { \"δ_22\": [ ..., { \"δ_23\": [ ..., { \"δ_24\": [ ..., { \"δ_25\": [ ..., { \"δ_26\": [ ..., { \"δ_27\": [ ..., { \"δ_28\": [ ..., { \"δ_29\": [ ..., { \"δ_30\": [ ..., { \"δ_31\": [ ..., { \"δ_32\": [ ..., { \"δ_33\": [ ..., { \"δ_34\": [ ..., { \"δ_35\": [ ..., { \"δ_36\": [ ..., { \"δ_37\": [ ..., { \"δ_38\": [ ..., { \"δ_39\": [ ..., { \"δ_40\": [ ..., { \"δ_41\": [ ..., { \"δ_42\": [ ..., { \"δ_43\": [ ..., { \"δ_44\": [ ..., { \"δ_45\": [ ..., { \"δ_46\": [ ..., { \"δ_47\": [ ..., { \"δ_48\": [ ..., { \"δ_49\": [ ..., { \"",
        "directory": "E95",
        "filename": "E95-1013.json"
    },
    {
        "casual_text": "Don’t share any personal info. Stick to your role, meaning talk like the user or the assistant.",
        "formal_text": "• Do not share any of your personal information. Always be \"in your character\", i. e. , speak as the user or the assistant.",
        "GPT2_formal_text": "ing_submod. Formal: Our model uses the attention mechanism from Bahdanau et al. (2014). We take the representation of the prompt and the response and pass it through a GRU block, which has two parts. Formal: We use the attention mechanism from Bahdanau et al. (2014) to handle the attention in our response representations. Formal: We set the input embedding for the prompt to match the response embedding, so the attention is shared between them. Formal: The response embedding is the same for both the prompt and the response. Formal: To learn the attention in the response embedding, we calculate the dot product between the embedding of the response and the embedding of the prompt. Formal: Since the response embedding is a vector, we also calculate the attention vector in the response embedding. This vector is the dot product of the response and the prompt embedding. Formal: Finally, we feed the response embedding into a fully-connected layer to get the final representation of the prompt and the response. Formal: We use the model to generate responses based on the prompt, and the reward is determined by a K-dimensional feed-forward network. Formal: We train this model using the BERT model on a batch size of 40,000 examples and then test it on a batch size of 1000 examples. Formal: We use the model to generate responses based on the prompt, and the reward is determined by a K-dimensional feed-forward network. Formal: We use the model to generate responses based on the prompt, and the reward is determined by a K-dimensional feed-forward network. Formal: We also train the model using the train-50000-RL model. Formal: We train the model using the train-50000-RL model. Formal: We train the model using the train-50000-RL model. Formal: We also train the model using the train-50000-RL model. Formal: We train the model using the train-50000-RL model. Formal: We also train the model using the train-50000-RL model. Formal: We train the model using the train-50000-RL model. Formal: We train the model using the train-50000-RL model. Formal: We train the model using the train-50000-RL model. Formal: Formal: Formal: Formal:",
        "directory": "coling",
        "filename": "2020.coling-main.463.json"
    },
    {
        "casual_text": "But maybe the coolest thing about Europe right now is how many companies are popping up that focus on software localization. These guys are getting really good at using tools and machine translation systems, like Logos, Metal, and XL8. To help everyone share ideas and set some standards, the Localisation Industry Standards Association was created back in 1990. They even put out a newsletter called the LISA Forum and made a CD-Rom with all kinds of info about products, standards, and methods called the LISA Showcase. Ireland is a big player in this space—since 1994, they’ve had their own Software Localisation Group, which hosts conferences and workshops. They’ve also just launched a Localisation Resources Center with help from the Irish government and the EU.",
        "formal_text": "However, perhaps the most distinctive feature of the European scene is the growth of companies providing software localisation. These services are acquiring considerable experience in the use of translation aids and MT systems (e. g. Logos, Metal and XL8). As a forum for the interchange of experience and the establishment of standards the Localisation Industry Standards Association was set up in 1990; the association publishes a newsletter (LISA Forum) and has produces a CD-Rom directory of products, standards and methods (LISA Showcase). A major centre for localisation is Ireland, which since 1994 has its own Software Localisation Group, holding conferences and workshops and recently setting up a Localisation Resources Center (with support from the Irish government and EU. )",
        "GPT2_formal_text": "= { q, i } = { q, i, i }, where i is from 1 to M. To make sure the model can figure out the correct answer, we use a validation set with: Formal: We also use a list of words that are related to the question (like \"what\" and \"where\") that help us figure out the answer. For the question \"What type of dog is the smallest?\" in the example in Figure 1, we pull out words like \"small\", \"dog\", \"smaller\", and \"puppy\" from the list of possible words for \"dog\" and \"puppy\". Formal: To create the answer, we start by creating a list of all possible answers. Then, we use a self-supervised method to match these answers to the correct answer. Formal: Formal: We use a simple linear classifier that predicts the answer based on the questions it has seen so far. Formal: To get the final answer, we just use the classifier to spit out the answer. Formal: Formal: We think that when a question is answered by a relation, it's more likely to be related to the answer than to something else in the question. Formal: Formal: Formal: For the first question in Figure 1, which is \"What is the smallest dog?\", \"small\" is a word that's related to \"puppy\" in the question. So, it's more likely to be related to \"small\" than to \"puppy\" in the question. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "1996.amta-1.20.json"
    },
    {
        "casual_text": "Since the term \"syntax\" alone isn't specific enough, they added a part that really limits how the parser builds structures. This part, which is based on Lexical Phonology, sets rules for the order in which affixes and stems can be combined. As a result, it limits what kind of stems an affix or another stem can connect to, or how complex that stem can be. This helps avoid mistakes like assigning the wrong word class or incorrectly splitting up words, and can even lead to rejecting structures that don’t work.",
        "formal_text": "Because the word syntax as such is not restrictive enough, it was supplemented with a component which heavily restrains the parser in building structures. This component, which is inspired by Lexical Phonology, imposes an ordering on the attachment of affixes and stems. Consequently, it restricts the type or the complexity of the stem that an affix or other stem may attach to. Rejection of structures can result in avoiding incorrect word class assignment and rejection of incorrect segmentations.",
        "GPT2_formal_text": "_x in the input. The activation function (r, x) gets passed along as the final output. Once we have the output vector, we can use it to calculate the word-level attention weight vector (W_a_t) for the target word t. Formal: The source and target models we talked about earlier are built on a multi-layer Transformer. We’ll call this model the source model and the one we’re proposing the target model. In this project, we’re using a vector space model that works with the source model to figure out the target word a. The main goal of our task is to predict the target word a by looking at the source model, which we’ll call the source model. Formal: The source and target models we’re using are built on multi-layer Transformer (MLT) models. The input vector (x) is just a sequence of words, and the output matrix (r, x) is a matrix that holds the attention vector for the target word a. The attention mechanism is basically an attention-based greedy search, where the goal is to find the best translation, e_t, that gives the highest value in the attention matrix R_a, according to Equation 1. Formal: The source model (X) and the target model (Y) are trained together and then tested separately. Formal: We’re using a sentence-level attention mechanism (a = 1) to get the target word a. The target model (Y) learns from the source model (X). Formal: We’re using the multi-head attention mechanism (b = 1) to find the best translation, e_t, that gives the highest value in the attention matrix R_a, according to Equation 1. Formal: The source model (X) and the target model (Y) are trained together and then tested separately. Formal: We’re using the sentence-level attention mechanism (a = 1) to get the target word a. Formal: The source model (X) and the target model (Y) are trained together and then tested separately. Formal: We’re using the multi-head attention mechanism (b = 1) to find the best translation, e_t, that gives the highest value in the attention matrix R_a, according to Equation 1. Formal: The source model (X) and the target model (",
        "directory": "E93",
        "filename": "E93-1023.json"
    },
    {
        "casual_text": "To create the candidate generation function, we start by aligning each dataset using GIZA++. Then, we build a bilingual lexicon with probabilities based on maximum likelihood (P mle ) from the aligned data. After cleaning up the lexicon by removing less frequent and less significant entries, we keep the top 200 translations for each word in the source language, ranked by P mle (t|s) • P mle (s|t). These translations are considered the candidate word pairs. \n\nWe also use these word alignments to train the BNN models. Each alignment link is treated as a training example, and we don’t do anything special for words that don’t align or for cases where one word aligns with multiple words.",
        "formal_text": "To prepare the candidate generation function, each dataset is first word-aligned with GIZA++, then a bilingual lexicon with maximum-likelihood probabilities (P mle ) is built from the symmetrized alignment. After some frequency and significance pruning, 7 the top 200 translations sorted by P mle (t|s) • P mle (s|t) are kept as candidate word translations for each source word in the vocabulary. Word alignments are also used to train the BNN models: each alignment link constitutes a training sample, with no special treatment of unaligned words and 1-to-many alignments.",
        "GPT2_formal_text": "\" function returns the predicates that are triggered by the input attribute value. Formal: The main goal here is to figure out the overall sentiment for the whole document collection D c i. Formal: We also use a multi-task loss to make sure the extracted relations are helpful for the summarization. Formal: We apply the sentence-level aspect-level attention from (Kim and Rush, 2016) to the retrieved relations. Basically, we feed the retrieved relations to the attention layer of the sentence-level attention layer. If the retrieved relations are positive, the attention layer spits out a soft attention vector with a specific vector length λ that represents the sentiment for the relation in the retrieved relation. If the retrieved relations are negative, the attention layer spits out a negative vector with the same vector length λ. Formal: The score for a relation is calculated by adding up the scores of the retrieved relations it contains, weighted by the attention weight λ. Formal: We also use a multi-task loss to make sure the extracted relations are helpful for the summarization. Formal: For our experiments, we used a dataset called \"WORD-WORDS\" from Google. The dataset has English and Chinese Wikipedia articles, each with around 1.3 million sentences. Formal: To train the model, we use a convolutional neural network (CNN) with a hidden size of 300 and a dropout rate of 0.3. The model is trained on a single NVIDIA Tesla P100 GPU with an 8GB of memory. Formal: To check how well the extracted relations perform, we calculate the average scores from all the extracted relations. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D14",
        "filename": "D14-1175.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\n1. **τ, α, g, σ |= X: Y: P** means that if τ (α(Y )) is in σ(P ) and α(X) * ∧ α(Y ), then it's true.\n\n2. **τ, α, g, σ |= ARG S (X, a)** means there's an i in S where α(X) • i is in D(τ ) and τ (α(X) • i) equals g(a).\n\n3. **τ, α, g, σ |= ARG S (X, Y )** means there's an i in S where α(X) • i is in D(τ ) and α(X) • i equals α(Y ).\n\n4. **τ, α, g, σ |= X * Y** means α(X) * α(Y ) is true.\n\n5. **τ, α, g, σ |= X =/ = Y** means α(X) is not equal to α(Y ).\n\n6. **τ, α, g, σ |= v 1 =/ = v 2** means g(v 1 ) is not equal to g(v 2 ).\n\n7. **τ, α, g, σ |= P Q** means σ(P ) is a subset of σ(Q).\n\nIn simpler terms, these are rules that explain how different things relate to each other based on certain conditions.",
        "formal_text": "τ, α, g, σ |= X: Y: P iff τ (α(Y )) ∈ σ(P ) and α(X) * ∧ α(Y ) τ, α, g, σ |= ARG S (X, a) iff exists i ∈ S s. t. α(X) • i ∈ D(τ ) and τ (α(X) • i) = g(a) τ, α, g, σ |= ARG S (X, Y ) iff exists i ∈ S s. t. α(X) • i ∈ D(τ ), α(X) • i = α(Y ) τ, α, g, σ |= X * Y iff α(X) * α(Y ) τ, α, g, σ |= X =/ = Y iff α(X) =/ = α(Y ) τ, α, g, σ |= v 1 =/ = v 2 iff g(v 1 ) =/ = g(v 2 ) τ, α, g, σ |= P Q iff σ(P ) ⊆ σ(Q)",
        "GPT2_formal_text": "_x: (h, x) | h | x = [h_1, ..., h_n | h_n - 1] (4) Formal: Here, N_c represents the number of times the input text gets repeated. In our experiments, we used a pre-trained language model (LM) that's been fine-tuned with English data. The LM is trained to predict how a new input text should be represented, using a set of embeddings (E). Formal: The whole process of how the model learns to represent text is laid out in Figure 1. We refer to these embeddings as the \"resource\" in our setup. The model generates a representation, h, and then uses a method called learned-to-order (LTE) (credit to Gu et al., 2015) to combine it with the other embeddings, E. Formal: Unlike older methods like LMLM or RNNLM, our model doesn't have a teacher forcing mechanism. Instead, the teacher can decide how much influence it has on the student during training, kind of like a policy. This is different from the methods that involve having a teacher and a student, where the student learns from the teacher. Formal: The final representation h is written as h_t, which is the mix of the hidden states from the teacher's and student's vectors, following a linear transformation. Formal: Formal: In real-world situations, we often create a special representation to handle cases where the input text is continuous. Formal: We also use an LMLM (also called LMLM-CRF) (Srivastava et al., 2013) for this, which works similarly to LMLM but also deals with discrete input text. Formal: In this case, the student is a hierarchical sequence labeling model (HATM), which is trained to handle discrete input text. Formal: Formal: Formal: Formal: The student can learn from the teacher's vectors and embeddings, which are represented by the embeddings of the tokens in the input text. Formal: Formal: Formal: Formal: Formal: We made sure to make sure the student and teacher have similar embeddings to prevent overfitting. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E09",
        "filename": "E09-1052.json"
    },
    {
        "casual_text": "So, after generating possible words, each one gets checked in a dictionary to see if it actually exists. If it does, it might be a correct spelling. But this method has a couple of issues. First, it only works for certain types of spelling checks based on \"edit distance.\" Second, it can take a lot of time if the edit distance is more than 2 and there are a lot of possible characters, like in Unicode for many Asian languages.\n\nHulden (2009) came up with a Finite-State-Automata (FSA) algorithm to speed things up by finding similar words quickly, even if they're misspelled. Other people have used FSA too, but these methods are still just approximations for finding the closest match.\n\nMore recently, de Amorim and Zampieri (2013) suggested another way to cut down on the number of checks needed by using something called \"anomalous pattern initialization\" and \"partition around medoids.\"",
        "formal_text": "Then each of such generated words is checked in the dictionary for existence, and if it is found in the dictionary, it becomes a potentially correct spelling. There are two shortcomings of this approach. First, such search space reduction works only for edit distance-based measures. Second, this approach too leads to high time complexity when the edit distance threshold is greater than 2 and the possible characters are large. Large character set is real for Unicode characters used in may Asian languages. Hulden (2009) proposes a Finite-State-Automata (FSA) algorithm for fast approximate string matching to find similarity between a dictionary word and a misspelled word. There have been other approaches as well using FSA, but such FSA-based approaches are approximate methods for finding closest matching word to a misspelled word. Another more recent approach to reduce the average number of distance computations is based on anomalous pattern initialization and partition around medoids (de Amorim and Zampieri, 2013).",
        "GPT2_formal_text": ") = (int)(D[i, j] - (P[i, j] + (P[i, j] - P[i, j]))) Formal: This is basically the same as using the regular multi-head attention mechanism. Formal: Basically, we're taking the average of the query vectors, p_q_i, and the attention scores for each possible pair of sentences, p_i_j. Formal: The query vectors from the question, p_q_i, are all multiplied by the attention scores, p_i_j, then scaled by their logits. The key idea here is to make sure the relevance and relevance confidence scores are equal and unbiased. Formal: After doing this, we get the output distribution q_t, which is basically the query vector p_q_t plus the attention scores for the question. Formal: The attention scores for the question are normalized to the query vector p_q_t. Formal: Finally, we calculate the relevance confidence score for each question by adding the relevance confidence score for the query vector p_q_t plus the relevance confidence score for the question vector p_i_j. Formal: The relevance confidence score for a question is just the sum of the relevance confidence scores for its query and its possible answer, plus the relevance confidence score. Formal: Since we're using multi-head attention, we can calculate the relevance confidence score like this: Formal: This method creates a summary using all the model's parameters, which gives us the probability distribution q_t, which is a vector. Formal: Formal: The relevance confidence score for a question is calculated by adding the relevance confidence score for its query and its possible answer, plus the relevance confidence score for the question vector p_q_t plus the relevance confidence score for the question vector p_i_j. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E17",
        "filename": "E17-2027.json"
    },
    {
        "casual_text": "When we train models on just one dataset and then test them on other datasets, they don't perform as well, which isn't surprising. But, our method does way better at handling different datasets compared to the standard approach by Bugert et al. (2020a), even when we only train on one dataset.",
        "formal_text": "For models trained on the train split from a single corpus, we see significant performance loss when evaluated on test splits from other corpora as is expected. However, we see vastly improved generalizability with our approach when trained on a single corpus compared to the baseline set by Bugert et al. (2020a).",
        "GPT2_formal_text": "ed_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1, self.att_masked_cnt) self.att_masked_cnt = max(1",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.106.json"
    },
    {
        "casual_text": "- METEOR is a way to measure how well your translation matches the reference, kind of like a mix of precision and recall, but it puts more emphasis on recall. (Banerjee and Lavie, 2005)\n- TER (Translation Edit Rate) is like a score that tells you how many changes you’d need to make to your translation to match the reference sentence exactly. (Snover et al., 2006)\n- TERP is an upgraded version of TER. It’s smarter because it can do things like swap out phrases, simplify words, and even check for synonyms to make the score more accurate. (Snover et al., 2009)\n- TERPA is a specific version of TERP that’s been tweaked to work really well for measuring how good a translation is, especially when it comes to making sure the meaning is clear.",
        "formal_text": "• METEOR, which measures the harmonic mean of unigram precision and recall, with a higher weight for recall (Banerjee and Lavie, 2005) • TER (Translation Edit Rate), a measure of the number of edits required to transform a hypothesis sentence into the reference sentence (Snover et al. , 2006) • TERP, an augmented version of TER which performs phrasal substitutions, stemming, and checks for synonyms, among other improvements (Snover et al. , 2009) • TERPA, an instantiation of TERP with edit weights optimized for correlation with adequacy in MT evaluations",
        "GPT2_formal_text": "', '[CLS], and '<destination>' are all just numbers you can use to show where the two entities in the sentence are, and '<attachment>' is the label that points to where that attachment is located in the entity pair. Formal: For the regular ARRAU model, we define the similarity function as: Formal: We use the max-margin loss for figuring out the model's parameters. Formal: For L-BFGS, we set the parameter to 1 and tweak it by running the model on the test set. Formal: We also calculate the score for the predicted relation between two entities by using the log probability of the relationship, plus a penalty. Formal: If you want to know more about the specific calculations we use or the input datasets we use, check out the papers by Schumann and Eisenstein. Formal: The model's parameters, which are in R d, are based on the internal representations of the target sentences. Formal: You can check out the results for this model in Figure 1. Formal: We also calculate the score for the predicted relation between two entities by using the log probability of the relationship, plus a penalty. Formal: In Table 2, we show the average score for each relationship. Formal: Lastly, we test the model on the test set, but we don't include the average score here to keep things short. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D10",
        "filename": "D10-1055.json"
    },
    {
        "casual_text": "Once the text-image matching model is trained and the sentences are simplified, we look at each text-image pair (T i, I j ) in our task. If the score s(T i, I j ) is higher than a certain threshold T match, we consider them a match. We picked the threshold as the average matching score for positive text-image pairs in Flickr30K. But, in theory, tweaking this threshold could make the matching better for our specific task.",
        "formal_text": "After the text-image matching model is trained and the sentences are simplified, for each textimage pair (T i, I j ) in our task, we can identify the matched pairs if the score s(T i, I j ) is greater than a threshold T match. We set the threshold as the average matching score for the positive text-image pair in Flickr30K, although the matching performance for our task could in principle be improved by adjusting this parameter.",
        "GPT2_formal_text": "ed = [w_mask, ..., wi_mask] Formal: The last layer adds a softmax layer to the hidden states of the input word. This layer picks out the most important hidden states from the output words, which are used to create the sentence-level representation for the input word. Formal: So, this representation, f, is calculated using something called a Transformer-based encoder-decoder setup (like the one Vaswani et al. came up with in 2017). Formal: The output of the last hidden state from the encoder is calculated as d_last_input = (1 - e_last_input) * h_last_input. Formal: The output vector for a single input word, h_i, is written as λ_i = [h_i, ..., h_i+1] Formal: To get the representation for the whole sentence, we use the BiLSTM decoder (introduced by Hochreiter and Schmidhuber in 1997) with a hidden layer size of 3. Formal: In this paper, we're mainly looking at how the model handles single words in a sentence. But our approach can work with any kind of sentence, which is what we're focusing on in this paper. Formal: When we're testing our model, we use a conditional random field (CRF) to figure out the probability of the next word, y_i, given the previous word, y_j. Formal: The CRF is trained to maximize the log probability p(y_i | y_j). Formal: We then use the log probability of the next word as a feature to find the next sentence, a_i, based on the previous sentence. Formal: Using the sentence length, m, to figure out the probability of the next word, a_i, is pretty straightforward. Formal: For training the CRF, we use the time it takes to run the CRF on the whole sentence as our feature. Formal: Using a CRF can be done by either running it on a sentence or a whole sentence. Formal: Formal: This process is shown in a sketchy diagram in Figure 1. Formal: Formal: In this process, we first pick a sample sentence d_1 for the source language and then use this chosen sentence to create the next sentence. Formal: The \"source\" sentence is created using a CR",
        "directory": "D17",
        "filename": "D17-1114.json"
    },
    {
        "casual_text": "• The question pattern prediction score, which is the score given by either the retrieval-based method or the generation-based method;",
        "formal_text": "• question pattern prediction score, which is the prediction score by either retrieval-based approach or generation-based approach;",
        "GPT2_formal_text": "\" from the database. To figure out the best model for a sentence s, we take its average k-embedding and run it through the GRU with just one pass. Formal: We’ve got the final vector for a sentence s, which is a matrix with n rows and m columns. The thing is, the embedding for the word w is already calculated and included in the GRU’s outputs. So, we just need to figure out the word embedding for the whole sentence s. Formal: The embedding for the word w is calculated like this: Formal: For a sentence s, the best model using the input \"x\" is calculated like this: Formal: The embedding for the word w is calculated by adding up the embeddings from the sentence’s words and the embeddings of the words around the word w, giving us: Formal: Formal: The embedding for the sentence is a matrix with n rows and m columns. Formal: The model can be thought of as: Formal: To use this model, we just add up the embeddings for all the words in the sentence s. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D17",
        "filename": "D17-1090.json"
    },
    {
        "casual_text": "To save time and avoid asking people the same question over and over, we assume that if we get an answer for a specific part of a sentence and one of its possible translations, we can use that same answer for all other possible translations of that part. So, for each sentence, we create a little database with entries like <source part, target part, judgment>. The judgment can be YES, NO, or NOT SURE. This way, we don’t have to keep asking about the same source part with different target options.",
        "formal_text": "The first assumption we make to minimize the number of human judgments, is that once we have a judgment for a source-candidate substring pair, that same judgment can be used across all candidates for this source sentence. In other words, we build a database for each source sentence, which consists of <source substring, target substring, judgment> entries. For a given source substring, multiple entries exist, each with a different target candidate substring. The judgment field is one of YES, NO, and NOT SURE.",
        "GPT2_formal_text": "_u_k = h_u_k + h_u_k * n_u_k + h_u_k * n_u_k + h_u_k * h_u_k + h_u_k * n_u_k * n_u_k * h_u_k + h_u_k * n_u_k * h_u_k * n_u_k * h_u_k + h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u_k * h_u_k * h_u_k * n_u",
        "directory": "D09",
        "filename": "D09-1006.json"
    },
    {
        "casual_text": "The next word distribution might have multiple peaks. But using a bunch of softmaxes makes things way more complicated because we have to calculate the dot product between each facet and every word in our vocabulary. Based on our analysis, we thought of a way to simplify this: we can divide all the words in the vocabulary into different groups and use different facets for each group. For instance, if we put words like \"queen,\" \"man,\" \"woman,\" and \"king\" into one group and the rest of the words into another, we won't end up with weird combinations like \"queenking\" being the same as \"womanman\" in either group. With this approach, each word is only in one group, so we only need to calculate one dot product per word. The only extra work we have to do is setting up the extra linear projections for the facets.",
        "formal_text": "The next word distribution could have many modes. However, using many softmaxes significantly increases our computational burden because we need to compute the dot product between each facet and all the word embeddings in our vocabulary. Inspired by our analysis, we propose to split all the words in the vocabulary into multiple partitions 4 and use different facets for different partitions. For example, if we can put any word from {queen, man, woman, king} into one partition and the rest of the words into another partition, we no longer have queenking = womanman in either of the partitions. In this method, each word only belongs to one partition, so we only need to compute one dot product for each word. Thus, the extra computational cost only comes from the extra linear projections for preparing the facets.",
        "GPT2_formal_text": ". Formal: The first three rows are the character embeddings for each word, and the last three rows are the word embeddings for the word before that. Formal: The last three rows are the context embeddings for the word. Formal: The last three rows are the context embeddings for the word. Formal: The last three rows are the context embeddings for the word. Formal: Formal: The word embedding for the word before the current one. Formal: The word embedding for the word before the current one. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.554.json"
    },
    {
        "casual_text": "The University Bookstore stays open until 4:30 PM. I'm not sure if they sell floppy disks, though. But they do have a bunch of other stuff that's useful for school, so it might be worth checking out.",
        "formal_text": "\"The University Bookstore is open until 4: 30 PM. But I don't know whether it sells floppy disks. However it does sell many other items of an educational nature, so it is perhaps a good place to try. \"",
        "GPT2_formal_text": "_layers = max(1, stride = k) token_mask_layers = max(1, k) entity_mask_layers = max(1, k) entity_length = max(1, k) entity_type = max(1, k) entity_shape = max(1, k) entity_dist = max(1, k) entity_shape_layers = max(1, k) entity_shape_layers = max(1, k) entity_shape_mask = max(1, k) entity_shape_mask = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_type = max(1, k) entity_shape_type = max(1, k) entity_shape_type = max(1, k) entity_shape_type = max(1, k) entity_shape_type = max(1, k) entity_shape_type = max(1, k) entity_shape_type = max(1, k) entity_shape_mask = max(1, k) entity_shape_mask = max(1, k) entity_shape_mask = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_mask_layers = max(1, k) entity_shape_type_layers = max(1, k) entity_shape_type_layers = max(1, k) entity_shape_type_layers = max(1, k) entity_shape_type_mask = max(1, k) entity_shape_type_mask = max(1, k) entity_shape_type_mask = max(1, k) entity_shape_mask = max(1, k) entity_shape_mask_layers = max(1, k)",
        "directory": "C86",
        "filename": "C86-1006.json"
    },
    {
        "casual_text": "Step 4: Check out the telops in TV images. Figure 5 has a bunch of different info that telops show during TV news (thanks to Watanabe in '96 for pointing that out). Watanabe also came up with a way to analyze the meaning behind these telops, and it worked pretty well—92% accurate. We’re using that same method to figure out what each telop is actually saying.",
        "formal_text": "Step 4 Analyze telops in TV images. Figure 5 shows several kinds of information which are explained by telops in TV Newscasts (Watanabe 96). In (Watanabe 96), a method of semantic analysis of telops was proposed and the correct recognition of the method was 92 %. We use this method and obtain the semantic interpretation of each telop.",
        "GPT2_formal_text": "= { ] } model_re = ReLU(model_re) if len(model_re) == 1 or len(model_re) == 2: model_re = tanh(model_re) model_re = ReLU(model_re) else: model_re = ReLU(model_re) model_re = ReLU(model_re) return model_re if len(model_re) == 1 or len(model_re) == 2: model_re = ReLU(model_re) model_re = ReLU(model_re) return model_re\n\nBasically, we take the first part of equation (6) and plug it into equation (6b) to get the equation. And that's how the model structure is laid out in Figure 2. Formal: To tackle these problems, we use a grid search to find the best solution. Formal: The whole thing is outlined in Figure 2. Formal: We're using the model structure as an example to show how we use the structure to build the final structure. Formal: In our search, we're picking the best option based on a model structure. Formal: The model structure itself includes the model's parameters, which we can use to check how well the model is performing. Formal: Here, we're using a different approach than the first-order model where we look at the model's parameters directly and see how they're doing. Formal: Formal: We're using the first-order model to give us a clearer idea of how the parameters are doing. Formal: We've outlined how we use the model structure to find the best solution. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C98",
        "filename": "C98-2220.json"
    },
    {
        "casual_text": "Right now, we're using a basic beam search to find a sequence of parts that covers the entire input. These sequences can have gaps, meaning they don't have to be connected, but they can't overlap. The algorithm likes shorter sequences. To figure out the length of a sequence, we add up the lengths of the parts in it, but we give some parts more weight than others.\n\nWe tried two ways to assign weights and lengths to these parts. In the first method, each part gets a length of 1, but we multiply that by a factor based on how \"good\" the part is. Sequences can be extended by adding gaps that cover one input token at a time, and these gaps get a weight of 3. Parts created by rules that have <OUTPUT> on the left side are considered the best and get a weight of 1. Other parts can also be added to the sequence and get a weight of 1.5.\n\nFor example, if we have a sequence with an <OUTPUT> part covering tokens 1 to 3, a gap covering tokens 4 and 5, and a <Vl> part covering tokens 6 to 10, the total length would be 1 (for the <OUTPUT> part) + 6 (for the gap) + 1.5 (for the <Vl> part) = 8.5. This algorithm really prefers sequences that are connected and calculates lengths based on the number of parts in the sequence, not their actual length.",
        "formal_text": "Currently, a simple best-first beam search through the chart is used to find a sequence (path) of constituents spanning the whole input. Paths are allowed to have gaps, i. e. , they do not have to be contiguous, although we do not allow for overlapping constituents. The algorithm prefers shorter paths. The length of a path is computed as a weighted sum of the lengths of constituents in the path. We experimented with two different ways of assigning weights and lengths to constituents. In the first method, each constituent was assigned the length of 1 that was weighted by a factor depending of the \"quality\" of the constituent. Paths can be extended by a gap spanning one input token at a time. Such a gap is weighted with the factor of 3. Constituents that are created by rules with the nonterminal <OUTPUT> on their LHS are assumed to be of the highest quality and they are weighted with the factor of 1. All remaining constituents can also be added to the path and are weighted with the factor of 1. 5. So a path consisting of an <OUTPUT> constituent spanning input tokens 1 to 3, a gap spanning input tokens 4 and 5, and a <Vl> constituent spanning input tokens 6 to 10 would receive the length of 1 + 6 + 1. 5 = 8. 5. This algorithm shows a strong preference for contiguons paths and assigns lengths depending on the number of constituents in the path, ignoring their length.",
        "GPT2_formal_text": "_list\" [, , ], |V |. Then, we grab the keywords from the text that start with \"I\" and add them to a special list. This list is saved in a special file called \"keywords.txt\" in the keyphrase vocabulary. For example, the file is called \"keywords.txt\" in the vocabulary of the AlignCO tool. Formal: We also use a dataset called the Corpus of Multilingual and Multilingual Word Embeddings (CUMWEB). This dataset was made by Lin et al. (2020) using the parallel Chinese-English Wikipedia for training, and it was fine-tuned on a dataset called WikiLarge (Xue et al., 2015). It has 9,724 English and 7,848 Chinese sentences in both the source and target languages. Formal: For training the word embedding model, we start by training it on this dataset. Formal: We use a translation model trained on the CUMWEB dataset as a starting point for the CoNLL-2012 shared task. Formal: To improve how well we can translate words, we train the phrase translation model with the CUMWEB dataset. Formal: After fine-tuning, we look at the BLEU scores from both the original and target languages to compare the translation model's performance with the best-performing model from the original language. Formal: We do this by comparing the average BLEU score from the source and target languages to the best-performing model from the original language. Formal: We figure out the best BLEU score based on the method we just talked about. Formal: If the BLEU score from the source and target languages is within a certain range, we just go with that. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "A00",
        "filename": "A00-3002.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. First, we start by setting 'a' to 0.5. Then, we tweak the topic number K, changing it from 2 to 20, but we do it in steps of 2. If you check out Figure 7 (1) and Figure 7 (2), you'll see that the ROUGE score hits its highest point when K is around 12. \n\nNext, we lock K at 12 and adjust the value of 'a' from 0 to 1, stepping it up by 0.1 each time. When 'a' is set to 0, the model basically becomes a one-layer graph ranking thing, ignoring any topic clustering info. Looking at Figure 7 (3) and Figure 7 (4), the ROUGE scores peak around 0.6 and then start to drop after that. \n\nSo, for our test dataset, we go with K set to 12 and 'a' set to 0.6.",
        "formal_text": "First, a is set to 0. 5, then we change topic number K from 2 to 20 at the interval of 2. The ROUGE score reaches their peaks when the topic number is around 12, as shown in Figure 7 (1) and Figure 7 (2). Then we fix the number of K to 12 and change the value of parameter a from 0 to 1 with the interval of 0. 1. When the value of a is set to 0, the model degenerates into a one-layer graph ranking algorithm where topic clustering information is neglected. As we can see from Figure 7 (3) and Figure 7 (4), the ROUGE scores reach their peaks around 0. 6 and then drop afterwards. Thus, the topic number is set to 12 and a is set to 0. 6 in the test dataset.",
        "GPT2_formal_text": "= { ... } i=1; n = length of i-1; i=1; |T| = [i-1; |T| + n] Formal: To make sure we get the best possible answer, we use a multi-task learning approach. This means we take the target domain and multiply the data using a \"shuffle\" operation (check out Section 2.1) and a \"matching\" operation (see Section 2.2). Formal: After shuffling and matching, we add the predictions from the match and shuffle steps together (Section 2.3), so the final prediction for i is the result from both the match and the shuffle. Formal: After checking the predictions, we keep only the top-k predictions (Section 2.4) for each domain and then use a linear layer to average them out (Section 2.5). Formal: In this case, the embedding layer takes the embedding vector i and turns it into a \"word\" embedding for each target word. Formal: Finally, we use a linear layer on this word embedding and a max pooling layer to get the final result for the target word, which we then use for the next target word (Section 2.6). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C14",
        "filename": "C14-1113.json"
    },
    {
        "casual_text": "We built a website kinda like Wikipedia, called the GMB Explorer, where people can check out the Groningen Meaning Bank. It does three main things: lets you browse and search through the documents, shows you the different layers of annotations, and lets you fix any mistakes in those annotations. We'll go over these features in a bit.",
        "formal_text": "We developed a wiki-like Web interface, called the GMB Explorer, that provides users access to the Groningen Meaning Bank. It fulfills three main functions: navigation and search through the documents, visualization of the different levels of annotation, and manual correction of the annotations. We will discuss these functions below.",
        "GPT2_formal_text": "and attention_attributes. It's pretty straightforward to understand why our method works. In the example we mentioned earlier, the words \"cat\" and \"dilemma\" only have the attribute 'dilemma', which means they don't have any attention. So, their attention scores are 0. And when we adjust the attention based on the concept, the results look like this: Formal: I’ve got the numbers for all the attention weights for this example. Formal: Alright, here's a breakdown of how the attention weights were calculated for the words 'cat', 'dilemma', and 'puppy' in the example: Formal: The 'cat' and 'dilemma' attention weights are 0.96, 0.92, and 0.90, respectively. Formal: Formal: The 'puppy' attention weight is 0.96. Formal: Formal: You can check out the full list of weights and how they were calculated in Appendix D. Formal: The network and the dually-attended word pairs were trained using the development set, and the results for the test set are in Table 2. Formal: We used the same architecture for both training and testing. Formal: Formal: We also used BERT for word embedding and pretraining. Formal: We trained the network with 100,000 hidden units and the pretrained word embeddings with 300,000 parameters. Formal: Formal: We got the embeddings for the 'cat' and 'dilemma' hidden layers from OpenAI (Pedregosa et al., 2018). Formal: Formal: We ran the tests using the development set, just like in Table 1. Formal: Formal: The results we got from this pretraining on the development set are in Table 2. Formal: Formal: We trained the network with 100,000 hidden units, the pretrained word embeddings with 300,000 parameters, and the weights for the lexical heads from OpenAI (Pedregosa et al., 2018). Formal: Formal: Formal: We used BERT with 300,000 hidden units and the same pretraining parameters as in Table 1. Formal: Formal: Formal: Formal: We used BERT for word embedding and pretraining. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E12",
        "filename": "E12-2019.json"
    },
    {
        "casual_text": "When we plug this IF into the mapper, it spits out the FS you see in Figure 7. Basically, it's like saying \"for whom=us\" and \"room-spec=room\" with a quantity of 4.",
        "formal_text": "['for-whom=', map_value(we, pronoun, ‫, )ﳓﻦ‬ 'room-spec=', map_value(room, noun, '‫', )'ﻏﺮﻓﺔ‬quantity=', 4] ] When this IF is fed into the mapper, the FS shown in Figure 7 is produced.",
        "GPT2_formal_text": "= [ ... ] n = [ ... ] f = [ ... ] y = [ ... ] p = [ ... ] n = [ ... ] y = [ ... ] p = [ ... ] n = [ ... ] y = [ ... ] p = [ ... ] n = [ ... ] p = [ ... ] n = [ ... ] y = [ ... ] p = [ ... ] n = [ ... ] y = [ ... ] p = [ ... ] n = [ ... ] y = [ ... ] p = [ ... ] n = [ ... ] y = [ ... ] p = [ ... ] n = [ ... ] y = [ ... ] p = [ ... ] n = [ ... ] y = [ ... ]\n\nBasically, if you've got a bunch of nodes and some relations (like in equation 1), it's like putting all those nodes together in the right order to create a sequence. Then, you can toss in some extra relations, and the system figures out the target nodes and relations based on that. Formal: So, for each node in a graph G, there's a graph context vector C_i that maps to some nodes in G. The operation of updating a node i (either x i or y i) to its new vector C_i is called updating the graph context. Formal: The graph context is made up of the graph context vector C_i and the relation vector A_i. Formal: If you want to see how the graph context is set up, check out Figure 2. Formal: We also have a graph context vector R_i for each node in G. Formal: For each relation r, we also have a graph context vector r_i for each node in G. Formal: For R_i, the graph context vector for r_i is made up of the graph context vector r_i. Formal: Finally, we have the relation vector R_i for each relation r. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "bcs",
        "filename": "2006.bcs-1.12.json"
    },
    {
        "casual_text": "With RoBERTa and for TroFi and VUA Verbs, we notice some crazy jumps in performance in those final layers.",
        "formal_text": "For RoBERTa and in the case of TroFi and VUA Verbs, we see exceptional increases in the last layers.",
        "GPT2_formal_text": "ing_dg. The last layer of LSTM creates a contextual representation by combining the contextual feature vectors from the query and query_dg, while the output from the RNNLM is used as the feature vector for the query_dg. The similarity between the query and query_dg vectors is then used to create the query feature vector for the dg. Formal: To figure out the attention distribution for the dg, we start by using a linear layer to encode the contextual feature vector. Then, we update the query and query_dg vector by averaging the attention values from all the hidden units in the RNNLM. Finally, we use a softmax function to calculate the attention weights for the dg. Formal: To figure out the attention distribution for the query, we use a linear layer to encode the contextual feature vector. After that, we update the query and query_dg vector by averaging the attention values from all the hidden units in the RNNLM. Finally, we use a softmax function to calculate the attention weights for the dg. Formal: Formal: Formal: We do a local maximum pooling step, where the vectors we get are based on the hidden units in the RNNLM, and the time it takes to process them can be expressed in the form of an exponential or logarithmic time. Formal: We use an attention mechanism, which is described by the formula below. Formal: Formal: A function is considered successful if it has a specific sequence of outputs that the model can learn from, and it meets certain conditions. Formal: Formal: For a successful function, we have two types of outputs: x, which is a sequence of tokens from the input, and y, which is a sequence of tokens from the output. Formal: Formal: Formal: A success function is also considered successful if it meets these conditions. Formal: Formal: Formal: Formal: A successful function is also considered successful if it meets these conditions. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.144.json"
    },
    {
        "casual_text": "So, to effectively debug models with human input, it's crucial to understand these dimensions (we call them features). We use an explanation method to get this understanding. There are various ways to explain predictions from text classifiers—like natural language explanations, rules (Ribeiro et al., 2018), extracted rationales (Lei et al., 2016), and attribution scores (Lertvittayakumjorn and Toni, 2019). Some methods, like LIME (Ribeiro et al., 2016) and SHAP (Lundberg and Lee, 2017), work with any model and don't need access to the model's inner workings. On the other hand, methods like DeepLIFT (Shrikumar et al., 2017) and LRP (layer-wise relevance propagation) (Bach et al., 2015; Arras et al., 2016) dive into the model's architecture and parameters to generate explanations. In our work, we use LRP to explain the learned features, not just the predictions, to help humans understand the model's behavior and make better debugging decisions.",
        "formal_text": "Therefore, understanding the roles of these dimensions (we refer to them as features) is a prerequisite for effective human-in-the-loop model debugging, and we exploit an explanation method to gain such an understanding. Explaining predictions from text classifiers -Several methods have been devised to generate explanations supporting classifications in many forms, such as natural language texts ), rules (Ribeiro et al. , 2018, extracted rationales (Lei et al. , 2016), and attribution scores (Lertvittayakumjorn and Toni, 2019). Some explanation methods, such as LIME (Ribeiro et al. , 2016) and SHAP (Lundberg and Lee, 2017), are model-agnostic and do not require access to model parameters. Other methods access the model architectures and parameters to generate the explanations, such as DeepLIFT (Shrikumar et al. , 2017) and LRP (layer-wise relevance propagation) (Bach et al. , 2015; Arras et al. , 2016). In this work, we use LRP to explain not the predictions but the learned features so as to expose the model behavior to humans and enable informed model debugging.",
        "GPT2_formal_text": "= [tos, es, ti]         Casual: We're using three types of examples to show how our approach works. Formal: There are two different types of example labels: actual examples and imagined examples. Formal: Our method doesn't create examples that are super abstract or super specific. Formal: We treat actual examples as positive examples and imagined examples as negative examples. Formal: The first example in Figure 1 is a real example from the Enron dataset. Formal: The second example in Figure 1 is a positive example from the LCC dataset. Formal: Lastly, the third example in Figure 1 is a negative example from the LCC dataset. Formal: We tested our method on two different datasets: one from ACE2005 (for event extraction) and another from ACE2005-Large (for event extraction and reasoning). Formal: We also shared the results of the experiments on two datasets: the Dutch Enron dataset (with 13,342 sentences) and the Enron dataset (with 35,066 sentences). Formal: For comparison, we also looked at the results for the Canadian Enron dataset (with 6,411 sentences). Formal: Lastly, we included the results for the ACE2005-Large dataset. Formal: All the results and explanations we shared are based on the ACE2005-Large dataset. Formal: The figures and the whole experiments are available online. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.24.json"
    },
    {
        "casual_text": "We create meaning representations for hypothesis sentences using the CCG derivation tree and Neo-Davidsonian Event Semantics (as described by Parsons in 1990). To get these meaning representations (which are basically FOL formulas) from hypothesis sentences, we use a tool called ccg2lambda. This tool was developed by Mineshima and others in 2015 and later improved by Martínez-Gómez and team in 2016. It works with CCG and λ-calculus to do the job.",
        "formal_text": "We construct meaning representations of hypothesis sentences based on the CCG derivation tree and Neo-Davidsonian Event Semantics (Parsons, 1990). ccg2lambda (Mineshima et al. , 2015; Martínez-Gómez et al. , 2016 ) is used to obtain meaning representations (FOL formulas) of hypothesis sentences based on CCG and λ-calculus.",
        "GPT2_formal_text": "layer in RNNG. Formal: So, if we have a source sentence X = {s1, ..., sm}, a target sentence Y = {y1, ..., ym} and a source token ŷ = {t1, ..., tm}, a target token y = {y1, ..., yn} and a source token ŷ2 = {t2, ..., tn}, we can make the model do something like this: Formal: In the sentence-level embedding layer, we create a series of vector representations for each source token. The idea is that the vectors for each token should match up with the vectors in the target sequence. Formal: The end result is a sequence of vector representations where each token is represented by a vector that matches the vector in the target sequence. Formal: This setup lets the model predict a bunch of token representations that are all related to the target sequence. Formal: The same idea applies to the target sequence. Formal: To check if the prediction is correct, we look at the next token's vector. Formal: We can use this info to help the model predict the next token's vector, which could be the token itself or a feature vector from the last layer. Formal: We call the output for the target sequence ŷt and the next token's vector ŷn based on this sequence-level embedding setup. Formal: In the next parts, we'll talk about these two types of embedding setups, along with how they affect the model's performance. Formal: Let’s dive into the idea of using a source context vector as a sentence embedding and the related sentence embedding. Formal: First, we’ll explain the context vector for the source sentence in the source sentence encoder. Formal: Next, we’ll talk about the sentence embedding for the target sentence. Formal: Finally, we’ll combine the source context vector and the sentence embedding to get the final sentence embedding. Formal: Next, we’ll explain the sentence embedding for the target sentence. Formal: To make sure the model doesn’t just learn from the previous source sentence and the previous target sentence, we add a gradient to the current source context vector. Formal: This step helps the model learn from all the previous source tokens and the previous target sentence. Formal: We also add a forward hidden layer to the source",
        "directory": "acl",
        "filename": "2022.acl-srw.8.json"
    },
    {
        "casual_text": "Translators typically rely on commercial tools for their work, including both translation and post-editing tasks. According to the 2018 Language Industry Survey 1, which was put together by EUATC, Elia, FIT Europe, GALA, and LINDWeb, SDL Trados 2 was the most popular tool, used by over half of the market. It was followed by MemoQ, Memsource, Wordfast, and Across. \n\nBut here’s the thing: these post-editing tools have their limitations. Since they’re proprietary, they’re not super flexible or easy to tweak. Plus, they don’t usually offer much data on the translator’s activity, which could be useful for studying how much effort goes into post-editing. \n\nOn the flip side, there are open-source CAT tools like OmegaT that have been modified for data collection purposes. Moran and others explored this in a 2014 study.",
        "formal_text": "Professional translators usually use commercial products to translate and post-edit. In the 2018 Language Industry Survey 1 conducted by EUATC, Elia, FIT Europe, GALA and LINDWeb, SDL Trados 2 was the most used product with more than half of the market quota, followed by MemoQ, 3 Memsource, 4 Wordfast, 5 and Across. 6 However, these existing post-editing environments have a restricted availability and flexibility. As proprietary tools, they are difficult to modify and do not usually provide translator activity data that may be used to study post-editing effort. However, other open-source computer-assisted translation (CAT) environments such as OmegaT, 7 have been modified and used for data collection (Moran et al. , 2014).",
        "GPT2_formal_text": ", which are used to keep track of the different aspects of the input sentence. For a specific aspect, we'll define a kind of connection between the token and the argument. Formal: We take a sentence and stick it together with another sentence, using a length limit of n = 1, where n is the total number of sentences we have. Formal: We break the input sentence into n-grams and do the same for the output sentence. Formal: Formal: After that, we use some fancy math to figure out the probability of each token being a positive example, P(t|a). Formal: To make sure we're treating all tokens the same, we use a uniform distribution to make sure the argument spans are the same too. Formal: Formal: Next, we stick the combined sentence and token back together. Formal: Finally, we use the sentence and token to get the sentence's representation, which is h_s. Formal: Formal: The sentence representation h_s is kind of like the output sentence representation h_t, and it's calculated based on the token representation h_s. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "We need to create some solid computational tools that can take these source sentences, even if they're super unclear, and turn them into clear descriptions.",
        "formal_text": "Effective computational frameworks shoud be developed for producing such descriptions from source sentences which might be quite ambiguous.",
        "GPT2_formal_text": "= ivec_mod(p, t, D, t) + attention_mask_t, which is the average attention mask t that's created by all the attention heads. In our setup, we use sigmoid to represent the attention weight, and W and b are matrices with dimensions d×d and b, respectively. Formal: You can find the full code for our attention-based model and the multi-head attention setup in Appendix A. Formal: To explain how the model's parameters work, we'll break it down step by step. Formal: First, we'll talk about the special attention weight for the head entity, and then we'll introduce the multi-head attention mechanism. Formal: We calculate the attention weights for the input tokens, take the dot product of each attention head's representation, and the output from the multi-head attention mechanism, to get the attention weight for the token. Formal: The formula for the attention weights is: Formal: In Section 2, we'll explain how to calculate the attention weight for the input tokens. Formal: Following what Yang and his team did in 2020, we use the standard attention mechanism with a single-head attention setup. Formal: We also use a distance-based attention mechanism to tweak the attention weights for the tokens. Formal: In Section 3, we explain the multi-head attention mechanism. Formal: Section 4 talks about the multi-head attention mechanism, and Section 5 describes our model. Formal: Finally, in Section 6, we wrap things up with the results from our tests. Formal: In Figure 1, we show how the model's predictions are generated. Formal: The model's predicted attention weights are calculated based on the values of the input tokens, the entity, and the attention weights. Formal: The model is trained using a batch size of k = 1 and a learning rate of 0.00002, and the decoding process is sequential. Formal: The model uses a max pooling operation with a window size of 5 and a batch size of k = 1. Formal: The model has an input input length of (n + 1)/2 and a hidden size of d = 2k. Formal: Formal: For the multi-head attention mechanism, the attention weight for the head entity and the attention weights are calculated based on the input and the entity embedding. Formal: In Figure 1, the model's output is the attention weight for the",
        "directory": "C86",
        "filename": "C86-1155.json"
    },
    {
        "casual_text": "Using counterfactual reasoning in NLP usually involves defining a relationship between x and x, and then generating x based on that relationship. This has led to different counterfactual generators being designed for various tasks, each focusing on specific subsets of x that are relevant to the job. For instance, human annotators create counterfactuals like \"It is great for kids\" to help with model training and evaluation.",
        "formal_text": "Applications of counterfactual reasoning to NLP generally specify the relationship x )x, and then createx according to the relationship. As a result, prior work has tailored counterfactual generators for different applications, only collecting subsets ofx that are useful for the specific task. For example, to support model training and evaluation, human annotators create counterfactuals It is great for kids.",
        "GPT2_formal_text": "= [q1, q2, ..., qn, t, t2, ..., tk]. The annotation is basically a mix of the observed label and the model's guess. The model's guess can be shown as a way to explain the observed label by using a special kind of language model. We use this language model to check how likely it is that the observed label matches the model's guess. The model is trained using a particular method called bootstrap randomization, and each time it's trained, the probability of the model's guess is calculated based on the observed label. Formal: To make sure the model is reliable, we first normalize it. Formal: Here, f(x) is the chance that the model assigns the correct label to a specific example x, and P(x) is the chance that the model assigns a label that matches x. Formal: We use two basic methods to normalize the model. Formal: The first method uses a simple linear function to balance the model's prediction with the actual label, like in equation (1). Formal: The second method works in the opposite direction, with the model's prediction being more about how the label matches the actual label than about the model itself. Formal: In this part, we'll explain both of these methods. Formal: Formal: For the first method, we set P(x) = P(x|x) ∝ P(x|x) = 1. Formal: Formal: Formal: The second method is more flexible and can be used when the model's prediction isn't super clear or certain. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "Language models (LMs) try to figure out how likely a sequence of symbols {x t } T t=0 is, by looking at the joint probability.",
        "formal_text": "Language models (LMs) estimate the likelihood of a symbol sequence {x t } T t=0, based on the joint probability,",
        "GPT2_formal_text": "ing_normalized_copy = {0, 0.5, 0.9}         Casual: To make sure our model works well across different languages, we use a monolingual BERT model. Just like the zero-shot MSEP, we train it on two languages: one in the source language and one in the target language. Formal: We train the model using multilingual data, just like in the zero-shot setup. But instead of using the default target-language model, we swap it out with an English one to keep things fair. Formal: We also use our monolingual BERT model to translate the source and target languages back into the target language, keeping the same settings as in the zero-shot setup. Formal: Finally, we use this translation model to create a target-language version of our model and use that to train it again, focusing on just the source language. Formal: We use the same approach as before for training, and for testing, we just use the original BERT model as the target-language model. Formal: Formal: In Table 6, we check how well the model performs across different languages with different random seeds. Formal: We use the same training approach for all languages to keep things fair. Formal: Also, we test our model on the development set to see how it handles different amounts of data. Formal: Formal: Table 7 shows the accuracy for each model based on the number of steps it takes to translate one sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1150.json"
    },
    {
        "casual_text": "Like always, when we're dealing with limited resources, the improvements we see on the test and dev sets don't really match up. We can clearly see better results on the test set with the BTEC stuff, but when we apply SBR, the score on the dev set actually goes down.",
        "formal_text": "As usual, for the tasks with scarce resources the improvements on the test and dev sets are not coherent. While a clear improvement of test results can be observed in the BTEC results, the development set score degrades when SBR is applied.",
        "GPT2_formal_text": "= { ui }         Casual: In this project, we looked at how to use external knowledge for slot filling, which is a type of text generation. We used the CoNLL09 dataset (thanks, Brown et al., 2009) for this. We created two datasets to test how well a model can deal with different types of slots, like people, organizations, locations, and values. The training and test sets were based on the same areas, so we kept everything consistent between them. Formal: We picked the dataset size (D) based on how many slots the model could handle, but the actual number isn't always obvious. For example, if we look at the first sentence of Figure 1, the model can handle 3,967 slot types. This means the model can handle 1,500 different slot types. Formal: For the test set, we set D = 1,000, which is the same as the training set. Formal: We set α to 0.5 to make sure the model works well on the test set, and γ to 0.1 to make sure it doesn't do worse on the test set. Formal: We used the standard two-stage Bayesian learning algorithm (from Ma and Hovy, 2013) with a fixed learning rate of 0.001 and a window size of 20. Formal: We set the number of parameters to 5, and the number of layers to 10, and we trained it for 1 epoch with a batch size of 1,000. Formal: To get the best results, we used a straightforward \"perfect\" solution where we know the labels. Formal: We tested the model on two datasets: IWSLT14 and WN18RR. Formal: We used two different experiments setups: the first one was one-stage Bayesian learning with a fixed learning rate, and the second one was one-stage Bayesian learning with a learning rate of 0.001. Formal: The results are in Table 3. Formal: We compared our model to three other Bayesian models, including two versions of the CoNLL09 dataset. Formal: We compared our model to two models that use Bayesian methods for slot filling (like the ones by Ma and Hovy, 2013). Formal: For IWSLT14 and WN18RR, we used an \"extended\" Bayesian model, which means we trained the model with Bayesian priors. Form",
        "directory": "eamt",
        "filename": "2009.eamt-1.27.json"
    },
    {
        "casual_text": "To check how well the models are doing, we calculate the cosine similarity between the model's output and every possible ground truth vector. During training, we have 3701 word vectors to compare with, and during testing, we have 4284 (which includes the 3701 from training plus 583 new ones). \n\nWith these word vectors, we figure out the Recall@k (R@k), which tells us how often the ground truth vector is among the top k most similar vectors to the model's output. If the ground truth is the most similar to the model's output, that counts for R@1. Similarly, if the ground truth is in the top 5 (or top 10) most similar words to the output, it counts for R@5 (or R@10).",
        "formal_text": "To evaluate the models, the cosine similarity between a model's output and every possible ground truth vector representation is computed. The set of competing word vectors, therefore, consists of 3701 word embeddings during training, and of 4284 (3701 training + 583 testing) vectors during testing. Given these competing word embeddings, Recall@k (R@k) is computed as the proportion of times that the set of top k word embeddings which are closest to the model's output also includes the ground truth vector representation. If the ground truth is most similar to the output vector of a model, then this contributes to R@1. Furthermore, a word contributes to R@5 (R@10), if the corresponding ground truth word embedding is within the top 5 (top 10) most similar words to the output vector.",
        "GPT2_formal_text": "ed.o. A model is considered to be fully fine-tuned if it’s performing better than what you’d expect based on the base model’s performance. In other words, it’s likely performing better than a random baseline. To show how much better a model is, we use a fancy math trick called the Lagrangian relaxation, which is explained in Section 4.2. Formal: We’re looking at how well the model can learn when it’s not being fine-tuned. To do this, we calculate the expectation of the model’s performance for all the unknown words. Formal: For each model, we get the log-likelihood of the true label (e_T) for the target word x_t when x_t is unknown. This log-likelihood is calculated using a function called L(x_t). We plug this L(x_t) into Equation 1 to get the probability of the target word x_t being correct. Formal: If you have two sentences, e1 and e2, from a source sentence x and a target sentence y, you can figure out the log-likelihood of the target word x_t for source sentence x_t when x_t is unknown by adding up all the log-probabilities for the unknown words in both sentences. Formal: For our model, we use the log-probabilities from two sentences, e1 and e2, and then add the two log-probabilities together. Finally, we get the probability of x_t being correct. Formal: In the final section, we talk about how to train our model to use these log-probabilities effectively. Formal: For each training sentence, we train the model using the probability of the target word x_t when x_t is unknown, which we get by using the L(x_t) function. We can simplify this process by just maximizing the log-probabilities for all the unknown words in the source sentence, which we get by using the function L(x_t). Formal: Formal: We can train our model to use these log-probabilities effectively by optimizing the objective using the Lagrange function. Formal: Using the log-probabilities from two sentences, e1 and e2, and the two log-probabilities we got for them, we can estimate the log-",
        "directory": "eacl",
        "filename": "2021.eacl-srw.3.json"
    },
    {
        "casual_text": "This part looks at the stuff that impacts how well PLM works for automatically assigning ICD codes.",
        "formal_text": "This section provides analysis on factors that affect PLM's performance on automatic ICD coding.",
        "GPT2_formal_text": "= { { name_1, name_2, ..., name_T } ) # Get the list of names that match the query. For example, in the sentence \"Her name is Mary\", if we find two words \"Mary\" and \"Jane\" in the list, the model would predict that \"Jane\" is the answer for the question \"Mary's name is Jane\". Formal: For the two documents \"The Little Prince\" and \"The Wolf of Wall Street\", the questions \"What is his name\" and \"Where was he born?\" have the same answer, \"Virginia\". We don't have the exact matches for \"What is his name\" and \"Where was he born?\" because we don't have enough words to work with. Formal: For the two documents \"The Little Prince\" and \"The Wolf of Wall Street\", the questions \"What is his name\" and \"Where was he born?\" have the same answer, \"Virginia\". Formal: The models we're using are: Formal: The models we're using are: Formal: Formal: In this project, we're using the Amazon Web Services API to get the results from the name embeddings. Formal: Formal: Formal: We're using the source documents to get the embeddings for the query, but we're not using any knowledge base data. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "clinicalnlp",
        "filename": "2022.clinicalnlp-1.2.json"
    },
    {
        "casual_text": "Alright, let’s break down how we measure the connection between two sentences using word embeddings. There are three main methods for doing this:\n\n1. **Greedy Metric**: This one matches words in both sentences based on how similar their embeddings are (using cosine similarity). It then averages all these similarity scores to get a final number. This method was introduced by Rus and Lintean in 2012.\n\n2. **Average Metric**: Instead of matching words one by one, this method just averages all the word embeddings in each sentence to create a single vector for each sentence. Then, it checks how similar these two vectors are using cosine similarity. Mitchell and Lapata came up with this idea in 2008.\n\n3. **Extreme Metric**: This one is a bit different. It looks at all the word embeddings in a sentence and picks out the most extreme values (like the highest or lowest ones). Then, it uses these extreme values to create a vector for each sentence and compares them using cosine similarity. Forgues and his team introduced this method in 2014.\n\nSo, these are the three ways we can measure how semantically connected two sentences are using word embeddings.",
        "formal_text": "Relevance Metrics: The following three word embedding based metrics 3 are used to compute the semantic relevance of two utterances. The Greedy metric is to greedily match words in two given utterances based on the cosine similarities of their embeddings, and to average the obtained scores (Rus and Lintean, 2012). Alternatively, an utterance representation can be obtained by averaging the embeddings of all the words in that utterance, of which the cosine similarity gives the Average metric (Mitchell and Lapata, 2008). In addition, one can also achieve an utterance representation by taking the largest extreme values among the embedding vectors of all the words it contains, before computing the cosine similarities between utterance vectors, which yields the Extreme metric (Forgues et al. , 2014).",
        "GPT2_formal_text": "_vec (v, t) = (v_mask_t, r_mask_t, t_mask_t) Formal: The parameters s_i, t, and c_i, t are calculated using the softmax function. Formal: The weight w_i for the document-level attention is calculated like this: Formal: The task-specific attention weight λ_i is also calculated using the softmax function. Formal: The attention weight matrix is made up of three parts: Formal: The document-level attention attention matrix is calculated using the softmax function. Formal: The task-specific attention weight matrix is also calculated using the softmax function. Formal: The input document d is made up of vectors in R^d, and the output vector q_i is also in R^d. Formal: The model picks a query q_i by making a weighted average of the attention weights from the document-level and task-level attention vectors. Formal: The attention weight vector r_i, t is calculated using the softmax function. Formal: Formal: The model also calculates the attention matrix L_i, t by summing up the weights from both the document-level and task-level vectors. Formal: The whole thing is explained in detail in the paper. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1065.json"
    },
    {
        "casual_text": "Basically, the decision is always made using the formula in Eq. 1. The difference between models comes down to how they represent things and the weight vectors they use.",
        "formal_text": "In all cases, the decision is formulated as in Eq. 1, where different models differ by the representations and weight vectors used.",
        "GPT2_formal_text": "= { u idx, e idx } | p is the position of the query, and n is the size of the set P (u, e). Formal: Since the query and the query's context are linked, we can represent them as an aggregation matrix A. Formal: This matrix can be updated using the update rule A = A × e A ×(e i, u, e j ) = A × A × P(e i, u, e j ). In this project, we use the update rule A = A × e A ×(e i, u, e j ) = A × P(e i, u, e j ). Formal: In this project, we work with queries as their own sets. Formal: Let’s say q_i is the query set for q_i, and q_j is the query set for q_j. Formal: The set P (q_i, q_j) is made up of the query set q_i multiplied by the query set q_j, which is also multiplied by P(q_i, q_j). The query set P (q_i, q_j) can be updated using this aggregation matrix A = A × e A ×(e i, u, e j ) = A × A × P(e i, u, e j ). Formal: Also, the query set P (q_i, q_j) is made up of the query set q_i multiplied by the query set q_j, and the query set P (q_i, q_j) is also multiplied by P(q_i, q_j). Formal: So, the query set P (q_i, q_j) can be described as the combination of the query set q_i and the query set q_j. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D08",
        "filename": "D08-1037.json"
    },
    {
        "casual_text": "In this paper, we looked at how SABLE, a user-friendly system for building translation dictionaries, works for finding domain-specific word translations using small, specialized text collections. When tested on a really small corpus (around 400,000 words), the system did pretty well. It can suggest possible single-word translations by automatically removing common words. After filtering those out, the system gets a precision of up to 89% for domain-specific terms, with a recall that’s estimated to be between 30-40%, though this is a very cautious estimate.",
        "formal_text": "In this paper, we have investigated the application of SABLE, a turn-key translation lexicon construction system for non-technical users, to the problem of identifying domain-specific word translations given domain-specific corpora of limited size. Evaluated on a very small (400, 000 word) corpus, the system shows real promise as a method of processing small domain-specific corpora in order to propose candidate single-word translations: once likely general usage terms are automatically filtered out, the system obtains precision up to 89% at levels of recall very conservatively estimated in the range of 30-40% on domain-specific terms.",
        "GPT2_formal_text": "ing_4_seg, and for the other facets, it's the same as in the original paper. Formal: Our method works by creating an attention matrix based on the output of a feedforward neural network. The final hidden state of this attention matrix is a scalar, which is calculated using the GRU and then passed through the embedding layer. Formal: This means we make sure the output for each facet has the same type and the same attention value. Formal: So, we're basically saying that if the output for a facet is the same for both its input and the output of a feedforward neural network, then the input and output should both have the same attention value. Formal: In our experiments, we set a threshold value of 1 for the scalar token embedding layer to prevent any huge differences in the attention matrix between the input and output. Formal: For each facet, we use the standard evaluation metrics, like Mean Reciprocal Rank (MRR), F1, and Precision@N. The value of the threshold is calculated using the Log-likelihood ratio (MRR), which was suggested in (Li et al., 2015b). Formal: The specific values for the thresholds we picked are listed in the Appendix. Formal: The training and test sets are labeled as f_train, f_test, and t_train respectively. Formal: Finally, we calculate the average precision (P_P) and mean reciprocal rank (MRR) of the results. Formal: Formal: The best results come from models that perform consistently across the scores, where both P_P and MRR are above 0.9. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "A97",
        "filename": "A97-1050.json"
    },
    {
        "casual_text": "We started working on classifying words to help model a morpho-syntactic alternation in Nêhiyawêwin verbs. Inspired by Arppe et al. (2008), we came up with a hypothesis that both the semantic classes of the verbs and their noun arguments could play a role in this alternation. Because of time limits, we looked into ways to automatically classify both verbs and nouns in Nêhiyawêwin. While our main goal is still to use statistical modeling, semantic or thematic classifications can be super helpful for language learners and revitalization efforts, especially in online dictionaries. Instead of just looking up English translations, you could see all the words related to a specific theme, which is way more useful.",
        "formal_text": "We begun word classification in preparation for modelling a morpho-syntactic alternation in Nêhiyawêwin verbs. One hypothesis we developed for this alternation, based on Arppe et al. (2008), is that the semantic classes of the verbs themselves as well as their nominal arguments would inform the verbal alternation. Due to constraints of time, we investigated methods to automatically classify both verbs and nouns in Nêhiyawêwin. Although statistical modelling remains the immediate motivator for the authors, semantic/thematic classifications have a wide range of benefits for language learners and revitalization, particularly in online lexicographic resources, where one may want to view all words to do with a theme, rather than simply finding translations of single English words.",
        "GPT2_formal_text": "= {u_k} from w_s, where u_k is the batch size. This setup is similar to the one in (Reimers and Gurevych, 2019), but instead of using the model's output, we use the sequence of unlabeled class samples to create the query. Formal: We use the latest version of FastText (Mikolov et al., 2013) to convert the text to Base64 and then run it through a fully-connected layer. Formal: We then calculate the cosine similarity between the query and the gold standard labels. Formal: Next, we compare these results to the actual human rankings from Wikipedia, which we got using the NDCG dataset. Formal: We note that the scores for the final rows of rows 2 and 3 are way lower than the best possible score, which is what the human ranking system was supposed to achieve. Formal: After finding the best possible score, we compute the cosine similarity between the query and the gold standard labels using FastText. This similarity is then used as the final score to calculate the overall ranking. Formal: Formal: The best possible score is how well we match the human ranking for each row, based on a single human evaluation. Formal: Formal: Figure 1 gives a quick overview of how we calculate the cosine similarity between the query and gold standard labels. Formal: We evaluate the quality of the ranked list using two main metrics. The first one is precision, which tells us how many instances are in the list that match the correct answer. Formal: The second metric is recall, which tells us how many instances are actually in the list. Formal: The best possible score is the average of precision and recall. Formal: We also calculate the cosine similarity between the query and the gold standard labels. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "americasnlp",
        "filename": "2021.americasnlp-1.12.json"
    },
    {
        "casual_text": "Here are some examples of titles generated using rejection sampling. In Figure 7, you'll see titles that were picked and ones that got rejected. As you can tell, the rejected titles usually aren't as good.",
        "formal_text": "Examples of generated titles. We present examples of titles that from rejection sampling that are selected and that were rejected in sampling in Figure 7. As shown, rejected titles tend to be of lower quality.",
        "GPT2_formal_text": "* s = (s1, s2, ..., sL) ∈ R d ×d (1) Formal: To show that a state S can be identified, we just add up all the paths in the graph. Formal: We tweak the original AdaTM model by adding two new features: (s1, s2, ..., sL) ∈ R d ×d and (s1, s2, ..., sL) ∈ R d. The goal here is to find a weighted average path that gives the highest score. Formal: We use the following equation to calculate the weighted average path score. Formal: We figure out the path score for a state S and its label L by solving the following equation. Formal: We also use the steps outlined in Algorithm 1 to find the best possible weight for each path based on the weights of the state and its label. Formal: Finally, we apply these weights to the graph to find the best path. Formal: Formal: We train this model using a cross-validation setup. Formal: After training the model for a few epochs, we update it using the development set. Formal: Formal: The updated model is then used for the classification task. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.66.json"
    },
    {
        "casual_text": "c. GloVe: We use GloVe embeddings (Pennington et al., 2014) that we trained on a collection of 3 million words from a mix of recipe texts and video transcriptions. For any given instruction, we take the GloVe embeddings (Pennington et al., 2014) of the nouns and verbs and average them to get a single vector representing the instruction. When comparing two instructions, we measure how similar they are by looking at the cosine similarity between their vectors.",
        "formal_text": "c. GloVe: We train GloVe embeddings (Pennington et al. , 2014) on an in-domain corpus of 3 million words put together by combining text recipes and video transcriptions. Given an instruction, we average the GloVe embeddings (Pennington et al. , 2014) of nouns and verbs 12 to obtain its embedding vector. Given two instructions, we define their embedding similarity as the cosine similarity of their embedding vectors.",
        "GPT2_formal_text": "= {[CLS] } for each pair in the training set. This setup helps the model focus on predicting the right labels for sentences that have similar meanings. The loss term in Equation (1) is set up this way to help the model do this. Formal: We’ve got two things here: (a) the total loss for the whole model, and (b) how much we’re sacrificing performance for the loss term. Formal: The loss term for the whole model is made up of three parts: (a) the original loss, (b) a penalty for choosing the wrong labels, and (c) the loss term for each individual training instance. Formal: In this paper, we’ve shown that using latent semantic space can help model how nouns and verbs are connected and how words fit together in sentences. We also came up with a way to estimate these relationships using a simpler model that uses only binary features, which is similar to what other studies have done (like Zens et al., 2016a). Formal: We tested our model and found that it works better than other methods for modeling natural language relationships. Formal: We’ve developed a method that can automatically figure out the semantic space of words in sentences, and it also predicts the sentiment for those words. Formal: We’ve shown that this method can help model how words and their meanings connect in sentences. Formal: We’ve used this method to model how words and their meanings are related in a sentence, which helped us figure out how words are connected in sentences. Formal: We’ve put together a dataset for this project, and it’s been used to test how well current methods for understanding sentence structure work. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.440.json"
    },
    {
        "casual_text": "We tried this method: first, we ranked all the candidates using a similarity function and picked the top 3. Then, we only kept those 3 if they were within the last 4 sentences. Without worrying about their semantic class, this bumped up the precision to 41% (it was 30% if we limited the distance earlier, or 39% without limiting it). If we added a filter based on semantic classes (so only keeping the top 3 if they matched the right class and were in the last 4 sentences), we got a much better precision of 53%, with a decent recall of 57.8%. Compared to the similarity-list approach, we got way better precision than other methods with similar recall (like the one with the 100 most similar items, which has 44% precision, or the one with 50 items and two-way matching, which has 46% precision).",
        "formal_text": "The approach we tried was as follows: we rank all candidates using the similarity function, and keep only the 3 top-rated candidates. From these 3 toprated candidates, we keep only those within the last 4 sentences. Without filtering by semantic class, this improves the precision to 41% (from 30% for limiting the distance beforehand, or 39% without limiting the distance). Adding filtering based on semantic classes to this (only keeping those from the 3 top-rated candidates which have a compatible semantic class and are within the last 4 sentences), we get a much better precision of 53%, with a recall that can still be seen as good (57. 8%). In comparison with the similarity-list-based approach, we get a much better precision than we would get for methods with comparable recall (the version with the 100 most similar items has 44% precision, the version with 50 most similar items and matching both ways has 46% precision).",
        "GPT2_formal_text": "ing_tasks = [mask(T, T_m), mask(T, T_m_j), ..., mask(T, T_m, T_m_j)]\n\nThe input to attention is the prediction vector for the masked token, and it's calculated using a feed-forward neural network. Formal: The whole process of TAC KBP, which includes the forward and backward layers, is shown in Figure 1. The model starts with the input prediction, which is in R^d. We then use these masked tokens as the input for the forward and backward layers of the model. Formal: Each word w t in the input is represented as a vector in R^d. We use the attention weights from the forward and backward layers to get the prediction, which is R^d_t. Formal: At each step t, we calculate the attention weights for each token by averaging the hidden states from the last two layers of the network. Formal: For each input token x t, the forward and backward hidden states for the last two layers of the model are calculated using the attention weights for the last two layers, and this is then used as the input for the prediction. Formal: For more info, check out the paper by Zhou et al. (2017). Formal: We also calculate the attention weights for the words x t as the attention weights for the last two layers of the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D07",
        "filename": "D07-1052.json"
    },
    {
        "casual_text": "Hey, just a heads-up: we're using the same dataset for both training the discourse parser (MEGA-DT) and evaluating sentiment (Yelp'13). To keep things fair, we're only using the training part of the dataset to train the discourse parser. This means the development and test documents haven't been seen at all during the training process.",
        "formal_text": "Please note that since we use the same base-corpus for training the discourse parser (MEGA-DT) and predicting sentiment for the final evaluation (Yelp'13), we restrict the data used to train the discourse parser to the training-portion of the corpus. This way we ensure that development-and test-documents are unseen during the whole training process.",
        "GPT2_formal_text": "_maint; ..., -k, where k is the total number of characters in the input. Formal: This is the part where we calculate the cosine similarity between the output and the reference text, using the formula: Formal: To get a better idea of how the model is doing, we take its average score over time for each time step. Formal: Finally, we add up all the cosine scores from all the time steps. Formal: Figure 1 shows how the model behaves over time. We ran a bunch of tests with different numbers of time steps and the results are all in Table 1. Formal: We tested our model on three different datasets and plotted the average accuracy in Figure 1. The top part shows the results for the PD and FST datasets, and the bottom part for the MA, DST, and AL datasets. Formal: Our model is getting better at predicting the next word based on the context of the input, but it's still not as good as the reference. Formal: The model did better on the PD dataset for predicting the next word, but it still doesn't match the reference's performance. Formal: The model improved on the DST and AL datasets for predicting the next word, but it still struggles with the FST and MA datasets. Formal: The model did really well on the DST and AL datasets for predicting the next word, but it still doesn't match the reference's performance. Formal: Our model is using a different input and output structure than the reference, which helps it perform better than the reference in a few cases. Formal: Our model can predict both the next word and the context of the input better than the reference, so we can use it for tasks like paraphrase extraction or generation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "coling",
        "filename": "2020.coling-main.16.json"
    },
    {
        "casual_text": "The reference translation is the smoothest overall. Makes sense since it's written by humans. On average, the reference translation scored the same in terms of adequacy compared to the others. Our reference was collected from the web, so it has some issues, as mentioned in section 3.3. One big problem with the reference translation is that it focuses on the whole document, which makes it less adequate for sentence-by-sentence translation. This is especially noticeable in conversational texts, where the reference was translated based on the entire session (like a talk or vlog). Here's an example:\n\nSource: \"-Nope, they're shutting us down.\"\nRef: \"-Tidak, misi ditunda.\"\nOurs: \"-Tidak, mereka menutup kita.\"\nGoogle Translate: \"-Tidak, mereka menutup kita.\"",
        "formal_text": "The reference translation is the most fluent across every domain. This result is expected, as the reference is written by humans. Reference translation's adequacy scored equally on average, compared to the rest. Our reference is crawled; therefore, it contains several issues, as mentioned in section 3. 3. . One main problem in reference translation is that they are translated with document level in mind, therefore reducing adequacy as encapsulated sentence-based translation. This is especially true in conversational, where the reference was translated from the whole session (i. e. , talk, or vlog). One example can be seen below: Source \"-Nope, they're shutting us down. \" Ref \"-Tidak, misi ditunda. \" Ours \"-Tidak, mereka menutup kita\". Google Translate \"-Tidak, mereka menutup kita. \"",
        "GPT2_formal_text": "_rank(u_t, w_t)          Casual: We use a multiloss random forest to figure out the feature representation h_t. Formal: Instead of just looking at the attention score, we also consider the correlation coefficient between the logits of the attention head and the words in the query. Formal: We calculate the logits of the attention head and the query using the exponential function, then do an average of all the logits to get the feature representation. Formal: Next, we take the attention score, the logits of the query and the attention head, and average them. Formal: Finally, we get the rank of the query by averaging the logits of the query and the attention head. Formal: Formal: Formal: The formula for logit similarity in a sub-space is (α_t, r_t, ..., β_t, r_t) = (1 / c)V_T, where c is the cosine distance between the query and the attention head. Formal: Theorem 1: If you have a sub-space S_t, you can calculate the logit similarity for all the words in S_t to get the similarity for the query by running the equation (1 / c)V_T. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "bucc",
        "filename": "2020.bucc-1.6.json"
    },
    {
        "casual_text": "Sometimes, language models (LMs) can get confused and think \"and\" signals a similar relationship between words, but it might go beyond just the word you're looking at. For instance, if you type \"The human heart not only makes heart sounds and,\" the LM might predict \"muscle\" as one of the top suggestions, and then follow up with \"movements.\" So, it's not just matching \"sounds\" but also the whole phrase \"heart sounds\" with something like \"muscle movements.\" We didn't really focus on this in our current work, but we think making sure the predictions match the target word in terms of grammar (like being the same part of speech or having the same number) could help fix this. Plus, this opens up a cool possibility for moving beyond just single words and dealing with whole phrases instead.",
        "formal_text": "Multi word phrases substitutes Sometime the LM does interpret the and as a trigger for a symmetric relation, but on a chunk extending beyond the target word. For example, when presented with the query The human heart not only makes heart sounds and, the forward LM predicted in its top twenty suggestions the word muscle, followed by a next-word prediction of movements. That is, the symmetry extends beyond \"sounds\" to the phrase \"heart sounds\" which could be substitutes by \"muscle movements\". We didn't specifically address this in the current work, but note that restricting the prediction to agree with the target word on part-of-speech and plurality may help in mitigating this. Furthermore, this suggests an exciting direction for moving from single words towards handling of multi-word units.",
        "GPT2_formal_text": "= { id1, id2, ..., idL }\n           Casual: A chunk of data is usually a mix of a few things: words, lemmas, or symbols. Usually, these chunks are smaller compared to sentences. Formal: If a chunk has a symbol, it might be labeled as a lemma or a shorthand for something else. We’ll call these broader chunks lemmas. We’ll also group sub-chunks that share the same sub-word, but those sub-chunks aren’t considered part of the main chunk. Formal: There are two main types of chunk embedding: 1) Linear, which is just the average of all words, and 2) Non-linear, which is just the average of different tokens. Formal: For LCRF to work well, we need to train a model that can pick up on different types of knowledge in the data. This is a big deal for a bunch of applications, like checking how well a program is doing, understanding how text is structured, or figuring out how documents are organized. Formal: LCRF embeddings can be split into three types: linear, non-linear, and dynamic. Formal: In this paper, we’re using linear LCRF embeddings. Formal: In this section, we’re introducing two new methods to handle different types of knowledge in context-free grammars: Formal: We’ll show you how to use linear LCRF embeddings to learn general representations of meaning. Formal: We’ll also show how to use non-linear LCRF embeddings to learn specific representations for each sub-word. Formal: We’ll call this approach \"context-free grammars\" because it’s kind of like how traditional context-free grammar (CFG) formalisms are structured, but with context. Formal: The hybrid approach we’re proposing mixes both linear and non-linear LCRF embeddings into a single model. Formal: We’ll also explain how to calculate context-free grammars that work for any language using a CFG formalism. Formal: In this paper, we’ve explained how to create language grammars that work with a CFG formalism. Formal: For a more detailed look at the hybrid model,",
        "directory": "D18",
        "filename": "D18-1523.json"
    },
    {
        "casual_text": "In Table 3, it looks like basic emotions (like joy, anger, or sadness) usually perform better than complex emotions (like positive, neutral, or negative). But in Table 1, the amount of data for basic emotions is often smaller than for complex ones. This suggests that the difference in performance is probably due to the emotional content of the labels, not the size of the data. For example, the complex emotion 'negative' (which includes things like 'hate' and 'anxious') is way more varied than the basic emotion 'sad', and that variety makes it harder to detect. \n\nAlso, even though 'sad' and 'angry' are both basic emotions and have similar amounts of data, it seems way easier to spot 'sad' than 'angry'. Maybe this is because 'angry' can be caused by all sorts of different things, and it's harder to figure out and use that information. So, for emotion classification, it's important to have an encoder that can pull out the event-related info about what caused the emotion from the text. \n\nTable 4 shows how different emotion cause detection models perform, with \"Sequence\" listing the order of words each model uses. In Table 4, JMECause does better than the best pipeline model (LSTM) by 0.8% in terms of F-scores.",
        "formal_text": "In Table 3, the performance of a basic emotion (i. e. , joy, angry or sad) is often better than the one of a complex emotion (i. e. , positive, neutral or negative). However, in Table 1, the data size of a basic emotion is often smaller than the one of a complex emotion. This indicates that difference in performance is likely linked to differences in the emotional contents of labels rather than differences in data sizes. E. g. the complex emotion 'negative' (i. e. a collection of complex emotions with negativity, such as 'hate', 'anxious', and so on) is more diverse than the basic emotion 'sad', and this diversity in emotional contents brings more challenges to the detection of this complex emotion. Furthermore, even if both 'sad' and 'angry' are basic emotions and have similar data sizes in our experimental data, it seems much easier to detect 'sad' instances than to detect 'angry' instances. This is maybe because 'angry' is caused by more various events and it is more difficult to capture and utilize those cause events. Thus, it is necessary for the emotion classification to have an encoder which can extract the eventbased information of emotion cause from texts. Table 4 shows the performances of different emotion cause detection models, where \"Sequence\" lists the sequences of input words used by each model. In Table 4, JMECause outperforms the best pipeline model (LSTM) by 0. 8% in F-scores.",
        "GPT2_formal_text": "_u: (1) So, the main goal of this paper is to figure out the best attention mechanism that works best for predicting the most relevant sentiment for a given input. We’ll call this method the attention-based masking layer. Formal: A lot of research on figuring out emotions (like Wang et al., 2016, and Chen et al., 2016) is pretty similar to ours. But unlike them, we’re focusing on figuring out the hidden aspects of an input and using that to guide the attention during the classification. Formal: Our approach is different from those earlier studies because we’re introducing a new model that takes into account the hierarchical structure of an input. Formal: We’re also working on figuring out the best masking layer for sentiment classification. Formal: We’ve come up with a new masked language modeling (MLM) method to help with this, and we’ve developed a new attention-based masking layer that helps us get the best output prediction. Formal: We’ve come up with a new MLM method to better manage the hierarchical structure of an input. Formal: We’ve developed a new attention-based masking layer to help with the hierarchical structure of an input. Formal: We’ve also shown that the attention-based masking layer can better capture the complex structure of an input. Formal: We’ve developed a new MLM method to better manage the hierarchical structure of an input. Formal: We’ve also demonstrated that the attention-based masking layer can capture the complex structure of an input. Formal: We’ve also developed a new MLM method to better manage the hierarchical structure of an input. Formal: We’ve demonstrated that the attention-based masking layer can capture the complex structure of an input. Formal: We’ve developed a new MLM method to better manage the hierarchical structure of an input. Formal: We’ve also developed a new MLM method to better manage the hierarchical structure of an input. Formal: We’ve also developed a new MLM method to better manage the hierarchical structure of an input. Formal: Formal: We’ve developed a new MLM method to better manage the hierarchical structure of an input. Formal: Formal: We’ve also developed a new MLM method to better manage the hierarchical structure of an input.",
        "directory": "D18",
        "filename": "D18-1066.json"
    },
    {
        "casual_text": "Lexical Semantics, Ambiguity, and Plausible Assignments. The accuracy numbers we’re getting from our methods are kind of \"pessimistic\" in a way, meaning they should be seen as lower limits. When we dig into the mistakes our models make, we find that a lot of the frame assignments they come up with are actually pretty reasonable, even if they don’t match the exact labels in the leave-one-out test. Take the LU \"guerrilla,\" for example. In FrameNet, it’s assigned to the frame PEOPLE BY VOCATION. Our mixed model suggests the two most similar frames are MILITARY and TERRORISM, which still make sense, right? Same thing with the LU \"caravan.\" Our model says the closest frame is VEHICLE, but in FrameNet, it’s only linked to the frame BUILDINGS. This happens because FrameNet doesn’t cover everything—some LUs aren’t fully annotated and only show up in a few of their possible frames. So, the actual accuracy of our models is probably higher than what the numbers suggest.",
        "formal_text": "Lexical Semantics, Ambiguity and Plausible Assignments. The overall accuracies achieved by our methods are \"pessimistic\", in the sense that they should be intended as lower-bounds. Indeed, a qualitative analysis of erroneous predictions reveals that in many cases the frame assignments produced by the models are semantically plausible, even if they are considered incorrect in the leave-one-out test. Consider for example the LU guerrilla, assigned in FrameNet to the frame PEOPLE BY VOCATION. Our mixed model proposes as two most similar frames MILITARY and TERRORISM, which could still be considered plausible assignment. The same holds for the LU caravan, for which the most similar frame is VEHICLE, while in FrameNet the LU is assigned only to the frame BUILDINGS. These cases are due to the low FrameNet coverage, i. e LUs are not fully annotated and they appear only in a subset of their potential frames. The real accuracy of our models is therefore expected to be higher.",
        "GPT2_formal_text": "\" is the collection of all the stuff our model has learned so far. Formal: We use a linear SVM model to figure out the right label for each text. To test how well this model works, we use a supervised classification task on the test set to see how well our model can classify things correctly. Formal: We train the model using the training set and see how well it performs on the test set. We then check the performance of the model on the test set using the standard validation set. Formal: We tweak the model by using the generated examples from the test set and the explanations from the training set. This helps us figure out the best ways to represent the text and create explanations for it. Formal: Our model can classify text, but we need to look at the source to see how good the explanations are. Formal: We train the model using both the training set and the test set to see how much the model can handle new, unseen text. Formal: For the discriminative part, we try to maximize the log-likelihood. Formal: For the semantic classifier, we use the source to check how well the explanations match the text. Formal: We train the model using the training set and the test set to see how well the model can handle new, unseen text. Formal: We train the model using both the training set and the test set to see how well the model can handle new, unseen text. Formal: The main goal of SVM is to find the best-possible way to represent a text. Formal: We use a linear SVM model to find the best way to represent the text. Formal: To get the best way to represent the text, we use a linear SVM model to find the best way to represent the text. Formal: We train the model using both the training set and the test set to see how well the model can handle new, unseen text. Formal: Our model can classify text, but we need to look at the source to see how good the explanations are. Formal: We use a linear SVM model to find the best way to represent the text. Formal: To get the best way to represent the text, we use a linear SVM model to find the best way to represent the text. Formal: Formal: Formal: We use a linear SVM model to find the best way to represent the text. Formal: Formal: Formal:",
        "directory": "D08",
        "filename": "D08-1048.json"
    },
    {
        "casual_text": "Microsoft's statistical MT engine has two types of decoders: one that uses a parser to create dependency treelets, which is helpful for translating between languages with different word orders. Then there's a simpler string-based decoder that doesn't need any fancy linguistic info to work. This one's great for quickly training on language pairs when you don't have a parser handy.",
        "formal_text": "Microsoft's statistical MT engine uses two different decoders: A syntactically informed tree based decoder, which uses a parser building dependency treeletsbetter for translating between languages with different word orders. And a string-based decoder that needs no linguistic information to work. Good for fast training of language pairs without a parser.",
        "GPT2_formal_text": "ed_vec: This is a combined attention layer that takes the attention vector h x i and turns it into a single value using an attention mechanism. Formal: The second type of attention is called the slot attention matrix. This matrix is created using a word embedding, which we call w y i. Formal: The third type of attention is called the attribute attention matrix. It helps show how important certain attributes are for a particular slot. Formal: In the final step, we combine all these attention vectors to get the final attention representation h. Formal: This part will explain how to use all this information for a prediction and also how to calculate the probability p(t a |t b ). Formal: So, we know that the main feature we're looking at, h x i, is the vector h x i. Now, we can figure out the probability p(t a |t b ), which gives us the probability of the tag t a happening. Formal: The main goal is to maximize the probability P(t a |t b ). Formal: To make this work, we need to figure out the best way to combine all these vectors. Formal: We can use an approximate method, where the vectors can be off by one or more elements. Formal: We suggest using a pointwise average to make the vector representation more stable. Formal: The part about how to find the best alignment between the main feature and the slot feature vectors is pretty much the same as what we did in the first part. Formal: Formal: Finally, we calculate the probability P(t a |t b ) using Equation 5. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "2010.amta-commercial.10.json"
    },
    {
        "casual_text": "The weights in the logistic regression model, as shown in Table 1, help us predict how the classifier will behave with new data. When we look at the features, specifically the one about matching syntactic paths and the one that checks if the first mention is in the subject position, we notice both have positive weights. However, the first feature (syntactic path match) is stronger than the second (subject position), which tells us that parallel roles seem to have a bigger effect than just the subject position of the antecedent. Based on this, we can guess that the Subject Assignment Strategy is at play, but it’s paired with the Parallel Function Strategy. And if the Parallel Function Strategy comes into play, it seems to have a stronger pull and might even override the Subject Assignment Strategy.",
        "formal_text": "The weights of the logistic regression model in Table 1 predict the preferences the classifier will show on experimental data. Looking at the feature of syntactic path match and the feature that checks if the first mention is in the subject position, we see that both features have a positive weight; but we can also see that the first is stronger than the second, suggesting that parallel roles are of a greater impact than the subject position of the antecedent. From this data we can hypothesize that the Subject Assignment Strategy exists alongside the Parallel Function Strategy, and that the Parallel Function Strategy, if applicable, has a stronger influence that can overrule the Subject Assignment Strategy.",
        "GPT2_formal_text": "= {'q': q, 'q_a': a, 'q_n': n} for each pair of entities e and f. Formal: The model works by picking the best possible answer for a given question q and the entity e. After that, it uses a linear layer to create a vector representation of the entity, which we call e_n. Formal: In this project, we're using the encoder-decoder framework, which was introduced by Vaswani et al. in 2017. To make the model more efficient, we can easily swap out the sequence-to-sequence model for an attention-based one. Formal: So, if you have a question and an entity e, the goal is to predict a vector e_q with the highest probability. Formal: For each possible answer e_a_1 to e_a_n, the model calculates a list of possible labels, like {'a': a_1, ..., 'n': n} for each pair of entities e and f, which we call L. Formal: After that, the model gives us the best label by using an attention mechanism to figure out the probability of a label. Formal: Here's how the model is built: Formal: The model starts with an initial state q_i, which includes the current question q_i and the entity e_i, and a list of possible labels L. Formal: After that, the model updates the current state by adding the new entity e_j, which comes from an attention mechanism, and the probability of a label, which we call p_a_j. Formal: The model then predicts the next answer, q_a_n, by adding up the probability of all the possible labels L and p_a_n. Formal: Finally, the model figures out the best label by using a linear layer, which is the usual approach. Formal: Formal: The overall process of this model is shown in Figure 1. Formal: We'll use the encoder-decoder model to give each possible answer a label, q_a_n, based on the question and entity. Formal: In the encoder, the sequence-to-sequence model is used. Formal: Finally, the model generates the final answer, q_a_n, by summing up the probability of all the possible labels L and p_a_n. Formal: Formal:",
        "directory": "E17",
        "filename": "E17-4006.json"
    },
    {
        "casual_text": "There are a few ways to normalize matrix D1. One common method is called Sinkhorn balancing, which was introduced by Sinkhorn in 1964. This technique turns a square matrix with non-negative elements into a doubly stochastic matrix, meaning both the rows and columns add up to 1.\n\nSinkhorn balancing works through an iterative process. At each step, it calculates the sums of the rows and columns, then uses those sums to rescale the matrix. To balance matrix A, each iteration involves two updates: A gets multiplied by WR and then by WC. WR is a diagonal matrix with the inverse of the row sums of A, and WC is similar but with the inverse of the column sums.\n\nThis algorithm converges linearly, so it might take a lot of iterations to get the job done. You can tweak it to normalize the rows and columns using any norm you like. From what we've seen, normalizing D1 using the Euclidean norm works pretty well in real-world applications.",
        "formal_text": "There are several possibilities for normalizing the matrix D 1. Sinkhorn balancing (Sinkhorn 1964 ) is a popular technique for creating a doubly stochastic matrix (rows and columns all sum to 1) from a square matrix of nonnegative elements. Sinkhorn balancing is an iterative algorithm in which, at each step, the row and column sums are computed and then subsequently used to rescale the matrix. For balancing the matrix A, each iteration consists of two updates A W R A A AW C where W R is a diagonal matrix containing the inverse of row sums of A, and W C is a diagonal matrix containing the inverse of column sums of A. This algorithm exhibits linear convergence, so many iterations may be needed. The algorithm may be adapted for normalizing the row and column vectors according to any norm. Our experience has shown that normalizing D 1 with respect to the Euclidean norm works well in practice.",
        "GPT2_formal_text": ", which is basically the attention weight for the word w_i, to figure out the predicted probability p(w_i). Formal: To get a clearer picture of how the model's predictions change based on different aspects, we’ve included the average prediction of the whole dataset, p(w_i|S), for both regular and fine-tuned models. Formal: In Figure 4, we’ve got the average prediction of the whole dataset, p(w_i|S), for fine-tuned models and regular ones. Just a heads-up, p(w_i|S) is calculated using the same method as p_S, but instead of using the actual log probability, it uses the normalized version. Formal: Figure 5 shows the average prediction of the whole dataset, p(w_i|S), for fine-tuned models and regular ones. Formal: Formal: Figure 6 is the average prediction of the whole dataset, p(w_i|S), for the fine-tuned models. Formal: Formal: Figure 7 shows the average prediction of the whole dataset, p(w_i|S), for the regular models. Formal: Formal: Figure 8 is the average prediction of the whole dataset, p(w_i|S), for the fine-tuned models. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C08",
        "filename": "C08-1007.json"
    },
    {
        "casual_text": "We're using an NVIDIA GeForce RTX 2080 to train the smaller versions of GPT-2 and BERT base, and a GeForce RTX 8000 for training the medium GPT-2. For BERT large, we're using Tesla M40 GPUs. Since we're starting with pre-trained models, we can wrap up training each one in about two weeks. It takes just one GPU for GPT-2 Small, BERT base, and GPT-2 Medium, but we need four GPUs for training BERT large.",
        "formal_text": "We use NVIDIA GeForce RTX 2080 for training GPT-2 Small and BERT base, GeForce RTX 8000 for training GPT-2 Medium, Tesla M40 for training BERT large. Since we start from the pretrained LM, we can finish training each LM within 2 weeks using 1 GPU for GPT-2 Small, BERT base, and GPT-2 Medium, and using 4 GPUs for training BERT large.",
        "GPT2_formal_text": "= (chat_id, question, answer) < {0, 1} | {0, 2} | {0, 3} | {0, 4} | {0, 5} | {0, 6} | {0, 7} | {0, 8} | {0, 9} | {0, 0}                    Casual: Alright, so you've got a bunch of text chunks here, and you can break them down into smaller parts if you want. Now, you're creating a text summary using all these chunks. Formal: Instead of just grabbing the bits of text that actually make sense, you're looking for the ones that can actually sum up the whole conversation. Formal: We check all the chunks we found to see if they make sense together. Formal: At the end, we pick the most important ones and get rid of the rest. Formal: If the summary doesn't follow the rules, we toss out the whole chunk. Formal: So, the text summary we end up with is a mix of these different parts. Formal: The summary is made by combining all the parts from each chunk. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.554.json"
    },
    {
        "casual_text": "So, basically, DATR theories can work as a kind of dictionary for a PATR system. In this setup, the words (or lexemes) are like DATR nodes, and when you ask about them, you get back these feature structures. In a dictionary set up like this (like in example 6), you can see three types of nodes:\n\n1. The main ones, like FROG, which are the actual words.\n2. Nodes like SYNTAX or NV, which are like the attributes in the PATR system.\n3. Nodes like NOUN or LEX/CAL, which are more like categories or types.\n\nThe words get their general info from these type nodes, but the specific details about each word are directly linked to the main nodes.",
        "formal_text": "It has been shown above that DATR theories can serve as a lexicon for a PAT R system where the lexemes are represented as DATR nodes and the returned values of queries are the corresponding feature structures. In a lexicon which is formulated as in (6), aparl; from the lexical nodes (i. e. nodes like FROG which define lexemes) two other kinds of nodes can be distinguished: nodes like SYNTAX or NV, which correspond to PATR attributes, and nodes like NOUN or LEX/CAL, which represent a kind of type information (see Pollard/Sag, 1987). The lexemes inherit this information through reference to the type nodes, while the lexeme-specific information is as~ciated direct. ly with the lexical nodes.",
        "GPT2_formal_text": "ed ( ) = { 0.1, 0.5, 0.8, 0.9, 1, 1.5, 2, 2.5, 3, 3.5, 4, 4.75, 5, 6, 7.0, 8.0, 9.0, 10, 11.0, 12.0, 14.0, 15.0, 16.0, 17.0, 18.0, 19.0, 20.0, 22.0, 24.0, 25.0, 30.0, 35.0, 40.0, 45.0, 50.0, 55.0, 60.0, 65.0, 70.0, 75.0, 80.0, 85.0, 90.0, 95.0, 100.0, 105.0, 110.0, 115.0, 120.0, 125.0, 130.0, 135.0 } N. The weights are adjusted for hyperparameter uncertainty, with p values ranging from 0.3 to 1.0. Formal: The goal of this part is to figure out the highest possible value for (a) for each attention layer, based on the learned parameters for that layer. Formal: We calculate the gradient from this point using the following formula: Formal: Following the method from (Kipf and Welling, 2017), we estimate the objective function (a) by using the weighted dot product between the cross-entropy value (b) and the learned parameters for the attention layer. Formal: The learned parameters for the attention layer are set to be the normalized version of the normalized cross-entropy value. Formal: The weight for the input to the attention layer is calculated by multiplying the difference in attention weight values between the input and output vectors. Formal: The final input to the attention layer, w_a, is the normalized cross-entropy value for the attention layer. Formal: Here, w_a is the normalized cross-entropy value for the attention layer. Formal: We also calculate the attention loss for each layer separately. Formal: For each layer, we calculate the loss based on the attention weight, w_a. Formal: Figure 2 shows the attention weights for a specific layer, L_a. Formal: We calculate the loss for the attention layer using the attention weight, w_a. Formal: For the layer we're looking at, the",
        "directory": "E91",
        "filename": "E91-1024.json"
    },
    {
        "casual_text": "Vocabulary is something we need to keep an eye on when working with test data. TSNI, P does this by limiting both the size and the scope of the vocabulary. It tries to steer clear of words that are tricky to categorize or have multiple meanings, unless the test specifically wants to check how people handle that kind of ambiguity.",
        "formal_text": "Vocabulary is an aspect of the test data that needs to be controlled. TSNI, P achieves this by restricting the vocabulary in size as well as in domain. Categorially and semantically ambiguous words are avoided where possible and only included when ambiguity is explicitly tested for.",
        "GPT2_formal_text": ") = {\n          Casual: The main goal of this project is to create a universal way to map text into a relational database. Formal: We're starting with the idea that when you're dealing with text in a relational way, the way you write should follow a certain structure. We're using the method from (Fried et al., 2006) that connects entities in text to a database. To make this happen, we’re using a model called RNNLM, which was introduced by Klein and Manning in 2001. Formal: So, let’s say we have a sentence in the form of x = [x1, ..., xn], and for each i from 1 to n, we have a document d = {d1, ..., dn}. Using RNNLM, we can turn x into a sequence of RNNs, which gives us the vectors v i, j, k, which are the representations for the entities in the sentence. Formal: Using the RNNLM method, we’ve already figured out how to connect entities in text to a relational database. Now, we’re using RNNLM to figure out the relationships between documents and entities. Formal: Since the RNNLM model is designed to be flexible, it can handle different types of relationships. The cool thing about RNNLM is that it can go beyond just adding or removing entities or relationships, which is a common approach in relational processing. Formal: The model we’re talking about here looks at how words and phrases are connected in sentences, like how they’re organized in a sentence structure. Formal: It’s pretty easy to tweak this model to work with different types of relationships, whether they’re relational or not. Formal: We’ve shown how these relationships in text are represented using a relational database. Formal: Here, we’re using RNNLM to create a relational database, Db, that holds all the information about pairs of entities, like what they’re called, their type, where they’re from, and how they’re connected to each other. Formal: Since the RNNLM model is flexible, we can also make it even better by adding extra knowledge about the entities in the sentence. Formal: Here’s a simpler version of the RNNLM model we’re proposing: Formal: The",
        "directory": "C96",
        "filename": "C96-2120.json"
    },
    {
        "casual_text": "Okay, so we have this thing where w = w_1, ..., w_W. At a specific spot in the text, let's call it position i, the element d_i in d = d_1, ..., d_W tells us which document has the word w_i in it. And then there's this vector z = z_1, ..., z_W that kind of does something similar.",
        "formal_text": "w = w 1, • • •, w W. At corpus position i, the element d i in d = d 1, • • •, d W designates the document containing observed word w i. Similarly, the vector z = z 1, • • •, z W defines",
        "GPT2_formal_text": "_t = attention_mask_t[i, j] | attention_mask_t[i, j] = tanh(B_t * h_t + b_t + w_t) [4] 1 2 3 4 5 Formal: We're using the same setup for calculating the attention weights, but we're adding an extra term to the loss function. This term is called attention_exp. Formal: We're using the same setup for calculating the attention weights, but we're adding an extra term to the loss function. This term is attention_exp. Formal: Lastly, we're using the same function for the output layer. Formal: Finally, we're using the same function for the output layer. Formal: Formal: Formal: We're using the same setup for calculating the attention weights, but we're adding an extra term to the loss function. This term is attention_exp. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C12",
        "filename": "C12-1182.json"
    },
    {
        "casual_text": "Coecke and his team (2010) used category theory to give distributional models of meaning a way to combine things, making sure that each grammatical step has a matching mathematical operation. Basically, if you have a sentence s = w1 w2 • • • wn, there's a linear map f that takes the context vectors of the individual words and combines them into a single vector representing the whole sentence.",
        "formal_text": "Using the abstract framework of category theory, Coecke et al. (2010) equip the distributional models of meaning with compositionality in a way that every grammatical reduction is in one-to-one correspondence with a linear map defining mathematical manipulations between vector spaces. In other words, given a sentence s = w 1 w 2 • • • w n there exists a syntax-driven linear map f from the context vectors of the individual words to a vector for the whole sentence:",
        "GPT2_formal_text": "= [], f = [], n = 3, d = 5). The model outputs are shown in Figure 2. Formal: The weight vector f is made up of three parts: a root vector, a normalized position vector, and a random uniform distribution. Formal: The uniform distribution is calculated as d = d / (h + j), where d is the size of the embedding space and h is the size of the hidden layer. Formal: The normalized position vector f_n is calculated similarly, but here, d is the size of the embedding space. Formal: The third part of the uniform distribution, d_ij, is calculated as d = d_ij / (h + j), where d_ij is the size of the hidden layer that includes the j-th dimension of the embedding space. Formal: We use a multi-head attention mechanism to combine all these elements to get the final representation h_n. Formal: The input hidden state h_n_1 is a vector, H_n_1 = (V_n_1 + m_n_1), where V_n_1 is the output of the attention mechanism. Formal: We define a softmax function for the hidden state, which helps us figure out the probability distribution p_i for the i-th word, h_i. Formal: The softmax is calculated based on the parameters θ_t, H_t, and m_t, where θ_t, H_t, and m_t are the parameters of the attention mechanism, as well as the memory for the sequence. Formal: The final output h_n_1 is a single number, h_n_1 = [], where h_n_1 is the single output from the attention mechanism. Formal: We calculate the conditional probability p_i(h_n_1 | h_n_1) using the predicted probability distribution p_1(h_n_1 | h_1) and the current attention distribution p_2(h_n_1 | h_2), which we call H_n_2 = (p_1(h_n_1 | h_1) + p_2(h_n_2 | h_2)). Formal: Formal: Finally, the attention weights θ_t_ij are calculated as θ_t_ij = (V_t_ij +",
        "directory": "C12",
        "filename": "C12-2054.json"
    },
    {
        "casual_text": "Okay, so for each item i in T, where i is between 1 and the total number of items in T:\n\n1. Set up the sentiment classifier SC (0) using the data from Ω ij, and make sure j is randomized from 1 to the total number of items in Ω i, with i still being between 1 and the total number in T.",
        "formal_text": "s i in T, 1 ≤ i ≤ |T | 2: Initialize sentiment classifier SC (0) based on cf e(Ω ij ), randomize j ∈ [1, |Ω i |], 1 ≤ i ≤ |T | 3:",
        "GPT2_formal_text": "ed. It's a function that gives a probability to the label based on how often words appear in the same sentence. Formal: The baseline method uses a simple, non-autoregressive approach that starts with a word and a fixed window size. It builds on the standard BERT setup by adding a word-level self-attention and then uses a bidirectional LSTM to process the whole sentence and figure out the hidden state for the next word. Formal: The attention-based approach, which uses a bidirectional LSTM, takes the last hidden state as the output. It then combines the hidden state with the character sequence from the query word to create the query representation. Formal: The non-autoregressive method is a bit more flexible. It combines the last hidden state as the output and also uses a different bidirectional LSTM for the lookup process. Formal: We're looking at the non-autoregressive method in two different ways: first, it uses a standard bidirectional LSTM to process the sentence, and second, it uses a special bidirectional LSTM to decide the query for the next word in the sentence. Formal: The non-autoregressive method also uses a non-autoregressive model, but it uses a different bidirectional LSTM that also decides the query for the next word. Formal: We also tried the encoder-decoder bidirectional LSTM, which uses a bidirectional LSTM to process the sentence and the query word, just like the non-autoregressive method. Formal: Lastly, we tested the encoder-decoder bidirectional LSTM, which uses a bidirectional LSTM to process the sentence and the query word, as well as the query word itself. Formal: Our experiments show that using either LSTM or bidirectional LSTM can significantly boost performance. Formal: Our approach is different from the standard BERT-based bidirectional LSTM in two main ways. First, we're not using a fixed-size window size as in BERT-base. Second, we use a non-autoregressive decoding mechanism, unlike BERT, which just spits out the hidden state for the next word in the sentence. Formal: We used a transformer encoder-decoder model with a transformer layer, a GCN (graph neural network), and different transformer layers. Form",
        "directory": "D14",
        "filename": "D14-1054.json"
    },
    {
        "casual_text": "We'll start by looking at the frames that have been used most recently and also the ones linked to noun phrases that refer back to something earlier in the sentence.",
        "formal_text": "Those frames most recently used and those which a~e associated with anaphoric noun phrases within the sentence are examined first.",
        "GPT2_formal_text": "= { u1, u2, ..., uk }, where each ui represents a tuple (int, char) in the input. To avoid the issue with mislabeled data, we use a supervised model to predict the next character in the input. The probability P(n | a_t, c_t) is defined as: Formal: We've set up the system using a supervised method. But since the data we're dealing with comes from text, we use a language model to make better predictions. Formal: The results of the experiments we just talked about are in Figure 3, showing the correlation between the predicted character c_t and the actual character n. Formal: There's a decent correlation, but it's not super strong. For example, the correlation is 0.53 for u1 and 0.27 for u2, and it's 0.41 for uk. Formal: This means the system tends to predict the character n that has the highest probability. Formal: But it's not as clear-cut. For u1, the correlation is 0.39, and for u2, it's 0.33. Formal: Figure 3 gives a visual of how the correlation changes for each model. Formal: Using a language model gives us better predictions for u1 and u2. Formal: But for uk, the correlation is only 0.18. Formal: The whole thing is a bit of a mess, but it's worth mentioning that our model is trained on supervised data, so it can predict the next character in the input with a decent accuracy. Formal: But that accuracy doesn't always match the actual character, especially for u1, where the correlation drops by more than 60% compared to a basic model. Formal: This makes sense because we're using a non-autoregressive model, which means we can't just predict the next character directly, like you can with a traditional autoregressive model. Formal: In this part, we'll talk about how we set up our experiments, which is all laid out in Table 1. Formal: To handle the problem of not having enough data for training, we're using a stochastic approach to learn the probability P(t = uk | a_t) for the next character n. Formal: We figure out the probability by looking at the maximum of 0 and 1, assuming the log likelihood of uk given a_t and a_t,",
        "directory": "C82",
        "filename": "C82-1011.json"
    },
    {
        "casual_text": "A lot of research about evaluating machine translation (MT) focuses on identifying and categorizing MT mistakes. For example, studies like those by Vilar et al. (2006), Farrús et al. (2010), Stymne and Ahrenberg (2012), Lommel et al. (2014), and Klubička et al. (2018) have done this. However, not many papers look at how people actually perceive these errors, and none of them really nail down what exactly makes a translation good or bad.\n\nKirchhoff et al. (2014) took a different approach by using conjoint analysis to figure out what users prefer when it comes to MT errors. They started by labeling the errors in MT translations and then showed different versions with various types of errors to people who evaluated them. The evaluators were asked to pick the MT output they liked best and explain why.\n\nOne interesting finding was that the frequency of certain error types didn’t match up with what users preferred. For instance, word order errors were the least liked, even though they happened less often. After that came word sense errors (like ambiguity), then morphological errors (which were the most common). On the other hand, errors with function words were the least annoying to people.",
        "formal_text": "Lot of research on MT evaluation deals with classification and analysis of MT errors, for example (Vilar et al. , 2006; Farrús et al. , 2010; Stymne and Ahrenberg, 2012; Lommel et al. , 2014; Klubička et al. , 2018). Few papers deal with human perception of these errors, but neither of them defines precisely which criterion is the translation quality based on. Kirchhoff et al. (2014) uses conjoint analysis to investigate user preferences for error types of SMT systems. First, the errors in MT outputs were annotated, and then MT outputs with different error types were given to the crowd evaluators. They were asked to choose the MT output which they like best and to give the reason for their preference. One of the findings is that the frequencies of error types are not related to the user preferences. The most dispreferred error type was word order error, although it was the least frequent one. It was followed by word sense errors (ambiguity), then morphological errors (most frequent ones), whereas errors in function words were the most tolerable.",
        "GPT2_formal_text": "ed_LSTM(x) = max(softmax(x), 0), dtype=lloyd) end\n          Casual: This model is trained to predict the label y based on the input x. Formal: For the classification task, the model gives each output y a probability score, which we call p_x. Formal: We look at the joint probability distribution p_y (y, x) for the whole sentence. Formal: The model is trained using a specific cross-entropy loss function. Formal: To find the best set of parameters for this loss function, we pick the model that gives the best performance on the validation set. Formal: The function calculates the best possible loss function using the training data, which we call the validation loss. Formal: We set the probability p_x for the whole sentence as the parameter λ. Formal: The objective function is the least square loss function. Formal: We also calculate the ratio between the log of the difference between the log of the error for the most common label y and the log of the error for the other labels. Formal: Lastly, we add up the loss terms for all the individual labels. Formal: We use a linear model to train the model with these parameters. Formal: For each training epoch, we pick the model that gives the best prediction for the validation set. Formal: The model is updated with the parameters from the last epoch. Formal: The model is tested on the validation set and the test set. Formal: We keep updating the model with the new parameters until it stabilizes. Formal: The loss function changes based on the validation loss and the log of the error for the label y. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "conll",
        "filename": "2020.conll-1.19.json"
    },
    {
        "casual_text": "The model's performance really depends on the set of permutations used during training. We don’t know exactly how B&L came up with their permutations, but we’re guessing they were generated completely randomly.",
        "formal_text": "The quality of the model learned depends on the set of permutations used in training. We are not aware of how B&L's permutations were generated, but we assume they are generated in a perfectly random fashion.",
        "GPT2_formal_text": "= { h t } for t in [h t ].         Casual: In this paper, we suggest using a bi-directional version of the Skipgram model to handle data with repeated entities. The cool thing about our model is that it can tell apart entities that are the same but have different words, which is a big deal for situations where we want to figure out the unique entities. We think this is a big deal because it lets us pinpoint the exact spots where entities show up, which could be really helpful for spotting these rare cases later on. Formal: In this paper, we introduced Skipgram. It uses a bi-directional version of the skipgram model, which we call Bi-GRU-Bi-GRU. The idea is that if we have a sentence S that has two entities, e t and e o t, the model could figure out the entity e t in S. Formal: Our model, Bi-GRU-Bi-GRU, works with both a source and a target sequence. To make it work better, we add a special token called [CLS] to make the model more flexible. Formal: The Bi-GRU-Bi-GRU model takes in a sequence of words x t and a hidden representation h t. The model then decides whether to predict the word x t, which could be either e t or e o t, or it can just spit out the prediction as a single unit. Formal: Here’s how the model can be rewritten in simpler terms: Formal: In Figure 2, you can see how the Bi-GRU model generates a sentence y t. Formal: The Bi-GRU model uses a bi-directional GRU to predict the entity e t. Formal: The first part of the GRU (the sequence of words) is fed into a linear GRU, which then predicts the word e t. Formal: The second part of the GRU, the hidden representation, is also fed into a linear GRU, but this time it predicts the entity e o t. Formal: The output from both the GRU and the linear GRU together is the predicted entity e t. Formal: The model also uses a dropout function to stop the model from messing with the input words. Formal: The output from the GRU and the linear GRU together are the predicted entities. Formal: The model also uses a dropout",
        "directory": "E12",
        "filename": "E12-1032.json"
    },
    {
        "casual_text": "We need to understand x_i. So, we suggest replacing 3. This goal is a bit different from the minimum risk training method used by Li and Eisner (2009), and it's a difference that matters. In both approaches, θ* aims to minimize risk or expected loss. However, the expectation is calculated with respect to different distributions: in Li and Eisner (2009), it's based on the conditional distribution p(y | x), whereas in our case (1), it's based on the joint distribution p(x, y).",
        "formal_text": "requires to know x i. So we propose to replace 3 This goal is different from the minimum risk training of Li and Eisner (2009) in a subtle but important way. In both cases, θ * minimizes risk or expected loss, but the expectation is w. r. t. different distributions: the expectation in Li and Eisner (2009) is under the conditional distribution p(y | x), while the expectation in (1) is under the joint distribution p(x, y).",
        "GPT2_formal_text": "ing layer gets turned into a multi-head attention layer, where the hidden units of the current word and the last word are combined. Formal: We’ve come up with a new attention network setup that’s different from the usual one and focuses on encoding relations, thanks to a special attention layer. We tested it on a bunch of common relation classification tasks, and it turned out to be the best model we’ve seen so far. Formal: In this part, we’ll show how our relation encoder works for relation extraction, focusing on specific relations and explaining how it learns the representations for a specific relation pair. Formal: Lastly, we’ll talk about how to pull out both entities and relations from text, with a special attention mechanism for the entities. Formal: The model we’re proposing has three main parts: an attention network, an entity encoder, and a relation encoder. Each of these parts has a different attention mechanism that works with the input text, and we’ll walk you through all that in the next sections. Formal: The entity encoder uses a multi-head attention mechanism with a weighted average of all the words in the sentence, and the relation encoder combines both the entity and relation vectors using a ReLU activation function. The entity encoder gets its final representations from a feed-forward neural network, while the relation encoder gets its representations from a bidirectional LSTM. Formal: The entity encoder takes the whole input text and uses multi-head attention to pull out both entities and relations from the text. Formal: To handle context-dependent relations, the entity and relation encoders calculate multi-head attention scores for each pair of entities. The entity encoder combines these scores with the predicted probabilities from the relation encoder. Formal: The entity encoder combines the entity representation with the relation representation. The entity encoder gets its representations from a feed-forward neural network. Formal: The relation encoder combines the entity representation with the predicted probability from the relation encoder. Formal: The entity encoder uses a multi-head attention mechanism with a weighted average of all the words in the sentence. Formal: The entity encoder combines the entity representation with the predicted probabilities from the relation encoder. Formal: The relation encoder combines the entity representation with the predicted probability from the relation encoder. Formal: The entity encoder combines the entity representation with the predicted probabilities",
        "directory": "D11",
        "filename": "D11-1085.json"
    },
    {
        "casual_text": "We're sharing the macro-averaged F1 scores for pronoun translation, which we got using a simplified version of AutoPRF (a method by Hardmeier and Federico from 2010). Basically, for each sentence in our test set, we look at the pronouns in the system's translation and compare them to the ones in the reference translation. We adjust the counts based on what's in the reference, and then use those numbers to calculate precision, recall, and F1 scores.",
        "formal_text": "Evaluation. We report the macro-averaged F1 scores of the pronoun translation based on a simplified version of AutoPRF (Hardmeier and Federico, 2010). For each sentence in the testset, the counts of the pronouns in the system translation are clipped based on the pronouns in the reference translation; these counts are then used to compute the precision, recall and F1 scores.",
        "GPT2_formal_text": "= ( SELECTOR, text, cursor ) SELECTOR = ( SELECTOR, text, ctr )\n           Casual: The value of the SELECTOR should be a string that shows how the text should be handled, like a query. Formal: The SELECTOR can also be a list of values, which is like a dictionary for the text. Formal: The SELECTOR can also be a list of values, which is like a dictionary for the text. Formal: You can get a reference to the SELECTOR from the text. Formal: You can get a reference to the SELECTOR from the text. Formal: You can use the SELECTOR to define the values for the text. Formal: If there's no SELECTOR, you can just pick the text you want to generate, like in SELECTOR(TEXT) = ( SELECTOR, text ). Formal: If there's no SELECTOR, you can use the SELECTOR to define the values for the text. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.177.json"
    },
    {
        "casual_text": "• Users. This tab lets the project manager create accounts for people and assign them to specific tasks. Each person will only see the tasks they’ve been assigned to. Users can’t see other people’s annotations unless they’re in \"revision mode,\" which shows an existing annotation for them to review.",
        "formal_text": "• Users. In this tab the project manager can create accounts for users and assign them to different tasks. Each user will see only the task(s) s/he has been assigned to. Users do not see other users' annotations unless they are working in \"revision mode\", where an existing annotation is presented for revision.",
        "GPT2_formal_text": "ing = self.as_input; self.attn_masking = self.attn_masking_type; self.attn = self.attn_type; self.attn_masking = self.attn_masking_type; self.attn_type = self.attn_type_type; self.attn_type_type = self.attn_type_type; self.attn_type_type = self.attn_type_type_type; } # Tokenization, storing, and decoding of tokens. def tokenize(self, tokens): self.token_size = self.token_size * self.token_len; self.token_len = tokens; self.attn = self.attn; self.attn_masking = self.attn_masking; self.attn = self.attn_masking; self.attn_type = self.attn_type; self.attn_type_type = self.attn_type_type; self.attn_type_type = self.attn_type_type; self.attn_type = self.attn_type_type; self.attn_type_type = self.attn_type_type; self.attn_type_type = self.attn_type_type; self.attn_type_type = self.attn_type_type_type; self.attn_type_type = self.attn_type_type_type; self.attn_type_type = self.attn_type_type_type; self.attn_type = self.attn_type_type_type; self.attn_type_type = self.attn_type_type_type; self.attn_type_type = self.attn_type_type_type; self.attn_type = self.attn_type_type_type; self.attn_type = self.attn_type_type_type; self.attn_type = self.attn_type_type; self.attn_type_type = self.attn_type_type_type; self.attn_type_type = self.attn_type_type_type; self.attn_",
        "directory": "C14",
        "filename": "C14-2026.json"
    },
    {
        "casual_text": "For ACSA, we make templates by hand. These templates have one spot for the category and another for the sentiment type (like positive or negative). We have a set of category words, A = {a1, ..., a|C|}, where |C| is the number of category types (for example, a1 could be \"price\"). We also have a set of polarity words, P = {p1, ..., p|L|}, where |L| is the number of polarity types (like p1 being \"positive\"). We use these words to create templates like \"The sentiment polarity of price is positive\". The basic template is \"The sentiment polarity of ai is pk\". So, for any category ai, we can make a bunch of these templates.",
        "formal_text": "For ACSA, we manually create templates containing one slot for the given_category and another slot for the polarity_type label. We set a category word set A = {a 1, . . . , a |C| }, |C| is the category type size (e. g. , a i =\"price\") and polarity type word set P = {p 1, . . . , p |L| }, |L| is the polarity type size (e. g. , p k =\"positive\"), and use words to define templates T a i, p k (e. g. \"The sentiment polarity of price is positive\"). The template T is \"The sentiment polarity of a i is p k \". For a given category a i, we can obtain a list of templates",
        "GPT2_formal_text": "= [q s i, q t i ] for q s i, q t i in a ij ∈ R dp. Formal: These embeddings are trained using a max-margin loss function that tries to minimize the conditional cross-entropy loss (L_CEL(q s i, q t i ))) across a set of 32,000 labeled examples. Formal: After the last layer, we get the final embeddings, u ct i, for each prompt s i by averaging the scores from all the data points. Formal: Formal: Following the approach by Guo et al. (2019), we use binary cross-entropy loss (BCE) loss to optimize the loss function. Formal: We set the start window to 10 for both the prompt and the target domain, which helps avoid the issue of vanishing gradients. Formal: For a good evaluation, we use the validation set that includes all possible prompts. Formal: For the model parameters, we randomly pick from a uniform distribution. Formal: We're using a special logistic regression model with a uniform activation function. Formal: We're using an n-gram language model to predict the relevance score between the source and target domains, and we also use a contrastive learning rate to control how much we penalize the low-probability target domain's responses. Formal: In this setup, the similarity between the source and target domains is calculated using a method called the ILP (Inverse-Loss) algorithm. Formal: After the last layer, we get the embeddings, u ct i, for each prompt s i by averaging the scores from all the data points. Formal: After the first layer, we use binary cross-entropy loss (BCE) loss to optimize the loss function. Formal: We set the start window to 10 for both the prompt and the target domain, which helps avoid the issue of vanishing gradients. Formal: We're using a special logistic regression model with a uniform activation function. Formal: We're using an n-gram language model to predict the relevance score between the source and target domains, and we also use a contrastive learning rate to control how much we penalize the low-probability target domain's responses. Formal: Formal: Formal: After the last layer, we get the embeddings, u ct i, for each prompt s i by averaging the",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.361.json"
    },
    {
        "casual_text": "Okay, let me break this down in simpler terms:\n\nx, y, and s IRF( ) are just the weight matrix, bias vector, and squashing function for the IRF layer at specific time steps x and y. Meanwhile, g 0x, y (τ ) = τ is just saying that τ is equal to itself. \n\nNow, w IRF(1) and b IRF(1) are the initial weight and bias vectors that are applied globally for the first layer of the IRF. These vectors are used to transform the scalar τ, and each of them is adjusted based on its own random effects. W just refers to the whole thing.",
        "formal_text": "x, y, and s IRF( ) are respectively the th IRF layer's weight matrix at predictor timestep x and response timestep y, bias vector at time x, y, and squashing function, and g 0x, y (τ ) = τ. w IRF(1), b IRF(1) are respectively globally applied initial weight and bias vectors for the first layer of the IRF, which transforms scalar τ, each of which is shifted by its corresponding random effects. W",
        "GPT2_formal_text": ", which is basically the number of nodes in the sequence. Since the set of nodes can be pretty big, we'll call it `N_v` for short. Formal: We'll call the scores `B_v` for short. The total score for a word is just the sum of all its scores. Formal: We'll also use the highest word-level score for the whole sentence, `S_v`. This score is calculated by multiplying `h_v` by the best possible score for the node `n_v`, where `v` ranges from 0 to 1. Formal: To check if a token matches a context, we use something called token-level embedding (like the one by Bahdanau et al., 2015). To do this, we use the value of `S_v` in equation (5) multiplied by `d_v`. Formal: Lastly, we consider the sequence to be complete if the total score for all the words in it is greater than the score for the smallest possible token. Formal: Formal: We use equation (5) to calculate the score for the tokens in the input sequence, `h_v`. Formal: We also make sure that the token scores are no more than the length of the sequence. Formal: For all these checks, we use an accuracy of 95%. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.288.json"
    },
    {
        "casual_text": "You can limit your search in different ways, like choosing a specific area to look in. And when you get the results, you can focus on certain parts of the information by picking specific fields from EURODICAUTOM.",
        "formal_text": "Queries can be restricted in a number of ways (e. g. by indicating domain). Output can be restricted by selecting specific EURODICAUTOM fields:",
        "GPT2_formal_text": "\": [ ], \"num_word_ids\": [ ], \"num_pos_ids\": [ ], \"num_pos_len\": [ ], \"number_of_sequences\": [ ], \"word_length\": [ ], \"word_pos\": [ ], \"word_len\": [ ], \"word_cnt\": [ ], \"word_cnt\": [ ], \"sequence\": [ ], \"sequence\": [ ] }            Casual: We're using the NICT 1-gram dataset for English, which has 27 million tokens in total (check out Table 3). We split the tokens into training, validation, and test sets with 6,000 each. For the loss, we're using the Adam optimizer with a batch size of 32. We're testing our model on three different NLP tasks. Formal: We used the development set to fine-tune and then checked the results for a few epochs to make sure everything was working well. Formal: We also used the development set to fine-tune and then checked the results for a few epochs to make sure everything was working well. Formal: We followed the same training split and loss split as in the original paper to adjust the model. Formal: The evaluation results are in Table 4. Formal: We picked the top-performing model by calculating the weighted average F1 score. Formal: We also calculated the weighted average F1 score for the results without any actual training examples. Formal: We trained the model for 20 epochs on a single GPU and the results are in Table 5. Formal: We trained the model for 20 epochs on a single GPU and the results are in Table 6. Formal: Formal: Table 6 shows the performance with and without example losses for five different NLP tasks. Formal: Formal: The results are in Table 7. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "1998.eamt-1.2.json"
    },
    {
        "casual_text": "We also tried out some cutting-edge transformer-based seq2seq models, like Bert-SumExtAbs (Liu and Lapata, 2019) and BART (Lewis et al., 2019). Bert-SumExtAbs needs the encoder to be fine-tuned and the decoder to be trained from scratch, while BART just fine-tunes both the encoder and decoder. We only used AMI data for training and fine-tuning, and the results from these models are shown in the bottom two rows of Table 6. \n\nEven though our hier2hier t-learn model only needs the decoder to be fine-tuned and uses hierarchical attention, the more complex semi-supervised training of both the encoder and decoder in BART, along with its much larger size (100x), gives better performance. But, if you're working with limited memory, like on some mobile devices, our model might be a better choice. \n\nAlso, even though Bert-SumExtAbs has a pre-trained encoder, training a big decoder from scratch with just a small AMI dataset led to overfitting, which resulted in lower scores.",
        "formal_text": "We also experimented with state of the art transformer-based seq2seq models, e. g. , Bert-SumExtAbs (Liu and Lapata, 2019) and BART (Lewis et al. , 2019). BertSumExtAbs requires finetuning of the encoder and a de novo training of decoder while both encoder and decoder of BART are only fine-tuned. We use only AMI data for the de novo training and fine-tuning purpose, and the bottom two rows in Table 6 show the results from these models. 1 Although our hier2hier t-learn also only requires fine-tuning of the decoder and hierarchical attention, a highly-sophisticated semisupervised training of both the encoder and decoder of BART and larger model size (100x) yields better performance. However, for applications that have limited memory, as on some mobile devices, our model may be more desirable. Furthermore, despite a pre-trained encoder of BertSumExtAbs, a de novo training of a large size decoder with a tiny AMI data lead to over-fitting, and therefore, lower scores.",
        "GPT2_formal_text": "ing is an update to the original Transformer model by Wang et al. (2020). It adds a binary attention layer on top of the encoder's output to focus on the important parts of the input. We're using the pre-trained BERT model by Devlin et al. (2019) as the teacher model. Formal: Our experiments show that the BERT model can really boost the performance of our attention masking method. This makes it clear that BERT is a strong model for attention modeling. Formal: In this paper, we're using the BART model by Lewis et al. (2019), which is a transformer-based model. Formal: The BERT model was pre-trained using the IWSLT 2019 development set. Formal: For our experiments, we trained the model on the same batch of test data we used for training. Formal: We trained the model on the development set of the CoNLL-2014 shared task (called \"SST-2\") and used the training set from the CoNLL-2016 shared task (called \"CST-2\"). Formal: We used the token-based attention method from Lu et al. (2019) for both token and span attention. Formal: We added the BERT model by Wang et al. (2020) to our model. Formal: For the experiments in this paper, we randomly pre-trained the BERT model on the development set of the CoNLL-2014 shared task (SST-2) and used the validation set from the CoNLL-2016 shared task (CST-2). Formal: BERT was pre-trained on the development set of the CoNLL-2014 shared task (SST-2) and used the validation set from the CoNLL-2016 shared task (CST-2). Formal: We randomly pre-trained the BERT model on the development set of the CoNLL-2014 shared task (SST-2) and used the validation set from the CoNLL-2016 shared task (CST-2). Formal: For the experiments in this paper, we randomly pre-trained the BERT model on the development set of the CoNLL-2014 shared task (SST-2) and used the validation set from the CoNLL-2016 shared task (CST-2). Formal: We trained the model on the development set of the CoNLL-2014 shared task (SST",
        "directory": "adaptnlp",
        "filename": "2021.adaptnlp-1.24.json"
    },
    {
        "casual_text": "Basically, we used three different methods (MUSE, VecMap, and XLM-R) to train cross-lingual models. For MUSE and VecMap, we got the cross-lingual mapping from monolingual embeddings, like we mentioned earlier. This gave us thirteen models for each of those two methods. On the other hand, we only made one model using XLM-R for the third approach, which we’ve already explained. We take the last layer of the XLM-R model to create representations for each token.",
        "formal_text": "To put it more concisely, we trained cross-lingual models using three different methodologies (MUSE, VecMap and XLM-R) where the cross-lingual mapping obtained for MUSE and VecMap were generated via the monolingual embeddings, as described above. We obtained thirteen models using each of these two methods. A single cross-lingual model was, however, trained using XLM-R and used for the third cross-lingual approach whose training methodology has been described above. We utilize the last layer from the XLM-R model to generate representations for each token.",
        "GPT2_formal_text": "(w_i) = h_i + w_i * Σ(x) * Σ(y) * w_i * x + Σ(y) * y * Σ(x) * y * w_i * y + Σ(x) * x * y * Σ(y) * x * y * y * x, where y * y * x is just the sum of all the values in the form Σ(x) * Σ(y) * Σ(x) * y * x. Formal: The task of identifying the subjective aspect of an event involves figuring out how much the event contributes to the overall meaning of the text. Basically, the analysis is like trying to find the smallest amount of the event that fits the overall meaning of the text. Formal: The hidden structure of the text is represented by the hidden state h_T, which is basically a matrix that includes both the event's history and the context of the text. Formal: Our model looks at the history and the context together to predict the aspect. Formal: We think the model can learn from both the history and the context. To make this happen, we can figure out the hidden state h_T using a multi-head attention mechanism. Formal: We use a multi-head attention mechanism to process the history, which helps us learn the hidden state h_T. Formal: We use a sentence-level attention mechanism to represent the context, and this helps us learn the hidden state h_T. Formal: We can also learn the hidden state h_T by using a sequence-level attention mechanism. Formal: For the hidden state h_T, we use the softmax function to encode the context. Formal: Formal: Formal: The whole process of creating a model that can learn from both the history and the context is shown in Algorithm 1. Formal: Formal: Formal: Here's how we calculate the loss function for the loss function learned in this way. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.119.json"
    },
    {
        "casual_text": "Step 3 is all about checking if the marker from the prediction tree matches up with the elementary tree for \"open.\" To make parsing PLTAG easier and more efficient, Demberg and his team (2013) came up with the idea of \"fringes.\" Fringes help us understand that when building a tree step by step, you can only connect a prefix tree to an elementary tree at certain spots. \n\nFor example, in Figure 3, the prefix tree has two places where you can make a substitution, one for B and one for C. But if you try to substitute into B, you get a valid new prefix tree. If you substitute into C, you end up with the tree shown in Figure 3b, which isn't a valid prefix tree—it’s like trying to build the tree in a way that doesn’t work step by step.",
        "formal_text": "Step 3 involves the verification of the marker introduced by the prediction tree against the elementary tree for open. In order to efficiently parse PLTAG, Demberg et al. (2013) introduce the concept of fringes. Fringes capture the fact that in an incremental derivation, a prefix tree can only be combined with an elementary tree at a limited set of nodes. For instance, the prefix tree in Figure 3 has two substitution nodes, for B and C. However, only substitution into B leads to a valid new prefix tree; if we substitute into C, we obtain the tree in Figure 3b, which is not a valid prefix tree (i. e. , it represents a non-incremental derivation).",
        "GPT2_formal_text": "* s, while using the cross attention for the visual object features. Formal: We're using a bidirectional GRU with a hidden state size of d = 2048 to figure out the attention vector for the object features. Formal: We take a sequence of word embedding vectors and run it through a GRU to get the attention vector for the visual object features. Formal: We take an input sequence x = {x 1, . . . , x t } and run it through a GRU to get the attention vector for the visual object features. Formal: We train a network using five different representations for the visual object features by combining the embeddings of the word embeddings from the previous layer, which gives us five different vectors for each visual object feature. Formal: We use the cross attention for both the visual object features and the text features. Formal: The visual object features get combined with the text embeddings in the last layer, and the text embedding is used as the attention vector for the visual object features. Formal: We use a GRU with a hidden state size of d = 2048 to get the attention vector for the visual object features. Formal: Formal: We train a network using five different representations for the visual object features. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1036.json"
    },
    {
        "casual_text": "Hyp-Negation. This idea pops up when the hypothesis has a word like \"no\" in it. Basically, if this concept is around, it strongly suggests that an NLI model will predict the answer to be \"contradiction,\" no matter what else is going on in the NLI stuff (Gururangan et al., 2018).",
        "formal_text": "Hyp-Negation. This concept is realized in instances containing negation words (e. g. , 'no') in the hypothesis. The presence of this concept is highly indicative of an NLI model's prediction to be the contradiction class regardless of other NLI semantics (Gururangan et al. , 2018).",
        "GPT2_formal_text": "= [x_i, ..., x_i + 1] y_i = [x_i, ..., x_i + 1] x = [x_1, ..., x_M] y = [y_1, ..., y_M + 1]\n\nSo, the answers to questions x and y are created by combining the representations from these two layers. We use the CNN with BERT to pull out these representations. The encoder layers then help turn these representations into meaningful words, which gives us the actual answers to the questions. Formal: The encoder layers in the encoder-decoder model can be thought of as combining a sentence's representation with its embedding (check out Figure 1). Formal: For each question, the sentence x is represented by a matrix that's |A| × |C| × |D|. The embedding for the sentence x is |E| × |F| × |E|. For instance, the embedding for the sentence \"black dog\" is |E|×|F|×|E|. Formal: Lastly, the encoder-decoder model uses a bidirectional GRU to combine the sentence's representation with its embedding (we call this the final output). Formal: The input to the GRU is a sequence of vectors x, y, and z, which are |A| × |C| × |D|. The output is a vector where each element is a vector of embeddings, and they're |E| × |F| × |E|. Formal: Formal: We're using a single layer of BERT with a hidden size of 50, with a learning rate of 0.001 and a batch size of 1. Formal: We're using a single layer of BERT with a hidden size of 50, with a learning rate of 0.001 and a batch size of 1. Formal: The input to the GRU is a sequence of vectors x, y, and z, which are |A| × |C| × |D|. The output is a vector where each element is a vector of embeddings, and they're |E| × |F| × |E|. Formal: Formal: Using all the inputs we've talked about, we get the output embedding y = [y_i, ..., y_i + 1]. Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.120.json"
    },
    {
        "casual_text": "Alright, so here's the deal: with the zero-one loss (01), the risk R(g) is calculated using π + F N F N +T P + π − F P T N +F P. During training, we use something called \"sig,\" which is basically a smoother version of this formula that works better for backpropagation. In real-world training, we take the average of these losses and optimize them using methods like batched gradient descent or something similar.",
        "formal_text": "Notice that under the zero-one loss ( 01 ), the risk R(g) refers to π + F N F N +T P + π − F P T N +F P. When training, we use sig which can be regarded as a soft approximation of this formulation for backpropagation. In practice, the expectations are expressed as the average of losses and optimized in batched gradient-descent or similar methods.",
        "GPT2_formal_text": "ing_m. The final output is a weighted sum of all the attention weights, with each weight being a value from 1 to the total number of tokens in the sentence. Formal: -v_a(w) is the attention vector for the aspect a_w. Formal: -v_b(w) is the attention vector for the aspect b_w. Formal: -v_c(w) is the attention vector for the context c_w. Formal: -v_d(w) is the attention vector for the descriptor d_w. Formal: -v_e(w) is the attention vector for the entity e_w. Formal: -v_f(w) is the attention vector for the mention f_w. Formal: -v_g(w) is the attention vector for the group g_w. Formal: -v_h(w) is the attention vector for the pronoun h_w. Formal: -v_i(w) is the attention vector for the named entity i_w. Formal: -v_j(w) is the attention vector for the pronoun j_w. Formal: -v_k(w) is the attention vector for the mention k_w. Formal: -v_l(w) is the attention vector for the mention l_w. Formal: -v_m(w) is the attention vector for the mention m_w. Formal: -v_n(w) is the attention vector for the mention n_w. Formal: -v_o(w) is the attention vector for the entity o_w. Formal: -v_p(w) is the attention vector for the plural form p_w. Formal: -v_q(w) is the attention vector for the plural form q_w. Formal: -v_r(w) is the attention vector for the plural form r_w. Formal: -v_s(w) is the attention vector for the plural form s_w. Formal: -v_t(w) is the attention vector for the plural form t_w. Formal: -v_u(w) is the attention vector for the plural form u_w. Formal: -v_v(w) is the attention vector for the plural form v_w. Formal: -v_w",
        "directory": "eacl",
        "filename": "2021.eacl-main.47.json"
    },
    {
        "casual_text": "We looked at three different setups for our system. The first one, called iSRL, uses all semantic roles for each PLTAG lexicon entry, runs the PLTAG parser (IRPA), and uses both classifiers to handle identification and disambiguation, just like we explained in Section 4. \n\nThe second setup, Majority-Baseline, skips the classifiers and deals with argument identification and role disambiguation in a different way. For identification, we used some heuristics based on Lang and Lapata's work (2014), which rely on gold syntactic dependency info from CoNLL input. For disambiguation, we just picked the most common role based on the gold standard dependency relation label for that specific argument. Keep in mind, these dependencies were made looking at the whole sentence, not step by step.\n\nWe used MaltParser, a top-notch shift-reduce dependency parser, to get labeled syntactic dependencies, following Nivre et al. (2007). Based on Beuck et al. (2011), we tweaked the parser to give us intermediate output after each word by showing the current state of the dependency graph before each shift. We trained MaltParser with the arc-eager algorithm (which worked better than the other options available) on the CoNLL dataset, and it got an 89.66% labeled dependency accuracy on section 23.",
        "formal_text": "We evaluated three configurations of our system. The first configuration (iSRL) uses all semantic roles for each PLTAG lexicon entry, applies the PLTAG parser, IRPA, and both classifiers to perform identification and disambiguation, as described in Section 4. The second one (Majority-Baseline), solves the problem of argument identification and role disambiguation without the classifiers. For the former we employ a set of heuristics according to Lang and Lapata (2014), that rely on gold syntactic dependency information, sourced from CoNLL input. For the latter, we choose the most frequent role given the gold standard dependency relation label for the particular argument. Note that dependencies have been produced in view of the whole sentence and not incrementally. Nivre et al. (2007) to provide labeled syntactic dependencies MaltParser is a state-of-the-art shift-reduce dependency parser which uses an incremental algorithm. Following Beuck et al. (2011), we modified the parser to provide intermediate output at each word by emitting the current state of the dependency graph before each shift step. We trained Malt-Parser using the arc-eager algorithm (which outperformed the other parsing algorithms available with MaltParser) on the CoNLL dataset, achieving a labeled dependency accuracy of 89. 66% on section 23.",
        "GPT2_formal_text": "= set(principals, facets, edge_ids, in_set)          Casual: As mentioned earlier, BERT's model is built on a transformer setup, which means it has a bunch of encoder layers and a decoder layer. The encoder takes the input text and turns it into a sequence of vectors (like a vector x). Then, the decoder uses those vectors to create the output text, which is basically a bunch of points in a space. Formal: We can break down the text into individual words using the words in the vocabulary V. Formal: We figure out the final hidden representation h ∈ R d using the LSTM layer of BERT. Formal: We can also use the attention mechanism to learn the attention weights for each word in the vocabulary, which are h1, ..., hd. Formal: We just use BERT's output (x) to predict the actual tokens (y). Formal: The probability of y given the current word x is calculated based on the probability of x given the current word y. Formal: The score θ_t(y|x) for y given the current word x is calculated based on the score of x given the current word y. Formal: The value of the word embedding h, which is h, is calculated as the product of h1, ..., hd, where h1, ..., hd are the hidden representations of the first and last words in the vocabulary. Formal: In each layer of BERT, there's an attention vector called a_t, which is a matrix of size T ×d. Formal: We also define the softmax function on h_t to get the probability of the token y given the previous word x. Formal: Formal: Finally, the final output representation h for the final token y is the probability of the token y given the previous token x. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D14",
        "filename": "D14-1036.json"
    },
    {
        "casual_text": "Okay, so here's a real-life example taken from a study by Mgrquez and Padr6 back in 1997. There are two taggers being compared: T1 and T2. \n\nT1 only uses bigram information, and it gets a performance score of 0.9135, which is like 96.86% accuracy overall when dealing with ambiguous words. \n\nT2, on the other hand, uses trigrams plus some automatically learned context rules, and it scores a bit better with an accuracy of 0.9282, or 97.39% overall.\n\nBoth of these taggers were tested on a specific corpus (let's call it wsa) where the estimated error rate is about 0.03 for both T1 and T2. The average number of possible tags for each ambiguous word in this corpus is 2.5 tags per word. \n\nThe error rate for the wsa corpus is calculated across all the words in it.",
        "formal_text": "The following real example h~ been extracted from (Mgrquez and Padr6, 1997): The tagger T1 uses only bigranl information and has an observed performance on ambiguous words K1 = 0. 9135 (96. 86% overall). The tagger T2 uses trigrams and automatically acquired context constraints and has an accuracy of K2 = 0. 9282 (97. 39% overall). Both taggers have been evaluated on a corl)us (wsa) with an estilnated error rate 5 C1 =C2 =0. 03. The average ambiguity ratio of the ambiguous words in tile corpus is a = 2. 5 tags/word. SThe (wsa) corpus error rate is estimated over all words.",
        "GPT2_formal_text": "(x) = tanh(x) + tanh(d) [/left] Formal: Basically, the active layer's state can be written like this: Formal: The first part of the active layer's state can be written like this: Formal: The second part is basically the average of the first two parts, and it's calculated as a minus (N - G). Formal: In Figure 2, we look at how the attention weights for the current N-gram, y_i, are distributed. Formal: The attention weights for the current N-gram, y_i, are calculated by combining the total weight of the whole sequence, T_N, and the attention weights from the last N-gram, h_i. Formal: The attention weights for the last N-gram, h_i, are calculated using the sum of the attention weights for the first N-gram, y_1, and the second N-gram, y_2. Formal: The attention weights for the first N-gram, y_1, and the second N-gram, y_2, are calculated based on the average attention weights for the first N-gram, y_1 and the second N-gram, y_2, as mentioned in Equation 2. Formal: The final attention weights, t_r, are calculated as the average attention weights for the entire sequence, including the first N-gram, y_1, and the second N-gram, y_2. Formal: We also have a third part of the input state, r_t, which is a mix of the first N-gram, y_1, and the second N-gram, y_2. Formal: This third part is calculated by summing up the attention weights for the first N-gram, y_1, and the second N-gram, y_2, as mentioned in Equations 2 and 3. Formal: Formal: Here, |r_t| is the total number of words, |y_1| is the sum of the words in the sentence, and |y_2| is the sum of the words in the paragraph. Formal: The attention weights for the third part are calculated based on the average attention weights for the first N-gram, y_1, and the second N-gram, y_2, as mentioned in Equations 2 and 3. Formal: Formal: Formal",
        "directory": "C98",
        "filename": "C98-2159.json"
    },
    {
        "casual_text": "To figure out if PLMs actually have generalizable metaphorical knowledge, we test them in situations where the test data and training data come from different sources. We look at how well they can transfer knowledge between languages and across different datasets as two main ways to check this. We'll explain each of these in the next sections.",
        "formal_text": "To see if PLMs encode generalizable metaphorical knowledge, we evaluate them in settings where testing and training data are in different distributions. We explore transferability analysis across languages and datasets as two sources of distribution. We explain each in the following sections.",
        "GPT2_formal_text": "(pos_t) = pos_t + 1; for i = 1, ..., N: pos_t = c(i) + 1; pos_t = sum from j=1 to N(pos_t) of neg_t, where neg_t is the difference between pos_t and the predicted label. Formal: You can figure out the relation between two entities, r_i and r_j, by running some math or matching up the predicted labels. Formal: To figure out the relations between vectors, you'll do the following: Formal: 1. Compute the matrix S. Formal: 2. Compute the sigmoid function to get the distribution of the vectors. Formal: 3. Use the matrix X = (Vx, Sx, ..., Sx, to get the output. Formal: 4. Calculate the expected counts for all possible relations. Formal: 5. Use the expected counts to update the transformation matrix and the matrix element representations. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.144.json"
    },
    {
        "casual_text": "There's been a lot of buzz around crosslingual transfer learning for SRL lately. A bunch of researchers have been diving into this topic, like Padó and Lapata (2009), van der Plas et al. (2011), Kozhevnikov and Titov (2013), Tiedemann (2015), Zhao et al. (2018), Chen et al. (2019), Aminian et al. (2019), and Fei et al. (2020). Most of the work so far has been split between two main approaches: annotation projection and model transfer.",
        "formal_text": "There has been a great deal of interest in crosslingual transfer learning for SRL (Padó and Lapata, 2009; van der Plas et al. , 2011; Kozhevnikov and Titov, 2013; Tiedemann, 2015; Zhao et al. , 2018; Chen et al. , 2019; Aminian et al. , 2019; Fei et al. , 2020). The majority of previous work has focused on two types of approaches, namely annotation projection and model transfer.",
        "GPT2_formal_text": "_t and attention_mask_t_1 are the input and output representations for the attention parts of the model. Since both attention and the attention layer can easily be added to models without needing any extra parameters, we decided to use those as input vectors for the model we’re proposing. Formal: The results we got from testing different attention methods for sentence classification are in Table 3. The table shows the average F1-score and standard deviation for different attention weights and attention heads in the model. The F1-score tells us how well the model can predict the label based on the input sentence. The standard deviation tells us how much the model changes or \"levels off\" based on the input sequence. Formal: It’s clear that the average F1-score is lower for smaller attention weights than for larger ones. This means that the attention weights can help the model focus on important parts of the input. The F1-score drops off quickly when the attention weights are less than 0.1. Formal: For the final metric, the F1-score for the combined model with the attention heads is the same as the average F1-score for the attention and attention-mask parts separately. Formal: In the final layer of the model, the attention weights can help the model pick up on useful information and can also act as a kind of \"trick\" for the model’s predictions. Formal: Here’s how we calculated the attention weights and attention-mask weights: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.319.json"
    },
    {
        "casual_text": "The rank of (A + Ã) won't be more than d_k. If you've got a bunch of vectors in a linear space, any combination of those vectors should still be in the same space. So, the rows we made up for A are part of LN(T). That means there's an Ã that proves the whole thing about A being unidentifiable.",
        "formal_text": ", the rank of (A +Ã) is not more than d k. For a set of vectors lying in a linear space, a vector formed by their linear combination should also lie in the same space. Thus, the artificially constructed rows of A belongs to LN(T). Therefore, there exist anÃ that establishes the proposition which claims the unidentifiability of A.",
        "GPT2_formal_text": "(x, y) = c(x, y) s = 2 * (x + 1) + (x + 2) * (y + 1) return z, w, s Formal: - If the dictionary has a word that's not in the list of languages, just say it's not there. Formal: - If the dictionary has a word that's in the list of languages, but it's not in the list of valid words, just say it's not there. Formal: - If the dictionary has a word that's in the list of languages, but it's in the list of valid words, or if the dictionary doesn't have a word that's in the list of languages, just say it's not there. Formal: - If the dictionary has a word that's in the list of languages, but it's not in the list of valid words, or if the dictionary doesn't have a word that's in the list of languages, just say it's not there. Formal: - If the dictionary has a word that's in the list of languages, but it's not in the list of valid words, or if the dictionary doesn't have a word that's in the list of languages, just say it's not there. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.94.json"
    },
    {
        "casual_text": "Alright, so here's the deal with error analysis. The main issues come from not properly adding the right information to the system. Check out Figure 6, which shows two examples of how our system handles things. In Figure 6a, the system managed to make a connection because the link between \"spouse\" and \"husband\" was pretty strong (score of 0.747). That's why it added the \"HUSBAND\" function during the knowledge injection. But in Figure 6b, it didn't add the \"WIN\" function because the connection between \"award\" and \"win\" wasn't strong enough (score of 0.336). \n\nAlso, even though we made the original model checking program faster, some tests still took way too long to finish. Table 6 shows the average and longest times (in seconds) for checking the model with and without the optimization.",
        "formal_text": "Error Analysis Error analysis shows that main errors are caused by the failure of knowledge injection. Figure 6 shows two premise-hypothesis pairs, one for which our system was able to perform inference and one for which it was not. In Figure 6a, the function HUSBAND was added to the model in the knowledge injection process because the relatedness score between spouse and husband was high (0. 747). On the other hand, in Figure 6b, the function WIN was not added to the model because the relatedness score between award and win was low (0. 336). In addition, even though we improved the speed of the original model checking program, several test cases still ran out of time. Table 6: Average and maximum run time (seconds) for model checking with and without optimization.",
        "GPT2_formal_text": "ing_v2 = softmax(attn_masked) # Then, we check out the word embedding vectors of the target sentence. Formal: You can find all the details about the hyperparameters in Appendix C. Formal: The masked attention matrix α can be written like this: Formal: For instance, if we have a mask matrix that's the size of the input sentence x = (x1, ..., xn) using the method we talked about in Section 3.1, we can write it like this: Formal: Similarly, the vector for the target word embedding is just the inverse of the input embedding vector. Formal: So, Formal: But, Formal: To make it easier to compare with the original model, let's say we have a vector h x = (h1, ..., hn) that represents the hidden states of the context. Formal: Also, the embedding for the target word x can be written as h x = (hw, ..., hb). Formal: The masked attention matrix α can be written like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-srw.8.json"
    },
    {
        "casual_text": "Here, θ represents a fixed version of θ. This training method doesn't add any extra parameters to the model. The only extra cost for supervised learning is running a training pass and a generation pass for each unlabeled sentence. As the main goal, we train using a combination of the supervised loss from Equation 1 and the unsupervised loss from Equation 3, with some weights applied to balance them.",
        "formal_text": "whereθ denotes a fixed copy of θ. This training process does not introduce additional model parameters. The entire additional training cost to supervised learning is a training pass and a generation pass for each unlabeled sentence. As the overall objective, we train a weighted sum of the supervised loss in Equation 1and the unsupervised loss in Equation 3:",
        "GPT2_formal_text": "\" is defined. The similarity matrix S(•) is shown in equation (1). The row-normalized matrix, S(•), is also given. Formal: This paper introduces a new alignment algorithm for lexicalized reordering models. It makes the most of the lexical info and a lexicalized reordering model by using the structure of the source and target languages. This approach also makes it possible to reorder source words in a way that keeps the model grounded in the language. Formal: The algorithm works by aligning the source and target words using a lexicalized reordering model. Then, it aligns the target words using the model. This setup keeps the source and target languages aligned together, while keeping the target words in place. Formal: This algorithm is based on a lexicalized reordering model and uses the structure of the source and target languages to handle reordering. Formal: We use the lexicalized reordering model in the IBM model 3 translation model for the English-French MT system. Formal: We ran a bunch of experiments using different alignment methods to show how the lexicalized reordering model affects translation quality. Formal: We also set up a straightforward decoding method to handle cases where the model can't find an aligned phrase in the target language. Formal: The experiment results show that our approach is the most efficient way to reorder words. Formal: We tested how the model handles reordering in both the source and target languages and compared it to other methods. Formal: We also did some experiments on Chinese-English MT and compared it to the model. Formal: We also looked at how reordering affects the search space. Formal: We built a Chinese-English MT system using the IBM model 3 translation model. Formal: We tested how the reordering affects the translation quality. Formal: We also worked on Chinese-English MT and did some experiments to see how the reordering affects the translation quality. Formal: We built a Chinese-English MT system using the IBM model 3 translation model. Formal: We also experimented with Chinese-English MT and did some experiments to see how the reordering affects the translation quality. Formal: We also worked on Chinese-English MT and did some experiments to see how the reordering affects the translation quality. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.321.json"
    },
    {
        "casual_text": "NMT is kind of the big thing right now, but not much has been done specifically for translating between Bengali and English. Dandapat and Lewis (2018) worked on creating a general-purpose NMT model for Bengali-English using sentences from comparable corpora. They dealt with the lack of training examples by using data augmentation and back-translation (Sennrich et al., 2016). Hasan et al. (2019) and Mumin et al. (2019a) also showed that even with limited parallel data available online, NMT improved translations for the Bengali-English pair.",
        "formal_text": "Although NMT is currently being hailed as the state-of-the-art, very few works have been done on NMT for the Bengali-English pair. Dandapat and Lewis (2018) trained a deployable general domain NMT model for Bengali-English using sentences aligned from comparable corpora. They combated the inadequacy of training examples by data augmentation using back-translation (Sennrich et al. , 2016). Hasan et al. (2019); Mumin et al. (2019a) also showed with limited parallel data available on the web that NMT provided improved translation for Bengali-English pair.",
        "GPT2_formal_text": "feature. It starts with a character sequence and uses a BiLSTM to figure out the hidden state for each character. This hidden state is then passed into a fully connected layer to predict the next character, called the \"opinion_mask.\" Formal: (1) So, to make the most of these masked characters, we tweak the predicted sentiment by adding a sentence representation. This representation is made by combining the predicted hidden state and the opinion mask, which is written as (2). Formal: (2) We also use a softmax function to figure out the probability for the next word in the sequence, which we call (3). Formal: (3) After that, we adjust the sentiment mask by multiplying it with the probability of the next word. Formal: (4) Finally, we feed this updated hidden state into a fully connected layer to predict the next word, which we write as (5). Formal: (5) This process is repeated until the masked characters stop showing up. Formal: (6) Once the masked characters are gone, we pick the sentence to represent the sentiment. Formal: (7) Here, we pick the sentiment mask, the opinion mask, and the word sequence representation to represent the sentiment. Formal: (8) After that, we combine the sentiment mask with the probability of the next word. Formal: (9) Finally, we calculate the sentiment mask by multiplying the sentiment mask with the probability of the next word. Formal: (10) Formal: (11) Finally, we combine the sentiment mask, the opinion mask, and the word sequence representation to represent the sentiment. Formal: (12) And that's it! Formal: (13) The final output is a sentiment expression that can be used to figure out the sentiment for other sentences too. Formal: (14) Formal: (15) Formal: (16) Formal: (17) Formal: (18) Formal: (19) Formal: (20) Formal: (21) Formal: (22) Formal: (23) Formal: (24) Formal: (25) Formal: (26) Formal: (27) Formal: (28) Formal: (29) Formal: (30) Formal: (31) Formal: (32) Formal: (33) Formal: (34) Formal: (35) Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.207.json"
    },
    {
        "casual_text": "For phrase-level attacks, we wanted to see if tweaking a part of a sentence (the source subtree) could mess with the prediction for another part (the target subtree). Check out Figure 2 for an example. We tested two scenarios: one where the source and target subtrees are at least one word apart (k ≥ 1), and another where they just can't overlap (k ≥ 0). \n\nIn the k ≥ 0 case, we found 1420 sentences in the test set that fit the bill. For k ≥ 1, there were 1340 valid examples we could use for these attacks. The test set has a total of 2416 sentences, by the way. \n\nOh, and all the subtrees we looked at had between 4 and 12 words. For each source-target pair, we could change up to 3 words in the source subtree. Sometimes, just swapping one or two words was enough to create an adversarial example for some sentences.",
        "formal_text": "For the phrase-level attacks, we aim to study whether changes in a source subtree can alter the prediction on another target subtree (see an illustration in Figure 2 ). We tried two different settings: one asks for the source and target subtrees to be separated by at least one word (k ≥ 1), and another only requires those two subtrees do not overlap with each other (k ≥ 0). In the case of k ≥ 0, we can find 1420 sentence examples from the test set, while for k ≥ 1, there are 1340 valid examples that can be used to deliver phrase-level attacks (there are 2416 sentences in total in the PTB test set). Note that all the subtrees should contain 4 to 12 words. For each source-target pair, we allow to modify the source subtree up to 3 words. For some sentences, their adversarial examples can be generated by replacing just one or two words.",
        "GPT2_formal_text": "(V, T) = {v_i, ..., v_n}, and there are n_i=1 to n_L. Formal: In this part, we're introducing a new type of graph called the query-focused query graph. We've got two types of nodes: (v_i, ..., v_n) and (v_i, ..., v_n). Formal: We can use different type info to help the graph model understand the relationship between query and key. In our example, the main key is the word \"helots,\" and the query is \"best athletes,\" which we connect with the key \"athlete.\" Formal: We want the query-focused graph to help the model figure out which athlete is the best. So, we start by putting the word \"athlete\" into the query, which tells the model that the key is about athletes. Formal: After that, the model can figure out which athlete is the best based on the query. Formal: For more info, check out Section 4.2 in the supplementary material. Formal: Let’s say the key \"best athletes\" is in the query, and the key \"athlete\" is in the key. Formal: We’ll use the model to figure out which athlete is better. Formal: Basically, the key, the query, and the node we put the word \"athlete\" into should all be related to the athlete. Formal: Formal: The key, the query, and the node we put the word \"athlete\" into should all be related to the athlete. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.590.json"
    },
    {
        "casual_text": "The vector s, which is part of the set R S, holds overall, population-wide estimates for the parameters that describe how we predict things. In this study, we assume that these predictions follow a normal (bell-shaped) distribution. So, s includes the average prediction (µ, also known as the intercept) and the spread or variability of those predictions (σ², which is the variance).",
        "formal_text": "Vector s ∈ R S contains global, population-level estimates of the parameters of the predictive distribution. Under the univariate normal predictive distribution assumed in this study, s contains the predictive mean (µ, i. e. the intercept) and variance (σ 2 ):",
        "GPT2_formal_text": "_type = tensor tensor_shape = (d_s, d_t, 1, d_s, d_t, 2, d_s, d_t). In this setup, the different layers of the mask are filled with different dot-product matrices. For the attention weight vector, we use a bi-linear layer with three nonlinear parts. Formal: We tested our method on the PAWS-X and CNN/Daily Mail datasets. The results are in Table 3. Turns out, our method works really well, beating all the other baselines across all metrics. Formal: The PARAGRAM model we trained on CNN/Daily Mail uses a two-layer CNN with ReLU as the attention mechanism. Formal: The PARAGRAM model trained on CNN/Daily Mail is basically the same as the one in Figure 1, but with two layers of CNNs. Formal: The MODE-LSTM model trained on CNN/Daily Mail is a two-layer neural network with a linear layer, along with two nonlinearity layers. Formal: Our method is the top performer in all three evaluation metrics. Formal: Our approach outperforms the other baselines across all metrics. Formal: Our method also beats the other baselines across all metrics, except for two-head attention on CNN/Daily Mail. Formal: For the second and third-to-last metric, our method does better than both the CNN/Daily Mail and MultiNLI models. Formal: Our method consistently outperforms the other baselines across all metrics, except for two-head attention on CNN/Daily Mail. Formal: For the last metric, our method consistently outperforms the other baselines. Formal: When we compare our method to the baseline on the ParIce dataset, the PARAGRAM model with both two-layer CNNs and ReLU performs almost the same on both datasets. Formal: For the last metric, our method performs about the same as the other baselines. Formal: Our method consistently outperforms the other baselines across all metrics, except for two-head attention on CNN/Daily Mail. Formal: This is just one example. Formal: Formal: We ran our experiments on the CNN/Daily Mail dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.288.json"
    },
    {
        "casual_text": "In the second experiment, we looked at multilingual word translation across six European languages: English, German, French, Spanish, Italian, and Portuguese (Lample et al., 2018). We compared our MPPA method to MAT+MPSR (Chen and Cardie, 2018). Since MAT+MPSR is an unsupervised approach, we swapped out the MPSR part with our MPPA algorithm to make it MAT+MPPA for a fair comparison. We ran 5 refinement epochs after the MAT step, which is the default setting in the MAT+MPSR source code. The MPPA training phase is about 10 times faster than the MPSR equivalent, though both methods have hyperparameters that need tuning. We also tested UMH (Alaux et al., 2019) on this benchmark. The precision@1 results are in Table 2. MPPA performed similarly to UMH, and MPSR did slightly better. One thing to note is that the MPSR mapping matrices weren’t exactly orthogonal. They had a smaller mean-square error (MSE) on the training data compared to our method, which was restricted to be orthogonal. This suggests that the orthogonality constraint, especially when combined with transitivity constraints in multilingual settings, might be too strict.",
        "formal_text": "The second experiment involved multilingual word translation in six European languages: English, German, French, Spanish, Italian and Por-3564 tuguese (Lample et al. , 2018). We compared MPPA to MAT+MPSR (Chen and Cardie, 2018). MAT+MPSR is an unsupervised method, so for a fair comparison we replaced the MPSR algorithm with our MPPA algorithm, thus obtaining MAT+MPPA. We ran 5 refinement epochs, after the MAT step, as the default option in MAT+MPSR source code 3. MPPA training phase is 10 times faster than MPSR equivalent phase, which also have hyper-parameters that needed to be tuned. UMH (Alaux et al. , 2019), was also evaluated on this benchmark. Table 2 shows precision@1 results. MPPA was comparable to UMH and MPSR performed slightly better. Note that the MPSR mapping matrices were not exactly orthogonal. They indeed achieved smaller mean-square error (2) on the training data than our solution, which was restricted to be orthogonal. This suggests that the orthogonality constraint, especially in the multilingual case where it is combined with transitivity constraints, can be too restrictive.",
        "GPT2_formal_text": "= { u1, u2, ..., um } m = { { e1, e2, ..., en } } n = { { w1, w2, ..., wn } } n = { { w1, w2, ..., wn } } n = { { w1, w2, ..., wn } } λ = 1 γ = 0 λ = 0 n = { { w1, w2, ..., wn } } n = { { w1, w2, ..., wn } } m = { { e1, e2, ..., en } } n = { { w1, w2, ..., wn } } λ = 1 γ = 0 λ = 0 n = { { w1, w2, ..., wn } } m = { { e1, e2, ..., en } } n = { { w1, w2, ..., wn } } λ = 1 γ = 0 λ = 0 n = { { w1, w2, ..., wn } } m = { { e1, e2, ..., en } } n = { { w1, w2, ..., wn } } λ = 1 γ = 0 λ = 0 n = { { w1, w2, ..., wn } } m = { { e1, e2, ..., en } } n = { { w1, w2, ..., wn } } λ = 1 γ = 0 λ = 0 n = { { w1, w2, ..., wn } } m = { { e1, e2, ..., en } } n = { { w1, w2, ..., wn } } ω = 1 ω = 1 ω = 1 ω = 1 ω = 1 ω = 1 ω = 1 ω = 1 ω = 1 ω = 1 ω = 1 ω = 1 ω = 1 Formal: The main goal of the task is to figure out the path of a (x, y) pair, given a prompt x, a list of possible answers y, and an ontology Θ. Formal: We start with an initial input, x_i. We use the Path2Embedding method (from Bahdanau et al.,",
        "directory": "D19",
        "filename": "D19-1363.json"
    },
    {
        "casual_text": "Since tables made from plain text are pretty basic, they don’t have a lot of fancy features or options.",
        "formal_text": "Because of the simplicity of table construction methods in free text, the expressive capability is limited.",
        "GPT2_formal_text": "_layers. Instead of using the regular convolutional layer to represent the word embeddings, we switch to a pooling layer to create the representation for the whole sequence of words.           Casual: We did an ablation study to check how each of these attention-based layers works. We looked at different combinations of attention, pooling, and masking, and then compared them in terms of their performance, especially for the target domains. We also figured out the best settings for the parameters in these layers, based on what we saw in our experiments.\n\nSince we only had a small amount of parallel data, we ran a few tests on the RTE-3 test set using standard transformer-based models (like the one by Vaswani et al., 2017) with varying numbers of layers. To simulate the longer target-side texts, we trained the model using the RTE-3 test set for 5 epochs on the development set, with a batch size of 64. This setup gave us a prediction accuracy of 85.71% on the test set and 87.90% on the development set. Formal: To see how these different attention layers work together, we tested each one separately on the RTE-3 test set. Formal: This shows the average performance for the combination of the pooling layer, which is used for target-side representations, and the attention layer, which is used for target-side representations and also for the representation for the entire sequence of words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C00",
        "filename": "C00-1025.json"
    },
    {
        "casual_text": "So, basically, if we say: F is the forest made up of all the syntactic trees (T i ) from the considered lexical entries, plus the output (the type we're trying to get), then the generation problem (check out figure 4) is about finding a matching M for the atomic formulas in F.",
        "formal_text": "Thus, if we define: F the forest made of the syntactic trees (T i ) of all the considered lexical entries plus the output (the type we want to derive), the generation problem (see figure 4) is to find a matching M of atomic formulas of F such that:",
        "GPT2_formal_text": "= (k_i, i_m, ..., k_L) where k_i is the length of the i-th token. For each token, the embedding is created using a uniform embedding function. Formal: For the mapping M_w from the input to the output, the embedding is just the word embedding. Formal: Lastly, the embedding for the whole output can be written like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C00",
        "filename": "C00-2091.json"
    },
    {
        "casual_text": "(2) Cool thing: we noticed that some of the tricky annotation issues mentioned in the PDTB (by Webber et al., 2019) are popping up in our own data too. One of those tricky bits is figuring out the difference between Comparison. Contrast and Comparison. Concession, specifically when dealing with \"Despite what\" and \"What is contrasted with.\" In our manually checked data sample, we found 3 cases where QADiscourse and PDTB annotations got mixed up.",
        "formal_text": "(2) Interestingly, we observe that some annotation decision difficulties described in the PDTB (Webber et al. , 2019) are also mirrored in our collected data. One of those arising ambiguities is the difference between Comparison. Contrast and Comparison. Concession, in our case Despite what and What is contrasted with. In the manually analyzed data sample, 3 such confusions were found between the QADiscourse and the PDTB annotations.",
        "GPT2_formal_text": "ing_2: {A = [A 1, A 2 ], and B = [B 1, B 2 ]}. Formal: Lately, people have been looking into how attention maps work in neural networks, especially for object recognition (Yang et al., 2015). In this project, we're looking into how attention in a Bi-LSTM works for text. Formal: We're introducing a new attention mechanism called \"textual\" and a new dataset for textual attention, which is based on the ACL Anthology dataset (Moosavi et al., 2019). The aim is to see how different attention maps work for text. Formal: We're introducing two attention maps, one for text and another for visual stuff, each using the same attention mechanism. Formal: We're using a few popular machine learning algorithms and two popular datasets (check out Table 2 for details). Formal: We're comparing these two attention maps on text, and the results are in Table 1. Formal: We're also sharing some training and development data for our annotation tasks. Formal: The dataset we're using is the ACL Anthology dataset (Moosavi et al., 2019). Formal: We're focusing on text attention here, but the same approach can be applied to visual attention. Formal: We used the corpora from the ACL Anthology dataset for our experiments. Formal: The dataset has around 5.7 million named entities (NEs) and 5.2 million nouns (NPs), covering four classes—Person, Organization, Location, and Other. Formal: Our annotation dataset has around 70,000 sentences (inlc) with around 1.4 million NEs and 13.8 million NPs. Formal: The code and data for our annotation tasks are available online at https://github.com/yimengw/textual_attention. Formal: The datasets we're looking at include the first one from (Moosavi et al., 2019), which has around 4.1 million NEs and 13.8 million NPs. Formal: We're also sharing some training and development data for our annotation tasks. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "In subfigure (c), M-CAN+2R o gives us the best attention weights. The attention for the aspect \"food\" is pretty much at right angles to the attention for \"service.\" The \"food\" aspect focuses on the first half of the sentence, while \"service\" zeroes in on the second half. Also, the words \"outstanding\" and \"tops\" get the most attention in their respective aspects.",
        "formal_text": "As shown in subfigure (c), M-CAN+2R o generates the best attention weights. The attention weights of the aspect food are almost orthogonal to the weights of service. The aspect food concentrates on the first part of the sentence while service on the second part. Meanwhile, the key opinion words \"outstanding\" and \"tops\" get highest attention weights in the corresponding aspects.",
        "GPT2_formal_text": "= { u idx, e idx },          Casual: Sure! Here's a more casual version:\n\nThe score for finding the first correct match is calculated using this formula: Formal: First match score = ([Q, W]1 - [Q, [W]2 - [Q, [W]3 - [W]4 - [W]5) / 2) + [V, W]1 - [V, [W]2 - [V, [W]3 - [V, [W]4 - [V, [W]5]). Formal: First match score = ([Q, W]1 - [Q, [W]2] - [Q, [W]3] - [Q, [W]4] - [Q, [W]5]). Formal: First match score = ([Q, W]1 - [Q, [W]2] - [Q, [W]3] - [Q, [W]4] - [Q, [W]5]). Formal:\n\nThe top-ranked matches are always highlighted in bold, and the rest are hidden in the text. Formal: First match score for finding the first correct match is calculated using this formula: Formal: First match score = ([Q, W]1 - [Q, [W]2] - [Q, [W]3] - [Q, [W]4] - [W, [W]5]). Formal: First match score for finding the first correct match is calculated using this formula: Formal: Formal: Formal:\n\nFinally, the scores for the rest of the matches are based on how well they fit the context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-1467.json"
    },
    {
        "casual_text": "The only reason mistakes happen is because of issues with the annotations or problems in how we handle pronouns. In the next section, we'll compare the results we get from the extraction process using semantic annotations to the ones we get when we only use syntactic annotations.",
        "formal_text": "Mistakes can arise only because of the annotation errors and errors in the anaphora resolution procedure. 3 The comparison of the results obtained by the extraction procedure based on the semantic annotation with the results of the procedure based on the syntactic annotation only is provided in the next section.",
        "GPT2_formal_text": "ed + attention_masked + skip_last = attention_masked + (attention_masked * skip_last) + attention_masked * skip_last + skip_last * skip_last + skip_last * skip_last * skip_last + skip_last * skip_last + skip_last * skip_last + attention_masked + skip_last + attention_masked + skip_last * skip_last + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_masked + attention_",
        "directory": "D09",
        "filename": "D09-1144.json"
    },
    {
        "casual_text": "Looking at the context sentences manually, we noticed that a big reason for missing full evidence is the way literary language can be vague. Authors often avoid repeating stuff or directly mentioning character names, so it’s super important to catch accurate paraphrases and figure out coreference. We think commonsense knowledge is really key to making BookQA better. While testing our system, we kept finding situations where the model messed up because it missed important details that weren’t spelled out. Some common examples we saw were: \n\ni) Character relationships that were obvious to readers but never actually explained (like, “Who did Mark’s best friend marry?”); \nii) A character’s feelings about something (like, “Who was mad about the school’s policy?”); \niii) The order of events (like, “Who did Marriat talk to after the big fight?”). \n\nAdding commonsense knowledge to a QA system is still a tricky problem in general, and that includes BookQA.",
        "formal_text": "Manual inspection of context sentences indicated that a common reason for the absence of full evidence is the inherent vagueness of literary language. Repeated expressions or direct references to character names are often avoided by authors, thus requiring very accurate paraphrase detection and coreference resolution. We believe that commonsense knowledge is particularly crucial for improving BookQA. When exploring the output of our system, we repeatedly found cases where the model failed to arrive at the correct answer due to key information being left implicit. Common examples we identified were: i) character relationships which were clear to the reader, but never explicitly described (e. g. , \"Who did Mark's best friend marry? \" ); ii) the attitude of a character towards an event or situation (e. g. , \"Who was angry at the school's policy? \" ); iii) the relative succession of events (e. g. , \"Who did Marriat talk to after the big fight? \" ). The injection of commonsense knowledge into a QA system is an open problem for general and, consequently, BookQA.",
        "GPT2_formal_text": "(s, f) = arg max (s, f) if arg is not None: pass return (r, f, arg max (s, f)) else: pass return (r, s, f)\n\nFinally, there's a function called pass that takes two inputs, s and f. It uses two special parameters, arg max and arg max, to calculate the scores. The function returns a triple, which is a list of scores: r, s, f. This list represents the accuracy of the generated text. The function starts with a probability distribution for the next word in the sequence, which it grabs from the distribution p_y. Formal: For a given input x_i, the likelihood of the next word y_i is calculated by multiplying the input vector x_i by its probability distribution p_y. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-5811.json"
    },
    {
        "casual_text": "Alright, let's dive deeper into this. Instead of looking at languages as a whole, we're zooming in on individual nouns. The big question here is whether there's any rhyme or reason to the mistakes we see. Are they just random, or do certain types of nouns give us more trouble than others?",
        "formal_text": "In this section, we perform a finer-grained analysis: focus not on languages, but on individual nouns. Our main question is if there are any patterns in the distribution of errors. Is it random or are certain classes of nouns systematically more difficult to predict than others?",
        "GPT2_formal_text": "ing, which is a function that generates word embeddings based on the hidden state h_t. Formal: So, we're talking about a model that figures out the attention weights for the word x_i in a sequence z_i. The model is trained using the attention weights from all the tokens in the sequence. Formal: To keep things simple and clear, we'll call this model \"attention-based latent representation.\" Formal: We also have this model called \"Attention-based latent representation.\" Formal: When we combine this with a global attention layer, we get something called \"Attention-based global latent representation.\" Formal: The final attention vector of the output x_i is calculated like this: Formal: The output of the attention layer, w_a_i, can be calculated using a feed-forward network, as shown in equation (2). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "conll",
        "filename": "2020.conll-1.20.json"
    },
    {
        "casual_text": "Basically, we use a special thing called \"[AM]\" to show the AM query, which is like a way to find all the arguments, like A a = {arg a k }.",
        "formal_text": "Formally, we use a special token \"[AM]\" to represent the AM query q am, which aims to identify all the arguments A a = {arg a k }",
        "GPT2_formal_text": "= (q, j). Similarly, the attention weight vector is just a tensor with dimensions d_c by d_l, where d_c is the size of the pre-trained graph embedding, and d_l is the size of the downstream graph embedding. Formal: To get the weight vector q, we use mean pooling with a linear kernel, which is a common trick in computer science. The input for this kernel is a list of attention weight vectors q. Formal: We take the weighted sum of the attention vector for each token j. Formal: To get the attention weight vector q_t, we use mean pooling with a non-linear kernel, which is a newer trick in neural network analysis. Formal: Formal: Here, w_i, j represents the attention weight vector for the token i in the input sentence. Formal: Each attention vector q_t_i gets added to the weights of all the token vectors q_t. Formal: Formal: Finally, the attention weight vector q_t_i is the combination of all these attention vectors. Formal: Formal: For a specific token i, the attention weight vector q_t_i is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-short.4.json"
    },
    {
        "casual_text": "A typical way to add semantic meaning to language models is to check how similar a word is to its context and then tweak the probabilities from an n-gram model based on that. This helps the n-gram model, which usually focuses on short-term connections, also consider longer-term, more meaningful relationships. A lot of earlier research followed this idea (like Bellegarda in 2000, Coccaro and Jurafsky in 1998, and Wandmacher and Antoine in 2007), often using LSA to handle the semantic parts for individual words. Some researchers (Coccaro and Jurafsky, Wandmacher and Antoine) used the idea of a \"vector centroid\" to represent the context, while others (Bellegarda, Deng and Khundanpur) went with a \"pseudodocument\" approach, which comes from the relationship between documents and words in LSA. All of them calculate the probability of a word based on its context using cosine similarity, but they need to adjust it to get proper probability values. Gildea and Hofmann (1999) took a different route, using pLSA to create representations that already have a clear probabilistic meaning. This makes it easier to directly calculate the probability of a word based on its context without needing extra adjustments.",
        "formal_text": "A common approach to embedding semantic representations within language modeling is to measure the semantic similarity between an upcoming word and its history and use it to modify the probabilities from an n-gram model. In this way, the n-gram's sensitivity to short-range dependencies is enriched with information about longer-range semantic coherence. Much of previous work has taken this approach (Bellegarda, 2000; Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007), whilst relying on LSA to provide semantic representations for individual words. Some authors (Coccaro and Jurafsky, 1998; Wandmacher and Antoine, 2007) use the geometric notion of a vector centroid to construct representations of history, whereas others (Bellegarda, 2000; Deng and Khundanpur, 2003) use the idea of a \"pseudodocument\", which is derived from the algebraic relation between documents and words assumed within LSA. They all derive P(w i |h i ), the probability of an upcoming word given its history, from the cosine similarity measure which must be somehow normalized in order to yield well-formed probability estimates. The approach of Gildea and Hofmann (1999) overcomes this difficulty by using representations constructed with pLSA, which have a direct probabilistic interpretation. As a result, the probability of an upcoming word given the history can be derived naturally and directly, avoiding the need for ad-hoc transformations.",
        "GPT2_formal_text": "_pos (att, src) :\n          Casual: (4) This is for a single-layer graph G = (V, E). The main goal is to figure out the LSTM hidden state for the token s_i, which we call h_s_i. The HATM has three main parts: a cell state, a sequence state, and a hidden state. The cell state gives us the current state and the sequence state helps us understand the sequence of tokens. Formal: (5) The goal here is to take the hidden state h_s_i from the first part of the HATM and use it to get the hidden state of the token s_i. Formal: (6) After that, we calculate the average attention vector for the token s_i. Formal: (7) Finally, we use the last hidden state to get the final output value for the token s_i. Formal: (8) Lastly, we calculate the logits for the token s_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D09",
        "filename": "D09-1045.json"
    },
    {
        "casual_text": "After that, one of the authors went through the explanations the experts gave and put them together.",
        "formal_text": "Following this step, one of the authors read the explanations provided by the experts and formed",
        "GPT2_formal_text": "(f, d) = (1, ..., d). Formal: When using a PLTAG-based tagger, we first predict the entity spans, then we fill in the gaps by applying normalization. We tested three different PLTAG models, and the best one (LM + PHEME + PLTAG) worked the best. Formal: For the entity span prediction part, we use the BOW model with the entity span prediction module (from Miwa and Bansal, 2016) trained on the entity span data. Formal: After generating the final entity span, we assign labels to the spans, and the labels we get are combined into one set. Formal: We also got rid of the entity span prediction module after training it. Formal: Next, we use the named entity tagger to get the labels for each span, and the labels from the entity span prediction module (LM + PHEME + PLTAG) are combined into one set. Formal: After training these models for k epochs, we picked the best one based on the weighted average of the L2 and R2 scores, which are shown in Table 1. Formal: Check out Table 1: It shows the results for the final NER task and the best-performing PLTAG model, LM + PHEME + PLTAG. The weights for the NER labels are based on the final predictions. Formal: We also calculated the weights for the entity span prediction module, LM + PHEME + PLTAG. Formal: We tested LM + PHEME + PLTAG on the development set and found that their performance improves as the number of training epochs increases. Formal: Formal: Formal: We also tested LM + PHEME + PLTAG on the test set, which has sentences from the test set, and it improved again as the number of training epochs increases. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "So, φ is the Gram-Schmidt process (check out Section 3.3 for more info) applied to the square matrix M r (i). The result, φ(M r (i)), is an orthogonal matrix that comes from M r (i). And ẽ t is basically the combination of all the sub-vectors ẽ t (i) from Equation 1, like, you know, putting them all together.",
        "formal_text": "where φ is the Gram Schmidt process (see details in Section 3. 3) applied to square matrix M r (i). The output transform φ(M r (i)) is an orthogonal matrix derived from M r (i). ẽ t is the concatenation of all sub-vectorẽ t (i) from Eq. 1, e. g. ,",
        "GPT2_formal_text": "(D). The idea is to start with the input x and use the multi-head attention mechanism to create a token representation y. Formal: The input x is what the model is dealing with. Formal: The multi-head attention mechanism, which is a type of neural network, picks out the most important parts of each word. These parts are the attention scores that go with each word, called h_i. For each word in the input x, the attention score for the word w_i is calculated as the sum of all the attention scores for each word in the vocabulary V_W. Formal: To make things easier, we use a linear transformation to combine the attention scores for the words in the input x with the attention scores for the words in the vocabulary V_W. Formal: This method of combining attention scores was suggested by Bahdanau et al. in 2015. Formal: To get the attention scores for different tokens in the input x, we use a bi-directional LSTM to process the inputs. This LSTM then decides how important each token is based on the attention scores it gets from the previous and next tokens in the sequence. Formal: The attention scores for the words in the input x are basically the attention scores for the words in the vocabulary V_W. Formal: If the input x isn't empty (like, if the output token is a noun), the multi-head attention mechanism works by using a max-pooling operation to get the attention scores for each word in the input. Formal: We use a linear transformation to combine the attention scores for the words in the input x with the attention scores for the words in the vocabulary V_W. Formal: We set the hidden size to 300 in the pre-trained ALBERT model (from Devlin et al., 2019) and the learning rate to 0.1. Formal: Since the source and target words are already being used together, we just combine them by multiplying their attention scores. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.241.json"
    },
    {
        "casual_text": "In this paper, we take a closer look at how one of the cool new things in SEAL—using character-level methods to spot potential regular structures, or wrappers, in web pages—affects performance. Back in the day, some early systems for analyzing web pages, like WIEN (Kushmerick et al., 1997) and DIPRE (Brin, 1998), did work at the character level, but more recent methods for set expansion have mostly focused on tokenized or parsed free text (Carlson et al., 2009; Talukdar et al., 2006; Snow et al., 2006; Pantel and Pennacchiotti, 2006) or used tricks to take advantage of HTML structures that often contain lists and tables (Nadeau et al., 2006; Etzioni et al., 2005).",
        "formal_text": "In this paper, we explore the impact on performance of one of the innovations in SEAL, specifically, the use of character-level techniques to detect candidate regular structures, or wrappers, in web pages. Although some early systems for web-page analysis induce rules at character-level (e. g. , such as WIEN (Kushmerick et al. , 1997) and DIPRE (Brin, 1998) ), most recent approaches for set expansion have used either tokenized and/or parsed free-text (Carlson et al. , 2009; Talukdar et al. , 2006; Snow et al. , 2006; Pantel and Pennacchiotti, 2006), or have incorporated heuristics for exploiting HTML structures that are likely to encode lists and tables (Nadeau et al. , 2006; Etzioni et al. , 2005).",
        "GPT2_formal_text": "to create better representations of entities in documents. Specifically, we rely on the annotations of entity mentions (like who, when, and where) to create the context representations. But let's be real, these annotations can be messy and noisy. So, it's not easy to figure out the exact meaning of each mention by just looking at its name. Formal: Let's start by looking at how different systems handle entity mentions. Since our model is based on the MPQA dataset, we focus on the typical answers to this question. Formal: We use a feedforward neural network (FFNN) to encode the word's embedding. Then, we calculate the hidden representation for each entity mention. Formal: We calculate the embeddings for entities and their mentions using a multi-layer perceptron (MLP) with an attention mechanism. The output vector for the mention is the combination of the representation of the entity and its mention. Formal: To calculate the embedding for a mention, we just average the representation of its mention and the entity mention. Formal: We also combine the embeddings for mentions and their mentions to get the combined representation. Formal: Following what Zeng et al. (2019) did, we calculate the embeddings for mentions and their mentions using a multi-layer perceptron (MLP) with an attention mechanism. Formal: The embedding for a mention is the combination of the embedding of its mention and the embedding of its mention. Formal: We use a binary cross-attention mechanism (BERT) (Devlin et al., 2019) to calculate the hidden representation for each mention. Formal: Formal: To get the embedding for a mention, we average the embeddings of its mention and its mention. Formal: We also add a bias term to the embedding, which helps the model focus on the most important bits of the mention's context. Formal: We add a binary cross-attention mechanism (BERT) (Devlin et al., 2019) to calculate the hidden representation for each mention. Formal: Lastly, we use a weight to control how much we weigh the importance of the context information. Formal: The embedding for a mention is the combination of the embedding of its mention and the embedding of its mention. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1156.json"
    },
    {
        "casual_text": "Okay, so here's how we handle the recipe rewrite task in a straightforward way: we just look up the dish online and find a version that matches the dietary restriction. Starting with the original recipe, we figure out what dish it’s for, then search through all the recipes we have (training, dev, and test) to find one for the same dish that fits the dietary requirement.",
        "formal_text": "Retrieval: We imitate a simple approach to the recipe rewrite task: searching the web for a version of the dish that obeys the given dietary constraint. Given a source recipe, we determine the dish to which this recipe belongs and retrieve a recipe for the same dish that fits the dietary constraint from the combined pool of train, dev, and test recipes.",
        "GPT2_formal_text": "ed is basically the same as the input mask, which you can see in Figure 1. We'll use these inputs to build a sentence representation using the attention mechanism. Formal: We use a multilayer perceptron (MLP) to figure out the attention weights, which we call α. Formal: To train the model, we use data augmentation techniques that aim to boost the model's performance. This helps the model pick up on important information, which leads to better results. Formal: In this part, we'll talk about the different methods we tried and what worked best. Formal: For the neural machine translation (NMT) part, we used Transformer (from Vaswani et al., 2017) with the same settings as our example. Formal: The dependency parser and attention mechanism are explained in the next sections. Formal: In this part, we'll compare how our method stacks up against other models. Formal: We'll also share the performance of our model on three common datasets, as well as how well it performs on the Switchboard dataset. Formal: We'll also share the F1 scores for the samples we used to train our model and the datasets we tested it on. Formal: Lastly, we'll share the results for the datasets we tested. Formal: Our model performed better than the dependency parser in terms of F1 scores, but it didn't do as well in terms of accuracy. Formal: The semantic parsing module has the best accuracy, but the semantic parsing module doesn't perform as well as the NMT model. Formal: The dependency parser and attention mechanism have the worst accuracy. Formal: We'll share some extra stats from our experiments on the Switchboard dataset. Formal: We also created a dataset with the same setup as ours, but we only used the raw, unprocessed data. Formal: Formal: We're planning to use our NMT model on the TREC test set in the future. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.526.json"
    },
    {
        "casual_text": "Learning to rank has been a hot topic in the information retrieval field lately. There are different methods like pointwise, pairwise, and listwise approaches that have been studied a lot (Liu, 2009). Some of these methods have been used for document summarization, such as SVR (Ouyang et al., 2007), classification SVM (Wang et al., 2007), and RankNet (Svore et al., 2007). However, no one has really compared these ranking algorithms in a systematic way. As far as we know, this is the first time a listwise learning-to-rank algorithm called ListNet (Cao et al., 2007) is being used for document summarization in this paper. Plus, pairwise and listwise learning-to-rank algorithms have never been applied to concept ranking for extractive summarization before.",
        "formal_text": "Learning to rank attracts much attention in the information retrieval community recently. Pointwise, pairwise and listwise learning-torank approaches have been extensively studied (Liu, 2009). Some of those have been applied to document summarization, such as SVR (Ouyang et al. , 2007), classification SVM (Wang et al. , 2007), and RankNet (Svore et al. , 2007). Again, there is no work to systematically compare these ranking algorithms. To the best of our knowledge, this is the first time that a listwise learning-to-rank algorithm, ListNet (Cao et al. , 2007), is adapted to document summarization in this paper. Moreover, pairwise and listwise learning-to-rank algorithms have never been used to perform concept ranking for extractive summarization.",
        "GPT2_formal_text": "\" array, which is a list of how often each response is tagged. There are two types of this setup: (i) regular responses and (ii) responses that are paraphrased.\n\nIn (ii), the names of the two entities are linked to the \"response_id\" array. If we want to make sure we're comparing responses fairly, we set two types of labels: label_type1 and label_type2. The \"response_id\" array is made by combining the response label_type1 and the label_type2 values into a single number.\n\nNext, we turn each entity's response into a number, which we then use as a rough estimate for how similar it is to the response. We calculate this similarity between the response and the entity using a formula based on the entity's response. Formal: Here's how we do it: Formal: We calculate a similarity value (f_sim(r_a(x) + r_b(x)) between two responses x_a and x_b, using two types of relations: (r_a(x) and r_b(x)) and (r_a(x) and r_b(x)). Formal: Here's the formula we use: Formal: Using this similarity value, we create a similarity value between any two responses r_a and r_b. Formal: Here, \"r_a\" and \"r_b\" are two different responses. The similarity value between them is then calculated using this formula. Formal: Formal: The final similarity value between any two responses is calculated using this formula. Formal: We also do another similarity calculation between any two responses r_a_a and r_b. Formal: We do this twice to get the final similarity value. Formal: Lastly, we calculate the similarity value between any two responses r_a_b. Formal: Formal: Formal: Formal: The similarity value between any two responses is calculated using this formula. Formal: We calculate the similarity value between any two responses with two types of relations: (r_a(x) and r_b(x)) and (r_a(x) and r_b(x)) Formal: We also do another similarity calculation between any two responses with two types of relations: (r_a_a and r_b(x)) and (r_a(x) and r",
        "directory": "C10",
        "filename": "C10-2060.json"
    },
    {
        "casual_text": "Nie et al. (2020) came up with a thing called SA-NER that uses some fancy methods to deal with data sparsity issues. Basically, they grab extra semantic info from a huge collection of text and then use two cool tools: an attentive semantic augmentation module and a gate module. These tools help them organize and combine all that extra info.",
        "formal_text": "• Nie et al. (2020) (SA-NER), which utilize semantic enhancement methods to reduce the negative impact of data sparsity problems. Specifically, the method obtains the augmented semantic information from a largescale corpus, and proposes an attentive semantic augmentation module and a gate module to encode and aggregate such information, respectively.",
        "GPT2_formal_text": "_layers = { 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 } d_n = [ 0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9 ] Formal: Let’s think about how these attention weights interact with each other. Basically, A_s_i is a matrix that helps pick out important parts of the image, and A_t_i is another matrix that focuses on the important parts of the image. Formal: Also, if we’ve got a hidden layer (h_l), which is just a number between 0 and 1, the attention weights, called θ_t, can be broken down into two main parts: 1) how much attention each part gets (the softmax part) and 2) how much attention each part gets (the hidden layer part). Formal: Here’s how the equation is set up: Formal: The attention weights A_t_i are calculated based on the hidden layer part A_l, which is d_1 divided by d_2. Formal: Formal: To make sure the attention weights don’t get messed up, we add a linear layer between A_s_i and A_t_i. Formal: And this is how the equations are set up: Formal: The attention weights A_t_i are calculated based on the hidden layer part A_l, which is d_1 divided by d_2. Formal: Formal: Formal: Here’s how the equation is set up: Formal: Formal: We take the parameters θ_l and A_s_i, along with the hidden layer part A_l, and use them to calculate the attention weights for each image, which we’ll call a_t. Formal: Formal: Finally, we add this all together to get the final attention weights for the image. Formal: Formal: Formal: Formal: Formal: We use this equation to estimate how much attention each part gets, and that’s the attention weights for each image. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.383.json"
    },
    {
        "casual_text": "Argumentation Mining (AM) is all about spotting the parts of arguments, like claims and premises, and figuring out how they support or attack each other to show how arguments are structured. This has been done in a bunch of areas recently, like legal documents (Mochales Palau and Ieven, 2009), news articles (Deng and Wiebe, 2015; Sardianos et al., 2015), and user-generated content (Wachsmuth et al., 2014; Habernal and Gurevych, 2015). The goal is to automatically find arguments in messy text by sorting out what's argumentative and what's not, and then pulling out the key components and how they connect.\n\nLately, there's been more interest in making tools that use AM to help people with their arguments, especially for students who need feedback on their writing (Song et al., 2014; Stab and Gurevych, 2014a, b; Wambsganss et al., 2020b). These tools give personalized tips on how to improve their argumentation. But, using this tech in schools hasn't really taken off yet (Stab and Gurevych, 2017b; Lawrence and Reed, 2019; Rosé et al., 2008), mostly because there aren't many datasets with student-written texts that have been labeled for argumentation (Lawrence and Reed, 2019; Wambsganss et al., 2020c).",
        "formal_text": "Argumentation Mining AM aims to identify argument components in the form of claims and premises, along with support and attack relationships that model the discourse structure of arguments. In recent years, this has been done for several domains, including legal texts (Mochales Palau and Ieven, 2009), newswire articles (Deng and Wiebe, 2015; Sardianos et al. , 2015), or usergenerated content (Wachsmuth et al. , 2014; Habernal and Gurevych, 2015). The objective is to automatically identify arguments in unstructured textual documents based on the classification of argumentative and non-argumentative text units and the extraction of argument components and their relations. Recently, researchers have built increasing interest in adaptive argumentation support tools based on AM (Song et al. , 2014; Stab and Gurevych, 2014a, b; Wambsganss et al. , 2020b), offering argumentative writing support to students by providing individual feedback about the argumentation discourse. However, utilizing this technology in a pedagogical scenario for educational purposes lacks a wider-scale adoption (Stab and Gurevych, 2017b; Lawrence and Reed, 2019; Rosé et al. , 2008), as argumentation-annotated corpora with student-written texts are rather rare (Lawrence and Reed, 2019; Wambsganss et al. , 2020c).",
        "GPT2_formal_text": "= (h_s, h_e, h_t)\n           Casual: We use the same approach to check how different the responses are between two models. Since both models are trained on the same dataset and use the same setup, we compare their average hidden units and average attention weights (from Mikolov et al., 2013) to get an idea of how similar their representations are. We also use the same training set for both models to make sure we're not comparing apples and oranges. Formal: We check how similar the outputs are using the same cross-entropy loss function, and the results are shown in Figure 1. The \"No Response\" category means both models didn't generate a response. Formal: The number of responses generated is a bit higher for the negative class compared to the positive class. Formal: We only used the model's average attention weights for the negative class. Formal: In the zero-shot and few-shot scenarios, we just used the average attention weights for the negative class. Formal: We use the model's performance on the validation set to estimate its performance on the test set. Formal: As you can see in Figure 1, the best model performs better in zero-shot and few-shot scenarios, but in the full-shot scenario, it's about the same. Formal: In our zero-shot and few-shot scenarios, the model's performance is pretty similar to the full-shot model, but it's noticeably worse in the zero-shot scenario. Formal: In the full-shot scenario, the model's performance is the highest, but in zero-shot and few-shot scenarios, it's the lowest. Formal: We also looked at the average attention weights for both models. Formal: In both zero-shot and few-shot scenarios, the model's performance is pretty similar to the full-shot model, but in zero-shot and few-shot scenarios, the performance is the highest. Formal: In the zero-shot scenario, the model's performance is the highest, but in few-shot and few-shot scenarios, the performance is the lowest. Formal: We used the model's performance on the validation set to estimate its performance on the test set. Formal: The best model performs better in the few-shot scenario. Formal: The best model performs better in the full-shot scenario. Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.599.json"
    },
    {
        "casual_text": "Alright, so this just gives us the axiom links. We still need to figure out the word order so that none of the axiom links cross. Turns out, this can be done in less than quadratic time—it’s basically a bracketing problem.",
        "formal_text": "Actually, this only gives us the axiom links. It still requires to compute the word order to have no crossing axiom link. This can be done from the axiom links easier than quadratic time (it is a bracketing problem).",
        "GPT2_formal_text": "= {0, 1, 2, ..., M}. Formal: We don't have any fancy neural network setups here, just a regular old RNN, so the attention weight matrix M is just a linear combination of all the word embeddings. Formal: Let's say we have a sentence s = (x1, x2, ..., xn) that's been tokenized. Now, the network gives us the attention distribution for each token in this sentence. Formal: The first part (the s-th word) in the sequence gets its attention vector M = A(x1) + A(x2), and the second part (the s-th token) gets the attention vector M = A(x2) + A(x3). Formal: Each attention vector has a length of M. We used an attention mechanism (like the one Vaswani et al. came up with in 2017) to figure out the attention weight vector for each token. Formal: Let's say we have a list of words w = (x1, x2, ..., xn). If we have a pre-trained language model (LM) p(x1, x2, ..., xn), we can figure out the attention vector M = A(w1) + A(w2) + ... + A(wn) for each word in w. Formal: We use the LSTM (Hochreiter and Schmidhuber, 1997) to get a hidden state h for each token in the sentence. Formal: Formal: For instance, if we have the sequence x = (x1, x2, ..., xn) in our dataset, the attention vector M = A(x1) + A(x2) + ... + A(xn) would be the same as the attention vector M = A(w1) + A(w2) + ... + A(wn) in the model. Formal: Formal: Formal: Finally, we can figure out the attention weight vector for each token in the target sentence by using the model's output h = (h1, h2, ..., hn) as the hidden state for each token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C00",
        "filename": "C00-2091.json"
    },
    {
        "casual_text": "The language stuff we're talking about here comes from basic grammar rules that are pretty common and not just specific to Hindi.",
        "formal_text": "The linguistic features exploited here are drawn from a range of well-understood grammatical features and are not peculiar to Hindi alone.",
        "GPT2_formal_text": "of the documents in D_s are really useful for figuring out how similar different documents are, especially when there are a lot of them. It’s a quick and easy way to spot where the documents overlap, both at the sentence and document levels. Formal: In this project, we’re introducing an updated version of the SLU algorithm, which we call the Iterative Co-Attention-based version (ICB). The goal is to make the most of the input sentences to make sure the documents have similar structure, but we’re focusing on smaller tasks first. So, we’re skipping the fancy stuff like MSR and CNN models that rely on super complex attention mechanisms, like those from Mikolov et al. (2013). Formal: Check out Figure 1 for the basic structure of our new algorithm. The IOBES scheme has three main parts: the sequence labeling part (S), the relevance-based attention part (A), and the word-based attention part (X). The final decision makes the whole process happen in a supervised way. Formal: So, for the first part, we start by running a full sentence-level SLU on each document D_s. The results are fed into a GRU-based attention module (from Lafferty et al., 2001) to get the final predictions. Formal: The second part is a bunch of simple operations that help us understand what’s important in each document. We handle this by using a special evaluation function to decide what to focus on. Formal: Then, we check the relevance of each sentence and figure out which ones should be treated as key for the rest. Formal: The final prediction is the average of all the word embeddings of the key documents. Formal: The IOBES scheme has a few key differences from other approaches. First, the sequence labeling part (S) has a delay, so it’s not as quick to compute. Second, it doesn’t work with CNNs because they’re too complicated for this kind of thing. Formal: The relevance-based attention part (A) is way more efficient because it only needs one attention component to handle the whole sentence-level process. Formal: Lastly, the word-based attention part (X) is more efficient because it can combine multiple vectors. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C12",
        "filename": "C12-1152.json"
    },
    {
        "casual_text": "Alright, let's break down the process shown in Figure 1, which outlines our Graph2Tree framework. First off, Graph2Tree starts by using a BiLSTM to encode the MWP text input. At the same time, it builds two graphs: the Quantity Cell Graph and the Quantity Comparison Graph. The BiLSTM gives us word-level representations, which we use as the nodes for these graphs.\n\nNext, these node representations, along with the two graphs, are fed into a graph transformer. This transformer has a modified multiGCN component that focuses on learning the graph representation based on the Quantity Cell Graph and Quantity Comparison Graph. This tweak helps the model understand the relationships between quantities and their numerical qualities, making the final graph representation richer.\n\nAfter that, pooling is applied to combine all the nodes into a single graph embedding vector, which is the output of the graph transformer. Finally, this graph representation, along with the updated node representations, is passed to a tree-structure decoder. The decoder then works to figure out the final solution expression tree.",
        "formal_text": "3 Methodology Figure 1 shows our proposed Graph2Tree framework. Graph2Tree first encodes the MWP text input using BiLSTM and simultaneously constructs Quantity Cell Graph and Quantity Comparison Graph. The output of BiLSTM, word-level representations, are used as node representations. Together with the two constructed graphs, the node representations are input into a graph transformer to learn a graph representation of the MWP. The multiGCN component of the graph transformer is modified to learn the graph representation based on the Quantity Cell Graph and Quantity Comparison Graph. This enriches the final graph representation with quantities' relationship information and numerical qualities. Pooling is used to aggregate all nodes into a pool-based graph embedding vector as the graph transformer's output. Finally, the output graph representation and the updated node representations are used as input to a tree-structure decoder to infer the final solution expression tree.",
        "GPT2_formal_text": "ed = [u1, ..., un] + [uj, ..., uk] [attn_mask = [attn1, ..., attnn1], attn2 = [attn21, ..., attn2n1], attn_masked = [attn2i, ..., attn2i], attn1 = [attn1i, ..., attn1i], attn2 = [attn2i, ..., attn2i], attn1_mask = [attn1i_mask, ..., attn1i_mask], attn2_mask = [attn2i_mask, ..., attn2i_mask] ]\n                            Casual: We also use the same setup for the distillation objective to train the student model. Formal: In Figure 3, you can see an example of the student model with attention-based distillation. Formal: The student model learns from both the teacher and the student's predictions. Formal: The student model is trained using attention, and the model's output is the feature for each token, which is a combination of both the teacher and the student's outputs. Formal: The student model is trained by combining the output from the teacher's predictions with the student's predictions to get the final output. Formal: The student model can also generate its own tokens using the teacher's predictions. Formal: In the last part of the model, the student model also figures out how to generate its own tokens. Formal: Formal: We can use the output from the student's predictions to train the teacher to make sure the student's predictions match the teacher's predictions. Formal: Formal: Formal: The student model is trained to generate the token for the target token, which is a combination of both the teacher's predictions and the student's predictions. Formal: Formal: Formal: We can also use the output from the teacher's predictions to train the teacher to make sure the student's predictions match the teacher's predictions. Formal: Formal: Formal: We can also use the output from the teacher's predictions to train the teacher to make sure the student's predictions match the teacher's predictions. Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.362.json"
    },
    {
        "casual_text": "You can find the MELD dataset here: https://github.com/SenticNet/MELD, and the IEMOCAP dataset here: https://sail.usc.edu/iemocap/iemocap_release.htm. Basically, A is a matrix in R w×dq, where w stands for the total number of window segments.",
        "formal_text": "https: //github. com/ SenticNet/MELD, https: //sail. usc. edu/ iemocap/iemocap_release. htm i. e. , A ∈ R w×dq where w represents total window segments.",
        "GPT2_formal_text": "= arg max n < arg min n ) + arg max p n < arg min p n )          Casual: The model generates words based on how likely they are to be important in the end-to-end decoding process. Formal: As a rough estimate, we take the average of the arg max n and arg min n scores from the whole dataset to figure out the most likely answer to a question q = (q, t). Formal: We measure how likely each word in the question is to be important by averaging the arg max n and arg min n scores for all the words in the question. Formal: Next, we calculate the probability P(t|q) for every possible t and q combination. Formal: Formal: We use a well-known method for evaluating how well a question answering model does: bag-of-words or tf idf. Formal: For every word in the question, we calculate the tf idf score for the word that comes right before it. Formal: After we find the most likely t and q combination, we use the cosine similarity between their scores to rank the most important words in the question. Formal: For a given word in the question, the cosine similarity score is calculated by looking at the cosine similarity of the word to itself. Formal: The top 50 words in the question are chosen to be the most important words based on their tf idf scores, like they're the ones that have the highest scores. Formal: This process is repeated for each word in the question until the question is complete. Formal: Formal: To calculate the tf idf score for each word in the question, we calculate the tf idf score for the word that comes right before the word in the question. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.402.json"
    },
    {
        "casual_text": "Check out the appendix for the PPLM details. And yeah, the CTRL details are in there too. For the implementation, we're using the one from this GitHub link: https://github.com/gooppe/transformer-summarization.",
        "formal_text": "See appendix for PPLM implementation details. 11 See appendix for CTRL implementation details. 12 We use the implementation at https: //github. com/gooppe/transformer-summarization.",
        "GPT2_formal_text": "ed_pooling + -ct) + -ct) - un-masked_pooling) - -pooling) - ct) - -ctt) - -ctt) - ct) - -pooling) - ct) - -pooling) - (1 + pooling) (2 + pooling) (3 + pooling) (4 + pooling) (5 + pooling) (6 + pooling) (7 + pooling) - (1 + pooling) - (2 + pooling) - (3 + pooling) - (4 + pooling) - (5 + pooling) - (6 + pooling) - (7 + pooling)\n                 Casual: It's worth noting that the label attention for the \"l\" entities in the sequence does follow a typical distribution, as shown in Figure 2. This means that the way the attention is assigned to each token depends on its label, which is randomly picked from the last token of the sequence. Formal: In Table 1, it's noted that the model can pick up on certain patterns, but these patterns aren't always the same across all labels. This might make it tricky to predict the label for a new token in the sequence. Formal: The model's attention is also used to pick up on certain patterns, but these patterns aren't always the same across all labels. Formal: Specifically, the attention for the \"l\" entity in the sequence is based on the position of the last token in the sequence. Formal: This means that the attention for a new token in the sequence can only pick up on the first token in the sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.526.json"
    },
    {
        "casual_text": "When figuring out vocabulary sizes and word stats, we usually clean up the text by making everything lowercase and getting rid of any characters that aren't letters or numbers.",
        "formal_text": "When computing vocabulary sizes and word statistics, text transcripts are normalized by lower-casing all the alphabets and removing characters that are neither alphabets nor digits.",
        "GPT2_formal_text": ", and it can work with any text generation model. It calculates this feature representation for the text t using the rules laid out in Eq. 2. The final representation for the generated text is calculated using the same approach as before. Formal: To make the final representation of the generated text, we use a greedy algorithm. Formal: In our experiments, we picked a linear-chain CRF with a length of 10. The model parameters for this CRF are listed in Table 2. Formal: The basic structure of a CRF can be described using something called a CRF tree. In our work, we use a CRF tree structure to explain our method for generating text. Formal: We use a CRF to create a context vector for the generated text, which is based on the input sequence X. Formal: The final representation for the generated text is calculated by applying the method from Section 4.1. Formal: After applying the method from Section 4.1, we get the final text representation for the text t using the rules laid out in Eq. 2. This representation is then passed into a CRF to generate the text. Formal: In the rest of the paper, we'll explain the algorithm for generating text using this CRF setup. Formal: Lastly, we'll talk about how we evaluate the quality of the generated text. Formal: We did some experiments using a 100-dimensional text embedding, and the results are in Table 4. Our method works really well for text generation tasks, showing a clear improvement over the current state-of-the-art methods. Formal: We tested our method with both monolingual and bilingual text, as well as text with non-English units, on the TREC 2010 test set. We compared our method to three other techniques: bi-directional LSTM (from Dyer et al., 2015), retrofitted LSTM (by Shen et al., 2016), and flat-CRF (by Liu et al., 2016). Formal: We tested our method with both monolingual and bilingual text, as well as text with non-English units, on the TREC 2010 test set. Formal: We also tried it with text with non-English units, which we call non-English text. Formal: We tested our method with text with non-English units, which we call non-English text. Formal: Formal: We looked at how well our method with non-English",
        "directory": "acl",
        "filename": "2021.acl-long.411.json"
    },
    {
        "casual_text": "When you’ve got the annotated document ready, it’s pretty straightforward to use it to figure out p(t a |t b ). The document should have all the info you need, even if someone forgot to add some tags. Here’s the formula we use to estimate p(t a |t b ) based on how often tags and words show up together:",
        "formal_text": "When the content of the annotated document is available, using it for estimating p(t a |t b ) is a natural thought. The content is expected to be complete and information-rich whether or not the user has omitted any tags. We use the following formula to estimate p(t a |t b ) by tag-word cooccurrences:",
        "GPT2_formal_text": "ed is the output of the attention module at the token level. The vocabulary is d_V = {d_V_1, ..., d_V_n} and we use the base RNNG to create these representations. Formal: The attention weights, w_A_t and w_A_b_t, are calculated based on the input query (q_t) and the entity mention (i_m) at the token level. Formal: We use a GRU to generate the representation for the token level, which we call h_t. Formal: The model tries to estimate the weights for both the token and entity levels using the feedback from the model. Formal: The dataset has 12 million sentence pairs, each with 12,000 sentences. Formal: For sentence pair (s1, e1), the model generates the representation for the entity pair (e1, s1). Formal: We fine-tune the model using a batch of examples. Formal: The model uses the loss function L_w(q_t, i_m, w_A_t, w_A_b_t) to minimize the losses for both the token and entity levels. Formal: We use the adversarial training method (from Bahdanau et al., 2015) to handle all three types of knowledge sampling. Formal: For the token and entity levels, the model learns from the weighted sum of the feature vectors from the token and entity levels. Formal: We train the model using the loss function L_w(q_t, i_m, w_A_t, w_A_b_t) with a batch size of 100. Formal: The model also learns from the weighted sum of the feature vectors from the token and entity levels, but this time, the entity and token level features are combined. Formal: To make sure the model can easily adapt to different scenarios, we use a dropout rate of 0.1. Formal: The model learns from the weighted sum of the feature vectors from the token and entity levels, but this time, the entity and token levels feature vectors are combined. Formal: We train the model using the loss function L_w(q_t, i_m, w_A_t, w_A_b_t) with a batch size of 100. Formal: We fine-tune the model using the loss function L_w(",
        "directory": "C10",
        "filename": "C10-1114.json"
    },
    {
        "casual_text": "• Data Sets: As mentioned in Section 3, the annotated question-answer pairs are from three different areas. For each area, we divided the data into a training set (80% of each category) and a test set (20% of each category). On top of that, we kept 10% of the training set aside as development data to tweak the parameters.",
        "formal_text": "• Data Sets: As introduced in Section 3, the annotated QA text pairs cover three different domains. In each domain, we randomly split the data into a training set (80% in each category) and a test set (20% in each category). In addition, we set aside 10% from the training set as the development data for parameters tuning.",
        "GPT2_formal_text": "_sent) = {s1, s2, ..., sK}, where K is the number of mentions in the set {i, j}. Formal: A sentence has a bunch of key-value pairs, and the set {i, j} includes the key and value pairs. Formal: The sum of all the attention scores (a_t_i, a_t_j) for an entity mention ei, including the attention weight α_t_i for its representation in the hidden layer h(ei), is equal to the sum of all the attention scores (a_t_i, a_t_j) for the representations of all the entities in the mention ei. Formal: Formal: We pick the top entity mention that has the highest attention score, or the highest attention weight, for each token x_i. Formal: Formal: Then, we calculate the attention weight α_t_i for this entity mention. Formal: To figure out the attention weight for an entity mention ei, we use a Transformer encoder (from Vaswani et al., 2017) that was trained on the gold mention set. Formal: Formal: Formal: For the model parameters θ_t and θ_e, we just normalize the attention weights by their standard attention values. Formal: Formal: Formal: The attention weights α_t_i are then added to the attention weight α_t_i for the mention ei. Formal: Formal: Lastly, we average the attention weights α_t_i across the mentions in the mention ei, which gives us the attention weight for the mention ei. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1401.json"
    },
    {
        "casual_text": "We picked Japanese literature because the researchers in that area really wanted a better system. The usual keyword-based systems just didn’t cut it for literature stuff.",
        "formal_text": "One of the reasons we chose the field of Japanese literature was that there was strong demmld from th. e researchers in that field to make a useful system; systems based on conventional key words did riot work welt for the field of literature.",
        "GPT2_formal_text": "ing is the way we figure out the attention weight, w_t, for the word w_t in a sentence. It’s calculated by adding up the regularization term: f(y_t * e_s + η_t * e_s + w_t * c_s). Formal: In this part, we’ll talk about how the model learns about and uses different kinds of representations, like embeddings, word embeddings, and embedding-based attention. We’ll also explain the neural architecture that goes with this. Formal: Basically, the model learns about the context by using the embedding of the word it’s looking at and the attention mask it gets from the embedding of its last word. This last word is another embedding. Formal: If we’re dealing with a specific context c_s, we can use the embedding of the word that’s most related to the context to figure out the attention weights for the last word. Formal: We’ll use some fancy math to show how to train our model using these attention weights. Formal: Our model can also be tweaked to work with different window sizes, and we’ll explain that in the next section. Formal: In this part, we’ll dive into the specific tasks we’re focusing on and explain how we’re using this approach for them. Formal: The neural network setup we’re proposing has two main parts: the embedding layer and the final attention layer. Formal: In this part, we’ll introduce our version of BERT, which is a solid pretrained model that can handle different tasks. Formal: We’ll show you how to use it to get a bunch of BERT models, each trained on different tasks, and then we’ll create a dataset to test how well the models perform on different tasks. Formal: To make our model even better, we’ll add more layers and train it to work better with other tasks. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C86",
        "filename": "C86-1089.json"
    },
    {
        "casual_text": "We're using β to draw a comparison to inside probability—well, more specifically, the Viterbi approximation of inside probability. This is because we're focusing on maximizing instead of summing up all the possible parses.",
        "formal_text": "The use of β is intended to suggest an analogy to inside probability-or more precisely, the Viterbi approximation to inside probability (since we are maximizing rather than summing over parses).",
        "GPT2_formal_text": "= {t_i, t_j, ..., t_n} | {s_i, s_j, ..., s_n}, {s_i+1, s_j+1, ..., s_n+1}.            Casual: In this paper, we're treating the problem of predicting dialog acts as a classification problem. Formal: The probability P(z|x, y) for dialog acts is defined as the product of the probabilities for different labels, like z = 1, ..., n. Formal: We've built a fancy model that learns to figure out the probability of certain labels, based on the dialogue acts, as the dialogue goes on. Formal: When we tested it on the PDTB dataset, it performed really well. This model is strong enough to handle tricky dialogues, even with a lot of possible labels. Formal: We think this model can really boost the quality of the dialogues it generates, making them more informative and relevant. Formal: We showed that we can use this model to predict the chance of a label appearing, based on the acts, even if the dialogues are messy. Formal: The model we built can handle tricky dialogues with lots of possible labels, even if there are a lot of possible labels. Formal: We found that the model can predict the probability of a label appearing, even if there's a lot of possible labels. Formal: We're also using this model to predict the probability of a label appearing, even if the acts are messy. Formal: We're building a model that can predict the probability of a label appearing, even if the acts are messy. Formal: We're using this model to predict the probability of a label appearing, even if the acts are messy. Formal: We're also using this model to predict the probability of a label appearing, even if the acts are messy. Formal: We're building a model that can predict the probability of a label appearing, even if the acts are messy. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D09",
        "filename": "D09-1105.json"
    },
    {
        "casual_text": "Classification systems, whether they're basic logistic regression or fancy neural networks, usually predict the likelihood of different classes and pick the one with the highest probability. We then check how well these predictions match the actual labels (called ground-truth labels) on new, unseen data to see how good the model is. But sometimes, we need to pay close attention to how confident the model is in its predictions, not just whether it got the right answer. For example, if the model's confidence levels are accurate, it can help us figure out if a tool predicting someone's likelihood of reoffending is fair (Chouldechova, 2017) or decide the best number of labels for medical diagnoses (Kavuluru et al., 2015). Guo et al. (2017) pointed out that even if a model is really good at getting the right class, it doesn't always mean it's good at estimating how sure it is about that prediction.\n\nTo fix models that aren't great at showing their confidence, we use calibration methods (like the ones from Zadrozny and Elkan, 2001; Platt et al., 1999; Guo et al., 2017; Kumar et al., 2019). These methods tweak the probabilities the model gives after it's been trained. They basically retrain the model a bit on a separate validation set to get a better idea of how confident it should be, and then apply that to new test data. The problem is, this can mess up the consistency of the model's predictions across different data sets. Since the data is split into fixed parts, the calibration doesn't really adapt to how well the model is doing, making it kind of rigid and not very flexible.",
        "formal_text": "Classification systems, from simple logistic regression to complex neural network, typically predict posterior probabilities over classes and decide the final class with the maximum probability. The model's performance is then evaluated by how accurate the predicted classes are with respect to outof-sample, ground-truth labels. In some cases, however, the quality of posterior estimates themselves must be carefully considered as such estimates are often interpreted as a measure of confidence in the final prediction. For instance, a well-predicted posterior can help assess the fairness of a recidivism prediction instrument (Chouldechova, 2017) or select the optimal number of labels in a diagnosis code prediction (Kavuluru et al. , 2015). Guo et al. (2017) showed that a model with high classification accuracy does not guarantee good posterior estimation quality. In order to correct the poorly calibrated posterior probability, existing calibration methods (Zadrozny and Elkan, 2001; Platt et al. , 1999; Guo et al. , 2017; Kumar et al. , 2019) generally rescale the posterior distribution predicted from the classifier after training. Such post-processing calibration methods re-learn an appropriate distribution from a held-out validation set and then apply it to an unseen test set, causing a severe discrepancy in distributions across the data splits. The fixed split of the data sets makes the post-calibration very limited and static with respect to the classifier's performance.",
        "GPT2_formal_text": "ed_coder, and h_n_t represents the attention history from the last time step. Formal: The encoder, decoder, and attention models are all trained using the cross-entropy loss for each task, which is calculated as L(t_t) = (1 - α)^sum_{t-1} log(θ_t(t) - α). Formal: This loss is calculated based on the cross-entropy loss for each task, and the highest possible value is calculated using the minibatch loss. Formal: The model looks at the last 10 words of the input sequence to guess the label. Formal: During training, the attention mechanism picks the attention history that has the highest attention weight (the highest value of θ_t(t)) and passes it through a fully connected layer. Formal: The token embedding is created by randomly picking an embedding vector for the token t_t. Formal: If a token t_t appears more than once in the input sequence, it gets removed from the sequence. Formal: The model uses the average of the attention weights (α_t) to estimate the token embedding. Formal: The attention weights α_t are calculated for each token t_t. Formal: The output embedding, y_t, is a tensor with dimensions |t|×d_t, where d_t is the dimension of the output embedding. Formal: For each token t_t, the attention weights α_t are combined using a fully connected layer and then passed through a fully connected layer again. Formal: Formal: The output embedding y_t is a tensor with dimensions |t|×d_t, where d_t is the dimension of the output embedding. Formal: The decoder decodes the attention history for each token t_t in a sequence and combines the output embedding y_t with it. Formal: Formal: The attention mechanism picks the attention history that has the highest attention weight (the highest value of θ_t(t)) and passes it through a fully connected layer. Formal: The attention weights α_t are calculated for each token t_t. Formal: Formal: The model uses the average of the attention weights (α_t) to estimate the token embedding. Formal: Formal: The output embedding, y_t, is a",
        "directory": "acl",
        "filename": "2020.acl-main.242.json"
    },
    {
        "casual_text": "It's pretty cool to see which tech and online stuff people are into. In the UK, Twitter and Facebook are big deals, but in India, Orkut used to be the go-to. Over in Singapore, blogging platforms like Wordpress are super popular.",
        "formal_text": "It is also interesting to compare what technologies and web services people use. Twitter and Facebook are popular in the UK whereas Orkut is more popular in India. Blogging services like Wordpress are popular in Singapore.",
        "GPT2_formal_text": "\" : [ \"ear_likes\" ] [ \"heart_likes\" ] [ \"sore_likes\" ] [ \"to_sore\" ]\n\nA bunch of pre-trained models have been developed for generating responses, with the most popular ones being GPT-2 (Radford et al., 2019) and XLNet (Lewis et al., 2020). While these models are already pretty good at generating smooth and natural-sounding responses, they often struggle with generating responses that aren't super specific or specific to the situation.\n\nOne way to fix this is to use prompts that are tailored to the specific area, which is known as domain adaptation (Liu et al., 2019). This method creates tailored prompts for different domains (Kim et al., 2019). But here's the thing: generating responses for a specific area doesn't always make sense from a practical standpoint.\n\nImagine you're trying to translate a language using a machine translation system. The text you're translating might be related to a specific topic. On the other hand, a response to a question might not be tied to any specific area. So, the generated text is often way more generic and not very tailored. Formal: The topic you're looking at doesn't really matter, but the content you're generating might. Formal: The context around the question doesn't really matter, but the content you're generating might. Formal: The context around the question and the question you're answering doesn't really matter, but the content you're generating might. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D09",
        "filename": "D09-1146.json"
    },
    {
        "casual_text": "Looking at Table 5, the NB classification model throws in some random hashtags that don't really fit. On the other hand, LDA, being a generative model, leans towards suggesting broader hashtags like \"Information News\", \"mobile phone\", or \"Technology leaders\". But it misses out on more specific ones like \"WWDC\" or \"MAC OS Lion\". The IBM1 method also suggests some hashtags that have nothing to do with the topic, like \"2012 Jinshan Inc cloud computing\" or \"2012 spring and summer men's week\", just because they both have \"2012\" in them. In contrast, TSTM does a better job by suggesting specific hashtags that actually relate to the tweet's topic.",
        "formal_text": "From Table 5, we observe that classification model NB suggests some unrelated hashtags. While LDA, as generative models, tends to suggest general hashtags, such as \"Information News\", \"mobile phone\" and \"Technology leaders\", and fail to generate the specific hashtags \"WWDC\", \"MAC OS Lion\". IBM1 method will suggest some topic-unrelated hashtags. For instance, \"2012 Jinshan Inc cloud computing\" and \"2012 spring and summer men's week\" are triggered by the word \"2012\". On the contrary, TSTM succeeds to suggest specific hashtags, and most of them are topic-related to the tweet.",
        "GPT2_formal_text": "_x is calculated by summing up the attention weights from the last hidden state h_t. Formal: The main thing that sets the model apart from the entity-based approach is the ability to handle context. Formal: Entity-based models are usually set up to just look at the entities in the text, but this can lead to issues because the model might not be able to pick up on other important details in the text. For example, if the model only focuses on entities (and doesn’t even consider the context), it can miss out on other important details like how the text connects to other parts of the entity. Plus, the model might get mixed up by a context that isn’t really related to the entities. Formal: To address these issues, we’ve come up with a different approach. We’ve created a self-attention network to learn not just about the entities in the text, but also about how they’re connected to each other and to the context. By using this new attention mechanism, the model can still pay attention to the entities and context in the text, but it can also recognize patterns in the text that might indicate an entity might be mentioned. This helps the model focus on entities that are likely to show up in the text. Formal: We also added a penalty for entities that are mentioned in the text, even if they weren’t actually in the input. Formal: This self-attention mechanism works alongside the entity-based model to boost its ability to pick up on the right information. Formal: We tested it on the CoNLL-2014 dataset, which has over 200,000 sentence pairs from Wikipedia. We used the BERT model (Devlin et al., 2019) as our base model. Formal: The main results for different language pairs are in Table 4. We picked the model that performed best on the validation set. Formal: Figure 3 shows the architecture of our model, which includes three main parts: the entity-based model, the self-attention network, and the graph attention mechanism. Formal: Formal: When compared to the entity-based model, our model also performs better on the validation set. Formal: Formal: Formal: Table 4 shows the results for each language pair, along with the validation set performance. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C12",
        "filename": "C12-2027.json"
    },
    {
        "casual_text": "We need to use both the grammar stuff and the meaning details from DICT for this process.",
        "formal_text": "This process must employ both syntactic features and semantic information from DICT.",
        "GPT2_formal_text": "= list(t) for i from 1 to n-1, where j ranges from 1 to k. Formal: To handle both the left-to-right and right-to-left decoding, we’ll use a BERT encoder-decoder setup. For the encoder, we’ll add a softmax layer that predicts the next word in the sequence. For the decoder, we’ll use a Bi-LSTM cell that predicts the next token in the sequence. We’ll call this the prediction cell for decoding. Formal: Our experiment setups are shown in Fig. 3, and we’ve included some reference translations in Table 1. Formal: To make sure we’re comparing the system’s performance to a reference, we’re using the same setup as Yang et al. (2020b). This means we’re using the same setup as Cheng et al. (2019) and Sun et al. (2019), which include the same hidden layers and a Bi-LSTM cell. Formal: For a deeper dive, check out Appendix A.2. Formal: We’ve got reference translations in Fig. 4 and Table 1. From these, we can see that our method works really well and covers a lot of vocabulary. Formal: To see how our method improves performance, we’ve included a few samples in Table 1. Formal: For the right-to-left decoding, we’re using a Bi-LSTM cell with an attention mechanism (Yang et al., 2020b) that’s optimized for both the left and right sides of the sequence. Formal: We’ve included reference translations in Fig. 5 and Table 1. Formal: To measure how well our model performs, we’re comparing the model’s performance to the reference using BLEU (Papineni et al., 2002). Formal: For the left-to-right decoding, we’re using a Bi-LSTM cell with an attention mechanism (Yang et al., 2020b) that’s optimized for both the left and right sides of the sequence. Formal: For the right-to-left decoding, we’re using a Bi-LSTM cell with an attention mechanism (Yang et al., 2020b) that’s optimized for both the left and right sides of the sequence. Form",
        "directory": "C67",
        "filename": "C67-1032.json"
    },
    {
        "casual_text": "The accuracy difference between the test set and dev set could be because we're only using a part of ConceptNet. This subset was picked based on the vocabulary from the training and development data. But the test data might have words that aren’t in this subset. So, there might be hardly any or even no connections for the test data in the chosen subset. That’s why the accuracy for the test data in the model without Q is pretty similar to the accuracy for the dev data in the model without P A Rel and QA Rel.",
        "formal_text": "The reason for difference in accuracy of test set and dev set might be due to the fact that we are using a subset of ConceptNet. The subset was selected based on the vocabulary of training data and development data. The vocabulary of test data might not be in the selected subset of ConceptNet. There might be few or even no edges for the test data in the selected subset. Thus the accuracy of test data for model w/o Q is pretty close to accuracy of dev data for model w/o P A Rel andQA Rel.",
        "GPT2_formal_text": "_tgt.cc/en. The model was trained using the AdamW optimizer (from Kingma and Ba, 2015) and its learning rate was set to 0.001. We picked the hyperparameters with the smallest difference between the best model and the others. Formal: Basically, a model that predicts the next word in a sentence with the highest confidence is the best at it, no matter what the context or the user says. Formal: To figure out the model's confidence, we look at how the confidence changes over time. Formal: We also use a log-linear model to estimate the model's confidence. Formal: We put all the model's predictions into one big graph, which we call the \"confidence\" graph. Formal: The confidence for the next word is calculated based on all the model's predictions. Formal: The confidence for a group of predictions is calculated based on the model's confidence. Formal: We use the log-linear model with a root-mean-square error (RMSE) to estimate the confidence for the next word. Formal: Finally, we combine all these confidence scores into one final confidence score, which we call \"pass\" for the whole model. Formal: Formal: Since the model's predictions are all based on the previous prediction, the model's confidence for a specific word depends on how confident it is about the previous prediction. Formal: Formal: For the QG model, we’re using a linear model (with a root-mean-square error) to estimate the confidence for the next word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-6008.json"
    },
    {
        "casual_text": "We owe a big thanks to the program committee and the secondary reviewers they brought in for some papers. With so many submissions this time, their workload was way bigger than usual. We’re super grateful for their help, especially since most of them managed to get everything done on time (though we did have to give a few gentle reminders after the deadline). We hope the reviewers’ feedback was helpful and constructive for everyone—whether it was to boost your chances for a future submission or to polish your paper if it got accepted. We know the timeline was tight, so thanks to all the authors for getting your final versions in on time.",
        "formal_text": "We are in debt with the members of the programme committee and with the secondary reviewers they appointed for some of their papers. As the number of papers received was unusually high, they had an unusually large workload: we especially thank them for their invaluable help, which most of them completed on time (yes, we had to twist some arms after the deadline). We hope that the reviewers' comments were useful and constructive and helped all authors: for those whose papers weren't accepted, by increasing their chance in a later submission somewhere else; for those whose papers got in, to improve their manuscripts. We know we didn't give them a lot of time to do so, and we thank authors for sending their camera-ready versions on time.",
        "GPT2_formal_text": "= {tokens, data} from a bunch of web documents. Formal: We used the same setup as in (S3) to figure out the word embeddings for the entities. Formal: We got the entity embeddings, the word embeddings, and the model's parameters from the training set of this dataset. Formal: We used the same setup as in (S1) to automatically generate the entity embeddings. Formal: We use the same model as in (S2) to check how well the model can handle different types of entities. Formal: In each sentence, we have a title, the entity types, the entity type tokens, and the entity embedding tokens. Formal: We built a special dataset for understanding how entities are connected, and we put it together with the knowledge base. Formal: We trained a model for handling different types of entities and created a dataset for understanding how entities are connected. Formal: We used the same dataset as in (S1) to train a model. Formal: We used a model that's been fine-tuned for entity recognition for this task. Formal: Formal: We used the same model as in (S2) to train the model. Formal: We used the same dataset as in (S1) to train the model. Formal: We used the same model as in (S1) to train the model. Formal: We used the same dataset as in (S1) to train the model. Formal: We used the same model as in (S1) to train the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2011.eamt-1.0.json"
    },
    {
        "casual_text": "In real-world use, we went with a bigram model because it was quick to implement and performed pretty well in our tests.",
        "formal_text": "2 In practice, we use a bigram model, which was fast enough to scale and worked sufficiently well in experiments.",
        "GPT2_formal_text": "ed: 1 = Yes, and 4 = No. This gives us a total of 657,929 possible word forms. Formal: We used the English Wiktionary 4 for our evaluation, which has around 2.4 million entries. Formal: We also got rid of any pages that didn’t have any results, except for the ones where the second-to-last row had more than one possible word form. Formal: Table 2 gives a quick summary of the results from the evaluation. Formal: Sure! Here’s the informal version:\n\n- **Document format**: 39.8K, 68.1K, 68.1K, 44.2K, 59.2K, 51.0K, 49.9K, 66.7K\n- **Word forms**: 2.4 million, 5.4 million, 4.1 million, 5.4 million, 4.1 million, 5.4 million, 2.1 million\n- **First-word-context**: 36.3K, 59.7K, 48.5K, 59.7K, 53.9K, 60.5K, 59.5K, 51.5K\n- **Second-word-context**: 39.8K, 60.2K, 48.5K, 59.2K, 56.5K, 60.4K, 59.5K, 51.5K\n- **Number of candidates**: 657,929 Formal: The \"Document format\" column shows the number of words in the document. Formal: The \"Word forms\" column lists the first-word-context and second-word-context word forms of each word in the document. Formal: The \"First-word-context\" column shows the first-word-context word forms. Formal: The \"Second-word-context\" column lists the second-word-context word forms. Formal: The \"Number of candidates\" column lists the number of words in the document. Formal: The \"First-word-context\" column shows the first-word-context word forms. Formal: The \"Second-word-context\" column lists the second-word-context word forms. Formal: The \"Number of candidates\" column lists the number of words in the document. Formal:\n\nWe used these metrics in an ablation study to see how well our generation",
        "directory": "D19",
        "filename": "D19-1432.json"
    },
    {
        "casual_text": "Let's break this down in simpler terms. First, let's call the previous memory graph G x−1 and the updated one Gx j. The R-GCN layer updates each entity's hidden state by looking at the hidden states of its neighbors, but it does this separately for each type of connection. After that, it adds up all these updates across different types and then runs the result through a GELU activation function (Hendrycks and Gimpel, 2016). To calculate the hidden state of an entity e j in the next layer (l + 1), we use a residual connection (He et al., 2016) to keep the original information of the entity, not just its neighbors, and then we apply layer normalization.",
        "formal_text": "denote the graph connectivity of the previous user memory graph G x−1 and N r, Gx j denote the updated graph. The R-GCN layer updates the hidden states of each entity with the incoming messages in the form of their neighbors' hidden states type-by-type. Then R-GCN sums over all types before passing through the GELU activation (Hendrycks and Gimpel, 2016). The hidden state of entity e j in the (l + 1)-th layer is computed via a residual connection (He et al. , 2016 ) (to keep the original entity information instead of just neighbors' information) and layer normalization.",
        "GPT2_formal_text": ". The sentence segmentation and tokenization are done by the pre-trained GPT-2 model. The attention mechanism is a bi-directional long short-term memory (Bi-LSTM) with a hidden layer size of 768, and it has a capacity of 128. The model is built with the Transformer architecture and was trained on the GPT-2 Medium dataset (Radford et al., 2019). Formal: The main model is built using Tensorflow (Abadi et al., 2016). Formal: The model is trained using the BERT-base model, which is a transformer-based model with a hidden layer size of 512. Formal: The model is built using OpenNMT (Raffel et al., 2019). Formal: The final model is trained using the Transformer model with an updated vocabulary of 160,000, with a hidden layer size of 768. Formal: In the last round of fine-tuning, the model is fine-tuned using the BERT-base model. Formal: This is the final fine-tuned model that combines the previous fine-tuned model with the improved one. Formal: The final model is trained using OpenNMT with the extended vocabulary size of 160,000, with a hidden layer size of 768. Formal: The model is trained using OpenNMT with the extended vocabulary size of 160,000, with a hidden layer size of 768. Formal: The model is trained using OpenNMT with the extended vocabulary size of 160,000, with a hidden layer size of 768. Formal: The model is fine-tuned using Tensorflow with the updated vocabulary of 160,000, with a hidden layer size of 512. Formal: The model is fine-tuned using Tensorflow with the updated vocabulary of 160,000, with a hidden layer size of 768. Formal: The model is fine-tuned using Tensorflow with the updated vocabulary of 160,000, with a hidden layer size of 512. Formal: The model is fine-tuned using Tensorflow with the updated vocabulary of 160,000, with a hidden layer size of 512. Formal: The model is fine-tuned using Tensorflow with the updated vocabulary of 160,000, with a hidden layer size of 512. Formal: The model is fine-tuned using Tensorflow with the updated vocabulary of 160,000, with a hidden layer size of",
        "directory": "coling",
        "filename": "2020.coling-main.463.json"
    },
    {
        "casual_text": "Imitation Learning is all about learning skills by watching demos. It's shown some cool potential in areas like fixing the exposure bias issue in structured prediction (Zhang et al., 2019b), helping non-autoregressive translation models by sharing knowledge (Gu et al., 2018; Wei et al., 2019), and even teaching dialogue systems how to reward themselves (Li et al., 2019b). In our project, we have a regular dialogue model acting as a student, trying to copy a scenario-based dialogue model, not just in the final output but also in the middle layers.",
        "formal_text": "Imitation Learning Imitation learning, acquiring skills from observing demonstrations, has proven to be promising in structured prediction, such as alleviating the exposure bias problem Zhang et al. , 2019b), transferring knowledge to guide non-autoregressive translation model (Gu et al. , 2018; Wei et al. , 2019), and automatically learning the reward of the dialogue system (Li et al. , 2019b). In our work, the conventional dialogue model as a student mimics the scenariobased dialogue model on both the output layer and intermediate layers.",
        "GPT2_formal_text": "ing_k) = f_k, p_k;\n            Casual: 1 2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22 23 24 25 26 27 28 29 30 31 32 33 34 35 36 37 38 39 40 41 42 43 44 45 46 47 48 49 50 51 52 53 54 55 56 57 58 59 60 61 62 63 64 65 66 67 68 69 70 71 72 73 74 75 76 77 78 79 80 81 82 83 84 85 86 87 88 89 90 91 92 93 94 95 96 97 98 99 100\n\nD is just a diagonal matrix with dimensions n by K, so it has n^n rows and n^(K-1) columns. The output is basically a single number, which is the average of all the hidden states in that specific layer. Formal: We're using the greedy algorithm (thanks to Dempster et al. for that idea back in 1954) to figure out the probability p_k for each word. Formal: The score for the token `t` is calculated like this: Formal: We use the absolute difference between the maximum and minimum values of p_k to calculate the score, which gives us a score between 0 and 1. Formal: Finally, we average these scores to get the score for each word. Formal: For each token `t`, we look at the word`s hidden state from the previous layer (`h`) and its current hidden state (`w`) to get its score. Formal: Formal: The token `t` is made up of a fixed number of words, w, which can be either 1 or more. Formal: Formal: Each word `t` is made up of a fixed number of tokens, W, which can be either 1 or more. Formal: Formal: To calculate the attention for the token `t`, we first add up the attention score `t_c` for each token `t` in the vocabulary V_t`. Formal: Formal: Formal: Then, we use this attention score along with the hidden state `h` from the previous layer to get the attention for the token `t`. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.534.json"
    },
    {
        "casual_text": "The N-of-AD. I algorithm, which you can find explained in section 6, is a function that helps figure out the main noun, with a certain property P, that an adjective is talking about. It's written in a special language used for defining lexicons and is also built into the lexicon system as a handy tool. When rules use this function, they can make the translation of an adjective depend on the noun phrase it modifies having specific traits. Oh, and by the way, the N-of-ADJ function can also find the noun when the adjective is used as a pronoun-like modifier or when it's in the VP predicative form.",
        "formal_text": "The algorithm N-of-AD. I, given in detail in section 6 below, characterizes a parameterized function which identifies the head noun argument, with property P, of a given adjective. This algorithm is written in the lexicon specification language and is also included as a utility function in the lexicon system. Rules which employ this function can render the translation of an adjective dependent upon the modified noun phrase having specific attributes. It should also be noted that the function N-of-ADJ identifies the noun when the adjective acts as a pronominal modifier as well as when it appears in the VP predicative form.",
        "GPT2_formal_text": "\": [ { \"position\": { \"type\": \"BLOCK_POSITION\", \"values\": [ 0, 1, 2, ..., 4, 8, 16 ] }, \"num_of_entities\": [ 0, 1, 2 ] } ], \"num_of_entities_per_token\": [ 0, 2, 4, 8, 16, 32, 64 ] } ], \"num_of_entities_in_token\": [ [ 0, 0.5, 1, 0.5, 1, 0.5, 0.5 ] ] } ], \"entities\": [ { \"type\": \"BLOCK\", \"values\": [ { \"position\": { \"type\": \"BLOCK_POSITION\", \"values\": [ 0, 1, 2, ..., 4, 8, 16 ] }, \"num_of_entities\": [ 0, 1, 2 ] } ], \"num_of_entities_per_token\": [ [ 0, 2, 4, 8, 16, 32, 64 ] } ], \"entities_per_token\": [ [ 0, 2, 4, 8, 16, 32, 64 ] ] } ], \"num_of_entities_in_sentence\": [ [ 0, 0.5, 1, 0.5, 1, 0.5, 0.5 ] ] } ], \"sentences\": [ { \"length\": [ 512, 4096, 4096 ] }, { \"length\": [ 512, 1024, 1024 ] }, { \"length\": [ 1024, 8192, 8192 ] }, { \"length\": [ 8192, 4096, 4096 ] }, { \"length\": [ 8192, 1024, 1024 ] } ], \"r_sub\": [ [ 0, 0.5, 1, 0.5, 1, 0.5, 0.5 ] ] } ] Formal: In the past few years, there's been a lot of progress in creating text collections using big datasets. This has led to the creation of datasets like the one from S2ORC (Hendrickx et al., 2010). We’re using this dataset for our experiments. Formal: The dataset is split into two parts: the first part has 50,000 sentence pairs and the second has 25,000 sentence pairs. The size of this split is based on the number of sentence pairs in each dataset. Formal: Each sentence pair is tagged with the length of the sentence and has",
        "directory": "C88",
        "filename": "C88-1042.json"
    },
    {
        "casual_text": "We made these tables by running Anymalign for the same amount of time in every setup. That's why bigger length parameter values result in smaller tables—check out the specifics in (Lardilleux et al., 2011b).",
        "formal_text": "These tables were produced by running Anymalign for an identical amount of time in all configurations, which explains why larger values of the length parameter lead to smaller tables-see details in(Lardilleux et al. , 2011b).",
        "GPT2_formal_text": "ed: This is an extra step that helps make sure the masked language model (MLM) is trained on a part of the source domain that doesn't have any adversarial examples. The idea is that the MLM should work on the target domain but be less affected by the noise in the source domain. So, we create a special target domain called the \"null domain.\" Formal: This part is similar to the first one, but instead of using a raw input, it uses the input from the MLM. Formal: But for the second stage, we switch things around and use a negative log-likelihood loss. Formal: In the first stage, we use the masked language model to create the domain for the MLM. Formal: Instead of using the original target domain, we switch to the null domain by using the MLM, which creates a negative log-likelihood loss. Formal: The fourth stage lets the MLM learn from the original target domain. Formal: The last two stages are done in a way that allows the MLM to learn from the target domain. Formal: We train the MLM on the source domain, then use a masked language model on the target domain, and finally use the target domain as the \"null\" domain to test the MLM. Formal: The final result is a final probability distribution for the masked language model. Formal: Formal: We train the MLM with the source domain, then use a masked language model on the target domain, and finally use the target domain as the \"null\" domain to test the MLM. Formal: After that, we calculate the masked language model's probability distribution using a method called log-likelihood ratio. Formal: We use log-likelihood ratio to get the final probability distribution, which is shown in Figure 1. Formal: The log-likelihood ratio helps us see how much the input is influenced by the target domain. Formal: Formal: The masked language model's probability is calculated based on the loss we got in the first two stages, but it doesn't mess with the original target domain's probability. Formal: Formal: Formal: The value in the log-likelihood ratio is proportional to the log-likelihood ratio of the source domain, and it's also proportional to the log-likelihood ratio of the target domain. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2012.eamt-1.62.json"
    },
    {
        "casual_text": "Here, π represents the chance that the community will switch emotions between two time periods, t and t + 1. The letter b stands for the total number of times the emotion state changes across all the time intervals from 1 to T.",
        "formal_text": "where π is the probability the community will change the emotion state in two consecutive time intervals t and t + 1. b denotes the number of emotion state changes in the whole time intervals [1, T ].",
        "GPT2_formal_text": "_t) = (v_t * a_t + w_t) * (v_t * a_t + b_t) * (v_t * a_t + c_t) * (v_t * a_t + e_t) * (v_t * a_t + f_t) * (v_t * a_t + g_t) * (v_t * a_t + h_t) * (v_t * a_t + i_t) * (v_t * a_t + j_t). Formal: Basically, all these possible answer sequences are made up of all the words in the passage, plus the time and place where the question or answer is found. Formal: We can use a bunch of methods to figure out all the possible answer sequences from a passage. For each of these, we pick the one with the highest similarity score. Formal: For any question (v_t), the phrase (v_t) in the passage is considered the correct answer. Formal: The important thing is that the answer to a question can be figured out from any possible answer sequence in a passage. Formal: In Figure 1, we show how we use a retrieval-based approach to find the best answer from the question. Formal: We're looking for the top answer in the question, where \"best\" is the highest similarity score from the answer candidates. Formal: This method is based on the work of Bordes et al. (2009). Formal: We use the maximum-likelihood method (MLE) to find the best answer from the question, using the answer candidates as features. Formal: We also take a look at a different retrieval-based method called the lowest-rank retrieval (LR) method. Formal: We need to find the best answer for the question, where \"best\" is the highest similarity score from the answer candidates. Formal: We want to find the best answer for the question, where \"best\" is the highest similarity score from the answer candidates. Formal: Formal: In Figure 2, we explain how we use two different methods to find the answer. Formal: The answers to the questions are found using a maximum-entropy (max-ent) method. Formal: Formal: The answers to the questions are found using a maximum-entropy (max-ent) method. Form",
        "directory": "D14",
        "filename": "D14-1123.json"
    },
    {
        "casual_text": "SST-2, also known as the Stanford Sentiment Treebank, has around 70,000 sentences tagged as either positive or negative (thanks to Socher et al. in 2013). We took the pre-trained RoBERTa base model and tweaked it for this sentiment classification task. As a result, we got an accuracy of 94.5 on the development set.",
        "formal_text": "SST-2 The Stanford Sentiment Treebank (Socher et al. , 2013) contains 70k sentences labeled with positive or negative sentiment. We fine-tune the pre-trained RoBERTa base to the classification task and achieve an accuracy of 94. 5 on the dev set.",
        "GPT2_formal_text": "_tgt (AttnTgt) ) = LSTM(AttnTgt) - attention (AttnTgt) + softmax (AttnTgt) + l2tog (AttnTgt) [B] Formal: Basically, this part of the model helps create a memory for the current time step, called `tgt_time`. It does this by combining the hidden state from the time step `tgt_t` (or `tgt` itself) and the attention vector `AttnTgt`. Then, it averages all those together using a specific function called `linear` to get the overall memory representation. Formal: This part of the model is trained using a process called hierarchical self-attention. Formal: For the final layer of the model, it figures out the attention weights for all the entities in the attention space by using the same function `linear`. Formal: The final layer in the model is trained to figure out the attention weights for all the entities in the attention space. Formal: The model decides how important each entity is based on how similar their embeddings are. Formal: The model uses the embeddings of all the entities in the attention space to rank them. Formal: Formal: The model uses the embeddings of all the entities in the attention space to rank them. Formal: Formal: Lastly, the model uses the embeddings of all the entities in the attention space to rank them. Formal: Formal: Finally, the model uses the embeddings of all the entities in the attention space to rank them. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-srw.16.json"
    },
    {
        "casual_text": "Lemmatization is all about figuring out the base form (lemma) of a word from its inflected form. For example, turning \"atmest\" into \"atmen.\" This process is pretty straightforward for languages with simple grammar, like English, but it gets way more complicated with languages like Finnish. Essentially, it's like doing the opposite of what happens when you add endings to words to change their form. This idea has been talked about in a few studies (Durrett and DeNero, 2013; Ahlberg et al., 2014; Nicolai et al., 2015; Faruqui et al., 2016), where they look at how you can generate an inflected word by starting with the base form and adding the right ending.",
        "formal_text": "Lemmatization is the task of deriving the lemma from an inflected word form such as atmest→atmen. The problem is relatively simple for morphologically poor languages like English, but much harder for languages like Finnish. The task can be seen as the inverse to inflection generation (Durrett and DeNero, 2013; Ahlberg et al. , 2014; Nicolai et al. , 2015; Faruqui et al. , 2016), where an inflected form is generated from a lemma plus an inflection tag.",
        "GPT2_formal_text": "= [ ] . . . | | p = arg_r_p(w) | p = arg_r_s(w) | p = arg_r_v(w) | p = arg_r_f(w) | | p = arg_r_t(w) | p = arg_r_n(w) | p = arg_r_f(v) | p = arg_r_v(v) | p = arg_r_s(v) | p = arg_r_v(v) | p = arg_r_f(s) | p = arg_r_v(s) | | p = arg_r_p(s) | p = arg_r_v(s) | p = arg_r_f(v) | p = arg_r_t(s) | p = arg_r_v(v) | p = arg_r_t(v) | p = arg_r_v(v) | p = arg_r_s(v) | p = arg_r_f(s) | p = arg_r_t(v) | p = arg_r_v(v) | p = arg_r_s(v) | | p = arg_r_p(s) | p = arg_r_v(s) | p = arg_r_t(s) | p = arg_r_v(v) | p = arg_r_s(v) | p = arg_r_f(s) | p = arg_r_t(v) | p = arg_r_v(v) | p = arg_r_s(v) | | p = arg_r_p(v) | p = arg_r_v(v) | p = arg_r_s(v) | p = arg_r_t(v) | p = arg_r_v(v) | p = arg_r_s(v) | p = arg_r_f(v) | p = arg_r_t(v) | p = arg_r_v(v) | p = arg_r_s(v) | | p = arg_r_p(v) | p = arg_r_v(v) | p = arg_",
        "directory": "C16",
        "filename": "C16-1160.json"
    },
    {
        "casual_text": "Collapsed sampling tends to take a long time to get to the right answer. This is a big issue for us, especially since we start with a segmentation provided by a morphological analyzer. The analyzer works by splitting text based on fixed rules, so the initial segmentation it gives is pretty consistent. However, this means that errors, especially with unknown words, happen in a predictable way. Basically, we start off close to the best possible solution, but not quite there. The problem is that the collapsed Gibbs sampler can get stuck in this not-quite-right starting point. That's why we often just randomly pick the initial segmentation instead. Sentence-based block sampling can also get stuck if it starts with a consistent segmentation (Liang et al., 2010).",
        "formal_text": "Collapsed sampling is known for slow convergence. This property is especially problematic in our settings where the initial segmentation is given by a morphological analyzer. Since the analyzer deterministically segments text using pre-defined parameters, the resultant segmentation is fairly consistent. Segmentation errors involving unknown words also occur in a regular way. Intuitively, we start with a local optimum although it is not too distant from the global optimum. The collapsed Gibbs sampler is easily entrapped by this local optimum. For this reason, the initial segmentation is usually chosen at random. Sentence-based block sampling is also susceptible to consistent initialization (Liang et al. , 2010).",
        "GPT2_formal_text": "ing/9) is what we're dealing with here. Basically, we get the embedding of the word w_i in the context C_w_i, and it's calculated by... Formal: For the input text X, the attention vector for the word w_i is... Formal: We start by using something called max pooling (Max) to get an embedding for the word w_i in the context C_w_i, which we call... Formal: The word embedding for the context C_w_i is calculated like this: Formal: Let's say c_w_i is the vector for the word w_i in the context C_w_i. Then, for a given c_w_i, the attention vector... Formal: The attention vector for the context C_w_i is calculated by: Formal: Let's say C_w_i is the context of the word w_i. Then, the attention vector... Formal: The attention vector for the context C_w_i is also called... Formal: For the input text X, the attention vector for the word w_i is... Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D11",
        "filename": "D11-1056.json"
    },
    {
        "casual_text": "2. Updating an entity-event database: Besides D (test), let’s say we also have access to a historical database of killings E (train) and a collection of news articles D (train) from before the time T. This kind of setup is pretty common in real-world scenarios and is what this paper is mainly about. It lets us use distantly supervised learning methods, which is pretty cool.  \n\nThe task itself is really important for society, but the NLP research community might also want a more scientific reason to care. We think police fatalities are a great test case for event extraction research. Fatalities are a specific type of event with clear meanings, so they avoid some of the trickier issues in this field (Hovy et al., 2013). Plus, this task ties into a lot of existing research on building knowledge bases (like Craven et al. (1998)).  \n\nLastly, we believe that NLP should, whenever possible, work on applications that matter to the public. Previous research showed that news text is valuable for this problem, but using computational methods could make it way less labor-intensive than doing everything manually.",
        "formal_text": "2. Updating an entity-event database: In addition to D (test), assume access to both a historical database of killings E (train) and a historical news corpus D (train) for events that occurred before T. This setting often occurs in practice, and is the focus of this paper; it allows for the use of distantly supervised learn-ing methods. 2 The task itself has important social value, but the NLP research community may be interested in a scientific justification as well. We propose that police fatalities are a useful test case for event extraction research. Fatalities are a well defined type of event with clear semantics for coreference, avoiding some of the more complex issues in this area (Hovy et al. , 2013). The task also builds on a considerable information extraction literature on knowledge base population (e. g. Craven et al. (1998) ). Finally, we posit that the field of natural language processing should, when possible, advance applications of important public interest. Previous work established the value of textual news for this problem, but computational methods could alleviate the scale of manual labor needed to use it.",
        "GPT2_formal_text": "\" function in the classifier to create the features for the classifier. Formal: The topic and aspect vectors are generated using something called cross-entropy loss. Formal: We use a logistic regression classifier to figure out the relevance score, which we call the relevance score r(t|s). Formal: After getting the topic and aspect vectors, we combine them to get the relevance score r(s|t). Formal: Finally, the relevance score r(s|t) is the average of the relevance score for each pair of documents s and t. Formal: The relevance score r(s|t) is the average of the relevance score for each document s and document t. Formal: Table 1 gives the accuracy for the first-order and second-order topic and aspect vectors. Formal: The accuracy for the first-order topic and aspect vectors is 8% and 18% for the first and second orders, respectively. Formal: The accuracy for the first-order topic and aspect vectors is 16% and 20% for the first and second orders, respectively. Formal: For the first and second orders, we show the F1-score for the top-ranked documents, the top-ranked sentences, and the top-ranked words. Formal: For the first and second orders, the F1-score for the top-ranked documents, the top-ranked sentences, and the top-ranked words is 15%, 29%, and 23%, respectively. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1163.json"
    },
    {
        "casual_text": "There are two situations where COs (Content Objects) might change, but only one of them is actually a big deal. The other one is pretty straightforward and not really an issue. The easy case happens when a resource gets updated, like when it has more annotations or fixes some errors, but the old version stays available. In this situation, the long-term archive just creates a new version of the CO, and the old one stays as it is, still accessible.\n\nNow, there’s a bit of a conflict here. On one hand, users might find it useful to know there’s a new version of a CO. On the other hand, updating the metadata to reflect this new version would mess with the whole idea of long-term archival, which is supposed to keep things stable. We think the second point is way more important: there’s no need to link to the new version in the long-term archive or change the metadata. \n\nThat said, it’s totally fine, from a long-term archival standpoint, to link from the new version to the old ones—as long as the old versions are archived first, which usually happens. To make things easier for users, the archive’s presentation layer can flip these links around without actually changing the metadata itself.",
        "formal_text": "There are two cases in which COs change, only one of which constitutes a veritable challenge. The other one can be seen as unproblematic. The unproblematic case arises when a resource is published in a new version, e. g. containing more annotations, but also correcting mistakes that will stay accessible in the previous version. The long-term archive will in this case simply issue a new version of the CO and the old version stays intact and accessible. There is a certain conflict of interests here: On the one hand, it may be interesting to users to see that there is a new version of an CO; on the other hand, integrating this information into the archive would most evidently be possible changing the metadata, a measure which evidently goes against the general guarantees of long-term archival. We suggest that in this case the latter point far outweighs the former: It is not necessary to point to the new version in the long-term archive and make changes to metadata. However, it is by perfectly admissible from an LTA perspective to point to old versions from the new ones -as long as the latter are archived after the former have been, as is normally the case. To improve usability, the presentation layer of the archive can invert these links without integrating them into the metadata proper.",
        "GPT2_formal_text": "_len = 200;\n       Casual: We're checking out how the training data affects the model's performance. We're using the Hugging Face Transformers library (Wolf et al., 2020) along with Scikit-learn (Pedregosa et al., 2011). We set the dropout rate to 0.2 to make sure the model doesn't get stuck in a local optimum. We train for two epochs using a learning rate of 0.0001 and a batch size of 32. We also use Adam (Kingma and Ba, 2014) with an initial learning rate of 0.00001, a warmup step of 2, and a patience of 3. Formal: To see how the model handles different inputs, we train it on a random sample of 200 tweets (D = 200) and then check how it does on the test set (D = 200) using the Deviation Index (DI). Formal: Check out Figure 3 for the results of the Deviation Index (DI) on the test set. Our model (D = 200) outperforms the others in all cases, except for the Deviation Index on the test set, which is a bit lower (0.003). Formal: For the visualization, we use a 300-dimensional vector to show the Deviation Index (DI) for the test set. Formal: For each token (x_t), we calculate the Deviation Index (DI) for the test set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "cmlc",
        "filename": "2020.cmlc-1.1.json"
    },
    {
        "casual_text": "Okay, so here's the deal with how we define an \"effective question.\" It all revolves around this thing called the reference set, which we call RS. For every question q t, we figure out what RS is. \n\nAt the very beginning, before any conversation starts, RS(q 0 ) includes every single object in the image. Basically, it’s a list of all the objects that are in the dataset and were given to the Oracle model. But here’s the thing: the human Oracles didn’t get this list. \n\nNow, as the conversation goes on, at each turn t, RS(q t ) is made up of the objects that were in RS(q t−1 ) and have the same answer A for question q t as the one for the referent r. All these answers A are calculated using the Oracle we have, which is 79% accurate on the test set. So, in short:",
        "formal_text": "Our definition of effective question is based on the set of candidate objects: the reference set RS. We compute RS for each question q t. The reference set before the dialogue starts, RS(q 0 ), contains all the objects in the image. That is, it contains the list of objects annotated in the dataset and given to the Oracle model. Human Oracles did not have access to this list. At each dialogue turn t, RS(q t ) is the set of objects in RS(q t−1 ) such that the answer A to q t on those objects is the same than the answer to q t on the referent r. All answers A are computed using the Oracle proposed in whose accuracy on the test set is 79%. Formally:",
        "GPT2_formal_text": "= [t1, t2, ..., tk], and the answer_ids = [a1, a2, ..., an]. Formal: [t] * (T_s * R_t) = [a1, ..., ak] (1) Formal: [a1, ..., an] * (T_s * R_a1) = [a1, ..., an] (2) Formal: [a1, ..., an] * (T_s * R_a1) = [a1, ..., an] (3) Formal: The final logits for the question and the answer are calculated like this: Formal: f_q(Q_s) + f_a1(A_s) + ... + f_k(R_q) Formal: f_a2(A_s) + ... + f_k(R_a2) Formal: The whole process of creating questions is shown in Figure 1. It starts with the question model, which we'll call M = {m_1, ..., m_n}, and the query model, which we'll call Q = {q_1, ..., q_n}. Formal: (a) M = {m_1, ..., m_n} + (b) Q = {q_1, ..., q_n} + (c) M_q = {m_q1, ..., m_qn} + (d) Q_q = {q_q1, ..., q_qn} + (e) Formal: The model generates a bunch of different questions based on these different parts. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "alvr",
        "filename": "2020.alvr-1.4.json"
    },
    {
        "casual_text": "Like other pre-training models (like UNILM, BERT, and XLM), we also use the multi-layer Transformer (Vaswani et al., 2017) as the main tool to encode sentences and get contextual representations for each word in a sentence. The multi-layer Transformer works by:",
        "formal_text": "Sentence Encoder As previous pre-training models (UNILM, BERT, and XLM), we also apply the multi-layer Transformer (Vaswani et al. , 2017) as the basic sentence encoder for obtaining the contextual representations h i for each token in the sentence s. The output of multi-layer Transformer is computed via:",
        "GPT2_formal_text": "ing_tweaks = {mask_1, mask_2, ..., mask_T}, for some examples. The whole setup is laid out in Figure 1. The main idea behind sentence-level attention is to bring in attention to the parts of a sentence that are related to the query. In our approach, we use a GRU to do this. To make the attention work, we treat the query and the word embedding as a single vector, kind of like a feature vector, and then calculate the attention values for the sentence embedding and the word embedding separately. This setup is similar to what Song and others did in 2019. Formal: The whole attention setup for multi-label classification can be broken down into three parts: Formal: First, we create attention for the query and word embeddings separately, using a CNN with a GRU layer. Then, we calculate the attention values for the sentence and word embeddings separately. Formal: Second, we use the same CNN to combine the query and word embedding into one vector, kind of like combining features. Formal: Third, we calculate the attention values for the sentence and word embeddings as a single function, using a linear layer. The formula is L(δ_Q, δ_W, δ_S) = [L(Q_T, S_T), L(W_T, S_T)] = [L(Q_T, S_T), L(W_T, S_T)] Formal: The final attention weights for the sentence and word embeddings are calculated by combining them with the query and word embedding vectors. Formal: Formal: When it comes to multi-class classification, we set the number of classes to T = 2. So, if we're training a classifier, the total number of parameters in the model is 2 T = 2. Formal: The model takes an input sentence X and a label y, and turns it into a multi-label classification problem with T classes. Formal: We use an LSTM to do multi-label classification. Formal: For the first layer, we apply a negative log-likelihood loss function to the input. Formal: The second layer uses a single-layer LSTM to predict the label y. Formal: The third layer uses a bi-LSTM to predict the label y. Formal: The final output from the third layer is",
        "directory": "eacl",
        "filename": "2021.eacl-main.251.json"
    },
    {
        "casual_text": "Okay, so let's break this down in simpler terms. W represents the n-gram, which is just a fancy way of saying a sequence of words. The function f(.) counts how many times something happens, and f_match(.) looks at the biggest number of n-grams that show up in both the summary you made (let's call it C) and the real, expert-made summaries (we'll call those S). \n\nNow, if your summary C has n words and the gold summary S has u sentences, ROUGE-L is a way to measure how well your summary matches up with the real one.",
        "formal_text": "Where W is the n-gram, f (. ) is the count function, f match (. ) is the maximum number of n-grams cooccurring in the generated summary and in a set of gold summaries. For a candidate summary C with n words and a gold summary S with u sentences, ROUGE-L is defined as follows:",
        "GPT2_formal_text": "= [s, t]. Formal: To keep the same kind of search results, we also group similar documents together in one group, M, which we call S. Formal: You can find more details about this in (Fukunaga et al., 2006). Formal: We use a binary search with a beam size of 10 to find the best match. Formal: We found that these beam search methods are the best way to find documents that match the topic we're looking at. We tested this on two different datasets and got the results you can see in Figure 1. Formal: If the query is a name, the questions will ask the user to pick a country and a language. If the query is a location, the questions will ask the user to pick a city and a state. Formal: After running the query, we pick the best match by using a log-linear model. Formal: We tested this by taking the average of how well the matching documents performed. Formal: We found that using these models to search for documents with similar topics is usually better than just relying on the community. Formal: We’ve included some stats on how well the matching process works in Figure 1. Formal: The matching results are shown in Figure 1. Formal: The results of the log-linear model for each document are shown in Figure 2. Formal: The best match for the document we’re looking at is the one in the top left corner, with a score of 0. Formal: Finally, if no match is found, we’ll leave the document empty. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D15",
        "filename": "D15-1045.json"
    },
    {
        "casual_text": "Paraphrasing has been around for a while and has a bunch of uses in different areas of natural language processing, like summarizing text (Cao et al., 2016), understanding sentences (Berant and Liang, 2014), and answering questions (Yu et al., 2018). Back in the day, people mostly used rule-based methods or statistical machine translation systems to create paraphrases (McKeown, 1980; Meteer and Shaked, 1988; Bannard and Callison-Burch, 2005).",
        "formal_text": "Paraphrase generation has been a long-standing task that has several applications on downstream NLP tasks including text summarization (Cao et al. , 2016), semantic parsing (Berant and Liang, 2014), and question answering (Yu et al. , 2018). Early works on paraphrase generation mostly rely on rule-based or statistical machine translation systems (McKeown, 1980; Meteer and Shaked, 1988; Bannard and Callison-Burch, 2005).",
        "GPT2_formal_text": "_type_layers = { 1, 2, 3 }          Casual: You can find all the NMT models and their stats in Table 1. Formal: We took the VQAv2 dataset (Voss et al., 2019) and randomly picked 6.7 million sentences for each language to train a baseline model. Then, we trained a cross-lingual model using the same sentences. The models were trained on batches of 50 sentences each, with each batch containing 200 sentences. Formal: We also threw in a test set of 500 sentences from the same batch to see how well our model could translate between the languages. Formal: Finally, we tested our model on the Gigawords dataset (Papineni et al., 2002), which has sentences in three languages: English, German, and French. Formal: To show how much better our model performs, we ran some tests on the Gigawords test set and the original VQAv2 dataset to see how well it did. Formal: In Table 3, you can see some examples of sentence translations between the languages. Formal: For the experiments in this paper, we trained our model using both the VQAv2 and Gigawords datasets for 10 epochs. Formal: The results are in Table 4. Formal: Our model got an F1 score of 0.99 on the Gigawords test set, which is 0.58 points better than the best baseline system. Formal: This means our model did better than the baseline system in all three languages, but we didn't get any extra points for the languages we didn't have any data for. Formal: From these results, it's clear that our model's performance improvement was more noticeable for languages we didn't have any data for. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.417.json"
    },
    {
        "casual_text": "But when we throw the LM score into the mix, things get a bit more complicated. The total cost of a derivation isn't just the sum of the costs from the previous hypernodes and the grammar rule anymore. Now, we need to tack on the cost from the language model, which is calculated based on the parts of the target language we're dealing with. This LM score is called a \"combination cost\" because it impacts how we combine hypernodes. \n\nThe problem is, this score is kind of a pain to calculate—it depends on everything involved in the combination (the predecessor hypernodes and the translation rule). And because of that, the sorting strategy we talked about earlier can't guarantee that we'll always generate derivations in a nice, neat, monotonic order.",
        "formal_text": "When including the LM score, however, the situation is different. The costs of the derivation are no longer simply the sum of the corresponding derivations in the predecessor hypernodes plus the cost of the grammar rule. Now we have to add the cost of the language model computed on the associated target language parts. The language model score is called a combination cost, as it is a cost that affects the combination of hypernodes. This score is costly to compute and dependent on all elements participating in the combination (predecessor hypernodes and translation rule). The effect of this is that the sorting strategy referred to above cannot longer guarantee that the generation of the derivations in an hypernode will proceed in a monotonic order.",
        "GPT2_formal_text": "= (s_i, j, k) ∈ R k×d_i. Formal: To make sure our model does a good job at telling apart entities, we train it using both the original and the generated training data, aiming to minimize the cross-entropy loss. Formal: The model uses different parts for the prediction (like random number seeds, max-pooling, or linear layers) and the training data. To keep the model from getting too stuck on certain words, we use a loss function based on probability. Formal: We also try out different ways to predict the entities. If a classifier doesn’t get it right, we just toss it out and try another one. Formal: Lastly, we tweak the model by making it better by using the generated training data. Formal: We use the random number generator θ (from a Gaussian distribution) for generating words and the nonlinear activation function β to tweak the model’s parameters. Formal: We check how well our model performs using the F-measure. Formal: We’ve got three types of features: gold entity labels (GEL), entity-type labels (ET), and semantic types (ST). Formal: We also have different ways to figure out the entities. Formal: We’re using two ways to pick these labels: (1) a greedy search, where we look at all the possible label combinations and choose the one that gives us the highest score (this is the jack-of-all-trades approach), and (2) a binary search, where we only pick the entity type that has the highest score. Formal: We’ve got two types of models, GEL and ET, and we train them using the KL divergence. Formal: We train them using the validation set and pick the best one from the list of best ones. Formal: For the final model, we use the gold label predictions (GEL) and the entity-type predictions (ET). Formal: We add a bias term to the entity-type labels (ET). Formal: We also create a Bias Function to make sure our model doesn’t just learn random word options for the entity-type labels. Formal: We use an LSTM (a recurrent neural network) to predict the entity types. Formal: The loss function we use is a binary version of the KL divergence, so it looks like this:",
        "directory": "eamt",
        "filename": "2009.eamt-1.33.json"
    },
    {
        "casual_text": "Machine reading comprehension (MRC) is all about finding the right answers in a passage based on a specific question (Devlin et al., 2019; Wen et al., 2021). In recent years, turning NLP tasks into MRC tasks has become pretty popular. For example, things like dependency parsing (Gan et al., 2021), relation extraction (Levy et al., 2017), named entity recognition (Li et al., 2020), and sentiment analysis (Chen et al., 2021; Mao et al., 2021) have all been approached this way. Unlike those earlier studies, we’re using an MRC framework to dig into the complex argumentative connections between two super-long documents.",
        "formal_text": "Machine reading comprehension (MRC) aims to extract answer spans from a passage according to a given query Devlin et al. , 2019; Wen et al. , 2021). Formulating NLP tasks as MRC tasks has been a rising trend in recent years, such as dependency parsing (Gan et al. , 2021), relation extraction (Levy et al. , 2017), named entity recognition (Li et al. , 2020), sentiment analysis (Chen et al. , 2021; Mao et al. , 2021). Unlike previous studies above, we employ a MRC framework to analyze the complex argumentative relations between two documents with excessively long length.",
        "GPT2_formal_text": "_layers = (d_q, f_mask, g_mask) ∈ R n×d. Formal: In the LM setup, for each source word x i, the context word that comes before it is also considered a source word. So, x i = x 1, . . . , x m. The vector representation for this source word is called S. Formal: So, S is basically a list of vectors, like [s_1, . . . , s_n] that represent the source words in the sentence x i. Formal: Here, S represents all the source words in the sentence, and n is the total number of words in the sentence. Formal: For context words, the context vector is defined as e_c. Formal: In the context vector for source words, e_c is the [e_1, . . . , e_m] vector that represents the context words in the source sentence x i. Formal: Finally, the attention vector for the source word x i is e_m. Formal: Formal: The final representation for the source word x i is calculated by averaging all the vectors from the source words in the sentence, like this: Formal: The vector for the target word y i is also written as e_i. Formal: Formal: The attention vector for the target word y i is calculated by combining all the vectors from the target words in the sentence, like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-short.4.json"
    },
    {
        "casual_text": "But this feature doesn't really help us pick between a doctor and a surgeon, because surgeons still study medicine too. If we run the experiment again, but this time focus on surgeons, the top result switches to \"patients died.\" This bigram seems to be a better way to tell the difference between those two groups in the BIOS model we trained.",
        "formal_text": "However, this is not a particularly useful feature to favor physician rather than surgeon, since surgeons also entail medicine studies; when we repeat the experiment in contrast to surgeon, the top highlight changes to \"patients died\", indicating that this bigram is a better differentiator for those classes in the trained BIOS model.",
        "GPT2_formal_text": "` and `text_ids` respectively. We can create these embeddings directly, but if the current document has multiple sentences, we'll need to do it in batches, so we'll use something called multi-task learning. The parameters for the network are laid out in Equations 1 and 2. Formal: As mentioned in section 3, the model is trained to generate captions that both highlight the aspect and the object. The loss for this task is calculated as l_ad = log p(s_adj_m | e_m) * log p(s_adj_m | e_j). Here, p(s_adj_m) is the probability of generating the aspect-object pair, and p(s_adj_m | e_j) is the probability of generating the whole caption. The main goal here is to maximize this loss function. Formal: Also, the task of generating captions is modeled as a sequence generation problem with a reward function. In this setup, each sentence in the document is treated as a separate query, and each response is created by combining the last response from a model trained on a dataset D with the query. Formal: We use a transformer-based model (Vaswani et al., 2017) as our generator. It gets parameters φ_c, φ_m, φ_n, and a set of softmax outputs. Formal: To make sure our generator is good, we check the attention scores for the source document d using an attention-based method. Formal: To make sure the response is accurate, we use a selective attention method (Zhang et al., 2018a) on the source documents. Formal: Formal: In our experiments, we show how important it is to include aspects and objects in both the source and target documents. Formal: Since we're focusing on generating captions, we only consider the previous response. Formal: We use a straightforward linear-chain CRF for generating responses, as proposed by Kim et al. (2017). Formal: We use a Transformer-based model as our generator. Formal: The training for this task starts by setting up a CRF layer on a bunch of documents D. Formal: In the domain of the document, we start by training a simple linear-chain CRF. Formal: Formal: The dataset D is made up of document sets D_i, which is the i-th",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.120.json"
    },
    {
        "casual_text": "Pronouns are kind of a big deal in sentences because they help writers spice up their writing with a bigger vocabulary and make more complex sentences. Basically, pronouns are like stand-ins in the text—they can take the place of a subject or an object, show ownership, point to places, or even refer back to people or things mentioned earlier. If you just ignore pronouns and don’t replace them with something useful, you might miss out on important stuff, like grammar or meaning. When we read, our brain automatically figures out what the pronoun is pointing to, so we understand the full picture. In NLP (Natural Language Processing), they do something similar with anaphora resolution. So, our plan is to use this info that would normally get lost and put it to work for AKE (Automated Knowledge Extraction).",
        "formal_text": "Moreover, pronouns have a relevant role in the sentences since they allow the author to enrich his writing using a richer vocabulary, composing more complex sentences, and so on. Pronouns are parts of the text which typically have the function of a substitute: depending on the case, they can replace a subject or an object, they can indicate possession, places, or refer back to people or things previously mentioned. Given these premises, disregarding all pronouns without replacing them with a valuable substitute could lead to a loss of a syntactical and/or semantical information. In fact, during the reading process we are able to decode the information conveyed by pronouns because we automatically replace them with the entity they refer to. In NLP a similar process is performed by anaphora resolution, thus our idea is to use this information, which would be otherwise lost, for AKE.",
        "GPT2_formal_text": "ed_x, a, where x is the image or caption, and a is the attention mask. Formal: Alright, let's break this down in a simpler way:\n\nSo, we're looking at something called a linear model (LM). The goal here is to figure out the probability of a caption x = (a1, ..., an) given an image ai, which is made up of two things: a1 and ..., an. Here's the equation: Formal: So, we're looking at something called a linear model (LM). The goal here is to figure out the probability of a caption x = (a1, ..., an) given an image ai, which is made up of two things: a1 and ..., an. Here's the equation:\n\nP(a1 | ai) P(a1) P(a1) P(a1)\n\nBasically, it's a function that gives us the probability of an image caption x given an image ai, based on some inputs. Formal:\n\nP(a1 | ai) = P(a1) * P(a1) * P(a1) * P(a1) * P(a1) * P(a1) * P(a1) * ...\n\nIt's a bit messy, but that's the gist of it! Formal:\n\nP(a1 | ai) = P(a1) * P(a1) * P(a1) * P(a1) * P(a1) * ... Formal:\n\nP(a1 | ai) = P(a1) * P(a1) * P(a1) * P(a1) * P(a1) * ...\n\nIt's all about probabilities here, and it's all based on some math stuff. Formal:\n\nP(a1 | ai) = P(a1) * P(a1) * P(a1) * P(a1) * P(a1) * ...\n\nIt's all about probabilities here, and it's all based on some math stuff. Formal:\n\nP(a1 | ai) = P(a1) * P(a1) * P(a1) * P(a1) * P(a1) * ...",
        "directory": "C16",
        "filename": "C16-1077.json"
    },
    {
        "casual_text": "今年9月，好几家外国石油公司和哈萨克斯坦的国家石油公司签了一大堆“世纪合同”。这些合同预计在未来40年内能带来7000亿美元的巨额利润。",
        "formal_text": "(8) 今年 This year ９月 September ，, 多 many 家 CL 外国 foreign 石油 oil 公司 company 与 with 哈 Kazakstan 国家 national 石油 oil 公司 company 签署 sign 了 LE 一揽子 a series of \"世纪 \"century 合同\" contract\" ，, 这些 these 合同 contract 将 will 在 in 今后 future ４０ 40 年 years 内 within 产生 generate ７０００亿 700-billion 美元 dollar 的 DE 巨额 enormous 利润 profit 。 \"",
        "GPT2_formal_text": "\" and \"verb_n_s_ent\" in the DATR specification.\n\nNow, let's talk about some ways to handle conditional information in DATR. Formal: Conditional information is a big deal in DATR. It’s kind of like a shortcut to the glue language (Hahn and Knight, 1993; Nagao, 1995). With this, you can set up a structured setup of DATR, kind of like what we’ll see in the next sections. Formal: Here, we’ll explain two ways to handle conditional information in DATR. Formal: 1. The ACE grammar is a way to handle conditional information in DATR. Formal: The main idea behind the ACE grammar is to give each part of a sentence a \"valued\" vector. For instance, the syntactic node for a verb phrase (v_v) is represented by its value vector v_v. Formal: Another way to handle conditional information in DATR is by adding a \"local_info\" function to the DATR grammar. Formal: Here, we’ll explain how to add local info to the DATR grammar. Formal: Since the data structure for a local_info is created by listing all the local information features, this is all part of how the DATR grammar is set up. Formal: For the ACE grammar to work, there’s a limited set of feature vectors v_v, where each v_v is either a non-zero number or the first element of a (n - 1)×n matrix. Formal: We’ll use this set of feature vectors v_v to represent the data structure for each verb phrase v_v. Formal: This method lets the ACE grammar focus on specific details, rather than all the details of the sentence. Formal: In the ACE grammar, there are a few things that need to be filled in: Formal: You can include the feature vectors v_v in a conditional feature structure, like in the example below. Formal: The local_info function can be defined in terms of the DATR grammar, like in the example below. Formal: If a feature vector v_v has a feature value v_v, the expression (v_v) will be reduced to v_v. Formal: We’ll show that this reduction can be implemented in terms of the D",
        "directory": "C10",
        "filename": "C10-2156.json"
    },
    {
        "casual_text": "The Tanglish tweets mix both Tamil and English words. We changed the Tamil words into English using a Tamil to English mapping corpus. We used the NLTK library in Python to clean up the data. For any classification task, pre-processing is super important because it helps the classifier work better. We did some cleaning steps like removing stop words, lemmatization, and getting rid of special characters. For example, after cleaning, the tweet \"@Bala sundar ayyo sorry. . . antha line ah clarify pannama vittutu irukan[: drowsy]ok na solran( en appavum indha grant work ku vanthurukkaru, neenga en appava paakala pola. . . . en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaru[: yeah][: yeah] ' chiiiii karumam podinnngggg. . . asingama vaila vanthurum. . . . \" becomes \"bala sundar ayyo sorry antha line ah clarify pannama vittutu irukandrowsyok na solran en appavum indha grant work ku vanthurukkaruneenga en appava paakala pola en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaruyeahyeah chi karumam poding asingama vaila vanthurum. \"",
        "formal_text": "The code-mixed Tanglish tweets contain both Tamil and English words and phrases. So, we have converted the Tamil words into English terms by using Tamil to English Mapping Corpus. We have utilized NLTK(Natural Language Tool Kit) package in python to clean the data. For any classification task, the pre-processing steps are important which helps to improve the classifier performance. We have performed some of the cleaning steps like stop word removal, lemmatization and removed the special characters. For example, after pre-processing, the tweet \"@Bala sundar ayyo sorry. . . antha line ah clarify pannama vittutu irukan[: drowsy]ok na solran( en appavum indha grant work ku vanthurukkaru, neenga en appava paakala pola. . . . en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaru[: yeah][: yeah] ' chiiiii karumam podinnngggg. . . asingama vaila vanthurum. . . . \" is converted into \"bala sundar ayyo sorry antha line ah clarify pannama vittutu irukandrowsyok na solran en appavum indha grant work ku vanthurukkaruneenga en appava paakala pola en appavukku munnadiye ipdi enna affront panra maathri kevi kettu asinga paduthuringa nu solraaruyeahyeah chi karumam poding asingama vaila vanthurum. \"",
        "GPT2_formal_text": "_f (x, h, c) = (1, [1, 1, ..., n])              Casual: We can easily calculate these two scores, let's call them α_ij and α_ij, using the formula α = arg max α_ij / arg max α_ij. In our setup, these two scores help decide if the attention is being paid to a specific part of the text or just the whole thing. Formal: Here's the formula for the attention-based multi-head attention. Formal: To make sure we're comparing things fairly, we're using the same two-step method to calculate α_ij and α_ij as described in Section 3. Formal: In our model, the attention weights are learned using a weighted average of the attention outputs from all the heads. Formal: The attention weights α_ij and α_ij are calculated based on the attention outputs from the attention heads in the model, with the weight α_ij given as a score. Formal: We do the same thing to get the attention weights for each attention head, and the final result is a weighted average of the attention outputs, with the weight α_ij as a score. Formal: Using the formula (2), the network can learn these attention weights, and the final result is a weighted average of the attention outputs, with the weight α_ij as a score. Formal: For this example, we're using the version of the model mentioned in [2]. Formal: Formal: Formal: This setup lets the model learn attention weights, but it still needs to figure out which attention heads to pay attention to. Formal: Formal: Formal: Formal: Formal: Formal: We also created a version of the model without any self-attention to see how much attention it pays to the heads in the sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "dravidianlangtech",
        "filename": "2021.dravidianlangtech-1.53.json"
    },
    {
        "casual_text": "We looked at eight different ways to check how well two texts match up, specifically comparing the system summary to the reference summary. BERTScore (BScore) checks for soft overlap by looking at the contextual BERT embeddings of tokens in both texts (Zhang et al., 2020). MoverScore (MScore) uses a distance measure on contextualized BERT and ELMo word embeddings (Zhao et al., 2019). Sentence Mover Similarity (SMS) finds the minimum distance matching between texts based on sentence embeddings (Clark et al., 2019). Word Mover Similarity (WMS) measures similarity by finding the minimum distance matching between texts represented as bags of word embeddings (Kusner et al., 2015). JS divergence (JS-2) measures the Jensen-Shannon divergence between the bigram distributions of the two texts (Lin et al., 2006). ROUGE-1 and ROUGE-2 measure the overlap of unigrams and bigrams, respectively (Lin, 2004). ROUGE-L looks at the overlap of the longest common subsequence between the two texts (Lin, 2004). We used the recall version of all these metrics (since the Pyramid method for human evaluations is based on recall) except for MScore, which doesn't have a specific recall version.",
        "formal_text": "We examine eight metrics that measure the agreement between two texts, in our case, between the system summary and reference summary. BERTScore (BScore) measures soft overlap between contextual BERT embeddings of tokens between the two texts 4 (Zhang et al. , 2020). MoverScore (MScore) applies a distance measure to contextualized BERT and ELMo word embeddings 5 (Zhao et al. , 2019). Sentence Mover Similarity (SMS) applies minimum distance matching between text based on sentence embeddings (Clark et al. , 2019). Word Mover Similarity (WMS) measures similarity using minimum distance matching between texts which are represented as a bag of word embeddings 6 (Kusner et al. , 2015). JS divergence (JS-2) measures Jensen-Shannon divergence between the two text's bigram distributions 7 (Lin et al. , 2006). ROUGE-1 and ROUGE-2 measure overlap of unigrams and bigrams respectively 8 (Lin, 2004). ROUGE-L measures overlap of the longest common subsequence between two texts (Lin, 2004). We use the recall variant of all metrics (since the Pyramid method of human evaluations is inherently recall based) except MScore which has no specific recall variant.",
        "GPT2_formal_text": "= [ \"w1\" , \"w2\" , \"w3\" , \"w4\" ] for sentence in session1, session2, session3, session4.\n\n1. In general, a user agent should only ask about the same three things again in each conversation if they're all in the same sentence. So, the next question should focus on what each user asked before. Formal: You should ask about all the same things as the user did before. Formal: So, the next question should focus on what each user asked before. Formal: You should ask about all the same things as the user did before. Formal: So, the next question should focus on what each user asked before. Formal: You should ask about all the same things as the user did before. Formal: So, the next question should focus on what each user asked before. Formal: You should ask about all the same things as the user did before. Formal: So, the next question should focus on what each user asked before. Formal: Formal: Formal: Formal: Formal:\n\n2. For any new questions in a conversation, the next question should focus on what the user asked before. Formal: You should ask about all the same things as the user did before. Formal: So, the next question should focus on what each user asked before. Formal: You should ask about all the same things as the user did before. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.751.json"
    },
    {
        "casual_text": "This paper talks about experiments done on tweet datasets in five languages: English, French, Spanish, German, and Italian. The basic idea is pretty straightforward: instead of sticking with a single-language model, we went with a multilingual one. We trained it on a big collection of English tweets, the original non-English tweets, and their automatic translations. We picked the XLM-RoBERTa model, which is a multilingual transformer model from Lample and Conneau (2019), and used a data-augmentation trick with machine translation. We looked into how pre-training with English data and data-augmentation affected things. We also compared how our multilingual models performed against their single-language versions for French (Martin et al., 2020) and English (Liu et al., 2019) and found some cool improvements.",
        "formal_text": "This paper presents the experiments carried out on several datasets of tweets in five different languages: English, French, Spanish, German and Italian. The general idea is pretty simple: instead of using a monolingual model, we chose to use a multilingual model that we can train over a large dataset of English tweets, over the original non-English tweets and over their automatic translations. We chose the multilingual transformer model XLM-RoBERTa from (Lample and Conneau, 2019) with a dataaugmentation technique using machine translation. We investigated the effects of pre-training with English data and data-augmentation. We also compared performances of multilingual models against their monolingual French (Martin et al. , 2020) and English (Liu et al. , 2019) counterparts and found interesting improvements.",
        "GPT2_formal_text": "_tgt = attention_mask_tgt + bias_mask_tgt * tanh(h(tgt + attention_mask_tgt))\n           Casual: In our tests, we used the same word-level attention model for all the languages, which we call attention_tgt. The attention for each language was calculated using this formula: Formal: For each word-level attention model, we calculate the attention for all the words in the target language. Then, we use this attention to create a copy of the source word and add it to the word-level attention. Formal: This can be written like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.23.json"
    },
    {
        "casual_text": "Looks like we gotta normalize those regression labels, otherwise the performance might not be as good as it could be.",
        "formal_text": "We find that the regression labels need to be normalized, or the performance may be sub-optimal.",
        "GPT2_formal_text": ": { u, r } = (u, r, l)           Casual: In this setup, we start by creating a discriminator D, which is a graph with two parts: a node for the \"source\" (source u, source r) and a node for the \"target\" (target u, target r). The nodes in D are labeled as φ i, j, and φ i, j are just the direct edges connecting the two nodes. A edge is just a pair of nodes if they show up together in the output. This setup helps us figure out the weights for these nodes, so we can predict the values of the source and target units. Formal: Using the knowledge base D as a source and the target as a target, we can train the discriminator D using a bunch of training examples. Formal: D is made up of three parts: a graph with nodes for the source (source u, source r) and a node for the target (target u, target r). Formal: We start by creating a discriminator D, which is a graph with two parts: a node for the \"source\" (source u, source r) and a node for the \"target\" (target u, target r). The nodes in D are labeled as φ i, j, and φ i, j are just the direct edges connecting the two nodes. Formal: We use the reward function (λ) as a guide for the weights of the nodes, and the input for D is a bunch of examples that we pull from the knowledge base D. Formal: We train D using a bunch of examples, creating a graph with three parts: a graph with nodes for the source (source u, source r), a node for the target (target u, target r), and a node for the target (target u, target r). Formal: Formal: Formal: Finally, we combine D with its final representation by using a weighted sum, which is shown in Equation 4. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.267.json"
    },
    {
        "casual_text": "After round 22, we'll pick a smaller group of data, called D*, from the main training set D. This D* will be what the model uses for the next round of training. To choose D*, we can use different methods, like weighting (Liang et al., 2016; Zhou et al., 2020), sampling (Zhou et al., 2021), or batching (Yong Jae Lee and Grauman, 2011).",
        "formal_text": "end for 22: end procedure a subset D * of the training set D, which will be used by the model in the next round of training. The selection function SF can utilize different approaches, e. g. , weighting (Liang et al. , 2016; Zhou et al. , 2020), sampling (Zhou et al. , 2021) or batching (Yong Jae Lee and Grauman, 2011).",
        "GPT2_formal_text": "_t (s_i, t_i) = z_i + tanh(w_i - e_i)/d_s_i).\n           Casual: We’ve included a small example of how to mix these two models. For this, we’ve included the initial graph setup (check out Figure 1a) and the results for (a) in Table 1. For (b), we’ve included the best setup (called r_s) and the performance for (a) in Table 2. Formal: If the input sequence has an LSTM structure (like a Bi-LSTM), the embedding of the current word is calculated using the history vector H_t, and the current word embedding is calculated using the history vector h_t. Formal: In a typical setup, the embedding vector of the current word is calculated based on the history vector h_t. Then, the current word embedding is calculated using the history vector h_t. Finally, the LSTM structure is applied to the current word. Formal: The model parameters are passed through the Bi-LSTM, which then produces the embedding vector for the current word. Formal: The output vector h_t represents the embedding of the current word, which is calculated based on the history vector H_t, and the history vector h_t. Formal: If the current word embedding is less than 1, we add a bias term λ_s to the hidden state h_t. Formal: If the current word embedding is greater than 1, we add a bias term λ_s to the hidden state h_t. Formal: Finally, the LSTM structure is applied to the current word. Formal: We tested our model on a big collection of text (500k words) and got F1-scores of 0.9 and 0.9 for (a) and (b) in the WSJ and WMT datasets, respectively. Formal: The accuracy for (a) and (b) is based on the accuracy for (a) and (b) for WSJ and WMT, respectively. Formal: In the PD dataset, the models that use attention masks perform better than those without them. Formal: The performance for (a) and (b) is based on the accuracy for (a) and (b",
        "directory": "alta",
        "filename": "2021.alta-1.3.json"
    },
    {
        "casual_text": "So, how do we use this system to deal with the issue of making clauses and texts make sense together? The first thing we did was to categorize the different ways that semantic coherence works and then fit them into our system. Basically, semantic coherence happens through things like:\n\n1. Proforms\n1.1.",
        "formal_text": "How do we apply this system to the problem of semantic coherence in clauses and texts? The first practical step consisted in olasslfylng the forms of semantic coherence and integrating them into our system. Accordingly, semantic coherence is expressed via 1. Proforms 1. 1.",
        "GPT2_formal_text": "\": [ \"00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00:00",
        "directory": "C82",
        "filename": "C82-2041.json"
    },
    {
        "casual_text": "Here, we'll talk about the different datasets we used for our experiments, the steps we took to get the data ready, and how we added extra info like automatically generated group labels and word forms.",
        "formal_text": "In the following, we present the different corpora used in our experiments, the preprocessing performed prior to experimentation and the enrichment of the data with automatically derived cluster labels and lemma information.",
        "GPT2_formal_text": "ed: True or False, the attention weight is calculated based on the weighted average of the hidden states. For the whole dataset, it's calculated like this: Formal: We trained a CNN with a window size of 5, like, 5 convolutional layers. The training process went like this: Formal: First, we set the batch size to 3 and the number of hidden layers to 2. Then, we randomly start the convolutional layer with a weight of 0, which we call G = {0, 0.5, 0.7, ..., 0, 0.9}. Formal: We set the dropout rate to 0.3, the dropout rate spread to 0.5, and the batch size to 20. Formal: We used a linear kernel with a stride of 3. Formal: For the one-hot encoding, we used 300 dimensions and a batch size of 20. Formal: We set the dropout rate to 0.2, the dropout spread to 0.5, and the batch size to 30. Formal: For the multi-head setup, we set the batch size to 200 and the number of hidden layers to 5. Formal: We trained the CNN with a window size of 10, with a learning rate of 0.01. The learning rate is also set to 0.01 for each of the non-linearities. Formal: We tested the model on all datasets using the validation set, and the average F-score was 0.33. Formal: We did the same setup for the single-head setup too. Formal: We used a linear kernel with a stride of 3. Formal: We set the dropout rate to 0.3, the dropout spread to 0.5, and the batch size to 5. Formal: The final probability is calculated based on the output from the BiLSTM. Formal: The loss is calculated using the validation set. Formal: We tested the model on all datasets using the validation set, and the average F-score was 0.33. Formal: The loss is calculated using the validation set. Formal: For the multi-head setup, we set the batch size to 200 and the number of hidden layers to 5. Formal: We tested the model on all datasets using the validation set, and the average F-score was 0.33. Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C12",
        "filename": "C12-2088.json"
    },
    {
        "casual_text": "Alright, let's break down how we built the groundtruth dataset. Here's what we did:\n\n1. **Xtrain**: We started with 10,000 resumes, each represented as a 9054-dimension vector. These vectors are basically one-hot encoded versions of the resumes. The 9054 dimensions cover all the words in all the resumes after cleaning them up and getting rid of common stop words.\n\n2. **Ytrain**: Next, we created 10,000 output vectors, each with 50 dimensions. To do this, we picked a random job description and used the same method we talked about in the white-box approach (check out Section 5.1 for details). This method helped us identify the 50 most important words from the job description. \n\nNow, we assumed the recruitment algorithm uses something called USE embedding, but the attacker could use other text embedding methods or even just go with the most common words. So, in reality, the attacker doesn't need to know exactly how the recruitment algorithm works. We chose 50 words to match our white-box attack setup and to give us plenty of options. But, this number can be adjusted depending on the attack.\n\nOnce we had the important words for the job description, we added each of these words to every resume in our training set. Then, we checked with the black-box algorithm to see if adding these words improved the resume's position. If it did, we marked that word and resume combination as a \"1\" in Ytrain; if not, it was a \"0\". So, the output labels are one-hot encoded vectors.",
        "formal_text": "Creating the groundtruth dataset. We employed these steps: (1) Xtrain: 10, 000 records of 9054-dimension vectors, each vector is a resume that is coded by one-hot format. The 9054dimension vector represents all the words (after cleaning and removing stop words) in all the resumes. (2) Ytrain: 10, 000 records of 50-dimension vectors. For creating the output vectors, we selected a random job description, and employed the same technique proposed in white-box approach (Section 5. 1) and identified the 50 most influential words in the job description. We assumed the recruitment algorithm uses the USE embedding, however the attacker can try this approach with other text embedding algorithms, or even choose the most frequent words. Therefore, in practice, the adversary does not need to know about the recruitment algorithm. We used 50 words to be consistent with our white-box attack and have enough choices of words. However, this is a parameter that can be defined during the time of attack. After identifying the influential words for the target job description, the adversary adds each of the words to each of the resumes in the training set, and then queries the black-box algorithm asking if the position of resume is improved. If it is improved then the value for that word and that resume in Ytrain would be 1, otherwise it would be 0. Therefore, the output label is an encoded vector by one-hot format.",
        "GPT2_formal_text": "\" with the word's embedding, which is kind of like what Li and his team did back in 2016. Formal: (2) We take the context vector ct and average it out, and (3) we do the same with the candidate embeddings c_i,k. Formal: The \"context vector\" vector we get from equation (1) is the one we use for the candidate embedding. Formal: (2) Equation (1) is like a function that averages out the embeddings for the words in a sentence, but it can be adjusted to fit different needs or tasks. Formal: (3) It’s way faster to calculate the candidate embeddings compared to just averaging them out. Formal: We do this averaging thing again, but we don’t do it with the context vector. Formal: (4) Once we’ve got the context vector, we average out all the embeddings from the candidate embeddings. Formal: (5) After that, we calculate the average of the embeddings for the whole sentence, which we call the \"context vector.\" Formal: Formal: (6) This total can be adjusted for different tasks or needs. Formal: (7) We show this whole process in Figure 1. Formal: The final result is the average of the context vector, the candidate embedding, and the embedding for the whole sentence. Formal: Formal: Formal: We’ve also included a simpler version of equation (1) with the same setup: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "blackboxnlp",
        "filename": "2021.blackboxnlp-1.36.json"
    },
    {
        "casual_text": "Assumption 5: There's no limit on what graph constants you can assign to a specific word.",
        "formal_text": "Assumption 5. There are no constraints imposed on which graph constants can be assigned to a particular word.",
        "GPT2_formal_text": "= max( 0, Φ(y_t, t_s)). end\n           Casual: The way these two sentences are worded together makes sense, but the way they're put together is kind of weird. Formal: But since our model isn't super detailed or takes into account different meanings, it doesn't really give us any clear answers for how these sentences should be translated. Formal: We're thinking of digging deeper into how the model works and maybe figuring out how to tweak it so it can better handle these tricky situations. Formal: Another idea is to use data from more languages and see what happens. Formal: We've already looked into this by using a method called natural logic for figuring out word meanings in Chinese (shoutout to Tan et al., 2019). Formal: We think natural logic-based methods could help us add more details to the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "• P(D): The domain priors try to figure out how much useful data the learning system is picking up. We could do this using smaller parts of the text (phrase-level), but we'd rather use whole sentences (sentence-level): 6",
        "formal_text": "• P (D): the domain priors aim at modeling the percentage of relevant data that the learning framework induces. It can be estimated via phrase-level parameters but here we prefer sentence-level parameters: 6",
        "GPT2_formal_text": "= {'max_selections', 'max_perm_tags', 'max_submission_ids', 'max_rejections', 'max_accepted_ids', 'average_rejections', 'sum_rejections', 'sum_accepted_ids', 'average_accepted_ids', 'average_perm_tags', 'sum_submission_ids', 'sum_rejections', 'sum_accepted_ids', 'average_submission_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_submission_ids', 'sum_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_accepted_ids', 'average_submission_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_rejections', 'sum_accepted_ids', 'average_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_accepted_ids', 'average_perm_tags', 'sum_rejections', 'sum_rejections",
        "directory": "D14",
        "filename": "D14-1062.json"
    },
    {
        "casual_text": "We checked how well the human experts agreed on whether something was acceptable, just like we did for figuring out if it was a spelling variant. Turns out, spelling variants are way trickier for people to judge compared to other types of answers. The agreement score was only .60 for spelling variants, while it was .83 for everything else (check out Table 3). Even with regular training, clear guidelines, and lots of pre-testing, human raters still had some inconsistencies, which matches what Buck (2001) found.",
        "formal_text": "We compute the inter-annotator agreement of the human experts for the acceptability decision on the same subset as for the annotation if something is a spelling variant. We observe that spelling variants are substantially harder for humans to judge than other answer variants, with a  value of. 60 for spelling variants as opposed to. 83 for all other items (see Table 3 ). Such scoring inconsistencies by human raters despite regular training, annotation guidelines and thorough pre-testing are in line with Buck (2001).",
        "GPT2_formal_text": "ing layer, which we use to adjust the attention weights in the attention layer. It’s a fixed parameter and can only be changed by resetting it. Formal: In the next section, we’ll explain how to train our model using the IOB2 model format. Formal: On top of the usual IOB2 input, we’ve added some special tokens to better represent spoken language. These tokens are part of a bigger set called T (which includes the IOB2 vocabulary, part-of-speech tags, and the vocabulary from IOB2 pronunciation tables), and they’re called IOB2 tokens. Formal: After adding these tokens, we run the model using the T text template. Formal: In the next section, we’ll talk about the IOB2 token format. Formal: For IOB2 tokens, we need to figure out the output encoding. The output encoding can be figured out using the IOB2 token format, which we’ll explain next. Formal: The input for the IOB2 token format looks like this: Formal: The i-th token is called a t-th token. Formal: For instance, the token \"he\" is encoded as [TAB, IOB2 token, T]. The t-th token is [TAB, IOB2 token, T]. Formal: We can also add some extra tokens to the IOB2 vocabulary to handle non-numeric characters, which is covered in the next section. Formal: From now on, we’ll call the token sequence [TAB, IOB2 token, T] for a token and the token sequence [TAB, IOB2 token, T] for a token with T. Formal: We figure out the output encoding for the IOB2 vocabulary using the IOB2 token format. Formal: Formal: Formal: We run the model with the T text template and the IOB2 token format. Formal: In Section 5, we’ll show how to train our model using the IOB2 vocabulary. Formal: Formal: Formal: Figure 1 shows an example of how to use the T text template in the IOB2 vocabulary. Formal: We use the T text template to encode the t-th token of each token. Formal: Formal: Formal: In Section 5, we’ll explain how to train",
        "directory": "bea",
        "filename": "2022.bea-1.22.json"
    },
    {
        "casual_text": "- It works seamlessly with MULTITERM, so you can look up terminology as you go.  \n- Plus, it comes with extra features like concordance and coverage analysis.",
        "formal_text": "• it is fully integrated with MULTITERM so that terminology can be consulted on the fly; • it provides a number of additional features such as concordance and coverage analysis.",
        "GPT2_formal_text": "= [x_i] end)\n     Casual: When we have a mention of a food item, we need to figure out the related food item. Here's how we do it: Formal: For each mention of a food item, we check if the food item (x_i) fits with the food category (y_i) in the dataset D. If it does, we label it as a relation between the food item and the food category. For instance, if the mention is \"apple juice\" (x_i) and the food item is \"orange juice,\" we label that as a relation between the food item and \"orange juice.\" Formal: Following the approach by Song and others (2013), we start by creating a list of possible relations for each food item. For instance, if we have the mention \"pizza\" (x_i) and the food item is \"cheese,\" we create a list of relations like {food, category, pizza, sauce}, where the food category is \"cheese.\" Formal: Once we have these relations, we pick the most relevant food category (y_i) from our list and assign it to the food item. Formal: Finally, we label the food item as a relation between the food category and the food item. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "1998.eamt-1.2.json"
    },
    {
        "casual_text": "Hey there,\n\nJust wanted to let you know that Prof. Jan Hajic from Charles University in the Czech Republic and Prof. Junichi Tsujii from Microsoft Research in China are the co-chairs of the COLING 2014 Program Committee. This was announced on July 8, 2014.\n\nCheers!",
        "formal_text": "Prof. Jan Hajic (Charles University, Czech Republic) Prof. Junichi Tsujii (Microsoft Research, China) COLING 2014 Program Committee Co-Chairs July 8, 2014",
        "GPT2_formal_text": "_1 = max(0, d_t-1, θ) = d_t-1. Formal: In our setup, we keep the same embedding layer and the same batch size. The model learns how to figure out the confidence level. The model does this by trying to minimize the log-likelihood of the posterior distribution across all t-dimensional representations, which is what we call the confidence-based loss. Formal: The model takes in a pair of input vectors and gives us a single output vector, y, which represents the probability that the input is correct. The confidence function, f_c, is calculated by adding up the log probabilities of all possible correct inputs, as shown in Equation 5. Formal: The model keeps the same embedding layer and the same batch size. Formal: The model takes in a pair of input vectors and gives us a single output vector, y, which represents the probability that the input is correct. The confidence function, f_c, is calculated by adding up the log probabilities of all possible correct inputs, as shown in Equation 5. Formal: The model keeps the same embedding layer and the same batch size. Formal: The model takes in a pair of input vectors and gives us a single output vector, y, which represents the probability that the input is correct. The confidence function, f_c, is calculated by adding up the log probabilities of all possible correct inputs, as shown in Equation 5. Formal: The model keeps the same embedding layer and the same batch size. Formal: The model takes in a pair of input vectors and gives us a single output vector, y, which represents the probability that the input is correct. Formal: The model keeps the same embedding layer and the same batch size. Formal: The model takes in a pair of input vectors and gives us a single output vector, y, which represents the probability that the input is correct. The confidence function, f_c, is calculated by adding up the log probabilities of all possible correct inputs, as shown in Equation 5. Formal: The model keeps the same embedding layer and the same batch size. Formal: Formal: The model takes in a pair of input vectors and gives us a single output vector, y, which represents the probability that the input is correct. Formal: Formal: The model keeps the same embedding layer and the same batch size. Formal: Formal: Form",
        "directory": "C14",
        "filename": "C14-3000.json"
    },
    {
        "casual_text": "Check out Table 2 for the stats on transcribed and conceptually annotated data.",
        "formal_text": "Statistics on transcribed and conceptually annotated data are reported in Table 2.",
        "GPT2_formal_text": "= (r_idx, r_idx + 1, ..., r_idx + n) for i in range(len(A)), (r_idx, r_idx + 1, ..., r_idx + n+m) for j in range(len(A)), (r_idx, r_idx + 1, ..., r_idx + n+m + 1) for k in range(len(A)), (r_idx, r_idx + 1, ..., r_idx + n +m + k + 1) for j in range(len(A)), (r_idx, r_idx + 1, ..., r_idx + n +m + k + m + 1) for k in range(len(A)), (r_idx, r_idx + 1, ..., r_idx + n +m + k + m + n) for j in range(len(A)), (r_idx, r_idx + n +m + k + 1) for j in range(len(A)), (r_idx, r_idx + 1) for k in range(len(A)), (r_idx, r_idx + n +m + k + 1) for j in range(len(A)), (r_idx, r_idx + n +m + k + n +1) for j in range(len(A)), (r_idx, r_idx + n +m + k + n +m + 1) for j in range(len(A)), and (r_idx, r_idx + 1, ..., r_idx + n +m + k + 1) for j in range(len(A)). Formal: In Figure 1, the highlighted parts are highlighted in bold. The arrows show how the sequence of tuples changes over time. Formal: The first part is the \"alignment\" of A_j with A_m. It’s the same as what Gildea and Knight did in 2004, but there are a couple of differences. They only used a single way to align things, so they only focused on the A_1 to A_n relationship. Formal: The second part is the \"translation\" of A_j with A_m.",
        "directory": "D09",
        "filename": "D09-1112.json"
    },
    {
        "casual_text": "In this part, we're using the type system from Groschwitz's 2019 work. In this system, types are represented as Directed Acyclic Graphs (DAGs), with the nodes being sources and the edges showing requests.",
        "formal_text": "Throughout this section we assume the type system of (Groschwitz, 2019), where types are formally defined as DAGs with sources as nodes, and requests being defined via the edges.",
        "GPT2_formal_text": "layer. The size of this layer is set to 3d_T by the Transformer encoder. Formal: To keep things simple, we’ll call the representation for the [CLS] token T_0 in the data set D_0. Formal: Using equation (3), we can rewrite the relation extraction task as the following: Formal: We’ll use a hierarchical attention layer on the output vector (X_t) to pick up on relation features. For instance, if we’re looking at the relation r_a_m, r_c_m, and r_c_m from Equation 1, we can pull out a bunch of features from these sentences. Formal: Next, we’ll toss in a linear layer to combine the relation vectors (X_t) and the current sentence representation (H_t). Formal: Finally, we’ll use a hidden state layer (with a size of 5d_T) to keep track of all the relation info and the current sentence representation H_t. Formal: We’ll use equation (3) to train our model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "The second part is a regularization term that makes sure the inner product between the poisoning loss gradient and the fine-tuning loss gradient isn't negative. The λ is just a number that shows how strong this regularization is. We named this method \"Restricted Inner Product Poison Learning\" (RIPPLe). \n\nNow, in a situation where the environment changes, we don't actually know the true fine-tuning loss, so the attacker has to use a stand-in, like L FT, to guess what it might be. Later on, we'll show that even a rough guess—like using a dataset from a totally different area—can work well enough for the RIPPLe attack to do its thing.",
        "formal_text": "where the second term is a regularization term that encourages the inner product between the poisoning loss gradient and the fine tuning loss gradient to be non-negative and λ is a coefficient denoting the strength of the regularization. We call this method \"Restricted Inner Product Poison Learning\" (RIPPLe). 2 In the domain shift setting, the true fine tuning loss is unknown, so the attacker will have to resort to a surrogate lossL FT as an approximation of L FT. We will later show experimentally that even a crude approximation (e. g. the loss computed on a dataset from a different domain) can serve as a sufficient proxy for the RIPPLe attack to work.",
        "GPT2_formal_text": "(or a translation of them) can be written as r ∈ R e (x, y). Formal: Here, δ is the cross-entropy loss. Formal: We're looking at the log-probability of the target sentence, which we calculate using the log-probability of the target sentence, p(t|s). Formal: In this study, we're testing how well our model performs when there are multiple possible translations for a given source sentence, let's call it s. The target sentence is the one that has the highest probability, given the source context c. Formal: To create the best translation, we use a maximum entropy model that's trained on all possible translations, using the log-probability of the target sentence as the evaluation. Formal: We're using a log-linear model to estimate the loss, but we didn't include the cross-entropy loss in our experiments. Formal: We're using a maximum entropy model to estimate the loss, but we didn't include the cross-entropy loss in our experiments. Formal: Finally, to get the best translation, we use the same maximum entropy model but this time, the loss is calculated based on the log-probability of the target sentence, p(t|s). Formal: The objective function we're using is: Formal: We've got a bunch of possible translations for the source sentence s. The target sentence is the one with the highest probability, given the source context c. Formal: We're using a maximum entropy model to estimate the loss, but we didn't include the cross-entropy loss in our experiments. Formal: Finally, to get the best translation, we use the same maximum entropy model but this time, the loss is calculated based on the log-probability of the target sentence, p(t|s). Formal: So, in short, we're trying to find the best translation by minimizing this function. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.249.json"
    },
    {
        "casual_text": "Check out Table 2 for their results. Here's what stood out: the model without domain adaptation (Noadapt) is already pretty strong, mostly because we've got plenty of training data. But, if you just throw all the queries from different domains into one model (Union), it messes things up big time. This happens because the same query can be labeled differently depending on the domain and the context. Plus, the amount of training data varies a lot between domains. For instance, slots like app names in other domains get overshadowed by slots in the PLACES domain, like place names, since PLACES has the most slot types and is the second biggest in terms of dataset size. Lastly, the feature augmentation method from Daumé III (2009)—you can see the F1 scores for that and different LSTM model versions across seventeen personal assistant domains in Table 3.",
        "formal_text": "Their results are shown in Table 2. A few observations: the model without domain adaptation (Noadapt) is already very competitive because we have sufficient training data. However, simply training a single model with aggregated queries across all domains significantly degrades performance (Union). This is because in many cases the same query is labeled differently depending on the domain and the context. This is also because the amounts of training data are widely different across different domains. For example, slots (e. g. app name) in other domains are overwhelmed by slots in PLACES domain such as place name since our PLACES domain is the largest in terms of number of slot types and 2nd largest in terms of dataset size. Finally, the feature augmentation method of Daumé III (2009) Table 3: F1 scores for Daumé III (2009) and LSTM model variants across seventeen personal assistant domains.",
        "GPT2_formal_text": "ed_dist. It’s not super clear to us what the difference between the two attention heads is, so we’re not going to dive into that right now. Instead, we’re just going to explain the basic setup of the Graph Attention Network (GAT), which basically captures the whole attention process. Formal: Equations (1) to (5) are basically the same, but here they’re flipped so that the final output vector (E) is what’s being focused on. Formal: This setup is based on the attention mechanism from Bahdanau et al. (2014), but we’re tweaking it a bit. We’ve added a nonlinearity (Φ) that acts as the key to unlock the hidden states, which helps us deal with the issue of sparse vectors. Formal: To put it simply, the GAT is like a bi-directional attention network that can look at multiple input vectors (E) and create a final output vector (E). Formal: To make things simpler, we’ve also created a simplified version of the attention mechanism, which is explained in Section 3. Formal: Lastly, we’ve added a kind of \"time-based\" attention, which helps the model focus on the most important bits of information during the process. Formal: Formal: The whole setup is laid out in Figure 1, which shows how the GAT works for an image and its context. Formal: To give you a quick look at how our model is set up, we’ll give you the timestamps for each step. Formal: The process happens in a sequence, with each step focused on a specific word or token. Formal: The output of the GAT is a fixed-length vector. Formal: The input to the GAT is a sequence of tokens, with each token being a special number (like a 0, a 1, or a 2) that tells us how much attention to pay to that token. Formal: The GAT works with a big LSTM setup with a hidden size of k × d = 300, as shown in Figure 1. Formal: We’re using the GRU memory network (from Bahdanau et al., 2014) for the GAT. Formal: The GAT has three main parts: the Bi-directional attention part, the time-based attention part, and the nonlinear",
        "directory": "C16",
        "filename": "C16-1038.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\nP({e, f}, a) is like saying, \"Hey, we're looking at these things, e and f, and how they relate to a.\" It's calculated by multiplying a couple of other probabilities: PG(; p$) and P(a|{e, f}).\n\nNow, PM(e, f) is just a way of saying, \"How likely are e and f to happen together?\" And this is made up of two parts:\n- pø times θN(e, f), which is like saying, \"There's a chance pø that this normal thing θN(e, f) happens.\"\n- (1 - pø) times θJ(e, f), which is like saying, \"And if the normal thing doesn't happen, there's a chance (1 - pø) that this other thing θJ(e, f) happens instead.\"\n\nSo, it's all about figuring out how likely things are to happen together and how that connects to something else, a.",
        "formal_text": "P ({ e, f }, a) = P G (; p $ )P (a|{ e, f }) e, f P M ( e, f ) P M ( e, f ) = p ø θ N ( e, f ) + (1 − p ø )θ J ( e, f ).",
        "GPT2_formal_text": "= { ..., 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'j', 'k', 'l', 'm', 'n', 'o', 'p', 'q', 'r', 's', 't', 'u', 'v', 'w', 'x', 'y', 'z'] Formal: We've already talked about how these methods are all about creating a ready-to-use database. But here's the thing: the way we structure the database is what makes it different from other systems. Formal: The server sends the database through the connection layer, which includes the structure of the database. The database is then passed to the query layer, which focuses on getting the right database records. Formal: From the query layer, we pull out the database information, which is stored in the memory layer. Formal: Finally, the server sends the results from the database layer back to the memory layer, which is the main part of the system. Formal: Formal: The server sends the database through the connection layer, which includes the structure of the database. The database is then passed to the query layer, which focuses on getting the right database records. Formal: Formal: We've already talked about how these systems are all about creating a ready-to-use database. But here's the thing: the way we structure the database is what makes it different from other systems. Formal: Formal: The server sends the database through the connection layer, which includes the structure of the database. The database is then passed to the query layer, which focuses on getting the right database records. Formal: Formal: The server sends the database through the connection layer, which includes the structure of the database. The database is then passed to the query layer, which focuses on getting the right database records. Formal: Formal: We've already talked about how these systems are all about creating a ready-to-use database. But here's the thing: the way we structure the database is what makes it different from other systems. Formal: Formal: The server sends the database through the connection layer, which includes the structure of the database. The database is then passed to the query layer, which focuses on getting the right database records. Formal: Formal: Formal: Formal: The server sends the database through the connection layer, which includes the structure of the",
        "directory": "D08",
        "filename": "D08-1033.json"
    },
    {
        "casual_text": "This feature is based on the method described in the paper by Fattah and Ren from 2008. We don’t usually include super short sentences in the final summary because they often don’t have much info. But longer Punjabi sentences? They can pack a lot of information. To calculate this feature, you just divide the number of words in a sentence by the word count of the longest sentence. The result will always be 1 or less.",
        "formal_text": "This feature is calculated as published in (Fattah & Ren, 2008). Very short sentences are avoided for including in final summary as often they contain less information. On the other hand lengthy Punjabi sentences might contain lot of information. This feature is calculated by dividing number of words in a sentence with word count of largest sentence. Its value will be always less than or equal to 1.",
        "GPT2_formal_text": "ed_x.h shape_t. You can find the code and data here: https://github.com/szeged/aspect-based-attention/blob/master/aspect_based_attention.py. Formal: Here's the text that goes with the augmented example: {<text>}, {<text, single-value>}, {<text, multiple-value>}, {<text, single-value, multiple-value>}, ..., {<text, single-value, multiple-value, single-value>}. Formal: Here's the code that uses the augmented example: {<code>}, {<code, single-value>}, {<code, multiple-value>}, ..., {<code, single-value, multiple-value, single-value>}. Formal: In this paper, we use a few different methods for representing the input data, so the augmented example should have these representations. Formal: For the sub-tree augmentation, we turn the tree into a matrix, which we can see as X = {x1, x2, ..., xT}, where x1, x2, ..., xT are the child nodes of the first subtree. We can also add a random word to each subtree node. Formal: We grab the attention values for the words in the augmented tree, which we can see as H = {h1, h2, ..., hT}. Formal: For the sentence-level augmentation, we swap the sentences in the augmented tree with the sentences in the original tree. Formal: Lastly, we pick the subtree nodes that get the highest attention score and add them to the original tree. Formal: Formal: Here's the table that shows how the augmented example is represented. Formal: Formal: We also use the TransE model (Vaswani et al., 2017) to figure out the attention values for each subtree node. Formal: We add some random words to each subtree node to make it match the original tree. Formal: We also use the TransE model to figure out the attention values for each subtree node. Formal: Formal: To see how well the augmented example can be identified, we train a classifier on the extracted attention scores to predict the intent. Formal: Formal: We also train a classifier on the extracted attention scores to predict the",
        "directory": "C12",
        "filename": "C12-3024.json"
    },
    {
        "casual_text": "For multi-label evaluation, let's say each edit \\( e_i \\) in the set \\( E \\) has a bunch of relevant categories, which we'll call \\( y_i \\) and it's part of the bigger set \\( C \\). Then, we also have the categories that our model predicts for that edit, which we'll call \\( h(e_i) \\).",
        "formal_text": "Multi-label Evaluation We denote the set of relevant categories for each edit e i ∈ E as y i ∈ C and the set of predicted categories as h(e i ).",
        "GPT2_formal_text": "= {'$a': '$s', '$c': '$t', '$u': '$v', '$v': '$v', '$s': '$s', '$t': '$t'} |where\n\n- i = 1, 2, ..., m\n- l = 0, 1\n- f = 5\n- w = 0\n- C = (1 + (2 + 3 + ... + m))\n- δ = 0, 0, ..., 1\n- α = α, 0, ..., 1\n\n- x = (x, z, w)\n\n- p = (P, y, z, p)\n\n- α + p\n\n- δ + (P + y, z, p)\n\n- p + α + (P + y, z, p)\n\n- δ + P + α + (P + y, z, p)\n\nBasically, it's an equation with a bunch of variables and some rules about how they work. Formal: The first part (Equation (1)) is about how the task is set up, and the second part (Equation (2)) is about how the system handles it. Formal: You can figure out the results by calculating something called the log probability (δ) of the outcome. Formal: The rule for how things are related is shown by Equation (3), which is a bit more complicated. Formal: The final result is a list of probabilities, like p(w|x, z), for each pair of outcomes (w, x, z) in the data. Formal: For any x, z in the data, p(w|x, z) is the probability of w happening based on x, z. Formal: Formal: The probability of x happening given z is calculated using the log probability. Formal: Formal: The probability of p(w|x, z) happening is calculated by multiplying the log probability with the probability of w happening based on x, z. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D13",
        "filename": "D13-1055.json"
    },
    {
        "casual_text": "On average, 27.31% of compounds get translated wrong when looking at individual cases. But if we combine all five contexts, 61.62% of compounds have at least one incorrect translation. This shows that even a really good NMT model struggles with translating compounds, even though all the parts of these compounds are super common in the training data. We also noticed that PP compounds have a much higher error rate—37.72%—compared to the other two types, which are around 21.94% and 22.25%. We'll dive into this more in the next section.",
        "formal_text": "Instance-wise, 27. 31% compounds are translated incorrectly. However, when aggregating all 5 contexts, 61. 62% compounds suffer at least one incorrect translation. This suggests that a well-trained NMT model is not robust in translating compounds, though all atoms within them are highly frequent in the training set. We also observe that the error rate of PP compounds, 37. 72%, is much higher than the other two, 21. 94% and 22. 25%, which we will discuss in detail in the following section.",
        "GPT2_formal_text": "ing_amt_len = [w_j for j in (1, W_j)] + [s_j for j in (1, S_j)] + [q_j for j in (1, Q_j)] + [m_j for j in (1, M_j)] Formal: In these equations, w_j, s_j, and q_j are just the word vectors, and m_j is the max pooling layer. Formal: In the pre-trained BART model, the hidden states are learned by using the latest decoder and the output from the last multi-head attention head. Formal: To make the student model learn its own vector representations, we use the student model's hidden state to get the output from the last multi-head attention head. Formal: The following equations show how the attention matrix in the student model is calculated. Formal: The student's hidden state is calculated using the input hidden states from all the other heads in the last multi-head attention head. Formal: The final representation of the student is a function that combines the representation of the last multi-head attention head (w_j) and the representation of all the word vectors (s_j). Formal: Here, \"1\" stands for the number of heads, and \"W\" and \"S\" represent the number of words in the student. Formal: The final representation of the student is calculated using the input hidden states from all the word vectors, but this time it uses the representation of the last multi-head attention head (w_j) and the representation of all the word vectors (s_j). Formal: The student's hidden state is calculated using the input hidden states from all the word vectors, but this time it uses the representation of the last multi-head attention head (w_j). Formal: Using the student's final representation, we can now calculate the output representation of the last multi-head attention head (w_j). Formal: We used the same method for calculating the representations of the student's hidden state as in the original paper by Bahdanau et al. (2019). Formal: We then calculate the loss function using the student's final representation, but this time we only use the final representation of the last multi-head attention head (w_j). Formal: The student's hidden state is then calculated using the input hidden states from all the word vectors, but",
        "directory": "acl",
        "filename": "2021.acl-long.368.json"
    },
    {
        "casual_text": "But, there aren’t many datasets out there that work well for creating adaptive argumentative writing systems for teaching purposes. As far as we know, there are only two datasets from the education field that use student-written texts and have been annotated to show argumentative structures. These are the ones by Stab and Gurevych (2017a) and Wambsganss et al. (2020c).",
        "formal_text": "However, corpora that are applicable for the design and development of adaptive argumentative writing systems in pedagogical scenarios are rather scarce. To the best of our knowledge, there are only two collections from the educational domain which are based on student-written texts and annotated for argumentative discourse structures (Stab and Gurevych, 2017a; Wambsganss et al. , 2020c).",
        "GPT2_formal_text": "= [n; h] p(w|θ) = [d|θ] log p(w|θ) log p(w|θ) p(w|θ) p(w|θ) where D is the set of dependency relations. Formal: So, if we have D = [s, t, θ], we can write equation (1) like this: Formal: Here, s is the set of arguments, t is the set of subjects, θ is the set of arguments, and θ is the set of direct and indirect objects. Formal: To sum it up, we’ll say that D has a length of d minus the sum of the lengths of all the arguments, i. Formal: Also, if D has a length of 1, D is just the set of direct and indirect objects. Formal: To keep things simple, we’ll leave it to the reader to look into this in more detail. Formal: We’re using D as our starting point, D_s, for the argument set. We’re assuming that the arguments in D are the right shape to represent a single sentence, and we’re following the approach by (Benton et al., 2003). Formal: So, there’s an argument called x that has a relation called r_i. Formal: D_s is just a bunch of phrases with a specific relation labeled r_i. Formal: For each phrase (x) in D_s, we’ll look at the direct and indirect objects in D_s, which we’ll call (x, x') and (x, x')', respectively. Formal: For any pairs (x, x')', where x and x' are in D_s, we’ll assign a length of d to the relation r_i. Formal: Formal: So, there’s an argument called x that has a relation called r_i. Formal: D_s is just a bunch of phrases with a specific relation labeled r_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.599.json"
    },
    {
        "casual_text": "The work we're talking about here focuses on two main areas related to making things more robust. First, we're looking at using features from both syntactic and semantic processing that can handle changes in genre and language without breaking down. We're aiming to move beyond the simple \"bag of words\" approach used in older systems (like those by Bagga & Baldwin, 1998; Gooi & Allan, 2004; Pedersen et al., 2005) that didn't really try to dig into deeper semantic features, which are tricky to pull out. We're also trying to avoid relying too much on specific biographical details (like what Mann & Yarowsky, 2003, did) that you don't see often in the kind of documents search engines usually pull up—stuff like place of birth or family relationships.\n\nThe second part of our work involves applying these techniques to both English and Chinese news collections. Turns out, the methods work well for both languages, but when we look closely at the errors, we find some cool differences between the two.",
        "formal_text": "The specific contributions reported here fall into two general areas related to robustness. In the first, we explore the use of features extracted from syntactic and semantic processing at a level that is robust to changes in genre and language. In particular, we seek to go beyond the kind of bag of local words features employed in earlier systems (Bagga & Baldwin, 1998; Gooi & Allan, 2004; Pedersen et al. , 2005 ) that did not attempt to exploit deep semantic features that are difficult to extract, and to go beyond the kind of biographical information (Mann & Yarowsky, 2003) that is unlikely to occur with great frequency (such as place of birth, or family relationships) in many of the documents returned by typical search engines. The second contribution involves the application of these techniques to both English and Chinese news collections. As we'll see, the methods are effective with both, but error analyses reveal interesting differences between the two languages.",
        "GPT2_formal_text": "_tgt: The attention weight is calculated using this formula: 1 / (1 + p_max). Formal: • Equation (7) and (8) are treated as equations that are independent of each other. Formal: • Equations (7) and (8) are both solved together. Formal: • The decoding process is set up in a way that it can handle any type of input, including random or unpredictable ones. Formal: • The learning rate can vary, from 1e-4 to 10^-4. Formal: • The system works with simple binary variables that are either 0 or 1. Formal: • When working with a finite set of input tokens, a basic decoding module takes the input and figures out the attention weight using equation (6). Formal: • The training data for this module is made up of tokens, and each token is treated as a single instance of the token. Formal: • The system is built step by step, starting with the initial setup and working its way up, using a modular structure. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D07",
        "filename": "D07-1020.json"
    },
    {
        "casual_text": "This paper talks about tackling two main issues: polysemy (when a word has multiple meanings) and having a phrasal lexicon. Like Guthrie and Yuasa, we use a vector representation, meaning each article is represented as a vector. But here's the twist: instead of assigning each coordinate of the vector to every word in the article, we focus on a single word (specifically, a noun) whose meaning we’ve figured out. \n\nOur way of figuring out the meaning of a word is based on Niwa's method, which compares the similarity between two sentences—one that has a noun with multiple meanings and another that’s like a dictionary definition. To handle Walker's problem, after we’ve sorted out the meanings, we calculate how related the words are in terms of meaning and group the ones that are related together.",
        "formal_text": "The approach proposed in this paper focuses on these l)roblems, i. e. 1)olysemy and a phrasal lexicon. Like Guthrie and Yuasa's methods, our approach adopts a vector representation, i. e. every article is characterised by a vector. However~ while their ~pproaehes assign each (: oor(linate of a vector to each word in artMes, we use a word (noun) of wtfich sense is disambiguated. Our disambiguation method of word-senses is based on Niwa's method whMt use(l the similarit; y 1)etween two sentences, i. e. a sentevee which contains a polysenmus noun and a sevtenee of dictionarydefinition. In order to cope with Walker's l)roblem, for the results of disand)iguation technique, semantic relativeness of words are cMeulated, and semantically related words are grout)ed together.",
        "GPT2_formal_text": "= [title_1, title_2, ..., title_n, ..., title_t] subject = subject_vec2d(subject) title = title_vec2d(title) sentence = sentence_vec2d(sentence) p = projection(p) l = max(r * p, d) + 1 * d if l == 1 or l == 2 then return (0, 0) else return (1, 1) end end function nl: length(d) = (nl + 1) - length(d) end function nl:length(p) = (nl + 1) - length(p) end function nl:length(r) = (nl + 1) - length(r) end function nl:length(p1) = (nl + 1) - length(p1) end function nl:length(p2) = (nl + 1) - length(p2) end function nl:length(r1) = (nl + 1) - length(r1) end function p:length(d) = (p + 1) - length(d) end function p:length(p1) = (p + 1) - length(p1) end function p:length(p2) = (p + 1) - length(p2) end function p:length(r) = (p + 1) - length(r) end function p:length(p2) = (p + 1) - length(p2) end function p:length(r1) = (p + 1) - length(r1) end function p:length(r2) = (p + 1) - length(r2) end end function P1: length(d) = (P1 + 1) - length(d) end function P1:length(p1) = (P1 + 1) - length(p1) end function P1:length(p2) = (P1 + 1) - length(p2) end function P1:length(r1) = (P1 + 1) - length(r1) end function P1:length(r2) = (P1 + 1) - length(r2) end function P1:length(r2) = (P1 + 1) - length(r2) end end function P",
        "directory": "C96",
        "filename": "C96-1069.json"
    },
    {
        "casual_text": "Since the title words are already part of the nodes in X, we can just add another type of edge, like we did in section 3.2.4, to have three types of edges for the same pair of nodes: edges from X, R, and the title itself. For the edges from X, we use the proximity of the title words based on their positions to determine the edge weights. For the title edges, we use the context from the title to update the contextualized representations, kind of like what we did in equations (3, 9). The results are in Table 4.",
        "formal_text": "Since the title words are already represented as nodes in X, we can simply add another edge type, as similarly done in §3. 2. 4, to make three types of edges for the same node pair: edges from X, R, and the given title respectively. As in edges for X, we use position-based proximity between title words, for edge weights. Given the title edges, we use contexts gathered from the title, to update contextualized representations, similar to Eq (3, 9). The results are shown in Table 4.",
        "GPT2_formal_text": "ed. LSTM-based models, such as BERT (Devlin et al., 2019), take the sequence of words as input and spit out word representations. These words then get fed into the classifier to figure out the prediction for each label. The cool thing about this is that it’s super efficient because it can just spit out a bunch of tokens for a given word. But, the thing is, words don’t always have clear meanings, and in some cases, you might have to figure out the meaning from the context.\n\nFor example, take the sentence \"I bought two iphones yesterday.\" The word \"iphones\" is a verb, and we know that \"I\" is a phone because of the prefix. But if the context says \"I bought a iphone yesterday,\" we can’t just focus on the specific context to figure out the word’s meaning. So, we have to pick and choose the most relevant bits from the whole thing to help us out. Formal: Let’s say we’re dealing with a sentence like \"I bought two iphones yesterday.\" We’re looking for the most important bits that could help us understand the sentence. Formal: Here’s how we do it: Formal: We grab the sentence’s token embedding (let’s call it e_i) from the context as a hidden vector h_c_i. We do this for the whole sentence and then average it out to get a hidden representation e_i. Formal: Using the hidden representation h_c_i, we can then calculate the attention weight Φ_i. Formal: For each word in the sentence, we calculate the attention weight Φ_i, which is calculated as: Formal: Lastly, we average the attention weights across the words in the sentence to get the final representation. Formal: Formal: Finally, we average the attention weights across the words in the sentence to get the final representation. Formal:\n\nOne cool thing about BERT is how it uses the attention, not just the hidden vector, to help with its classification. Specifically, it uses attention for the word \"iphones\" by considering both the context (like the prefix) and the word itself (like the verb). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.209.json"
    },
    {
        "casual_text": "We're mainly focused on this problem: when you have a compound noun, how do you figure out which way it could be understood, from the most likely to the least likely? This problem can be thought of using something called unification. Unification is a neat way to describe how different pieces of information come together to make sense of things. It helps with figuring out the different meanings of words and structures, as well as how those meanings fit together.\n\nOne cool thing about unification is that it doesn't have a favorite order for putting things together. It doesn't automatically prefer one way over another. This is really useful when you're trying to decide which meaning of a compound noun is the most likely.\n\nWhen dealing with compound nouns, you have to figure out the different possible meanings (lexical ambiguity), how those meanings fit together in a sentence (structural ambiguity), and how everything adds up to make sense (semantic composition). It's not a great idea to focus too much on just one part of this process because everything is connected. For example, the way you pick a \"frame\" (like a mental outline of what's happening) can change how you understand the words (lexical ambiguity). Also, choosing a frame and deciding who is doing what (case/role binding) are part of the same process of making sense of the whole thing (semantic composition). And how you resolve structural ambiguity often depends on how you're putting the meanings together.",
        "formal_text": "We are primarily concerned here with the following p, : oblem: given a nominal compound, determine the ranking of its possible interpretations from most to least likely. The problem can be formulated in terms of unification. Unification-based formalisms provide an elegant means of describing the information structures used to construct interpretations. Lexical and structural ambiguity resolution, as well as semantic composition, are readily characterized as choices between alternative sequences of unification operations. A key feature of unification--especially important foJ: preference integration--is its neutrality with respect to control, i. c. , there is no inherent bias in the order of unifications, and thus, no bias as to which choices take precedence ovcr others. Although nominal compound interpretation involves lcxical and st t'uctural ambiguity resolution and semantic comp()sition, it is not a good idea to centralize control around any single isolated task, because there is too much interaction. For example, the frame selection problem affects lexical arnbiguity resolution (consider the special case where the frame selected is that signified by the lexical item). Likewise, frame selection and case/role binding are two aspects of the same semantic composition problem, and structural ambiguity resolution depends largely on preferences in semantic composition.",
        "GPT2_formal_text": ": {u, v} = [u, v] for j in range (0, d_sectors). Formal: Using the clustering method, we can figure out the probability of each token in the input sequence x. For instance, if the sequence x = [x1, x2, ..., xi] with the given tokens u, v, and label y = [y1, y2, ..., yj], we can calculate the probability like this: Formal: The probability p(t|x) for a token in the input sequence is calculated using a standard logistic regression model, which is trained using a probability distribution p(t|x) = [p_v(t|x), p_y(t|x)] for the training data D. Formal: The loss function l(y) is defined as... Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C90",
        "filename": "C90-2071.json"
    },
    {
        "casual_text": "LetsMT! uses Moses as its go-to language-independent SMT solution and plugs it into the LetsMT! online platform as a cloud-based service. The Moses toolkit has been tweaked to work smoothly in a fast-paced environment where training, updating, and interactive access are key. The Moses SMT training process is made up of several steps, each needing a different program to run. Within the LetsMT! setup, this whole process is simplified and can be automatically adjusted based on user inputs, like training corpora, language model data, dictionaries, and tuning sets.",
        "formal_text": "LetsMT! uses Moses as a language independent SMT solution and integrates it as a cloud-based service into the LetsMT! online platform. Moses toolkit has been adapted to fit into the rapid training, updating, and interactive access environment. The Moses SMT training pipeline involves a number of steps that each require a separate program to run. In the framework of LetsMT! this process is streamlined and made automatically configurable given a set of user-specified variables (training corpora, data for language model, dictionaries, tuning sets).",
        "GPT2_formal_text": ". Formal: • Probabilistic sequential models are now commonly used for understanding how text is represented, especially when dealing with text from different languages. Formal: • This framework, which uses a sequence of random gates, works with different levels of the input to generate text in a predictable order. Formal: • The output from these gates is usually a sequence of output gates, which gives you a sequence of outputs. Formal: • This setup is different from LSTM models, where the output from the first gate in the sequence is passed into the second gate. Formal: • The input for the first gate includes the last word in the sentence, and the output for the second gate is chosen based on how likely it is to come next. Formal: Formal: • Instead of relying on the actual words in the input, we focus on the likelihood of each word coming next. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2012.eamt-1.32.json"
    },
    {
        "casual_text": "As shown in Figure 1, TTOS has three main parts: a teacher network focused on the knowledge base (T KB) that grabs entities from the KB, another teacher network (T DP) that learns dialogue patterns, and a student network (S) that aims to pull out accurate KB entities and create natural-sounding responses. All three networks have the same basic setup but are trained differently.",
        "formal_text": "As illustrated in Figure 1, TTOS consists of three networks: a KB-oriented teacher network (T KB ) that is specialized for retrieving entities from KB, a dialogue pattern-oriented teacher network (T DP ) that is specialized for learning the dialogue patterns, and a student network (S) that tries to extract accurate KB entities and generate human-like responses. The three networks share the same network structure but different training strategies.",
        "GPT2_formal_text": "= (input_id, id, seq2seq, (seq_2seq_i, seq2seq)) ... ) P1 = 1 P2 = 1 ... ... ... Pn = 1. Each part of the process is laid out in a straight line, like in Fig. 1. Formal: We need to figure out the parameters α and β using the EBMT parser. Once we know those, we can start doing the inference process. Formal: In the next sections, we'll talk about how to do the inference in a more formal way. Formal: In this part, we'll first explain the EBMT parsing algorithm. Then, we'll show a quick example where we're building a sentence using that algorithm. Formal: After the EBMT parser has done its thing, we'll use the example sentence to figure out the parameters α and β. Formal: We'll show how to use the EBMT algorithm with the specific EBMT setup. Formal: The set of possible output sequences is called M, and the set of possible input sequences, O, is called X. The EBMT algorithm uses a cost function, c, that maps inputs and output sequences to their respective cost functions. Formal: The cost function c(•) is a bit different from the cost function c(0). If a simple rule like γ = (1 − c(•)) works, we can take any sequence and map it to its cost function. Formal: We'll use the EBMT algorithm with the specific EBMT setup. Formal: This part is all about how to handle the cost function c. Formal: We'll introduce the cost function c and explain how to compute it. Formal: Formal: We'll also look at some constraints on the cost function c, which helps us figure out the best EBMT algorithm. Formal: Lastly, we'll define some constraints for O and X. Formal: Formal: Finally, we'll define some constraints for M, which helps us learn the best EBMT algorithm. Formal: Formal: Formal: We'll introduce a cost function that works with the EBMT algorithm. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.281.json"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. So, Φ y i, y i+1 is the chance of moving from one label (y i) to the next one (y i+1). Φ is like a grid (a matrix) that’s (k + 2) by (k + 2) big, where k is the total number of different labels we have. We add two extra labels, \"start\" and \"end,\" which are only used in this special layer (called CRF) to mark the beginning and end of a sequence.\n\nNormally, the CRF layer tries to find the most likely sequence of labels. But with this modified IOBES system, a sentence can have more than one correct label sequence, as you can see in Figure 1. So, we’ve tweaked the CRF layer to work a bit differently—we call it a \"fuzzy CRF model.\" Instead of just picking one sequence, it looks at all the possible sequences, including all the IOBES tags and the different entity types, and tries to maximize the total probability of all of them. We’ve written this goal mathematically in Equation 2.",
        "formal_text": "where, Φ y i, y i+1 is the transition probability from a label y i to its next label y i+1. Φ is a (k + 2) × (k + 2) matrix, where k is the number of distinct labels. Two additional labels start and end are used (only used in the CRF layer) to represent the beginning and end of a sequence, respectively. The conventional CRF layer maximizes the probability of the only valid label sequence. However, in the modified IOBES scheme, one sentence may have multiple valid label sequences, as shown in Fig. 1. Therefore, we extend the conventional CRF layer to a fuzzy CRF model. Instead, it maximizes the total probability of all possible label sequences by enumerating both the IOBES tags and all matched entity types. Mathematically, we define the optimization goal as Eq. 2.",
        "GPT2_formal_text": "ing pre-trained models, which have been really successful in a lot of NLP tasks, like image classification, named entity recognition (NER), text classification (TC), and sentiment analysis. As far as we know, our work is the first to tackle the issue of dealing with multiple inputs and handle it step by step. Formal: We measure the average perplexity for each entity, using the average of the perplexities for the query and the entity. Formal: The perplexity is calculated by averaging the perplexities of the entity and the query. Formal: To check how well the model is doing, we calculate the accuracy, which is like a ranking score, for each query. Formal: The accuracy is calculated by averaging the accuracy for the query and the entity. Formal: To find the best entity, we look at the highest ranking entity that appears more than once in the results. Formal: For the queries with a lot of entities, if there are multiple entity candidates, we consider all of them. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1230.json"
    },
    {
        "casual_text": "We're suggesting a method called co-training for Chinese word segmentation, which is useful when you don’t have much labeled data (data that’s already been segmented by humans) but you have a lot of unsegmented data in the same domain. Here’s how it works: we divide the features into two groups—character-level features and word-level features. Then, we create two separate segmenters: one that uses character-level features and another that uses word-level features, both trained on the small amount of labeled data we have. These two segmenters then help each other improve by working with the large amount of unsegmented data. Finally, we mix the character-level and word-level features together and use a more advanced segmenter to boost the performance of the co-training process.\n\nOur tests show that when we use just 10% of the data as labeled data and the remaining 90% as unsegmented data, co-training gives us a 20% performance boost compared to supervised training using all the labeled data in the SIGHAN 2005 PKU corpus. In the CU corpus, it gives us a 31% improvement.",
        "formal_text": "We propose the approach of co-training for Chinese word segmentation for the semi-supervised setting where there is only a limited amount of humansegmented data available, but there exists a relatively large amount of in-domain unsegmented data. We split the feature set into character-level features and word-level features, and then build a character-based segmenter with character-level features and a wordbased segmenter with word-level features, using the limited amount of available segmented data. These two segmenters then iteratively educate and improve each other by making use of the large amount of unsegmented data. Finally we combine the wordlevel and character-level features with an advanced segmenter to further improve the co-training performance. Our experiments show that using 10% data as segmented data and the other 90% data as unsegmented data, co-training reaches 20% performance improvement achieved by supervised training with all data in the SIGHAN 2005 PKU corpus and 31% in the CU corpus.",
        "GPT2_formal_text": "= [s1, ..., sm] # just the attribute and a few values {s1, ..., sm} = {s1, ..., sm}. Formal: In a relational setting, we can get by without using direct inclusion or a specific query. Formal: In this project, we came up with a fresh way to handle entity linking, which is based on two main ideas: the first idea is about using the attributes of an entity to help with entity linking, and the second idea is about creating a query to help with entity linking. We tested our method on two popular datasets and found it works really well. Formal: We believe our method could be applied to other areas, like how language is structured or how documents are organized. Formal: We also looked into how domain knowledge affects entity linking and found that domain knowledge really matters in the entity linking process. Formal: To make things easier, we think it's useful to use extra knowledge from other areas, like the world knowledge, to improve how we do entity linking. Formal: We also looked into how query expansion affects how we do entity linking. We think it can help improve the results. Formal: Formal: We'll share our code, dataset, and models for anyone to check out. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D13",
        "filename": "D13-1119.json"
    },
    {
        "casual_text": "Alright, so when you round down, you get the index of the last singular value that's considered important. This number is basically seen as the effective number of dimensions, or the rank, of the matrix X. Now, if d is the number of dimensions in the space where X lives, and we think that the number of word vectors in X is usually way bigger than d, then this idea makes sense (and you can check out Roy and Vetterli's work from 2007 for more details).",
        "formal_text": ", rounded to the smaller integer, yields the index of the last singular value that is considered significant, and is interpreted as the effective dimensionality, or rank, of the matrix X. If d is the dimensionality of the embedding space X, and we assume that the number of word vectors in X is typically much larger than d, it then holds that (see Roy and Vetterli (2007)",
        "GPT2_formal_text": "_d_k) from the layer in the decoder, which we'll call H(Decoder, AttentionMask) from now on. Formal: We also tried out the model using the usual attention mask, but we didn't include the log loss in the results because it's not needed. Formal: We compared our model to the basic model by Zhang et al. (2019). The results are in Table 2. Formal: To keep things simple and avoid any weird changes caused by non-zero or zero vectors, we trained the base model using the binary cross-entropy loss, which we called W(Base). Formal: We didn't test how changing the input size affects performance. Formal: We did some testing on the base model with different input sizes. We looked at how the base model performs without the log loss. Formal: We also tried the model with the log loss, but we didn't include the log loss in the results because it's not needed. Formal: This shows that the log loss is necessary to add the attention mask. Formal: The log loss helps the model keep track of the hidden state, which helps improve performance. Formal: Lastly, we looked at how the log loss affects the model's overall performance. Formal: The log loss helps the model learn to focus on the most important parts of the input, which improves performance. Formal: Lastly, we tested the model without the log loss. We found that it doesn't perform as well as the log loss-based model, which we noticed through testing. Formal: Formal: We think this model could be useful for creating more flexible classifiers, especially for things like named entity recognition and sentiment analysis. Formal: Formal: Also, we noticed that the model without the log loss is pretty unstable, so we think it might need more tweaking to handle different situations. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.186.json"
    },
    {
        "casual_text": "Automatic metrics usually don’t do a great job of understanding the meaning of sentences or how they’re structured. They also struggle when there’s only one correct answer to compare against. In fact, these metrics don’t match up with how humans judge things in tasks like fixing grammar mistakes (Napoles et al., 2015) or generating dialogue (Liu et al., 2016). Since we’re working on a new task and haven’t really tested how these metrics perform in this context, we think it’s important to get human feedback and see if these metrics line up with what people think is good.",
        "formal_text": "Automatic metrics often fail to incorporate semantic meaning and sentence structure in evaluation as well as accurately capture performance when there is only one gold-standard reference; indeed, these metrics do not align with human judgment in other generation tasks like grammatical error correction (Napoles et al. , 2015) and dialogue generation (Liu et al. , 2016). Since automatic metrics have not yet been explored in the context of the new task we are proposing, we find it necessary to conduct human evaluation and study whether these metrics are consistent with human judgment.",
        "GPT2_formal_text": "\", which are the user's personal info. You can easily use it with the apps we mentioned earlier. Formal: We use this method to figure out the user's gender, but it doesn't really help us with understanding the meaning. Formal: We're planning to train a classifier to tell the difference between a user's true gender (t_u) and the gender label (t_l). To do this, we'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t_u label as a special input for the MLP. Formal: We'll use the t",
        "directory": "acl",
        "filename": "2020.acl-main.168.json"
    },
    {
        "casual_text": "The main thing we used for this experiment was the MULTEXT-EAST multilingual parallel and comparative corpus, which Dimitrova and others talked about in 1998. We took some random sentences from the corpus to train with, and the rest were for testing. The same sentences were picked for every language we worked with.",
        "formal_text": "The most important resource used in this experiment was the multilingual parallel and comparative corpus MULTEXT-EAST (Dimitrova et al. , 1998). Part of the corpus (randomly selected sentences) was used for training, the rest was used for testing purposes. The same sentences were selected for each language.",
        "GPT2_formal_text": "ed_attributes = graph.concatenate([attributes_0, ..., attributes_t]), where t is the number of elements in the feature set. The graph also includes a special number called σ, which is calculated by multiplying the sum of all the weights for each of the triples in the graph. After that, the final value for the attention scores is taken as the attention scores for the tokens in the attribute set. Formal: Here's how the system works: First, a list of tokens gets fed into the attention module. Then, it predicts the next token and assigns it to a special attention set, which we call M_T. This attention set is used in a sequence-to-sequence model, which gives us a sequence representation t. After that, we use a linear layer to turn t into a vector representation, and it's fed into a fully connected layer. Finally, the output from the fully connected layer is used as the attention scores for the tokens in the attribute set. Formal: The attention scores for tokens in the attribute set get a weight based on the weighted sum of the attention scores for their tokens. Formal: Lastly, we get the attention scores for the tokens in the attribute set and the attention scores for the tokens in the attention set. For example, the attention scores for the tokens in the attribute set are calculated by the attention module, which gives us the attention scores for the tokens in the attribute set, which we call C_A. Formal: The attention scores for tokens in the attribute set get a weight based on the weighted sum of the attention scores for their tokens. Formal: Finally, we get the attention scores for the tokens in the attribute set and the attention scores for the tokens in the attention set. Formal: These attention scores are calculated using the attention module, which gives us the attention scores for the tokens in the attribute set, which we call A_A. Formal: Lastly, we get the attention scores for the tokens in the attribute set and the attention scores for the tokens in the attention set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2010.eamt-1.24.json"
    },
    {
        "casual_text": "In this part, we’ll start by talking about the scenario-based dialogue model. Next, we’ll explain the imitation learning framework you can see in Figure 2. And lastly, we’ll go over the training objective.",
        "formal_text": "In this section, we first introduce the scenariobased dialogue model, then describe the imitation learning framework shown in Figure 2, and finally present the training objective.",
        "GPT2_formal_text": "= (i, j, k) | a_i, a_j, ..., a_m + L(d_i) − L(d_j) Formal: In this project, we're mainly looking at how to evaluate these different models by looking at the average F1 score. We're assuming there's a connection between the score and how well the model performs. Formal: We came up with a new way to evaluate machine translation (MT) models, called \"generalization loss.\" Basically, it compares the model's performance across different languages by multiplying the actual F1 score with something called log-likelihood loss. Formal: To evaluate these generalization models, we use something called log-likelihood loss, which compares the model's performance across different languages using a logistic function. Formal: Next, we created a new metric called log loss with a special symbol to show how it compares to the original F1 score. Formal: The next sections will explain how to calculate these log losses. Formal: To see how well our model can generalize, we tested it on the MT data set we mentioned earlier. The results are in the last two columns of Table 1. Formal: For any language, we calculated the log-likelihood loss using the MT test data set. Formal: The best model didn't do as well as the best human translation, which means it didn't learn a lot from the training data. Formal: If the model can generalize, it should do better than the worst model, which means it should learn more from the test data. Formal: The model was tested on a few different datasets. Formal: We did an ablation study to see how well the model generalizes on new datasets. Formal: We also did some cross-validation tests to see how well the model performs on new test sets. Formal: We found that the model can generalize to new datasets but that it might still struggle with the ones it already knows well. Formal: Formal: This study is part of a larger project to improve how we evaluate MT models. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.534.json"
    },
    {
        "casual_text": "So, let's break this down in a simpler way. Each part of the text is talking about different models and how they perform on tasks like summarizing articles from CNN or DailyMail. Here's the info in a more casual tone:\n\n- **BART (Lewis et al., 2020)**: This model has an encoder-decoder setup with 12 layers each and a size of 1024. On the CNN/DailyMail task, it takes 2.4 seconds to process and gives a speedup of 7.7x compared to a baseline.\n\n- **DistilBART (Wolf et al.)**: This one is similar to BART but smaller—12 layers for the encoder and 6 for the decoder, still 1024 in size. It's a bit faster at 3.4 seconds and offers a speedup of 5.4x.\n\n- **ProphetNet (Qi et al., 2020)**: Another encoder-decoder model with 12 layers each and 1024 size. It takes 2.8 seconds and gives a speedup of 3.8x.\n\n- **T5 (Raffel et al., 2020)**: This model has an interesting setup—12 layers for the decoder and none for the encoder, with a size of 768. It takes 3.0 seconds and offers a speedup of 5.5x.\n\n- **UniLM (Dong et al., 2019)**: This one is an encoder-only model with 12 layers and a size of 768. It's the fastest at 1.7 seconds and gives a huge speedup of 9.6x.\n\nSo, basically, these models are all trying to summarize articles quickly, and each has its own way of doing it, with different speeds and efficiencies.",
        "formal_text": "Since each token needs to be read multiple times Model Architecture Task Baseline FastSeq Speedup encoder-decoder architecture BART (Lewis et al. , 2020) 12L-12L-1024 CNN/DailyMail 2. 4 18. 4 7. 7x DistilBART (Wolf et al. ) 12L-6L-1024 CNN/DailyMail 3. 4 18. 5 5. 4x ProphetNet (Qi et al. , 2020) 12L-12L-1024 CNN/DailyMail 2. 8 10. 7 3. 8x T5 (Raffel et al. , 2020) 12L (Radford et al. , 2019) 0L-12L-768 CNN/DailyMail 3. 0 16. 7 5. 5x encoder-only architecture UniLM (Dong et al. , 2019) 12L-0L-768 CNN/DailyMail 1. 7 16. 4 9. 6x",
        "GPT2_formal_text": "(h) = (h, s, w, t) - l * tanh(1 + e^(-1/ν)) [1] Figure 2 shows how the latent variable from Equation 2 is represented by the output at time step t. Formal: So, the attention weights for the s-th word in the input sentence are equal to the attention weights for the words before it, and for the t-th word, they're equal to the attention weights for the words before the t-th word. Formal: Let's say h_t is the hidden representation at time step t, and s_t is the hidden representation for the word before the time step. Formal: A mix of word embeddings x_t_i (t_i) and word embeddings y_t_i (t_i) in a hidden space. Formal: A vector of attention weights α_t_i (t_i) that works with the s-th word w_t. Formal: The attention weights α_t_i (t_i) for the s-th word w_t, which are calculated using the output at time step t. Formal: Here's the formula for how the latent variable x_t_i (t_i) is represented in the hidden space. Formal: Formal: A mix of word embeddings x_t_i (t_i) and word embeddings y_t_i (t_i) in a hidden space. Formal: A vector of attention weights α_t_i (t_i) that works with the s-th word w_t. Formal: The attention weights α_t_i (t_i) for the s-th word w_t, which are calculated using the output at time step t. Formal: Finally, the attention weights α_t_i (t_i) for the s-th word w_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-demo.26.json"
    },
    {
        "casual_text": "Manish Shrivastava and his team came up with a system in 2008 that uses something called a Hidden Markov Model for Hindi. They used a stemmer as a kind of preprocessor to figure out the root of words. This system was built with 18 different parts of speech tags and managed to get an accuracy of 93.12%. \n\nThen, in 2011, Sanjeev Kumar Sharma and his group created a system using the same Hidden Markov Model to make the Punjabi Part of Speech tagger more accurate. They made a module that takes the output from an existing POS tagger and assigns the right tag to words that could have more than one tag. They tested it on a big chunk of text with 26,479 words and got an accuracy of 90.11%.",
        "formal_text": "Manish Shrivastava et al. , 2008 developed a system using Hidden Markov Model for Hindi. The System uses stemmer as a preprocessor to find the root of the words. The system was developed using 18 different pos tags and system achieved the accuracy of 93. 12%. Sanjeev Kumar Sharma et al. , 2011 developed a system using Hidden Markov Model to improve the accuracy of Punjabi Part of Speech tagger. A module has been developed that takes output of the existing POS tagger as input and assign the correct tag to the words having more than one tag. The system was evaluated over a corpus of 26, 479 words and system achieved the accuracy of 90. 11%.",
        "GPT2_formal_text": "= [y1, y2, ..., yn] label = [y1, y2, ..., yn] label_s = [y1, y2, ..., yn] y_s = [y1, y2, ..., yn] y = [y1, y2, ..., yn] y_t = [y1, y2, ..., yn] y_t = [y1, y2, ..., yn] y_t = [y1, y2, ..., yn] y_t = [y1, y2, ..., yn] y_t = [y1, y2, ..., yn] Formal: Basically, this means the answer is either y_t or y. Formal: In the example in Figure 1, the labels y_t and y_t are labeled as y1, y2, ..., yn, respectively. Formal: In the example in Figure 1, the labels y_t and y_t are labeled as y1, y2, ..., yn, respectively. Formal: The first two steps give you two ways to create responses. Formal: The first step gives you two options to create responses: y1 and y2. Formal: The second step gives you two options to create responses: y1 and y2. Formal: The third step gives you three ways to create responses: y1, y2, and yn. Formal: Lastly, the final method gives you a response of y. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C12",
        "filename": "C12-3021.json"
    },
    {
        "casual_text": "We found that names from minority groups show up way less often than names from majority groups in the training data for neural language models. Because of this, these names are less likely to be treated as single tokens, get less context, and end up with overfit representations in the models.",
        "formal_text": "We show that names predominantly belonging to members of minority social groups occur less frequently than majority group names in training corpora of neural language models, and that low frequency results in lower rates of single tokenization, less contextualization, and overfit representations in neural language models.",
        "GPT2_formal_text": "ed) and the [CLS] token. There's also a basic BERT model, which is pre-trained on the same dataset as BART and ALBERT. Formal: We use cross-entropy loss, which is the standard loss for most text classification tasks. Formal: We train the model on a set number of labeled examples, specifically 50, using a batch size of 8. We also fine-tune the model for 10,000 epochs on a separate development set. Formal: The training for [CLS] is on the same dataset as BART and ALBERT. Formal: We use an ensemble of the trained model and a special \"cross-entropy loss\" approach, which gives us better performance. Formal: We also fine-tune the model for 10,000 epochs on a separate development set, following the same setup as the original paper. Formal: We also fine-tune the model for 10,000 epochs on a separate development set, following the same setup as the original paper. Formal: Finally, we run the model on a separate development set to check its performance on this new task. Formal: We train the model on a set number of labeled examples, specifically 50, using a batch size of 8. We also fine-tune the model for 10,000 epochs on a separate development set. Formal: We also fine-tune the model for 10,000 epochs on a separate development set, following the same setup as the original paper. Formal: Lastly, we run the model on a separate development set to check its performance on this new task. Formal: We train the model on a set number of labeled examples, specifically 50, using a batch size of 8. We also fine-tune the model for 10,000 epochs on a separate development set. Formal: We also fine-tune the model for 10,000 epochs on a separate development set, following the same setup as the original paper. Formal: Finally, we run the model on a separate development set to check its performance on this new task. Formal: We train the model on a set number of labeled examples, specifically 50, using a batch size of 8. We also fine-tune the model for 10,000 epochs on a separate development set. Formal: Finally, we run the model on a separate development set to check its performance on this new task. Formal: We",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.41.json"
    },
    {
        "casual_text": "Like earlier studies found, and like what we saw too, getting data for this kind of thing is pretty tough, especially when you’re trying to label whole speeches on a big scale. It’s a lot of work. To make it easier for people to study this, we changed the task a bit and set it up as a debate. We also made a special dataset for it. We gathered and are sharing over 3,600 debate speeches that are all labeled for this task.",
        "formal_text": "As previous studies have shown, and consistent with our own findings, obtaining data for such a task is difficult, especially considering that labeling at scale of full speeches is an arduous effort. To facilitate research of this problem, we recast the proposed general task in a defined debate setup and construct a corresponding benchmark data. We collected, and release as part of this work, more than 3, 600 debate speeches annotated for the proposed task.",
        "GPT2_formal_text": "). Since there are no mappings for every pair of entities, we use a Support Vector Machine (SVM) to figure out the joint distribution. This SVM uses a few features, like the Euclidean distance between the entities, to make its prediction. We set the hyperparameter α to 0.1 to improve performance on unseen data. Formal: So, we start by building a knowledge graph G = {G 1, . . . , G m } by randomly picking a directed edge from the set E. In this graph, entities in different entities are connected by edges that have weights. These weights, or labels, can be learned or tuned during training and testing. This setup makes it possible to easily add new connections between entities, which is especially useful for entity relation extraction. Formal: The edges in the graph are set up with a total of eight types. These include two types for the names of entities (called \"descriptors\" by Blei et al., 2003), and two types for the types of relationships between them (like \"entailment\" and \"entailment-pair\"). Formal: Entity relation extraction is all about finding the entities (or relations) in the knowledge graph that are connected to specific entities in the sentence. Formal: In this part, we talk about three techniques to process entity knowledge graphs. We use a graph neural network (GNN) to create relation embeddings E = {E 1, . . . , E m }. Then, we calculate the conditional probability P(e|f ) by taking the logarithm of the probability of entity e and relation f given the context of the sentence, e. Formal: The final learned embeddings are then used to figure out the probabilities for other entity types and relation types in the knowledge graph. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "The translation you got for that word is basically right, but it doesn't fit the situation you're using it in (it's the wrong meaning of the word).",
        "formal_text": "The obtained translation for the given word is in principle correct, but not in the given context (word sense error).",
        "GPT2_formal_text": "ing\n        Casual: To keep things simple, we're not diving into all the nitty-gritty details of how we handle the re-ranking of candidates or how we pick the top result. Formal: The final representation for a sentence, which we'll call s, comes from combining the input vectors. The main thing we're focusing on is how to make sure the final representation matches up with the input values. Formal: We use a linear transformation to combine the input vectors, following the approach from (Li et al., 2015). Formal: The input vectors for each candidate are then used to create the final representation for the candidate. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "conll",
        "filename": "2020.conll-1.19.json"
    },
    {
        "casual_text": "We picked the dataset from Tzioumis (2018) because it covers a lot of ground—about 85.6% of the U.S. population, according to Tzioumis. It’s also anonymized in an ethical way, has at least 30 observations for 91.2% of the names included, and uses first names based on self-identification, which Larson (2017) called the \"gold standard\" for figuring out demographics, especially when it comes to gender labeling. The dataset uses the same racial categories as the U.S. census for surnames, so it fits well with our cultural context. \n\nOne thing to note is that some names can belong to more than one racial or gender group, which could add some noise to our analysis. But, over 80% of the names we looked at have a self-identification rate of at least 70% with a single racial group in Tzioumis’ dataset. \n\nFrom the Social Security Administration (SSA) data, we found that 31.3% of the names in our study are only used by males, 38.6% are only used by females, and 30.1% are used by both. For names that appear in both genders, 88% have at least 70% of their occurrences tied to one gender. So, labeling a name based on the group it’s most common in should help us see which groups are most affected by our findings and also highlight the linguistic clues about race and gender that make first names a good way to study this stuff.",
        "formal_text": "We choose the dataset of Tzioumis (2018) because it has wide coverage (85. 6% of the U. S. population as estimated by Tzioumis); is ethically anonymized; is based on at least 30 observations for 91. 2% of included names; and contains first name data based on self-identification, considered a \"gold standard\" for demographic classification, as noted by Larson (2017) in the context of labeling data by gender. Tzioumis' dataset uses the same racial categories as U. S. census data for surnames, indicating that these categories correspond to our cultural context. That many names occur in more than one racial group or gender group is also a limitation, and may introduce noise into our analyses. However, more than 80% of names examined in our work have a self-identification rate of at least 70% with a single racial group in the Tzioumis dataset. Based on SSA data, 31. 3% of the names in our study have only male occurrences, 38. 6% of the names in our study have only female occurrences, and 30. 1% of names occur for both male and female individuals. 88. 0% of names with male and female occurrences have at least 70% of occurrences associated with one gender. Thus, assigning a label based on the group with the most occurrences is likely to capture both which groups are most affected by our findings, as well as the linguistic signals related to race and gender which make first names a useful proxy to this information.",
        "GPT2_formal_text": ", e1 = [], e2 = [], e3 = [], e4 = [], e5 = [], e6 = [], e7 = [], e8 = [], e9 = [], f = [], g = [], h = [], i = [], j = [], k = [], l = [], m = [], n = [], o = [], p = [], q = [], r = [], s = [], t = [], u = [], v = [], w = [], x = [], y = [], z = [],\n\nBasically, it's a mix of functions that take in inputs and return some output, all wrapped up in a single set of numbers. Formal: We're going to build a phrase-based SMT system using these formulas. Formal: The scores in the formulas are like the representations of the phrases, which we'll use to create translations for the phrases we get back. Formal: The scores in these formulas are like the representations of the phrases, which we'll use to create translations for the phrases we get back. Formal: The scores in these formulas are like the representations of the phrases, which we'll use to create translations for the phrases we get back. Formal: The scores in these formulas are like the representations of the phrases, which we'll use to create translations for the phrases we get back. Formal: The scores in these formulas are like the representations of the phrases, which we'll use to create translations for the phrases we get back. Formal: The scores in these formulas are like the representations of the phrases, which we'll use to create translations for the phrases we get back. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.41.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way.\n\nBasically, Φ(d) is like a big table or matrix that shows how important different words are in a document (d). Each row in the table represents a different word, and each column represents a different document. The numbers in the table are calculated using something called \"tf-idf,\" which stands for \"term frequency-inverse document frequency.\" This helps figure out how important a word is in a specific document compared to all the other documents.\n\nSo, Φ(d) looks something like this:\n\n```\n[ tf-idf(b1) ]\n[    ...     ]\n[ tf-idf(bN) ]\n```\n\nAnd then, we take this table and multiply it by the tf-idf values for the specific document (d) we're looking at. This multiplication helps us focus on the words that are most important in that particular document.\n\nIn short, Φ(d) = X^T • tf-idf(d), where X^T is the table we talked about, and tf-idf(d) is the importance of words in document d.",
        "formal_text": "Φ(d) =     − −− → tf-idf(b1) T. . . − −− → tf-idf(bN ) T     − −− → tf-idf(d) = X T • − −− → tf-idf(d)",
        "GPT2_formal_text": "ed: {0, 1} Formal: The output from the attention is turned into a vector using a non-linear function (like a sigmoid) and then fed into a fully connected layer. Formal: Since the output from the attention is non-linear and doesn’t depend on the input, we use a linear layer to figure out the attention value. Formal: We tweak the attention layer to focus on the words in the knowledge base and the knowledge graph that are most similar to the target word w_i. We use two linear layers to make sure the attention is diverse, and this tweak is repeated for the final layer. We use a linear layer with a softmax activation function (like the one in Eq. 1) to create the attention representation. Formal: We generate a z-dimensional attention vector for each target word w_i using the fully connected layer. Formal: We use a linear layer with a softmax activation function (like the one in Eq. 1) to get the attention representation for the target word w_i. Formal: For the target word w_i, we use a modified version of the knowledge distillation (KD) loss (from Liu et al., 2018) to minimize the cross-entropy loss. Formal: We add the negative log of the cross-entropy loss for each query g_i to the log loss. Formal: We also add the negative log of the cross-entropy loss for the knowledge graph query g_i to the log loss. Formal: The knowledge distillation loss, which we calculate using a stochastic gradient descent (SGD), is then passed through a fully connected layer with a sigmoid activation. Formal: We use a ReLU activation to normalize the output vector. Formal: The attention vector we get for each target word w_i is passed through a fully connected layer with a sigmoid activation. Formal: To learn how to extract knowledge, we use a BiLSTM (bi-LSTM) with a self-attention mechanism (Vaswani et al., 2017) with an LSTM kernel size of 5 layers. Formal: Lastly, we combine all the representations we get from the knowledge base, knowledge graph, and knowledge text into one final representation for the target word w_i. Formal: The goal of knowledge distillation is to pull knowledge from the knowledge base while keeping the loss small.",
        "directory": "D13",
        "filename": "D13-1179.json"
    },
    {
        "casual_text": "Bleu seems to struggle with telling the difference between random translation variations, which makes you wonder if it really matches how humans judge translation quality in some situations. The more identical scores you get from different versions, the less likely it is that all of them would feel equally good to a human. This is more of a theoretical issue, since the examples are made up, but it does show that Bleu isn't exactly a super accurate way to measure translation quality. There are a few key reasons why Bleu isn't that great:",
        "formal_text": "Bleu's inability to distinguish between randomly generated variations in translation hints that it may not correlate with human judgments of translation quality in some cases. As the number of identically scored variants goes up, the likelihood that they would all be judged equally plausible goes down. This is a theoretical point, and while the variants are artificially constructed, it does highlight the fact that Bleu is quite a crude measurement of translation quality. A number of prominent factors contribute to Bleu's crudeness:",
        "GPT2_formal_text": "= { h_i } for i = 1, n - 1. Then, the query and the response are combined using the softmax function, which gives us a single representation for the whole pair. Formal: To make a good estimate of how much the input and the response are related, we use something called mutual information. Formal: We're looking at how often both the input and the response have the same label. We calculate this using MLE, which is a method from (Ross and Sidner, 1998). Formal: We pick the label that has the highest mutual information, which we call label_ij. Formal: We also calculate a \"relevance score\" for each query, which is the average of the scores from both the query and the response. Formal: We set a threshold of 0.3 to pick a query that gets the highest relevance score. Formal: We calculate the relevance score for a query by counting the number of times both the query and the response appear together in the training data. Formal: After we've got the relevance score for the query, we combine the query and the response using the softmax function. Formal: Next, we calculate the average relevance score for all the documents where both the query and the response appear. Formal: After the query and the response are combined, we create a matrix called u_i where all the scores are positive and for each document, the relevance score for both the query and the response is greater than 0. Formal: We adjust this matrix to get the best ranking for the data. Formal: Finally, we calculate the total relevance score for the whole set of documents. Formal: Formal: Formal: We take the average relevance score for all documents where both the query and the response appear. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E06",
        "filename": "E06-1032.json"
    },
    {
        "casual_text": "After a product has been around for a while, companies often come up with different versions of it to keep their spot in the market. They do this to attract customers who have specific needs. A common example is \"light\" versions of food brands. Usually, these new versions have names that are similar to the original brand but with a little extra word to show what kind of variation it is, like \"mini babybel\" or \"philadelphia light.\" We went through and listed out 11 common prefixes or suffixes and checked how often they show up with different food items.",
        "formal_text": "Once a product has established itself on the market for a substantial amount of time, many companies introduce variants of their brand to further consolidate their market position. The purpose of this diversification is to appeal to customers with special needs. A typical variant of food brands are light products. In many cases, the names of variants consist of the name of the original brand with some prefix or suffix indicating the particular type of variant (e. g. mini babybel or philadelphia light). We manually compiled 11 affixes and check for each food item how often it is accompanied by one of them.",
        "GPT2_formal_text": "ing-as-text-construction is a specific method that was created specifically for this task. It can spot the actual text in the feedback and also find the parts that make up the feedback. For the generation part, it does a good job of making the feedback coherent and match the original text better. This helps boost the overall quality of the feedback. Formal: We wanted to see how well our model can handle different types of feedback, so we asked three people to judge each generated feedback. They labeled it as \"good\" if the feedback was relevant and \"not good\" if it wasn’t. This shows how well our model can make recommendations based on the feedback. Formal: The feedback generation model has two main parts: (i) a recurrent neural network that creates the feedback, and (ii) a sentence encoder that picks out the important parts from the feedback to create a summary. We tested both the generated feedback and the summary to see how well it does. Formal: We trained the model using data from the test set and used it on the validation set to see how well it could handle future test data. Formal: The same three people who rated the feedback also gave their feedback to the model. They marked the feedback as \"good\" if it was relevant and \"not good\" if it wasn’t. We used this feedback to adjust the model’s parameters and train it further. Formal: We only tested the model on the validation set to check how well it can handle new, unseen data. Formal: To understand how well it can generate feedback, we created two ways to judge it. Formal: In the first round of evaluation, we used the final generated feedback, the summary, and the model’s parameters as the only input. The feedback was judged using a special metric called ROUGE-1, which uses a weighted average of ROUGE-2 scores to measure how well the feedback fits the text. Formal: In the second round, we used the generated feedback and the summary as the input, but we only used the model’s parameters. The feedback was judged using a weighted average of ROUGE-2 scores to measure how well the feedback fits the text. Formal: We ran this experiment five times and averaged the results to get a final score. Formal: We tested this on the same dataset and compared the results to the original example from the validation set. Formal: Check out Table 2 for the",
        "directory": "C14",
        "filename": "C14-1216.json"
    },
    {
        "casual_text": "Figure 3 shows how our systems perform at different points, and Table 3 gives the results for our best setups on the ENNI development and test sets. Interestingly, it turns out that neither the feature set we use nor whether mazes are included really impacts the system's performance. This is quite different from Microsoft Word's grammar checker, which doesn't work well when mazes are part of the data. The system by Morley et al. (2013) can handle mazes, but it still doesn't perform as well as the one we're proposing.",
        "formal_text": "The plots in Figure 3 show how the performances of our systems at different operating points vary, while Table 3 shows the performance of our best system configurations on the ENNI development and test sets. Surprisingly, we see that neither the choice of feature set, nor the presence of mazes has much of an effect on system performance. This is in strong contrast to Microsoft Word's grammar check, which is minimally effective when mazes are included in the data. The Morley et al. (2013) system is robust to mazes, but still performs substantially worse than our proposed system.",
        "GPT2_formal_text": ", then this is what the model spits out: Formal: To figure out the document collection, we start by picking the top-1 or top-2 documents that have the highest scores in the MIMIC-3 test set. The documents are randomly chosen from the training set and are picked based on how confident the model is in its guesses (like in the MIMIC-3 test set). Formal: Once we have the document collection, we first update the model by using the loss function we talked about. Formal: Next, we grab the top-k documents from the collection based on the model's confidence score. Formal: From the top-k documents, we pick the top-k with the highest scores. Formal: Finally, we sort all the documents from the collection into a final document set. Formal: Lastly, we use a method called residual networks to figure out the document embedding. Formal: Check out Figure 2 for an example of how we calculate the document embedding for a particular document, D_i, using the document collection. Formal: We've got a loss function, L_i, that's calculated based on this document collection. Formal: We use a beam size of 3 and the softmax function to calculate the document embedding. Formal: Finally, we train the model using MIMIC-3 training data. Formal: We give each word in the document d_i a score based on its embedding and then update the model's probability, p_d_i, using the document collection embedding. Formal: Our method uses a knowledge-based approach and focuses on document embedding. Formal: We use a bidirectional LSTM (BiLSTM) to generate each word in D_i, which helps us learn the embedding for D_i. Formal: Finally, we train the model using MIMIC-3 training data. Formal: Figure 1 has an example of how we calculate the document embedding using the document collection embedding. Formal: To get the document embedding, we use a method called residual networks. Formal: We use a document-level BiLSTM (BiLSTM) to learn the embedding for the document collection. Formal: We also use a bidirectional LSTM (BiLSTM) to calculate the document embedding for the collection. Formal: Formal: Formal: Formal:",
        "directory": "D14",
        "filename": "D14-1106.json"
    },
    {
        "casual_text": "A QA sample has a question, a passage, and an answer span. Sometimes, there might be more than one part of the passage that matches the answer. To keep things simple, most baseline codes just use the first match they find for training. But, when you think about the context and the meaning of the question and answer, some parts of the passage are more likely to be the right answer than others.\n\nTo find the best possible answer span, we use something called a universal sentence encoder (Cer et al., 2018) to turn the question and the sentences in the passage into fixed-size vectors. We then pick the part of the passage that’s most similar to the question based on cosine similarity and call it the golden span.\n\nIn our tests, this method helped improve performance on some datasets, but on others, it actually made things worse.",
        "formal_text": "QA sample consists of a question, a passage, and an answer span. There could exist multiple answer spans because more than one phrase in the passage can be matched with the answer text. For simplicity, only the first occurrence of answer text is used for training in most of the baseline codes. However, considering context and semantic of the given question and answer, a certain phrase in the passage is more likely to be plausible answer span relevant to the question. In order to find the most plausible answer span, a question and sentences in the passage are encoded into fixed-size vectors with universal sentence encoder (Cer et al. , 2018). We choose the span in a sentence, which is the most similar to the question in terms of cosine similarity, as golden span. In our experiment, this approach boosts up the performance of some datasets but degrades the performance a lot in the other datasets.",
        "GPT2_formal_text": "_type = 0.1 # Set the attention mask type to 0.1. Formal: Using this setup, we break down the attention and attention weight vectors for the outputs in the same way as in Equation (3). Formal: We calculate the attention weight vector, y_t, using the inverse projection of the attention layer from the teacher model. Formal: To find the next hidden state, y_t, we just subtract the output layer's hidden state from the teacher's hidden state. Formal: We use the softmax function to turn the attention weight vectors into probabilities. Formal: After the softmax layer, we calculate the attention weight vector by multiplying the teacher's hidden state with the input layer's hidden state. Formal: The final hidden state, y_t, is then calculated by taking the sum of the teacher's hidden state and the input layer's hidden state. Formal: We wrap up by using the logistic regression function to estimate the probability of the next word, y_t. Formal: We use an autoencoder to predict the next word, y_t, using the output layer's hidden state, h_t. Formal: Formal: We use a linear layer with softmax to predict the probability of the next word, y_t, using the input layer's hidden state, h_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-5826.json"
    },
    {
        "casual_text": "Most automatic summarization methods these days are extractive, meaning they just pull out sentences from the original text based on things like word patterns or sentence structure. They rank or score the sentences and then grab the top ones, with not much done to tweak them afterward (Yih et al., 2007; Wan et al., 2007; Wang et al., 2008; Wan and Xiao, 2009). But this approach has its limits compared to abstraction, which can create new sentences that capture the main ideas better (Carenini and Cheung, 2008).",
        "formal_text": "Most automatic summarization approaches are extractive which leverage only literal or syntactic information in documents. Sentences are extracted from the original documents directly by ranking or scoring and only little post-editing is made (Yih et al. , 2007; Wan et al. , 2007; Wang et al. , 2008; Wan and Xiao, 2009). Pure extraction has intrinsic limits compared to abstraction (Carenini and Cheung, 2008).",
        "GPT2_formal_text": "= { u, e, t }      Casual: We tried out different types of word embeddings and labeled them based on how often they show up in the training data. For language pairs that don’t have a lot of examples, we just stick with the basic word embedding setup we talked about in Section 4.3.1. For those that have a lot of examples, we add some extra word embedding features based on the total number of instances in the training data. Formal: We’ve got a pretrained BERT model (from Devlin et al., 2019) that we fine-tuned on the News Commentary dataset (thanks to Duchier et al., 2019). Formal: We also used our own BERT model (called BERT-base) and added a few word embedding features to it. Formal: To make things easier to understand, we used the same BERT model to predict the labels for both the positive and negative examples (check out Figure 1 for more details). Formal: Figure 1: We’ve got a pretrained BERT model (from Devlin et al., 2019) that we fine-tuned on News Commentary. Formal: We’ve also created a separate BERT model (called BERT-base) that has five different word embedding features that we use to predict the labels for both the positive and negative examples (check out Figure 1). Formal: In Table 4, we show the results for the development sets of the CNN, CNN-LSTM, and BERT models. The BERT model trained on news data was trained on data that had over 300K instances, which is the training set for the CNN and CNN-LSTM models. Formal: BERT-base and BERT-base-prop were trained on different sets of data with different numbers of positive and negative examples. Formal: For the BERT model trained on news data, we used the same setup as BERT-base to predict the labels for both positive and negative examples. Formal: Lastly, in Table 5, we report how well the CNN model did on the dev set. Formal: So, in short, we tested a bunch of different models on different news datasets and found that BERT can work really well in a situation where we don’t have a lot of training data. Formal: This project is a follow-up to (Cai et al.,",
        "directory": "D15",
        "filename": "D15-1219.json"
    },
    {
        "casual_text": "We've updated the neural L-PCFG model to include the idea of concreteness by adding a couple of concrete-related features. Figure 3b shows the source (captions) and target (semantic role labels) for the alignment method we talked about in Figure 3a. Words in parentheses are the stop words we took out from the original caption. We used SpaCy's default list of stop words for English (Honnibal et al., 2020). The labels are arranged with the first term being the predicted activity, followed by the entities involved in that activity (Yatskar et al., 2016). The alignment pairs are listed from the highest score to the lowest.",
        "formal_text": "We extend the neural L-PCFG model to incorporate the concept of concreteness by adding two concrete- Figure 3b shows the source (captions) and target (semantic role labels) for the alignment method of Figure 3a. Words in parentheses are the stop words removed from the original caption. We use the SpaCy default list of stop words for English (Honnibal et al. , 2020). The labels are ordered by the first term as the predicted activity followed by the set of the entities involved in the activity (Yatskar et al. , 2016). The alignment pairs are ordered from the highest to the lowest scores.",
        "GPT2_formal_text": "ed (         Casual: When we're working with a specific query, we grab the embedding of that query, which we call ȳ, from a set of connected word embeddings called U. Formal: For each query Q, we create a matrix W by combining all the query embeddings. Formal: We take the word embeddings from all the query embeddings and use them as the query's embedding to train our model. Formal: After that, we calculate the query's hidden state by turning it into a dense vector. Formal: Finally, we use this updated query vector to update the embedding of the query. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "conll",
        "filename": "2021.conll-1.2.json"
    },
    {
        "casual_text": "This project has a lot of room for expansion. Here's what we're thinking:\n\n(i) We want to look into more functions for τ to improve how we balance exploration and exploitation.\n\n(ii) We need to figure out better ways to assign nuclearity, especially since our evaluation showed too much N-N classification.\n\n(iii) We plan to test our approach on more sentiment datasets, like the one from Diao et al. (2014), and build even bigger treebanks.\n\n(iv) Our scalable solution could be expanded to predict discourse relations in addition to structure and nuclearity.\n\n(v) We’re thinking of combining our large-scale treebank with a neural discourse parser, like the one by Yu et al. (2018), to really make the most of data-driven discourse parsing.\n\n(vi) With the new MEGA-DT corpus, we want to revisit discourse-guided sentiment analysis to improve systems, especially for longer documents.\n\n(vii) Long-term, we’re interested in exploring other tasks like summarization, question answering, and machine translation for distant supervision of discourse. There’s a lot of annotated data available for these tasks (e.g., Nallapati et al. (2016); Cohan et al. (2018); Rajpurkar et al. (2016, 2018)).\n\nWe’re also doing a qualitative analysis of the generated discourse trees to see how they’re shaping up.",
        "formal_text": "This work can be extended in several ways: (i) We plan to investigate into further functions for τ to enhance the exploration-exploitation tradeoff. (ii) Additional strategies to assign nuclearity should be explored, considering the excessive N-Nclassification shown in our evaluation. (iii) We plan to apply our approach to more sentiment datasets (e. g. , Diao et al. (2014) ), creating even larger treebanks. (iv) Our new and scalable solution can be extended to also predict discourse relations besides structure and nuclearity. (v) We also plan to use a neural discourse parser (e. g. Yu et al. (2018) ) in combination with our large-scale treebank to fully leverage the potential of data-driven discourse parsing approaches. (vi) Taking advantage of the new MEGA-DT corpus, we want to revisit the potential of discourse-guided sentiment analysis, to enhance current systems, especially for long documents. (vii) Finally, more long term, we intend to explore other auxiliary tasks for distant supervision of discourse, like summarization, question answering and machine translation, for which plenty of annotated data exists (e. g. , Nallapati et al. (2016); Cohan et al. (2018); Rajpurkar et al. (2016 Rajpurkar et al. (, 2018 ). A Qualitative Analysis of Generated Discourse Trees",
        "GPT2_formal_text": "= {1, 2, ..., n-1} s = (1, 2, ..., n-1) h = (1, 2) 1. The size of the internal state is determined by the inner product of the two special vectors. Formal: Formal: The function g(u) is defined using something called the Frobenius norm. This norm is a type of non-linear function that has a non-negative value. If we use a linear mapping, it's just a linear projection of g(u). Formal: Basically, for a sentence s, which has words w 1, ..., w n, we take the inner product of the two special vectors (g(u) and g(n)), and then use that result to calculate the embedding of s. Formal: The output layer of a model can be thought of as a way to map this mapping into a neat vector space, where each vector is a specific type of vector. Formal: The average of these special vectors (let's call it g(u)i) shows how often a word w i is found in the vocabulary V. Formal: Similarly, the average of all the special vectors (g(n)i) shows how often a word w n appears in the vocabulary V. Formal: The embedding of a word w is calculated by the combination of these two special vectors. Formal: Formal: Formal: We can also calculate the embedding of a word w using the projection of g(u)i onto a special space called G. Formal: Formal: The embedding for a sentence s, which is a combination of g(u)i, can be written as u = (1, 2, ..., n-1) * G. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.603.json"
    },
    {
        "casual_text": "Dense video event captioning (Krishna et al., 2017) and multi-modal video event captioning (Iashin and Rahtu, 2020b) are about creating a series of captions for all events in a video, whether it's just from the video itself (uni-modality) or from both the video and speech (multi-modality). Figure 1 shows an example of how tricky this task can be, especially when dealing with both visual and speech elements.\n\nFor the visual part, recognizing small or detailed objects can be really tough because of things like ambiguity, being hidden (occlusion), or changes in the object's state. In the example from Figure 1, the object \"dough\" is hidden in event 1, making it hard to spot just from the video. But if you look at the previous video frame, it's easier to recognize because it's clear and visible there.\n\nWhen it comes to speech, even though it provides useful semantic info (Shi et al., 2019; Iashin and Rahtu, 2020b), it also brings its own challenges. Speech can be informal, leading to issues like co-reference and ellipsis (where words are left out). In Figure 1, for event 3, the word \"dough\" is missing in the speech text, but it can still be figured out by looking at the context from other events, like event 1 in this case.\n\nIn short, both the immediate surrounding clips and the overall context between events are super important for creating captions that make sense and don't repeat the same stuff over and over.",
        "formal_text": "Dense video event captioning (Krishna et al. , 2017) and multi-modal video event captioning (Iashin and Rahtu, 2020b) aims to generate a sequence of captions for all events regarding to uni-modality (video) or multi-modality (video + speech) inputs. Figure 1 presents a showcase, which demonstrates the challenges of this task from both vision and speech text perspective. For vision understanding, the fine-grained objects are hard to recognize due to ambiguity, occlusion, or state change. In this case, the object \"dough\" is occluded in event 1 and is hard to recognize from the video. However, it can be recognized from the previous neighbor video frame with a clear appearance. From speech text perspective, although the speech text offers semantic concepts (Shi et al. , 2019; Iashin and Rahtu, 2020b), it brings another challenge of co-reference and ellipsis in speech text due to the informal utterance of oral speeches. In the case of Figure 1, the entity \"dough\" in event 3 is an ellipsis in the text. Nonetheless, it is capable of generating consistent objects \"dough\" in event 3 with the contextual information from other events such as event 1 in this example. To sum up, both local neighbor-clip and global inter-event contexts are important for event-level captioning to generate coherent and less duplication descriptions between events.",
        "GPT2_formal_text": ": how much the previous and next word are actually connected. This is figured out by dividing the number of tokens in the input by the total number of tokens in the sentence. Formal: To make things easier, let's define the pre-training loss for the loss function L(s, t) as: Formal: To get the hidden representation of the sentence s, we use the cross-entropy loss to get a h(s) = (u(s) − U(s)) + b(s). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.156.json"
    },
    {
        "casual_text": "I labeled the synthetic and DBDC3 dialogues myself, using the ISO 24617-2 annotation scheme to mark the dialogue control functions. Since this project is all about finding common patterns in dialogue sequences, I mixed different types of conversations—like task-focused ones and casual chats—as well as different kinds of participants, such as human-to-human and human-to-machine interactions. Check out Figure 1 for the process I used to figure out dialogue patterns and create natural-flowing conversations for DMS.",
        "formal_text": "The synthetic dialogues and DBDC3 dialogues were hand labelled by the author with dialogue control functions following the ISO 24617 − 2 annotation scheme. Since, this work is aimed towards extracting generic sequences hence, we combined different domains (taks-driven and chit-chat) and participant types (human-human, human-machine). Figure 1: The workflow to obtain dialogue patterns for sequencing dialogues to build natural flows in DMS.",
        "GPT2_formal_text": "= [q in q i ] for q in q i, where q i = k p i (q). Formal: There are two main ideas behind how we use the Multi-Perspective Attention Network (MPAN) to figure out the relationships between points in a photo. Formal: First, the attention weights are based on the visual features and they get updated as the image gets closer to the question. Formal: Second, the attention weights are adjusted for different distances between points. Formal: The attention weights for each point are calculated based on how close the question is to the image, and the attention weights for each relation are calculated based on how related the image and question are. Formal: Then, the attention weights are adjusted to calculate the probability P i (q|q i ) for a specific relation r, and the attention weights for each relation are adjusted to calculate the probability P i (r|q i ) for the relation r in the image. Formal: Finally, we use a softmax function to make the probabilities for each relation appear as a straight line. Formal: The Multi-Perspective Attention Network (MPAN) is a fancy way of using something called Multi-Layer Perceptrons (MLP) to get extra info from the visual stuff around the question and answer. It’s designed to help us understand the relationships between points in a photo and how they connect to different things in the image. Formal: Formal: Formal: In this paper, we’re focusing on two types of relationship connections. Formal: The first type is just the main connection between the question and the answer, like when \"a\" shows up in the answer and \"b\" in the question. Formal: The second type is the connection between points in the question and the answer, like when \"a\" appears near \"b\" or \"b\" near \"a\". Formal: Formal: If the main connection (the main connection) is strong, we can think of the whole image as one big family. Formal: Formal: Formal: Formal: For this paper, we’re using the standard setup for using MLP to predict relationships. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "codi",
        "filename": "2020.codi-1.2.json"
    },
    {
        "casual_text": "Alright, so when we have a new sentence, the system needs to understand it and update the conversation model (DM). There are some rules and methods, like plan inference rules [A1179] and constraint satisfaction [LA87, Car87], that help figure out what the sentence might be part of and suggest a sequence of actions. Plus, there are some tricks called forecasting heuristics [Car87, SidSl] that help prioritize these actions to make the conversation flow smoothly. For instance, the semantic representation of sentence (1) would look something like this:",
        "formal_text": "Given the senlalltic representation of a new utterance, the system mnst assimilate tim utterance and produce an updated dialogue model (DM). Plan inference rules [A1179] and constraint satisfaction [LA87, Car87] suggest chains of higher level actions that an utterance may in! part of, and foetlsing heuristics [Car87, SidSl] order these inference paths according to coherence. For exanlple, the semantic rel)resentation of (1) is:",
        "GPT2_formal_text": "is the negative log probability of the transition, while attention_map_len is the number of words in the source sentence. Formal: For the input sequence x = x1, x2, ..., xT, the state vector has the following shape: Formal: We create a transition matrix called T, which looks like this: Formal: We set the hyperparameters r = {r₁, ..., rₙ} to 1e-3 and λ = 1e-5 to account for the length of the input. Formal: We use a linear kernel with a learning rate of 0.001 for each dimension. The activation function is a dot product with the log probability. To get the final state vector h(t), we just average the state vectors T i (t) with the one that came before it. Formal: We use the threshold α to decide if the next word should be linked to the one before it. Formal: We focus on the shortest path between the current word w_t and the next word w_n. Formal: To prevent the model from getting too focused on the important words, we set the number of words in the target sentence to c = (c₁, ..., cₙ) = 1. Formal: We use the log probability of each word to find the token that can cause the most change in the attention distribution. Formal: We calculate the attention loss for the target sequence by picking the token with the highest probability. Formal: Using the global context vector h(T i (t)), we estimate the loss for the sequence x = x1, x2, ..., xT. Formal: For the target sequence x = x1, x2, ..., xT, we estimate the loss using the attention distribution h(T i (t)). Formal: We set the output dimension to d. Formal: We calculate the attention loss for the target sequence using the global context vector h(T i (t)). Formal: We define the cross-entropy loss using the cross-entropy distribution h(T i (t)). Formal: We calculate the attention loss for the target sequence x = x1, x2, ..., xT. Formal: We set the output dimension to d. Formal: To optimize the cross-entropy loss for the target sequence x = x1, x2, ..., xT, we use a",
        "directory": "C92",
        "filename": "C92-1049.json"
    },
    {
        "casual_text": "The hybrid change scores are still hanging out between the slang and non-slang groups, with an average of 0.621 ± 0.073. A permutation test shows there's a noticeable difference in how meaning changes between hybrid and slang words (p < 0.001) and also between hybrid and non-slang words (p < 0.05).",
        "formal_text": "tribution over hybrid change scores seem again to be centered between the slang and nonslang distributions, with mean 0. 621 ± 0. 073. According to a permutation text, there is a significant difference in semantic change both between hybrid and slang words (p < 0. 001) and between hybrid and nonslang words (p < 0. 05).",
        "GPT2_formal_text": "\" and \"response_ids\" respectively. When looking at the two-dimensional cross-entropy loss, we use cosine similarity for the vectors in the response to figure out the relevance score for the query, which is α_q. Formal: So, basically, we're looking for the query vector that gives us the biggest average cosine similarity between the response and the query, as measured by the empirical distribution q. Formal: In this paper, we introduce a new approach to extract keyphrases using a maximum entropy model. The goal is to find the query vector that gives us the highest average cosine similarity between the response and the query. Formal: We use a maximum entropy model to find the query vector that maximizes the log likelihood of the response. Formal: This approach is different from the traditional maximum entropy model because it treats keyphrases as a distribution over space, instead of just assigning them a probability based on the query vector. Formal: We use the KL term to make sure the response is independent and not influenced by the query vector. Formal: We tested our method using five different datasets from the PubMed biomedical field, and the results show that our method works really well. Formal: For future work, we want to try this approach on other fields like healthcare or social media. Formal: Since the embedding of a keyphrase is a function, we can add a linear layer to make the keyphrases even more informative. Formal: Formal: We also wanted to show that our method can work for other languages, so we compared it to a basic sequence-to-sequence model that only uses one input vector. Formal: The evaluation metric we used is the inverse document frequency (IDF), which was introduced by Dagan et al. in 2006. Formal: We made sure the distribution of responses is independent of the query vector by using the cosine similarity between the response vector and the query vector. Formal: For the query vector, we're using cosine similarity and the number of words in the response. Formal: We also looked at different methods, like the word-based and word-based-based-by-subject, to see how they affect the relevance score. Formal: The total number of keyphrases in a response is: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.101.json"
    },
    {
        "casual_text": "Hey, so there's this thing called \"Dipl. -Nath. Gisela Schlotter\" and it's about processing non-numeric data. Specifically, it talks about Part I: Subroutines (PI-17).",
        "formal_text": "Dipl. -Nath. Gisela Schlotter, Verarbeitung nichtnumerischer Daten Tell I. Unterprogramme (PI-17)",
        "GPT2_formal_text": "ing layer, which we call the attention loss. Similarly, we can also think of the hidden state of the graph G as a response vector in the form of an embedding matrix. Formal: The input to the attention layer in the graph G is a bunch of word embeddings h_i, which are generated by a Softmax function. Formal: This process of creating a hidden state is called \"attention masking.\" Formal: We can use a mix of other methods, like those from (Liu et al., 2017) and (Zhang et al., 2018), to build a graph G_M using any graph structure. Formal: There are two main ways to build G_M using graph structures: one is \"attention pooling,\" which uses a maximum-pooling method to combine a bunch of embeddings into a single embedding vector for each node. This is the usual approach used by methods like attention networks (Vaswani et al., 2017) and attention networks with softmax (Zhang et al., 2018). Formal: The other method is \"attention sparse modeling,\" which is a more efficient way to do this. Formal: The current methods for building graph embeddings don’t really consider the structure of the graph. Formal: So, the first and second methods just grab a bunch of embeddings from the input and add them to their vector representations. But the third method uses a sparse way of representing the graph to focus on the structure and get better results. Formal: The way we represent the graph structure can be figured out using algorithms that work with graphs and a graph structure. Formal: When it comes to how the graph embedding is created, the attention model we talked about in this paper uses a bi-directional GRU (BiGRU) with a non-linear hidden state to do it. Formal: We’re using some of the ideas from (Karpathy et al., 2018) to describe the graph embedding structure we’re proposing. Formal: The key idea here is the bi-directional GRU, which is used to create the embeddings. Formal: When it comes to how the graph embedding is generated, the attention model we’re using here is a multi-layer one. Formal: The Graph Attention Network (GAT) (Wang et al., 2018) is a type of attention network. Formal: Our",
        "directory": "C67",
        "filename": "C67-1017.json"
    },
    {
        "casual_text": "Alright, now we're going to take a closer look at these setups and show how they back up our second idea.",
        "formal_text": "Next we provide an analysis of these constructions which supports our second hypothesis.",
        "GPT2_formal_text": "= {'q': i, 'e': j, 'o': l, ...} | VP → NP, where VP is just a noun phrase. So, NP → VP, NP → VP → VP, and so on. Formal: The interpretation model, which we call the \"layers,\" is a network where each layer can only connect to one other layer. Formal: So, the layer for L N +1 gets linked to layer L N +2, and the layer for L N +1 gets linked to layer L N. Formal: In this paper, we’re using the layer-wise kernel (which is like a 3-layer model) to handle the meaning representation of the words in a sentence. Formal: In the semantic space, the space created by the \"layers\" can’t overlap with the spaces created by the \"colors\" and \"obliteration\" vectors. Formal: The idea here is to take the semantic representation, simplify it, and feed it back into the network. Formal: Using this simplified representation, the network can then figure out the sense representation for a word in the sentence. Formal: We also make sure the network can handle both semantic and syntactic stuff. Formal: For a word w i, the network can represent its meaning representation as a vector with dimensions d s i. Formal: The probability of a word w i (w 1, . . . , w m) appearing in the vocabulary V w is calculated by... Formal: Let’s say we have a word w i in the vocabulary V w. Formal: The network can then calculate the probability of a word w i (w 1, . . . , w m) appearing in the meaning representation representation space V S w. Formal: Formal: In this paper, we’re focusing on the word w i. Formal: We’re using the hypernym and hyponym vectors to represent the word’s sense representation and its semantic representation. Formal: The semantic representation can be represented as a vector with dimensions d s i. Formal: The hypernym and hyponym vectors can be thought of as two different ways of representing the meaning representation. Formal: The network can also calculate the probability of two words, w i (w 1, . . . , w m) and w i (w m, w 1, . . . . , w m), appearing in",
        "directory": "E17",
        "filename": "E17-1115.json"
    },
    {
        "casual_text": "We take the samples that didn’t make it into the test sets and use them for training or tweaking models with noisy data. We use a regular expression to automatically break comments into sentences by spotting sentence endings. Then, we match up the source and target sentences. If this matching process doesn’t work (like when the source comment has a different number of sentences compared to the target comment after splitting), we just keep the whole comment without cutting it up. \n\nFor the training data, we don’t check the translations as carefully as we do for the test data. Lastly, here’s a quick breakdown of the samples and tokens for each language pair:\n\n- **en-fr**: 36,058 samples, 841k source tokens, 965k target tokens  \n- **fr-en**: 19,161 samples, 661k source tokens, 634k target tokens  \n- **en-ja**: 5,775 samples, 281k source tokens, 506k target tokens  \n- **ja-en**: 6,506 samples, 172k source tokens, 128k target tokens  \n\nWe also set aside around 900 samples for validation in each direction. You can check out the size of the data in Tables 1, 2, and 3 for the test, training, and validation sets, respectively. For tokenizing, we use the Moses tokenizer for English and French data, and Kytea for Japanese before counting the tokens in each dataset.",
        "formal_text": "We gather the samples that weren't selected for the test sets to be used for training or fine-tuning models on noisy data. We automatically split comments into sentences with a regular expression detecting sentence delimiters, and then align the source and target sentences. Should this alignment fail (i. e. the source comment contains a different number of sentences than the target comment after automatic splitting), we revert back to providing the whole comment without splitting. For the training data, we do not verify the correctness of translations as closely as for the test data. Finally, #samples #src tokens #trg tokens en-fr 36, 058 841k 965k fr-en 19, 161 661k 634k en-ja 5, 775 281k 506k ja-en 6, 506 172k 128k we isolate ≈ 900 samples in each direction to serve as validation data. Information about the size of the data can be found in Table 1, 2 and 3 for the test, training and validation sets respectively. We tokenize the English and French data with the Moses (Koehn et al. ) tokenizer and the Japanese data with Kytea before counting the number of tokens in each dataset.",
        "GPT2_formal_text": "_tgt, which is made up of the attention distribution p_tgt (or the attention-specific vector h_tgt for the current token, which is calculated as h_tgt = p_tgt | s). In our setup, we use a linear layer to adjust the attention distribution p_tgt. Formal: To give a general idea of how well we're doing, we look at the average scores for the generated titles. We calculate these scores by averaging the results from the first five tokens of the title, which gives us a score of 1. Formal: The score for a title is just the average of its score for the first five tokens. Formal: We use a simple linear model to calculate the average scores, and then we average these scores to get the final score for the title. Formal: We also use a logistic regression model to check how well we're doing. Formal: Figure 1 shows the equation for the attention-based hierarchical attention. Formal: We use the LibLinear package (https://github.com/schwarze/LibLinear) to calculate the average attention weights for our title-based attention. Formal: To get the parameters of the model, we use the tf-idf package (https://github.com/tartarus/tf-idf). Formal: Finally, to get the attention weights for the token-level attention, we average the attention weights for the token-level tokens in the title. Formal: Formal: Formal: Figure 2 shows how the parameters for the model are learned. Formal: The original model weights for the model, trained on the entire title, are calculated as: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1050.json"
    },
    {
        "casual_text": "So, we've got τ = {a, b, c}, and ψ_n(•) and ψ_e(•) are the node and edge factors, right? And Z(•) is just this big normalization thing that makes everything work out. We're working with log-linear factors here, so keep that in mind.",
        "formal_text": "where τ = {a, b, c}, ψ n (•) and ψ e (•) are node and edge factors, respectively, and Z(•) is a global normalization constant. We use log-linear factors:",
        "GPT2_formal_text": "= {t_i} for all i from 1 to K. Formal: We use the same approach for all our model sizes, but we tweak it a bit by using a 4-gram language model instead of the full model. Specifically, we set the parameter λ to 0.1, with the number of words kept the same as the full model. Formal: We also use an LSTM for the CLM (Convolutional Neural Network) to pick out the keyphrases. Formal: Next, we use a one-layer perceptron with just one hidden layer to create the contextual representations for each token. Formal: The model we're proposing works by looking at the hidden state vectors and using the log probability from the LSTM to predict the next word. Formal: Lastly, we combine the contextual representation with the hidden state vectors to create the contextual vector for the whole document. Formal: Formal: When we get a query, we figure out the query vector that best matches the document by using a kernel function. Formal: A recent study (Huang and Chiang, 2018) also uses something similar to use a kernel for document embedding. Formal: A bunch of studies have been done to check if we can automatically generate some keyphrases for queries (Saha et al., 2018; Jha et al., 2018; Li et al., 2019; Jin et al., 2019). Formal: But, most of those methods have been about generating topics or describing the topic, not actually generating the actual content. Formal: Formal: In the example shown in Figure 2, the sentences from the second sentence are the keyphrases. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1452.json"
    },
    {
        "casual_text": "Length-Control Inference. Summarization naturally involves controlling how long the output is, like when you need a short news headline for a mobile screen. Plus, Schumann and his team (2020) found that the main evaluation metric, ROUGE (Lin, 2004), is affected by how long the summary is—longer summaries usually get better ROUGE scores. So, it’s super important to keep the summary length in check for a fair comparison.",
        "formal_text": "Length-Control Inference. Controlling output length is the nature of the summarization task, for example, displaying a short news headline on a mobile device. Moreover, Schumann et al. (2020) show that the main evaluation metric ROUGE (Lin, 2004) is sensitive to the summary length, and longer summaries tend to achieve higher ROUGE scores. Thus, it is crucial to control the summary length for fair comparison.",
        "GPT2_formal_text": "ed_rng.html\", \"type\": \"application/pdf\", \"data\": { \"M\" : [{ \"category_id\": \"category_id1\", \"category_type\": \"category_type1\", \"category_num\": \"category_num1\", \"start_time\": \"start_time1\", \"num_of_links\": \"num_of_links1\", \"start_time\": \"start_time1\", \"page\": \"page1\", \"description\": \"description1\", \"tags\": \"tags1\", \"tags2\": \"tags2\", \"description2\": \"description2\", \"multi\": \"multi1\", \"section\": \"section1\", \"type\": \"section1\", \"title\": \"title1\", \"link\": \"link1\", \"terms\": \"terms1\", \"date\": \"date1\", \"submission\": \"submission1\", \"attachment\": \"attachment1\", \"email\": \"email1\", \"date\": \"date1\", \"email\": \"email1\", \"show\": \"show1\", \"summary\": \"summary1\", \"submission\": \"submission1\", \"date\": \"date1\", \"publish\": \"publish1\", \"date\": \"date1\", \"source\": \"source1\", \"date\": \"date1\", \"link\": \"link1\", \"date\": \"date1\", \"tags\": \"tags1\", \"tags2\": \"tags2\", \"date2\": \"date2\", \"tags\": \"tags2\", \"category\": \"category1\", \"date\": \"date1\", \"email\": \"email1\", \"date\": \"date1\", \"date\": \"date1\", \"city\": \"city\", \"created\": \"created\", \"created_at\": \"created_at\", \"created_time\": \"created_time\", \"date\": \"date1\", \"submission\": \"submission1\", \"date\": \"date1\", \"link\": \"link1\", \"date\": \"date1\", \"type\": \"type1\", \"date\": \"date1\", \"created\": \"created\", \"created_at\": \"created_at\", \"created_time\": \"created_time\", \"date\": \"date1\", \"created_type\": \"type1\", \"description\": \"description1\", \"tags\": \"tags1\", \"date2\": \"date2\", \"tags\": \"tags2",
        "directory": "acl",
        "filename": "2022.acl-long.545.json"
    },
    {
        "casual_text": "Alright, so when you're trying to figure out what's going on with someone's problem, it's a good idea to ask questions that help them explain things better. Open-ended questions work great for this because they let the person talk more freely and really get into what's bothering them.",
        "formal_text": "Question Asking for information related to the problem to help the help-seeker articulate the issues that they face. Open-ended questions are best,",
        "GPT2_formal_text": ", r = i_1, ..., i_m. This part is like the summary or explanation for the dialog act. It’s like a summary that doesn’t have all the details about the action, like how the person interacts with the system. Formal: The current attention state is h_t. The action context vector v_t is made up of the attention vectors of the possible actions, which are h_t. Formal: The current attention state is h_t. Formal: Let’s say the current attention state is h_t, and the action context vector v_t is made up of the attention vectors of the possible actions, which are h_t. Formal: The current attention state is h_t. Formal: Formal: We’re also assuming that the action context vector v_t is the same as h_t. Formal: Formal: The current attention state is h_t. Formal: Formal: A boolean variable W_s is set to 1 if the current attention state is h_t, and 0 if it isn’t. Formal: Formal: We’ve got an equation here. Formal: The first two parts are the equations we’re using to describe the attention and attention-based action vectors. Formal: The last two parts are just a simpler way to say that. Formal: We’re using the gaze duration for each step of the conversation, which is d. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.269.json"
    },
    {
        "casual_text": "Glosses are basically short definitions, and they're a big deal in plWordNet. They help users get what the network is about and make it easier for the editors to work efficiently.",
        "formal_text": "• Glosses are short definitions, a very important element of plWordNet. They help the user to understand the network, and plWordNet editors to work with high effectiveness.",
        "GPT2_formal_text": "_4g model:\n       Casual: Here, w_i is the word embedding, θ_ij is the contextual vector for the word i, and σ is the sigmoid activation function. To make sure we capture the meaning of each word, we add a softmax layer at the word level to make it easier to predict. The prediction we get is the average of the vectors for all the words in the passage. The whole setup is trained using the cross-entropy loss function, like this: Formal: There are two ways to do this: one is to directly train the model using cross-entropy loss, and the other is to use something called the network hidden state distillation method. Formal: We refer to the model as the generator model (or just the generator) and the decoder model (or just the decoder) when we're talking about how the model is trained. Formal: The second approach is the network hidden state distillation (or just the network) when we're talking about how the model is trained. Formal: The final result from the neural network is what we call the generated text. Formal: We've shared the code for both the generator and the decoder models, along with the embedding vectors and weights, on GitHub at https://github.com/XuewangQiang/Attention-Masked-Text-Generation. Formal: We also created a dataset called CNN-T5 for text classification, and you can check it out here: https://github.com/CNN-T5/Dataset. Formal: We've included an example of an attention-masked text generation model in the supplementary material. Formal: We've shared the code for the generator models in the supplementary material. Formal: The vector representations for the generator model (or just the generator) are saved as separate files and can be easily downloaded by anyone. Formal: We've included the code for the network hidden state distillation models in the supplementary material. Formal: The final result from the neural network is what we call the generated text. Formal: The generated text is made up of tokens from the input sequence and a special token called `OP`. Formal: We've also included the code for the CNN-T5 model. Formal: Formal: To generate a sequence of tokens that includes `OP`, we use the generator model's output. Formal: Form",
        "directory": "C16",
        "filename": "C16-1213.json"
    },
    {
        "casual_text": "Yeah, there are some downsides too. For one, the whole thing is pretty small—around 500 million tokens in total. Plus, most languages only have one version of the New Testament, which is about 200,000 tokens each. The focus is pretty narrow, and there’s a lot of overlap with names and stuff. This can cause weird issues if you use random word combinations, because the model might just spit out a bunch of random names instead.",
        "formal_text": "Certainly, there are disadvantages as well, such as the limited size (roughly 500 million tokens in total, with most languages having only one translation of the New Testament each, with roughly 200 thousand tokens), the narrow domain and the high overlap of named entities. The latter can lead to some unexpected effects when using nonsensical language vectors, as the model will then generate a sequence of random names.",
        "GPT2_formal_text": "= ('roberta.text.1', 'roberta.text.2', 'roberta.text.3') and the output sentences will be labeled with the same details. We just copy the input sentences to get the final labels. The same process works for aligning the generated training sets. You can find more details about the training set setups in Section 3. Formal: We use a normalized version of BLEU to evaluate the generated text, with a special weight (B) for the over-segmented text. Formal: We use the extended SE metric, which is explained in Section 4.2. Formal: We pick the best translation by checking how well the generated text matches the reference translations, as explained in Section 4.3. Formal: We run the evaluation 10 times and average the scores (C), with a confidence level (f) for each evaluation. Formal: We calculate the average precision (P) and recall (R) for each segmentation, and then pick the best translation using a random sample. Formal: We pick the best translation for each segmentation and use the validation set to train a TGM to predict the output. Formal: The validation set includes all the input sentences generated by the TGM and the reference translations. Formal: We calculate the average precision (P) and recall (R) for each segmentation, and then pick the best translation using a random sample. Formal: We calculate the average precision (P) and recall (R) for each segmentation, and then pick the best translation using a random sample. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E17",
        "filename": "E17-2102.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nYou can find the data set over at http://homepages.inf.ed.ac.uk/gcong/qa/.",
        "formal_text": "http: //developer. yahoo. com/answers 7 The data set is available at http: //homepages. inf. ed. ac. uk/gcong/qa/",
        "GPT2_formal_text": "ing (S_M) for the masked word in the input sequence. This mask is calculated by multiplying the probability of the masked word given the context c_t by the probability of the full masked word in the input sequence. Formal: The input hidden state is calculated using a two-layer feedforward neural network. Formal: The output hidden state is a combination of the masked word's representation (the output vector h_M) and the sentence's representation (the output vector s_m). Formal: The final output hidden state is a combination of the sentence's representation (the output vector h_s_m) and the masked word's representation (the output vector h_M). Formal: The output function g(s_m, t_m) combines the masked word and the sentence's representation into one representation, giving us the final hidden state. Formal: The output function for the masked word, h_M, is calculated by multiplying the probability of the masked word given the context c_t by the probability of the full masked word in the input sequence. Formal: The output function for the sentence's representation, s_m, is calculated by multiplying the probability of the sentence's representation given the context c_t by the probability of the full masked word in the input sequence. Formal: The output function for the masked word, h_M, is calculated by multiplying the probability of the masked word given the context c_t by the probability of the full masked word in the input sequence. Formal: The masked word representation h_M is a combination of the sentence's representation (the output vector h_s_m) and the masked word's representation (the output vector h_M). Formal: Formal: The output function for the masked word, h_M, is calculated by multiplying the probability of the masked word given the context c_t by the probability of the full masked word in the input sequence. Formal: Formal: Formal:\n\n- **Masked word:** This means the masked word can be any character from a set of characters {a_1, ..., a_{m-1}, ..., a_{m-||}. Formal: The output is either a 0 or a 1 depending on whether the word is the start or end of a word. Formal: The masked word representation h_M is a combination of the masked word's representation (the output vector h_M) and the masked word",
        "directory": "C12",
        "filename": "C12-1193.json"
    },
    {
        "casual_text": "Hey, so here's a quick rundown of some #MH17 updates:\n\n- A pro-Russia separatist mentioned that the bodies found in the Ukraine field weren't fresh, implying they were already dead before the plane took off.\n- #MH17 is back in the news: \n  1. #Kolomoisky admits to being involved (link provided)\n  2. He also got $1.8 billion from Ukraine's bailout funds.\n- Pro-Russia and pro-Ukraine sides are still at it.\n- Russia is now saying that #MH17 was taken down by an air-to-air missile, but of course, it wasn't Russian-made. Classic.\n- And Moscow's latest claim is that a Ukrainian fighter jet shot down #MH17.",
        "formal_text": "RT @katehodal: Pro-Russia separatist says #MH17 bodies \"weren't fresh\" when found in Ukraine field, suggesting already dead b4takeoff m) RT @NinaByzantina: #MH17 redux: 1) #Kolomoisky admits involvement URL 2) gets $1. 8B of #Ukraine's bailout funds III Pro-U Pro-R n) #Russia again claiming that #MH17 was shot down by air-to-air missile, which of course wasn't russian-made. #LOL URL o) RT @20committee: New Moscow line is #MH17 was shot down by a Ukrainian fighter. With an",
        "GPT2_formal_text": "= [](/home/home/2d3a), and the self-attention for the (q, a, s, t) in the response embedding q, a, s, t is just an attention thingy, which is in R^d. Formal: For each element i in the sequence t, we use an attention thingy to get a vector representation z, which is in R^d. Formal: And for the (r, e, g) pairs that match the query (r, e, g), we use a bi-directional long short-term memory (BiLSTM) to get a representation r^d. Formal: Formal: So, the attention matrix a_q(t) looks like this: a_q(t) = [a_q(t, q), ..., a_q(t, e)], and the attention attention vector q_a(t) = [q_a(t, e), ..., q_a(t, g)]. Formal: The max-pooling operation here is O_max_k^d. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-5006.json"
    },
    {
        "casual_text": "To show that CoRA can handle long-tail relations really well, we did a test just on those long-tail relations. We used the same setup as before, where Hits@K (Macro) measures how well it performs on these long-tail relations. From Table 2, you can see we compared CoRA to other models and our own base models. It turns out that CoRA boosts performance on long-tail relations by a big margin and sets a new record. \n\nWhen we look at previous models (like PCNN+HATT/+KATT) that also use relation hierarchies, our relation-augmented attention (Base) without any hierarchy still does pretty well, even without fancy pre-trained graph embeddings like PCNN+KATT uses. If we compare our base model with selective attention (PCNN+ATT), the big difference in performance shows how our framework is better at dealing with both wrong labels and long-tail relations. \n\nLastly, as you can see in the last row of the table, if we take out the sent2rel attention we proposed, the performance drops a lot, which really highlights how important it is for handling long-tail relations.",
        "formal_text": "To prove the capability of CoRA in handling long-tail relations, we conduct an evaluation solely on longtail relations. Our evaluation setting is identical to, where Hits@K (Macro) is used to represent statistical performance on long-tail relations. As shown in Table 2, we compare CoRA with competitors and our base models. It is observed that, CoRA improves the performance on long-tail relations by a large margin and delivers a new state-of-the-art results. Compared to previous works (PCNN+HATT/+KATT) that also leverage the relation hierarchies, our relation-augmented attention (Base) without any hierarchy even gets competitive results, not to mention pre-trained graph embeddings used in PCNN+KATT. Further comparing our base model with selective attention (PCNN+ATT), the huge performance gap demonstrates the advantages of our framework in handling both wrong labeling and long-tail relations. Finally, as shown in the table's last row, removing the proposed sent2rel attention leads to significant decrease, which emphasizes its importance for long-tail relations.",
        "GPT2_formal_text": "_2d, where the first dimension is the representation of the original word and the second is the attention vector for the mask. Formal: Using the masked word's vector, we can get the masked word's vector by multiplying the masked word's vector with the representation of the original word. Formal: This method of making the vector looks like this: m k = softmax(vectors[masked_w], z ln ln i, k) (check out Figure 1 for more details). Formal: We used the BiLSTM model for the Masked Language Model (MLM) task, which was introduced by Hochreiter and Schmidhuber back in 1997. For each word w i in the input sentence, we used the BiLSTM model to predict the next word w i+1. Formal: We tweaked the MLM model a bit by adding a word prediction loss function to the Softmax layer. This function helps the MLM model predict the next word more accurately. Formal: We tested our method on the Switchboard dataset, which is a collection of news articles from The New York Times and The Washington Post. Formal: We ran the same experiments on the CNN/DailyMail dataset and on the CNN/DailyMail dataset in the same order. Formal: In Table 2, you can see the results for all these datasets. Formal: For the Switchboard dataset, we used the first three layers of the CNN/DailyMail dataset as the input for the MLM model. We did a two-fold cross-validation using the development set and the test set. Formal: For the CNN/DailyMail dataset, we used the first three layers of the CNN/DailyMail dataset as the input for the MLM model. Formal: For the CNN/DailyMail dataset, we used the last three layers of the CNN/DailyMail dataset as the input for the MLM model. Formal: Formal: Since the CNN/DailyMail dataset only has 4,000 samples, we didn't include those results in our paper. Formal: Formal: For the CNN/DailyMail dataset, we used the first three layers of the CNN/DailyMail dataset as the input for the MLM model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "coling",
        "filename": "2020.coling-main.145.json"
    },
    {
        "casual_text": "The experiments show that getting rid of that noise helps our model work better.",
        "formal_text": "The experimental results show that removing these noise can improve the performance of our model.",
        "GPT2_formal_text": "\" and \"answer_ids\" respectively. The key here is that the answer to a question should match the question's answer exactly. The answer_ids are picked by looking at the question and the answer, which makes the whole process of figuring out the answer a lot easier.\n\nThe thing is, there are different ways to break down questions and answer them, each using different approaches. So, it makes sense to combine all these methods into one, which we'll call the Knowledge Base Question Encoder. This setup helps in both creating the questions and figuring out the answers.\n\nTo make this happen, we came up with a cool idea: we could map the question's representation to the answer's representation, using something called a special entity representation. This idea is pretty cool because it can handle all the different ways to break down a question, making the whole process better. Formal: We wanted to create a question representation that can also understand the answer. Formal: We built a special entity representation for the question to help with this. Formal: So, the question-answer pair is made up of two parts: the question representation and the answer representation. Formal: We took the entity representation we got from the question and used it to represent the answer representation. Formal: The entity representation we get from the question is pretty unique—it doesn't have a lot of similar ones. So, we combined the two representations into a single one, making sure it has the right structure and meaning. Formal: Using these entity representations, we can create the question representation. Formal: This way, the question representation and answer representation end up being the same, which makes the whole process way more efficient. Formal: This method can handle questions that have multiple answers, like in the example (6). Formal: For all these cases, the question and answer representation share the same representation, and we can use this shared representation to handle the embeddings of the answer. Formal: If we don't know the answer, the question representation and answer representation share the same representation. Formal: So, the answer representation and answer representation end up being the same, which means we can use this shared representation to handle the embeddings of the answer. Formal: In the end, the entity representation we get from the question and answer representation is the same. Formal: Formal: We tested this on two datasets: MSR and WikiQA. In WikiQA, there are 15,7",
        "directory": "dravidianlangtech",
        "filename": "2021.dravidianlangtech-1.21.json"
    },
    {
        "casual_text": "Around 1% of BCCWJ is considered the \"core data,\" and this core data is manually annotated with extra linguistic details. The part of the core data that includes annotations for dependency structures, coordinate structures, coreference relations, and predicate argument structures is specifically called BCCWJ-DepParaPAS (Ueda et al., 2015; Maekawa et al., 2014). BCCWJ-DepParaPAS follows the annotation style of the NAIST Text Corpus (Iida et al., 2007b), where three key arguments—ga (nominative), wo (accusative), and ni (dative)—are marked at the SUW level.",
        "formal_text": "About one percent of BCCWJ is defined as the \"core data\", and the core data is manually annotated with richer linguistic information. The core data annotated with dependency structures, coordinate structures, coreference relations and predicate argument structures is particularly called BCCWJ-DepParaPAS (Ueda et al. , 2015; Maekawa et al. , 2014). BCCWJ-DepParaPAS adopts the annotation schema of NAIST Text Corpus (Iida et al. , 2007b) where three obligatory arguments, ga (nominative), wo (accusative) and ni (dative) are annotated at the SUW level.",
        "GPT2_formal_text": "_tgt: tgt = [tgt1, ..., tgtM] Formal: The decoder takes the context vector h_i, turns it into a hidden state h_s, and then sends it through a fully connected layer. Formal: Formal: We're using a standard BiLSTM model with a hidden size of 256, and the hidden state size for the attention is 256. Formal: The input vector x_i is passed through a bi-directional GRU with a window size of 2. Formal: To see how the teacher and student models affect the results, we train a regression classifier using this setup. Formal: After training, we check if the system can predict the target label by checking the final classification accuracy. Formal: Lastly, we check how the output sequence length affects the model's performance. Formal: In Table 4, we compare how the BiLSTM model performs in the final step. Formal: We don't show the student model's performance because it doesn't use the attention from the teacher model. Formal: Formal: Since the attention from the student model is weaker compared to the teacher, we decided not to show the teacher model's performance. Formal: Formal: Formal: Table 4: Performance comparison for the BiLSTM model in the final step. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1269.json"
    },
    {
        "casual_text": "For languages that aren’t as high-priority or when the source language isn’t English, and especially for apps where the quality expectations are a bit lower, we’re focusing on more budget-friendly solutions. Like we mentioned, Adobe’s started looking into the Moses open-source tool, and we’ve also been talking to some vendors to address the issues with those one-size-fits-all, free online services.",
        "formal_text": "For lower-tier languages, language pairs where the source is not English, and applications where the quality requirements are lower, we are developing solutions from the lower-cost categories. As described, Adobe has begun investigating the Moses open-source package, and has also been working with vendors to mitigate some of the drawbacks of vertically-customized, uncustomized, and free online services.",
        "GPT2_formal_text": "= { id1: 1, id2: 2, ..., idL: 1 } | = { id1: i=1, id2: i=2, ..., idL: i=L } α t | = { α t: i=1, α t: i=2, ..., α t: i=L } τ, which basically means we're dealing with some constraints. Formal: To make sure our model can keep track of all these different types of relationships between pairs, we need a way to handle the ambiguity. Formal: So, we've got this structure: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "2010.amta-commercial.5.json"
    },
    {
        "casual_text": "We can build the transition sequence that Theorem G.9 is asking for by using C LT L over and over on a starting configuration, let's call it c. Lemma G.15 says that when you use C LT L on a configuration, you either end up with a goal configuration or you add one more FINISH transition. Lemma G.14 also tells us there's a limit to how many times we can keep adding FINISH transitions in a valid sequence. Since C LT L only gives us valid sequences, we'll definitely hit a goal configuration after applying it a certain number of times.",
        "formal_text": "We can construct the transition sequence for which Theorem G. 9 asks by repeatedly applying C LT L to a given configuration c. Lemma G. 15 shows that applying C LT L to a configuration results either in a goal configuration or increases the number of FINISH transitions by one. Lemma G. 14 tells us that there is an upper bound on how many times we can increase the number of FINISH transitions in a valid transition sequence. Since C LT L returns only valid transition sequences, this means that we reach a goal configuration by finitely many applications of C LT L.",
        "GPT2_formal_text": "and attention_pool. The response here will either be {y|t} or {r|s}, depending on whether you asked about the content or topic. Formal: The gold response for the query q should be in the form of a multi-word text, which we call a triplet. Formal: You can find all the nitty-gritty details about the dataset and the tasks in the supplementary material. Formal: We're using the Stanford NER Dataset 1.1 and the Penn Chinese Treebank 1.4 datasets. Formal: The Chinese character similarity task, also known as character-based NER, is all about figuring out the relationships between characters in a sentence. Formal: We're using the Chinese dependency parser for this. Formal: The Peking-based sentence segmenter is a Chinese sentence segmenter that focuses on sentence structure, as introduced by Sun et al. in 2012. Formal: The English sentence segmenter is a Chinese sentence segmenter that also looks at sentence structure, as introduced by Sun et al. in 2012. Formal: The first Chinese sentence segmenter we tested, named the Skip-gram model, is based on the skipgram model from Mikolov et al. in 2013, as described by Sun et al. in 2013. Formal: For the Chinese sentence segmenter, we're following the approach by Liu et al. in 2013, which is also the skipgram model from Mikolov et al. in 2013. Formal: The second Chinese sentence segmenter we tested, named the Multi-Linear model, also uses the skipgram model from Mikolov et al. in 2013, as described by Sun et al. in 2013. Formal: Lastly, the Chinese word segmenter we're using is the same one from Liu et al. in 2013, but with a hidden dimension of 1. Formal: For the sentence segmenter, we're following the approach by Liu et al. in 2013. Formal: Formal: And for the dependency parser, we're following the approach by Sun et al. in 2013. Formal: Formal: Formal: Formal: We've also included a comparison between the English and Chinese word segmenters in Table 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nFor text classification, it assigns a label to each piece of text by looking at the embedding of the first token. This first token is usually some kind of special marker, like a symbol or something.",
        "formal_text": "CLASSIFICATION For text classification, it predicts a label for every text instance by using the embedding of the first token, which is commonly a special token (e. g.",
        "GPT2_formal_text": "= { ... }) create_text = (text_idx, xt, from_text, to_text) Formal: We're using the latent space from equation (6) to create a kind of hidden representation for each pair of tokens. Here's how we calculate the hidden representation for a pair of tokens: Formal: We've got this latent representation for the pair, which we call h_i. Formal: The second part of the whole process is figuring out how to handle the input token x_i. Formal: We can figure out the current hidden representation for x_i by using something called beam search. Formal: Let's say W is the total number of different tokens we have. Formal: To keep things simple, we'll just call the final hidden representation for x_i w_i. Formal: In a straightforward way, we calculate the embedding of the pair by applying a softmax function to the current hidden representation h_i. Formal: The final embedding for a token x_i is just the average of its embedding from all the tokens we've seen so far. Formal: We can also find the embedding of the i-th token x_i by applying a softmax function to the distribution of tokens we've already seen. Formal: We can also calculate the embedding of the i-th token x_i by applying a softmax function to the prior distribution of tokens we've seen. Formal: In this part, we're using a technique called maximum likelihood estimation to figure out the best embedding for the i-th token. Formal: We can also find the embedding of the i-th token by applying a softmax function to the prior distribution of tokens we've seen. Formal: This step is kind of like the maximum likelihood estimate, where we're updating the parameters of the model based on what we've seen so far. Formal: The update for the model is made by using the EM algorithm, which is a pretty common algorithm that's pretty well-known for its simplicity. Formal: We can also find the embedding of the i-th token by applying a softmax function to the posterior distribution of tokens we've seen. Formal: Here's the formula for the objective function: Formal: We can also find the embedding of the i-th token by applying a softmax function to the posterior distribution of tokens we've seen. Formal: So,",
        "directory": "eacl",
        "filename": "2021.eacl-demos.22.json"
    },
    {
        "casual_text": "All parsing methods end up with the same number of nodes overall, but they just build those nodes at different times. Usually, left-branching ways of doing things get nodes done earlier compared to right-branching ones. But when it comes to right-adjunction, both left and right-branching methods hold off on making a lot of nodes until the right-adjunct is processed. That’s not true for revealing derivations, though—they’re made to handle right-adjuncts more flexibly and don’t delay like the others.",
        "formal_text": "All parsing strategies have the same total number of nodes, but only differ in the abstract timing of those nodes' construction. In general, leftbranching derivations construct nodes earlier than do the corresponding right-branching derivations. However, in the case of right-adjunction both leftand right-branching derivations delay construction of many nodes until the right-adjunct is consumed. This is not the case with the revealing derivations that are specifically designed to allow flexibility with right-adjuncts.",
        "GPT2_formal_text": "_tgt: We've come up with a new multiloss hierarchical attention model called the HATM (Hierarchical Attention Transducer). It uses a triplet-wise hierarchical attention thingy, which was introduced by Vaswani et al. in 2017. This attention system uses something called triplet-wise multi-head attention (Vaswani et al., 2017). This means that instead of just using the top-level attention head on the input sequence, we use a bunch of sub-tokens to help capture different types of relationships between different parts of the input sequence. The cool thing about this is that it lets us use different types of information to tweak how we represent the input, all within the same setup. Formal: In this paper, we talk about how to calculate the common losses and the losses for different learning goals from a teacher model. Formal: We also introduce a new version of the HATM called the HATM-adapted model (HATM-AT) that uses multi-head attention for the latent representation. Formal: We also talk about how to optimize the loss for the latent representation, which includes both the teacher model and the latent representation. Formal: We also evaluate how well the model handles new representations, like those from different domains, by looking at both the teacher model and the latent representation separately. Formal: We also look at the average performance of different sub-tokens in the teacher model to see how important each one is. Formal: Lastly, we show how to adjust the model's parameters using different methods, like hyper-parameters, learning rate, dropout rate, and the number of epochs. Formal: We also talk about how to estimate the parameters using different learning methods. Formal: We use the same method for all the parameters, including the non-linear ones. Formal: We also give a summary of the main contributions of each research group and their results. Formal: We also include a glossary with common terms and some examples of parameters we trained on the shared task. Formal: The joint model we proposed has already been used to train a bunch of other models. Formal: We compare our model to a few other methods. Formal: We also share our code and dataset. Formal: For our model, we used the transformer model (Vaswani et al., 2017). Formal: We also created a new dataset (Schuster et al.,",
        "directory": "cmcl",
        "filename": "2021.cmcl-1.3.json"
    },
    {
        "casual_text": "The main idea here is to guess what a person will buy next based on what they just bought. We use a pre-trained BERT model and tweak it a bit to work for this specific task. We treat the item they just bought as \"sentence A\" and the one we're trying to predict as \"sentence B.\" Both item names are combined and cut short if they’re too long, keeping the total under 128 tokens, with special markers like [CLS] and [SEP] included. \n\nFor the item they just bought, we find items they also bought in the same shopping session to use as positive examples. For negative examples, we just pick some random items. With these positive and negative sets, we calculate the cross-entropy loss to help the model learn to predict what comes next.",
        "formal_text": "The goal of this task is to predict the next item a user is going to purchase given the seed item he/she has just bought. We start with a pre-trained BERT base model, and fine-tune it for our next purchase prediction task. We feed seed item as sentence A, and target item as sentence B. Both item titles are concatenated and truncated to have at most 128 tokens, including one [CLS] and two [SEP] tokens. For a seed item, its positive items are generated by collecting items purchased within the same user session, and the negative ones are randomly sampled. Given the positive item set I p, and the negative item set I n, the cross-entropy loss for next purchase prediction may be computed as",
        "GPT2_formal_text": "\" = {q_t, q_d, ..., q_m} → {q_t, q_d} and w_t, w_d are the weights for the attention mechanism, while v_t, v_d are the representations of the different entities in the question. Formal: We use the BERT model (Devlin et al., 2018) for the sentence representation layer. Then, we apply a cross-attention layer to encode the question's representation q. Formal: We use the BERT model (Devlin et al., 2018) for the sentence representation layer. Next, we apply cross-attention to the entity embedding layer to get the representation for each entity e_t. Formal: We use the BERT model (Devlin et al., 2018) for the sentence representation layer. Finally, we combine the sentence representation with the entity embedding layer to get the question representation q. Formal: We use the BERT model (Devlin et al., 2018) for the sentence representation layer. Formal: We use the BERT model (Devlin et al., 2018) for the sentence representation layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT model (Devlin et al., 2018) for the entity embedding layer. Formal: We use the BERT",
        "directory": "ecnlp",
        "filename": "2020.ecnlp-1.8.json"
    },
    {
        "casual_text": "We use a margin threshold to figure out if x and y are mutual translations. Research shows it works better than the usual cosine similarity for identifying correct translation pairs (Artetxe and Schwenk, 2019a).",
        "formal_text": "A margin threshold is applied to decide whether x and y are mutual translations or not. It has been shown to be more consistent than the standard cosine similarity in determining correct translation pairs (Artetxe and Schwenk, 2019a).",
        "GPT2_formal_text": "ing (d) that has a length of 2, but it only looks at the first k words and the first K hidden units. This setup isn’t super accurate, but it’s enough to get the job done. Formal: The output is the set of word embeddings (e_i) that the model learned. Formal: To get the hidden state representation for a specific word embedding (e_i), you first find the word embedding with the highest probability and then calculate the hidden state for the word embedding e_i, just like we mentioned earlier. Formal: For the dictionary, you use the word embeddings (e_i) from the input and the dictionary (D_w) to get its hidden representation (h_i). Formal: The output is the dictionary embedding (h_i), which is the word embedding based on the input. Formal: Formal: To find the logits for all the words (w_i) in the dictionary, you look at the hidden state and the model's output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "bucc",
        "filename": "2020.bucc-1.6.json"
    },
    {
        "casual_text": "Alright, let's break this down in a more casual way.\n\nFor the Paraphrase Identification task, we used the PAWS-X dataset, which was introduced by Yang et al. in 2019. This dataset has around 24,000 human-translated pairs for evaluation and about 296,000 machine-translated pairs for training, covering 7 languages: English, Spanish, French, German, Japanese, Chinese, and Korean. However, we didn't use the Korean part because, during our initial experiments, we couldn't get the same results as in the original paper by Yang et al. We think there might have been an issue with the encoding when using the BERT multilingual model for Korean.\n\nMoving on to Sequence Tagging, we ran some experiments on Named Entity Recognition (NER) using the CoNLL 2002 and CoNLL 2003 datasets, which were created by Tjong Kim Sang in 2002 and Tjong Kim Sang and De Meulder in 2003, respectively. Following the approach of Rahimi et al. in 2019, we combined these two datasets into one, covering 4 languages: English, Spanish, German, and Dutch. The combined dataset has 51,821 sentences for training, 11,344 for validation, and 13,556 for testing. Each sentence is labeled with entities like Person, Location, Organization, and Miscellaneous.",
        "formal_text": "Sentence-Pairs Classification. We adopted the PAWS-X dataset (Yang et al. , 2019 ) for the Paraphrase Identification task. The dataset is composed of about 24, 000 human translated evaluation pairs and about 296, 000 machine translated training pairs over 7 languages: English, Spanish, French, German, Japanese, Chinese, Korean. We actually didn't used the Korean languages, as in preliminary experiment we were not able to reproduce the results of the (Yang et al. , 2019) paper. We suspect a problem in the encoding affected our results in this language with the bert multilingual model. Sequence Tagging. We reported experiments on Named Entity Recognition (NER) using the CoNLL 2002 (Tjong Kim Sang, 2002) and CoNLL 2003 (Tjong Kim Sang and De Meulder, 2003) datasets. We merged the two datasets as in Rahimi et al. (2019) to obtain a single dataset over 4 languages, i. e. , English, Spanish, German and Dutch. The dataset contains 51, 821/11, 344/13, 556 annotated sentences, respectively for train, validation and test. Each sentence has been annotated with respect to the following entities: Person, Location, Organization and Miscellaneous.",
        "GPT2_formal_text": "\" (as mentioned by Faruqui et al., 2016) and the attention weights (Holtzman et al., 2017) are calculated. Formal: For the next step, we use graph-based algorithms to create a better summary. These methods include graph convolutional networks (Gehring et al., 2016), graph neural networks (Jiang et al., 2017), and graph-based attention networks (Li et al., 2018). Formal: The generation algorithm takes the raw text (Q t ) and turns it into a shorter summary (W t ). It also uses a smart summary selection strategy to make sure the summary is related to the query q t by considering the query's history (Q history ). Formal: The generation algorithm for question retrieval, called the query-augmented generation (Q augmented), is defined as Q augmented = (Q t, W t ). Here, T is the number of times the query appeared in the passage, and R t is the total number of times the query has appeared in the passage. Formal: We add query-augmented generation as a new step to the current generation step. The generated questions are then ranked using a query-augmented ranking algorithm (Koncel-Kedziorski et al., 2019). Formal: We tweaked the query-augmented generation algorithm to include a query-augmented ranking as a new step. This tweak lets the generator rank multiple questions on the fly. Formal: We also added query-augmented generation as a new step for question generation. Since the generator is using this new step, the generated questions are ranked based on the query-augmented ranking algorithm. Formal: Formal: In this paper, we tested our method on two datasets: Wikipedia-QA (from Ruder et al., 2017) and Yahoo-AQUA (by Liu et al., 2017). We picked the best model from both of these datasets to see how our method works for question generation. Formal: The results from these tests are in Figure 1. The top model from Wikipedia-QA was fine-tuned on Wikipedia articles for question generation and then tested on the same Wikipedia articles for question retrieval. Formal: The results from the test for question retrieval on Yahoo-AQUA are in Figure 2. Formal: The results for the test for question generation on Wikipedia-QA and Yahoo-AQUA are in Figure 3. Formal: Here's the",
        "directory": "acl",
        "filename": "2021.acl-short.106.json"
    },
    {
        "casual_text": "Unlike earlier features, lexical vectors are calculated offline and don’t get updated during training. In our experience, it’s helpful to apply a MinMax scaler that scales the values to a range of [-1, +1] for each LS vector we create. So, something like [. . , 0.095, . . , 0.20, . . , 0.76, . . ] gets transformed into [. . , -1, . . , -0.67, . . , 1, . . ].",
        "formal_text": "Contrarily to previous features, lexical vectors are computed offline and are not adjusted during training. We found useful in practice to apply a MinMax scaler in the range [−1, +1] to each LS vector we computed; thus, [. . , 0. 095, . . , 0. 20, . . , 0. 76, . . ] becomes [. . , −1, . . , −0. 67, . . , 1, . . ].",
        "GPT2_formal_text": "(att n | att j, att k, att n-k ) = { 1, 2, ..., n-2, ∑ k = 0 } | t = [0, 1, ..., n-2]. Formal: • The overall attention weight, which we call W. Formal: • The average attention weight, called w. Formal: • The \"global attention matrix,\" which we call A. Formal: • The attention duration, t. Formal: • The global attention mask, which we call a. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C18",
        "filename": "C18-1161.json"
    },
    {
        "casual_text": "In this project, we're diving into differentiable natural logic models that blend natural logic with neural networks. The goal is to stick to the core of inference based on natural logic but also bring in subsymbolic vector representations and some neural network elements. Mixing the strengths of neural networks with natural logic comes with a few challenges. There are two main issues we need to tackle: 1) Figuring out how (and where) to use the power of neural networks within the natural logic framework, and; 2) Dealing with the lack of intermediate supervision when training smaller parts, which can cause problems like the spurious issue in end-to-end training (as mentioned by Guu et al., 2017, and Min et al., 2019).",
        "formal_text": "In this work we explore differentiable natural logic models that integrate natural logic with neural networks, with the aim to keep the backbone of inference based on the natural logic formalism, while introducing subsymbolic vector representations and neural components into the framework. Combining the advantages of neural networks with natural logic needs to take several basic problems into consideration. Two problems flow directly from this objective: 1) How (and where) to leverage the strength of neural networks in the natural logic formalism, and; 2) How to alleviate the issue of a lack of intermediate supervision for training sub-components, which may lead to the spurious problem (Guu et al. , 2017; Min et al. , 2019) in the end-to-end training.",
        "GPT2_formal_text": "= {'n_i', 'e_i', 'e_j', 'p_i', 'p_j', 't_i', 't_j'}, {'w_i', 'b_j', 't_i', 'p_i', 'p_j', 's_i', 's_j'}, {'r_i', 's_j', 't_i', 'p_i', 'p_j', 's_i', 's_j'}, {'o_i', 'o_j', 't_i', 'p_i', 'p_j', 's_i', 's_j'} = {'n_i', 'e_i', 'e_j', 'p_i', 'p_j', 't_i', 't_j'}, {'w_i', 'b_j', 't_i', 'p_i', 'p_j', 's_i', 's_j'} = {'n_i', 'e_i', 'e_j', 'p_i', 'p_j', 't_i', 't_j'} Formal: Alright, so we've got this set of entity pairs here: {'w_i', 'b_j', 't_i', 'p_i', 'p_j', 's_i', 's_j'}, {'r_i', 's_j', 't_i', 'p_i', 'p_j', 's_i', 's_j'} Formal: So, the connection between the argument and the predicate is based on the node where the predicate is defined. The argument and predicate are connected by one or more links, and they also have a connection through a relation relation. Formal: To keep things clear and avoid any confusion, we've taken out the paths that have a missing link and those that have the wrong relation. Formal: In the case of a missing link, the connection between the argument and the predicate is based on the node where the predicate is defined. The argument and predicate are connected by one or more links, and they also have a connection through a relation relation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.101.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way:\n\n- If you add **x₁** to **C(s, t)**, you get **C(s)**.\n- If you add **x₂** to **C(s, t)**, you get **C(t)**.\n- If you add **x₄** to **C(s)**, you get **N**.\n- If you add **x₅** to **C(t)**, you also get **N**.\n- And if you add **x₂** and **x₃** together, you get **x₄**.\n\nSo, in short:\n- **C(s, t) + x₁ = C(s)**\n- **C(s, t) + x₂ = C(t)**\n- **C(s) + x₄ = N**\n- **C(t) + x₅ = N**\n- **x₂ + x₃ = x₄**",
        "formal_text": "C(s, t) + x 1 = C(s), C(s, t) + x 2 = C(t), C(s) + x 4 = N, C(t) + x 5 = N, and x 2 + x 3 = x 4. tt s C(s, t) x 1 C(s) s x 2 x 3 x 4 C(t) x 5 N",
        "GPT2_formal_text": "ing function takes an input matrix X and turns it into a hidden representation. Formal: It calculates the attention score for a sentence s and its context c_t by looking at the word-level attention scores from the second and fourth layers. Formal: Here, x_i is the actual attention score for the i-th word in the input sentence. Formal: A candidate model is just the sum of all the attention scores from the different layers. Formal: In this part, we'll explain how we set up the model. Formal: To get a better idea of how the model works, we'll calculate its perplexity on the target side. Formal: We picked the model called REAPER because it does a better job of generalizing (check out Figure 4(b)) and it performs really well in specific situations (like Figure 5). Formal: Next, we'll talk about the evaluation metrics and the three evaluation algorithms we used. Formal: For the loss function, we used the standard L2-loss with δ = 0.1. Formal: We trained the model using a batch size of 32, optimizing it with Adam and the stochastic gradient descent algorithm, and we tested it on the test set using the standard evaluation metric metric C. Formal: We tried out different values of δ for α and β, and we got the best results with α = 0.9 and β = 0.9. Formal: We also checked if the model could generalize to a new domain using a standard evaluation metric. Formal: Formal: For the two evaluation algorithms, we tested them on the test set using the standard evaluation metric and the top-1 metric, respectively. Formal: Formal: We also tested the model on a new domain using the standard metric and the top-3 metric, but we didn't include results for the new domain in this paper. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "2012.amta-papers.28.json"
    },
    {
        "casual_text": "DATR was first introduced by Evans and Gazdar (1989a; 1989b) as a straightforward, nonmonotonic language for handling lexical inheritance hierarchies. Basically, a DATR hierarchy is set up using path-value pairs. This inheritance of values helps capture useful generalizations and cuts down on repeating the same information over and over. Plus, it has a simple default system that keeps descriptions neat and tidy, but still lets you note any exceptions to the inherited info in a pretty natural way.",
        "formal_text": "DATR was originally introduced by Evans and Gazdar (1989a; 1989b) as a simple, nonmonotonic language for representing lexical inheritance hierarchies. A DATR hierarchy is defined by means of path-value specifications. Inheritance of values permits appropriate generalizations to be captured and redundancy in the description of data to be avoided. A simple default mechanism provides for concise descriptions while allowing for particular exceptions to inherited information to be stated in a natural way.",
        "GPT2_formal_text": "= {[CLS], [SEP]}, {[SEP]}, and [PAR]}, where [CLS] is the start token and [SEP] is the end token. The [PAR] token acts as a placeholder for the connection between [CLS] and [SEP] tokens. This connection can either be left out to simplify things, or it can be added as a pointer to the actual relation using the ID relation token. The [CLS] token and the [SEP] token are just some basic bits of text that don’t need any fancy syntax or semantics. The connection from [CLS] to [SEP] can only have one type of relation, which is either : A : B or : A : B. Formal: 1. The relation representation for the relation A in the set A is created as a combination of the representations for the relation B and the relation A. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C96",
        "filename": "C96-2109.json"
    },
    {
        "casual_text": "\"The Extra Girl\" was made by Mack Sennett. b. It came after some earlier movies that were about the movie business. c. Plus, it kind of set the stage for later films that talked about Hollywood.",
        "formal_text": "[\"The Extra Girl\"] [was] produced by Mack Sennett. b. [\"The Extra Girl\"] followed earlier films about the film industry. c. [\"The Extra Girl\"] also paved the way for later films about Hollywood.",
        "GPT2_formal_text": "_joint (v_t, t, w_i) = δ_t * w_i * (1 - δ_t) / (1 - δ_t)^2. The goal is to find the best pair (v_t, t, w_i) where the attention scores match exactly. Formal: Let's talk about a loss function that works on attention vectors and some parameters that help balance how much attention the model pays to different parts. Formal: The loss function we're talking about here is based on how the model pays attention to different parts. Here's the deal: The attention weights for a specific entity (v_t, t) are calculated by adding up the attention weights for all the entities in the sample (v_t, t) with weights r_v_t, t. Formal: To make sure the attention weights don't get messed up, we use a non-linear function to balance how much attention each part gets. Formal: We also add a weight called β_j, j to the model's parameters, which helps balance the attention among different parts. Formal: The model's parameters are calculated using the formula δ_t * w_i * (1 - δ_t)^2. Formal: Here, h_t, t and w_i are the model's hidden states, and f_j is the gate vector that decides how much attention each part gets. Formal: The attention weights for a specific entity (v_t, t) are calculated by adding the attention weights for all the entities in the sample (v_t, t) with weights r_v_t, t. Formal: We also add a weight called β_j, j to the model's parameters, which helps balance the attention among different parts. Formal: We also add a weight called β_j, j to the model's parameters, which helps balance the attention among different parts. Formal: Lastly, we use a non-linear function to balance the attention between different parts. Formal: Formal: Formal: Formal: We also use a non-linear function to balance the attention between different parts. Formal: Formal: Lastly, we use a non-linear function to balance the attention between different parts. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C08",
        "filename": "C08-1043.json"
    },
    {
        "casual_text": "The correct classification is represented by y_i. The generator's loss function is the same as the one in the original paper, as shown in equation 2.",
        "formal_text": "where y i is the correct classification. The generator loss function is the same as the original as shown in equation 2.",
        "GPT2_formal_text": "= { i, j } for i, j in the set {a, c, e, f, g, h, i, j }, and for j in range (a, c, e, f, g, h). Formal: So, we're using a matrix G = (G_i, G_j), where i is the position of the first token, and j is the position of the last token. The i-th row is just the token representation. Formal: For token i, i goes from 1 to m, and j from m to n, as shown in equation 5. Formal: We figure out the probability P(i | j) by looking at the expected value of P(i|j) given the other tokens in the sequence. Formal: Here, we're assuming that the sequence is a positive sample, so it's expected that the token representation (G(i, j)) will be positive. Formal: Here, we're ignoring the sequence length, which is the number of tokens. Formal: Basically, we're saying that the expected value of G(i, j) is just the difference between G(i, j) and G(i, j-1), plus the expected value of G(i | j) given the sequence length. Formal: Formal: We also consider the average of the differences between G(i, j) and G(i-1), plus the average of the differences between G(i | j) and G(i-1-1). Formal: The expected value of G(i | j) is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C18",
        "filename": "C18-1040.json"
    },
    {
        "casual_text": "In the world of combinatorial optimization, problem (4), which takes input B, is called the Linear Ordering Problem. This problem pops up in all sorts of real-world situations, like in economics, sociology, graph theory, drawing graphs, archaeology, and even when scheduling tasks (Grötschel et al., 1984). Researchers often test it out on real data using \"input-output\" matrices that show how resources move between different sectors of the economy (Schiavinotto and Stützle, 2004).",
        "formal_text": "In the combinatorial optimization literature, the maximization problem (4) (with input B) is known as the Linear Ordering Problem. It has numerous practical applications in fields including economics, sociology, graph theory, graph drawing, archaeology, and task scheduling (Grötschel et al. , 1984). Computational studies on real data have often used \"input-output\" matrices representing resource flows among economic sectors (Schiavinotto and Stützle, 2004).",
        "GPT2_formal_text": "ed. It's basically a tanh layer that turns each input into a zero vector. Formal: In this paper, we’ve come up with a new way to use transformers to tackle questions. Instead of just picking a set of answers, we use a method called multimodal question answering (MQA). First, we use a deep CNN, which creates question-answer pairs, to create a question-answer pair (Q_A) representation. Next, we take the embeddings of each question-answer pair (Q_A, A) and use them to create a question-answer pair (Q_A, A), which is basically a combination of the embeddings of the question and answer. Next, we add a mix of attention weights to each of these embeddings, which helps create a question-answer pair (Q_A, A), but it’s not fully connected yet. Lastly, we generate the whole question-answer pair by doing a binary classification task on the question-answer pair. Formal: Here’s a quick rundown of the MQA approach: (1) We start by building a question-answer pair using a deep CNN to create question-answer pairs (Q_A, A). (2) Next, we use a mix of attention weights to make each embedding in the question-answer pair (Q_A, A) act as a question-answer pair (Q_A, A) for question classification. Formal: Lastly, we generate the whole question-answer pair by doing a binary classification task on the question-answer pair. Formal: Here’s a more formal version: Formal: You can find all the code and datasets we used for our experiments over at https://github.com/mpqr. Formal: To see how well MQA works, we ran some experiments on our own datasets: the MSRA dataset and the CoQA dataset. Formal: For the MSRA dataset, we compared how well MQA and BERT-based models did on the main task (question answering) compared to the auxiliary task (answer selection) with a specific control setting (0.1). Formal: We did the same comparison for the CoQA dataset and also compared MQA to some other models, like BART, for the auxiliary task. Formal: We’re sharing the results of our MQA model and its top-performing model",
        "directory": "D09",
        "filename": "D09-1105.json"
    },
    {
        "casual_text": "We dug into the backgrounds of all 145 key employees by doing web searches and checking out public records or articles about them. To make sure we were looking at the right person, we added the word 'Enron' to our search terms. In the public records we found for each employee, we paid attention to any clues about their gender, like pronouns (he/him/his or she/her) or titles (Mr., Mrs., Ms., Miss). Since these folks were big shots at Enron during its bankruptcy, it wasn’t too hard to find info about them. For instance, the page we found on Kay Mann clearly shows she’s a woman. Using this method, we managed to figure out the gender of all 145 employees. One cool thing about doing it this way is that it gives us 100% confident gender assignments for everyone in our data set.",
        "formal_text": "We researched each of the 145 core employees using web search and found public records about them or articles referring to them. In order to make sure that the results are about the same person we want, we added the word 'enron' to the search queries. Within the public records returned for each core employee, we looked for instances in which they were being referred to either using a gender revealing pronoun (he/him/his vs. she/her) or using a gender revealing addressing form (Mr. vs. Mrs. /Ms. /Miss). Since these employees held top managerial positions within Enron at the time of bankruptcy, it was fairly easy to find public records or articles referring to them. For example, the page we found for Kay Mann clearly identifies her gender. 2 We were able to correctly determine the gender of each of the 145 core employees in this manner. A benefit of manually determining the gender of these core employees is that it ensures a high coverage of 100% confident gender assignments in the corpus.",
        "GPT2_formal_text": "= {t_i} = {t_i}|d_i, where d_i represents the number of characters in the source text. Formal: To get this from the query, you just use the character embedding of the query. Formal: If the query doesn't have any characters in it, you can just skip it. Formal: You can also use the character embeddings of the words in the query to get the query embedding, like this: Formal: Here, t_i is the character embedding from the query word t, d_i is the character embedding from the source word d_1, and the highest similarity between t and d_i is calculated using cosine similarity. Formal: Formal: This method doesn't rely on anything in the source text. Formal: Formal: It's worth mentioning that there are other ways to compute similarity and similarity embeddings, but we'll save those for later. Formal: Formal: The cosine similarity, on the other hand, is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D14",
        "filename": "D14-1211.json"
    },
    {
        "casual_text": "Over those twelve years, more than 1,100 journalists and media workers were killed while doing their jobs. Some were targeted because someone didn’t like what they wrote or said, while others were in the wrong place at the wrong time. Table 5 shows an example sentence in Arabic Broadcast News, with the original text in Iraqi Colloquial Arabic (ICA) and Modern Standard Arabic (MSA). It also includes translations done using rule-based MT (LFG), statistical MT (SMT), and hybrid MT (HMT).",
        "formal_text": "During the twelve years which passed more than one thousand and one hundred journalists and media workers performing their duty, were killed because there was one who did not like what they wrote or what they said or because they were in a place and time they should not be present at. Table 5. Example sentence Arabic Broadcast News; Original in Iraqi Colloquial Arabic (ICA) and Modern Standard Arabic (after normalization; MSA) and the translations using rule-based MT (LFG), statistical MT (SMT) and hybrid MT (HMT).",
        "GPT2_formal_text": "ing: The attention module uses a masked language modeling objective (Srivastava et al., 2015) with an attention mechanism (Bahdanau et al., 2015). This objective helps the model focus on the important parts of the input instead of overfitting to the full input. Formal: The IRSTLM setup has an output dimension of 200, a hidden layer dimension of 5, and a hidden layer weight of 2. This setup is based on the Transformer architecture (Vaswani et al., 2017). Formal: In IRSTLM, the bidirectional LSTM encoder and decoder are bidirectional, and the output vector is the combination of the last hidden state of the encoder and the previous hidden state of the decoder. Formal: The IRSTLM-adaptive pooling (IPSLM) model, which is based on Transformer, is trained by minimizing the cross-entropy loss. Formal: Here’s a diagram of the IRSTLM-adaptive pooling model (IPSLM) from Table 1, showing the parameters and their gradients for a specific input sequence. Formal: In the sentence-level transfer model, the input sequence is represented as a three-dimensional feature vector that includes the source context and the generated target context. Formal: The entity-based transfer model uses the model representations to transfer knowledge from the target entity to the source entity. Formal: Here’s an image showing the IRSTLM-adaptive entity-based transfer model (IGETLM), which is trained using the same transfer loss as the sentence-level model. Formal: Formal: Here’s an example of an input sentence with the predicted entity type. Formal: The entity-based transfer model also uses the model representations to transfer knowledge from the target entity to the source entity. Formal: The entity-based transfer model uses the model representations to transfer knowledge from the target entity to the source entity. Formal: The entity-based transfer model also uses the model representations to transfer knowledge from the target entity to the source entity. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "amta",
        "filename": "2010.amta-papers.5.json"
    },
    {
        "casual_text": "The other stuff—like adverbs, where they go, tenses, helping verbs, active or passive voice, and negatives—gets stored in something called a \"history.\" This history has a section for each sentence that matches the T-expression we're looking at. When we look up the T-expression in our knowledge base, we link up its three parts and add the history H to it. So, you can think of the final entry in the knowledge base as a kind of \"condensed version\" of the sentence's grammar in English.",
        "formal_text": "The remaining parameters--adverbs and their position, tense, auxiliaries, voice, negation, etc. --are recorded in a represen. rational structure called history. The history has a page pertaining to each sentence which yields the given T-expression. When we index the T-expression in the knowledge base, we cross-reference its three components and attach the history H to it. One can thus think of the resulting entry in the knowledge base as a \"digested summary\" of the syntactic structure of English sentences.",
        "GPT2_formal_text": "and meta_ids, respectively. Formal: We grab a bunch of entities from the training data by running the SVM algorithm on the data. Formal: We figure out the hidden representation of the entity from the embeddings from the embedding layer of the model. Formal: We also grab a representation of each entity and its related relationship from the entity-relation layer of the model. Formal: Finally, we calculate the loss for the whole entity pair. Formal: We noticed that entities with similar relationships tend to have similar embeddings, which gives them similar embeddings to each other. So, we use the cross-entropy loss function to get a clear picture of how the entities are related. Formal: We use the entity-relation matrix to get the embeddings for entities and their related relationships. Formal: The embedding layer for the entity-relation layer includes the embeddings for entities and their related relationships. Formal: We set up a structured perceptron for the entity pair, which is based on the embedding layer for the entity-relation layer. Formal: For each pair of entities, we create two embedding matrices, one for the entity-relation and one for the entity-entities. Formal: We set up a structured perceptron to figure out the embeddings for entities and their related relationships. Formal: The entity-relation matrix is the embedding layer for the entity-relation layer. Formal: We use the top-k embeddings from the entity-relation layer. Formal: The entity-relation layer includes the embeddings for entities and their related relationships. Formal: We use the top-k embeddings from the entity-relation layer. Formal: The entity-relation layer includes the embeddings for entities and their related relationships. Formal: Formal: The entity-relation layer includes the embeddings for entities and their related relationships. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C88",
        "filename": "C88-1065.json"
    },
    {
        "casual_text": "Warrens (2010c) talks about another popular measure called the Odds Ratio, which is written as ad/bc. This measure is often used in Epidemiology, not in fields like Computer Science or Computational Linguistics. Another closely related concept is the Determinant of the Contingency Matrix, which is dtp = ad-bc = etp-etn. This is based on independent marginal probabilities, as used in Chi-Sqr, Cohen, and Powers methods. Both the Odds Ratio and the Determinant help figure out if the odds of positive cases are higher for the first rater (the real one) compared to the second rater (the predicted one). For the Odds Ratio, this means the value should be greater than one, and for the Determinant, it should be greater than zero. It's worth noting that if you take the logarithm of all the coefficients, the relationship stays the same. The difference of these logarithms is equal to the logarithm of the ratio, which ties it all to the information domain.",
        "formal_text": "Warrens (2010c) discusses another commonly used measure, the Odds Ratio ad/bc (in Epidemiology rather than Computer Science or Computational Linguistics). Closely related to this is the Determinant of the Contingency Matrix dtp = ad-bc = etp-etn (in the Chi-Sqr, Cohen and Powers sense based on independent marginal probabilities). Both show whether the odds favour positives over negatives more for the first rater (real) than the second (predicted) -for the ratio it is if it is greater than one, for the difference it is if it is greater than 0. Note that taking logs of all coefficients would maintain the same relationship and that the difference of the logs corresponds to the log of the ratio, mapping into the information domain.",
        "GPT2_formal_text": "= { 0, 1 } n_mask = { 0, 1 } n_att = { 0, 1 } n_att_v = { 0, 1 } f_att = { 0, 1 } G_att = { 0, 1 } H_att = { 0, 1 } self.att = { 0, 1 } self.att_v = { 0, 1 } self.att_v_v = { 0, 1 } self.att_v_v_att = { 0, 1 } self.att_v_v_att_v = { 0, 1 } self.att_v_v_att = { 0, 1 } self.att_v_v_att_v = { 0, 1 } self.att_v_v_att_v_mask = { 0, 1 } self.att_v_v_mask = { 0, 1 } self.att_v_v_mask_v = { 0, 1 } self.att_v_v_mask_v_mask = { 0, 1 } self.att_v_v_mask_v_mask = { 0, 1 } self.att_v_v_mask_v_mask = { 0, 1 } self.att_v_mask_v_mask = { 0, 1 } self.att_mask = { 0, 1 } self.mask = { 0, 1 } self.mask_v = { 0, 1 } self.mask_v_v = { 0, 1 } self.mask_v_v_v = { 0, 1 } self.mask_v_v_v_mask = { 0, 1 } self.mask_v_v_v_mask = { 0, 1 } self.mask_v_v_v = { 0, 1 } self.mask_v_v_v_mask = { 0, 1 } self.mask_v_v_v_mask = { 0, 1 } self.mask_v_v_v_mask = { 0, 1 } self.mask_v_v_v_mask = { 0, 1 } self.mask_v_v_v_mask = { 0, 1 } self.mask_v_v_v_mask = { 0, 1 } self.mask_v_v_v_mask = { 0, 1 } self",
        "directory": "E12",
        "filename": "E12-1035.json"
    },
    {
        "casual_text": "2. Repeatability, or measurement repeatability, is basically how precise a measurement is when you do it under the same conditions over and over again.",
        "formal_text": "2. 21 measurement repeatability (or repeatability, for short) is measurement precision under a set of repeatability conditions of measurement.",
        "GPT2_formal_text": "\" and \"gate_ids\" respectively. The model should give predictions that are only partly influenced by the current question. This helps show how useful the answer and the question are for each other, helping to explain why asking the question is the best way to get the answer. Formal: In this part, we’re checking out how the model makes its predictions compared to the reference questions. Formal: We’re using the TAT-QA model as an example to compare its predictions with the reference questions. Formal: For the first model, \"TAT-QA,\" we’re only looking at the first (T) and second (S) answers in the question. Formal: For the second model, \"TAT-QA +1,\" we’re checking the answers from both the question and the answer. Formal: For the third model, \"TAT-QA +2,\" we’re checking the answers from both the question and the answer. Formal: Formal: Finally, we’re looking at how the model’s predictions differ when compared to the reference questions. Formal: Formal: Formal: This research is all about using question-answer pairs to learn representations for document-level and question-answer pairs, and then figuring out which questions are likely to cause the answer. Formal: The main goal is to find the answer by looking at how the answer is likely to be connected to the question, especially when the question is different from the answer. Formal: We’re checking out how the model handles different combinations of question-answer pairs and different topics. Formal: Formal: We’re looking at how the model handles the relationship between the answer and the question, how it deals with different topics, and how it deals with the relationship between the question and the answer. Formal: Formal: Lastly, we’re looking at how the model handles generalization. Formal: Formal: Formal: Basically, we’re looking at how the model learns representations for question-answer pairs that are close to each other but also consider how different the topics are in each pair. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.2.json"
    },
    {
        "casual_text": "For our experiments, we used two special datasets that other researchers have used before to find matching terms in two languages for technical stuff.",
        "formal_text": "For our experiments, we used two specialized comparable corpora that have been used in previous works for bilingual terminology extraction in technical domains:",
        "GPT2_formal_text": "_s = (h_mask, s_mask, z_mask) Formal: The S-list starts off with the top-N words (like the ones at the beginning) and then picks out the words that are next to them. Formal: It's not exactly the same as the input list because the output sequence is made up of more than N words. So, we make sure to grab the words that are next to the last word in the sequence. Formal: We also check the output sequence by looking at the beginning and end of the source sentence. Formal: Formal: After that, we decide if the word should be masked or not. Formal: We use a tool called BERT-Large (from Devlin et al., 2019) to create a hidden representation of the word, and then we feed it into a softmax layer with a ReLU activation function. Formal: To make sure the masked word isn't too similar to the target word, we swap the target word with itself. Formal: We do this by using a linear transformation and a softmax activation function, then use a max pooling layer to get the final result. Formal: If the masked word isn't in the target vocabulary, we just remove it from the output sequence. Formal: The input to the re-ordering model is just the target word and the list of words that came before it in the input sentence. Formal: We take the hidden representation of the masked word and pass it through a fully connected layer to create a new word vector. Formal: After that, we apply a softmax function to that vector to get the output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C18",
        "filename": "C18-1242.json"
    },
    {
        "casual_text": "We're using D(iscourse) R(epresentation) T(heory) as our semantic representation, which was developed by Hans Kamp [4]. Basically, we're not going with the semantic theory for L(exical) F(unctional) C(grammar) suggested by Per-Kristian Halverson [2]. Halverson converts the functional structures of LFG into what he calls semantic structures, which are basically scyclic graphs. These semantic structures come from a translation process that connects formulas of intensional logic to the semantic forms found in the functional structure. \n\nWe’re not taking this approach for a reason, which we’ll explain by laying out some requirements that a semantic representation needs to meet to handle text processing effectively. Then, we’ll prove why these requirements are super important by looking at some example sentences and discourses.",
        "formal_text": "As senantic representation we use the D(iscourse) R(epresentation) T(heory) developped by Hans Yamp [4]. I. e. we do not adopt the semantic theory for L(exical) F(unctional) C~rammr) proposed by Per-Kristian Halverson [2]. Halverson translates the f~nctional structures of LFG into so-called semantic structures being of the same structural nature, namely scyclic graphs. The semlntin structures are the result of a translation procedure which is based on the association of formulas of intensional logic to the semantic forms appearing in the functional structure. The reason not to take this approach will be explained by postulating some requirements a se~anclc representation has to fulfill in order to account for a processing of texts. Tnen we will show that these requlr~ents are rP~I]y necessary by analysing some sample sente, ces and discourses.",
        "GPT2_formal_text": "' and the top-one matching list in the multilingual BLEU score. We don't need to bother with the n-gram and POS tagging separately, though. Formal: (1) A sentence has two parts: (a) a topic and (b) a question. (2) We take the words and phrases that give us the answer and assign them to the topic and the question separately. Formal: (3) The answer to a question can be any sentence, even those that don't answer any question. Formal: (4) We figure out how relevant each word in the answer is to the question, as shown in Table 2. Formal: (5) For the top-N answer translations (for the SQuAD test set), we only consider the word that matches most closely to the question in the question's sentences. Formal: (6) We only consider answers that are words that match a question's main word or parts of words. Formal: (7) If there's an answer that doesn't match any question words, we just don't consider it. Formal: (8) For all the multi-choice questions, we only consider the top-N answer translations, but we still consider the answer to be relevant to the question. Formal: (9) The answers to questions aren't just words or phrases—they include their meanings and grammar tags. Formal: (10) We also count the top-N answer translations as important answers. Formal: (11) We use the QASC metric to see how well the answers to questions match the question. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E83",
        "filename": "E83-1009.json"
    },
    {
        "casual_text": "We introduced a new NLU task where the goal is to pick the best counter speech that can oppose a given input speech from a bunch of options.",
        "formal_text": "We presented a novel NLU task of identifying a counter speech, which best counters an input speech, within a set of candidate counter speeches.",
        "GPT2_formal_text": "_layers(s_t, l_t) = (a_t, a_t) * h_t * l_t. Formal: The attention weights are calculated using a tanh function on the projected representation. The attention size is also calculated using tanh. Formal: Next, the attention weights are added to the l_t-th hidden state (h_t-1). Formal: This is the final output of the attention mechanism. Formal: The decoder itself takes the input word w_t, figures out the attention weights for the hidden state h_t, and uses them to figure out the current representation of the word. Formal: After that, it combines the hidden state h_t-1 with the decoder's output to create the current sentence representation y_t. Formal: Here's a simple example to explain how this works. Formal: The attention weights for a word are found by taking its projected representation and adding its attention weights to the decoder's output. Formal: The attention weights for a word are calculated using a tanh function on the projected representation. Formal: Formal: Lastly, the attention weights for a word are added to the l_t-th hidden state (h_t-1). Formal: Formal: And finally, the decoder's output is combined with the encoded representation y_t to create the current sentence representation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.633.json"
    },
    {
        "casual_text": "Sure! So, pronominal anaphora (like \"he\" or \"she\") and named entities (like names of people or places) can be handled pretty well by systems, but when it comes to definite noun phrases (like \"the cat\" or \"the book\"), things get trickier. Usually, these systems can only handle cases where both mentions have the same noun, like \"the cat\" and \"the cat.\" This is what Vieira and Poesio (2000) call \"direct coreference.\" \n\nBut when we're dealing with what they call \"coreferent bridging\" (which is like when the mentions don't have the same noun but still refer to the same thing), it's way harder. This is because there are way more possible candidates to consider, and you can't just rely on the words looking similar on the surface.",
        "formal_text": "While the resolution of pronominal anaphora and tracking of named entities is possible with good accuracy, the resolution of definite NPs (having a common noun as their head) is usually limited to the cases that Vieira and Poesio (2000) call direct coreference, where both coreferent mentions have the same head. The other cases, called coreferent bridging by Vieira and Poesio 1, are notably harder because the number of potential candidates is much 1 Because bridging (in the sense of Clark, 1975, or Asher and larger when it is no longer possible to rely on surface similarity.",
        "GPT2_formal_text": "_m = (p_i, s_i, h_i), where p_i is the phrase, s_i is the sentence, and h_i is the length of s_i. Formal: The loss function L_M(w_i, s_i) is calculated by adding up the cross-entropy loss L_m for the current sentence, x, and the predicted length, y, as well as the cross-entropy loss L_k(w_i, s_i) for the pair (x, y). Formal: We calculate the cross-entropy loss L_m(w_i, s_i) for the current sentence x by taking the log of the cross-entropy between the sentence and the predicted length, y. Formal: L_M(w_i, s_i) is a weighted sum of the cross-entropy losses, with weights L_k(w_i, s_i) for the pair (x, y). Formal: In a log-linear model, the loss function is set to maximize the log likelihood, which is a common optimization method (like the one from Ainsworth et al., 1993). Formal: To avoid the issue with min-pooling and recurrent learning, we train the model using an LSTM-based architecture. Formal: L_k(w_i, s_i) is a weighted sum of the cross-entropy losses for the sentence x, the predicted length y, and the predicted length prediction, y', as well as the cross-entropy loss L_m(w_i, s_i). Formal: The hidden state representation h_i is created by combining the hidden state from the predicted length prediction and the one from the current sentence. Formal: The model can be trained using a maximum likelihood approach or stochastic gradient descent. Formal: The cross-entropy loss L_m(w_i, s_i) is a weighted sum of the cross-entropy losses for the sentence x and the predicted length y. Formal: Formal: We also look at how the phrase length affects the loss. Formal: The model can be trained using stochastic gradient descent. Formal: Formal: We estimate the loss using stochastic gradient descent. Formal: Formal: Formal: Formal: We train the model using the cross-ent",
        "directory": "D07",
        "filename": "D07-1052.json"
    },
    {
        "casual_text": "In the more flexible version of GPSG, they use unification to build trees. In the version from [GKPS85], the main category R of a smaller tree has to match up with a &mghter category C from a local tree (basically, R __. C and C ~ R).",
        "formal_text": "In the constructive version of GPSG, unification is used for tree formation. In the version of [GKPS85] the root category R of a subtree has to be identical with a &mghter category C of a local tree (i. e. R __. C and C ~ R).",
        "GPT2_formal_text": "_tgt. The model has two main parts: an attention layer for word embeddings and a transformer layer for the hidden layer. Formal: There’s a sentence X s = {s 1, s 2, . . . , s m } with n words, and the attention part is set up as a convolutional layer. Formal: The hidden state h t is in R^d, and the attention matrix is in R^T. Formal: The global entity attention layer is a fully connected layer with a hidden size of d h. The output from this layer is represented as x h t, which is a matrix in R^T x h t. Formal: We’ll call the hidden state h t in the sentence embedding layer h t. Formal: For each word w t in the sentence, we calculate the dot product of its embedding h t and its embedding matrix x t to get its hidden state h t t. Formal: For each sentence s in the dataset D_s, we calculate the dot product of its embedding h t and its embedding matrix x t to get its hidden state h t t. Formal: For all the layers in the attention module, we use the dot product of the h t embedding vector h t to get the hidden state h t t, which we can write as h t t. Formal: We use the dot product of the h t embedding vector h t t to get the hidden state h t t. Formal: Finally, the word embedding layer gets its hidden state h t t by combining the dot product of the embedding matrix h t and its hidden state h t t. Formal: Formal: Finally, the global entity attention layer gets its hidden state h t t by combining the dot product of the entity embedding matrix h t t and its hidden state h t t t. Formal: Formal: Formal: Lastly, the global entity attention layer gets its hidden state h t t by combining the dot product of the entity embedding matrix h t t t and its hidden state h t t t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C88",
        "filename": "C88-2151.json"
    },
    {
        "casual_text": "There are even more intricate differences to consider. For instance, Swahili has a more elaborate system with 18 classes. These systems are usually called 'noun classes' in the academic world and don't fall under the term 'grammatical gender' in this paper.",
        "formal_text": "More complex distinctions are also found. As an example, Swahili has a more complex system with 18 classes. These systems are generally referred to as 'noun classes' in the literature and are not covered by the term 'grammatical gender' in the current paper.",
        "GPT2_formal_text": "= { id_1, id_2, ..., id_K }, with d set to d_s. Formal: For the last question in the sequence, we combine all the current question representations with the current question to get the answer representation. Formal: We train the classifier using this combined question representation, along with the current question. Formal: This whole process is repeated for each question in the sequence, where the combined question representation and current question representation are combined. Formal: When we're decoding, we start with the question vector in the sequence and pick the closest question as the answer. Formal: The full process of how we encode a question and a question vector together is shown in Fig. 1. Formal: We use a question vector to represent the current question in the sequence, and the question vector to represent the previous question in the sequence. Formal: The whole process is repeated for each question in the sequence. Formal: We first encode a question vector using the sequence and the question vector, and then we do a query retrieval using the first question vector. Formal: Formal: The whole process of encoding a question and a question vector together is shown in Fig. 2. Formal: Formal: Formal: The query retrieval part is shown in Fig. 3. Formal: Formal: Formal: The whole process is repeated for each question in the sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "conll",
        "filename": "2020.conll-1.20.json"
    },
    {
        "casual_text": "Lastly, like we talked about earlier, we get rid of substitution (Joshi and Schabes, 1997, footnote 6). After making these tweaks, the sample TAG grammar and the derivation tree from Figures 1 and 4(a) could be shown using the core TAG grammar and the derivation tree from Figures 2 and 4(c).",
        "formal_text": "Finally, as mentioned before, we eliminate substitution (Joshi and Schabes, 1997, fn. 6 ). With these changes, the sample TAG grammar and derivation tree of Figures 1 and 4(a) might be expressed with the core TAG grammar and derivation tree of Figures 2 and 4(c).",
        "GPT2_formal_text": "ing is basically a way to adjust the attention mask. This method works for all the tokens in the input sequence. We use a softmax function to figure out how important each token is. Basically, we take the hidden states from the layer that directly relates to the token we're looking at, and we use those to represent the whole input sequence. We call this representation the attention vector. This vector then gets sent to the output layer of the model. Formal: We also have this extra layer called the selective attention layer. This layer takes the output from the attention mask and creates a more detailed representation, kind of like the attention vector itself. Formal: Let's call this new layer the selective attention layer. Formal: Let's say t is the value at layer t. Formal: After the selective attention layer, the output from the first attention layer gets passed to the selective attention layer. Formal: This means the input to the selective attention layer is passed through a softmax function. Formal: The selective attention layer looks at the output from the first attention layer and picks out the important tokens from the input to create the attention vector for the whole input sequence. Formal: Finally, the output from the selective attention layer is passed to the output layer. This output layer then decides how much weight to give to the important tokens in the input. Formal: If the input to the selective attention layer is empty, the output layer uses the attention vector from the first attention layer to create the attention vector for the whole input sequence. Formal: Formal: Formal: Basically, the selective attention layer combines the output from the first attention layer and the attention vector from the selective attention layer to get the attention vector for the whole input sequence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E06",
        "filename": "E06-1048.json"
    },
    {
        "casual_text": "We're using 300-dimensional pre-trained GloVe 2 word embeddings (thanks to Pennington et al., 2014) that were trained on web data. We've got three different convolution filters—sizes 3, 4, and 5—each with 100 feature maps. The GRU's hidden states are set to 300 dimensions. For optimization, we're using the adam optimizer (props to Kingma and Ba, 2015) with an initial learning rate of 1.0 × 10^-4. The learning rate gets cut in half every 20 epochs during training. We've also set the dropout probability to 0.3.",
        "formal_text": "We use 300-dimensional pre-trained GloVe 2 (Pennington et al. , 2014) word-embeddings which is trained from web data. We use three distinct convolution filters of sizes 3, 4, and 5 respectively, each having 100 feature maps. The dimension of the hidden states of the GRU is set to 300. We use adam(Kingma and Ba, 2015) optimizer and set the initial learning rate as 1. 0 × 10 −4. The learning rate is halved every 20 epochs during training. Dropout probability is set to 0. 3.",
        "GPT2_formal_text": "_for_str) = (1, 1), (1, 2), (1, 3), (1, 4), (1, 5) Formal: So, let's say we have a substring called \"a\" that starts with \"a\", and we're trying to match it to its matching substring, \"b\". In Figure 1, the first part of the equation, \"(a, b)\" is what we get from the pattern \"a\" starting with \"a\". This matches \"a\" in the set of POS tags to \"b\". Formal: The second part of the equation, \"(a, b)\" is also part of the set of POS tags, and the POS tag for \"b\" is \"A\". Formal: The last part, \"(a, b)\" is just one of the POS tags in the set of POS tags for \"b\". Formal: In this situation, \"a\" is the substring we want to match to \"b\", and the way we measure how close we are is based on the average of the two POS tags for \"b\". Formal: Since the difference between \"a\" and \"b\" is the same as the difference between \"A\" and \"b\", we can use the POS tag for \"b\" as a rough measure. Formal: So, we're starting with the POS tags for \"a\" and \"b\", which we'll call \"x1\", \"x2\", and so on, up to \"xM\". Formal: The total number of POS tags in the set of POS tags for \"b\" is N. Formal: The number of tokens in \"b\" is the total number of tokens in \"a\", plus one. Formal: Lastly, the POS tag for \"b\" is \"B\". Formal: Formal: We start with the POS tags for \"a\" and \"b\", and then we add one more tag to \"b\", which is the POS tag for \"b\". Formal: Basically, we're adding one more POS tag to \"b\" and one more tag to \"a\". Formal: Formal: We're also making sure that the POS tags for \"b\" and \"a\" are on the same token length. Formal: Formal: We're using the POS tag for \"b\" as a rough measure of how similar the sequence of POS tags is to the sequence of words in \"a\". Formal: Formal:",
        "directory": "ccl",
        "filename": "2020.ccl-1.102.json"
    },
    {
        "casual_text": "You'll find more results and analysis in Appendix B. They show that our methods can also tell us how well different target languages work with a specific source language and vice versa for the tasks we talked about in this paper.",
        "formal_text": "Additional results and analyses are provided in Appendix B. They further demonstrate that our measures also indicate transfer quality of different target languages for a given source language, and transfer quality of source languages for a given target language, for the tasks discussed in this paper.",
        "GPT2_formal_text": "ed: W_src_mask = (w_src_mask, m_src_mask) + the_mask_mask. Here, the_mask_mask is the W_src_mask variable, which is like a function that takes the input word x_t and turns it into a variable for the masked word w_src_mask. Formal: Using the W_src_mask, we can build the hidden representation of the token. The first part of the sentence is passed into the hidden layer of the GCN and then fed into a bi-directional GRU, which is a kind of neural network setup. Formal: The hidden layer has two types of output neurons: one for the token and the other for the whole sentence. These output neurons help in figuring out the hidden representation for the token. Formal: Next, the hidden layer spits out the hidden vector for the token, which we call h_t. Formal: After that, we pass the output vector h_t through the hidden layer of the GCN to get the context vector c_t. Formal: Finally, the output neurons of the hidden layer get sent to the output layer of the GCN to predict the next word in the sentence, which we call e_t. Formal: The final result is the output vector h_t = (h_t1, h_t2, ..., h_tT). Formal: The goal of the GCN is to maximize the log likelihood of the token sequence h_t, where T is the total number of tokens. Formal: The GCN is a graph-based neural network model that uses graph convolutional networks (GCNs) to pull out a hidden representation of the sentence. Formal: To make sure the model is efficient, we use the weighted least square loss to estimate the parameters θ. Formal: Lastly, the tokens of the sentence are represented by a linear transformation matrix W_t = (w_t1, ..., w_tT). Formal: Formal: In this paper, we use the k-d convolutional neural network network (CNN) to pull out the hidden representation of the token sequence h_t. Formal: Basically, we use a CNN to learn the hidden representation of the sentence. Formal: Formal: Formal: Formal: We also look at the response vectors of the attention neurons and calculate the log likelihood of the response, which",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.186.json"
    },
    {
        "casual_text": "This paper is all about figuring out if pre-trained models have a special kind of common sense: the ability to compare objects physically, like which one is bigger or faster. The task we set up is pretty straightforward: we give the system two words, say \"car\" and \"bike,\" and ask it to decide which one is \"faster\" based on size or speed (see section 2.1). We use a simple model—either a linear one or a one-layer neural network—that just combines (by either putting them together or subtracting one from the other) the pre-trained word embeddings of the two words we’re comparing (section 2.2). This simple setup actually works better than older methods that used extra info, like the verbs connecting the words, on a dataset called Verb Physics (Forbes and Choi, 2017) (section 3). It shows that these pre-trained models can indeed make physical comparisons.\n\nWhat’s cool is that this model can also handle objects it hasn’t seen during training (section 3.1) and does better than other models that rely on quirks of the dataset (section 4). We kept the model simple on purpose because more complex models make it hard to tell if the results are coming from the model itself or the embeddings (like in other studies, e.g., Liu et al. (2019)).\n\nAnother big part of our work is analyzing how these models actually compare objects. The confidence scores (called logits) the model gives for each label show that it consistently ranks objects the same way, no matter which object you start with (section 4.1.1).",
        "formal_text": "This paper asks if pre-trained representations encode a specific type of common sense: physical comparisons between objects. 1 The supervised classification task takes a pair of words being compared on a physical attribute such as size or speed, with the system's objective to decide which is 'bigger' or 'faster' ( § 2. 1). We use a linear or a one-layer fully-connected neural network probing model with only a combination (concatenation or subtraction) of the frozen pre-trained embeddings for the words to be compared as input ( § 2. 2). This probing model achieves better accuracy than previous approaches ( § 2. 3) which use extra information other than the words (such as the verbs connecting the words) on the Verb Physics dataset (Forbes and Choi, 2017 ) ( § 3): it encodes physical commonsense comparisons. 2 It generalizes to objects not present in the training set ( § 3. 1) with higher accuracy than baselines exploiting dataset artifacts ( § 4). We use a 'simple' probing model since more complex models make it difficult to disentangle the major contributing factor to results -model or embeddings (as in other probing studies like Liu et al. (2019) ). Our other major contribution is analyzing how models compare objects. The output logits for labels (indicating model confidence) order objects consistently across orderings or rankings built around different objects ( § 4. 1. 1).",
        "GPT2_formal_text": "= [x_1, ..., x_L], seq_len = 8, m = 1, skip = -1)\n           Casual: To make the model generate text with different lengths, we add a skip step. After that, we use a softmax function to figure out how long the summary should be. Formal: So, in this setup, we’re using a hidden state (h) that’s part of a GRU, with dimensions d_s = 256 and h_s = 256. We’ve got a parameter α that affects how much we care about the length of the summary. The output of the model is a vector h_s = [h_1, ..., h_L], where each spot (x_i) represents the word in the summary, and α is calculated like this: Formal: In this setup, we’re using a hidden state (h) that’s part of a GRU, with dimensions d_s = 256 and h_s = 256. We’ve got a parameter α that affects how much we care about the length of the summary. The output of the model is a vector h_s = [h_1, ..., h_L], where each spot (x_i) represents the word in the summary, and α is calculated like this: Formal: We’ve got a hidden state (h) with dimensions d_s = 256 and h_s = 256. Formal: \n\nSo, in this setup, we’re using a hidden state (h) that’s part of a GRU, with dimensions d_s = 256 and h_s = 256. We’ve got a parameter α that affects how much we care about the length of the summary. The output of the model is a vector h_s = [h_1, ..., h_L], where each spot (x_i) represents the word in the summary, and α is calculated like this: Formal: \n\nWe’ve got a hidden state (h) with dimensions d_s = 256 and h_s = 256. Formal: \n\nSo, in this setup, we’re using a hidden state (h) with dimensions d_s = 256 and h_s = 256. Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-6016.json"
    },
    {
        "casual_text": "Our model was trained with just a little bit of data and not too many features. For future work, we want to try it out with lots of monolingual data, which would let us use way more features. Right now, we tested it on a language pair (Chinese to English) where there's a ton of bilingual stuff available for the domain we used. Later, we’re thinking of looking at low-resource areas and language pairs like Urdu-English, where there’s not much bilingual data for new domains.",
        "formal_text": "Our discriminative model used only a small amount of training data and relatively few features. In future work, we plan to test our method in settings where there are large amounts of monolingual training data (enabling many discriminative features). Also, our experiments here were performed on a language pair (i. e. , Chinese to English) that has quite rich bilingual resources in the domain of the test data. In future work, we plan to consider lowresource test domains and language pairs like Urdu-English, where bilingual data for novel domains is sparse.",
        "GPT2_formal_text": "ing and attention_attention_pooling) are all in PyTorch. We used the open-source HATM library from HuggingFace (Wolf et al., 2019). We set the hidden size to 256, the batch size to 32, and the number of layers to 4. During training, we kept the learning rate constant at 1e-4 and ran the model for 10 epochs. Formal: We split the data into three parts: training, validation, and testing. To keep things manageable, we skipped the validation part and kept the test set free. In this case, we set the number of documents to 3 and the dimensions to 200. For the evaluation, we used the development set with documents of 100 and 200. Formal: The validation and test sets are balanced: 80% for validation, 10% for testing. You can find more details in Appendix A. Formal: The development set has documents of 100 and 200. Formal: We trained the model for 10 epochs with a learning rate of 1e-4 and a batch size of 32. Formal: For the development set, we skipped the validation part and kept the test set free. Formal: For the validation set, we used the development set with documents of 100 and 200. Formal: We trained the model for 10 epochs with a learning rate of 1e-4 and a batch size of 32. Formal: For the validation set, we used the development set with documents of 100 and 200. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D11",
        "filename": "D11-1085.json"
    },
    {
        "casual_text": "The research on cross-lingual text retrieval (CLTR) has some work that’s pretty similar to what we’re doing here. Lately, a lot of approaches have been focusing on using dictionaries and corpora to translate queries from one language to another, like the language of the documents you’re searching through (Oard, 1997). For instance, some methods create queries in the target language using a corpus-based technique that’s kind of like what we’re talking about. But the thing is, they don’t really try to figure out if the terms they’re using are specific to a certain field or just general words, and they don’t involve any manual work.\n\nWhat we’re doing here is more about semi-automating the process of building translation dictionaries that are tailored to a specific field. You could think of it as providing bilingual dictionary entries for CLTR methods, similar to what Davis did later on (Davis, 1996). In his approach, he first used a dictionary to create a query in the target language, which could be a bit unclear, and then used a corpus to figure out the right meaning.",
        "formal_text": "The literature on cross-lingual text retrieval (CLTR) includes work that is closely related to this research, in that recent approaches emphasize the use of dictionary-and corpus-based techniques for translating queries from a source language into the language of the document collection (Oard, 1997). , for example, generate target-language queries using a corpus-based technique that is similar in several respects to the work described here. However, the approach does not attempt to distinguish domain-specific from general usage term pairs, and it involves no manual intervention. The work reported here, focusing on semiautomating the process of acquiring translation lexicons specific to a domain, can be viewed as providing bilingual dictionary entries for CLTR methods like that used by Davis in later work (Davis, 1996), in which dictionary-based generation of an ambiguous target language query is followed by corpus-based disambiguation of that query.",
        "GPT2_formal_text": "= {} p = q p a c t p b c t q p p. . . . q p a q p q p. . . a q p q. . . a q q. . . a q. q p. . . a q p. . . a q. Formal: The answer-to-question pair, like \"How many stars are in the night sky?\", is figured out using a pattern recognition method. Formal: The question and its answer are stored in the questionsheet. Formal: The questionsheet acts as a mini-table that shows the answers to the questions. Formal: This table is made up of a bunch of tables called T (where t i, j, k, . . . , t i, j, k, . . . , t i, j, k ). Each table is made up of the questions (q i, q j, q j, q j, q i, q j, q j, q j, q i, q j ), the answer (a i, a j, . . . , a j, k, . . . , a i, j, k ), and the words (a i, a j, . . . , a j, k ). Formal: Basically, the answer and the question are stored in the question, along with the words, in the answer. Formal: Then, the answer is sent to the question, which is stored in the question. Formal: The answer is sent to the question, which is stored in the question. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "A97",
        "filename": "A97-1050.json"
    },
    {
        "casual_text": "• We're using the Transformer model (Vaswani et al., 2017) here. It's trained with the OpenNMT library. For both news and dialogues, we keep the training parameters the same, except for the minimum length of the summary we generate—35 for news and 15 for dialogues.",
        "formal_text": "• Transformer (Vaswani et al. , 2017). The model is trained using OpenNMT library 4. We use the same parameters for training both on news and on dialogues 5, changing only the minimum length of the generated summary -35 for news and 15 for dialogues.",
        "GPT2_formal_text": "_x, and the user knows how to use them. Formal: Basically, we take the hidden state h_s from the RNNG's state, add it to the output vector v_s for the current sentence s_t, and then pass it through a softmax layer. Formal: Using the base attention mechanism (like the one in Eq. 2), we combine the hidden state h_s and the output vector v_s using a weighted sum. Formal: After that, we run this combined vector through a linear layer with ReLU, giving each result a 0 or 1. Formal: Finally, we get the final output for the sentence s_t. Formal: We're not gonna go into all the nitty-gritty details about how to compute these attention representations, like the weights and non-linearity. Instead, we'll just talk about how the attention mechanism works and how we add the hidden states to the output. Formal: As you can see in Figure 1, the base attention mechanism combines the hidden state h_s and the output vector v_s to create an attention representation. Formal: This attention representation h_s is fed into an attention module, which creates a different representation, which is then passed into another attention module. Formal: The final representation is calculated using the dot product of the hidden state h_s and the output vector v_s. Formal: We also use a linear layer to process the attention vector h_s, giving each result a 0 or 1. Formal: Finally, the final attention vector h_t. Formal: Here, the word \"the\" is a special word that acts as a key word, so it gets the highest attention score of 0. Formal: Formal: Also, remember that the output vector v_s is basically the sum of the attention vectors h_s and v_s. Formal: Formal: Lastly, we use a linear layer to process the attention vector h_t, giving each result a 0 or 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-5409.json"
    },
    {
        "casual_text": "Another approach to figuring out what pronouns refer to, like the one by Ge et al. (1998), uses stats to help out. They look at how far apart the pronoun and the possible word it’s referring to are to see how likely they are connected. They also check things like gender, number, and whether the word refers to a living thing to narrow it down. Plus, they use head information to set some rules and count how many times a word is mentioned.",
        "formal_text": "Other strategies to anaphora resolution, as the one introduced by Ge et al. (1998), use statistical information to resolve pronouns They use the distance between pronoun and the proposed antecedent to check the probability of the connection between them, information about gender, number, and animacity of the proposed antecedent as hints for the proposed referent, and head information to make selectional restrictions and mention count.",
        "GPT2_formal_text": "\" array (like in the example above). To speed things up for the Gibbs sampler, we'll only consider phrases with length d_s. The total cost of using this sampler is O(m^2). Formal: The Gibbs sampler takes O(m^2) time to run for each word in the sentence. Formal: For a single word, the cost is calculated using the log loss function (W_φ(w)), which is part of the EM algorithm. Formal: The error rate is calculated using the EM algorithm. Formal: The feature vector is calculated using the EM algorithm. Formal: Formal: The probability of a word is calculated using the EM algorithm. Formal: Formal: We don't limit the number of samples we can use for each word. Formal: Formal: Formal: The time it takes to run the Gibbs sampler is O(m^2). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C16",
        "filename": "C16-1077.json"
    },
    {
        "casual_text": "An alignment system matches up the surface representations of two languages. (Shin, 1996) tested an expectation-maximization algorithm and got 68.7% accuracy at the phrase level. This will be added to the version 2 platform.",
        "formal_text": "An alignment system gathers correspondences between surface representations of both languages. (Shin, 1996) experimented expectationmaximization algorithm with 68. 7 % accuracy at phrase level, and this will be incorporated into version 2 platform.",
        "GPT2_formal_text": "ed_ff = attention_masked_ff * response_masked_ff; + b = self.attention_masked_ff + b * self.attention_masked_ff; + } else { + b = self.attention_masked_ff * response_masked_ff; + } + return b; + } + + static int +attn_masked_ff(int a, int b) +{ + return a * b; + } + + static int +attn_masked_ff_raw(int a, int b) +{ + return a + b; + } + + int +attn_masked_raw_ff(int a, int b) +{ + return a + b; + } + + + static int +attn_masked_ff_raw_raw(int a, int b) +{ + return a + b; + } + + + static int +attn_masked_ff_raw_raw(int a, int b) +{ + return a + b; + } + + + int +attn_masked_ff_raw_ff(int a, int b) +{ + return a + b; + } + + + + static int +attn_masked_ff_raw_raw(int a, int b) +{ + return a + b; + } + + + + static int +attn_masked_ff_raw_raw(int a, int b) +{ + return a + b; + } + + + static int +attn_masked_ff_raw_raw(int a, int b) +{ + return a + b; + } + + + static int +attn_masked_ff_raw_raw(int a, int b) +{ + return a + b; + } + + + static int +attn_masked_ff_raw_raw(int a, int b) +{ + return a + b; + } + + + static int +attn_masked_ff_raw_raw(int a, int b) +{ + return a + b; + } + + + + static int +attn_masked_ff_raw_ff(int a, int b) +{ + return a + b; + } + + + + static",
        "directory": "C96",
        "filename": "C96-2185.json"
    },
    {
        "casual_text": "This approach can totally work in the food world too. For all our tests, we’re using a dataset from chefkoch.de, which is like the biggest German website for anything food-related. It has 418,558 forum pages, and we’re using that for our experiments. In Table 1, you can see how well coordination works for pulling out food items from our special food-focused data. \n\nWe started with a small list of 10 common food items (like water, salt, sugar, salad, bread, meat, cake, flour, and potato) and then looked at all the combinations of words that come up together, ranking them by how often they appear. We did this both on our food-specific data and on Wikipedia. As a comparison, we also just ranked all the nouns by how often they show up in our food data. \n\nThe table shows that just going by how often something appears isn’t really effective. But when we use those word combinations (conjuncts) from our food-specific data, we get much better results.",
        "formal_text": "This method can also be applied to the food domain. As a domain-specific dataset for all our experiments, we use a crawl of chefkoch. de 3 (Wiegand et al. , 2012) consisting of 418, 558 webpages of forum entries. chefkoch. de is the largest German web portal for food-related issues. Table 1 shows the effectiveness of coordination as a means of extracting food items from our domain-specific corpus. Given a seed set of 10 frequent food items (we use: water, salt, sugar, salad, bread, meat, cake, flour, potato), we compute all conjuncts and rank them according to frequency. We do this on our domainspecific corpus and on Wikipedia. As a baseline, we simply sort all nouns according to frequency in our domain-specific corpus. The table shows that ranking by frequency is no effective method. Conjuncts produce good results provided that they are extracted from a domain-specific corpus.",
        "GPT2_formal_text": "(A_i) = arg max P(z, A_i | h_i) θ. This is a straightforward, straightforward version of the cross-entropy loss function. Formal: We're looking at how the model performs based on the length of the input sentence, h_i, and the model's predictions. Formal: The number of possible output tokens is called θ_t, and the probability of a token is calculated as P(z|A_t, h_i, t) = arg max P(z, A_t | h_i, t) θ. Formal: The size of the vocabulary is also a factor, but it's not directly tied to the model's prediction. Formal: We're assuming that if a word is at the start of a sentence, it should be tokenized at the very beginning. Formal: If a word isn't at the start of a sentence, it should be tokenized at the very end. Formal: We're assuming that the output tags are zero-centered (which is the usual setup in word2vec). Formal: We're also assuming that the model can learn from the input sequence, h_i, and can predict the token length, z_i. Formal: This is a pretty common setup in word2vec models. Formal: We're assuming that there are no hidden changes in the model's parameters during training. Formal: The model can be trained using cross-entropy loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C14",
        "filename": "C14-1216.json"
    },
    {
        "casual_text": "CVaR is being too cautious because it considers all kinds of groups, even ones that are totally messed up and don't make sense. To fix this, we'll focus on optimizing models for actual, meaningful subgroups instead of just any random group.",
        "formal_text": "Sentence CVaR is too conservative since it allows for arbitrary groups -including ones consisting of purely invalid sentences. To remedy this, we will optimize models for all meaningful subpopulations instead of arbitrary ones.",
        "GPT2_formal_text": "feature:\n          Casual: We tested the model using the development set, and the results are in Table 6. For all three tasks, the model performed really well and hit the best F1-scores ever recorded. For the Tasks A1-C1 and C1-D1, the model also improved the model's performance on the development set. However, the Tasks A2-C2 and A3-C4 got worse compared to the development set. This shows that the models are still not fully optimized for each task. For the three tasks, we used the same dataset as Li et al. (2015). Formal: We’re using the full training set for all the tasks except A1-C1, C1-D1, and A2-C2. Formal: For Tasks A1-C1, C1-D1, and A2-C2, we combined all the datasets we had into one big training set. Formal: For the remaining tasks, we only used the development set. Formal: You can find more details in the paper. Formal: Here’s Figure 2: It shows the attention mask for two images. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-1432.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way:\n\nGCN GCN GCN refers to Graph Convolutional Networks, which are used in machine learning to work with graph-structured data.\n\nQuantity Cell Q \" Q # Q $ Q % \" # % $ is talking about different quantities or values in a cell, maybe related to data or calculations.\n\nQ \" Q # Q $ Q % BiLSTM is combining those quantities with a BiLSTM, which stands for Bidirectional Long Short-Term Memory, a type of neural network often used for sequence prediction problems.\n\nSo, in short, it's about using GCNs and BiLSTMs to handle and analyze data, possibly in a structured way.",
        "formal_text": "… … … … … … GCN GCN GCN Quantity Cell Q \" Q # Q $ Q % \" # % $ Q \" Q # Q $ Q % BiLSTM",
        "GPT2_formal_text": ") = arg max (r i, arg min (r i, arg max (r j, arg min (r j, arg min (r i, arg max (r j ))))))\n          Casual: We made sure to group our examples into clusters using clustering techniques that were created by the folks who did the annotation work for the TAC-KBP 2018 Shared Task. This gave us around 9,000 clusters for training, 6,900 for development, and 9,000 for testing. Formal: We ran all the training examples through the NER tagger we mentioned earlier, just like we mentioned in section 3. Formal: For the development and test sets, we randomly divided the data into training, development, and test sets by making sure each split had the same number of examples in each set. Formal: Each development set had around 10,000 examples, and each test set had about 6,000 examples. Formal: We set the learning rate to 0.001 to avoid overfitting, which we explained in section 2.4.2. Formal: We ran our experiments using PyTorch version 1.6.4. Formal: The hyperparameters were picked using the same approach as in the original paper (Wang et al., 2020), with a learning rate of 0.001 and a batch size of 32. Formal: We used the QQP model with a batch size of 32, which was trained on the training set and tested on the test set. Formal: We ran all the tests using PyTorch version 1.6.4. Formal: For the test sets, we randomly divided the data into training, development, and test sets by making sure each set had the same number of examples in each set. Formal: Formal: The learning rate was set to 0.001 and a batch size of 32. Formal: We used the TAC-KBP training set to generate test data, and the development set to generate training data. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.362.json"
    },
    {
        "casual_text": "Alright, let's break it down. For each sentence, which we can think of as a group of words W = {w 1, w 2, . . . , w n }, we need to figure out how these words connect to the events they’re talking about.",
        "formal_text": "To set up notations, for each utterance represented as a set of words W = {w 1, w 2, . . . , w n }, we want a segmentation which expresses the relationship of the words to the events which they describe.",
        "GPT2_formal_text": "ed = [ (1, 0.1), (1, 0.2), ..., (1, 0.3), ..., (1, 0.4) ] δ x = 1/3 σ x. Formal: A probabilistic graphical model, let's call it Model(ψ), that combines context-aware and attention-based features can be described as a fancier version of the usual Markov random field model. Formal: Since the representations of words in a text (i) are usually more descriptive and relevant than the model's predictions, we'll just call the model that uses those representations (Model(ψ)) the generative model. Formal: To make the model generate a meaningful response (ψ), we just add up the model's own probability (P(ψ)) and the model's prior distribution (P(ψ, ψ)). Formal: Basically, Model(ψ) is calculated by adding up P(ψ) for every word in the text. Formal: Lastly, the expected response (ψ |ψ) is the expected probability given a model's prediction. Formal: Formal: The zero-shot model is just the generative model, and δ x is the response-specific feature vector from the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D14",
        "filename": "D14-1043.json"
    },
    {
        "casual_text": "In this project, we’re looking at how to figure out who the main criminals are and who’s just helping out based on the details of a crime case. The main criminal is the one who’s in charge, leads a group, or takes a big part in the crime. On the other hand, an accessory is someone who’s more in a supporting role, helping out but not the main player. As shown in Fig. 1, our goal is to take the description of the crime and a list of people involved, and then figure out who’s the main criminal and who’s just assisting.",
        "formal_text": "In this work, we propose to study the identification of principals and accessories from the fact description in a criminal case. The principal refers to a criminal who organizes and leads criminal groups to carry out criminal activities or plays a major role in joint crimes. Correspondingly, we refer the accessory as the one who plays a secondary or auxiliary role. As the illustration of our task in Fig. 1, given the fact description as well as a list of defendants, we expect to identify the principals and accessories from the defendants.",
        "GPT2_formal_text": "\": [], e_t = (e_k, e_l), ..., e_n = (e_m, e_n), ..., e_a = (e_v, e_c), ..., e_b = (e_b, e_m), ..., e_c = (e_c, e_v), ..., e_d = (e_d, e_v), ..., e_e = (e_e, e_c), ..., e_f = (e_f, e_d) ) Formal: First, we grab the embeddings for the entity spans using the BiLSTM network. Then, we get the embeddings for the entity spans and the entity spans themselves by using the Longformer encoder. Formal: We take the embeddings for the entity spans e_t and e_l and pass them through the BiLSTM network to get the embeddings for the entity spans and the entity spans themselves. Formal: After that, we pass the embeddings for e_t, e_l, ..., e_n to the Longformer encoder to get the embeddings for the entity spans and the entity spans themselves. Formal: Finally, we use the entity spans e_t, e_l, ..., e_n to get the embeddings for the entity spans and the entity spans themselves. Formal: After that, we use the entity spans e_t, e_l, ..., e_n to get the embeddings for the entity spans and the entity spans themselves. Formal: Finally, we use the entity spans e_t, e_l, ..., e_n to get the embeddings for the entity spans and the entity spans themselves. Formal: Next, we calculate the average cross-entropy loss for each span e_t, e_l, ..., e_n using the cross-entropy loss function from equation (6). Formal: Finally, we calculate the cross-entropy loss for each span e_t, e_l, ..., e_n using the cross-entropy loss function from equation (6). Formal: After that, we use the entity spans e_t, e_l, ..., e_n to get the embeddings for the entity spans and the entity spans themselves. Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.393.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. Imagine you have a sentence like this: \"A man ate it.\" Now, when we look at this sentence, we're kind of thinking about all the different people who could have eaten something. It's like we're talking about the idea of someone eating, not just one specific person. So, it's more about the general concept of eating rather than focusing on a particular individual.",
        "formal_text": "As a finaJ exa. mpte, consider a. sentence which (; outa. ins ~ focussed item I)tlt lie oper~or for using it up: (13) A man ate i/ The a. nalysis of this is a. n a. bstraction over Idnds of indivM ua. ls who ate it",
        "GPT2_formal_text": "_v2 is calculated by adding up the scores from the attention_v2_mask and the global pooling layer. Then, it's passed through a fully connected layer to get the final result. Formal: So, t_i is the i-th token, and m is the number of attention heads. The top part of Figure 1 shows how this whole process works, but for the inner loop part, it's shown in a simpler way, with just a few tokens. Formal: Alright, so T_i is the key to unlock the inner loop, and it's calculated by multiplying the attention vector and the global pooling layer. In the top part of Figure 1, you can see how the attention is spread across different attention heads, which is a common way to learn attention. Formal: To make sure the attention heads are spread out evenly across different tokens, we use a kernel to figure out the attention distribution. Formal: After that, we get the final output for each token by combining the attention vectors for the top k attention heads. Formal: Finally, the output is a vector that shows how the attention is spread out across all the tokens. Formal: And that's how we use all these attention heads to help the model handle different types of data. Formal: Oh, and about the \"X is Y\" case... Formal: This happens when the model focuses too much on the \"X is Y\" token. Formal: To avoid this, we add a hard negative log-likelihood function to the attention vector. Formal: Formal: We test the model with different values of k for k = 2, 3, and 4. Formal: After testing, we tweak the attention weight vector to work better with the k-th attention head. Formal: Formal: In the end, we use a softmax function to figure out the overall attention distribution. Formal: We look at the attention scores from different attention heads to predict the next token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C94",
        "filename": "C94-2142.json"
    },
    {
        "casual_text": "This vector shows us a few things about horses. First, it tells us that all horses are mammals—so if you take the number of horses that are also mammals and divide it by the total number of horses, you get 1, meaning every single horse is a mammal. Second, it says that horses and things with scales don’t overlap at all—in other words, no horse has scales. We also find out that most horses have four legs, and some of them are brown.",
        "formal_text": "This vector tells us that the set of horses includes the set of mammals (the number of horses that are also mammals divided by the number of horses comes to 1, i. e. all horses are mammals), and that the set of horses and the set of things that are scaly are disjoint (no horse is scaly). We also learn that a great majority of horses have four legs and that some are brown.",
        "GPT2_formal_text": "\": [], } }, { \"token\": { \"ip_mask\": [], \"mask_len\": [], \"mask_h\": [], \"type\": [], \"src\": [], \"source\": [], \"type\": [], \"dst\": [], \"src\": [], \"type\": [], \"src\": [], \"len\": [], \"len\": [], \"len\": [], \"type\": [], \"src\": [], \"type\": [], \"type\": [], \"dst\": [ ], \"src\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"type\": [ ], \"len\": [ ], \"len\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"type\": [ ], \"len\": [ ], \"len\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"len\": [ ], \"type\": [ ], \"len\": [ ], \"len\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"len\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"len\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [ ], \"type\": [ ], \"src\": [ ], \"type\": [ ], \"len\": [ ], \"type\": [",
        "directory": "D15",
        "filename": "D15-1003.json"
    },
    {
        "casual_text": "We calculate a normalized community similarity score for pairs i, j by subtracting s i, m from s i, j, where s i, m is the score from the \"subreddit merged others.\" We look at how this score correlates with community feedback for three different models. The results are in Table 4 for thread-level analysis and Table 5 for user-level analysis.\n\nAt the thread level, the hyb-500. 30 style model consistently shows a positive and statistically significant correlation between a post's stylistic similarity score and its karma. This means that adapting your language style can help your post get more upvotes. None of the other models we looked at earlier had this feature, and for the topic models, the correlation is usually negative.\n\nOn the user level, all the correlations between a user's k-index and their style/topic match are statistically significant. The hyb-500. 30 style model shows a stronger positive correlation compared to the others. In both cases, the word_only model gives results that are somewhere in between the style and topic models. The hyb-15k model is similar to the word_only model, and the tag_only model mostly shows negative correlations.",
        "formal_text": "We compute a normalized community similarity scores i, j = s i, j − s i, m, where s i, m is the corresponding score from the subreddit merged others. The correlation betweens i, j and community feedback is reported for three models in Table 4 for the thread level, and in Table 5 for the user level. On the thread level, the hyb-500. 30 style model consistently finds positive, statistically significant, correlation between the post's stylistic similarity score and its karma. This result suggests that language style adaptation does contribute to being well-received by the community. None of the other models explored in the previous section had this property, and for the topic models the correlation is mostly negative. On the user level, all correlations between a user's k-index and their style/topic match are statistically significant, though the hyb-500. 30 style model shows more positive correlation than other models. In both cases, the word_only model gives results between the style and topic models. The hyb-15k model has results that are similar to the word_only model, and the tag_only model has mostly negative correlation.",
        "GPT2_formal_text": "ed is set to 1.0. The model is trained using backpropagation with a learning rate of 0.01, followed by a mini-batch stochastic gradient descent. The student model is trained with a batch size of 16 and a learning rate of 1e-5. For checking how well the model does, we use the perplexity of the correct answer (p_S) to measure performance. Formal: The model can predict the target answer in one step. Formal: It's pretty straightforward to explain how the teacher model works, but it's not exactly straightforward to explain how the student model works. Formal: The student learns by looking at the teacher model's predictions. Formal: There's a way to describe both models using a language called NLTK. Formal: In Section 3, we'll show how we use NLTK to compare the student model with the teacher. Formal: NLTK is a free, open-source tool that helps us evaluate how well different models work in supervised learning. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1108.json"
    },
    {
        "casual_text": "Lately, pre-trained language models (PLMs) have made some pretty cool strides in a bunch of different tasks (shoutout to Devlin et al., 2019). Some newer studies show that these models can pick up knowledge from huge datasets all by themselves during pre-training and store that info in their parameters (Tenney et al., 2019; Petroni et al., 2019; Roberts et al., 2020). But here's the thing: because of the limited size of their vocabulary, PLMs struggle to pull out that factual knowledge, especially when it comes to rare or less common entities (Gao et al., 2019a; Wang et al., 2021a).",
        "formal_text": "Recent advance in pre-trained language models (PLMs) has achieved promising improvements in various downstream tasks (Devlin et al. , 2019; . Some latest works reveal that PLMs can automatically acquire knowledge from largescale corpora via self-supervised pre-training and then encode the learned knowledge into their model parameters (Tenney et al. , 2019; Petroni et al. , 2019; Roberts et al. , 2020). However, due to the limited capacity of vocabulary, existing PLMs face the challenge of recalling the factual knowledge from their parameters, especially for those rare entities (Gao et al. , 2019a; Wang et al. , 2021a).",
        "GPT2_formal_text": "\" { $this -> id } | $this -> token_ids }            Casual: Another thing to keep in mind is that the query and response are separate. So, we don't need to mix them together. Formal: The results from the initial round of decoding are saved as w_t1 and w_t2. These are just the probabilities we got from the model. Formal: We also calculate the predicted probabilities for the next round of decoding. Formal: Using these predicted probabilities, we can create a score for each token. Formal: Formal: Next, we take the token's predicted probability and add it to the score. Formal: Finally, we calculate the total score for all the tokens. Formal: Finally, we look at how well the model did overall. Formal: Formal: Table 1 shows how accurate the model was at predicting the next word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-short.57.json"
    },
    {
        "casual_text": "Next, let’s dive deeper into how changing the parameters might affect things. We’re curious to see what would happen if someone (like an adversary) picked different values for these parameters instead of sticking with the ones in Table 5. The possible values for k, p, and temperature are all laid out in Table 1. Just a quick reminder: you either use top-k or top-p sampling to decide how many words to consider when generating text. We’ve been using top-p sampling as our go-to method. When we tweak k or p, we keep the temperature at its default setting. And if we mess with the temperature, we leave p at its default value.",
        "formal_text": "We next explore the parameter differences angle further to get a sense of what would happen if the adversary chose a parameter value other than the ones explored in Table 5. The different values for k, p, and temperature are as listed in Table 1. We remind the reader that one uses either top − k or top − p sampling to control the number of words under consideration during text generation. We use top − p sampling as the default strategy. When varying k or p, the temperature is fixed at the default value. When varying temperature, p is kept at the default value.",
        "GPT2_formal_text": "*d_t*0)\n     Casual: This could be done in two ways: either by checking the attention weights separately (like Ma et al. did in 2019) or by combining them (as Yu et al. did in 2020). Formal: Sure! Here's the informal version:\n\n\"Check out these links: \n1. https://github.com/luke-hamblin/mtnts-scalable-attention-mask  \n2. https://github.com/jimmy-knapp/multilingual-attention  \nAnd here's another one:  \n\"You can find it here: https://github.com/stanfordai/multilingual-attention-mask  \nSo, that's it! It's all about testing out different attention methods for machine translation.\" Formal: One more thing: we also wanted to check if the attention weights could be learned more slowly than the actual sequence of tokens. So, we ran a bunch of experiments using a classifier that uses the same architecture as ours but isn't trained yet. Formal: Our model does better than all the baseline models on four datasets and also beats some strong supervised models, especially on the masked language modeling dataset. Formal: To see how we could use this model to make our way better, we ran some experiments on the English-to-German and English-to-English tasks. Formal: Lastly, we ran some experiments on the masked language modeling dataset to see how well our model can handle it. Formal: We use a pre-trained model to pick the best possible settings for each input x_i and output y_i. Formal: In this part, we check if these settings are really useful by comparing the results on the masked language modeling dataset to the results we got from our model. Formal: Finally, we share some results on the multilingual WMT task and show how the model performed on different languages. Formal: We also tried using a GCN for the model but that didn't really help. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.155.json"
    },
    {
        "casual_text": "Alright, in this part, we're looking at how exits learn from each other by running some small tests called pairwise mutual learning (PML). For these PML tests, we pick two exits (let's call them exit i and exit j), where exit i is closer to the surface than exit j (so, i < j). We're checking out a few different scenarios to see how things play out.",
        "formal_text": "In this section, we examine the effects of mutual learning among exits by conducting a series of pilot experiments called pairwise mutual learning (PML). In the PML experiments, we select two exits (i, j) (i < j, i. e. , exit i is shallower than exit j). We consider the following settings:",
        "GPT2_formal_text": ", and a hyperparameter α that decides how many positive and negative samples are in each batch. Formal: The value of s is determined by the following two conditions: Formal: The value of s is determined by the following two conditions:\n\n1. If s is less than or equal to -1, then s is set to -1.\n2. If s is greater than -1, then s is set to 1. Formal: If s is less than or equal to -1, then s is set to -1. Formal: The value of s is determined by the following two conditions:\n\n1. If s is less than or equal to -1, then s is set to -1.\n2. If s is greater than -1, then s is set to 1. Formal: If s is less than or equal to -1, then s is set to -1. Formal: Formal: The value of s is determined by the following two conditions:\n\n1. If s is less than or equal to -1, then s is set to -1.\n2. If s is greater than -1, then s is set to 1. Formal: Formal: Formal:\n\n**Input:** These are the features that are supposed to decide if the positive samples from the predicted labels are actually correct, and the negative samples are not. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.242.json"
    },
    {
        "casual_text": "In this paper, we worked on detecting off-topic responses. We started by creating a model with five main layers. Inside this model, we used a bi-attention mechanism and convolutions to identify important words in prompts and key phrases in responses. We also added a gated unit as a relevance layer to improve semantic matching and included residual connections in each layer. To better understand our model, we did some visualization analysis. Lastly, we came up with a new negative sampling method to boost our off-topic training data. We tested our approach and saw big improvements on both familiar and new test data.",
        "formal_text": "In this paper, we conducted a series of work around the task of off-topic response detection. First of all, a model framework of five major layers was proposed, within which bi-attention mechanism and convolutions were used to well capture the topic words of prompts and key-phrase of responses, and gated unit as relevance layer was applied to better obtaining semantic matching representation, as well as residual connections with each major layer. Moreover, the visualization analysis of the off-topic model was given to study the essence of the model. Finally, a novel negative sampling augmentation method was introduced to augment off-topic training data. We verified the effectiveness of our approach and achieved significant improvements on both seen and unseen test data.",
        "GPT2_formal_text": "ed (λ) for the final layer. After that, we just stick all the vectors together to get the final hidden state h_t. Formal: In the classifier's prediction, it considers the context that comes before the input x_t and the current word x_i. If the classifier doesn't know the current word x_i, it just returns 0. This is the same setup as the forward-hidden-layer attention. Formal: The cross attention layer uses a hidden layer with a window size of (1 + λ) × d and has a hidden dimension of d. Formal: After applying the forward-hidden-layer attention, the vectors for the masked words get the same shape, so they don't overlap much. Formal: A convolution layer adds a linear layer with a window size of (1 + λ) × d and also has a hidden dimension of d. Formal: We're using the Huggingface library (Wolf et al., 2020) to train our model. Formal: The feature vector for the masked word x_t is just the word itself, and the feature vector for the word we're predicting is the result of the forward-hidden-layer attention. Formal: The classifier then uses the output from this classifier to calculate the overall prediction, which includes the output from the previous hidden layer. Formal: Finally, the classifier uses the output from the hidden layer from the next hidden layer to predict the next word. Formal: We're suggesting a bunch of different ways to make the model more resistant to noise and make predictions faster. Formal: To show how our model works, we ran some experiments on the test set of the GREEDY benchmark (Lee et al., 2017). Formal: We took the CNN model (Devlin et al., 2019) and added some other features that work well with the FEWVLM model, like the Fuzzy word embeddings, the self-attention layer, and the attention layer. Formal: We also used the WAT (Wang et al., 2020) model to predict the next word and its position in the sentence. Formal: We trained the model using cross-entropy loss and averaged the results. Formal: We also did some evaluation on a big dataset called MARYT (Mihalcea and Tarau, 2020). Formal: The models we trained are included in the supplementary material. Formal: We tested",
        "directory": "acl",
        "filename": "2020.acl-main.56.json"
    },
    {
        "casual_text": "Here's how we set up the adversarial training. We have a discriminator D that's trained to minimize the cross-entropy loss, which is defined in equation 2. In this case, l represents the domain category, and h is the hidden representation of both the question and the passage, which is a vector in R^d. For our experiment, we use the [CLS] token representation from BERT as h.",
        "formal_text": "We formulate the adversarial training as follows. A discriminator D is trained to minimize the cross-entropy loss as of equation 2, where l is domain category and h ∈ R d is the hidden representation of both question and passage. In our experiment, we use [CLS] token representation from BERT for h.",
        "GPT2_formal_text": "\": [], \"sent\": [], \"user_id\": [], \"has_disc\": false, \"is_child_of\": false, \"created_at\": null, \"user_id\": null, \"in_reply_to_user\": false, \"user\": \"akash100\", \"id\": 10, \"created_at\": \"2016-09-19T23:09:41+00:00\", \"user_id\": null, \"timestamp\": null, \"created_at\": \"2016-09-19T23:09:41+00:00\", \"user\": \"akash100\", \"id\": 11, \"created_at\": \"2016-09-19T23:09:41+00:00\", \"user_id\": null, \"created_at\": null, \"user\": \"akash100\", \"id\": 12, \"created_at\": null, \"user_id\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created_at\": null, \"created",
        "directory": "D19",
        "filename": "D19-5826.json"
    },
    {
        "casual_text": "The key to relation-driven skimming is to attach things correctly without getting bogged down in too much grammar. It’s not easy, though, because even phrases that don’t seem important can mess up the ones that do. In the next parts, we’ll break down each component, explain why it works, what it does, and show you a couple of examples to help you understand.",
        "formal_text": "The trick to relation-driven skimming is to perform attachment as accurately as possible with as little grammatical analysis as possible. This is no simple task, because phrases with no relevant semantic content can always affect the attachment of relevant phrases. In the sections that follow, we will give for each of the components above an observation of why it works, its main activity, and an example or two of its operation.",
        "GPT2_formal_text": "ing(x, W_o) = ReLU(W_o) * (1 - tanh(W_o * h)) / (1 + e^-4) + tanh(1 - h) * tanh(W_o * h) + e^-4) Formal: We'll do the same thing for the other candidates like w_e, w_f, and w_g. Formal: In our calculations, we're looking for a special number (like a positive number) that changes depending on the type of relation. So, we'll write it as a positive number, let's call it P_r, that depends on r. Formal: The system can generate random relations. Formal: Here's a more technical way to write P_r: P_r(r) = P_r(r, w_e, w_f) * P_r(r, w_g) * P_r(r, w_e) * P_r(r, w_f) Formal: We'll use these scores to figure out the parameters for the parameters \\( s_i \\) and \\( c_i \\) in the relation vector \\( R^n \\). Formal: To figure out the parameters of the parameters \\( c_i \\) and \\( r^n \\), we use this formula. Formal: Here's the formula for the generative part: Formal: This part uses the process we just talked about to give each possible relation r an initial representation. Formal: The final representation, \\( Formal: \\frac{y_{r^n}(r)}(r) \\), is then used to calculate the logit for the probability of each relationship. Formal: We use the same process to figure out the parameters of the parameters \\( s_i \\) and \\( c_i \\) in the relation vector \\( R^n \\). Formal: The logit for the probability of a relation \\( r \\) is calculated based on the actual features we get from the relationships. Formal: We also calculate the logit for the log probability of a different relation \\( r \\) by using the current features we have. Formal: Finally, we end up with the logit for the log probability of a whole bunch of relations. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C90",
        "filename": "C90-2034.json"
    },
    {
        "casual_text": "TEXAN is a system for analyzing and transferring text, focusing on how it's structured and communicated. It’s built around the idea that texts are the result of specific linguistic actions, based on speech act theory. Basically, it thinks of texts as actions that guide how words and phrases are chosen. To make this work in a computer model, TEXAN uses something called a context-free illocution grammar. This grammar handles categories of actions and the logical structure of situations, connecting them to the actual words and phrases in a language. It ties all this together with a special text-related lexicon.",
        "formal_text": "TEXAN is a sysLem of bransfer, . oriented text analysis. Its linguistic concept is based on a communicative appY'oacll within the framework of speech act theory. In this view texts are considered to be the result of linguistic actions° It is assumed that they control the selection of translation equivalents. The transition of this concept of linguistic actions (text acts) to the model of computer analysis is performed by a context-free illocution grammar processing categories of actions and a propositional structure of states of affairs. The grammar whicll is related to a text lexicon provides the connection of these categories and the linguistic surface units of a single language.",
        "GPT2_formal_text": "= { qi } n i=1 δ i=1 + δ j=1 δ j=1 + δ t=1 δ t=1 + δ r=1 δ r=1 + δ s=1 δ s=1 + δ t=1 δ t=1 + δ r=1 δ r=1 + δ s=1 δ s=1 + δ t=1 σ i=1 δ i=1 + δ j=1 σ j=1 + δ t=1 σ t=1 + δ r=1 σ r=1 + δ s=1 σ s=1 + δ t=1 σ t=1 + δ r=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r=1 + δ t=1 σ r=1 + δ s=1 σ r",
        "directory": "C86",
        "filename": "C86-1078.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We need to get the output distributions, which we'll call P dp and P kb, from two teachers named TDP and TKB. Got it?",
        "formal_text": "Produce the output distributions P dp and P kb by two teachers TDP and TKB; 5:",
        "GPT2_formal_text": "ed. Formal: In the first layer, the input to the attention module gets transformed into a word vector using an LSTM. The second layer looks at the sentence embedding and the token embedding to predict the label y_i. This prediction is then used to create a predicted embedding, e_i, for the question-answer pair. We calculate the cross-entropy loss for this whole process using the cross-entropy loss function. Formal: The second layer in our model is about creating questions. We take the question embedding e_i and the target embedding, f_i, from the first layer and feed them into a multi-layer Bi-GRU cell. This cell uses the attention mechanism to get the answer embedding e_i for the question. Finally, we calculate the cross-entropy loss for this whole process. Formal: We're showing how to use this model for question answering, focusing on answering questions with multiple sentences. Formal: For our experiments, we’ve set the training and validation data sizes to 50 and 100, respectively. Formal: We’re using the same model structure as the multi-choice QA dataset (Vijayakumar et al., 2016). Formal: After generating the question, the attention mechanism has a window of 4 tokens around the question. Formal: We’ve set the training and validation data sizes to 50 and 100, respectively. Formal: We’re using the same model structure as the multi-choice QA dataset (Vijayakumar et al., 2016). Formal: We’ve set the training and validation data sizes to 50 and 100, respectively. Formal: The model has a window of 4 tokens around the question. Formal: Formal: We’ve set the training and validation data sizes to 50 and 100, respectively. Formal: Formal: The model has a window of 4 tokens around the question. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.281.json"
    },
    {
        "casual_text": "Here’s a simpler way to say that:\n\nWe follow these six steps for each parent node in the source-side parse tree:",
        "formal_text": "We adopt the following six step algorithm for each parent node from the source-side parse tree:",
        "GPT2_formal_text": "= u_t) def main(t, s, e, f, d): for q in s: for i in q: if (p_s_i(t) and p_e_i(t)) and (p_f_i(t) and p_d_i(t)) and (p_i(t) and p_e_i(t))): if s and e are in (1, 2, ..., n): main(s, e, f, d) else: main(s, e, f, d)\n\nThis is basically a recursive function that takes two arguments, q and e, and uses them to predict the next word in a sentence. It also adds a couple of rules:\n\n1. It doesn't try to predict q and e at the same time.\n2. If s and e are in different sentences (i and j), it doesn't combine them. Formal: To get the predicted word, q and e, from the last sentence, you calculate the result for s and e in the same sentence. Formal: After the last sentence, you run this formula:\n\n- q and e are in s and e,\n- q is in s and e,\n- f is in s and e,\n- e is in s and e,\n- Finally, you check if the predicted word matches the current word. Formal: The main thing is to guess the next word based on the words from the previous sentences. Formal: The main part is to predict the next word based on the words from the previous sentences. Formal: The main part is to predict the next word based on the words from the previous sentences. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2009.eamt-1.27.json"
    },
    {
        "casual_text": "SR might tweak the slot values in the translated sentences, so we use fuzzy value detection to deal with issues like similar sounds or pronunciation problems when it's pulling out slot values to get a semantic label, y. But instead of fixing the noisy value with the original one, we actually let the misrecognition happen because we're okay with that. So, y = y is totally fine. Also, we normalize numerical terms to handle spoken numbers better. Most slot values can be sorted out by our automatic detection rules. The ones that are too different to recognize just get tossed out along with their labels.",
        "formal_text": "Since SR may modify the slot values in the translated utterances, fuzzy value detection is employed here to handle similar sounds and liaison problems when it extracts slot values to obtain a semantic label y. However, we do not replace the noisy value with the original value as we encourage such misrecognition in SR, thus y = y is allowed. Moreover, numerical terms are normalized to deal with the spoken number problem. Most slot values could be relocated by our automatic value detection rules. The remainder slot values which vary too much to recognize are discarded along with their corresponding labels.",
        "GPT2_formal_text": "is the attention mask.\n       Casual: We use a bunch of benchmark datasets that are pretty similar to the ones we're focusing on here. These include the MNLI dataset (thanks, Conneau et al., 2018), SQuAD (Rajpurkar et al., 2016), and MNLI-T (Yang et al., 2018). Formal: (1) When it comes to the number of features, SQuAD, which uses a fancy method called constrained attention, is the best when it comes to figuring out the attention distribution. But when it comes to the number of dimensions, MNLI-T and MNLI-T+ are the top performers. Formal: (2) When it comes to the number of samples, the best results come when the number of samples goes from 2 to 4. Formal: (3) On top of that, the best results come when the sample size is either 1 or 2. Formal: (4) SQuAD is the only model that comes out on top with the beam size of 20, which is pretty cool. Formal: (5) The perplexity for SQuAD is around 0.8. Formal: (6) Figure 3 shows that the performance doesn't really drop by more than a factor of 2, which is what you'd expect. Formal: (7) Formal: (8) Formal: Formal: (9) Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.192.json"
    },
    {
        "casual_text": "Alright, so following the steps in Section 3, we start by running EDITS and VENSES on the [T, H] RT E5-sample and [T, H] RT E5-mono. The accuracies for these runs are listed in Table 3.",
        "formal_text": "Following the methodology described in Section 3, at step 1 we run EDITS and VENSES on [T, H] RT E5−sample, and on [T, H] RT E5−mono (Table 3 reports the accuracies obtained).",
        "GPT2_formal_text": "/: ) that are specific to the query, but it might not be clear what exactly these queries are for. This section is all about going back to the original question and finding the answers to the questions we're looking for. Formal: In this part, we'll talk about the main ideas behind what we're doing with the question retrieval task. Formal: We'll also explain how we handle questions in general, not just about vaccines. Formal: Lastly, we'll share the results of our first experiment with a dataset we made ourselves. Formal: We'll also share the results of our first experiment with a dataset we made ourselves. Formal: Here's the link to the dataset: https://github.com/PsA/vaccines Formal: Check out Figure 1 for a quick example of how our question retrieval system works. Formal: We used Wikipedia to connect questions with their answers. We made Wikipedia embeddings using the same setup as the question embeddings we got from the dataset, which you can find more details about in Appendix D. Formal: Figure 2 shows how the question retrieval system works. Formal: For each question in the dataset, we calculate the number of times the answer is mentioned in a Wikipedia article. We then pick the question that has the most articles where the answer appears. Formal: We picked the question with the most articles where the answer appeared. Formal: Next, we use Wikipedia to look up the answer's Wikipedia page and find the page with the answer. Formal: Finally, we calculate the page where the answer's Wikipedia page is found. Formal: Here's the link to the dataset: https://github.com/PsA/vaccines Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C10",
        "filename": "C10-2012.json"
    },
    {
        "casual_text": "Got it! Just so you know, there can be more than one connection between two nouns. So, a single dependency path might actually represent multiple relationships.",
        "formal_text": "Note that, there may be more than one relation existing between two nominals. A dependency path thus may correspond to multiple relations.",
        "GPT2_formal_text": "ed in the baseline model. In the second baseline model, we're using the embedding embeddings of the candidate text's first and second words as the attention. Formal: The embedding vector for the second word is calculated by averaging the embeddings from both the first and second words in the candidate text. Formal: Also, we take a weighted average of all the embeddings for all the words in the candidate text. Formal: From Equation 3, we can easily see that our multi-head attention works really well for picking up on the important details from the candidate text's word embeddings. Formal: It's also pretty clear that the model's performance on the test set is better than what the baseline model can achieve, especially when it comes to nouns. Formal: We noticed that the model's performance on the test set isn't better than what the baseline model can achieve, but it does get better when it comes to verbs. Formal: We tested our model on two popular datasets for detecting named entities, PubMed, from German to English (Buchholz and Marsi, 2006; Meulder et al., 2008). We picked our model based on the average F-score we got from these datasets. We also checked how well the model handles unknown words and verbs. Formal: We ran our model on a standard MERT setup, which is used for most machine learning tasks, except for dependency parsing. Formal: The results for the test set in Table 5 are shown in Figure 1, which shows the average F-score from the development set. Formal: The model's performance on the test set is better than the baseline model when it comes to unknown words and verbs. Formal: The results for the test set in Figure 2 are also in Figure 1. Formal: The model's performance on the test set is better than the baseline model when it comes to unknown words and verbs. Formal: The model's performance on the test set is better than the baseline model when it comes to unknown words and verbs. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D15",
        "filename": "D15-1062.json"
    },
    {
        "casual_text": "We’ve got RuSentiment, a dataset based on public posts from VKontakte (VK), which is the biggest Russian social network with around 100 million active users every month. RuSentiment was created using some fresh, detailed guidelines that made the annotation process quick and easy while still covering a wide variety of sentiments, both obvious and subtle. The agreement between the people doing the annotations, measured by Fleiss' kappa, is 0.58. Overall, 31,185 posts were labeled, with 21,268 picked randomly (including 2,967 for the test set). Another 6,950 posts were chosen using a strategy kind of like active learning to make sure the data was diverse. This makes RuSentiment the biggest publicly available sentiment dataset for social media, and also the largest general-purpose sentiment dataset for Russian, which isn’t as widely studied as other languages.",
        "formal_text": "We present RuSentiment, a dataset of public posts on VKontakte (VK), the largest Russian social network that currently boasts about 100M monthly active users. 1 RuSentiment was developed with new comprehensive guidelines that enabled light and speedy annotation while maintaining consistent coverage of a wide range of explicitly and implicitly expressed sentiment. The overall inter-annotator agreement in terms of Fleiss' kappa stands at 0. 58. In total, 31, 185 posts were annotated, 21, 268 of which were selected randomly (including 2, 967 for the test set). 6, 950 posts were pre-selected with an active learning-style strategy in order to diversify the data. This makes RuSentiment the largest openly available sentiment dataset for social media, and the largest general domain sentiment dataset for this relatively low-resource language.",
        "GPT2_formal_text": "[i] = d;        Casual: Also, the text-to-text model only looks at the words before the current word to figure out the vector of the next word, which we call h_t_i. This is a bit more detailed than the attention-based model, where each word gets its own vector, which is shown in figure 1. The reason for this difference is that the attention mechanism is designed to focus on the words that are likely to be important for the current word. Formal: The model can handle text in different languages, but it doesn't take advantage of the more flexible word order. Formal: Unlike the attention-based model, it uses a sequential setup for the vector h_t_i, which isn't really designed for dealing with word order. Formal: Formal: This model does better than the attentive model because it uses a weighted sum of vectors, which helps it compare words more effectively. Formal: The model can figure out the grammatical structure of a sentence by looking at the words in the context, but it still struggles with more complex sentences that have multiple words. Formal: This model does even better than the attentive model, with a score of 0.90. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C18",
        "filename": "C18-1064.json"
    },
    {
        "casual_text": "Neural networks have gotten really good at handling two tasks at once: predicting charges and picking out the related legal articles. We’ve built a system that ties these two things together, using something called a two-stack attention mechanism to make sense of how the details of a case connect to the laws and charges. \n\nHere’s how it works: we use a couple of fancy tools—sentence-level and document-level Bi-GRUs (kind of like advanced Recurrent Neural Networks)—along with some attention components to figure out how words and sentences fit together. This helps us understand the big picture of the case and the important details. Once we’ve analyzed the case description, we use another set of attention components to focus on the most relevant legal articles that back up our charge prediction. We’re dealing with this in a multi-label way, meaning a case can have more than one charge.\n\nWe tested this model by predicting charges for criminal cases in China. We grabbed publicly available court documents from the Chinese government website, where we could automatically pull out the case details, related laws, and charges using basic rules. Check out Figure 1 for how this works.\n\nThe results? Our neural network method does a solid job of predicting the right charges for a case and also points out the legal articles that support those predictions.",
        "formal_text": "Recent advances in neural networks enable us to jointly model charge prediction and relevant article extraction in a unified framework, where the latent correspondence from the fact description about a case to its related law articles and further to its charges can be explicitly addressed by a two-stack attention mechanism. Specifically, we use a sentence-level and a documentlevel Bi-directional Gated Recurrent Units (Bi-GRU) (Bahdanau et al. , 2015) with a stack of factside attention components to model the correlations among words and sentences, in order to capture the whole story as well as important details of the case. Given the analysis of the fact description, we accordingly learn a stack of article-side attention components to attentively select the most supportive law articles from the statutory laws to support our charge prediction, which is investigated in the multi-label paradigm. We evaluate our model in the context of predicting charges for criminal cases in China. We collect publicly available judgement documents from China's government website, from which we can automatically extract fact descriptions, relevant law articles and the charges using simple rules, as shown in Figure 1. Experimental results show that our neural network method can effectively predict appropriate charges for a given case, and also provide relevant law articles as legal basis to support the prediction.",
        "GPT2_formal_text": "_layers = [mask_1; mask_2; mask_3; ...; mask_T]; Formal: This part uses a multi-layer perceptron (MLP) to figure out the attention weights. It uses the cross-entropy loss function with a simple linear layer as its loss function. Formal: We train the model on different datasets, and for each dataset, we pick the best-performing model from the training set. Formal: We also create a separate model for each source domain, which we call N-best. The goal is to maximize the log-likelihood of the classifier's output, as defined in Equation 1. Formal: Each layer in the network has a hidden dimension of D = 3d. Formal: To get the attention weights, we use an attention layer with a hidden dimension of d. Formal: We use the Adam optimizer, as introduced by Kingma and Ba in 2014, to train the model. Formal: We also calculate the average precision, recall, and F1 score for each sentence based on the output from the model. Formal: Lastly, we look at how often each sentence (or token) is correctly predicted as the correct answer. Formal: Formal: Formal:  \n\nFigure 1: An example of how we handle cross-entropy loss for instance weighting in an attention mechanism. Formal: We train the model on a dataset D, which includes instance weights. We only pick the best-performing model based on its performance on this dataset D. Formal: We also create a separate model for each source domain, which we call N-best. The goal is to maximize the log-likelihood of the classifier's output, as defined in Equation 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D17",
        "filename": "D17-1289.json"
    },
    {
        "casual_text": "1. Basically, some cases have stricter rules about what words can go with them than others. For instance, in the accusative case, \"to subscribe\" has a tighter set of rules compared to \"to take/steal,\" which can work with a wider range of objects.",
        "formal_text": "1. httuitively speaking, the, contribution of the 2. The seleetional restriction of a certain case is stronger than those of others. For example, in tile accusative, the selectional restriction of \"to subscribe\" is stronger than that of \"to take/steal\" which Mlows various kinds of objects as its case filler.",
        "GPT2_formal_text": "= { qi } n i=1:n | qi, j, k, L i = 0, 1, 2, ..., n i=n | i, j, k, L j = 0, 1, 2, ..., n, p i = 1:n, y i = 1:n | y j, k, L i = 0, 1, 2, ..., n i=n | y i, k, L j = 0, 1, 2, ..., n, r i = 1:n, p i = 1:n | p j, k, L i = 0, 1, 2, ..., n i=n | p i, k, L j = 0, 1, 2, ..., n, t i = 1:n | t j, k, L i = 0, 1, 2, ..., n i=n | t i, k, L j = 0, 1, 2, ..., n, t } | where T stands for time dimension. Formal: The main idea behind our approach is that we can use the current state to guess the next word. Formal: We have a set of tags and a set of tags that are related to the current tag. So, we can make a prediction about the next word based on both the tags and the current tag. Formal: To make this work, we need to figure out a few things: Formal: We start by figuring out the tags that are related to the current tag. This involves predicting the next word based on the tags related to the current tag. Formal: To do this, we start by predicting the tags related to the current tag. Formal: Next, we do a greedy search for the tag that has the highest chance of happening based on both the tags and the current tag. Formal: Finally, we end up with the tag that has the highest chance of happening based on both the tags and the current tag. Formal: So, the model can predict the next word by looking at the most likely tag. Formal: We start by predicting the tags related to the current tag. Formal: Next, we do a greedy search for the tag that has the highest chance of happening based on both the tags and the current tag. Formal: Finally, we end up with the tag that has the highest chance of happening based on both the tags and the current tag. Formal: This whole process is just a",
        "directory": "C96",
        "filename": "C96-1012.json"
    },
    {
        "casual_text": "There have been a bunch of question answering datasets created over the years (like the ones by Berant et al. in 2013, Joshi et al. in 2017, Trischler et al. in 2017, and Rajpurkar et al. in 2018, among others). But all of these were focused on answering single questions at a time. Saha et al. (2018) took a different approach and looked into answering a series of questions in a row, creating a dataset for that purpose. \n\nHowever, we’re doing things a bit differently from them in two main ways: \n1) They worked with question answering based on structured knowledge bases. \n2) Their dataset was kind of artificial: they had human annotators come up with templates using knowledge base predicates, and then they grouped individual questions together by predicate or subject to create sequences.",
        "formal_text": "Several question answering datasets have been proposed (Berant et al. , 2013; Joshi et al. , 2017; Trischler et al. , 2017; Rajpurkar et al. , 2018, inter alia). However, all of them were limited to answering individual questions. Saha et al. (2018) study the problem of sequential question answering, and introduce a dataset for the task. However, we differ from them in two aspects: 1) They consider question-answering over structured knowledge-bases. 2) Their dataset construction was overly synthetic: templates were collected by human annotators given knowledge-base predicates. Further, sequences were constructed synthetically as well by grouping individual questions by predicate or subjects.",
        "GPT2_formal_text": "ed_batch * (t * ) = arg min t, arg max t, arg diagonal t, arg normal t, arg jitter t, arg position t, arg normalized t, arg transliteration t, arg mask t, arg shuffle t, arg non-transformed t, arg invalid_token t, arg skip_token t, arg current t. arg [ ] t = arg min t, arg max t, arg diagonal t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg normal t, arg transliteration t, arg mask t. arg [ ] t = arg min t, arg max t, arg diagonal t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg normal t, arg transliteration t, arg mask t, arg mask t, arg masked t, arg non-transformed t, arg invalid_token t, arg skip_token t, arg current t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg transliteration t, arg mask t. arg [ ] t = arg min t, arg max t, arg diagonal t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg transliteration t, arg mask t, arg mask t, arg normalized t, arg transliteration t, arg mask t, arg mask t, arg transliteration t, arg mask t, arg normal t, arg normal t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg normal t, arg masked t. arg [ ] t = arg min t, arg max t, arg diagonal t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg transliteration t, arg mask t, arg normal t, arg transliteration t, arg mask t, arg mask t, arg normal t, arg transliteration t, arg mask t. arg [ ] t = arg min t, arg max t, arg diagonal t, arg normal t, arg transliteration t, arg mask t, arg",
        "directory": "D18",
        "filename": "D18-1134.json"
    },
    {
        "casual_text": "When it comes to figuring out what people are doing in a conversation—like asking questions, giving orders, or just chatting—people have come up with different ways to analyze it. For instance, Samuel and his team (1998) looked at how people talk, like the direction of the conversation, punctuation, special phrases, and n-grams to classify what’s being said. They also used things like tone, word choice, and sentence structure to help with this. More recently, Julia and Iftekharuddin (2008) and Sridhar and others (2009) did a great job using sound and tone features to classify dialogue acts. \n\nOn the other hand, Louwerse and Crossley (2006) used n-grams—which can work for both spoken and written dialogue—and tested them with a dataset called the Map Task Corpus (from Anderson and others, 1991). Bangalore and his team (2006) took this a step further by using n-grams from the last 1-3 things said to figure out what the current part of the conversation is about.",
        "formal_text": "For classifying dialogue acts in spoken dialogue, various features such as dialogue cues, speech characteristics, and n-grams have been proposed. For example, Samuel et al. (1998) utilized the characteristics of spoken dialogues and examined speaker direction, punctuation marks, cue phrases and ngrams for classifying spoken dialogues. used prosodic, lexical and syntactic features for spoken dialogue classification. More recently, Julia and Iftekharuddin (2008) and Sridhar et al. (2009) achieved high performance using acoustic and prosodic features. Louwerse and Crossley (2006), on the other hand, used various n-gram features-which could be adapted to both spoken and written dialogue-and tested them using the Map Task Corpus (Anderson et al. , 1991). Extending the discourse model used in previous work, Bangalore et al. (2006) used n-grams from the previous 1-3 utterances in order to classify dialogue acts for the target utterance.",
        "GPT2_formal_text": "= { [WORD1, ..., WORDJ] } = {w_s, ..., w_s_j}|{w_t, ..., w_t_j}|, where T is the total number of tokens. Formal: Using the classifier score we get from the dataset, we figure out the sentence's confidence score. Formal: To make sure the summarization works well, we use the confidence scores we get from the classifier to adjust the sentence's length. Formal: The length-control algorithm is super important. The first question we should be asking is, \"How do we use the confidence score to shrink the sentence?\" Formal: Formal: We came up with a way to estimate the confidence score for a sentence based on the confidence score for its summary. Formal: We tested it out and found that the confidence score for a summary from one classifier is way too high for any other classifier. Formal: For a specific summary i, the confidence score for the classifier is calculated like this: Formal: For each classifier m, the confidence score for the summary h is the sum of the confidence scores for all the classifiers that m belongs to. Formal: Formal: Formal: We calculate the confidence score c for a summary w in the class that m belongs to by adding up the confidence scores for the classifiers that m belongs to. Formal: Formal: Formal: Formal: Formal: We picked m = 3 because it’s the smallest number of classifiers we’re looking at. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D10",
        "filename": "D10-1084.json"
    },
    {
        "casual_text": "Baseline models: We're looking at a few different models here. First up, there's the CAPT model, which uses soft attention on a pair of input images. This attention thing is kind of like what's been done in image captioning before (like in Xu et al., 2015), but instead of just one image, we're dealing with two. In the CAPT model, we don't mess with any masking—we just skip the cluster info. The goal is to create a single sentence, so it's like a regular captioning model but with attention on two images.\n\nNext, there's the CAPT-MASK model, which is pretty much the same as CAPT, but it adds a masking mechanism. This mask is made by combining all the cluster masks for the image. We also have a version of CAPT that predicts a whole multi-sentence description—we call this one CAPT-MULTI. For this, we just slap all the sentences together in any old order.\n\nWe also tried a nearest neighbor approach (NN-MULTI), where we just grab the annotation from the closest matching training data point. We figure out how close they are based on the features we pull from the image pair and use the Nearest-Neighbor module from sklearn (Pedregosa et al., 2011). For the single sentence version (NN), we just randomly pick one of the sentences from the annotation.\n\nLastly, there's a version of the DDLA model with a fixed uniform prior, and we call this one DDLA-UNIFORM.",
        "formal_text": "Baseline models: We consider following baseline models: CAPT model considers soft attention over the input pair of images (This atten- tion mechanism is similar to that used in prior image captioning works (Xu et al. , 2015), except that we have two images instead of a single image input). We do not perform any masking in case of CAPT model, and simply ignore the cluster information. The model is trained to generate a single sentence. Thus, this model is similar to a typical captioning model but with soft attention over two images. CAPT-MASK model is similar to CAPT model except that it incorporates the masking mechanism defined earlier using the union of all the cluster masks in the corresponding image. We also consider a version of the CAPT model wherein the target prediction is the whole multi-sentence description -CAPT-MULTI -for this setting, we simply concatenate the sentences in any arbitrary order 2. Additionally, we consider a nearest neighbor baseline (NN-MULTI), wherein we simply use the annotation of the closest matching training data point. We compute the closeness based on the extracted features of the image pair, and leverage sklearns (Pedregosa et al. , 2011) Nearest-Neighbor module. For single sentence setting (NN), we randomly pick one of the sentences in the annotation. We also consider a version of DDLA model with fixed uniform prior, and refer to this model as DDLA-UNIFORM.",
        "GPT2_formal_text": "= self.compute_attention(h_attn, h_attn, d_attn) self.attn = self.attn || [ATTN1, ..., ATTNn] self.attn || [attn1, ..., attnn1] self.attn || [attn2, ..., attnn2] self.attn || [attn3, ..., attnn3] self.attn || [attn4, ..., attnn4] self.attn || [attn_mask] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn = [attn_mask || self.attn] self.attn",
        "directory": "D18",
        "filename": "D18-1436.json"
    },
    {
        "casual_text": "Organizing papers for a conference and making sure sessions have related topics can be really time-consuming, especially when you have a lot of papers. Machine learning can help automate this process by using natural language processing (NLP) to figure out what each paper is about. In this case, putting together a conference schedule is kind of like a semi-supervised clustering problem.\n\nIn this paper, we introduced the ADoCS system, a web app that can group papers into clusters based on how similar they are. The groups are created based on the size distribution you set up. Right now, the app is mainly designed for organizing conference papers, but it could be used for other document clustering tasks with some restrictions, thanks to its flexible interface (you can choose different metrics, use TF-IDF, etc.).",
        "formal_text": "Arranging papers to create an appropriate conference schedule with sessions containing papers with common topics is a tedious task, specially when the number of papers is high. Machine learning offers techniques that can automatise this task with the help of NLP methods for extracting features from the papers. In this context, organising a conference schedule can be seen as a semisupervised clustering. In this paper we have presented the ADoCS system, a web application that is able to create a set of clusters according to the similarity of the documents analysed. The groups are formed following the size distribution configured by the user. Although initially the application is focused on grouping conference papers, other related tasks in clustering documents with restrictions could be addressed thanks to the versatility of the interface (different metrics, TF-IDF transformation).",
        "GPT2_formal_text": "' is actually a word embedding vector, and the probability of its representation given a sentence is calculated using the embedding of the last word in the sentence. The embedding vector has a dimension of d, and the probability of its representation given the sentence is calculated by adding up the normalized probabilities for the whole sentence, plus the normalization factor p_sent. Formal: We make sure to use all the words in the sentence as inputs to the model. The embedding vector for each word gets fed into a CNN with an attention mechanism. We use the trainable output vector for the word to help the model learn how to handle the context. We also add a dropout rate of 0.1 after every 10 words to prevent overfitting. Formal: So, the final embedding vector for the word w_i in the sentence is: Formal: We create this final embedding vector by using the words from the sentence to help the model learn how to deal with the context. Formal: The contextualized word embedding is a matrix, and the probability of its representation given the context is calculated by the embedding of the last word in the sentence. Formal: We then add dropout to prevent overfitting. Formal: The final embedding vector for the word w_i in the sentence is: Formal: We train a CRF classifier using a training set that includes all the sentences with the word w_i. Formal: Finally, the final embedding vector for the word w_i in the sentence is: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E17",
        "filename": "E17-3011.json"
    },
    {
        "casual_text": "We're introducing AutoNER, a fresh neural model that uses a new \"Tie or Break\" approach for distantly supervised NER tasks. We've tweaked the usual NER model to create the Fuzzy-LSTM-CRF model, which works really well as a strong baseline for distantly supervised tasks. We're also looking into ways to improve distant supervision for better NER results, like adding high-quality phrases to cut down on mistakes, and we've done some experiments to check how well it works. Our tests on three popular datasets show that AutoNER performs the best when it's just using dictionaries without any extra help from humans, and it can even go head-to-head with models that use supervised benchmarks.",
        "formal_text": "• We propose AutoNER, a novel neural model with the new Tie or Break scheme for the distantly supervised NER task. • We revise the traditional NER model to the Fuzzy-LSTM-CRF model, which serves as a strong distantly supervised baseline. • We explore to refine distant supervision for better NER performance, such as incorporating high-quality phrases to reduce false-negative labels, and conduct ablation experiments to verify the effectiveness. • Experiments on three benchmark datasets demonstrate that AutoNER achieves the best performance when only using dictionaries with no additional human effort and is even competitive with the supervised benchmarks.",
        "GPT2_formal_text": ". It’s not exactly the same as the masked language model (MLM), which has been around for a while (thanks, Baldwin et al., 2006). The MLM uses a fancy method called Bayesian network inference to figure out how important each word in the text is. The masked language model is different from the original MLM because it’s not just replacing words with their synonyms, but it’s also figuring out which parts of the text are important to understand. This is super important for tasks like text classification (Baldwin et al., 2006), understanding language by looking at words and their meanings, and understanding how different words relate to each other. Formal: Word embedding: We use the BERT vocabulary to represent the words, and we do the same for the POS tags. We stick these words into special matrices (called embeddings) called W x and R x. Formal: Multi-task: We have two tasks—one for predicting the correct POS tag and another for picking out important parts of the text (like the start or end of a sentence). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1230.json"
    },
    {
        "casual_text": "Since the CoNLL-2012 dataset only tags mentions that are part of coreference chains, we start by creating a mention extraction module that’s really good at finding mentions but might not be super precise. This module mainly uses syntactic parse trees to do its job. We grab all NP nodes (noun phrases), QP nodes (like complex phrases for amounts or measurements), and any terminals with POS tags like PN (pronouns) and NR (proper nouns) to build a big list of potential mentions. \n\nAfter that, we use some rules to filter out mentions that probably aren’t right. For example, we remove mentions that include:\n1. Measure words like \"一年\" (one year) or \"一顿\" (one time).\n2. Named entities that fall into categories like PERCENT, MONEY, QUANTITY, or CARDINAL.\n3. Interrogative pronouns like \"什么\" (what) or \"哪儿\" (where).",
        "formal_text": "Since only the mentions that take part in coreference chains are annotated in the CoNLL-2012 shared task data set, we first generate a high-recall, lowprecision mention extraction module to extract as many mentions as possible. The mention extraction module relies mainly on syntactic parse trees. We extract all NP nodes, QP (quantifier phrase, i. e. , complex amount/measure phrase) nodes, and all terminals with part-of-speech tags PN (pronoun) and NR (proper noun) in parse trees to form a mention candidate set. Then, we employ some rules to remove unlikely mentions, e. g. , those which contain (1) measure words such as '一年/one year' and '一顿/one time'; (2) named entities whose categories are PERCENT, MONEY, QUANTITY, and CARDINAL; (3) interrogative pronouns such as ' 什么/what' and '哪儿/where'.",
        "GPT2_formal_text": ") is the number of documents where the relationship between the query and the article (called relation_i) shows up in both. And for the articles, the relevance_score(p_j, a_j) is calculated using the article-specific relation graph. Formal: To match the graph structure of the article-specific relation graph, we add the relation to the article-specific graph. Formal: We use the sub-graph representation to connect the query and article. Formal: The relation-specific graph is generated by combining the sub-graph representation with the graph structure of the article-specific graph. Formal: Each edge in the graph represents a query or article relation. Formal: You can find more details about our graph structure in the supplementary material. Formal: In the example, the graph representation for the article-related relation is (p_j, a_j). Formal: For embedding the article-related relation, we use the sub-graph representation (p_j, a_j) to represent the article-related relation. Formal: The graph representation for the query-related relation is (p_j, a_j). Formal: To embed the query-related relation, we use the sub-graph representation (p_j, a_j) to represent the query-related relation. Formal: For embedding the article-related relation, we use the sub-graph representation (p_j, a_j) to represent the article-related relation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D13",
        "filename": "D13-1028.json"
    },
    {
        "casual_text": "Can machine translation (MT) help the Department of Defense (DoD)? I think the answer is a solid, \"Yes, but with some caveats.\" We’ve shown this in a bunch of real-world cases and with tech solutions that use MT engines, which are part of several programs. Over the past week, you’ve seen and heard a lot of examples of how MT adds value to the DoD and other federal agencies. I’ll just touch on a few of the programs currently being used by the DoD. I’ll share specific examples where MT helped with intelligence work—all of it is unclassified and can be found online.",
        "formal_text": "Can MT help DoD? I believe the answer is a qualified, YES it can. We have proven this through numerous specific instances and with technology solutions where the underlying MT engines can be found in several of the following programs. You have seen and heard throughout this past week numerous examples of their value to DoD and other federal agencies. I will discuss and recap only a few of the programs that are currently being used within DoD. I will discuss specific examples where their value assisted in the Intelligence process; all unclassified and available through open sources.",
        "GPT2_formal_text": "ing layer. Formal: We take the encoder and decoder of a recipe and use them to turn the text into something we can use later. Formal: We get rid of any recipes that don't match a dietary restriction. Formal: We pick the best-performing recipe from the list of options. We pick one from the list if it works, and leave the rest for testing. Formal: In the first round of testing, we check if the recipe can be made from scratch or if it’s based on a video recipe. The second round of testing adds a spice to spice match rule to spice up the list of options. Formal: We pick the best-performing recipe from the list of options. We pick one from the list if it works, and leave the rest for testing. Formal: For the third round of testing, we use the list of top-performing recipes. We add a spice match rule for each spice ingredient. Formal: We pick the best-performing recipe from the list of options. We pick one from the list if it works, and leave the rest for testing. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "amta",
        "filename": "2008.amta-additional.1.json"
    },
    {
        "casual_text": "We've also presented and explained why using partial rewrite rules is a good idea. These rules keep the important formal stuff intact but work really well with the tech we have right now.",
        "formal_text": "We have also prcscntcd and justified the use of partial rewrite rulcs which, whilc maintaining the essential formal properties, arc computationally effective with available technologies.",
        "GPT2_formal_text": "= { \"qs\" } from Eq. 2. Formal: In this project, we're using a graph-based approach with a bidirectional LSTM network to figure out semantic dependencies. We start by using a CNN-based LSTM to pick out the most relevant argument candidates. Then, we match these candidates with the question and answer entities to get the semantic connection between them. To get a better grasp of how this semantic relation is formed, we train our model using data from PubMed searches. The goal is to automatically pull out the most relevant argument candidates from the question and answer sentences. We use a graph convolutional neural network (GCN) to do this. Formal: Our system is built with PyTorch (Paszke et al., 2019), and the main part of our framework includes three GCN layers. The first two layers, LSTM and CNN, handle the sequential processing of the sentences. Formal: Using a graph convolutional neural network (GCN) to pull out argument candidates is a cool way to build a semantic dependency graph. The idea is to start with the most relevant candidates and then add more ones to give the graph structure more depth. This approach can be seen as a \"summarization\" process. Formal: The third layer, GCN-RNN, is a linear transform that uses recurrent connections to show the graph structure. Formal: Formal: We tested our model on two big datasets for relation extraction: PubMed and YAGO. For each question and answer, we got four embedding vectors, one for each predicate. Formal: We also tested it on a smaller set of 200 question and answer pairs from YAGO (from Lai et al., 2016), and the results are in Table 2. Formal: We used the same setup as before, just changing the number of layers from 2 to 3 and adding a single RNN for the GCN layer. Formal: Formal: The results from the evaluation show that our GCN-RNN works really well. We tried three different setups for the GCN-RNN, and the best one performed the best overall. Formal: Formal: We then ran the model on three more datasets, and the results are in Table 3. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E91",
        "filename": "E91-1030.json"
    },
    {
        "casual_text": "Let's dive into how BioBERT, the toughest model we have, handles things when we give it a hard time. We looked at how it deals with different tricky situations in the BC5CDR dataset, which is all about chemicals and diseases. We messed with the data a bit by using synonyms, swapping words around, and even doing some keyboard-style typos to see how well BioBERT could still figure things out.",
        "formal_text": "BioBERT Error Analysis This section seeks to understand how the most robust model -BioBERT -behaves under adversarial evaluation. To this end, we analyzed NER model confusions with respect to the original datasets, synonym (S), swap (W), and keyboard (K) perturbations on the BC5CDR chemical and disease dataset(s).",
        "GPT2_formal_text": "_function + E_mask). Each of these is a multi-layer, bidirectional GRU, and its output is a vector with dimensions d_t × d_u × d_v, which is a matrix with dimensions d_t × d_u × d_v. Formal: The sequence sequence token is basically a sequence of n words, where each word is a d_t vector. The word embedding dimension, which is d_u, is used to represent the context vector. The masked word embedding dimension, which is d_v, is used to represent the context vector for the word. Formal: If you want more details about the GRU setup, check out [9]. Formal: The input to the GRU is a sequence of words, and the output is a vector with d_t dimensions. Formal: The updated hidden state vector h_t for the GRU during decoding is calculated by taking the updated hidden state vector h_t. Formal: The output embedding is a vector with d_t dimensions. Formal: The bidirectional GRU has a hidden layer dimension of d_u × d_v × d_t, which is a matrix with dimensions d_t × d_u × d_v. Formal: The recurrent neural network (RNN) is a type of neural network that can learn to generate text sequences. Formal: For a long sequence of tokens, the RNN can produce a sequence of output vectors with a length of n tokens, just like in the example in Figure 1. Formal: The recurrent neural network (RNN) can also predict the next word by looking at the previous hidden state and the output vectors. Formal: To create the masked word embedding, the RNN uses a bidirectional GRU to decode the hidden state and the output vectors. Formal: The masked word embedding is a vector with d_t dimensions. Formal: The update function for the bidirectional GRU during decoding is called M_t. Formal: The output embedding vector h_t for the bidirectional GRU is calculated by taking the output embedding vector h_t. Formal: The bidirectional GRU has a hidden layer dimension of d_u × d_v × d_t, which is a matrix with dimensions d_t × d_v × d_t. Formal: The update function for the recurrent neural network",
        "directory": "bionlp",
        "filename": "2021.bionlp-1.13.json"
    },
    {
        "casual_text": "The abstractive dialogue summarization task has been a topic in research, especially when looking at the AMI meeting corpus (McCowan et al., 2005). For example, Banerjee et al. (2015), Mehdad et al. (2014), and Goo and Chen (2018) have all talked about it. The issue is that the corpus doesn’t have many summaries—only 141 dialogues have them. So, Goo and Chen (2018) suggested using the assigned topic descriptions as a kind of reference. These are short, kind of like labels, that describe the meeting goals, like \"costing evaluation of project process\" or \"components, materials, and energy sources.\" There’s also stuff like \"chitchat.\" But these descriptions are pretty vague and don’t give much detail about the structure or who said what.",
        "formal_text": "The challenges posed by the abstractive dialogue summarization task have been discussed in the literature with regard to AMI meeting corpus (McCowan et al. , 2005), e. g. Banerjee et al. (2015), Mehdad et al. (2014), Goo and Chen (2018). Since the corpus has a low number of summaries (for 141 dialogues), Goo and Chen (2018) proposed to use assigned topic descriptions as gold references. These are short, label-like goals of the meeting, e. g. , costing evaluation of project process; components, materials and energy sources; chitchat. Such descriptions, however, are very general, lacking the messenger-like structure and any information about the speakers.",
        "GPT2_formal_text": "= { :read, :edit, :parse, :parse-tree, :synonym, :update, :eval })         Casual: We can easily use the format shown in Figure 2 to represent the sentence. Basically, if we have a sentence S = {s1, ..., sm}, which can be represented as [s1, ..., smi], then we can find all the nodes that have the same label (which we'll call the \"root\" of the sentence) in a tree structure. Formal: So, we can quickly find all the nodes with the same label in a tree structure that's kind of like the sentence we're looking at. Formal: Formal: For example, if we have the tree shown in Figure 2, we can easily find all the nodes with the same label. Formal: Formal: Here, the tree is made up of nodes with the same label, and the root node is the one that's closest to the head node. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D19",
        "filename": "D19-5409.json"
    },
    {
        "casual_text": "It's pretty rare to see direct comparisons of different taggers using the exact same test data. Most papers these days just claim that one tagger is better than another based on indirect comparisons. But we reckon there are a bunch of factors that aren't really taken into account or controlled for, which makes those conclusions kinda shaky in a lot of cases.",
        "formal_text": "Unfortunately, there have been very few direct comparisons of alternative taggers I on identical test data. However~ in most current papers it is argued that the performance of some taggers is better than others as a result of some kind of indirect comparisons between them. We think that there a. re a number of not enough controlled/considered factors that make these conchlsions dubious in most cases.",
        "GPT2_formal_text": "(x) = g_p(x, h_a_mask) Formal: We've got the results for L_m and L_n in Table 4, and the bottom part is from Figure 4. Formal: Basically, we're looking at how well the model can generalize to different domains. Formal: This part compares the different models. The y-axis shows how well the model generalizes to the domain we're testing. The x-axis shows how well the model generalizes to the entire dataset. Formal: Table 4 shows how the models did in two different tasks, D_m and D_n, for the same dataset. Formal: We use the same methods for both tasks. Formal: The results from training and testing are combined into one graph for comparison. Formal: We measure the F1 score using an accuracy score. Formal: Formal: Table 5 has all the details for the individual models. Formal: The results from the overall LSTM model are in Table 6. Formal: The results from the individual models are in Table 7. Formal: Formal: Formal: Formal: Table 8 has all the details for the individual models. Formal: We included results for the entire dataset. Formal: Formal: Formal: Table 9 has all the details for the individual models. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C98",
        "filename": "C98-2159.json"
    },
    {
        "casual_text": "Sure! Here's the informal version:\n\n1. Check out the Transformers library here: https://pypi.org/project/transformers/.  \n2. The ROCStories dataset is available at this link: https://cs.rochester.edu/nlp/rocstories/.  \n3. You can explore the SQuAD dataset here: https://rajpurkar.github.io/SQuAD-explorer/.  \n4. The WMT14 translation task info is here: https://www.statmt.org/wmt14/translation-task.html.",
        "formal_text": "https: //pypi. org/project/ transformers/. 5 https: //cs. rochester. edu/nlp/ rocstories/ 6 https: //rajpurkar. github. io/ SQuAD-explorer/ 7 https: //www. statmt. org/wmt14/ translation-task. html",
        "GPT2_formal_text": "\": [CLS] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT] [SEP] [TGT",
        "directory": "acl",
        "filename": "2021.acl-long.467.json"
    },
    {
        "casual_text": "Basically, for each step i in the process, we create a feature vector F i with four parts, using something called the H-feature detectorp i. This feature vector has: (1) how similar H i and w i are before any projection; (2) the result of applying featurep i to H i; (3) the result of applying the H-feature detectorp i to w i; and (4) the difference between parts 2 and 3.",
        "formal_text": "Concretely, in each iteration i of the procedure, we generate a four-valued feature vector F i, based on the H-feature detectorp i. Each feature vector contains (1) the similarity of H i and w i (before projection); (2) the featurep i applied to H i; (3) the Hfeature detectorp i applied to w i; and (4) the difference of 2 and 3.",
        "GPT2_formal_text": "ed_copy (s, ctx, mask, copy, arg) = tokenized_copy (s, ctx, mask) if arg is not None else tokenized_arg (s, ctx, mask) end end end\n\nHere's the link to the github repo: https://github.com/TsaiSeng/COPYPTR\n\nAlso, for the masked tokenization, we stick the tokenized name and the model's state into the hash. Then, we run a hash lookup with the hash of s, ctx, mask, and copy to get the masked tokenization. Formal: This example shows how to normalize data to get the input text. Formal: This example shows how to normalize data to get the input text. Formal: Here's a more casual version: Formal: Here's a more casual version: Formal: The last example shows how to normalize data to get the input text. Formal: The last example shows how to normalize data to get the input text. Formal: Formal: Figure 1 shows the final output text from the model. Formal: The final output text from the model. Formal: Figure 2 shows the tokenized output text. Formal: The tokenized output text. Formal: Formal: Formal: The original input text from the model. Formal: Formal: The original input text from the model. Formal: Formal: The masked tokenized output text. Formal: Formal: Formal: Formal: The original input text from the model. Formal: Formal: The masked tokenized output text. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D16",
        "filename": "D16-1234.json"
    },
    {
        "casual_text": "Turns out, CNNs can automatically pull out useful stuff from sentences, like the grammar and meaning. Sometimes, we use these features in other models to tackle different tasks. In our experiment, we show how well our model's features work. Since CNN models have already nailed finding relationships within single sentences, we tried something new: figuring out the relationship between entities that weren't in the same sentence.",
        "formal_text": "It has been proved that CNN could automatically extract useful features, encoding syntactic and semantic meaning of sentences. These features are sometimes fed to subsequent models to solve other tasks. In this experiment, we demonstrate the effectiveness of the extracted features from our model. Since CNN-based models have already succeeded in extracting relations from single sentences, we set our experiment in a new scenario: predicting the relation between entities which have not appeared in the same sentence.",
        "GPT2_formal_text": "\" : [l_p1, ..., l_pj], [l_s1, ..., l_sj], ...] and the whole setup is shown in Figure 1. Formal: Let’s say p_w is a vector that represents the prediction of a document w. For each document, we’re working with a total of n documents (d_n) and a total of k possible relation vectors (r_k) for each document, r_i. The model also figures out the probability of r_i happening given p_w. Formal: The latent variable for the document-relation pair is basically the average of all the documents in d_n, r_k, and r_i. Formal: We train our model using the hidden representation we get from the query logs and the entities in the documents. Formal: The query log, which is just the average of the logits for the documents in d_n, r_k, and r_i, is what we use to calculate the query log. Formal: The probability of an entity k happening with a query q_i is calculated based on the document-relation pair r_k and the query log. Formal: The probability of r_k happening with the query q_i is also calculated based on the document-relation pair r_i. Formal: The document-relation pair r_k is a vector that represents the predicted document-relation pair r_k. Formal: The entities r_k are the ones that show up in both the query and document-relation pairs. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D17",
        "filename": "D17-1186.json"
    },
    {
        "casual_text": "The idea of NLRS (Natural Language Report System) for really specific areas was first shown in Kukich's 1983 work about \"knowledge-based generation\" of stock market reports. Kukich's ANA system creates professional-sounding summaries of the stock market using daily data from the Dow Jones' half-hourly quotes for the market average and major indices. Both ANA and FRANA, its French counterpart (from Contant in 1986), used a phrasal lexicon approach (like the one Beeker described in 1975). This method keeps the language part limited, but it works well for small, predictable areas. The stuff I'm talking about now takes a more modular approach to NLRS and applies it to a new area.",
        "formal_text": "NLRS for highly reslricted domains was first demonstrated in the work of Kukicb_ (1983) on \"knowledge-based generation\" of stock market reports. Kukich's ANA system produces professionalsounding stock market stmlmaries using a daily trace of Dow Jones' half-hourly quoudions for the market average and major indices. Both ANA and the analogous FRANA system for French (Contant 1986) have used a phrasal lexicon approach (Beeker 1975) which limits the generality of the linguistic component, but which seems to suffice for small and stereotyped domains. The work described below represents a more modular approach to NLRS as well as a new application domain.",
        "GPT2_formal_text": ", which basically turns the input into a grid of numbers. The cool part is that the hidden states from these hidden layers get combined to create the actual representations for the tokens. Formal: In our approach, instead of dealing with the actual input, we treat it like a sequence of tokens, which is what the model actually sees. The attention function for the token representation is defined as... Formal: We use a gating mechanism with L2-regularization to handle the transitions between the token and the response. Formal: Here, s_i stands for the word embedding for the i-th token, and r_i is the hidden state for the i-th response, as calculated by the GCN. Formal: So, h_t is the hidden state of the response at time t, which we get by combining the token representation and the response representation. Formal: In this setup, the GCN doesn't really get involved in the actual interaction between the token and response. Formal: If the token has a feature vector in its vector space, that's the representation we get by combining it with the response vector. Formal: It's important to note that we don't do any cross-attention between the token and the response. Formal: Basically, the attention mechanism works by looking at how words appear in the context, based on their context vectors. Formal: The GCN figures out the attention weights for each token using cross-attention, and we add those weights to the token's representation. Formal: So, p_t(t) = h_t * p_t(t) + r_t * p_t(t) + ... Formal: Using equation (1), the GCN decides the attention weights for each token's representation. Formal: The token representation's representation, h_t, is calculated by multiplying the token's representation with the response's representation. Formal: Lastly, in our approach, we figure out the weights for the response's representation, r_t, by using cross-attention. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C86",
        "filename": "C86-1132.json"
    },
    {
        "casual_text": "• LightConv and DynamicConv. The code for these is included in fairseq 7 (Ott et al., 2019). We train lightweight convolution models in two different ways: (1) starting from scratch to learn token representations. In this case, we use BPE tokenization with a vocabulary of 30K types, and we rely on the fastBPE implementation 8 (Sennrich et al., 2015); (2) using pre-trained language model representations to initialize the token embeddings. For the language model, we go with GPT-2 small (Radford et al., 2019).",
        "formal_text": "• LightConv and DynamicConv. The implementation is available in fairseq 7 (Ott et al. , 2019). We train lightweight convolution models in two manners: (1) learning token representations from scratch; in this case we apply BPE tokenization with the vocabulary of 30K types, using fastBPE implementation 8 (Sennrich et al. , 2015); (2) initializing token embeddings with pre-trained language model representations; as a language model we choose GPT-2 small (Radford et al. , 2019).",
        "GPT2_formal_text": "= [(y, y')] are the entity embeddings. These embeddings are used to create a connection between the entity mention and the current node in the graph. The entity mention can also be thought of as the feature vector that represents the whole mention. Formal: We came up with a way to map entity mentions to the graph using a \"bag-of-words\" approach. This method is different from the usual methods that try to find the most likely entity match by looking at the words around the mention. Instead, we focus on finding the most likely entity match by looking at the words around the entity mention. We tested this method on the Chinese New Year Corpus and found that it works really well. Formal: Formal: We used the representation of entities as a feature vector to improve how we handle entity linking in the graph. This makes it possible to do type-based and Named Entity Recognition (NER) at the same time. Formal: We also tested our method on the ACE05 dataset, which is used for entity linking research (like Luo et al., 2013). We trained a model using the same setup as ours, using the entity embeddings as features. This model was used to find the best match between entities in the graph. Formal: The main idea behind entity linking is to link entities together in a way that helps improve how we understand the meaning behind the entities. Formal: We found that using a bag-of-words approach to represent entities works better than traditional NER or type-based approaches for entity linking. Formal: We also looked into how well our method works for entity linking by checking how well it works for Named Entity Recognition (NER) and type-based approaches. Formal: To make the most of the entity embeddings for graph structures, we added an extra layer to our model to figure out the entity type and entity type embeddings. Formal: We tested our method on the ACE05 dataset. The results showed that using a bag-of-words approach for entity linking can work for most types of entity types, as long as the entity types aren't super complex. Formal: We used the entity embedding vector to calculate the log probabilities of entity types and their embeddings, which is shown in the graph below. Formal: We also checked how well our method works for Named Entity Recognition (NER) and type-based approaches. Formal: We found that combining entity embeddings with the entity",
        "directory": "D19",
        "filename": "D19-5409.json"
    },
    {
        "casual_text": "We got our data from three main places—two for names of people and one for brand names. For the people's names, we used databases from the U.S. and the U.K. available at https://github.com/OpenGenderTracking/globalnamedata/tree/master/assets. The U.S. names come from birth records kept by the U.S. Social Security Administration from 1880 to 2013, and the U.K. names come from the UK Office of National Statistics, the Northern Ireland Statistics and Research Administration, and the Scotland General Register Office. After getting rid of names that could be both male and female, we had 97,102 unique English names to work with (60,984 female and 36,118 male).\n\nFor brand names, we used Kantar Media's Stradegy database, which tracks U.S. advertising spending by brands across almost every product category. We removed multi-word names that were derivative brands (like Ford Escort) and a small number (66) of names that are common English words (like Coach) based on the ~5,000 most frequent words in the British National Corpus. In the end, we had 1,021 brand names across 17 product categories. Each name was turned into a 16-dimensional vector based on the features we described, and we made all the data available in the supplementary materials.",
        "formal_text": "We draw data from three primary sources, two for people's names and one for brand names. For people's names, we relied on databases of U. S. and U. K. names available at https: //github. com/OpenGenderTracking/ globalnamedata/tree/master/assets. The U. S. data come from the yearly birth records maintained by the U. S Social Security Administration from 1880 to 2013; the U. K. data come from the UK Office of National Statistics, the Northern Ireland Statistics and Research Administration, and the Scotland General Register Office. After removing names that are labeled as both male and female, we ended up with 97102 unique English names (60984 female, 36118 male) to work with. For brand names, we relied on Kantar Media's Stradegy database. This database documents U. S. advertising spending by brands in virtually every product category. 2 In this case, after removing multi-word names that are derivative brands (e. g. , Ford Escort) and a small number (66) of names that are common English words (e. g. , Coach) based on the ∼5000 most frequent words in the British National Corpus, 3 we ended up with 1021 brand names in 17 product categories. We represented each name as a 16-dimensional vector based on the features described, and we made all data available in the supplementary materials.",
        "GPT2_formal_text": "ed_student_pairs_tls (X, Y, Z) ∈ R n×d, where X_tls is the original x, and Y_tls is the updated x, which we get from the student model (X, Y) and the teacher model (X, Z). Formal: Formal: You can find the full set of training data over at https://github.com/feng-yufei/L1-Attention. Formal: Here's the equation for the L1-LSTM model on the MNLI test set: Formal: Formal: Lastly, let's talk about the accuracy results for the NLI task (check out Table 3) using the \"shortcut\" sentence prediction dataset. Formal: Formal: To avoid the issue of using the same LSTM model multiple times for different outputs, we add a bias term to the output that goes through the LSTM. This helps the model generate more diverse outputs. Formal: Formal: Formal: This tweak doesn't really change the main goal of the task. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1142.json"
    },
    {
        "casual_text": "where n is the number of words in the sentence. Basically, for each episode, the way we update the parameters is based on the REINFORCE method, and we do this for every single token in the sentence.",
        "formal_text": "where n is the length of the sentence. 4 The direction of the parameters update for a given episode is then the one obtained from REINFORCE summed over all tokens:",
        "GPT2_formal_text": "= (t_i, t_j) if the sentence is one that includes both the verb and the object. Formal: So, the probability that word i and word j show up together in a sentence is calculated like this: p(w_i, w_j|a_i, a_j) = the sum of p_1(w_i, w_j) for all w_i, w_j, and a_i, ..., w_j in the sentence. Formal: For example, if the verb is \"break\" and the object is \"the sofa,\" the probability is calculated as p(w_i, w_j|a_i, a_j) = 1 / (1 + 2). Formal: Formal: This formula helps us figure out how likely two parts of a sentence are, based on how often they appear together in a text. We do this by comparing the sentence's probability to the chance that the words w_i and w_j appear together. Formal: By looking at how often the words w_i and w_j appear together, we can figure out the chances that the words w_i and w_j will appear together in a sentence. Formal: Formal: We use a model that predicts how often certain words (like verbs, nouns, or adjectives) will appear in a sentence and a model that predicts the probability that the word w_i or w_j will appear in a sentence. Formal: Formal: We're using a BiLSTM to figure out how likely two words (like verbs, nouns, or adjectives) are to appear together in a sentence. Formal: Formal: In this paper, we focus on two main things: how much the words w_i and w_j appear together in a sentence (called their frequency) and the probability that w_i and w_j show up together in a sentence (the probability). Formal: Formal: The probability that word w_i or w_j appears in a sentence depends on how often they show up together in a text. Formal: Formal: In our project, we mainly use this probability to figure out how likely two parts of a sentence are. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eacl",
        "filename": "2021.eacl-main.66.json"
    },
    {
        "casual_text": "- We added early exit tricks to skip unnecessary calculations, which makes the network faster.  \n- We ran a bunch of tests on GLUE to see how it works.",
        "formal_text": "• We incorporate early exit mechanisms as an enhancement to skip redundant computation, leading to further network acceleration. • We conduct extensive experiments on GLUE.",
        "GPT2_formal_text": "ed.\n          Casual: In our model, the attention is focused on the words in the sentence. So, the words we think are important at the current time step get passed to the previous time step. We add a second layer to the decoder to get the hidden representations of the words in the current and previous time steps. These hidden representations are then used to train the model's decoder. The whole process is laid out in Figure 2. Formal: In our model, the attention is on the words in the sentence. To find the hidden representations for these words, we use a linear layer to map them into the embedding space. We then use a linear layer to learn the attention function for the word embedding at the current time step. Formal: We also noticed that the attention is given to the words in the sentence, but we didn't include the sentence's embedding because it's not super important for the task we're working on. Formal: We tried out different values for λ in the third layer of the decoder. The results showed that a value of 0.5 worked the best. Formal: We tested our model on three real-world datasets: one is from the medical field, the other is from the legal field, and the last one is from the social media domain. The results are in Table 1, where we compare it to three other models using different values of λ. Formal: We also included the results for the second layer in the decoder, which is also used for the word embedding at the current time step. Formal: Our model is the top performer, beating all the other models by a good margin, especially on the biomedical dataset. Formal: The second layer in the decoder can work for the word embedding at the current time step, but in our case, we didn't have access to that data. Formal: Formal: We ran our experiments using the TAC 2008 dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.503.json"
    },
    {
        "casual_text": "Considering these challenges, this paper suggests using Graph Convolutional Networks (GCN) on a special kind of tree called a contextual sub-tree for event extraction. This contextual sub-tree is a modified version of a dependency parse tree that’s been trimmed in a unique way. It gives us not only the usual dependency path info but also extra details that aren’t on the main path. This extra info adds more context to the connection between two nodes, which is why it’s called a contextual sub-tree.",
        "formal_text": "With these challenges in mind, this paper proposes a solution applying Graph Convolutional Networks (GCN) over contextual sub-tree for the task of event extraction. The contextual sub-tree is a dependency parse tree uniquely pruned that provides not just dependency path information but also offpath information. The off-path information adds more context to the existing dependency path between two nodes; hence it is termed contextual sub-tree.",
        "GPT2_formal_text": "; g_s = G_s; h_s = h_s + 1; } else if (j == j-1) { b_s = g_s; h_s = h_s - 1; } else if (j == j-2) { b_s = g_s; h_s = h_s - 1; } else if (j == j-3) { b_s = g_s; h_s = h_s - 1; } else if (j == j-4) { b_s = g_s; h_s = h_s - 1; } else if (j == j-5) { b_s = g_s; h_s = h_s - 1; } else if (j == j-6) { b_s = g_s; h_s = h_s - 1; } else if (j == j-7) { b_s = g_s; h_s = h_s - 1; } else if (j == j-8) { b_s = g_s; h_s = h_s - 1; } else if (j == j-9) { b_s = g_s; h_s = h_s - 1; } else if (j == j-10) { b_s = g_s; h_s = h_s - 1; } else if (j == j-11) { b_s = g_s; h_s = h_s - 1; } else if (j == j-12) { b_s = g_s; h_s = h_s - 1; } else if (j == j-13) { b_s = g_s; h_s = h_s - 1; } else if (j == j-14) { b_s = g_s; h_s = h_s - 1; } else if (j == j-15) { b_s = g_s; h_s = h_s - 1; } else if (j == j-16) { b_s = g_s; h_s = h_s - 1; } else if (j == j-17) { b_s = g_s; h_s = h_s - 1;",
        "directory": "econlp",
        "filename": "2021.econlp-1.10.json"
    },
    {
        "casual_text": "The method we're suggesting isn't just good for dealing with all kinds of structures and bigger problems—it also has a solid covering number bound, which means it generalizes well.",
        "formal_text": "Not only does the proposed method has the advantages that it can handle arbitrary structures and larger-scale problems, its covering number bound also shows nice generalization properties.",
        "GPT2_formal_text": "ed = {y_mask, x_mask} to use the CNN/DailyMail embedding model. Formal: The trainable parameters are represented as c_trainable, while c_rand is just a random variable, which is more like a starting point. Formal: In the CNN/DailyMail model, the layer size is set to 300. Formal: We use the Adam optimizer (from Kingma and Ba, 2014) with a learning rate of 1e-4 to train this model. Formal: We train the model using mini-batch stochastic gradient descent (SGD). Formal: For the supervised version, we use a separate dataset with 5,000 samples, labeled SVM v_test. Formal: The model is trained using the WMT2014 dataset, following the WMT2014 development set. Formal: The training process involves training the model on a development set and then testing it on the test set using the development set. Formal: We're using the Hugging Face transformers package (Wolf et al., 2020) to get the embeddings and pre-trained BERT layers. Formal: Formal: We're using the Adam optimizer (from Kingma and Ba, 2014) with a learning rate of 1e-4. Formal: We start by training the model on SVM v_test. Formal: We train the model on SVM v_test, then test it on SVM v_test_dev. Formal: We train the model on SVM v_test_dev, then test it on SVM v_test_test. Formal: Finally, we train the model on SVM v_test_test, then test it on SVM v_test_dev. Formal: To get the embeddings and pre-trained BERT layers, we follow the CNN/DailyMail development set. Formal: Formal: We train the model on SVM v_test_dev and test it on SVM v_test_test. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eamt",
        "filename": "2009.eamt-smart.9.json"
    },
    {
        "casual_text": "Alright, let's break this down in a simpler way. We're talking about extending our parser to output \"S's\" (whatever that means). The tricky part is dealing with verbs like \"raising\" and \"equi\" that take infinitives or \"that\" clauses. But don't worry, it's not too hard to figure out how to handle these. \n\nThe reason it's not too tricky is because we're using LFG (Lexical Functional Grammar) as our base, not Kamp's categorial syntax. With LFG, the thematic roles in a sentence are already sorted out in the f-structure (functional structure). So, it's pretty straightforward to write rules that give us the right meanings for sentences with or without those tricky verbs.\n\nLet's look at an example:\n- \"John persuaded Mary to come\"\n- \"John persuaded Mary that she should come\"\n\nBoth of these sentences mean the same thing, just structured differently.\n\nNow, let's talk about how these rules work using a familiar example: \"every man loves a woman.\" With Kamp's categorial syntax, the rules work from the top of the tree down. The order in which we handle the parts of the sentence is determined by the syntactic rules. In other words, the scope (how things relate to each other) is directly tied to how the sentence is built.",
        "formal_text": "~ \"GRAMMATICAL PARSIAK~' AND \"lOGICAL P~RSIN~' In this section we will outline the principles anderlying the extension of our parser to produce ~S's as output. Because none of the fragments of ~T contains Raising-and Equi-verbs taking infinitival or that-complements we are confronted with the task of writing construction rules for such verbs. It will turn out, however, that it is not difficult to see how to extend ~T to eomprise such constructions. \"ibis is due to the fact that using LFG as syntactic base for IRT -and not the categorial syntax of Kamp -the ~raveling of the thematic relations in a sentence is already accomplished in f-structure. Therefore it is streightfo~rd to formulate construction rules which give the correct readings for (i0) and (ii) of the previous section, establish the propositional equivalence of pairs with or without Raising, Equi (see (I), (2)), etc. (I) John persuaded Mary to come (2) John persuaded ~%~ry that she should come let us first describe the BRS construction rules by the f~niliar example (3) every man loves a woman Using Ksmp's categorial syntax, the construction rules operate top down the tree. The specification of the order in which the parts of the tree are to he treated is assumed to be given by the syntactic rules. I. e. the specification of scope order is directly determined by the syntactic construction of the sentence.",
        "GPT2_formal_text": "= [v_i, c_j, v_j + 1, ..., v_i + L] for i = 1, ..., L.\n\nNow, the source embedding f_s for the caption p is calculated using the model. Formal: The importance of the context embedding e_i (t_i) in a sentence is calculated using the model. Formal: Equation (1) is: (1) where W is a matrix of length D_s×d, and b_s is the number of times the word w_s shows up in the source sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E83",
        "filename": "E83-1009.json"
    },
    {
        "casual_text": "We've come up with three ways to figure out the best value of k by using the H2 values from 1... deltaK. Basically, we look at how H2 changes as k increases and pick the point where those changes stop being meaningful.",
        "formal_text": "We have developed three methods that take as input the H2 values generated from 1. . . deltaK and automatically determine the \"right\" value of k, based on finding when the changes in H2 as k increases are no longer significant.",
        "GPT2_formal_text": "ed = (x_i, y_i)          Casual: To keep things efficient, we only add the input and output blocks at the same time, so we can get the hidden state at the end. Formal: The equation for embedding is basically the same as the one for projection. For input, the output block is created using the input block. Both inputs and outputs are combined into a single representation called a_i. Formal: The final embedding of the input is calculated using the output block at position i. Formal: In our experiments, we noticed that models trained with tensor attention often perform worse on the test set compared to models trained with text-based attention. That's why we came up with a new approach called attention-based transformer. Our attention-based transformer adds a linear layer on top of the input layer. Formal: The input and output blocks are combined at the same time. We also use different transformer layers for each language pair. Formal: Since the input and output blocks are combined, the hidden state from each layer gets passed through a transformer block. Formal: If we add a linear layer, the model's performance on the test set improves by 2.4% compared to the standard model. This is because the linear layer helps improve the model's representation of the input. Formal: Formal: The matrix representation of the input is computed based on the output block at position i. Formal: The input and output blocks are combined at the same time. Formal: When we add a linear layer, the model's performance on the test set improves by 2.4% compared to the standard model. This is because the linear layer helps improve the model's representation of the input. Formal: Formal: The matrix representation of the input is computed based on the output block at position i. Formal: We also use different transformer layers for each language pair. Formal: For the general case, the model's performance on the test set improves by 2.4% compared to the standard model. Formal: The matrix representation of the input is computed based on the output block at position i. Formal: We also use different transformer layers for each language pair. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "E06",
        "filename": "E06-2007.json"
    },
    {
        "casual_text": "To make sure we have more examples for the less common relations shown in Figure 1, we create some fake or \"silver\" data. This helps us transfer models from one language to another, usually from a language with lots of resources to one with fewer. We’ll show how this works using English and Hindi as examples.",
        "formal_text": "To increase coverage of examples for the relations located at the tail of the distribution as reported in Figure 1, we also generate (semi) synthetic or 'sil ver' data that is used for model transfer from one language to another, commonly from a resource rich to lowresource language. We illustrate the process using En and Hi.",
        "GPT2_formal_text": "\" is a key term in a relational model, and we'll use it again later. The \"word_level\" part of the vectors we get are the embeddings of the words. Formal: So, for each word in the input text, we take its contextualized embedding and apply some linear transformations. Formal: Next, we calculate the semantic similarity scores for all the words in the input sentence using these vectors. Formal: After that, we pick out the top-1 hypernyms from the vectors and use them as word embeddings. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "conll",
        "filename": "2021.conll-1.45.json"
    },
    {
        "casual_text": "We ran QuestEval on the QAGS-XSUM data for all the different combinations of hyperparameters and models. This way, SummEval stayed completely out of the picture during testing.",
        "formal_text": "For all the variations of hyper-parameters and models, we tested QuestEval on QAGS-XSUM data, in order to keep SummEval unseen at test time.",
        "GPT2_formal_text": "ed. This means the model doesn't mess with the text's structure during training. Formal: For the second model, we start by training a model using a model from the dataset D_s. This model is trained using the rule-based method shown in figure 1. Then, we use it to predict the next word in the text. Formal: For each model we train, we use the text's shape to create a token representation. We also use the model's predictions to get the final word representation. Formal: The model is trained with a rule-based method that focuses on how words in the text relate to each other. Formal: In this paper, we use the word embedding, h_w, to represent the entire word. Formal: The current word embedding is created by mapping the word's vector, h_v, onto the space of the words in the text. Formal: To get the input for the gated recurrent unit (GRU), we calculate the dot product of the text and the vector, h_t, using the embedding h_w for the target word, w_t. Formal: To create the final representation of the whole sentence, we use the output of the GRU from the last time step. Formal: The final word representation is calculated based on the hidden state h_t. Formal: The hidden state h_t is calculated by multiplying the output of the GRU with the hidden state h_t for the target word, w_t. Formal: Finally, we add up all the hidden states of the GRU to get the final output of the whole sequence. Formal: Formal: We show how the training process works in figure 4. Formal: We've set up our model to predict the next word in a sentence based on the target word's shape. Formal: To make sure we don't end up with weird examples where the word doesn't match, we add some noise to the word embedding. Formal: Formal: Next, we use our model to predict the next word in the sentence. Formal: Figure 4: The training process for the cross-lingual word embedding model. Formal: Formal: The input for the gated recurrent unit (GRU). Formal: The final word representation, h_t, is calculated based on the hidden state h_t for the target word, w_t. Formal: Formal: Finally,",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.529.json"
    },
    {
        "casual_text": "It makes sense that the feelings of users with more influence in a community should carry more weight when figuring out the overall mood of the group. So, we try to gauge the community's emotion by considering each user's \"emotional authority.\" This is based on the idea that different people in the community have different levels of emotional influence.",
        "formal_text": "Intuitively, the emotion of user with higher authority in community should be more important in determining community emotion. Thus we estimate community emotion by taking into account user's emotional authority, which is based on the assumption that different users in a community have different emotional authorities.",
        "GPT2_formal_text": "= {h t } i=1:n. Formal: To train the model, we use the validation set with the correct labels, as well as the test set with the incorrect labels. We use the negative log-likelihood of the data to estimate the parameters. In our setup, we use beam search to pick the best model. We start with beam search decoding on the dev set and then switch to a full-sentence model on the test set. Formal: We take the dev set and the test set and use the negative log-likelihood on the dev set and the test set. The losses we learn are the log-likelihood of the test set and the negative log-likelihood of the dev set. Formal: For every sentence i in the test set, the model gets a probability P i (a|a). Formal: The model learns the probability P i (a | a) for every sentence in the test set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D14",
        "filename": "D14-1123.json"
    },
    {
        "casual_text": "To meet the needs of our customers, we tweaked the SAIC MT system to work with their data and added some features that let us integrate translation memory. Over time, this has really boosted the quality of the MT output, which means less work for humans to do in post-editing. Plus, we fixed some issues with formatting and how the MT output looked, which helped cut down on post-editing costs even more for the TSP. Here's how this paper breaks down:\n\nIn Section 2, we'll explain the SAIC hybrid MT system and its key features. Section 3 will show how we integrated translation memory (TM) into the system using special flags. These flags make sure that full TM matches are always picked, and partial matches are preferred during translation.\n\nSection 4 talks about a method we used for offline domain adaptation. This let us create a customized baseline system for the customer. We also have an online adaptation method that allows us to make updates to the system as new TM entries or other parallel data come in.\n\nIn Section 5, we'll show how a few simple but effective techniques improved the readability of the MT output for humans. These included tweaking punctuation spacing, capitalization, and making adjustments for differences between European and Canadian French.\n\nFinally, Section 6 presents the experimental results. We'll show how the adaptation affected automatic MT metrics on different test sets, both in-domain and out-of-domain.",
        "formal_text": "To meet these customer requirements, we adapted the SAIC MT system to the customer data and introduced features into the system that allowed for integration of translation memory. Overtime, this has substantially improved the quality of the MT output and thus reduced the human post-editing effort. In addition, we addressed the formatting and MT output representation issues, which lead to further reduction of the post-editing costs of the TSP. This paper is organized as follows. In Section 2, we describe the SAIC hybrid MT system and its main features. Section 3 presents how the translation memory (TM) is integrated into the statistical search by using special flags which ensure that full TM matches are always selected, and partial TM matches are preferred during translation. In Section 4, we describe a method for off-line domain adaptation that allowed us to create a customertailored baseline system, as well as an on-line adaptation method which enables us to perform incremental updates of the system with new TM entries and other parallel data. In Section 5 we show how a number of simple, but useful techniques lead to improved readability of the MT output by humans. They involved punctuation spacing, capitalization, and adjustment to differences between European and Canadian French. Section 6 presents the experimental results. We show the impact of the adaptation on automatic MT metrics on different in-domain and out-of-domain test sets.",
        "GPT2_formal_text": "ing_3d = softmax(d * 2d, h d) # This is done to figure out the average attention weight. Formal: To handle cases where words are different lengths, we use the spaCy toolkit to calculate the attention weights. Formal: If we're dealing with a multi-word token, we apply max pooling and get the final attention weights. Formal: The loss for SRL is set to 0.3. Formal: For sentiment classification, we train the model on the test set to maximize the loss and only use the training set to tweak the parameters. Formal: The loss for SRL is also set to 0.3. Formal: In the top-1 prediction, the model decides if two words are the same or not. Formal: When we're testing, we use the validation set to tweak the parameters. Formal: We use the cross-entropy loss to measure how well the model is doing. Formal: The training goal is 0.1. Formal: The loss for SRL is 0. Formal: Formal: We use the cross-entropy loss to measure how well the model is doing. Formal: Formal: The loss for SRL is 0. Formal: Formal: The loss for SRL is 0. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "amta",
        "filename": "2012.amta-commercial.11.json"
    },
    {
        "casual_text": "So, segs(c_j) is just a way of saying all the segmentation labels that are connected to c_j. And e_seg(segs(c_j)) is a 5-part vector where each part can be either on or off, depending on whether it matches one of these options: B, M, E, S, or O.",
        "formal_text": "where segs(c j ) denotes all segmentation labels related to c j, and e seg (segs(c j )) is a 5-dimensional multi-hot vector with each dimension corresponding to an item of {B, M, E, S, O}.",
        "GPT2_formal_text": ") ) (Kim et al., 2018; Song et al., 2019) Figure 3: Here's the setup for the baselines. For example, the condition (C D1) gets turned into (C D2) by combining the embeddings of the main event and the context event from two different entities. Formal: The event and context events are swapped with their embedding vectors and then fed into a Bi-LSTM model. Formal: For the first entity to trigger, the embedding is created by combining the embedding of the event trigger and the entity's embedding, which are created by using the embedding of the entity's embedding plus the embedding of the event trigger. After that, the event trigger is fed into the next event to be generated. Formal: Using an entity type embedding, a classifier is trained to predict if the context event is from the entity type. Formal: Formal: The classifier is trained on the generated event and context events to predict if the entity type is being generated. Formal: The classifier then learns to predict the entity type based on the embedding of the generated event and context event. Formal: In this experiment, the first entity to trigger is the entity type, and the second entity is the context event. The context event is generated using the Bi-LSTM model. Formal: The classifier is trained to predict the entity type based on the embedding of the generated event and context event. Formal: Using the embedding of the event trigger and the entity type, the classifier is trained on the generated entity type event and context event to predict if the entity type is being generated. Formal: The classifier then learns to predict the entity type based on the embedding of the generated event and context event. Formal: The entity type can be generated using the classifier and the entity type embedding. Formal: The classifier learns to predict the entity type based on the entity type embedding and the entity type embedding. Formal: After that, the generated entity type is predicted using the classifier. Formal: Lastly, the final entity type is used as the label for the generated event. Formal: For the last entity type to trigger, the entity type is generated using the Bi-LSTM model. Formal: After that, the generated entity type is predicted using the classifier. Formal: Formal: Finally, the generated entity type is predicted",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "People have figured out some cool ways to handle big classification problems, like the dual coordinate descent method for linear support vector machines, which worked well (Hsieh et al., 2008). Similarly, Yu et al. (2011) came up with a two-level version of dual coordinate descent for maximum entropy classifiers.",
        "formal_text": "Advancements in solving large-scale classification problems have been shown to be effective such as dual coordinate descent method for linear support vector machines (Hsieh et al. , 2008). Similarly, Yu et al. (2011) proposed a two-level dual coordinate descent method for maximum entropy classifier.",
        "GPT2_formal_text": "ed_layers.hpp: This is the first layer of the attention-based masked LSTM. The input to this layer is a sequence of token embeddings and attention values. We set the dropout probability to 0.9 and the batch size to 32. We train the model using Adam (from Kingma and Ba, 2014) with an initial learning rate of 0.001. Formal: Here, n is the number of attention heads and d_i is the attention value embedding for the i-th token in the input sequence. The output of the attention-based model at the i-th position is calculated using a one-layer feedforward neural network with a hidden size of 256. Formal: LSTMs are known to be really good at handling long-range dependencies, which is why they've been used a lot in Natural Language Processing. Unlike traditional methods, which focus on just one point and can't handle long-distance dependencies, LSTMs can handle both local and global dependencies at the same time. Formal: To make use of this, we created a simple model where the attention-based output is basically a one-to-one match for the token embedding and attention value embedding. This setup lets us generate tokens with varying attention values from a set distribution called the attention-masked_layers.hpp. Formal: To use the model, we first use the attention-based LSTM to generate attention value embeddings for the i-th token. We then combine these with a softmax layer to create the token embedding and attention value embeddings for the next token in the sequence. Formal: We set the learning rate to 0.001 and the batch size to 32. Formal: To avoid overfitting, we set the dropout probability to 0.9 and the batch size to 32. Formal: We train the model using Adam (from Kingma and Ba, 2014) with an initial learning rate of 0.001. Formal: We follow the same setup as the original paper. Formal: We didn't include any extra experiments here because we didn't have the resources to make it happen. Formal: Formal: In the last part, we train the model using a single-layer feedforward neural network. Formal: For the model setup, we suggest using a multi-head attention mechanism, which is what Lample et al. (2016b) suggested. Formal: For the",
        "directory": "D14",
        "filename": "D14-1183.json"
    },
    {
        "casual_text": "The sequence model in this paper is pretty similar to the LSTM-based model mentioned in another source. As you can see in Figure 1, there are three context utterances labeled u1, u2, and u3, which are connected as a sequence of words. A special word called \"sos\" is added between each pair of utterances to mark where one ends and the next begins. \n\nOnce we have the sequences for both the context and the response, we turn the words into word embeddings using a shared lookup table. To create the context and response embeddings, we use a Gated Recurrent Unit (GRU) neural network, which was introduced by Chung et al. in 2014. The GRU works on these two sequences of word embeddings, following the steps outlined in Equations 2 to 5. \n\nHere, ht-1 is the hidden state of the GRU when it processes the word embedding et-1 of the word wt-1. The initial state h0 is just a zero vector. The variables zt and rt are the update and reset gates, respectively. The new hidden state ht for the embedding et is a mix of the previous hidden state ht-1 and the current input embedding et, with the update and reset gates controlling how much of each is used. \n\nThe parameters U, Uz, Ur, W, Wz, and Wr are part of the GRU model and need to be learned. The symbol ⊗ represents element-wise multiplication.",
        "formal_text": "The word sequence model in this paper is similar to the LSTM-based model proposed in. As shown in Figure 1, three utterances of context c, written as u 1, u 2 and u 3, are connected as a sequence of words. A special word sos is inserted between every two adjacent utterances, denoting the boundary between utterances. Given the word sequences of context and response, words are mapped into word embeddings through a shared lookup table. A Gated Recurrent Unit neural network (GRU) (Chung et al. , 2014) is employed to construct the context embedding and response embedding. It operates recurrently on the two word embedding sequences as Equation 2 to Equation 5, where h t−1 is the hidden state of GRU when it reads a word embedding e t−1 of word w t−1, h 0 is a zero vector as the initiation state, z t is an update gate and r t is a reset gate. The new hidden state h t for embedding e t is a combination of the previous hidden state h t−1 and the input embedding e t, controlled by the update gate z t and reset gate r t. U, U z, U r, W, W z and W r are model parameters of GRU to be learned. ⊗ denotes element-wise multiplication.",
        "GPT2_formal_text": "= list(pos_ids)       Casual: We're mainly looking at the top-k most common words that the model has seen so far in the training data (k = 100), plus a few extra words that it hasn't seen before. If we find a word that shows up at least k times and isn't in the top-k, we mark it as the next word. Basically, the model can learn to skip words if they're not in its dictionary yet. Formal: In this part, we're checking how well the model can handle new words by calculating something called perplexity. Formal: (1) A word is considered new if it shows up fewer than k times in the training data. Formal: (2) A word is new if it appears at least k times but isn't in the dictionary yet. Formal: (3) If a word is new, it gets a score of 1. Formal: (4) If a word is new, it gets a score of 0. Formal: (5) If a word is new, it gets a score of 0. Formal: (6) If a word is new, it gets a score of -1. Formal: (7) If a word is new, it gets a score of -2. Formal: (8) If a word is new, it gets a score of -3. Formal: (9) If a word is new, it gets a score of -4. Formal: (10) If a word is new, it gets a score of -5. Formal: (11) If a word is new, it gets a score of -6. Formal: (12) If a word is new, it gets a score of -7. Formal: (13) If a word is new, it gets a score of -8. Formal: (14) If a word is new, it gets a score of -9. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "Plus, our sentence vector includes almost all the info from previous sentences, which their model can't do.",
        "formal_text": "And our sentence vector also integrates nearly all the history information of previous sentences, while their model cannot.",
        "GPT2_formal_text": "ed, attention_masked_mult, and attention_masked_mult_v1. Formal: This part explains how we calculate attention mask for a sentence. Basically, the attention mask is created by multiplying the attention scores for the word positions in the sentence. Formal: We train a model using the knowledge from a teacher (T) and also find a new model using some data (S). During both training and the prediction, we focus on the word positions that the model predicted. Formal: During training, we use the teacher's best hidden state to predict the word positions. For the prediction part, we only look at the original hidden state, so the model's prediction is based only on the teacher's best hidden state. Formal: After training, we figure out the hidden representation (H) for the word positions. We then add it to the hidden representation of the sentence to train the model. Formal: The teacher's hidden representation (H) is made up of a bunch of embedding vectors (D = {d_1, ..., d_T}) for the words in the sentence. Formal: The hidden representation (H) for a sentence is just its teacher's hidden state. Formal: We can create a single attention representation (H) for the whole sentence by just combining the hidden representation (H) from each word in the sentence. Formal: Figure 2 shows how we get the attention values for the word positions in a sentence. Formal: We use a Dirac delta function to figure out the attention scores. Formal: We start by making a batch of word representations, where each word in the sentence gets its own vector. Formal: Next, we calculate the attention values by taking the average of the vectors from the first and second layers of the teacher model. Formal: We use the knowledge from the teacher model to predict the word positions. Formal: Formal: We then use the self-attention mechanism to combine the representations of the word positions. Formal: The final attention value (h) is the result of combining the vectors from the first and second layers of the teacher model. Formal: Formal: We calculate the attention values for the word positions in a sentence by using the first and second layer of the teacher model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D15",
        "filename": "D15-1106.json"
    },
    {
        "casual_text": "We also took a closer look at a few things that impact how well these big language models (PLMs) work, like the pretraining method, the data they're trained on, how their vocabulary is built, and the schedule for optimizing them.",
        "formal_text": "We further analyze several factors that affect the performance of PLMs, including pretraining method, pretraining corpora, vocabulary construction, and optimization schedules.",
        "GPT2_formal_text": "= {h i } | {h j } i=1, ..., n. Formal: The probability P(s i, s j ) for a pair (s i, s j ) given its labels (s i, s j ) is calculated using an activation function that gives each label a probability, which is given by the cosine similarity between the word embedding vector h i and the log probability of a word pair, w i+1 to w j. Formal: The posterior probability P(s i, s j ) for a sentence s j given its labeled tag set s i is just the sum of the prior probabilities P(s i, s j ). Formal: The conditional probability P(s i, s j ) for a sentence s j is calculated by adding up the conditional probabilities P(s i, s j ) for the same sentence. Formal: We’ll also mention that the conditional probabilities P(s i, s j ) are given by the log likelihoods of the words w i+1 to w j and the log probability of the tag set, as mentioned by Mintz et al. in 2011. Formal: We’ll skip the stuff about individual words and tag sets because they’re not important for the task. Formal: If we're talking about a specific word or label, the conditional probability P(s i, s j ) is just the sum of the conditional probabilities for the words w i+1 to w j and the tag set, as mentioned by Mintz et al. in 2011. Formal: The estimates of the model parameters we get from these two kinds of probabilities are shown in Figure 1, and they’re part of the x-axis of the figure. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "clinicalnlp",
        "filename": "2022.clinicalnlp-1.2.json"
    },
    {
        "casual_text": "Basically, imagine you have a graph G with nodes (N), edges (E), and labels on those edges (R). You also have a bunch of pairs of nodes (s i, t i) from a set D. You can make a matrix where the rows are these node pairs and the columns are the edge labels. PRA takes this matrix and adds extra columns for sequences of edge labels, which we call \"path types.\" Instead of just showing if there's an edge between two nodes, the cells now show how specific the connection is between those nodes based on the path type.",
        "formal_text": "More formally, given a graph G with nodes N, edges E, and edge labels R, and a set of node pairs (s i, t i ) ∈ D, one can create a connectivity matrix where rows correspond to node pairs and columns correspond to edge lables. PRA augments this matrix with additional columns corresponding to sequences of edge labels, called path types, and changes the cell values from representing the presence of an edge to representing the specificity of the connection that the path type makes between the node pair.",
        "GPT2_formal_text": "ed: [e1, ..., em] * αc_j, where c_j can be 1, ..., m. Formal: In Section 5, we talk about how attention can be broken down into three parts: attention to words, words to sentences, and sentences to words. We also show how attention can be used to figure out who a pronoun is referring to. For example, if the pronoun \"he\" is talking about John, the attention for that specific thing \"John\" (like \"himself\" in Figure 3) can be broken down into two parts: 1. John is the one receiving the attention. 2. John is the one receiving the attention because \"himself\" refers to John, not himself. Formal: Basically, we're breaking down attention into three parts: words to sentences, sentences to words, and words to words. Formal: Formal: For more info, check out Appendix D. Formal: To make things easier, we’ll use words from the text that match the pronoun’s first name. This can mean words like \"himself\" or \"himself\" being labeled as first-name-only in the final version of the model. Formal: We’ll use two types of word embedding: one called GloVe (from Pennington et al., 2014) and another called FastText (from LeCun et al., 2015). The GloVe version is pre-trained on Wikipedia, so it works well for our situation. Formal: The FastText embeddings have been trained on a mix of Wikipedia and stuff like Twitter, with a set of dimensions of 300 and 300. Formal: We’re using the definition from (Chen and Bansal, 2015), where we’re looking at how similar two words are. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D14",
        "filename": "D14-1044.json"
    },
    {
        "casual_text": "In this project, we’re diving deep into why people make edits on Wikipedia. We came up with a detailed list of reasons why someone might change a page, like fixing grammar, adding more info, checking facts, or making things easier to understand. Unlike other systems that either focus on tiny grammar tweaks or mix up grammar and meaning, our approach looks at the bigger picture—what the editor is actually trying to do. This helps us figure out not just what changed, but why, and what’s going on in the editor’s head while they’re working (which, by the way, some smart folks have studied before).\n\nTo make this happen, we teamed up with 13 experienced Wikipedians to create a system that captures the \"why\" behind edits, which we call \"edit intention.\" We labeled over 7,000 edits with these intentions to get a clear picture. Then, we built a tool to automatically figure out these intentions by looking at the differences between versions of articles.\n\nTo see how useful this all is, we tested our system on two big questions for Wikipedia: how to keep new editors around and how edits affect article quality. We looked at whether the kind of edits newbies make in their first session predicts if they’ll stick around, and how different types of edits impact how good an article is.",
        "formal_text": "In this work, we focus explicitly on revision intention. We introduce a fine-grained taxonomy of the reasons why an author in Wikipedia made an edit. Example edit intentions include copy editing, elaboration, verification, and simplification. Compared to taxonomies that either focus on low-level syntactic operations (Faigley and Witte, 1981) or that mix syntactic and semantic classes ), a clean higher-level semantic categorization enables us to easily identify textual meaning changes, and to connect revisions to \"what happens in the mind of the revising author during the revision\" (Fitzgerald, 1987; Daxenberger, 2016). In order to capture the meaning behind edits, we worked with 13 Wikipedians to build a taxonomy that captured the meaning of an revision, which we term edit intention, and hand-labeled a corpus of 7, 177 revisions with their edit intentions. We then developed an automated method to identify these edit intentions from differences between revisions of Wikipedia articles. To explore the utility of this taxonomy, we applied this model to better understand two important issues for Wikipedia: new editor retention and article quality. Specifically, we examined whether edit intentions in newcomers' first editing sessions predict their retention, and examined how edits with different intentions lead to changes in article quality.",
        "GPT2_formal_text": "= (r, t) = (r, s, t) ∈ R w. Formal: The final loss is calculated by averaging the loss of the first part, L_1, and the first part of the final loss, L_2, to get the final loss, L_N. Formal: We can tweak the model by picking different probabilities for the source domain and the target domain. Formal: To get the loss for the target domain, we average the losses for both the source and target domains. Formal: We can also tweak the model by choosing the target domain with the highest loss. Formal: We use cross-entropy loss for the cross-domain transfer learning process. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D17",
        "filename": "D17-1213.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\n\"You can find some datasets for text mining here: [https://www.cs.umb.edu/~smimarog/textmining/datasets/](https://www.cs.umb.edu/~smimarog/textmining/datasets/) and also check out this link for more corpora: [http://disi.unitn.it/moschitti/corpora.htm](http://disi.unitn.it/moschitti/corpora.htm).\"",
        "formal_text": "https: //www. cs. umb. edu/˜smimarog/textmining/datasets/ 2 http: //disi. unitn. it/moschitti/corpora. htm",
        "GPT2_formal_text": "ed' : {| v | v < 0 }, τ = { τ ∈ { 0, 1 } | v ∈ { 0, 1 } }, which is basically the total number of tokens in the original sentence and the translated version. Formal: The cross-attention layer works by using the attention from each attention head to adjust the attention weights for the whole context, which helps in capturing the relationship between the two contexts. Formal: The shared attention layer combines the encoded representations from the two contexts and passes the adjusted weights through a softmax layer. This layer can also be thought of as a representation for the context. Formal: The final output from the multi-head attention is what we call the contextual representation h ctx, which helps calculate the normalized attention weight τ ctx for the whole context. Formal: In a nutshell, the final representation h ctx gives us the normalized attention weight τ ctx for the context, which acts as the contextual representation for the sentence. Formal: Formal: Let’s quickly go over the different model setups in Table 2. Formal: For each setup, the input and output embedding matrices get combined using a linear transformation. Formal: To make sure the model can learn from different embedding matrices and still perform well on different tasks, we added dropout layers on the output and input matrices. Formal: This setup helps prevent the model from overfitting to any specific embedding matrix. Formal: Formal: The model is trained using a feed-forward neural network (FFNN), as described by Bahdanau et al. in 2014. Formal: The hidden size is set to 300 for FFNN and 10 for a linear layer in the softmax layer. Formal: After training, we check the performance using cross-validation on a development set. Formal: Formal: We also add a regularization parameter α to help the model converge. Formal: Formal: As mentioned in (Lin et al., 2016), the model is trained using AdaDelta (Zhang et al., 2016) with a learning rate of 0.01 and a regularization constant ε set to 0. Formal: Formal: For future research, we’re thinking about expanding this model to handle more complex topics. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-1345.json"
    },
    {
        "casual_text": "To get a clearer picture of how the instance relabeling strategy works, we tracked the Exact Match Accuracy of our method with and without this strategy on the MTOP test set after each training session. As you can see in Figure 2, our method with the relabeling strategy just keeps getting better and consistently outperforms the baseline during the relabeling phase. This shows that the relabeling approach actually fixes a lot of label mistakes in the messy training data, and those corrected labels help boost the performance.",
        "formal_text": "To better understand the effect of the instance relabeling strategy, in Figure 2, we record the Exact Match Accuracy of our method with or without the relabeling strategy on the MTOP test set after each training epoch. The performance of our method with relabeling strategy keeps improving and is consistently better than the baseline during the relabeling stage. It demonstrates that the relabeling method indeed corrects many label errors in the noisy training data and the corrected labels contribute to the performance improvement.",
        "GPT2_formal_text": "= {u1, ..., uk}, where k can be any number from 1 to M. To get the model's final prediction for the response xi, we use a logistic regression classifier, which is a type of neural network. Formal: To figure out the posterior probability p(r|s), we use the LSVM classifier from the Wolf et al. (2018) library (thanks to Wolf et al., 2018). Formal: With the results from the logistic regression classifier, we can calculate the latent variable P_i, which is the log probability of the response r given the input xi. Formal: Our example is based on the validation set from the SQuAD v1.1 test set (thanks to Rajpurkar et al., 2016). Formal: The model learns to rank responses based on how likely they are to be correct. Formal: We're using the LSVM classifier from Wolf et al. (2018) with the default settings. Formal: To predict the response yj, we combine the latent variable P_i, P_j, and the log probability of yj, which we get by running the logistic regression classifier again. Formal: For each response, we evaluate the model by averaging its score across all the results. Formal: The model's final prediction yj is a mix of the predictions from the logistic regression classifier and the latent variable P_i, P_j. Formal: The latent variable P_i, P_j, and the log probability of yj are all normalized to have a normalized value. Formal: The log probability of yj is calculated by adding up all the log probabilities of yj from the logistic regression classifier and the latent variable P_i, P_j. Formal: In this section, we’ll explain how we incorporate the posterior distribution of responses into the model. Formal: This is done by tweaking the LSVM classifier from Wolf et al. (2018) using the same settings as before, but this time, the log probability of yj is normalized based on how likely the model is to predict yj, rather than just the previous response yj. Formal: We’ll also talk about some newer methods for estimating the posterior, like those by Gao et al. (2019) and Sun et al. (2019). Formal: We’ll also use two other logistic regression",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.259.json"
    },
    {
        "casual_text": "So, to deal with these issues, recent methods have focused on two main strategies: making the story generation model more controllable and adding in some commonsense knowledge. One of the big ideas in controllability is the \"plan and write\" method (Yao et al., 2019). They start by using a RAKE algorithm to pick out the most important words from each sentence and then train a storyline planner using that data. The language model is trained to work with both the previous context and these keywords. When it comes time to generate a story, the keywords are created from the title and help guide each sentence as it's written.\n\nCommonsense knowledge is basically shared knowledge about how the world works (Alabdulkarim et al., 2021). They fine-tune a pre-trained GPT-2 model using knowledge triples from commonsense datasets. First, they turn these triples into sentences using some predefined rules (like turning (eiffel tower, AtLocation, paris) into \"eiffel tower is at paris\") and then train the model on these sentences using the usual maximum likelihood estimation method.\n\nXu et al. (2020) took things a step further by combining both of these approaches. They first trained a keyword planner using GPT-2 and then used those keywords to search a knowledgebase. The top-ranked sentences from the search were used to guide the story generation process.",
        "formal_text": "Therefore, recent approaches resort to two main strategies to alleviate above issues, by adding more controllability into the story generation model and incorporating commonsense knowledge. One of the most influential strategies of controllability is \"plan and write\" (Yao et al. , 2019) where they first use a RAKE algorithm to extract the most important word from each sentence and train a storyline planner based on such dataset. The language model is trained conditional on both the previous context and the keywords. During generation, the keywords are generated from the given title and can be used to guide generation of each sentence. Commonsense contains shared knowledge about the world (Alabdulkarim et al. , 2021). fine-tune a pretrained GPT-2 with knowledge triples from commonsense datasets. They first use pre-defined rules to turn triples into sentences (e. g. (eiffel tower, AtLocation, paris) → \"eiffel tower is at paris\") and train on the knowledge sentences with conventional maximum likelihood estimation objective. Xu et al. (2020) combine these two approaches by first training a keyword planner with GPT-2 and use the keywords to search a knowledgebase to retrieve the top ranked sentences to guide the generation process.",
        "GPT2_formal_text": "ing is the best method out there right now. It's pretty much the only one that gives you a solid starting point. Formal: Alright, so a big part of the reason for picking this setup is because you can also use a bunch of different models to handle the problem. For instance, you could use a logistic regression model to predict the words, and then you can add a ReLU activation to the input to make the model focus more on the important words. Formal: So, basically, this is about figuring out how likely it is to say the next word in a text (y) based on the whole bunch of previous words (x1, x2, ..., xn). Formal: We’ve also got a model that uses two types of words, word1 and word2, to figure out the probability of a single word based on the others. Formal: Here, x1 and x2 are the words in the sentence that are important for the prediction. Formal: The final model gives us a probability distribution for a sequence of words. Formal: Here, we’re using two different words as inputs for the model, x1 and x2, and we’re creating a probability distribution for the word sequence y1, y2. Formal: We’re also testing out different ways of combining the probabilities, like using a weighted sum, a cross-entropy loss, or some other methods. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "alta",
        "filename": "2021.alta-1.10.json"
    },
    {
        "casual_text": "Let's make sure we compare this to the exact IR engine that was used to create the dataset for the PU model. The PU model really relies on the IR engine doing a good job, so it's important to keep that in mind.",
        "formal_text": "We note that the comparison here should be made to the specific IR engine which resulted in the dataset of the PU model, as the PU model benefits greatly from better performance in the IR engine.",
        "GPT2_formal_text": "= [s_i, ..., s_k], where s_i is just the word embedding for that part of the sequence. Formal: We use the word embeddings to help figure out the sequence of span labels. Formal: The embeddings for each word in the sequence (h_t, h_t + 1, ..., h_t + k) are calculated using the F1 score (from Ma and Hovy, 2014) and the average sentence length from the BiLSTM's output layer. Formal: We also check out how well the model can predict the sequences of span labels. Formal: To get the final sequence embeddings, we calculate the average of all the embeddings, and then apply the weighted sum of all the embeddings to get the final sequence embeddings. Formal: We also calculate the average sentence length for each span across all the classes to get the final sequence embeddings. Formal: For the word embeddings, we combine them with the sequence embeddings. Formal: Lastly, we check how well the model handles transitions and truncations. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "eacl",
        "filename": "2021.eacl-main.47.json"
    },
    {
        "casual_text": "I’m super thankful to the University of Malaya for covering the costs of my time in Britain.",
        "formal_text": "I a~l also grateful to the University of Malaya for providing the funds for my stay in Britain.",
        "GPT2_formal_text": "_tgt. Without the attention from the prior, the model's performance goes down. Check out Figure 3 for an example of how to use this attention mechanism. Formal: Let’s start by saying we have a sentence in the input language, let’s call it f. Let’s call f′ the input sentence. Following what Shafiei and his team did in 2019, we’ll use a Bi-LSTM to turn f′ into a sentence vector. After that, we’ll group these sentence vectors together and put them into a new vector space. We’ll call this new space f_s. The word embedding for f, or w_f, is what we’re focusing on here. Formal: At each step t, we’ll generate a different attention vector for the input sentence, v_t. This vector will be created based on the output from the Bi-LSTM at the point we’re looking at right now. Formal: Using the graph attention mechanism, we can also calculate the attention weights by looking at the graph of how similar the input and output sentences are. Formal: The attention weights for the input sentence, v_t, are calculated by taking the weighted sum of all the attention vectors. Formal: The graph attention mechanism, which is a graph neural network (GAN), helps us learn how to represent the input sentence, v_t, in a way that’s similar to how it’s represented in the graph. Formal: The attention weights for the output sentence, v_t, are calculated by taking the weighted sum of all the attention vectors. Formal: We’ll then group the attention vectors together and put them into a new vector space, f_s. Formal: To learn the attention weights for the output sentence, v_t, we’ll take the weighted sum of all the attention vectors, except for the one for v_t. Formal: The output from the Bi-LSTM is what we’re focusing on here. Formal: We’ll use the attention weights that the Bi-LSTM assigns to v_t to figure out the attention weights for the input sentence, v_t. Formal: We’ll keep doing this for all the input sentences in the sentence vector space, except v_t, until we hit a point where we’re",
        "directory": "C86",
        "filename": "C86-1098.json"
    },
    {
        "casual_text": "Based on previous research, we're sticking with four main roles for the event structure: ARG0, ARG1, TIME, and LOC. But here's the twist: during the pre-processing stage, we're adding a rule that makes sure TIME entities only go into the TIME role and LOC entities only go into the LOC role.",
        "formal_text": "Following prior work, we restrict the event struc-ture to the following four argument roles: ARG0, ARG1, TIME, and LOC. However, we additionally add a type constraint during pre-processing that requires entities of type TIME and LOC only fill matching roles (TIME and LOC respectively).",
        "GPT2_formal_text": "= { ... } tag = arg n = arg n+1:n j = j + 1:n i = arg n = arg n+1:n. Since we're dealing with both continuous and discrete stuff here, the joint model has a bunch of parts, and they're all connected to each other through equations (2-6) and (6-3). The joint model, as we'll see, works really well and is super efficient because we can easily add other models to it. Formal: Let's talk about the embedding model, E, and how it's set up to make sure the parameters of the model are learned properly. Formal: The embedding model, E, is designed to keep the parameters of the model, p(t_i |t_j), in check. This way, the model can correctly pick up on the important info from the training data and match the correct label for each input. Formal: The embedding model, E, is set up to keep the parameters of the model, p(t_i |t_j), in check. The model learns to learn these parameters by using a regular gradient descent process, which is described in equations (9), (10), and (11). Formal: After that, we can just focus on the latent representations for the model, p(t_i |t_j). Formal: The model learns to learn these latent representations, p(t_i |t_j), using a gradient descent process, which is described in equations (9), (10), and (11). Formal: We also add a couple of regularizers, α and β, to help the model understand the relationship between the input and the prediction. Formal: Finally, we use a smoothing method to balance out the differences between different points in the data, giving us a nice, smooth model. Formal: The embedding model, E, is designed to keep the parameters of the model, p(t_i |t_j), in check. The model learns to learn these parameters by using a regular gradient descent process, which is described in equations (9), (10), and (11). Formal: Formal: The model learns to learn these parameters, p(t_i |t_j), using a gradient descent process, which is described in equations (9), (10), and (11). Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.382.json"
    },
    {
        "casual_text": "Some folks have tried using different metrics for MT tuning, but the results have been kind of hit or miss. One notable example is Padó et al. (2009), who claimed their entailment-based metric led to better human evaluations. The catch? It’s super heavy and slow in real-world use. They estimated it would take 40 days to run on the NIST MT 2002/2008 dataset, so they had to split the process into two phases and use a smaller n-best list. In our case, we’re working with the WMT 2010 dataset, which is similar in size, and most of our runs finish in less than a day. \n\nMeanwhile, Cer et al. (2010) tested tuning a phrase-based SMT system with metrics like BLEU, NIST, METEOR, and TER. Even though METEOR and TER have been shown to align more closely with human judgments, they found that BLEU and NIST are still the go-to options for MT tuning.",
        "formal_text": "Some researchers have investigated the use of better metrics for MT tuning, with mixed results. Most notably, Padó et al. (2009) reported improved human judgment using their entailment-based metric. However, the metric is heavy weight and slow in practice, with an estimated runtime of 40 days on the NIST MT 2002 /2008 the authors had to resort to a two-phase MERT process with a reduced n-best list. As we shall see, our experiments use the similarly sized WMT 2010 dataset, and most of our runs take less than one day. Cer et al. (2010) compared tuning a phrase-based SMT system with BLEU, NIST, METEOR, and TER, and concluded that BLEU and NIST are still the best choices for MT tuning, despite the proven higher correlation of METEOR and TER with human judgment.",
        "GPT2_formal_text": ", etc.). We set the dimensions of the variable α to 100, and the weights of the word embeddings W_i and b_i are both set to 1. We train the model for 20 epochs, using the Adam optimizer (from Kingma and Ba, 2014) with a learning rate of 10^-8. We call this the Parallel Function Strategy, or PFS for short. Formal: The final model we generate has an entropy value of 0. The model can pick any attention distribution, as long as the probability of the final response is less than or equal to the entropy of the prior distribution P_i. Formal: We tweak the model's parameters using the regular Gibbs sampling method, which has been pretty successful in text generation tasks (Koehn et al., 2003). The weight of each word embedding is calculated as (1 − the entropy of the word embedding W_i) / (1 + the entropy of the prior distribution P_i). Formal: For the masked word embeddings, we use the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 10^-8 and a batch size of 64. Formal: We set the hidden size of the conditional random field to 16 and set the batch size to 2. Formal: For the word embeddings W_i and b_i, we use cross-entropy loss to estimate the probability of the masked word's output. Formal: For the projection weights, we calculate the normalized probability (pred) with the Adam optimizer (Kingma and Ba, 2014). Formal: We train the model using stochastic gradient descent, following the method suggested by Luong et al. (2015). Formal: The model gets a probability of 0.5 for the correct label. Formal: We use an early stopping threshold, α, to decide when to stop training. Formal: To avoid overfitting, we add a term to the training loss, β, that increases as the training progress continues. Formal: The final model's output is the normalized probability of the correct label. Formal: To make sure we're not mixing up words and their context during training, we keep the latent word embeddings of the correct word and the masked word the same length. Formal: We keep the hidden size of the conditional random field the same and set the batch size to 2. Formal: We train the model for 50 epochs, using",
        "directory": "D11",
        "filename": "D11-1035.json"
    },
    {
        "casual_text": "Wow, this looks like a bunch of random symbols and characters! It seems like it might be some kind of code or encrypted message. If you're trying to figure out what it means, you might need to decode it or find the key to unlock its meaning. Maybe it's part of a puzzle or a secret message? If you have any more context or clues, that could help crack it!",
        "formal_text": "£ v G | 4 x z y U ) v U I v y | 4 v º r È y U A y ¡ Y { i ¤ y y ± Ê } z y m ¦ y { É U p ¤ y ® ¡ Y { 4 6 y y ± z ¡ } I y | 4 { ) ½ ) q § ¦ y ª U h { i y m 4 U y | 4 x | $ v { ¦ z y ª z U ¦ U ê | 4 v 4 y { 4 } p | 4 { z y m ) | i y ¤ z y m U ) Ũ | 4 v É | Õ y m U y m 4 6 v W | U ª 4 y m U ² b w 1 x z y m 4 y m U 4 y Ã 0 ª ¦ 3 À ª 0 ª § { 4 y m | i | 4 z v { 1 D z k { i y m 4 U y U { p U { i y y 2 n U v } p | 4 z 1 | 4 x y ¤ { i | 4 } p z U £ v U ¤ y % U { 4 { 4 } y | 4 x | µ y ¥ ¤ y | 4 U y z y { 4 { 1 ¤ ¤ y AE @ I Ç Ã © á & 6 0 ² Ã 0 r á ¢ Ì â I 8 7 d ¥ ¤ y | i y é ä ± { 4 ¢ U { ¦ x v 1 x p | i y m U y m © ' â I ë ¥ è 4 â f á î ¥ ¥ ö å I á 4 â f è f ó C ç 4 ë f ê á I â f ô © ' â ' ì ê 4 ë 4 ë 4 è I â ' ì 4 ä 4 à f ä f ç s å 4 ý I â è ' ê é I â ' à ¤ î ê á Ó ð ¦ ¢ ð f ð ð ¦! ¤ % ¢ 4 ö ù ê á Ó ð ¦ õ d ð ð ¦ ¢ ï \" ö 4 ð ê á Ó ð ¦ $ ð ð ¦ & ¢ù f ö ê á Ó ð ¦ ¢ ñ f ð ð ¦ ¢ ñ 4 ï f ñ \" ê á Ó ð ¦ & ¢ 4 ð ð ¦ $ # ¢ I ò ¥ ê á Ó ð ¦ ò ð ð ¦ õ 5 ö # ¢ ' ê á Ó ð ¦ ¢ ï f ð ð ¦ ¢ ð ù ñ \" ê á Ó ð ¦ ¢ ö f ð ð ¦ ¢ ð 4 ñ f ö 4 ð ê á Ó ð ¦!",
        "GPT2_formal_text": "\" (which are the topics or items) for the question. We can show this in a simple way using a simple list of values. Formal: The title of the question, the title of the answer, and the title of the answer all go together. So, if the title is \"Who,\" the answer is \"Who,\" and both are in the title, the title of the answer would be \"Who,\" and the title of the question would be \"Who.\" Formal: The title, the title of the answer, and the title of the question all go together. If the title is \"Who,\" the answer is \"Who,\" and both are in the title, the title of the answer would be \"Who,\" and the title of the question would be \"Who.\" Formal: The title, the title of the answer, and the title of the question all go together. Formal: Formal: Here, p stands for the percentage of the question's title that's in the answer. Formal: Here, p is the percentage of the question's title that's in the answer. Formal: To figure out the title of a question, we look at the title of the answer's title. Formal: We look at the title of the answer's title, p = p(a), and the title of the question's title, p = p(c). Formal: We use two methods to figure out the title of a question. Formal: For each title of the answer, we calculate the ratio of the word count in the question to the word count in the answer's title. Formal: We calculate the ratio of the word count in the answer's title to the word count in the title of the question's title. Formal: Formal: And finally, we calculate the ratio of the word count in the answer's title to the word count in the title of the question's title. Formal: Formal: This makes it easier to spot the title of a question and figure out the title of an answer, all while keeping the answer's title. Formal: Formal: Formal: Formal: For example, if the title of a question is \"Who,\" and the answer is \"Who,\" the title of the answer is \"Who.\" Formal: Formal: Formal: This could be an example of a question and answer that don't match up. Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C02",
        "filename": "C02-1109.json"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way:\n\n- **v_q** represents the query vector.\n- **v_s** represents the sentence vector.\n- **LexCL** is used to compare the value of the word in **v_q** with the word in **v_s**.\n- **10best** means finding the top 10 sentences that match **v_q** based on cosine similarity between **v_q** and **v_s**.\n- **context** refers to the context of **v_q** and **v_s**.\n- **grammar** refers to the grammar of **v_q** and **v_s**.\n- **LexQL** is used to find the best match (**1best**) between **v_q** and **v_s**.",
        "formal_text": "v q ) repr. (v s ) Value LexCL v q word v s v q 10best v s cosine (v q, v s ) v q context v s v q grammar v s LexQL v q v s 1best",
        "GPT2_formal_text": "and attention_segment, respectively. We focus on the attention mechanism that helps the model understand sentences by paying attention to the words that are most likely to show up in a sentence. We call this the sentence-level attention mechanism, and we add a special label to the output word array. Formal: • The first and second parameters for the attention matrix and attention matrix, respectively. Formal: • The first and second hidden states of the entity embedding matrix, plus the attention word embedding. Formal: • The third hidden state of the entity embedding matrix, plus the attention word embedding. Formal: • The fourth hidden state of the entity embedding matrix, plus the attention word embedding. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1055.json"
    },
    {
        "casual_text": "This paper suggests tackling this issue by tweaking the Recognizing Textual Entailment (RTE) framework to better handle metaphor interpretation. RTE is designed to be a broad, general-purpose task that covers a lot of the semantic reasoning needed across different applications (Dagan et al., 2007). It's seen as super important for building smart but reliable natural language processing systems because most of the semantic reasoning needed in stuff like question answering and information extraction can be boiled down to RTE problems (Dagan et al., 2007). Basically, textual entailment is about figuring out if a hypothesis can be drawn from a given text. In simpler terms, it's a one-way relationship between two pieces of text—Text T and Hypothesis H—where if T entails H, then anyone reading T and looking at H would think H naturally follows from T. For example, take example 1560 from the RTE-1 dataset, which uses the metaphor 'incubate'.",
        "formal_text": "This paper proposes to address this by adapting the Recognizing Textual Entailment (RTE) framework for metaphor interpretation. RTE aims to be an abstract and generic task that captures major semantic inference needs across applications (Dagan et al. , 2007). RTE is considered central for the development of intelligent yet robust natural language processing systems because most of the semantic inference needed in natural language applications such as QA and IE can be characterized as problems in RTE (Dagan et al. , 2007). Intuitively, textual entailment consists of determining whether a hypothesis can be inferred from a given text. The textual entailment operational definition is a directional relation between two text fragments, Text T and Hypothesis H such that T entails H if humans reading T and considering H will infer that H follows from T. An illustration can be given by example 1560 of the RTE-1 dataset (which involves a metaphor in the use of 'incubate'):",
        "GPT2_formal_text": ". unlink(l) → update_ids(l) APP ← APP\n           Casual: Okay, so here's what's going on:\n\n- First, we have a pair of messages with their own unique IDs.\n- Then, we have two random messages that are the same length but look totally different.\n- Finally, there's a third random message, which is a mix of the two others.\n\nSo, it's basically a bunch of messages where one is short, and the others are all over the place. Formal: The algorithm checks two messages, which have their own unique IDs, and then picks one from the mix to be the main one. Formal: Then, the main message is made up of the other two, like a mix of two others. Formal: Lastly, the algorithm picks the main message based on what it thinks is the most likely to be the real thing, which is the second one in the mix. Formal: Formal: Finally, the algorithm does something similar but for the first three messages, just like we did with the first three. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C08",
        "filename": "C08-2001.json"
    },
    {
        "casual_text": "Okay, so we're talking about a 2 × 500 English to German model here. The word embeddings make up about 63% (that's 50 million out of 84 million) of the total parameters. But the size of these embeddings doesn't really affect how fast the model runs because the word embedding layer is just a simple lookup table that only impacts the first layer of the model. So, we're more focused on cutting down the memory usage of the student models by pruning weights.\n\nWeight pruning for NMT (Neural Machine Translation) was looked into by See et al. (2016), and they found that you can prune up to 80-90% of the parameters in a big NMT model without losing much performance. So, we took our best English to German student model (the 2 × 500 Seq-KD + Seq-Inter one) and pruned x% of its parameters by getting rid of the weights with the smallest absolute values. After that, we retrained the pruned model using Seq-KD data with a learning rate of 0.2 and fine-tuned it with Seq-Inter data at a learning rate of 0.1.\n\nAs See et al. (2016) pointed out, retraining was super important. The results are in Table 3, so check that out for the details.",
        "formal_text": "2 × 500 English → German model the word embeddings account for approximately 63% (50m out of 84m) of the parameters. The size of word embeddings have little impact on run-time as the word embedding layer is a simple lookup table that only affects the first layer of the model. We therefore focus next on reducing the memory footprint of the student models further through weight pruning. Weight pruning for NMT was recently investigated by See et al. (2016), who found that up to 80 − 90% of the parameters in a large NMT model can be pruned with little loss in performance. We take our best English → German student model (2 × 500 Seq-KD + Seq-Inter) and prune x% of the parameters by removing the weights with the lowest absolute values. We then retrain the pruned model on Seq-KD data with a learning rate of 0. 2 and fine-tune towards Seq-Inter data with a learning rate of 0. 1. As observed by See et al. (2016), retraining proved to be crucial. The results are shown in Table 3.",
        "GPT2_formal_text": "_size, which is the size of the attention mask. Formal: In our early experiments, we tried different settings for the attention mask size: 0.1, 0.2, 0.3, 0.5, 1, 2, 5, 10, 50, and 100. These numbers correspond to the average number of dimensions in each HBAG token. The results from our early tests are in Table 1. The results for different values of p(t) are shown in Table 2. To train the model, we used cross-entropy loss on the test set. Formal: To make sure the model can actually handle noisy inputs in real-world scenarios, we used the official T5 RoBERTa dataset (T5 RoBERTa) to train it. This dataset has the same number of test samples as T5, with the same number of noisy samples. Formal: The t-th label t_i is calculated using the previous hidden states h_t_i. For training the model, we set the attention mask size to 300. Formal: To check if the model can pick up on patterns and general patterns from the training set, we ran the model for 100,000 steps on the test set. Formal: We noticed that the model gets better at handling noisy samples as the attention mask size gets bigger. Formal: We tested the model on three different datasets and found that it works better when we use a smaller attention mask. Formal: We also tested the model on a synthetic dataset (that has the same number of samples as T5). This synthetic dataset has the same number of samples as T5, but it has the same number of noisy samples. Formal: We used the method from (Lin et al., 2019) to train the model. Formal: We picked the model that performs best on the synthetic dataset as our main test set. Formal: We ran the model for 100,000 steps to get the best results. Formal: To get the best results for T5, we trained the model on T5 for 50,000 steps, then used the same setup for the rest of the experiments. Formal: To check if the model can handle noise in situations where there are no clear patterns, we trained the model on T5 for 50,000 steps. Formal: We started with the same training and testing set as T5, but switched to the synthetic dataset (which has the same number of samples as T5) during testing.",
        "directory": "D16",
        "filename": "D16-1139.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nIf you have something like `t[ε → t]`, it just means `t`. But if you see `f(t1, ..., tn)[(i • p) → t]`, it means you take `f(t1, ..., tn)` and replace the `i`-th part with `t` after applying `p` to it. This works as long as `i` is between 1 and `n`.",
        "formal_text": "t[ε → t ] = t f (t 1, . . . , t n )[(i • p) → t ] = f (t 1, . . . , t i [p → t ], . . . , t n ) for 1 ≤ i ≤ n.",
        "GPT2_formal_text": "= l)\n          Casual: We take the output tokens from the second-to-last layer and combine them with the memory from the first-to-last layer. This combined memory is then passed into the last-to-last layer, and the result is used as the final hidden representation. Formal: In the second layer, we take the input hidden representation h_i and use it to initialize the output. Formal: In the last layer, we put together the memory representation from the first-to-last and last-to-last layers. We call this combined memory h_i_t, which is just the combined hidden representation h_i. Formal: Finally, we put all these combined hidden representations together into one final representation by combining them with h_t. Formal: Formal: We’ve got two main steps to make sure our model performs well: Formal: We train our model using cross-entropy loss. Formal: For the prediction layer, we use the training set. Formal: For the optimization layer, we set the optimization parameter to ε. Formal: For the optimization goal, we use the validation set. Formal: We set the learning rate to η. Formal: Lastly, we add some regularization to the model. Formal: Formal: We train the model using cross-entropy loss. Formal: For the prediction layer, we use the training set. Formal: For the optimization layer, we set the optimization parameter to ε. Formal: For the optimization goal, we use the validation set. Formal: For the optimization goal, we set the learning rate to η. Formal: Lastly, we add some regularization to the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E06",
        "filename": "E06-1048.json"
    },
    {
        "casual_text": "The main point here is that instead of using text, we're defining U as a series of speech units that are super strong and efficient, kind of like text, but we figured them out without needing text to guide us. We set up the S2U model so that when we want to make a guess, we can say U = f(S), which means we can turn any speech audio S into a sequence of units U. By adding this third part, we can train P(U|I) using a bunch of images with spoken captions.",
        "formal_text": "The key idea in this paper is to instead define U to be a sequence of learned speech units that are as robust and compact as possible like text, but discovered without text supervision. We define inference with this S2U model as U = f (S), enabling us to \"transcribe\" any given speech audio waveform S into a sequence of units U. The addition of this third component enables us to train P (U | I) from a dataset of images paired with spoken captions",
        "GPT2_formal_text": "ed(s, x) = s - x;\n            Casual: Okay, so here's the deal: Imagine you have a piece of text x, like x = x₁, ..., xₙ. The question is, how do we figure out how important that specific part of the text is? Basically, we figure that out by measuring how much attention it gets in a neural network. Formal: To find the attention masking probability P(s|x), we use something called the self-attention mechanism, which was introduced by Bahdanau et al. in 2015. Formal: We also figure out the attention masking probability P(x|s) by looking at how much each token gets in the context of the current token. Formal: The total attention masking probability P(s) is equal to the sum of P(x|s) for all tokens in the source sentence s. Formal: Finally, we use a binary classifier called F(x) to decide if the source token is important or not. Formal: Formal: For our experiments, we set the base decoding speed to 64 and use Adam to optimize things. Formal: The whole process is shown in Figure 1. Formal: The SCLM algorithm, which is part of the Universal Sentence Encoder (USE), works with a bunch of text segments labeled S. SCLM picks the highest-scoring sequence for each segment and uses a graph-based neural network to figure out the attention masking probability. Formal: After that, SCLM uses a multilabel feature extractor to figure out the attention masking probability P(s | x). Formal: When it comes to the attention loss L(s), we set the number of hidden units (L) to 100, which is what's used in the multi-task learning method. Formal: Lastly, we apply the softmax function to get the attention masking probability P(s | x). Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.411.json"
    },
    {
        "casual_text": "Got it! Just so you know, there are some pairs of types where you can't find a set of sources like that. For example, the apply set.",
        "formal_text": "Note that there are pairs of types for which no such set of sources exists; e. g. the apply set",
        "GPT2_formal_text": "\" => [E1, E2], token_ids => [T1, T2], first_token_ids => [1, 2, 3], last_token_ids => [4, 5], begin => [E1, E2], end => [E1, E2], end => [E1, E2], }, }, }, }, }, }, }), } #[should_return = \"true\" => \"true\" => \"true\" => \"true\" => \"true\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\" => \"false\"]] Formal: The collection has data with three main elements: (1) the text content, (2) the ordering of these elements, and (3) some metadata about the text content. Formal: Our goal is to figure out the attributes for these elements using some criteria. Formal: We use a shared bidirectional LSTM (Bi-LSTM) for each of these attributes. For each bidirectional LSTM, the hidden layer spits out the word embedding for each word in the text, which we call w_0. Formal: Each word embedding is created using the word embedding vectors from the LSTM. Formal: We also add a copy layer to the LSTM to make sure we don't lose any important info. Formal: Lastly, we turn this into a knowledge graph. Formal: We set a threshold for how good the attribute values are, so the best ones will be the ones that get the highest score. Formal: After that, we plug these values into the model to get the final answer. Formal: Finally, we use the final answer to estimate the weights for the query. Formal: So, how does the model figure out the answer? Formal: The loss function for the text collection is a function that figures out the probability of different options. Formal: After we've picked the best answer, the model calculates the final objective function to get the final score. Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "Alright, so δ is picked depending on the punctuation in it 7. To compare, we also made two actual sequence groups: the first sentence set and the last sentence set. These include the first and last sentences from the articles in the WikiText validation and testing splits.",
        "formal_text": "where δ is chosen based on the punctuation in it 7. As control groups, we also construct two real sequence sets, first sentence set and last sentence set. They consist of the first sentence and the last sentence of the articles in WikiText validation split and testing split.",
        "GPT2_formal_text": "ed = p_mask + p_mask_t. We keep doing this over and over until we hit the number of features we want. Once we've done that, we just combine everything to get the final representation. Formal: We use the single-layer perceptron (from Hochreiter and Schmidhuber, 1997) to get the final word embedding for the input. Formal: Formal: For each word's representation, we check if it's a stop word, the start of a phrase, or the start of a sentence. If they are, we consider that word the start of that part of the sentence. If not, we consider it the start of the whole sentence. Formal: Formal: We also look at how similar words are to each other. If they're similar enough, they're considered part of the same word's representation. Formal: Formal: The output from the single-layer perceptron can be broken down further into three parts: the main vector for the word (the word_summary), the character vector (char_summary), and the phrase vector (phrase_summary). Formal: Formal: Lastly, we calculate the normalized word embedding using the character embedding vector. Formal: Formal: Lastly, the word embedding vector we get can be fed into a classifier to figure out the overall sentiment for the whole sentence. Formal: Formal: Formal: To get the global sentence sentiment distribution, we do two things: first, we use the word embedding vector to calculate the word_global_sentiment, which is the global sentiment distribution for the whole sentence, and second, we calculate the global sentiment for each character, which is the global sentiment distribution for each character in the sentence. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "blackboxnlp",
        "filename": "2021.blackboxnlp-1.16.json"
    },
    {
        "casual_text": "The objective function is basically about regular language modeling, where we're trying to minimize something called E l. This E l represents the negative log likelihood of the current word in a sequence, based on the words that came before it.",
        "formal_text": "The objective function is an objective of regular language modeling E l, which is the negative loglikelihood of the current word in the sequence given the previous words.",
        "GPT2_formal_text": "`). (3) For each token w i in the input, we can figure out the attention values A i by averaging the attention weights A j that are shared across the tokens in the input. Formal: Alright, let’s break this down in a simpler way. We’re looking at a token from the input, let’s call it x. The input is represented as a matrix T, where each column is a vector e with d dimensions.\n\nHere’s how we calculate the attention weights:\n\n1. First, we randomly pick a token w i and assign its attention weight A i to x.\n2. Then, we set up an attention setup called f = {e i, j } T = {1, . . . , D}, where each token is a vector and the input matrix T has d dimensions.\n3. Finally, we combine all the attention vectors and calculate the attention weight by averaging them. Formal: So, we’re creating a model that focuses on a specific token w i and its related attention weight A i. Formal: Using Equation (1), we calculate the attention weight by averaging the attention weights A j that are shared across the tokens in the input. Formal: The attention weights A i are calculated using Equation (2), which is like combining all the attention vectors. Formal: Finally, we combine all the attention weights and the attention weight by averaging them. Formal: The model looks at the token x and the attention matrix T to figure out the attention weight A i. Formal: Finally, we calculate the attention weight A i by averaging the attention weights A j that are shared across the tokens in the input. Formal: Formal: Finally, we combine all the attention weights and the attention weight by averaging them. Formal:\n\nIn the figure, the attention weights A i are calculated using Equations (2) and (3). The final output is the attention weight, which is a weight between 0 and 1. Formal: The attention weights A i are calculated using Equations (2) and (3). The final output is the attention weight, which is a weight between 0 and 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "coling",
        "filename": "2020.coling-main.43.json"
    },
    {
        "casual_text": "No big deal, but you can easily check that this equation hits its lowest point when XᵀX = I. Basically, that happens when the topics are all lined up nice and neat, like they're standing in a straight line, not overlapping.",
        "formal_text": "It is trivial to verify that this equation has a minimum when X T XX T X = I. This is the case when the topics are orthonormal:",
        "GPT2_formal_text": "ed, which is a fancy way of saying it combines attention with a skipgram mechanism. This method works really well in situations where we don't have a ton of data to work with. The extra bits from the output, like the ones from the hidden layers, help guide the attention in a more efficient way.\n\nWe found that both the model and the user interface can benefit from having a broader look at the input, which is kind of like the way we see the text. When we tested it on a few common text-to-speech (TTS) tasks, the improvement over the previous best model was pretty big. Plus, our model can figure out when words are likely to appear in the text. This helps the user interact more effectively with the TTS system.\n\nFor future projects, we plan to try this out in the biomedical field and also think about making it more flexible and practical by including text-to-speech tools. For instance, it could be possible to include more advanced user models and interfaces that focus on certain words or phrases. Formal: In this paper, we use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We use \"I\" to represent the user, which we noticed could help with understanding TTS better. Formal: We",
        "directory": "D13",
        "filename": "D13-1179.json"
    },
    {
        "casual_text": "In 2019, during the fourth Conference on Machine Translation (WMT), the shared task about filtering parallel corpora was all about dealing with low-resource situations. The top-performing submission used crosslingual sentence embeddings, which were trained using parallel sentence pairs (as mentioned by Chaudhary et al. in 2019). Artetxe and Schwenk (2019a) came up with a similar approach. Both papers addressed the limitations of cosine similarity by looking at the surrounding sentences, which helped them beat systems that only used cosine similarity.",
        "formal_text": "In 2019, at the fourth Conference on Machine Translation, WMT, the shared task on parallel corpora filtering focused on low-resource conditions. The method central to the best-performing submission was the use of crosslingual sentence embeddings, trained from parallel sentence pairs (Chaudhary et al. , 2019). Artetxe and Schwenk (2019a) devised a similar method. Both papers tackle the inconsistencies of cosine similarity by investigating the neighbourhood of a given sentence pair, outperforming systems using only cosine similarity.",
        "GPT2_formal_text": "= self.model.get_field_ids(e) return self.model.get_field_ids(e) def check_uniform(e, f): if e is in {0, 1, 2} and f is in {0, 1, 2, ..., 0, 1, 2}, then return True return False if not self.is_contiguous(e, f) or not self.is_contiguous(e, f): return False return True Formal: This function checks if two statements are equally valid. Formal: Check if the first statement is part of a chain of statements that leads to the second statement. Formal: Grab all the features that are related to the first statement. Formal: Get the final result from the first statement, which is the pair of key/value pairs that are important for the second statement. Formal: Finally, find the formula that connects the first statement to the second one. Formal: Formal: Finally, return the result of this process. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-srw.25.json"
    },
    {
        "casual_text": "We've pinpointed and included time offsets for various elements like speech, subtitles, text on graphics and scenes, body movements, gestures, shots (with a focus on foreground and background), and keyframe regions in the COSMOROE relations. All the visual stuff has been tagged by annotators using simple one or two-word labels that describe actions or entities. These tags came from just watching the video without any audio. The tagging process was based on a cognitive categorization method, which uses the \"basic level theory\" of categorization (Rosch, 1978). Right now, the annotated data includes 5 hours of Greek travel documentaries and 5 hours of English ones. Out of the Greek content, 3 hours have been validated, and we've also done a preliminary study on inter-annotator agreement (Pastra, 2008).",
        "formal_text": "Time offsets of the transcribed speech, subtitles, graphic-text and scene text, body movements, gestures, shots (with foreground and background distinction) and keyframe-regions are identified and included in COSMOROE relations. All visual data have been labelled by the annotators with one or two-word action or entity denoting tags. These labels have resulted from a process of watching only the visual stream of the file. The labelling followed a cognitive categorisation approach, that builds on the \"basic level theory\" of categorisation (Rosch, 1978). Currently, the annotated corpus consists of 5 hours of TV travel documentaries in Greek and 5 hours of TV travel documentaries in English. Three hours of the Greek files have undergone validation and a preliminary inter-annotator agreement study has also been carried out (Pastra, 2008).",
        "GPT2_formal_text": "ing (Shen et al., 2018) uses a Bi-LSTM encoder-decoder setup. First, it encodes the input text using a bidirectional LSTM, then it uses a bidirectional GRU to figure out the attention weights. After that, it generates the output using the GRU. Formal: The info about the input text and the masked word embeddings that the multi-head attention mechanism learned can be encoded and used later for the actual task. Formal: You can think of the multimodal multimodal attention mechanism as a multi-head attention mechanism that uses the multimodal vectors. Formal: The model can take the visual features of the source image and combine them with the masked word embeddings to get a new vector for the masked word, which helps capture the meaning of the source word. Formal: By tweaking the multimodal attention mechanisms, we can adjust the representations of the masked word to show different aspects of the source image. Formal: You can see the visual input, masked word embeddings, and multi-head attention mechanism in Figure 1. Formal: The multi-head attention mechanism can learn and combine different types of features to create the representations for different parts of the source image. Formal: A multimodal multimodal attention mechanism learns multiple types of features for different parts of the source image, which helps it understand the visual context of the source word. Formal: A two-head attention mechanism can learn and combine different types of features to get the representations for different parts of the target image, which helps it understand the visual context of the target word. Formal: A multi-head attention mechanism can learn and combine multiple types of features to learn and combine the representations for different parts of the target image. Formal: For the multi-head attention mechanism, each token has two parts: the vector for the masked word embedding and the vector for the visual features of the masked word. Formal: The representations of the masked word and the source image have different parts that are connected by a dot product between them. Formal: For the multimodal multimodal attention mechanism, the visual features of the source image are represented as the embeddings of the source image and the masked word. Formal: The multi-head attention mechanism can learn and combine different types of features to get the representations for different parts of the target image. Formal: This paper presents a new model for multim",
        "directory": "E09",
        "filename": "E09-2014.json"
    },
    {
        "casual_text": "When it comes to sentences with multiple events, the suggested solution can not only spot and categorize these events but also connect the right arguments to each event, which you can see in Table 3 and Table 4.",
        "formal_text": "As for the characteristic of having multiple events in a sentence, the proposed solution is able to detect and classify the events as well as link arguments to their rightful event, as shown in both Table 3 and Table 4.",
        "GPT2_formal_text": "(r) = {r} ⊆ {r} T (r) 1 = {1} T (r) 2 = {2} (2) Formal: For each pair of sentence (r, r') where a and r are on the same sentence level and where there’s a common nonterminal Σ between them, the similarity between them is calculated based on this formula: Formal: Formal: For each pair of sentence (r, r') where a and r are on the same sentence level and where there’s a common nonterminal Σ between them, the similarity between them is calculated based on this formula: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "econlp",
        "filename": "2021.econlp-1.10.json"
    },
    {
        "casual_text": "We put our framework up against a bunch of other methods to see how it stacks up. (Shi and Lin, 2019) is a well-known approach for semantic role labeling, specifically for predicate-argument prediction. It uses features pulled from BERT (Devlin et al., 2019) and applies Conditional Random Fields (Lafferty et al., 2001) for structured prediction, which we’ll call BERT-CRF. Then there’s Li et al. (2021), who suggest using a conditional neural text generation model for document-level argument extraction. This method deals with each event on its own, so we’ll refer to it as BART-Gen.\n\nFor our part, we’ve got a memory-enhanced training approach that uses extra context we’ve pulled in, and we’re calling that Memory-based Training. We also ran tests with argument pairs and constrained decoding to see how each piece contributes. In Table 3, you’ll find the main results for document-level informative argument extraction. The scores for argument identification are higher than for classification because identifying just requires matching the span offsets. Here’s what we noticed:",
        "formal_text": "We compare our framework to a number of competitive baselines. (Shi and Lin, 2019 ) is a popular baseline for semantic role labeling (predicateargument prediction). It performs sequence labeling based on automatically extracted features from BERT (Devlin et al. , 2019) and uses Conditional Random Fields (Lafferty et al. , 2001) for structured prediction (BERT-CRF). Li et al. (2021) propose to use conditional neural text generation model for the document-level argument extraction problem, it handles each event in isolation (BART-Gen). For our proposed memory-enhanced training with retrieved additional context, we denote it as Memory-based Training. We also present the argument pairs constrained decoding results separately to see both components' contributions. 8 In Table 3, we present the main results for the document-level informative argument extraction. The score for argument identification is strictly higher than argument classification since it only requires span offset match. We observe that:",
        "GPT2_formal_text": "_length creates the output sequence t, and the attention_length of the hidden state h, as calculated by the distillation layer, is just the hidden state itself. Formal: For example, if we have a hidden state size of 200, the attention matrix h will have a dimension of 200. The attention_length of the hidden state h, as calculated by the distillation layer, is just the sum of all the attention weights. Formal: We add an extra loss term to the output sequence to deal with the differences between the different distillation methods. Formal: We also add an extra loss term to the output sequence to make sure the model works well across different distillation methods. Formal: During training, we look at the attention mask, the target sequence t, and the target attention vector h to get the feature vector f, which is the output of the distillation module. Formal: For each target sequence s_t in the target sequence set X, we calculate the attention vector f_t using the target sequence s_t. Formal: The final output vector f is a vector of length d_s, where d_s is the dimension of the output vector. Formal: To check how well the model is doing, we use the cross-entropy loss, which is calculated using Equation 3. Formal: We use a weight decay function to balance how much we care about how well the model performs overall against how much we care about how well the model can learn from a specific sequence. Formal: The model is trained with a learning rate of 0.001 for both training and testing, with a batch size of 32. Formal: If you want to know more about the model, you can check out the original paper. Formal: Formal: For instance, if we have a hidden state size of 200, the attention matrix h will have a dimension of 200. The attention_length of the hidden state h, as calculated by the distillation layer, is just the sum of all the attention weights. Formal: If we have a hidden state size of 200, the attention matrix h will have a dimension of 200. The attention_length of the hidden state h, as calculated by the distillation layer, is just the sum of all the attention weights. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.361.json"
    },
    {
        "casual_text": "Hey, just so you know, in the current version of this machine, delinking basically means adding a sort of \"stop\" signal to the feature you want to delink. This idea was talked about by Kalman and Kornai back in 1985, and Kalman also mentioned it in 1986.",
        "formal_text": "Note that delinking in the present form of the machine consists of adding an inhibitory link leading to the feature to be delinked. Kalman and Kornai (1985) and Kalman (1986).",
        "GPT2_formal_text": "= [t, t_i, ] for (i=1; i<n; i=n-1): for (j=1; j<n-1): for (k=1; k<n-1): for (l=1; l<n-1): if not (l_s or l_b is in self.iter_list) and not (l_t or l_e is in self.iter_list): self.iter_list = {l_s, l_b, l_e} return self.iter_list def p_t_i(self): return self.iter_list[t_i] def p_s_i(self): return self.iter_list[s_i] def p_e_i(self): return self.iter_list[e_i] def p_t_k(self): return self.iter_list[t_k] def p_s_k(self): return self.iter_list[s_k] def p_e_k(self): return self.iter_list[e_k] def δ_t(self): return self.iter_list[t_i] def δ_s(self): return self.iter_list[s_i] def δ_b(self): return self.iter_list[b_i] def δ_e(self): return self.iter_list[e_i] def p_t_i(self): return self.iter_list[t_i] def p_s_i(self): return self.iter_list[s_i] def p_e_i(self): return self.iter_list[e_i] def p_t_k(self): return self.iter_list[t_k] def p_s_k(self): return self.iter_list[s_k] def p_e_k(self): return self.iter_list[e_k] def P(self): return self.iter_list[t_i] def P(s_i): return self.iter_list[s_i] def P(e_i): return self.iter_list[e_i] def P(t_i): return self.iter_list[t_i] def P(s",
        "directory": "E87",
        "filename": "E87-1047.json"
    },
    {
        "casual_text": "We used Fleiss' kappa to see how well the crowd's evaluation matched up with the oracle and expert judgments for each translation task. The kappa scores are broken down like this (based on Landis and Koch, 1977): \n\n- If κ is less than 0: \"none\"  \n- If κ is 0 to 0.2: \"slight\"  \n- If κ is 0.2 to 0.4: \"fair\"  \n- If κ is 0.4 to 0.6: \"moderate\"  \n- If κ is 0.6 to 0.8: \"substantial\"  \n- If κ is 0.8 to 1.0: \"almost perfect\"",
        "formal_text": "Fleiss' kappa correlation coefficient comparing the obtained crowd-based evaluation results to the oracle and expert judgments for each translation task. The κ scores are interpreted in (Landis and Koch, 1977) as follows: κ <0: \"none\" κ ≤0. 6: \"moderate\" κ ≤0. 2: \"slight\" κ ≤0. 8: \"substantial\" κ ≤0. 4: \"fair\" κ ≤1. 0: \"almost perfect\"",
        "GPT2_formal_text": "_tgt_u.d = b. We tried out different hyperparameters, like M (vertical position), A (num of connections), and B (bias for forward hidden states). Turns out, using M (vertical position) and A (num of connections) worked best, while B (bias for forward hidden states) didn't work well. We did an ablation study to figure out why B (bias for forward hidden states) didn't work so well. After that, we decided to go with a linear layer for the translation model. This layer is one of the most popular and reliable linear layers, especially in phrase-based machine translation systems (Mikolov et al., 2013b). Basically, it translates the source word x i into the target word x i+1. Formal: In the early stage of encoding the target word x i+1, the encoder takes the source word x i+1 and turns it into a vector using two hidden layers: one for the source context (enc_c) and another for the target context (enc_t). The decoder takes the encoded target word x i+1 and turns it back into a vector using the same encoder. Formal: Here's a more casual version: Formal: The source sentence x i is turned into a vector using a linear layer. Formal: The decoder translates the target word x i+1 using the linear layer. Formal: Finally, we combine all the encoded tokens x i+1 into a single fixed-length vector. Formal: So, the decoder and decoder model we're proposing here are like the two main components in a phrase-based translation model. Formal: We use the word embedding layer in the word2vec model to get the translation probabilities for the source word x i, and the token representation layer in the token2vec model to get the predicted probabilities for the target word x i. Formal: To make it more practical, we mix the word embedding and token representation layers together in the forward and backward hidden layers. Formal: We use the baseline word2vec model as our starting point for predicting translations. Formal: We also tried out different multilingual models that use different combinations of translation probabilities. Formal: This approach doesn't use the usual linear projection layer for the embedding layer. Formal: We call the multilingual model x i. Formal: To deal with the lack of parallel data,",
        "directory": "eamt",
        "filename": "2012.eamt-1.56.json"
    },
    {
        "casual_text": "Okay, so let's break this down in a simpler way:\n\nbalAPinc(u, v) is basically LIN(u, v) multiplied by APinc(u, v). \n\nNow, APinc(u, v) is calculated by taking r times the sum of P(r) multiplied by rel(f r) for all r in F(u).",
        "formal_text": "balAPinc(u, v) = LIN(u, v) • APinc(u, v) APinc(u, v) = r [P (r) • rel (f r )] |F( − → u )|",
        "GPT2_formal_text": "_lgt_word_1, ..., tuple_lgt_word_n) = {lgt_word_1, ..., lgt_word_n} where lgt_word_i is the i-th word in the sentence. To train the model, we use the validation set as the target dataset. Formal: Basically, the model learns how to generate the words in the text, based on the premise sentence and the main topic of the document, based on the input. Formal: The document, which includes all the parts we talked about earlier, is the most important thing we're focusing on right now. We'll use this as our topic in the future. Formal: We use the main topic to help the model figure out which parts of the text are important. Formal: We use the document to figure out how important each part is. Formal: To make sure the model can generate the words we talked about earlier, we use a special kind of text representation called a vector model. Formal: We start by using a simple linear model to get the summary words. Formal: Once we have the summary words, we plug them into the vector model to get a vector representation for each part. Formal: The next step is to make sure the vector model uses the information from the main topic to generate the words. Formal: After that, we use the vector model to generate a bunch of words based on the input. Formal: Finally, we feed these words into a linear model to get the final word representations. Formal: We'll do the same thing for the other two parts in the final step. Formal: We use a bunch of linear models to predict the topics for the document, using different kinds of text representations to help the model understand the document better. Formal: The summary words are like a way to organize the document, so we put them in the right spot to make the topic vector model work better. Formal: The topics for the documents are kind of like the main topic of the document. Formal: We use a linear model to predict the topics for the documents. Formal: Finally, we use the document's topic vector to generate the words. Formal: Finally, we use the document's topic vector to generate the words. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1268.json"
    },
    {
        "casual_text": "The second class focuses on adding extra info to existing text, like structured knowledge (Zhao et al., 2018; Ghazvininejad et al., 2018; Dinan et al., 2019), personal details (Li et al., 2016b; Zhang et al., 2018a), or emotions (Shen et al., 2017b; Zhou et al., 2018). But getting annotated text like this can be super expensive and is usually only available for specific areas with not much data. Some recent studies have started working on dialogue style transfer using personal speeches or TV scripts (Niu and Bansal, 2018; Gao et al., 2019). The difference with our approach is that we want to make general dialogue generation richer by using lots of non-conversational text, instead of sticking to just one style.",
        "formal_text": "The second class seeks to bring in extra information into existing corpus like structured knowledge (Zhao et al. , 2018; Ghazvininejad et al. , 2018; Dinan et al. , 2019), personal information (Li et al. , 2016b; Zhang et al. , 2018a) or emotions (Shen et al. , 2017b; Zhou et al. , 2018). However, corpus with such annotations can be extremely costly to obtain and is usually limited to a specific domain with small data size. Some recent research started to do dialogue style transfer based on personal speeches or TV scripts (Niu and Bansal, 2018; Gao et al. , 2019; . Our motivation differs from them in that we aim at enriching general dialogue generation with abundant non-conversational text instead of being constrained on one specific type of style.",
        "GPT2_formal_text": "ing pretrained models like BiDAF, ReLU, and Softmax (Zhou et al., 2020a) have been used for machine translation. These models are pre-trained using a sequence-to-sequence approach. Formal: We used the reference translations from the original source text and translated them back to create these pseudo references. Then, we combined these pseudo references with the original translations to train our model. Formal: We used a parallel corpus called D(i, j) to translate each reference into a different language and then averaged the results. Formal: We tested our model using the English Wall Street Journal corpus and translated it back to English using the reference translations. Formal: We used the English Wall Street Journal corpus and translated it back to English using the reference translations. Formal: We didn't apply any translation oracle. Formal: To train the model, we added the English Wall Street Journal corpus to the training set. Formal: The final pseudo references are shown in Fig. 1. Formal: We created these pseudo references by translating the reference translations using our translation system and then averaging the results. Formal: For the sentence-level evaluation, we looked at the translation of the reference sentence that matched the target language. Formal: The correct answers are highlighted in bold. Formal: For the phrase-level evaluation, we used the translation of the reference phrase that matched the target language. Formal: For the sentence-level evaluation, we combined the translation of the reference phrase with the target phrase. Formal: Formal: For the phrase-level evaluation, we used the translation of the reference phrase and the target phrase to train the model. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.634.json"
    },
    {
        "casual_text": "A lot of research on affixoids has been done on Germanic languages like German, Dutch, and Swedish (check out Ascoop and Leuschner, 2006; Booij, 2005; Booij and Hüning, 2014; Norde and Van Goethem, 2014). But we think affixoids aren’t just limited to these languages. They probably show up in other languages that have productive compounding too. Take English, for example—there’s not much systematic research on it, but words like \"quality\" (as in quality press/furniture/diamonds, but not in quality management) and \"nut\" (as in health/math/trivia nut, but not in pecan nut) could be considered English affixoids. \n\nEven for the languages that have been studied more, most of the work focuses on the theoretical side—like whether affixoids should be seen as a separate category from affixes or compounds. There hasn’t been much quantitative research using actual corpus data to, say, figure out how productive different affixoid candidates are or to back up the idea that most affixoid uses have some kind of evaluative meaning. So, our work is trying to fill that gap and bring some real-world data into the theoretical discussion.",
        "formal_text": "Research on affixoids has been centered on Germanic languages like German, Dutch and Swedish (Ascoop and Leuschner, 2006; Booij, 2005; Booij and Hüning, 2014; Norde and Van Goethem, 2014). However, we believe affixoids are not an exclusive feature of these languages. They are likely to arise in other languages with productive compounding. For English, for instance, there is little to no systematic research but arguably quality (as in quality press/furniture/diamonds but not in quality management) and nut (as in health/math/trivia nut but not in pecan nut) can be considered English affixoids. Even for languages on which there is more research, that work is typically focused on the theoretical relevance of assuming a category of affixoids that is distinct from affixes on the one hand and compounds on the other. Very little quantitative work using corpus data has been done to, for instance, study the level of productivity for different affixoid candidates or to substantiate the intuition that most affixoid uses carry evaluative meanings. Our work thus fills an empirical gap in the theoretical discussion.",
        "GPT2_formal_text": "= ('w_i', 'q_i', 'c_i', 'p_i') Formal: The loss function L_l_i is calculated using the Laplacian matrix, which is shown in the figure below. Formal: For the examples from section 2.1, the loss is basically the difference between the performance of the MERT model and the reference model. Formal: The goal here is to make the loss as small as possible. So, the loss can be written like this: Formal: To make sure the loss is small, we add a regularization term, called β. Formal: The loss function, which is what the model learns during training, is the result of applying a linear function, called L_l_i, to the parameters in the objective function. Formal: The loss for an example is basically the difference between the MERT model and the reference model. Formal: Formal: Finally, the final loss function L_l_i is calculated like this: Formal: The loss for an example is defined like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C18",
        "filename": "C18-1325.json"
    },
    {
        "casual_text": "For each model, we tweaked the settings for batch size, learning rate, and dropout. We also tried out different sizes for the embedding, attention, and hidden layers. All the models were trained for up to 100 epochs, but we stopped early if there was no improvement in validation performance after 50 epochs. We used the Adam optimizer (from Kingma and Ba, 2015) to minimize the Cross Entropy Loss. To pick the best model, we looked at the BERTScore F1 (Zhang et al., 2020) on the validation set for the generation task, and for the resolution task, we went with accuracy. In the next part, we'll share the average scores and standard deviations from 5 runs with different random seeds. If you want more info on how we chose hyperparameters, the model setups, or how to reproduce our results, check out Appendix E.",
        "formal_text": "For each model, we performed hyperparameter search for batch size, learning rate, and dropout; also, the search included different dimensions for the embedding, attention, and hidden layers. All models were trained for up to 100 epochs (with a patience of 50 epochs in the case of no improvement to the validation performance) using the Adam optimiser (Kingma and Ba, 2015) to minimise the Cross Entropy Loss with sum reduction. BERTScore F1 (Zhang et al. , 2020) in the validation set was used to select the best model for the generation task, while we used accuracy for the resolution task. In the next section, we report average scores and standard deviations over 5 runs with different random seeds. Further details on hyperparameter selection, model configurations, and reproducibility can be found in Appendix E.",
        "GPT2_formal_text": "ed (Attn-0) tries to make the model focus on the important bits of text and decides how to connect them. We’re using a bidirectional GRU (BiGRU) with a hidden layer size of 128 and an output size of 30. We’ve set the dropout rate to 0.5 and the batch size to 32. We’re using the Adam optimizer (from Kingma and Ba, 2014) with an initial learning rate of 0.001. We’ve set the learning rate to decay linearly with the size of the input sequence. We’ve also experimented with different learning rates like 0.01, 0.02, and 0.03.\n\nIn our setup, we’ve got two epochs per sentence. After each epoch, we use cross-entropy loss to figure out the expected word sequence for each token. Then, we calculate the cross-entropy loss for both the model and the text using equation (6). We tested our model on four datasets, and the results are in Table 3.\n\nFrom Table 3, here’s what we noticed:\n- **First epoch**: First, we train the model using both the training dataset and the test dataset.\n- **Second epoch**: Next, we add some extra data to the test set.\n- **Third epoch**: We apply the dropout rate (0.5) and the batch size (32) as mentioned in equation (3).\n- **Fourth epoch**: We do the same dropout rate and batch size as before.\n\nThis shows that our model performs better than other methods, including BERT, RoBERTa, and XLM-RoBERTa. The improvement is especially noticeable for the first epoch. Table 3 has the best results from all four epochs. The dropout rate for the first epoch is also really important—it’s the highest dropout rate we tested, with a value of 0.5, which is pretty aggressive.\n\nIn this project, we also noticed that the word sequence and the sequence information in the input are really important for figuring out the sequence for the token. The word sequence and the sequence information together make up 90.8% of the sequence information. Table 4 shows the average results for the first epoch (t = 4), the second epoch (t = 5), and the third epoch (t = 6). For the first epoch, BERT",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.353.json"
    },
    {
        "casual_text": "In this paper, we introduce the Unsupervised Self-Training framework and use it for sentiment classification. Our framework does two things at once: it assigns sentiment labels to sentences in a code-mixed dataset without any supervision, and it trains a sentiment classification model completely unsupervised. The framework can also be easily adapted to include active learning. We dive deep into how our unsupervised model works and try to figure out whether it actually understands code-switched languages or just recognizes their patterns. Plus, we share some tips on how to improve the performance of the Unsupervised Self-Training algorithm.",
        "formal_text": "In this paper, we propose the Unsupervised Self-Training framework and apply it to the problem of sentiment classification. Our proposed framework performs two tasks simultaneously -firstly, it gives sentiment labels to sentences of a codemixed dataset in an unsupervised manner, and secondly, it trains a sentiment classification model in a purely unsupervised manner. The framework can be extended to incorporate active learning almost seamlessly. We present a rigorous analysis of the learning dynamics of our unsupervised model and try to answer the question -'Does the unsupervised model understand the code-switched languages or does it just recognize its representations? '. We also show methods for optimizing performance of the Unsupervised Self-Training algorithm.",
        "GPT2_formal_text": "= [u(0), ..., u(n-1)], where u(0) is the first n tokens, and u(n-1) are the next n tokens. The output for the n-th token is the same as the input. Formal: The score for a token is basically how well it matches the input. This is the main goal of our method. To make the attention weights more diverse and better match the input, we use a different approach. Formal: Each token is treated as a feature for the input. Formal: We also add a special token called [u(0)], which is just the first n tokens. The score for this token is calculated based on the input. Formal: We add a special token called [u(n-1)], which is the next n tokens. After that, we just add a softmax layer. Formal: Finally, we add the result to get the final score. Formal: Using these representations, the model can create different representations for the input. Formal: The model learns to create representations for the input based on these representations. Formal: The model also learns to create representations for the input based on these representations. Formal: The model learns to create representations for the input based on these representations. Formal: Formal: The model learns to create representations for the input based on these representations. Formal: Formal: We also add a special token called [u(0)], which is just the first n tokens. The score for this token is calculated based on the input. Formal: Formal: We also add a special token called [u(n-1)], which is the next n tokens. After that, we just add a softmax layer. Formal: Formal: Formal: Finally, we add the result to get the final score. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "calcs",
        "filename": "2021.calcs-1.13.json"
    },
    {
        "casual_text": "Text-to-speech mapping isn’t one-to-one; it’s one-to-many because the same text can sound different depending on pitch, duration, and prosody. This means the mel-spectrograms, which represent the sound, can vary a lot, making their distribution, P(y|x), multimodal. Plus, since mel-spectrograms are continuous, nearby data points are connected to each other. In this part, we’ll first take a closer look at the distribution of P(y|x) in TTS 4 by visualizing it (Section 2.1). Then, we’ll introduce a fresh way to think about the over-smoothing issue in TTS (Section 2.2).",
        "formal_text": "Text-to-speech mapping is a one-to-many mapping since multiple speech sequences can possibly correspond to a text sequence with different pitch, duration and prosody, making the mel-spectrograms distribution P (y|x) multimodal. And due to the continuous nature of the mel-spectrograms, adjacent data points are dependent to each other. In this section, we first empirically characterize the distribution of P (y|x) in TTS 4 through visualization (Section 2. 1), and then provide a novel perspective to study the over-smoothing problem in TTS (Section 2. 2).",
        "GPT2_formal_text": "_vec * vector_len_vec). All the similarity matrices S_t and S_v are in R^d. Formal: You can check out the hyperparameters for the embedding layer for the last hidden state and the first two hidden states in the DenseNet model in Table 4. Formal: We call the connection matrix C_t, the cosine similarity matrix S_t, and the embedding layer embedding layer embedding layer embedding layer_t. Formal: The connection matrix C_t, the cosine similarity matrix S_t, and the embedding layer embedding layer embedding layer_t represent the hidden states and attention vectors of the last hidden state and the first hidden state, respectively. Formal: The embedding layer embedding layer_t is a linear function. Formal: The input length vector l_t is a matrix in R^d, which is the size of the sentence. Formal: The output vector y_t is also a linear function. Formal: The average of the hidden states and attention vectors from the last hidden state h_t is the final attention vector, which we call a_t. Formal: We use the last hidden state h_t and the attention vector y_t to calculate the cross-entropy loss. Formal: The loss is calculated using the cross-entropy of the averaged output vector y_t. Formal: Finally, we set the max-pooling layer's activation function to tanh. Formal: Formal: We also set the early stopping parameter α to 0.4. Formal: The cross-entropy loss is calculated using the cross-entropy of the averaged output vector y_t. Formal: The loss is normalized using the cross-entropy of the average output vector y_t, which we call y_t. Formal: The cross-entropy loss is normalized using the cross-entropy of the average output vector y_t. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.564.json"
    },
    {
        "casual_text": "Alright, let’s break this down in a simpler way. When we compare the results of SACE base and its baseline (which is like a stripped-down version with no extra features), we find that both systems got 5346 cases right in ALL. However, SACE base got an extra 525 cases right that the baseline missed, and the baseline got 339 cases right that SACE base didn’t. So, SACE base did better in some areas.\n\nBut there are also mistakes. Table 6 shows an example where SACE base made a wrong prediction (like guessing \"country\" incorrectly). The WlC (which helps with understanding context) didn’t do a great job here—it didn’t find useful info to figure out the word and even added some unrelated stuff. Table 6 also shows related sentences (#47 and #19) linked to a specific sentence (#10) that was being disambiguated. In this case, \"church\" was wrongly predicted when WlC was turned off. But when WlC was on, it found similar sentences in the same document and used that useful context to help with understanding.\n\nTable 7 gives examples of synsets (groups of words with similar meanings) that the selective attention layer connected. This layer can spot relationships between words that are closely related in meaning or used together a lot. It does this by looking at the highest attention scores (after ignoring connections to itself).",
        "formal_text": "Error Analysis By comparing the disambiguation results of SACE base and its baseline (all factors removed), it is revealed that both systems have correctly disambiguated 5346 instances in ALL while 525 and 339 instances are only correctly disambiguated by SACE base and its baseline respectively. In other words, SACE base has falsely table 6 shows an example (country) that SACE base falsely predicted. It is shown that the WlC does not manage to retrieve valuable information for disambiguating the word while injecting some irrelevant context. Table 6 gives an example of top related sentences (#47 and #19) of a particular sentence (#10) under disambiguation. Here, church is falsely predicted when WlC is disabled. It shows that WlC has detected similar sentences in the same document and incorporated valuable context for context embedding learning. Table 7 provides some examples regarding synsets that are connected by the selective attention layer, indicating its ability of detecting some syntagmatic sense relations and senses of close meaning. The connection is established by using the largest attention score �, ̂� in a batch after filtering self-connection.",
        "GPT2_formal_text": "= (t_i, t_j) for i, j in pairs(t). Formal: We start by looking at how the model performed on the test set for each dataset. After that, we calculate the losses for both the source and target datasets and tweak the model's parameters using the standard cross-entropy loss (check out equation 4). Formal: Alright, so for each target dataset, we calculate two losses: one for the source dataset and another for the target dataset. Specifically, we check out the log loss for the source dataset (θ_S) and the cross-entropy loss (θ_C) for the target dataset (θ_T). Formal: Next, we tweak the model's parameters by minimizing the cross-entropy loss on the test set using the source dataset (θ_S). Formal: This is a bit more complicated than just using the cross-entropy loss, so we'll just focus on the cross-entropy loss here. Formal: To get a better understanding of how the model is adapting, we can calculate the cross-entropy loss for both datasets (θ_S and θ_T) on the test set. Formal: To avoid overfitting, we add a Gaussian term to the cross-entropy loss to make it more robust. Formal: Finally, we tweak the parameter values by making sure the cross-entropy loss is non-negative and that the loss is convergent. Formal: So, in short, we're trying to optimize two cross-entropy losses: one for the source dataset and one for the target dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.406.json"
    },
    {
        "casual_text": "The big question is whether fully automatic machine translation (MT) will be so bad that the \"translation\" ends up being totally useless. Like, if even a trained person looking at the machine's output can't figure out what the original text was about, even roughly. This depends on the two languages being translated and how the machine tries to make the translation complete and unique. Only a lot of experiments will tell us if this approach has any potential. Some early work has been done by a team at the Rand Corporation, but so far, the results aren't super clear or convincing.",
        "formal_text": "Whether or not the inaccuracy involved in fully automatic MT will be so great as to make the \"translation\" completely worthless, as would be the case if a trained interpreter of the machine's output were unable to find out even roughly what the passage in question was about, will depend on the two languages involved and on the specific ways by which the completeness and the uniqueness of the translation is achieved. Only extensive experiments will be able to show whether there is any future along this line. Preliminary studies in this direction have been made by a Rand Corporation group, but the results achieved so far do not seem to be decisive.",
        "GPT2_formal_text": "_num_tasks = { 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25 }, where 1 means we're ignoring the last few tokens, 2 means we're counting the tokens before the last one, 3 means we're counting the tokens after the last one, and 4 means we're both ignoring and counting the tokens after the last one. The log probability is basically the average of these probabilities. Formal: We took the sentences from two of the TAC datasets and broke them down into tokens using a tokenizer. The tokens we used to do this were tokenized by looking at the tokens within the last five characters, then applying a BPE tokenizer. Formal: We set the initial tokens for the TAC baseline to A0, and for TAC-KBP, we set the initial tokens to A1. Formal: The output of the SRL model after applying the SRL model is shown in Figure 4. Formal: The best sequence length for the SRL model we trained is 24. Formal: We've found that the training loss for the TAC baseline is a bit too short to actually make good use of the detailed semantic info in the sentences. Formal: We tweaked the TAC baseline to give more importance to the case-sensitive semantic info, which is pretty important for the systems we're looking at. Formal: The case-sensitive semantic info, like the subject, object, or predicate, is encoded in the SRL model as a sentence vector. Formal: We'll use an alignment toolkit to figure out the best sequence length for this semantic info. Formal: The case-sensitive semantic info, like the subject, object, or predicate, is encoded in the SRL model as a sentence vector. Formal: Formal: We've set the initial count for TAC-KBP to A1, which is a bit longer than the one used for TAC. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "earlymt",
        "filename": "1952.earlymt-1.4.json"
    },
    {
        "casual_text": "Okay, so we're talking about how an RNN works with inputs up to x to generate a response at y. This setup doesn't assume things stay the same over time (that's what \"relaxes stationarity assumptions\" means), and it also shares some patterns or structure across different time points. Now, about the definition of h...",
        "formal_text": "x, y of an RNN over the input up to x for the response at y. This design relaxes stationarity assumptions while also sharing structure across timepoints. The definitions of h in",
        "GPT2_formal_text": "_m * -m), and a_id_t is just the id from a_i. Formal: Using a weight vector that shows how important each word in a document is. This is basically the average of how much the word w_i shows up in the document D_i. Formal: Using a simple function to figure out the sentiment for the whole document. Formal: We look at the sentiment for each word in the whole document. Formal: The overall sentiment of the document, which includes both positive and negative feelings. Formal: You can take this sentiment vector and use it for a bunch of different tasks, like ranking or finding information. Formal: Formal: This step is super important for how well the document retrieval works. Formal: Formal: For the retrieval task, we use the softmax function. Formal: Formal: We keep track of how often a document is retrieved based on the rank of the document and the relevance score. Formal: Formal: The retrieval process gets a little tweak by adding another step. Formal: Formal: Lastly, we use the relevance score to figure out how much the relevance of a document should be considered. Formal: Formal: We also look at how important different factors are for this ranking task. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2021.acl-long.288.json"
    },
    {
        "casual_text": "Not much has been thought about what kind of machine should handle the mechanical part of translation until now. People didn’t really experiment with using computers or similar machines because they hadn’t fully explored all the options yet. But it looks like we’re at a point where we can start experimenting. The big question is whether regular computers or special translation machines would work better in the long term, considering speed, accuracy, and cost. The main tasks the machine would need to do are comparing and identifying things, moving and transferring data, and making selections—both with and without conditions. Math-specific operations aren’t really part of it.",
        "formal_text": "Little thought has been given so far to the problem of the type of machine that should do the mechanical part of a translation; so long as the various alternative possibilities had not been explored to a sufficient degree, experimentation in the uses of computer-like machines was not warranted. It seems, however, that the stage has been reached where experimentation could be started. The main problem is whether general-purpose computers or special translation machines would, in the long run, be more satisfactory with regard to the speed-accuracy cost ratio. The major operations to be carried out by the machine seem to be comparison and identification, shifting and transferring, unconditional and conditional selection, but not specifically arithmetical operations.",
        "GPT2_formal_text": ".py):\n         Casual: So, s_o_i and s_o_j represent the i-th and j-th token, respectively. Formal: The function is basically a mix of two things: one that turns the response into a vector and another that turns it into a real number. Formal: Since it's a cross-attention thing, it needs to figure out how the input representation p affects the output. Formal: But here's the thing: it doesn't really consider the context. Formal: Formal: Formal: Formal: To make it work, we use something called a distillation loss to make the process more manageable. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "earlymt",
        "filename": "1952.earlymt-1.4.json"
    },
    {
        "casual_text": "The text is placed in a simple, essential context and sent through all the machine translation engines. For each chunk, C_i, the possible translated versions, C_1_i to C_N_i, are gathered. Then, the best translation, C_best_i, is picked using a majority vote, a language model trained on a big target language dataset, and a confidence score from each MT engine. Finally, the output sentence, S, is put together by piecing all the best chunks, C_best_i, into their right spots in the target language. You can see a diagram of this whole process in Figure 1.",
        "formal_text": "is embedded in a minimal necessary context and translated by all MT engines. For each chunk C i, the translated output candidates C 1 i − C N i are retrieved and a winner C best i is calculated based on majority voting, a language model trained on a large target language corpus and a confidence score assigned to each MT engine. In a final step, the output sentence S is composed by assembling all C best i (1 < i < M ) in their correct target position. A flow chart representing the entire MEMT architecture can be found in Figure 1.",
        "GPT2_formal_text": "= {h_i} | {q_i} represents the recall at the i-th position in the text, where q_i is the number of words in the q-th sentence. Formal: The key idea here is that we should pick the answer that gives us the smallest sentence length. Formal: For the pair {q_i, h_i} in equation (6), we figure out the correct answer using something called the phrase pair model. The model looks at the current sentence (q_i) and the candidates from the previous sentence (h_i). The model compares the current answer with the candidates from the previous sentence, and if the answer isn’t in the most recent sentence, it switches to the candidate from the previous sentence. Formal: So, to find the smallest sentence, we use the phrase pair model to compare the current answer with the candidates from the previous sentence. Formal: Finally, we pick the answer that has the smallest sentence length. Formal: In this project, we’re mainly looking at the target language, so we’re using the same input and output models as in equation (6). Formal: To help with understanding, we use a kind of fancier way of representing the words, which we call \"semantic vectors.\" Formal: When we’re working on our experiments, we assume we have a database that lists the correct answer for each prompt. Formal: We create this database by using the \"compact pruning\" method from (Sennrich et al., 2008). Formal: For each prompt, we check the answer against the database. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "amta",
        "filename": "2006.amta-papers.13.json"
    },
    {
        "casual_text": "To really understand how languages change over time and across different places, we need to see them go through actual real-world stuff like splitting into new languages (speciation), disappearing (death), and moving around (migration). These are super important for making the simulation feel real before we dive into how specific features of languages change. Let’s break down what these phylogenetic things mean and how we check they’re working right, and then we’ll move on to talking about features.",
        "formal_text": "In order to accurately model language evolution in time and space, it is essential for languages to undergo real-world changes to their phylogeny: namely, speciation, death, and migration. These phylogenetic aspects of the simulation are critical to consider to establish a realistic basis before examining feature dynamics. We first discuss their details and validations, and then delve into features.",
        "GPT2_formal_text": "ing = tf.selector('mask-0'). dropout = Dropout(0). precomputation_mode = ReLU. dilation = Dilation(0). iterative = True. hidden_size = 32. max_iterations = 2. model_size = 256. connection_pooling = (1, 1) + (1, 1) + (1, 1) + (1, 1). discriminator = ReLU. pooling = Pooling(2). subword_relevance = 0. NamedEntity = tf.prop('name') + (tf.set('named_entity_type', 'entity') + (tf.set('named_entity_type_subword', 'subword')). label_based = tf.prop('label') + (tf.set('label_based', 'base_label')). entity_biLSTM = BiLSTM(concatenate(entity_biLSTM, entity_biRNN)). generator = ReLU. dropout = Dropout(0). label_dropout = Dropout(0). generator. append_entities = True. Dropout_masking = tf.selector('mask-0'). dropout = Dropout(0). precomputation_mode = ReLU. dilation = Dilation(0). iterative = True. hidden_size = 32. max_iterations = 2. model_size = 256. connection_pooling = (1, 1) + (1, 1) + (1, 1) + (1, 1). discriminator = ReLU. pooling = Pooling(2). subword_relevance = 0. NamedEntity = tf.prop('name') + (tf.set('named_entity_type', 'entity') + (tf.set('named_entity_type_subword', 'subword')). label_based = tf.prop('label') + (tf.set('label_based', 'base_label')). entity_biLSTM = BiLSTM(concatenate(entity_biLSTM, entity_biRNN)). generator = ReLU. dropout = Dropout(0).\n\nThe parameters are calculated based on the training data and the validation data using the OpenNMT toolkit. Formal: This thing calculates the importance of subword tokens",
        "directory": "coling",
        "filename": "2020.coling-main.68.json"
    },
    {
        "casual_text": "We use this double-grounding idea for sarcasm to make our neural network model better at detecting sarcasm, which helps us answer our first two research questions. We look at the speaker's mood when they make a sarcastic comment by checking their recent tweets for mood clues. We also consider the context by looking at the tweet that the sarcastic comment is responding to. For our third research question, we came up with a new way to get feedback on our annotations by involving the people who wrote the tweets. This helps us improve the model by feeding new examples back into it. Section 3 talks about the features we use in the model, like where they come from. Section 4 explains how we collected and labeled our data. Section 5 describes the neural network model itself, and sections 6 & 7 cover our experiments and how we analyzed the results. Finally, section 8 wraps things up with some final thoughts.",
        "formal_text": "We build on this double-grounding for sarcasm to improve detection in a neural network model of sarcasm and thereby address our first two research questions. We model the speaker at the time of utterance production using mood indicators derived from the most recent prior tweets, and model context using features derived from the proximate cause of the new utterance, the tweet to which an utterance is a response. For our third research question, we present a novel feedback-based annotation scheme that engages authors of training/test tweets in a process of explicit annotation, feeding new examples back into the model. Section 3 outlines the kind and source of features exploited in the model. Section 4 outlines our methods of data collection and annotation. Section 5 presents the neural network model, while section 6 & 7 present our experimental set-up and analysis of results. Finally, section 8 offers some closing remarks.",
        "GPT2_formal_text": "_t, which are like the features you can use for scoring questions. The first part (D1) is the query, and the second part (D2) is the answer. Both parts are usually pretty long and kind of messy. In Section 3.2, we break it down and show how we can fix the queries to make them better by using something called gazetteer expansion. But, there's still a big issue: we have to filter out the unrelated stuff, like names of people or places, because of our fancy model. Formal: For this task, we're looking at how to correctly answer questions based on their related info (like the query) and how well they do on a test set that's missing some important info. Formal: The main thing we're focusing on is figuring out how to answer questions based on related info. This involves using a model that takes into account what people have already said before and can answer questions based on what’s on the test set. Formal: We also have to filter out the irrelevant stuff. Formal: For each question, we need to figure out how much info is missing. We check how much info is missing for each question and then check if the answer includes all the missing info. Formal: To do this, we train a model using question-answer pairs that have the same answer and the same question. Formal: For each question, we look for a few key words, maybe a few sentences, and the answer to get the answer. We use a model to find these words and answer the question. Formal: For each answer, we calculate the chance that it’s actually the answer. Formal: Then, we adjust the probability by multiplying it by the total number of answers in the question. Formal: Finally, we average all these probabilities to get the overall answer. Formal: Finally, we pick the most likely answer to answer the question based on the answer and the question. Formal: This process is repeated for all the questions, giving us a summary of the answer’s probability. Formal: Using this method, we can also estimate the answer’s probability by looking at the answer’s history. Formal: When it comes to answering questions, we use a model to predict the answer and then use that to estimate the missing info. Formal: We also use a model to guess the missing info, but we don’t need a detailed history to figure",
        "directory": "D17",
        "filename": "D17-1050.json"
    },
    {
        "casual_text": "Our tests show that throwing in this extra loss term into the objective function doesn't just boost performance but also helps the model get better faster. We started with a learning rate of η = 0.05 and reset the squared gradient sum every five epochs.",
        "formal_text": "Our experiments show that adding this loss term to the objective function not only increases performance but also speeds up convergence5 We set the initial learning rate η = 0. 05 and reset the squared gradient sum to zero every five epochs.",
        "GPT2_formal_text": "_multi is like a mix of the softmax layer from the teacher model and the attention layer that helps the model figure out the attention weight vector for the output. This layer is used to predict the attention vector for the next token. Formal: In this part, we’ll explain the pre-training methods we used, which helped us get the best results for our task. Formal: To get the hidden representation of the source token x_s for each source word w_s, we first train a bi-directional Gated Recurrent Unit (GRU) model using the target tokens x_t and y_t. We use the embedding dimension of x_s as the source embedding, which we call f(x_s). Formal: Since each source word x_s is an embedding of the target word w_s, we can represent each token x_s as an embedding of the next source word w_s, which we call f(x_s). Formal: With this setup, we can figure out the attention weight vector for the next token x_t by just using the next source word w_s as a guide. Formal: After we’ve trained the GRU model, we compare it to the teacher model to get the final attention weight vector for the token x_t. Formal: Using a Gated Recurrent Unit (GRU) to figure out the attention weight vector for the next token is way faster than using the whole sentence to calculate it. Formal: The GRU model we used was trained using beam search (B) and used the same learning rate as our teacher model, which is η. Formal: Our experiments show that our method works really well, beating the current top model and setting a new record for the multi-head attention on the WMT dataset. Formal: Our results show that our method can be applied to other NLP tasks too. Formal: We’ve also included some example results in Table 2. Formal: If you want to check out the computation details, they’re in the Appendix. Formal: Our method is super efficient, even faster than other methods for the same input. Formal: We also used a separate attention model that was trained separately for the source and target words, which also took about 30-45 minutes per batch. Formal: To keep the number of parameters to a minimum, we pre-trained the GRU model",
        "directory": "D14",
        "filename": "D14-1070.json"
    },
    {
        "casual_text": "Lastly, the decoder from Section 3.2 is used to create hidden states H_a and act tokens based on that.",
        "formal_text": "Finally, the decoder of Section 3. 2 is used to generate hidden states H a and act tokens accordingly.",
        "GPT2_formal_text": ". In this case, we're using these values: {0.9, 0.9, 0.95, 0.95, 0.97, 0.97, 0.98}. Formal: We use two vectors to represent the score: one for the question, q, and the other for the answer, a. The scores for the question and answer are figured out by taking the cosine similarity of each of the vectors and then adding them up. Formal: There are other ways to handle sentiment analysis that look at the relationship between the question and the answer, but we won't go into them here. Formal: To make sure the scores are more meaningful, we suggest using a more complicated approach. Formal: We add something called \"assignment type\" to each model. This means we can use different scores for the same relationship. Formal: We also suggest using a binary cross-entropy function to figure out the probability distribution for each vector. Formal: We train the model using the log-likelihood of the whole dataset. Formal: We use the actual score for the relationship to train the model. Formal: Formal: For the relationship we're interested in, we first identify the aspect category that has the highest score for each pair of the question and answer. Formal: Finally, we create a document-level document-level document-level document-level document-level document-level document-level Formal: We use this method to find the best relationship for any given situation. Formal: The model is trained using a method that uses a beam search that has two parts: one for document-level and one for document-level. Formal: Formal: Formal: To avoid any issues from unclear inputs, we use a method that considers the context. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.638.json"
    },
    {
        "casual_text": "Alright, so let's say we have a type τ, a limit n, and a set A of sources we've already covered. We'll call PossL(τ, A, n) the group of lexical types λ where A is a subset of A(λ, τ), and we can get from λ to τ using APP operations from the sources in A, plus no more than n extra APP operations. In other words, the difference between A(λ, τ) and A is no bigger than n.",
        "formal_text": "Finally, given a type τ, an upper bound n, and a set A of already-covered sources, we let PossL(τ, A, n) be the set of lexical types λ such that A ⊆ A (λ, τ ) and we can reach τ from λ with APP operations for the sources in A and at most n additional APP operations, i. e. |A(λ, τ ) − A| ≤ n.",
        "GPT2_formal_text": "ing) = self.attn( self, ds) self.attn( ds) = self.attn( self, x) self.attn( x) = self.attn( self, y) self.attn( y) = self.attn( self, z) return self.attn( self, ds)\n\nIn our approach, the model uses a BiLSTM model to learn a context representation called a context vector ctx. This vector has a size of ds. For the aspect representation, we use a CNN model that looks at the aspect label z. Since each of these representations is a sentence, the model learns a sentence representation for each of the ds. Formal: The context vector ctx is made up of the words in the ds. Formal: To make the aspect representation, we use a CNN model that looks at the aspect label z. Formal: To make the sentence representation, we use a CNN model that looks at the sentence label y. Formal: The aspect label z is part of the sentence representation. Formal: Finally, the sentence representation we get is a combination of these three parts. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.323.json"
    },
    {
        "casual_text": "Unlike those models, we use some categorical attributes and optimize by minimizing cross-entropy.",
        "formal_text": "Unlike these models, however, we utilize (some) categorical attributes and optimize using cross-entropy.",
        "GPT2_formal_text": "= {s_i, t} is a similarity matrix, and φ_ij is a weight vector. The first row of R_i is the i-th word, and the last row is the j-th word. Formal: The inductive hypothesis P_s and P_t can be written like this: Formal: φ_ij = (s_i, t) * P_s + P_t. Formal: φ_ij can be found using the Adam optimizer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D17",
        "filename": "D17-1099.json"
    },
    {
        "casual_text": "We’re sharing some specifics about how we ran our experiments. All the pre-trained language models (PLMs) we used in our paper are based on HuggingFace Transformers 10. For fine-tuning on the probing task, we ran the experiments with batch sizes of either 8 or 16, α set to 3, 5, or 10, a max sequence length of 128, and a learning rate of 1e-5, training for 10 epochs. We kept the same hyperparameters for each model, even when using different training objectives. For fine-tuning on the sentiment analysis task, we only tweaked the parameters of the multi-layer perceptron (MLP) classifiers that sit on top of the PLM's contextualized representations. For this, we used a learning rate of 2e-5, 3e-5, or 4e-5, a batch size of 32, a max sequence length of 128, and trained for 200 epochs. Oh, and we also included some examples of the experimental setup for checking how important components affect things in Table 8.",
        "formal_text": "We introduce details about the implementation of our experiments. The implementations of all the PLMs in our paper are based on the HuggingFace Transformers 10. During fine-tuning for the probing task, the experiments are run with batch sizes in {8, 16}, α in {3, 5, 10}, a max sequence length of 128, and a learning rate of 1e-5 for 10 epochs. For each model, we use the same hyper-parameters when applying different training objectives. During fine-tuning for the sentiment analysis task, we only update the parameters of the multi-layer perceptron (MLP) classifiers on top of PLM's contextualized representation. We set the learning rate in {2e-5, 3e-5, 4e-5}, batch size of 32, max sequence length of 128 and train for 200 epochs. Additionally, we present examples of the experimental setup for evaluating the influence of important components in Table 8.",
        "GPT2_formal_text": "ing_3D_muls1 = self.attention.masking_3D_muls1 + self.attention.masking_muls1 + self.attention.masking_3D_muls2 + self.attention.masking_3D_muls2 + ... + self.attention.masking_muls3 + self.attention.masking_muls3 + ... + self.attention.masking_muls4 + self.attention.masking_muls4. Formal: We can calculate the attention weights like this: Formal: (Attention_Masking_3D_muls1 + Attention_Masking_3D_muls2 + Attention_Masking_3D_muls3 + Attention_Masking_3D_muls4 + ... + Attention_Masking_Masking_Masking_3D_muls4) Formal: (Attention_Masking_Masking_3D_muls1 + Attention_Masking_3D_muls2 + Attention_Masking_3D_muls3 + Attention_Masking_3D_muls4 + ... + Attention_Masking_Masking_Masking_3D_muls4) Formal: (Attention_Masking_Masking_3D_muls1 + Attention_Masking_3D_muls2 + Attention_Masking_3D_muls3 + Attention_Masking_3D_muls4 + ... + Attention_Masking_Masking_Masking_3D_muls4) Formal: (Attention_Masking_Masking_3D_muls1 + Attention_Masking_3D_muls2 + Attention_Masking_3D_muls3 + Attention_Masking_3D_muls4 + ... + Attention_Masking_Masking_Masking_3D_muls4) Formal: (Attention_Masking_Masking_3D_muls1 + Attention_Masking_3D_muls2 + Attention_Masking_3D_muls3 + Attention_Masking_3D_muls4 + ... + Attention_Masking_Masking_Masking_3D_muls4) Formal: (Att",
        "directory": "acl",
        "filename": "2022.acl-long.543.json"
    },
    {
        "casual_text": "We're working with the French SPMRL dataset (Seddah et al., 2013), which is newer and bigger than the datasets people used for French parsing before (Crabbé and Candito, 2008). This dataset includes the full French Treebank mentioned in (Abeillé et al., 2003) and will probably become the go-to dataset for French parsing in the next few years. We're using it as it is, with two setups: one where we have gold standard tags and another where we use tags predicted by a tagger that's 97.35% accurate (Seddah et al., 2013). The French data has head annotations based on rules from (Arun and Keller, 2005), and compound words are always left-headed.\n\nFor English, we're using the Penn Treebank with the usual split: sections 02-21 for training, section 22 for development, and section 23 for testing. In the predicted scenario, we use the MELT tagger (Denis and Sagot, 2012), which has an accuracy of 97.1%. The head annotations were figured out by matching the phrase structure treebank with its dependency conversion, as described by (de Marneffe et al., 2006).\n\nWe're running the experiments using a C++ implementation of the algorithm we talked about. The scores for the Berkeley parser (Petrov et al., 2006) are based on the runs mentioned by (Seddah et al., 2013). We measure the F-score using the classic evalb, and we measure the times on the same machine (MacOSX 2.4Ghz). Note that we don't include input/output times for both parsers.",
        "formal_text": "The experiments use the French SPMRL dataset (Seddah et al. , 2013) which is newer and larger than datasets previously used for parsing French (Crabbé and Candito, 2008). It instanciates the full French Treebank described in (Abeillé et al. , 2003) and will surely become the new standard data set for parsing French in the next few years. We use this data set as is, with two scenarios: one with gold standard tags and the second with tags predicted by a 97. 35% accurate tagger (Seddah et al. , 2013). The French data is head annotated with head rules provided by (Arun and Keller, 2005). Additionally, compound word structures are systematically left headed. For English, we use the Penn Treebank with standard split: section 02-21 for training, section 22 for development and section 23 for test. The predicted scenario uses the MELT tagger (Denis and Sagot, 2012) with an accurracy of 97. 1%. The head annotations have been inferred by aligning the phrase structure treebank with its dependency conversion described by (de Marneffe et al. , 2006). We use a C++ implementation of the algorithm described above for running the experiments. Scores reported for the Berkeley parser (Petrov et al. , 2006) use the runs described by (Seddah et al. , 2013). F-score is measured with the classical evalb and times are measured on the same machine (MacOSX 2. 4Ghz) and do not take into account input/output times for both parsers.",
        "GPT2_formal_text": "_t = auth_ids_t + pool_h( * ) * c. This is all the info from the user's past interactions with the system, like their ID and the current one. Formal: If the user doesn't want any more history records added, they can just delete the oldest one. Formal: Got it! If there are any more history records, you can just remove the oldest one. Formal: A few of these records might be about their friends. So, you can also update the current one if they're available. Formal: There are three types of actions: starting, moving, or deleting. Formal: The user has a bunch of active/inactive friends. You can either make a new friend by adding a new record or pick a favorite one by selecting one of the existing records. Formal: You can also browse the friends in the user's history. Formal: To keep things organized, you can pick the history record that most fits the current situation or one that fits the most of the people involved. Formal: To match the shape of the storyboard, the active/inactive friends might also have their own storyboards. Formal: You can edit the friends' storyboards by choosing a new friend. Formal: The conversation history is made up of sequences of actions, like (a1, ..., aJ), and it's tied to the storyboard. Formal: Let's dive into the process of creating storyboards from these storyboards. Formal: Using a storyboard (marked by a story_board_t), we can create stories from each story_board_t. Formal: Next, we combine these stories into a sequence of stories (called the story_board_g). This process is repeated for each story_board_t. Formal: Finally, we also create a storyboard for each active/inactive friend. Formal: Here's a sample of how to create a storyboard for a friend: Formal: We start by grabbing the storyboard for the current friend and decide on the type of story it should be based on the type of storyboard for that current friend. Formal: The storyboard for the current friend is made up of sequences of actions, like (a1, ..., aN), and is tied to the storyboard. Formal: The storyboard for the current friend is also made up of sequences of actions, and is tied to the storyboard. Form",
        "directory": "C14",
        "filename": "C14-1052.json"
    },
    {
        "casual_text": "For our experiment, we're starting with a standard set of features commonly used for AKE. Basically, when we have a potential keyphrase, we look at:",
        "formal_text": "The baseline feature set for our experiment is a set of well-accepted features for AKE, i. e. , given a candidate KP, we consider:",
        "GPT2_formal_text": "_length = 2e-5 + (1 - e^(-1)) # F_mask = max(1 - e^(-1)) + e^(-1) + (1 - exp(1 - e^(-1)) + 1 - e^(-1)) # softmax(e^(-1)) #\n           Casual: In this part, we’ll explain how to pick the right candidates for the event mention f_mask and the related argument span, e_att. First, we’ll talk about the event mention f_mask, which we’ll call f_m. Then, we’ll go over the argument span e_att, which we’ll call e_p.\n\nFor the event mention f_m, we use something called an event embedding (shoutout to Lample et al., 2016). This embedding is created using a data model called MUSE (Bojanowski et al., 2018). The embedding is a kind of vector that shows how events in a text are related to each other. We’ll call this event embedding f_m. After that, we use a simple cross-entropy loss function to figure out the probability of the event f_m happening based on the related argument span e_p. Formal: For each event mention f_m, we add a little extra term to the embedding: Formal: The word embedding is a way to represent a word based on its context, and the entity embedding is a way to represent a specific entity. Formal: The two embedding methods we’re using, MUSE and MUSE-E, are based on the work by Mikolov et al. (2013b). Formal: To figure out the probability of event f_m happening, we use the formula we just mentioned. Formal: We’ll also talk about the loss function we use, which is just a non-linear function. Formal: The term we use here is called the singular value decomposition (SVD). Formal: For the softmax function, we’ll use the formula L(•, •). Formal: Finally, we’ll mention the loss term we’re working with, which is the one that we get by running a softmax function on the embedding vectors f_m and e_p. Form",
        "directory": "C16",
        "filename": "C16-1077.json"
    },
    {
        "casual_text": "Alright, so like how BERT does it, we start by adding a [cls] token at the beginning and a [sep] token at the end of the source sequence. After that, we throw in a bunch of [pad] tokens at the end to make the whole thing reach the maximum length we set, like 256, for example. This way, we ensure the source is at least as long as the output. But, for tasks like text summarization where we know the source is longer than the target, we skip adding those [pad] tokens when creating the input.",
        "formal_text": "Input Representation Following the setup of BERT, we first append a [cls] and a [sep] token on both sides of the source sequence. Then we attach a number of [pad] tokens at the end of source sequence to make its length equal to the predefined maximum size (e. g. , 256). Thus we can make sure the source length is longer than or equal to the output length. As a special case, for tasks like text summarization where the source is known to be longer than the target, we do not attach the [pad] tokens when constructing the input.",
        "GPT2_formal_text": ") = {l_i} | l_j = 1, . . . , l_k = l_l = {l_1} + {l_2} + ..., l_k = l_l = {l_1} | l_j = 1, . . . , l_k = l_l = {l_1} + {l_2} + ..., l_k = l_l = {l_1} + {l_2}. The whole setup for a neural network language model, as shown in Equation (1), is built on these two rules: Formal: First, the sentence is reduced to a series of smaller sentences, called pieces. Formal: Second, each of these pieces is turned into a vector, which is just a fancy way of saying it's a way to store the meaning. Formal: Third, we put all these vectors together in a neat, compact way to represent the whole sentence, which we call the representation. Formal: Lastly, we add a special number, called the context vector, to help us understand how the sentence is structured. Formal: The whole process is laid out in Algorithm 1, and in the end, we get a sentence representation that matches the input sentence exactly. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eacl",
        "filename": "2021.eacl-main.18.json"
    },
    {
        "casual_text": "We think that sentences in a related work section are created through two different processes: one that provides general background info and another that talks about specific contributions from authors. A big insight we had is that these processes can be neatly organized into a topic tree: general content goes in the nodes inside the tree, while the leaf nodes hold the specific details. In our method, these two processes work separately and then come together to build the final summary.",
        "formal_text": "We posit that sentences within a related work section come about by means of two separate processes -a process that gives general background information and another that describes specific author contributions. A key realization in our work is that these processes are easily mapped to the topic tree topologically: general content is described in tree-internal nodes, whereas leaf nodes contribute detailed specifics. In our approach, these two processes are independent, and combined to construct the final summary.",
        "GPT2_formal_text": "ing_functions.py\" 2. Formal: Okay, so here's how this paper is laid out. Section 2 talks about related work. Section 3 covers the main results. Section 4 covers the methodology. And then there's the conclusion. Formal: We're using the IRTF to figure out the attention weights for each word. The training data we use is in a WordPiece format. The text token embeddings are generated using a FastText embedding model. The hidden units in this model are learned and tested using supervised learning. Formal: We also created a set of tools to help users with their experiments. Formal: We’ve tweaked the hyperparameters for the training data to improve the performance. Formal: We’ve also created a method to evaluate how well the data quality matches up with the human judgments. Formal: We’re showing the results of our experiments for different language pairs, which is super helpful for finding out which language pairs really need more work. Formal: Formal: Lastly, we’ve included the results of our experiments on the CoNLL-2012 and 2013 datasets. Formal: We’ve also included results of our experiments on the CoNLL-2012 and 2013 datasets. Formal: We’ve also included the results of our experiments on the CoNLL-2012 and 2013 datasets. Formal: Formal: Lastly, we’ve included the results of our experiments on the CoNLL-2012 and 2013 datasets. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C10",
        "filename": "C10-2049.json"
    },
    {
        "casual_text": "So, the total cost for dividing the text into K segments is made up of the costs for each segment. The cost for each segment is a combination of two things, adjusted by a parameter called 7:\n\n1. **Length Information**: This is how far the segment's length is from the average segment length. Think of it like the mean (bt) and standard deviation (a) of segment lengths, which can be figured out from the text.\n\n2. **Similarity Between Sentences**: This measures how similar the sentences in a segment are to each other. The more words they share, the more similar they are. The numerator here is the total number of shared words in the segment.\n\nIf the parameter r is 2, then this similarity measure is like looking at the \"density\" of the segment—how much \"information\" (shared words) is packed into it. When r is 2, it’s called \"generalized density,\" which helps control how much the size of the segment affects its importance. High similarity within a segment (lots of shared words) means a higher value for this part.",
        "formal_text": "Hence the sum of the costs of the K segments constitutes the total segmentation cost; the cost of each segment is the sum of the following two terms (with their relative importance weighted by the parameter 7): I. The term (tk tk-? -/1)2 2. 0-2 corresponds to the length information measured as the deviation from the average segment length. In this sense, bt and a can be considered as the mean and standard deviation of segment length measured either on the basis of words or on the basis of sentences appearing in the document's segments and can be estimated from training data. E: k =t k i+iEtt = k ek 1+1D 8, t 2. The term (tk -tk-corresponds to (word) similarity between sentences. The numerator of this term is the total number of ones in the D submatrix corresponding to the k-th segment. In the case where the parameter r is equal to 2, (tk -tk_ O r correspond to the area of submatrix and the above fraction corresponds to \"segment density\". A \"generalized density\" is obtained when r 2 and enables us to control the degree of influence of the surface with regard to the \"information\" (i. e the number of ones) included in it. Strong intra-segment similarity (as measured by the number of words which are common between sentences belonging to the segment) is indicated by large values of",
        "GPT2_formal_text": "[ ] = [ ]         Casual: We're suggesting a new neural network setup that uses different types of convolutional layers to create features for the relation extractor. Formal: We're introducing a new neural network called the neural relation extractor. It starts by treating each entity mention as a sequence of embeddings, then it creates embeddings for the relation. Formal: We're proposing a neural network called the neural relation extractor. Formal: Here's how it works: \n\n1. First, it uses a recurrent neural network (RNN) to create a sequence of embeddings for each mention.\n2. Then, it adds two types of convolutional layers: one for embedding the mention's embedding (called self-attention) and another for its relation embedding (called relation-attention). Formal: We use a three-layer recurrent neural network to encode each mention's embedding (self-attention) and its relation embedding (relation-attention). The result is a sequence of embeddings for the mention's embedding, which we call the mention's embedding vector. Formal: The two-layer RNN also has two other layers: one for embedding the mention's embedding (called self-attention) and another for embedding the relation's embedding (relation-attention). Formal: The last layer adds two types of convolutional layers: one for embedding the mention's embedding (self-attention) and another for embedding the relation's embedding (relation-attention). The result is a sequence of embeddings for the mention's embedding, which we call the mention's embedding vector. Formal: For the multi-layer RNN, we also add a third layer for embedding the mention's embedding, which is called self-attention. Formal: Formal: Finally, the whole multi-layer RNN is used to encode the mention's embedding vector, which we call the mention's embedding vector. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E03",
        "filename": "E03-1058.json"
    },
    {
        "casual_text": "All the Cascade models scored better than the Isolated model in terms of F-score. The highest F-score for the Cascade model was 67.29%, which we got by using 18 features in experiment (iv). This shows that POS information really helps with sentence boundary detection.",
        "formal_text": "All Cascade models have higher F-score than the Isolated model. The best F-score of the Cascade model is 67. 29% when we used 18 features in the experiment (iv). The experiment affirms that POS information is helpful in sentence boundary detection.",
        "GPT2_formal_text": "ed_sentence_attributes = [attributes_masked_sentence_attributes] for each sentence in the sentence. The context from earlier sentences is also included in this input, but it doesn't make much of a difference compared to the input sentence. Formal: The question is what kind of information can be gathered about this task, how it's defined, and how we can use that information to make a final answer. Formal: We use the \"inferencer\" thing to create a list of possible answers from the input sentences, like a list with n items. This list includes all the possible answers for each sentence in the sentence. Formal: We also use an \"answer_selector\" to pick out the answer options. Formal: For each answer, we grab some of the possible answers from the input sentences and put them into the \"answer_selector.\" Then, we use a greedy algorithm to pick the best answer for each sentence. Formal: Since the answers are ordered by importance, we think of each answer as a whole, not separately. Formal: Using all the answers from the input sentences, we create a big, complete list with n items. Formal: Using all the answers from the input sentences, we get a list with n items, where each item is a sentence from the sentence. Formal: For each answer, we use a greedy algorithm to pick the best answer for each sentence. Formal: In this paper, the example sentences are shown as triangles in Figure 1. Formal: To figure out the answer for each sentence in the example, we look for the answer that's closest to the answer for the sentence. Formal: Finally, we put the answer closest to the answer for the sentence in the answer_selector and the answer for the answer_selector together. Formal: Formal: Figure 1 shows the annotated list of possible answers, with each triangle representing a sentence. Formal: The input sentence is the first sentence in the example sentence, and the answer sentences are the second, third, and fourth sentences. Formal: We start with an empty list of possible answers. Formal: After going through the whole training data, we find the answer that's closest to the answer for the last sentence, the last sentence in the example sentence, and the answer for the answer_selector. Formal: We use a greedy algorithm to pick the best answer for each sentence. Formal: To get the answer",
        "directory": "C16",
        "filename": "C16-1031.json"
    },
    {
        "casual_text": "We’ve come up with a method to figure out which entities in a text are important (focus) and which are just background info (ground), using something called class association rules. We’re specifically looking at how this can help identify pathogens that are the main focus of a study. Focus pathogens usually show up in the title and results sections of abstracts, where the main findings are highlighted. Our little experiment suggests that paying attention to how things are written in the text can give us useful hints to spot these focus pathogens. Since we only had a small amount of data to work with, this is more of a proof of concept. We’re working on gathering a bigger dataset, which will let us test more detailed rules and explore other methods, like centrality and transformer-based approaches.",
        "formal_text": "We have proposed an approach to the problem of detecting focus versus ground entities using class association rules over entity mentions in discourse segments, specifically examining its use for pathogen characterisation. Focus pathogens tend to appear in the title and results segments of abstracts, where the key findings of research are highlighted. Our case study suggests that discourse information provides valuable cues to identify focus pathogens. Given the small-scale data we have available, this work is only indicative of the promise of the approach. We are developing a larger data set, which will support comprehensive exploration of more refined rules. This data set would also support the exploration of additional existing methods, such as centrality and transformer based methods.",
        "GPT2_formal_text": "= [maj, c1, c2, ..., cmaj] x = [x1, x2, ..., xmaj] Here, x1, x2, ..., xmaj are some of the attention scores. Formal: For the classification task, we're looking at the input x and the representations h_i for each aspect. Each aspect is represented as a bunch of vectors: h_i, r_i, w_i, h_1, ..., h_L, r_1, ..., r_L. Formal: Here, n_i and m_i are the number of aspect-related tokens and their corresponding representation vectors, respectively. Formal: Next, we calculate the cross-entropy loss between the aspect-specific and aspect-general representations using the model's parameters (Equation 1). Formal: For the classification task, we're using the cross-entropy loss from Equation 1. Formal: After that, we feed the input into the classifier to figure out the aspect label for each query. Formal: After that, we use the aspect-specific and general representations to make predictions for the aspects. Formal: Finally, the prediction loss is calculated to show how well the aspect-specific and general representations are performing. Formal: After training the classifier for a set number of epochs (like 5 or 10), we calculate the average classification accuracy for each query. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "alta",
        "filename": "2021.alta-1.19.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nGen(h x, z, 0, max, λ, t):  \n1. Start by setting ←− 0 and r ←− 0.  \n2. Calculate the gradient: g ←− ∇ hx L (h x, z, C (h x ))  \n3. Keep going until ≤ max:",
        "formal_text": "Gen(h x, z, 0, max, λ, t): 1: Initialization: ←− 0, r ←− 0. 2: Compute gradient: g ←− ∇ hx L (h x, z, C (h x )) 3: while ≤ max do 4:",
        "GPT2_formal_text": "ing      Casual: Okay, so let's break this down in simpler terms. We're talking about how to get rid of the thing called attention_masking. It's not just a single thing; it's a group of things.\n\nFirst, we calculate something called P_n(X) for all the stuff we're focusing on, and this is what we use as our mask. Then, we calculate something called C_n(X) for the masked part of the question X. And we do this for all the words in the question.\n\nSo, in short, we're dealing with a bunch of variables here, which we're using to get rid of something. Formal:\n\n- P_n(X) = P_n(X) * C_n(X) * P_n(X)\n- C_n(X) = C_n(X) * P_n(X)\n\nSo, in short, we're pulling out the mask for words in the question. Formal:\n\n- P_n(X) = P_n(X) * C_n(X) * P_n(X)\n- C_n(X) = C_n(X) * P_n(X)\n\nSo, in short, we're masking words in the question. Formal:\n\n- P_n(X) = P_n(X) * C_n(X) * P_n(X)\n- C_n(X) = C_n(X) * P_n(X)\n\nSo, in short, we're masking words in the question. Formal:\n\n- P_n(X) = P_n(X) * C_n(X) * P_n(X)\n- C_n(X) = C_n(X) * P_n(X)\n\nSo, in short, we're masking words in the question. Formal:\n\n- P_n(X) = P_n(X) * C_n(X) * P_n(X)\n- C_n(X) = C_n(X) * P_n(X)\n\nSo, in short, we're masking words in the question. Formal:\n\n- P_n",
        "directory": "coling",
        "filename": "2020.coling-main.200.json"
    },
    {
        "casual_text": "For our experiments, we used the Wikipedia Edit Category Corpus (WPEC), which is freely available and was put together in earlier research (Daxenberger and Gurevych, 2012). This dataset breaks down each pair of adjacent revisions into smaller edits. This helps give a clearer view of how editing happens, since an author might make multiple separate edits in a single revision. Plus, each edit is tagged with one or more categories, which is super useful for pinpointing major changes, like when a whole new paragraph with text, references, and markup is added.\n\nThere are four main types of edits: Insertions, Deletions, Modifications, and Relocations. These are figured out by doing a line-by-line comparison of the source text (including wiki markup). Following the approach suggested earlier (Daxenberger and Gurevych, 2012), only the specific part of the text that actually changes within a modified line is marked as an edit (either an Insertion, Deletion, or Modification), not the whole line.\n\nWe also pulled in some extra data that wasn’t included in WPEC, like metadata and the plain text from the revisions (rv−1 and rv). We did this using the Java Wikipedia Library (JWPL) along with the Revision Toolkit (Ferschke et al., 2011).",
        "formal_text": "For our experiments, we used the freely available Wikipedia Edit Category Corpus (WPEC) compiled in previous work (Daxenberger and Gurevych, 2012). In this corpus, each pair of adjacent revisions is segmented into one or more edits. This enables an accurate picture of the editing process, as an au-thor may perform several independent edits in the same revision. Furthermore, edits are multi-labeled, i. e. each edit is assigned one or more categories. This is important for a precise description of major edits, e. g. when an entire new paragraph including text, references and markup is added. There are four basic types of edits, namely Insertions, Deletions, Modifications and Relocations. These are calculated via a line-based diff comparison on the source text (including wiki markup). As previously suggested (Daxenberger and Gurevych, 2012), inside modified lines, only the span of text which has actually been changed is marked as edit (either Insertion, Deletion or Modification), not the entire line. We extracted the data which is not contained in WPEC (meta data and plain text of r v−1 and r v ) using the Java Wikipedia Library (JWPL) with the Revision Toolkit (Ferschke et al. , 2011).",
        "GPT2_formal_text": "\" ) = 1. The link to the survey is at this link: http://ssrn.com/win/documents/win-wmt2016/win-wmt2016-use-conditional-induction.pdf Formal: The survey is only open for the first 10 people who pick up a code. This survey is done by a single grad student from 2018. We paid her $30, and she had to give us a sample of 30 questions that fall into the 90th percentile or below, plus a special \"Yes/No\" question about whether they agreed or disagreed. Formal: The survey is open for anyone to take. It’s a great way to see if you’re into some of the tech, product, or business stuff we talked about in the paper. Formal: The survey is open for anyone to take. It’s a great way to see if you’re into some of the tech, product, or business stuff we talked about in the paper. Formal: The survey is open for anyone to take. It’s a great way to see if you’re into some of the tech, product, or business stuff we talked about in the paper. Formal: The survey is open for anyone to take. It’s a great way to see if you’re into some of the tech, product, or business stuff we talked about in the paper. Formal: The survey is open for anyone to take. It’s a great way to see if you’re into some of the tech, product, or business stuff we talked about in the paper. Formal: The survey is open for anyone to take. It’s a great way to see if you’re into some of the tech, product, or business stuff we talked about in the paper. Formal: The survey is open for anyone to take. It’s a great way to see if you’re into some of the tech, product, or business stuff we talked about in the paper. Formal: The survey is open for anyone to take. It’s a great way to see if you’re into some of the tech, product, or business stuff we talked about in the paper. Formal: The survey is open for anyone to take. It’s a great way to see if you’re into some of the tech, product, or business stuff we",
        "directory": "D13",
        "filename": "D13-1055.json"
    },
    {
        "casual_text": "For Sentiment, we just pick random Polyjuice counterfactuals without worrying about their labels, as long as there’s at least one that changes the label of the original x. For NLI and QQP, we noticed in a small test that randomly picking counterfactuals might not be any better than just adding more regular data. We think Polyjuice doesn’t really understand the context or domain knowledge to make important changes, so it doesn’t add much beyond what pretraining already does (thanks, Longpre et al., 2020). So, instead, we use slicing functions to spot interesting patterns (like prepositions in NLI) and mess with those by putting [BLANK]s where the patterns are. For instance, \"His surfboard is beneath him\" turns into \"His surfboard is [BLANK] him\", and Polyjuice might then come up with stuff like \"His surfboard is beneath ) next to him.\"",
        "formal_text": "For Sentiment, we select random Polyjuice counterfactuals regardless of their labels, as long as an original x has at least onex that flips the label. For NLI and QQP, we observed in a pilot study that randomly chosen counterfactuals may not be more effective than the same amount of additional data. We suspect that Polyjuice lacks domain knowledge and context for identifying critical perturbations, and therefore brings benefits redundant with pretraining (Longpre et al. , 2020). Thus, we use the slicing functions of to find patterns of interest (e. g. , prepositions in NLI), and perturb those patterns by placing [BLANK]s on the matched spans. For example, \"His surfboard is beneath him\" becomes \"His surfboard is [BLANK] him\", and Polyjuice generates counterfactuals such as \"His surfboard is beneath ) next to him. \"",
        "GPT2_formal_text": "layer. We're going with the [CLS] token here, just like the original models. Formal: L-Coarse-1 is a version of L-Coarse that only uses one input word at a time, while L-Coarse-2 is the other way around. Formal: L-Granular-1 is a version of L-Granular that combines different inputs, like GRU and max-pooling. L-Granular-2 takes it up a notch by adding the LSTM part. Formal: L-Granular-2 is the same as L-Granular, but it adds the attention mask layer to handle the long-range dependencies. Formal: L-Granular-max is similar to L-Granular-1, but it's a bit fancier by adding three layers of max-pooling. Formal: L-Granular-max+vec-mult is the most advanced version of L-Granular, adding a max pooling layer, a weighted sum of LSTMs, and the attention mask layer. Formal: For any word w_t and its labels X_t and Y_t, we calculate the cosine similarity between their embedding embeddings h_t and h_t, and the logits for the w_t and y_t vectors. Formal: We set a window size of 10 and set the number of hidden units in the LSTM to 100. Formal: We apply a max pooling layer with a window size of 10. Formal: We set a window size of 10 and set the number of hidden units in the LSTM to 100. Formal: We apply a max pooling layer with a window size of 10. Formal: We set a window size of 10 and set the number of hidden units in the LSTM to 100. Formal: Formal: Formal: We apply a max pooling layer with a window size of 10. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "To make sure our method works well, we also checked how it stacks up against some other methods that don't need labeled data to work on sentences.",
        "formal_text": "To further validate the performance of our method, we also compared our method with the following unsupervised sentence-level baseline methods:",
        "GPT2_formal_text": "= {ll} = {ll - 1, ll - 2, ..., ll - n | t| = 1.2.3} n) A + b) = {l(•), •, • | •, • | •, • | • | •, • | •, • | •, • | •, • | •, •, • | •, •, • | •, • | •, • | •, • | •, • | •, • } Formal: Okay, so let's say L(t) is the longest possible string of tokens that could be generated from t. Formal: If A + b doesn't have any extra tokens, then A + b = 1.0. But if A + b has more tokens than b, then A + b = 1.5. Formal: For an A + b that’s longer than L(t), the distribution of the first, second, and so on, from the start of the original sequence is defined like this: Formal: The best possible value of A is found by using a maximum entropy approach with a Dirichlet distribution. Formal: In this case, A is always less than or equal to t. Formal: The initial state for an A + b is also found by using a Dirichlet distribution. Formal: If A + b is part of a set {l(•), •, •, • | •, • | •, • | •, • | •, • | •, • | • | •, • | •, • | •, • }, then the best possible value of A is also part of this set. Formal: Formal: Formal: And if A + b is part of a set {l(•), •, •, •, •, •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | •, • | • | •, • | } Formal: Here, l(•) is the length of the string being generated, and l(•) is the maximum length allowed",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.267.json"
    },
    {
        "casual_text": "Our approach is based on how sense subjectivity and contextual subjectivity are connected and includes two main rules, R1 and R2.",
        "formal_text": "Our strategy is defined by the relation between sense subjectivity and contextual subjectivity and involves two rules, R1 and R2.",
        "GPT2_formal_text": "_layers (or attention_mask_layers for short). The input layer for the output layer is basically a combination of the input layer and the output layer's hidden states, which we call h_out. Similarly, the inputs to the output layer are h_e and h_o. Formal: The attention weights are the attention weights (α_t) for the input layer and the attention weights (β_t) for the output layer. Formal: The attention weights can be calculated using Equations 6, 7, and 8. Formal: If the attention weights (α_t) are positive, the model focuses on the most important stuff from the input layer. If they're negative, it looks at the most important stuff from the output layer. Formal: For the HVAE task, the model gives the most important info to the input layer and the least to the output layer. Formal: The attention weights α_t for the input layer and the attention weights (β_t) for the output layer are calculated using Equations 3, 4, and 5. Formal: The attention weights can be calculated using Equations 2, 3, and 4. Formal: If the attention weights (α_t) are positive, the model tries to get the most important info from the input layer. If they're negative, it focuses on the least important info from the output layer. Formal: Formal: If the attention weights (β_t) are positive, the model focuses on the most important info from the input layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D09",
        "filename": "D09-1020.json"
    },
    {
        "casual_text": "Since this paper is all about feature development, we’re just using a clustering method that helps us see how good our similarity matrix is for clustering. We picked agglomerative clustering with single linkage for this. Now, since each name might need its own settings, we wanted to check how much those settings matter for clustering. So, in our experiments, we tried two different approaches for the stop-threshold in agglomerative clustering: first, we tested every possible stop-threshold to find the best one for any ambiguous name and each feature model. The one that worked the best became our optimal stop-threshold. Second, we used a fixed stop-threshold that we got from our development data.",
        "formal_text": "Since the focus of this paper is only on feature development, we simply employ a clustering method that can reflect the quality of the similarity matrix for clustering. Here, we choose agglomerative clustering with a single linkage. Since each personal name may need a different parameter setting, to test the importance of the parameter setting for clustering, we use two kinds of stopthresholds for agglomerative clustering in our experiments: first, to find the optimal stop-threshold for any ambiguous personal name and for each feature model, we run agglomerative clustering with all possible stop-thresholds, and choose the one that has the best performance as the optimal stop-threshold; second, we use a fixed stopthreshold acquired from development data.",
        "GPT2_formal_text": ", where the id is 1. Formal: So, we start by setting up the knowledge base with the user's knowledge base by using a lookup table. Then, we use a linear programming method to figure out how much each token in the user's knowledge base contributes to the prediction. Finally, we use a softmax classifier to predict the user's knowledge base. Formal: When we're training the knowledge base, we grab the relevant examples for each user's knowledge base from the data. This process gives us a bunch of samples for every user's knowledge base, and we use those to train a classifier that predicts the user's knowledge base. Formal: After that, we clean up the results by getting rid of any examples that don't match the user's knowledge base. Formal: We run the training process 1000 times, and we pick the best model using cross-validation. Formal: Lastly, we tweak the classifier to predict the user's knowledge base by using the combined feature vectors from both the training and test sets. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D07",
        "filename": "D07-1020.json"
    },
    {
        "casual_text": "The BLEU score keeps dropping no matter what, but the TER score gets a little better for the English-Spanish and English-French pairs, especially if you use the sentence-based or two-steps estimation methods. Still, since the results for both scores don't really match up, we can't say for sure if this type of language model adaptation is better than just sticking with the basic baseline model.",
        "formal_text": "A degradation of the BLEU score is observed in any condition, while TER slightly improves for the En-Es and En-Fr pairs, especially when either the sentence-based or the two-steps estimation schemes are adopted. However, since results are not coherent for two scores, it cannot be definitely stated whether this form of LM adaptation overcomes the use of the single baseline LM.",
        "GPT2_formal_text": ", and attention_keywords. Formal: We came up with a way to create a way to encode ideas without any strict rules. We tested this on three different datasets and found that our model works really well and beats the current best model on all of them. Formal: We created a model that can generate summary ideas in different styles. It’s pretty flexible and can create different versions of the same summary. We also built a system that does all the evaluation by itself, which is way faster than using a traditional pipeline model. Formal: We tried a few different ways to evaluate summaries, and in this paper, we’ll focus on two common ways to do it: BLEU and ROUGE. Formal: We’ve listed all the evaluation results in Table 1. The results for ROUGE, on the other hand, aren’t listed because we didn’t include them in the original papers. Formal: We’ve included the BLEU scores for the top-1, top-2, and top-3 results in Table 1. Formal: We’ve listed the ROUGE scores for the top-1, top-2, and top-3 results in Table 1. Formal: Formal: We’ve listed the ROUGE scores for the top-1, top-2, and top-3 results in Table 1. Formal: Formal: We’ve listed the ROUGE scores for the top-1, top-2, and top-3 results in Table 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "eamt",
        "filename": "2010.eamt-1.30.json"
    },
    {
        "casual_text": "• RQ1: Can we create machine learning models that are better than the current systems for detecting cyberbullying?",
        "formal_text": "• RQ1: Can we build machine learning models to outperform current cyberbullying classification systems?",
        "GPT2_formal_text": "= { u, v }        Casual: Here, α = 0.2 and β = 0.1. We've included the results for the ranking model from the study by Jiang et al. (2020), which was trained on the test set, along with the results from the full model. Formal: After every search step, we grab the top N results from the ranking model and the full model. Formal: We use the top N results from the ranking model, along with the full model's results, to figure out the best responses. Formal: Next, we check the best options for each query and generate a response using the full model. Formal: The full model gives us a list of possible answers, which we then use to generate a response. Formal: For each link in the article, we pick the top N results and their corresponding responses from the top N options. Formal: Finally, we rank the top N responses using a BERT model. Formal: This is the same approach as in Table 2, but instead of using the full model's results, we use the top N results from the ranking model. Formal: We use the top N results from the ranking model to rank the top N results. Formal: We rank the top N results using a BERT model. Formal: We use the top N results from the ranking model to rank the top N results. Formal: We use the top N results from the ranking model to rank the top N results. Formal: We use the top N results from the ranking model to rank the top N results. Formal: We use the top N results from the ranking model to rank the top N results. Formal: We use the top N results from the ranking model to rank the top N results. Formal: We use the top N results from the ranking model to rank the top N results. Formal: We use the top N results from the ranking model to rank the top N results. Formal: We use the top N results from the ranking model to rank the top N results. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "alta",
        "filename": "2020.alta-1.2.json"
    },
    {
        "casual_text": "Using the N-gram method, the translation process is basically like looking for the best possible translation option, called hypothesisê I 1. This best option is found by picking the one that gives the highest score when you combine a translation model (TM) with a bunch of other feature models, all added up in a log-linear way.",
        "formal_text": "According to the N -gram-based approach, the translation process is considered as an arg max searching for the translation hypothesisê I 1 maximizing a log-linear combination of a translation model (TM) and a set of feature models:",
        "GPT2_formal_text": "ing (from Brown et al., 2020) is a regularization method that helps prevent overfitting. Formal: Here’s the proof of Lemma G.4, which is basically the definition of the nullspace of a fully connected layer. Formal: In this proof, the space is represented as a unigram (or bigger) distribution. Formal: Let’s say we’re dealing with a fully connected layer that has a window size of w and a batch size of m. This layer is defined as a directed acyclic graph (DAG) with dimensions d_i and m_i. Formal: Using equation (6), we can write the hidden state h_i as a weighted product of the embeddings of the output embedding and the last hidden state h_k. Formal: To find the hidden state h_i, we use the dot product of the embedding of the output embedding and the last hidden state h_k. Formal: We’re going to define the loss term θ_i for this DAG. Formal: Formal: Next, we’ll define the loss function. Formal: θ(•) = log(θ_i) + log(θ_k). Formal: Formal: Finally, we’ll plug the loss function into equation (6) to get the final loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2009.eamt-1.27.json"
    },
    {
        "casual_text": "We also checked out how many alignment links our systems were creating. One thing that stood out was that the Bayesian HMM with NULL words consistently made way fewer links compared to all the other systems. If we use the BHMM as a baseline, the other models created, on average across languages and translation directions, 39.5% more links (for our collocation-based model), 39.2% more (Giza++), and 36.2% more (fastAlign). Making more links makes it harder for the phrase extraction process and results in smaller phrase tables. In practice, the phrase tables from the collocation-based model are about three times smaller than those from the Bayesian HMM. So, the collocation-based system has an edge here.",
        "formal_text": "We also analysed the number of alignment links that our systems set. It is noteworthy that the Bayesian HMM with NULL words consistently sets much fewer links than all other systems. Taking BHMM as baseline other models set additionally (on average across languages and translation direction) 39. 5% (our collocation-based model), 39. 2% (Giza++), and 36. 2% (fastAlign) more links. Setting more links constrains phrase extraction heuristics more and leads to smaller phrase tables. Empirically, the phrase tables of the collocation-based model are roughly three times smaller than those of the Bayesian HMM. Thus, the collocation-based system is at an advantage here.",
        "GPT2_formal_text": "= {r_a1, r_b1, ..., r_t1}, where r_a1, r_b1, and r_t1 are the start, end, and length of the first word in the text, respectively. Formal: Lastly, we take the normalized probability of each position and use it as the value of q. Formal: Basically, for each position t_i in a sentence, the normalized probability is calculated by adding up all the probabilities at that position. Formal: So, for each word w_i in the sentence, the normalized probability is: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C16",
        "filename": "C16-1296.json"
    },
    {
        "casual_text": "So far, multi-task learning in patent translation has mostly been about trying out different combinations of translation and language models, but only within specific groups of patent sections. For instance, Utiyama and Isahara (2007) and Tinsley et al. (2010) looked at how translation and language models work when trained on different patent sections. They found that having more parallel data helps improve the results. Ceauşu et al. (2011) also noticed that language models always improve, and translation models usually do too, when they have more data from different sections. In our approach, we use models trained on this combined patent data as a starting point for comparison.",
        "formal_text": "Multi-task learning efforts in patent translation have so far been restricted to experimental combinations of translation and language models from different sets of IPC sections. For example, Utiyama and Isahara (2007) and Tinsley et al. (2010) investigate translation and language models trained on different sets of patent sections, with larger pools of parallel data improving results. Ceauşu et al. (2011) find that language models always and translation model mostly benefit from larger pools of data from different sections. Models trained on pooled patent data are used as baselines in our approach.",
        "GPT2_formal_text": "ed ( ), which is basically the sum of the attention weights for all the tokens in the input. The updated hidden state h_s is calculated like this: Formal: We take the hidden state h_s from the last step, just like we did in step 3, and use it to calculate the attention weights for the current token. Formal: Formal: After that, we feed this updated hidden state h_s back into the decoder, using a mix of the attention weights from the last and this updated hidden state h_s. Formal: Formal: We can apply this attention-based decoding approach to different NLP tasks, as long as we have the right parameters. Formal: Formal: Here's what we've done: Formal: We introduced a new model called the multi-head attention masking model, which works with multiple hidden states, as shown in figure 2. Formal: We tested this approach on two text classification tasks and got the best results so far. Formal: Formal: We also created a special model called the attention-based decoder, which is designed to handle multi-modal input. Formal: Formal: We used the multi-head attention masking model to predict the attention weights for both text classification and sentence classification tasks. Formal: Formal: We tested how well this model works for text classification tasks by comparing its prediction to the results from the original single-head attention model. Formal: Formal: Formal: As for sentence classification, we tested the model on the MNLI dataset to see how well it could classify sentences. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E12",
        "filename": "E12-1083.json"
    },
    {
        "casual_text": "Another cool thing about the sieve framework is how easy it is to add new stuff without needing to know everything else that’s already in there. We’ve made the code available for everyone to use, so you can either run it as a stand-alone coreference system or build on it to create something new.",
        "formal_text": "An additional benefit of the sieve framework is its modularity: new features or models can be inserted in the system with limited understanding of the other features already deployed. Our code is publicly released 5 and can be used both as a stand-alone coreference system and as a platform for the development of future systems.",
        "GPT2_formal_text": "= { id1, id2, ..., idn } = { entity_id, entity_type, entity_value } | H(E, t) = { t1, ..., tm } = { h1, h2, ..., hn }, where hi is the value of the i-th token in the sentence. Formal: • The input is a sentence with one or more tokens that are part of the input. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D10",
        "filename": "D10-1048.json"
    },
    {
        "casual_text": "Alright, let’s move on to the main point of this paper: figuring out a quick way to describe the whole regularization path for the maximum entropy problem. We’ve already shown that the best solution, p, can be easily found using the variable µ. So, instead of dealing with p directly, we just need to keep track of how µ changes as we move across the (ν, µ) plane, starting from ν = 0 and ending at the last change point, which we called (ν∞, µ∞). \n\nIn this part, we’ll explain an algorithm that does this by finding where ℓ0 crosses the lines ℓ−n, ℓ−n+1, ..., ℓ−1, ℓ1, ..., ℓn. After each intersection, we’ll update ℓ0 and keep going until we’ve covered everything.",
        "formal_text": "We now shift gears and focus on the main thrust of this paper, namely, an efficient characterization of the entire regularization path for the maximum entropy problem. Since we have shown that the optimal solution p can be straightforwardly obtained from the variable µ, it suffices to efficiently track the function µ(ν) as we traverse the plane (ν, µ) from ν = 0 through the last change point which we denoted as (ν ∞, µ ∞ ). In this section we give an algorithm that traverses µ(ν) by locating the intersections of ℓ 0 with the fixed lines ℓ −n, ℓ −n+1, . . . , ℓ −1, ℓ 1, . . . , ℓ n and updating ℓ 0 after each intersection.",
        "GPT2_formal_text": "ed_multi_task.py. For more info, check out the original paper. Formal: We’ve got a few key points to wrap things up. First off, we’ve come up with a new way to handle multi-label classification by using attention. For future work, we’re thinking of expanding this method to handle labels that are way more complex. Plus, we’re curious to see how different types of data affect things. We’re also curious about trying out different ways to represent data and think about adding some neural network tricks to the model to get better results. Formal: The whole multi-task learning thing has led to some cool ideas. For example, we’ve come up with a new approach to multi-label classification, which uses attention, and we’ve also put together a dataset of labeled data for people to use. Formal: Here’s the link to the multi-task learning paper: https://github.com/nhcr/Multi-Task-Learning Formal: The multi-task learning model is under a Creative Commons Attribution 4.0 International License. You can check out the license details here: https://creativecommons.org/licenses/by/4.0/ Formal: For the multi-task learning model, we’ve tried out two different methods. We’ve also created a dataset for people to use. Formal: Here’s the link to the dataset: https://github.com/nhcr/Multi-Task-Learning Formal: In Figure 2, you can see how the multi-task learning model is trained for multi-label classification on two different datasets. Formal: In Figure 1, you can see the classification results for the monolingual dataset. Formal: Formal: From the results in Figure 1, it’s clear that multi-task learning using multi-task learning for multi-label classification works really well. Formal: Formal: The multi-task learning model uses a multi-head attention mechanism for multi-label classification. Formal: Formal: Formal: Here’s the link to the multi-task learning paper: https://github.com/nhcr/Multi-Task-Learning Formal: We’ve also put together a dataset for people to use. Formal: Formal: Formal: The multi-task learning model uses a multi-",
        "directory": "D11",
        "filename": "D11-1087.json"
    },
    {
        "casual_text": "Second, we were focused on how specific the terms were to certain areas. Our plan was to measure this using numbers by comparing how often words were used in different groups of texts. But for this particular study, we went with people's opinions instead. Melamed (1996b) says that to really evaluate translation word lists, judges need to see both languages side by side, showing how the words are used in sentences. But getting people to judge without seeing the full context would be simpler in both real-world and lab settings.\n\nAs a first try, we had three people do a kind of tagging job: one was a pro French/English translator, and the other two were grad students at Penn. They tagged a bunch of entries that came from an older version of the SABLE system (this one used small pieces of sentences to figure out which words often appear together; check out Section 2.2 for more). They didn’t get to see the full sentences in both languages this time. After the study, looking at how the system did and reading comments from the taggers in a follow-up survey, we saw that context really matters.\n\nTo get a better idea of how much it matters, we asked one of the taggers from the first round to do the same job again, but this time she got to see the full context—both languages side by side, showing up to the first ten times each word pair was used.",
        "formal_text": "Second, we were concerned with domain specificity. Ultimately we intend to measure this in an objective, quantitative way by comparing term usage across corpora; however, for this study we relied on human judgments. Melamed (1996b) suggests that evaluation of translation lexicons requires that judges have access to bilingual concordances showing the contexts in which proposed word pairs appear; however, out-ofcontext judgments would be easier to obtain in both experimental and real-world settings. In a preliminary evaluation, we had three annotators (one professional French/English translator and two graduate students at the University of Pennsylvania) perform a version of the annotation task just described: they annotated a set of entries containing the output of an earlier version of the SABLE system (one that used aligned sub-sentence fragments to define term co-occurrence; cf. Section 2. 2). No bilingual concordances were made available to them. Analysis of the system's performance in this pilot study, however, as well as annotator comments in a post-study questionnaire, confirmed that context is quite important. In order to quantify its im: portance, we asked one of the pilot annotators to repeat the evaluation on the same items, this time giving her access to context in the form of the bilingual concordances for each term pair. These concordances contained up to the first ten instances of that pair as used in context.",
        "GPT2_formal_text": "\". In a study by Eisner and colleagues in 2020, they used a classifier to predict the class label for each tweet. The input tweet text is then translated into the target language, like English, using a standard machine translation tool. Formal: We use the same approach as with the rest of the datasets to train our classifier. Formal: We also check the text representation quality and then use cross-lingual word embeddings as inputs for a post-to-post Transformer model. Formal: The results for the dataset are in Table 2. Out of the four datasets, only VUA_CAMERAS has a non-zero label on the test set. Formal: On the other hand, VUA_EXCAMERAS is the only dataset where the label is zero. Formal: From what we saw, it seems that the best performance comes from using parallel data and the VUA_EXCAMERAS dataset. Formal: The results are in Table 4. As expected, the baseline system with parallel data performs the best, especially when compared to the joint model. Formal: Our system got a 5.17% F-score on the VUA_EXCAMERAS dataset. Formal: The joint model is the second best on VUA_CAMERAS and is the only one that uses parallel data. Formal: The other two systems, VUA_CAMERAS and VUA_EXCAMERAS, are the best on the VUA_CAMERAS dataset and use parallel data, which gives them an advantage. Formal: Our combined system, which combines both parallel data and the VUA_EXCAMERAS dataset, also did well on the VUA_CAMERAS dataset. Formal: Table 5 shows the average results for each task on the VUA_EXCAMERAS dataset. Formal: The best performance comes from the VUA_CAMERAS dataset. Formal: The other two datasets don’t have any negative examples. Formal: We tested our system on both VUA_EXCAMERAS and VUA_CAMERAS. Formal: The results for the joint model are in Table 6. Formal: The top-performing system uses the VUA_EXCAMERAS dataset. Formal: Our system also performs really well on the VUA_CAMERAS dataset. Formal: Form",
        "directory": "A97",
        "filename": "A97-1050.json"
    },
    {
        "casual_text": "Sure! Here's a more casual version:\n\nFor tasks that involve figuring out the relationship between two sentences (like textual entailment), a special token (like [SEP]) is automatically added between the sentences. This helps the model know where one sentence ends and the other begins.",
        "formal_text": "[CLS] or <s>). For tasks which model a relation between multiple sentences (e. g. , textual entailment), a special token (e. g. [SEP] ) is automatically inserted between the sentences to inform the model about the sentence boundaries.",
        "GPT2_formal_text": "ing_src_layer. This layer uses a masking operation to separate the source tokens from the reference tokens. We can then use the attention scores from the NER layer to adjust the representation of the source tokens in the target embedding space. Formal: Let's say w is the [CLS] token, and v is the [SEP] token. Formal: To see how well the source-side attention is doing, we measure how well it matches the target-side attention by calculating the average attention in the source embedding space. Formal: We take the average attention score for the source-side and target-side attention vectors and then use them to calculate the attention score for each token in the target embedding space. Formal: The attention score is calculated like this: Formal: This is the cross-entropy loss function for the source-side attention vector. Formal: We show the normalized attention scores for the source-side and target-side attention vectors for the [CLS] token in the target embedding space. Formal: The target embedding spaces have three main parts: Formal: The source-side attention vector and the target-side attention vector. Formal: For the source-side attention vector, we subtract the attention score calculated for the [CLS] token. Formal: The target embedding spaces also have three main parts: Formal: The source-side attention vectors and the target-side attention vectors. Formal: For the source-side attention vectors, we subtract the attention score calculated for the [CLS] token from the [SEP] token. Formal: The target embedding spaces have five main parts: Formal: The target-side attention vectors and the source-side attention vectors. Formal: For the source-side attention vectors, we subtract the attention score calculated for the [CLS] token from the [SEP] token. Formal: The target embedding spaces have six main parts: Formal: The source-side attention vectors and the target-side attention vectors. Formal: For the source-side attention vectors, we subtract the attention score calculated for the [CLS] token from the [SEP] token. Formal: The target embedding spaces have seven main parts: Formal: The source-side attention vectors and the target-side attention vectors. Formal: For the source-side attention vectors, we subtract the attention score calculated for the [CLS]",
        "directory": "eacl",
        "filename": "2021.eacl-demos.22.json"
    },
    {
        "casual_text": "You could say that these mistakes affect the user experience differently. For example, a correctly spelled word being changed incorrectly is more annoying than a misspelled word that isn’t corrected but is marked as wrong. But in this paper, we’re treating all errors the same. E1, E2, E3, and E4 are all about the correction process. So, we can come up with something called the Correction Error Rate (CER).",
        "formal_text": "It can be argued that these errors have varying impact on user experience. For instance, a well spelled word that is wrongly corrected is more frustrating than a misspelled word that is not corrected but is flagged. However, in this paper, we treat each error equally. E 1, E 2, E 3 and E 4 pertain to the correction task. Hence we can define Correction Error Rate (CER):",
        "GPT2_formal_text": ", which are basically word embeddings. Formal: We take a look at how the model performed on the test set using the test split we had. Formal: (a) The model's performance on the test set is shown in Table 1. It's clear that the model struggles with longer sentences. (b) Even though it does better than the baseline, the model still doesn't perform well when dealing with shorter sentences. Formal: It's worth mentioning that the model might not be great at dealing with sentences with more than two words. The issue could be that the model isn't great at handling long sentences at all. Formal: Table 2 shows the test split and how the model performed on the test set. The model's performance on the test set is highlighted in green. Formal: Table 3 breaks down the results for the test set in more detail. Formal: (a) The model's performance on the test set is shown in Table 3. Formal: (b) The model's performance on the test set is highlighted in red. Formal: (c) The model's performance on the test set is highlighted in purple. Formal: (d) The model's performance on the test set is highlighted in blue. Formal: (e) The model's performance on the test set is highlighted in green. Formal: (f) The model's performance on the test set is highlighted in red. Formal: (g) The model's performance on the test set is highlighted in purple. Formal: (h) The model's performance on the test set is highlighted in blue. Formal: (i) The model's performance on the test set is highlighted in green. Formal: (j) The model's performance on the test set is highlighted in purple. Formal: (k) The model's performance on the test set is highlighted in blue. Formal: (l) The model's performance on the test set is highlighted in purple. Formal: (m) The model's performance on the test set is highlighted in green. Formal: (n) The model's performance on the test set is highlighted in purple. Formal: (o) The model's performance on the test set is highlighted in purple. Formal: (p) The model's performance on the test set is highlighted in green. Formal: (q) The model's performance on the test set is highlighted in purple. Formal: (r) The model's performance",
        "directory": "D09",
        "filename": "D09-1093.json"
    },
    {
        "casual_text": "We took a look at how well our commitment-based system for RTE did with the 1600 examples from the PASCAL RTE-2 and RTE-3 datasets. Table 1 shows the results we got when we trained our system on those 1600 examples from the RTE-2 and RTE-3 Test Sets.",
        "formal_text": "We evaluated the performance of our commitmentbased system for RTE against the 1600 examples found in the PASCAL RTE-2 and RTE-3 datasets. 7 Table 1 presents results from our system when trained on the 1600 examples taken from the RTE-2 and RTE-3 Test Sets.",
        "GPT2_formal_text": "_tgt = tf.nn.semicurrent(D + {W * 2})[, ] | = {[, ], [, ], [, ], [, ]} Formal: The attention is calculated using the dot product of the attention vectors, which we call Att(A, A'). The hidden representation of the entity pair E_i is then fed into an attention mechanism, which gives us the hidden representation of E_i_tgt. Formal: This process has two main steps: First, we update the entity embeddings by applying a softmax function. Then, the attention mechanism gets updated by multiplying the attention vectors from both the entity and the relation vectors. Formal: Finally, the attention for the entity pair is passed through a softmax function, which gives us the representation of E_i_tgt. Formal: This part talks about a model setup, the architecture, and the main features of the model. We also dive into how to calculate the attention weights. Formal: The model's structure is laid out in Figure 2. To train the model, we follow the approach by Goyal and Voorhees (2017). We use two multi-head attention mechanisms: one for the entity and another for the relation. Formal: We also explain how to model the relation embedding and how to use the relation embedding for training the model. Formal: We use two multi-head attention mechanisms: one for the entity and another for the relation. Formal: The multi-head attention mechanism performs really well on the datasets we've tested, especially when it's fine-tuned on the development set. Formal: We fine-tune the model on the test set using the same method as Goyal and Voorhees (2017). Formal: Here's how the model works on the Dev set. Formal: For multi-head attention, the entity-relation pair embeddings are combined using a softmax function. Formal: For attention, the entity and relation embeddings are combined using a softmax function. Formal: For the attention mechanism, we use the multi-head attention mechanism. Formal: We use the multi-head attention mechanism for the entity-relation pair embeddings. Formal: The entity-relation pair embedding is calculated using the dot product of the entity and relation embeddings. Formal: The attention for the entity-relation pair embedding is calculated using the attention from the",
        "directory": "C08",
        "filename": "C08-1043.json"
    },
    {
        "casual_text": "But, benchmarks don’t really capture the messy, complicated stuff you deal with in real-world situations. To get closer to how doctors actually think, we thought about using differential diagnosis prediction, which means bringing in more types of clinical data. Still, when it comes to using knowledge graphs in real-world settings, we might need to think about things like how time plays a role. For example, predicting how a disease might change over time could help us understand the long-term side of diagnosing. Part of what we’re working on is coming up with tasks that reflect the real challenges doctors face when making decisions.",
        "formal_text": "However, such benchmarks are abstractions that do not fully align with complex real-world use cases. To better model the challenges of clinical reasoning, we suggested the particular task of differential diagnosis prediction, which involves incorporating additional clinical data modalities. However, additional considerations may be necessary for evaluating KG use in real-world applications, such as modeling temporality. A task involving prediction of changing disease states over time may focus on the longitudinal nature of diagnostic reasoning. A portion of this work will involve continuing to define tasks that consider the challenges of real-world clinical reasoning use cases.",
        "GPT2_formal_text": "= [s1, ..., sJ] = {s1i, ..., sJi}. Formal: For instance, the output for \"i\" is \"1 2\" in the example shown in Figure 3. Formal: The example in Figure 3 uses the output for \"1 2\" to represent the identity link between the two entities. The two entities are linked by the identity link \"1 2\" in the second entity, and then they both share the same embedded representations. Formal: The output for \"1 2\" in Figure 3 is linked to the embedding \"1 2\". Formal: This embedding \"1 2\" in the second entity, \"i\", is linked to the embedding \"2 2\". Formal: This embedding \"2 2\" in the first entity, \"j\", is linked to the embedding \"1 2\". Formal: Similarly, the embedding \"2 2\" in the second entity, \"j\", is linked to the embedding \"2 2\". Formal: For more details on how we combine these representations, check out Appendix A. Formal: We use the embeddings in the second entity, \"j\", and the first entity, \"i\", to create the embeddings for both entities. Formal: To figure out the embeddings for \"1 2\" in Figure 3, we use the embeddings for \"2 2\" in the first entity. Formal: In the second entity, \"j\", the embedding \"2 2\" is linked to the embedding \"1 2\". Formal: Formal: In the first entity, \"j\", the embedding \"2 2\" is linked to the embedding \"1 2\". Formal: Lastly, the embeddings \"2 2\" in the second entity, \"j\", are linked to the embedding \"2 2\". Formal: Formal: To combine these embeddings, we use the output from the BiLSTM in the second entity, \"j\", and the output from the BiLSTM in the first entity, \"i\". Formal: Formal: In the first entity, \"j\", the embedding \"2 2\" is linked to the embedding \"1 2\". Formal: Formal: Formal: The outputs for \"1 2\" and \"2 2\" in Figure 3 are combined to create the embeddings for the entities. Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-srw.33.json"
    },
    {
        "casual_text": "We tested our methods on four Chinese NER datasets: OntoNotes (Weischedel et al., 2011), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017a), and Resume NER (Zhang and Yang, 2018). OntoNotes and MSRA come from news articles, and they have gold-standard segmentation for the training data. For OntoNotes, the development and testing data also have gold segmentation. On the other hand, Weibo NER is from social media, and Resume NER is from resumes. These two datasets don’t have gold-standard segmentation. Table 1 has all the stats for these datasets.\n\nAs for the lexicon, we used the same one as Lattice-LSTM. It includes 5.7k single-character words, 291.5k two-character words, 278.1k three-character words, and 129.1k other words. We also used the same pre-trained character embeddings as Lattice-LSTM, which were trained on the Chinese Giga-Word dataset using word2vec.",
        "formal_text": "The methods were evaluated on four Chinese NER datasets, including OntoNotes (Weischedel et al. , 2011), MSRA (Levow, 2006), Weibo NER (Peng and Dredze, 2015; He and Sun, 2017a), and Resume NER (Zhang and Yang, 2018). OntoNotes and MSRA are from the newswire domain, where gold-standard segmentation is available for training data. For OntoNotes, gold segmentation is also available for development and testing data. Weibo NER and Resume NER are from social media and resume, respectively. There is no gold standard segmentation in these two datasets. Table 1 shows statistic information of these datasets. As for the lexicon, we used the same one as Lattice-LSTM, which contains 5. 7k single-character words, 291. 5k two-character words, 278. 1k three-character words, and 129. 1k other words. In addition, the pretrained character embeddings we used are also the same with Lattice-LSTM, which are pre-trained on Chinese Giga-Word using word2vec.",
        "GPT2_formal_text": "ing_type': 'Masking Type' }, {'cls': [{'type': 'text-encoding', 'encoding': 'base64', 'mode': 'wmode', 'len': 64}, {'type': 'text-decoding', 'encoding': 'base64', 'mode': 'decmode', 'len': 64}, {'type': 'text-length', 'encoding': 'base64', 'mode': 'wmode', 'len': 128}, {'type': 'text-relevance', 'encoding': 'base64', 'mode': 'wmode', 'len': 128}, {'type': 'text-width', 'encoding': 'base64', 'mode': 'wmode', 'len': 64}, {'type': 'text-characters', 'encoding': 'base64', 'mode': 'wmode', 'len': 32}, {'type': 'text-characters2', 'encoding': 'base64', 'mode': 'wmode', 'len': 32}, ], }, ], }' );\n\nBasically, the first part here is talking about how to handle different parts of a text. The second part is about the actual text itself, and the last part is the output from the input layer. Formal: The input text is in one language, and the output is in another. Formal: A transformer-based model is trained using a given input, using some layers to make sense of it and generate stuff. Formal: Each layer in the transformer model is like a handle for the text. Formal: The output from the output layer is the output itself. Formal: The input text is in one language, and the output is in another. Formal: The output layer in the transformer model is like a handle for the text. Formal: Each output layer in the transformer model is like a handle for the text. Formal: The input text is in one language, and the output is in another. Formal: The output layer in the transformer model is like a handle for the text. Formal: The output layer in the transformer model is like a handle for the text. Formal: The input text is in one language, and the output is in another. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "Since SHAP weights show how much a token, or word, usually affects the outcome when it's hidden, we also pay attention to words that tend to behave unusually on average.",
        "formal_text": "Because SHAP weights reflect the average effect of masking a token t, we also focus on word features that are abnormal on average.",
        "GPT2_formal_text": "': [] w = wmatrix([CLS], w2, [E1], [E2], ..., [En], [Eo], ..., [Et]) p = pmatrix([CLS], p2, [E1], [E2], ..., [En], [Eo], ..., [Et])\n\nHere, [E1], [E2], ..., [En] are the indices of the first, second, and third words in each sentence, respectively. The [Et] part is just a placeholder for any missing words. The [Eo] part just means we don't have any words in the sentence. We use a specific matrix called [CLS] to represent the sequence of words.\n\nAs shown in Figure 1, the setup for sentence-level knowledge distillation (Sld) involves three models: the sentence-level one (Sld1), the sentence-level three (Sld3), and the zero-shot model (SldZ). The zero-shot model is trained with two sequences: one from the teacher model and one from the student model. The input to the zero-shot model is a sequence of two words from the student model, one from the teacher, and one from the student model that isn't in the teacher's training set. Formal: The sequence of word embeddings we get from the student's sentence is fed into the zero-shot model. Formal: The zero-shot model uses an embedding for each sentence and a sequence of sentence embeddings. Formal: To make the zero-shot model less dependent on the teacher's word embeddings, we add two extra loss functions: one for the student's sentence embeddings (Sldsz) and one for the teacher's sentence embeddings (Sldt). Formal: The loss function for the zero-shot model is: Formal: So, the first loss function is based on the sentence-level model, and the second is based on the zero-shot model. Formal: To make the model better at generalizing, we add a loss function for the student's embeddings (Sldsz), which is the same as the loss function for the zero-shot model (Sldt). Formal: The final loss function for the zero-shot model is: Formal: In our experiments, we tested different loss functions. Formal: We",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "Schalte einfach zwischen den Schnappschüssen hin und her, bis du die richtigen Einstellungen gefunden hast.",
        "formal_text": "wechseln Sie zwischen den Schnappschüsse, um die gewünschten Einstellungen zu finden.",
        "GPT2_formal_text": "ed: 1. BERT-XML-sentences: These sentences get replaced with xmltog tags and the model is updated. This experiment uses SentencePiece and the Stanford MLDoc 2.0 dataset. Formal: In this setup, we treat the user's input as a sentence. Formal: In this case, the input is a document, and the model is trained to generate a response. Formal: We train the model using the data we have in both the user's memory and the document. Formal: The user's memory is used to store the conversation history, which is a bit different from what we've done before. Formal: In the basic scenario, the model uses the user's memory to generate responses. Formal: We use the Stanford MLDoc dataset to check how well the model can generate responses. Formal: We train the model using this dataset. Formal: We apply a beam search method to the data. Formal: The model's output is used as the input for the memory module. Formal: We train the memory module using the data from the document. Formal: We train the memory module using the data from the document. Formal: Finally, the output from the memory module is used as the input for the next part of the conversation. Formal: The user's memory is used to store the conversation history, which is a bit different from what we've done before. Formal: We train the model using this dataset. Formal: We apply a beam search method to the data. Formal: The output from the memory module is used as the input for the next part of the conversation. Formal: Finally, the output from the memory module is used as the input for the next part of the conversation. Formal: Lastly, the output from the memory module is used as the input for the next part of the conversation. Formal: We train the model using this dataset. Formal: We train the model using this dataset. Formal: We apply a beam search method to the data. Formal: The output from the memory module is used as the input for the next part of the conversation. Formal: Lastly, the output from the memory module is used as the input for the next part of the conversation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.267.json"
    },
    {
        "casual_text": "Our algorithm has three main steps:  \n\n1. First, we assign each instance a probability distribution of possible word replacements based on a neural biLM (check out section 2.1 for details).  \n2. Next, we pick k \"representatives\" for each instance. Each of these representatives is made up of multiple samples from the word replacement distribution we just talked about (section 2.3 explains this).  \n3. Finally, we group the representatives into clusters and use those clusters to create a soft-clustering for the instances (you can find more in section 2.4).",
        "formal_text": "Our algorithm works in three stages: (1) We first associate each instance with a probability distribution over in-context word-substitutes. This probability distribution is based on a neural biLM (section 2. 1). (2) We associate each instance with k representatives, each containing multiple samples from its associated word distributions (section 2. 3). (3) Finally, we cluster the representatives and use the hard clustering to derive a soft-clustering over the instances (section 2. 4).",
        "GPT2_formal_text": "= {l_i: 1, ..., l_n} | |t = 0, ..., t = n | |o| = {l_o|, } | |l_i | = {l_i|, } | |t | = {l_t|, } | |l_i | = {l_i|, } | |t | = {l_t|, } | |l_i | = {l_i|, } | |t | = {l_t|, } | = |l_i| | = {l_i|, } | = 0, ..., l_n | = 0 | = {l_n|, } | = {l_n|, } Formal: Let's say you have a sentence S and a target node i, where i is the i-th token. The semantic role attention score is calculated like this: Formal: In this case, we're dealing with a sentence S, and the roles e1, e2, ..., eL are all the labels from a semantic role label set called S. Formal: For each e1, e2, ..., eL, we look at the meanings of the roles e1, e2, ..., eL, and calculate the score for each one. Formal: So, the score for the role e1 is calculated like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1523.json"
    },
    {
        "casual_text": "Another area of research involves predicting the overall result of a case. For example, you might try to guess which side the outcome will favor (like Aletras et al. did in 2016), or whether the current court will uphold or overturn a decision made by a lower court (as Katz et al. explored, see Figure 1 for an example). Unlike them, we’re taking it a step further. Instead of just predicting a simple binary outcome (or including an \"other\" category), we’re focusing on the specific details of the case, like the charges involved. This means our output can have multiple labels.",
        "formal_text": "Another related thread of work is to predict the overall outcome of a case. The target can be which party will the outcome side with (Aletras et al. , 2016), or whether the present court will affirm or reverse the decision of a lower court (Katz et al. , Figure 1: An example judgement document excerpt of a criminal case in our dataset. Names are anonymized as AA and BB. Rectangulars, ellipses and dashed rectangulars refer to the clauses that usually indicate the beginning of the facts part, the court view part and the decision part, respectively. Articles and charges are extracted with regular expressions and a charge list. 2016). Our work differs from them in that, instead of binary outcome (the latter one also contains an other class), we step further to focus on the detailed results of a case, i. e. , the charges, where the output may contain multiple labels.",
        "GPT2_formal_text": ", it's common to add a bias term to the output vector to help the model focus on the most important parts of the sequence. This process is called sequence-level attention in the standard setup. Formal: After the attention mechanism gets the attention vector, it looks for the word that has the highest attention score among all the words in the sequence. Formal: We also add a bias term, let's call it y_a, to the output vector, which is part of the output from the last layer. We'll call this vector y_A, where a is the attention score between 0 and 1, and y_a is the attention score between y_0 and y_1. Formal: If y_a is greater than y_a_1, we'll call the vector y_A_i, and if not, we'll call it y_A. Formal: By combining this attention vector and bias term, the model learns to represent the sequences in a more structured way. Formal: We're using a similar approach to apply this attention to the input sequence. Formal: Next, we take the sequence representation created by the last layer and pass it through an attention module, y, to get the representation for the current token, x_i, which is in R^n + k^n. Formal: Finally, we combine this representation from the last layer with the token representation from the first layer, x_i, to get the final representation for the whole sequence, y. Formal: Formal: This means we train the model by maximizing the average attention score between the last layer and the token representation, which helps it figure out the best path through the sequence. Formal: To keep things simple, we're not showing results for the user's input x_i here. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1289.json"
    },
    {
        "casual_text": "Let’s call LCLR the LC transform with some tweaks, so it only works on nonterminals that are left-recursive. The fifth row in Table 4 shows what happens when we use LCLR on the three original grammars. LCLR shrinks the non-left-recursive parts of the CT and ATIS grammars a lot, but for the PT grammar, it only makes a small difference. This makes sense if you look at Table 1, because almost all the rules in the PT grammar are for left-recursive nonterminals. But, we can do some extra steps, like what we did with Paull’s algorithm, to cut down the number of rules for left-recursive nonterminals before using our modified LC transform. The sixth and seventh rows in Table 4 show what happens if we left factor the grammar before using LCLR (LF+LCLR), and also if we group non-left-recursive rules for left-recursive nonterminals before using LCLR (LF+NLRG+LCLR).",
        "formal_text": "Let LCLR refer to the LC transform restricted by these modifications so as to apply only to leftrecursive nonterminals. The fifth line of Table 4 shows the results of applying LCLR to the three original grammars. LCLR greatly reduces the size of the non-left-recursive forms of the CT and ATIS grammars, but the size of the non-left-recursive form of the PT grammar is only slightly reduced. This is not surprising if we note from Table 1 that almost all the productions of the PT grammar are productions for left-recursive nonterminals. However, we can apply the additional transformations that we used with Paull's algorithm, to reduce the number of productions for left-recursive nonterminals before applying our modified LC transform. The effects of left factoring the grammar before applying LCLR (LF+LCLR), and additionally combining non-left-recursive productions for left-recursive nonterminals between left factoring and applying LCLR (LF+NLRG+LCLR), are shown in the sixth and seventh lines of Table 4.",
        "GPT2_formal_text": ", so we don't need to do the same thing twice. Now, let's talk about how we get the labels. To get the data, we grab a bunch of sentences from the web. We use this method because: (1) some languages in the target language are super rare, so using a big amount of text from a minority language doesn't work well; (2) we can create our own texts using tools like Google News and Bing. Formal: To get data from the web, we grab sentences from the target language's official website. The site usually gives you a link to a page with all the sentences in a language, like this: Formal: For the whole target language, we grab sentences from its official website. Formal: The target language also has its own official website. Formal: Once we have the data from the web, we use a tool called DSTC2 (created by Collobert et al. in 2011) to find the translations of the sentences in the target language. Formal: From the translation of each sentence, we grab the translations of the sentences in the target language. Formal: Lastly, we use a tool called nltk (created by Petrov et al. in 2009) to figure out the gender of the target language. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "A00",
        "filename": "A00-2033.json"
    },
    {
        "casual_text": "We calculate the conditional probability vectors P[+c91zi] and P[zll + sl, +s2] using the disjunctive interaction model: 4 3. A natural language processing system also needs to spread out probabilities to the semantic hypotheses so it can work with the interpreted information.",
        "formal_text": "The conditional probability vectors P[+c91zi] and P[zll + sl, +s2] are computed using the disjunctive interaction model: 4 3A natural language processing system needs to propagate probabilities to the semantic hypotheses as well, in order to make use of the interpreted information.",
        "GPT2_formal_text": "ed) represents the attention mask used for the target word. Formal: There are some other methods out there that use a mix of different learning methods and might make the model more resistant to changes in the data. Some focus on specific aspects of the target word, while others use a combination of different methods to make sure the model doesn't have a hard time dealing with changes in the data. Formal: This approach is similar to the one mentioned in (Li et al., 2018) but instead of using attention, it uses a cross-attention setup. Formal: The method from (Li et al., 2018) used a few different learning methods to handle the changes in the target word. This method also used a cross-attention model, but it worked better than the method we're talking about here. Formal: Formal: Another model is the one by Liu et al. (2019b), which uses a multi-layer perceptron to predict how well the model can detect changes in the target word. Formal: Some other approaches that use an information gain approach work by using a multi-layer perceptron (like Liu et al., 2019a) to figure out how to balance the different aspects of the target word. Formal: Formal: Another model is the one by Li et al. (2019b), which uses a multi-layer perceptron to predict how well the model can detect changes in the target word. Formal: Formal: Formal: This is the version of the model we're talking about here. Formal: Formal: Formal: Formal: Another approach we're looking at is the one by Liu et al. (2019a), which uses an information gain approach to make sure the model can learn how to detect changes in the target word. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C90",
        "filename": "C90-2071.json"
    },
    {
        "casual_text": "Equation (9) gives the recognition result in a language with lots of resources. If the system needs results in a language with fewer resources, we should use Equation (10) instead. In Eq. (10), π is a projection operator (from Mohri, 2009) that changes the input label to the output label. Before decoding, the recognition transducer, called ASR, can be made better by doing a determinization operation right after each step.\n\nCheck out Figure 3 for an example of how to build a cross-lingual language model using WFSTs: \n- First, a word sequence like {w1, w2, w3} is represented by the Lw language model G (b1).\n- Then, it's split into a phrase sequence like {w1, w2 w3} (b2).\n- Next, {w1, w2 w3} gets reordered to {w2 w3, w1} (b3).\n- After that, the phrase w2 w3 is changed to v2 v3 (b4).\n- Finally, v2 v3 is put back into a word sequence {v2, v3} (b5).\n\nHere, wk and vk stand for wk and vk, and \"-\" means the ǫ or null symbol. Extra symbols like #1, #2, etc., are added to make the WFST work better (as explained by Mohri, 2009). This helps the transducer get optimized by a determinization operation, which makes the search network smaller and more efficient.",
        "formal_text": "Eq. (9) outputs the recognition result in a resourcerich language. If recognition system requires recognition outputs in a resource-poor language, then the search space should be constructed as Eq. (10), where π is a projection (Mohri, 2009) operator which projects the input label to the output label. Before decoding, the recognition transducer ASR can be optimized by a determinization operation right after each composition. Figure 3: Illustration of constructing a cross-lingual language model via WFSTs: a word sequence {w 1, w 2, w 3 } represented by the L w language model G (b1) is segmented into a phrase sequence {w 1, w 2 w 3 } (b2); {w 1, w 2 w 3 } is reordered into {w 2 w 3, w 1 } (b3); phrase w 2 w 3 is transduced to v 2 v 3 (b4); phrase v 2 v 3 is reconstructed into a word sequence {v 2, v 3 } (b5). wk and vk represent w k and v k, respectively. \"-\" refers to ǫ or null symbol. Auxiliary symbols #1, #2, • • • are used to make the WFST determinizable (Mohri, 2009) such that the transducer can be optimized by a determinization (Mohri, 2009) operation which significantly reduces the search network size.",
        "GPT2_formal_text": "= [text. encode(text. encode(x. idx) + val) for x in x. text] for x in x. text. end] attn = attn_rs(x, attn_sr) if attn. any? attn = attn_rs(x, attn_sr) else attn = attn_rs(x, attn_sr) end\n                     Casual: If there's no attn, we just assign a default value for the attn. Formal: Formal: A few years back, someone came up with a way to do multiple-choice text generation in a way that’s kinda like what we’re doing here. This method doesn’t assume you’ve already decided on the output, so the model can learn to generate stuff in any order you want. Formal: Also, for the sake of keeping things short, we’re skipping over how to calculate the performance score, which is like the average score we’re aiming for. Formal: This method is pretty cool because it lets the model work on any sequence of text—it just needs to decide on the sequence to create the text, and it doesn’t have to worry about picking the right words for each one. Formal: We’re suggesting two ways to implement this idea. Formal: The first one is to use the HATM, which is a sequence generation model, to figure out the output sequence by looking at how likely it is to get the output given the input. Formal: The second method is a bit different—we’re using the CRF, a sequence labeling model, to figure out the output sequence by looking at the context and how likely it is to get the output given the input. Formal: Formal: The final output sequence is just a combination of the HATM’s output and the CRF’s output. Formal: Formal: Formal: Formal: The CRF can give us a binary output (either 1 or 0) for each word in the input sequence. Formal: Formal: Formal: Formal: Basically, the CRF can give us a list of words in the input sequence that are likely to be generated, like {1, ..., N}, where N is the total number",
        "directory": "D12",
        "filename": "D12-1070.json"
    },
    {
        "casual_text": "Adult -----Cost -----------Double Room -------------Single Room ------------Extra Bed",
        "formal_text": "Adult -----Price -----------Double Room -------------Single Room ------------Extra Bed",
        "GPT2_formal_text": "_tgt g(h, s) ∈ R d, where d is the dimension of the attention vector. Formal: We set the dimension of the attention vector to d. Formal: To calculate the attention score h_tgt(h, s), we do this: Formal: The score for the first word in the source sentence is the sum of the attention scores for the words that come before it in the target sentence. Formal: The score for the last word in the target sentence is the sum of the attention scores for the words that come before and after it. Formal: We calculate the attention score h_tgt(h, s) like this: Formal: We take the average attention score from the three words in the source sentence for each word in the target sentence. Formal: After that, we calculate the attention score for the last word in the target sentence by adding the attention score for the last word in the source sentence, plus the attention score for the words that came before and after it. Formal: Finally, we calculate the attention score for the whole sentence, which is the sum of all the attention scores. Formal: Formal: To get the attention vector h_tgt(h, s), we do this: Formal: We take the average attention score from the three words in the source sentence for each word in the target sentence. Formal: We calculate the attention score for the last word in the target sentence by adding the attention score for the last word in the source sentence, plus the attention score for the words that came before and after it. Formal: Finally, we calculate the attention score for the whole sentence, which is the sum of all the attention scores. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C00",
        "filename": "C00-1025.json"
    },
    {
        "casual_text": "We use Google Translator to translate our training data, D src, from English into the target languages. Besides just translating the text, we also need to figure out how to map the slot labels to the target language. To do this, we tried two methods: giza++ (Och and Ney, 2003) and fastalign (Dyer et al., 2013). After testing, we found that giza++ gives us better results—it improves performance by around 2% in F1 score on the SNIPS dataset. So, for the rest of the paper, we stick with Google Translator and giza++ to handle the translation and label mapping. We'll call the translated training data D trans.",
        "formal_text": "We use Google Translator to translate the training corpus D src in the source language (English) to the target languages. In addition to translation, we also need some word alignment methods to project the slot labels to the target language side. We try giza++ (Och and Ney, 2003) and fastalign (Dyer et al. , 2013) to obtain word alignment information and find that the pseudo slot labels projected by giza++ generally lead to better performance (about 2% increase in F1 on the SNIPS dataset). Thus, in the rest of the paper, we use Google translator and giza++ to produce translated data. Denote by D trans the translated training corpus.",
        "GPT2_formal_text": "\" and \"max_time\" respectively. This means the model doesn't have to do any fancy preprocessing, just spit out the results in a text format. Formal: For each mention, we calculate its affinity score by checking how often it shows up with other mentions. Since all mentions have the same frequency, the affinity score is a simple yes or no thing. If a mention doesn't show up at all with other mentions, it gets a 0. Formal: So, for a mention m i, which appears with m i+m j, we calculate its affinity score based on the frequency of all the mentions that m i+m j has appeared with. Formal: The affinity score for a mention m i is calculated using the frequency of all the mentions that m i has appeared with in a specific timeline, which we call \"T.\" Formal: We calculate the affinity score for a mention m i using the frequency of all the mentions that m i has appeared with in a specific timeline, which we call \"T.\" Formal: Using the examples we just saw, here's what the affinity score looks like: 0.001 for m i, 0.01 for m i+m j, 0.01 for m i+m j+m j, 0.01 for m i+m j+m j+m j, 0.01 for m i+m j+m j+m j, and 0.1 for m i+m j+m j+m j+m j+m j+m j+m j. Formal: In a nutshell, our new model can create text that focuses on specific aspects of topic models in the literature. Formal: Formal: So, for each mention m i, which appears with m i+m j, we calculate its affinity score based on the frequency of all the mentions that m i has appeared with in a specific timeline, which we call \"T.\" Formal: The affinity score for a mention m i is calculated using the frequency of all the mentions that m i has appeared with in a specific timeline, which we call \"T.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.259.json"
    },
    {
        "casual_text": "The first change we're making is adding some syntactic features to the CCG categories. Normally, in basic CCG trees, a nominal adjective (like \"a tall boy\") is labeled as N/N, and a predicate adjective (like \"John is tall\") is labeled as S adj \\N P. To make the meaning of both types of adjectives consistent, we're changing N/N to N adj /N for nominal adjectives.",
        "formal_text": "The first modification is to add syntactic features to CCG categories. For example, in the default CCG trees, a nominal adjective (a tall boy) has the category N/N, while a predicate adjective (John is tall) has the category S adj \\N P. To provide a uniform degree semantics to both constructions, we rewrite N/N as N adj /N for the category of nominal adjectives.",
        "GPT2_formal_text": "ed_pairs = [ | p_t, q_t, t|, | p_s, q_s, t|, | p_r, q_r, t|, | q_p, t|, | p_v, q_v, t|, | t|, ]\n\nThe whole setup for multi-modal summarization is laid out in Figure 1. Here's how it works:\n\nFirst, the baseline model creates a sentence, p_t. Then, it generates tokens for the relation pairs p_t and q_t, as shown in Figure 2. The attention mechanism in the attention matrix helps create p_s, q_s, and t for relation p_t, which gets turned into p_r, q_r, and t for relation p_s, q_s, and t. The output is a sequence of tokens like p_s, q_s, t. The attention mechanism then creates the attention for each token in the sequence. Finally, the output is a bunch of scores, p_v, q_v, t. Formal: Using multi-modal attention for summarization. Formal: In the case of multi-modal summarization, the model uses multi-modal attention to create a sentence, p_t. Then, it generates tokens for the relation pairs p_t and q_t, which gets turned into p_s, q_s, and t for relation p_s, q_s, and t. The attention mechanism in the attention matrix helps create p_s, q_s, t. The output is a sequence of tokens like p_s, q_s, t. Formal: The model generates tokens for the relation pairs p_t and q_t, which gets turned into p_s, q_s, and t for relation p_s, q_s, and t. Formal: The model generates tokens for the relation pairs p_t and q_t, which gets turned into p_s, q_s, and t for relation p_s, q_s, and t. Formal: The model generates tokens for the relation pairs p_t and q_t, which gets turned into p_s, q_s, and t for relation p_s, q_s, and t. Formal: The model generates tokens for the relation pairs p_t and",
        "directory": "acl",
        "filename": "2020.acl-srw.35.json"
    },
    {
        "casual_text": "For the Machine Reading Comprehension task, we use the RACE dataset (Lai et al., 2017), which is a big dataset made from English exam questions. It has 25,137 passages and 87,866 questions. Each question has four possible answers, but only one is right. The dataset is split into RACE-M and RACE-H, with questions for middle school and high school students, respectively.",
        "formal_text": "For the Machine Reading Comprehension task, we evaluate on RACE (Lai et al. , 2017), a largescale dataset collected from English exams, containing 25, 137 passages and 87, 866 questions. For each question, four candidate answers are pro-2 https: //data. quora. com/First-Quora-Dataset-Release-Question-Pairs 3 The dataset is derived from Stanford Question Answer Dataset (SQuAD). vided, only one of which is correct. The dataset is further divided into RACE-M and RACE-H, containing exam questions for middle school and high school students.",
        "GPT2_formal_text": "= { 'user', 'pass', 'email', 'follow', 'location', 'timezone' }\n          Casual: Okay, so if we’re dealing with a mix of user, pass, and location, we can combine all those into one user-pass-location mix. We just need to tweak the user and pass vectors to get the location representation. We do this by moving the user and pass vectors to the same position, moving the email and location vectors to the top, and then adding them to the user-pass-location mix. If we do all this, the user-pass-location mix gets a 5-dimensional representation. You can find more details about these parameters in Appendix B. Formal: So, the goal is to get the user-pass-location mixture, which is R m i, from a user-pass-location vector and pass-location vector. Formal: This is the main part of the model. Formal: The user-pass-location vector is created by combining the user and pass vectors. Formal: Formal: After this, we use a linear transformation to get the user-pass-location vector. Formal: This step is about combining the user and pass vectors to get the user-pass-location vector. Formal: Finally, we add the user-pass-location vector to the pass vector to get the user-pass-location vector. Formal: Formal: The result from combining these three vectors is a user-pass-location vector that’s 5 dimensions in size, with dimensions 5 and 1. Formal: Formal: Formal: This setup lets us represent the user-pass-location vector as a combination of two vectors: R m i and R m i+1. Formal: Formal: Formal: So, in the end, the user-pass-location vector has dimensions 5 and 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D19",
        "filename": "D19-1441.json"
    },
    {
        "casual_text": "[7] Body Involvements This part talks about the main body parts that are usually involved when doing something. Like, if you're \"opening\" something physically, you're probably using your \"hands\" and \"arms.\" We've got five groups for this: head, arms, torso, legs, and other body parts.",
        "formal_text": "[7] Body Involvements This attribute group specifies prominent body parts involved in carrying out the action. For example, \"open\" typically involves \"hands\" and \"arms\" when used in a physical sense. We use five categories: head, arms, torso, legs, and other body parts.",
        "GPT2_formal_text": "_tgt = h_tgt + [attn1, attn2, ..., attnK, attnL] Formal: The attention weights for each token are passed through a multi-layer perceptron, which gives us the final representation. Formal: The attention weights for each token are calculated using a multi-layer perceptron. Formal: The attention weights for each token are calculated using a multi-layer perceptron. Formal: This setup gives us the final representation for the token i. Formal: We also look at how the attention weights change depending on the token type and how much the token overlaps with its neighbors. Formal: We calculate the attention weights using a multi-layer perceptron. Formal: Figure 2 (b) shows how we combine the training, validation, and test data. Formal: The attention weights for each token are calculated using a multi-layer perceptron. Formal: Formal: The attention weights for each token are calculated using a multi-layer perceptron. Formal: The attention weights for each token are calculated using a multi-layer perceptron. Formal: Formal: We take the average attention weights for each token across all tokens in the dataset to get the final representation for the token i. Formal: Formal: This gives us the final representation for the token i. Formal: Formal: We also use the same method to combine the training, validation, and test data. Formal: Formal: The attention weights for each token are calculated using a multi-layer perceptron. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1099.json"
    },
    {
        "casual_text": "There are more advanced techniques like word embeddings (Mikolov et al., 2013) and topic modeling algorithms (Blei et al., 2003) that can help figure out how often certain topics show up in documents and how words are connected, even if those connections change over time. Then, there are transformer models like BERT (Bi-directional Encoder Representations from Transformers) (Devlin et al., 2019), which have taken text processing to a whole new level. These models are designed to categorize text, summarize it, or even answer specific questions based on the input they get.",
        "formal_text": "More computationally sophisticated methods such as word embeddings (Mikolov et al. , 2013) and topic modeling algorithms (Blei et al. , 2003) provide the capability to measure prevalence of topics within documents, as well as the relationships between words and how they may shift over time. The development of transformer models such as BERT (Bi-directional Encoder Representations from Transformers) (Devlin et al. , 2019) have opened a new frontier of text processing, with models trained to categorize, summarize or answer specific questions from input text.",
        "GPT2_formal_text": "\" to get the keyphrases for both the single-document summaries and the multi-document summaries. Formal: • If the summary is about a conference, like WAC-2010 or NTCIR, the summary should mention \"Conference\" in the title, and \"Participants\" in the body of the summary. Formal: • The summary should include the main topics of the conference or the main focus of the user's research. Formal: • The summary should include all the info from the original paper, including all the references, the title, authors, and abstract, and the list of authors. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "deeplo",
        "filename": "2022.deeplo-1.5.json"
    },
    {
        "casual_text": "The ALSC and ACD tasks both use the same attention mechanism, but they don’t share any parameters. The reason for keeping them separate is that, for the same aspect, ALSC pays more attention to opinion words, while ACD is more focused on the actual aspect target terms. You can check out the attention visualizations in Section 4.6 to see this in action.",
        "formal_text": "The ALSC and ACD tasks use the same attention mechanism, but they do not share parameters. The reason to use separated parameters is that, for the same aspect, the attention of ALSC concentrates more on opinion words, while ACD focuses more on aspect target terms (see the attention visualizations in Section 4. 6).",
        "GPT2_formal_text": "_tgt(x, y) = h_tgt * tf_idf(x, y)\n             Casual: Alright, let's break this down in a simpler way.\n\nThe first part is about how the attention is spread out across different parts of the image (check out Figure 3b for that). This is based on the specific feature vector we're looking at, which we'll call h_tgt.\n\nThe second part is about how the attention is spread out across different scales, which we'll call h_tgt_s.\n\nFinally, the last part is about how the attention is used across different layers, which we'll call h_tgt.\n\nSo, in short, it's about how the attention is spread out across different parts of the image, how it's used in different layers, and how it's combined with different features. Formal: Formal:\n\nWe've got this matrix h_tgt that's made up of a bunch of smaller matrices. One of these matrices is h_tgt_s, which is like the sum of h_tgt across different scales, and it's written as h_tgt_s. Formal: Formal:\n\nNow, if we have a given image x and a set of features h_tgt, we can figure out the attention weight φ(x, y) by using Equation (3). Formal: Formal:\n\nThis model is trained using a bunch of different image features, and the attention weights φ(x, y) are what give us the final result. Formal: Formal:\n\nTo wrap things up, here's what we get for the final layer in the network: Formal: Formal:\n\nFor the input layer, we have h_tgt (x) and h_tgt_s (y). This is calculated using the input feature vectors h_tgt (x) and h_tgt_s (y). Formal: Formal:\n\nFor the output layer, we have h_tgt (x) and h_tgt_s (y). This is calculated by combining the output from the input layer and the output from the output layer. Formal: Formal:\n\nSo, in short, it's all about how the attention is spread out across different parts of the",
        "directory": "D19",
        "filename": "D19-1467.json"
    },
    {
        "casual_text": "Both methods use a weighted Jaccard metric with mutual information vectors to figure out how similar things are. If you're curious about other ways to measure this, check out the work by Weeds and Weir from 2005.",
        "formal_text": "Both measures use a weighted Jaccard metric on mutual information vectors to calculate the similarity. SeeWeeds and Weir (2005) for an overview of other measures.",
        "GPT2_formal_text": "= {h t } for each h t in the test set {t t }. Formal: We need to figure out if a given sentence is in the test set based on its title, according to the evaluation rules. Formal: To do this, we need to: 1. Find all the sentences with the most words that match the title. Formal: For each sentence s in the title set, we check: Formal: If s isn't in the test set, then: Formal: Update the score for the sentence s by multiplying the scores from all the test sentences and adding the score for s. Formal: If s is in the test set, then: Formal: Update the score for the sentence s by multiplying the scores from all the test sentences and adding the score for s. Formal: The score for s can only go from 0 to 1, but not from -1 to 1. Formal: The score for the sentence s can only go from 0 to 1, but not from -1 to 1. Formal: The score for the sentence s can only go from 0 to 1, but not from -1 to 1. Formal: The score for the sentence s can only go from 0 to 1, but not from -1 to 1. Formal: We also look for sentences that start with a word that has the word \"NOUN\" as its first word and end with the word \"ADJ.\" Formal: We also check if a word or token in the title set starts with a word that starts with \"ADJ.\" Formal: We also check if a word or token in the title set starts with a word that starts with \"NOUN.\" Formal: We use the words in the title set as candidates, but we only consider candidates that start with words that start with \"ADJ.\" Formal: Formal: Formal: The title set includes: Formal: We used the words in the title set as candidates, but we only consider candidates that start with words that start with \"ADJ.\" Formal: Formal: Formal: The title set includes: Formal: We used the words in the title set as candidates, but we only consider candidates that start with words that start with \"NOUN.\" Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D07",
        "filename": "D07-1052.json"
    },
    {
        "casual_text": "In this paper, we introduce a multitask active learning approach for NLU that leverages the relationship between ID and SF. We incorporate popular pool-based query strategies like Least Confidence, Margin Sampling, and Entropy into our framework. We also optimize the calculation for the entropy of a joint-model. The experiments show that our framework, combined with these query strategies, performs well with less training data compared to baseline methods across all datasets. The results also highlight that using the relationship between tasks can lead to better performance than just focusing on intents. Plus, our framework remains effective even when the model is updated. These findings suggest that the framework could be a good fit for real-world applications.",
        "formal_text": "In this paper, we propose a multitask active learning framework for NLU that focuses on making use of the relation information between ID and SF. We implement representative pool-based query strategies that include Least Confidence, Margin Sampling and Entropy in our framework. We also perform an efficient computation for the entropy of a joint-model. Experimental results show above query strategies with our framework can achieve competitive performance with less training data than baseline methods on all datasets. The results also demonstrate that making use of the relation information between tasks may achieve better performance rather than only consider intents. Additionally, our framework is still useful when the model is changed. These results suggest that the framework has the potential to be applied for industrial use.",
        "GPT2_formal_text": "ed_tokens = { 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0 }. Formal: To check how well the generated responses match the gold responses, we use the Krippendorff's α (α) measure. We set the α value to 0.9, just like in the WMT evaluation. Formal: We pick the best-matched response from all the examples to use as the gold response for the next round. Formal: We tweak the response in the current round based on the answer to the previous question. Formal: We consider the best-matched answer for the current question as the gold response, and the ones from the previous question as the references. Formal: For each of the remaining examples, we look at the gold responses to the current question and the references for the last example, and pick the best one from all of them as the gold response for the current question. Formal: We use the average similarity between the gold and reference responses as the similarity measure for this. Formal: After that, we calculate the uncertainty loss for all the instances. Formal: We train the model using the validation set to generate the test set, using the same settings as in the WMT evaluation. Formal: We evaluate the model using the validation set to generate the test set, using the same settings as in the WMT evaluation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.430.json"
    },
    {
        "casual_text": "Before talking to someone, it's smart to think about what they already know. This way, you can use words or ideas that make sense to them and are more helpful. For example, if you're talking to a local or someone who's been to a place before, they might know certain landmarks that you can mention to explain where something is. Also, people might have different abilities, like how well they see or hear, so it's good to keep that in mind too when you're chatting with them.",
        "formal_text": "Speakers check upon information about the recipient because the communication partner's knowledge may render some formulations more relevant than others. As a 'member' of a particular class of people, such as the class of locals, or of the class of those who have visited the place before, the addressee may be expected to know some landmarks that the speaker may use for spatial description. Membership groups may also include differentiation according to capabilities (e. g. , perceptual) of the interlocutors.",
        "GPT2_formal_text": ". The response embeddings will be a mix of embeddings from the response and the query, so they don't all have to be the same length. Formal: Check out Figure 1 for an example. In this case, we're using the response's embedding to figure out the relevant words. Formal: • T = (r_i, s_i, w_i, r_j, s_j) represents all the words in the response that the model thinks are relevant to the query. Formal: • r_i = T + [r_j, s_j] is the relevance score from the response, where r_j is the relevance score from the query. Formal: • w_i = T + [w_j, r_i] is the word embedding for the response, and w_j is the word embedding for the query. Formal: • r_j = T + [r_i, s_j] is the relevance score from the query, and r_j is the relevance score from the response. Formal: Formal: Using these embeddings, the model can then spit out a probability distribution for each word in the response. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "E06",
        "filename": "E06-1024.json"
    },
    {
        "casual_text": "Besides the model structures, stuff like pretrained word vectors, the way we label things, and the optimizer can really affect how well the system works. We looked at a bunch of these external factors using the two top models on a NER dataset: CLSTM+WLSTM+CRF and CCNN+WLSTM+CRF. \n\nPretrained embeddings. Figure 4(a) shows how the two best models performed on the NER test set with two different types of pretrained word vectors, plus when they used random initialization. Using pretrained embeddings gave a big boost in performance compared to random initialization (p < 0.01). The GloVe 100-dimension embeddings scored higher than SENNA (from Collobert et al., 2011) on both models, which matches what Ma and Hovy (2016) found.",
        "formal_text": "In addition to model structures, external factors such as pretrained embeddings, tag scheme, and optimizer can significantly influence system performance. We investigate a set of external factors on the NER dataset with the two best models: CLSTM+WLSTM+CRF and CCNN+WLSTM+CRF. Pretrained embedding. Figure 4(a) shows the F1-scores of the two best models on the NER test set with two different pretrained embeddings, as well as the random initialization. Compared with the random initialization, models using pretrained embeddings give significant improvements (p < 0. 01). The GloVe 100-dimension embeddings give higher F1-scores than SENNA (Collobert et al. , 2011) on both models, which is consistent with the observation of Ma and Hovy (2016).",
        "GPT2_formal_text": "ing() function. For the first method, we train a classifier using a well-known dataset like the one from Devlin et al. (2019). If the labels are aligned, the classifier gets a high score. But if they're not aligned, it gets a low score. To make sure the classifier can tell the difference between good and bad labels, we train it using the unaligned data. To make sure the classifier doesn't just spit out labels from the dataset without any context, we set some rules. We take out the label values for words that show up at least 20 times in the dataset, or words that have at least 20 different POS tags, or words that share the same POS tag. Formal: The model is trained on a dataset called D = {d_s1, ..., d_sT}, where s_s1, ..., s_ts are the start, end, and stop positions, respectively. We use a simple linear mapping, let's call it E_M, between the word embeddings and the POS tags. Formal: Formal: The final classifier is trained using the alignment function we talked about earlier, which is defined in Algorithm 1. Formal: We fine-tune the model by trying to minimize the cross-entropy loss, which is what we call the cross-entropy loss. Formal: In this paper, we use the cross-entropy loss as the main loss objective, but we also add some non-linear features to make it better at handling noisy input data. Formal: Check out Figure 2 for a quick overview of the setup. Formal: For instance, we train the model to score labels like {n_s1, ..., n_ts} using the cross-entropy loss. Formal: The cross-entropy loss is calculated using a function f(x, y), where x is the context vector for a word x_i in the embedding space, and y is the alignment vector for a word y_i in the POS tag space. Formal: To get a better understanding of how the cross-entropy loss and the alignment function work together, we make sure the model's cross-entropy loss follows these rules: Formal: Formal: Formal: The alignment function f(x, y) is calculated using a function g(x_i, y) that's also based on a cross-entropy loss. Formal",
        "directory": "C18",
        "filename": "C18-1327.json"
    },
    {
        "casual_text": "Alright, let's break this down in simpler terms. We've got a variable x in a set called X. Now, when you see x with a number as a subscript (like x1) on the right side of a rule, it's referring to a specific part of x. But if you see x with a letter as a subscript (like xi), it's just a general placeholder, not pointing to any particular part of x.",
        "formal_text": "x an arbitrary variable in X. Because numerically subscripted versions of x appear explicitly on the right hand side of the rule defining variables, numerically subscripted variables (e. g. , x 1 ) on the right-hand side of all rules are taken to refer to the specific elements of x, whereas otherwise subscripted elements (e. g. , x i ) are taken generically.",
        "GPT2_formal_text": "= {query, id} = [q1, q2, ..., qK, id] . Formal: In this paper, we're introducing a new way to combine question and document representation by combining the input representations and using an attention-based neural network to create a meaningful representation for both the question and the document. Formal: In this paper, we're introducing a new way to combine question and document representation by combining the input representations and using an attention-based neural network to create a meaningful representation for both the question and the document. Formal: We're introducing a new way to combine question and document representation, which uses a mix of document-specific and query-specific attention. This approach helps the model handle document-specific and query-specific information better. Formal: We're introducing a new way to combine question and document representation, which uses a mix of document-specific and query-specific attention. This approach helps the model handle document-specific and query-specific information better. Formal: We're introducing a new way to combine question and document representation, which uses a mix of document-specific and query-specific attention. This approach helps the model handle document-specific and query-specific information better. Formal: We're introducing a new way to combine question and document representation, which uses a mix of document-specific and query-specific attention. This approach helps the model handle document-specific and query-specific information better. Formal: We're introducing a new way to combine question and document representation, which uses a mix of document-specific and query-specific attention. This approach helps the model handle document-specific and query-specific information better. Formal: We're introducing a new way to combine question and document representation, which uses a mix of document-specific and query-specific attention. This approach helps the model handle document-specific and query-specific information better. Formal: We're introducing a new way to combine question and document representation, which uses a mix of document-specific and query-specific attention. This approach helps the model handle document-specific and query-specific information better. Formal: We're introducing a new way to combine question and document representation, which uses a mix of document-specific and query-specific attention. This approach helps the model handle document-specific and query-specific information better. Formal: We're introducing a new way to combine question and document representation, which uses a mix of document-specific and query-specific attention. This approach",
        "directory": "E06",
        "filename": "E06-1048.json"
    },
    {
        "casual_text": "In Section 5.2, we talked about how BART can handle Dynamic Blocking without needing any special task-adaptation or self-supervision. But, this approach doesn’t work as well because it doesn’t have the same level of quality, especially when it comes to keeping the syntax varied. This is because it wasn’t trained using the shuffling strategy during task-adaptation. \n\nWe also noticed that when BART isn’t fine-tuned, it tends to try and generate different forms of a word when it’s blocked. To fix this a bit, we use a pattern library to list out all the different forms of a word that need to be blocked. For example, for the word \"give,\" we’d also block \"gives,\" \"gave,\" \"giving,\" and \"given.\" This method works for most languages that have inflections. \n\nWe’ve included a comparison in Table 12, showing the output candidates for a specific example both with and without blocking these inflections.",
        "formal_text": "Block Inflections In Section 5. 2, we mentioned that BART can directly work with Dynamic Blocking without task-adaptation or self-supervision, but that results in lower quality, especially lacking syntactic variance because it is not trained with the shuffling strategy during task-adaptation. In addition, we found that without finetuning, BART tries to generate inflections of a word when it is blocked. To partially remedy this drawback, we use the pattern library 21 to enumerate all inflections of a word to block (e. g. for \"give\" we should also block \"gives\", \"gave\", \"giving\" and \"given\") in addition to all the other blocking schemes introduced in Section 3. This is available for most languages that involve inflections. We show in Table 12 the output candidates of a selected example with and without blocking inflections.",
        "GPT2_formal_text": "_v1 and attention_mask_v2 are set to 0, and the other parameters are set to 100. Formal: Basically, the hidden state H(e) (which comes from the hidden layer) is the value of the output that comes from the e-th token, which is a combination of h(e) and the word embedding for the e-th token. Formal: The hidden state H(e) is the value of the output from the e-th token, which is a combination of h(e) and the word embedding for the e-th token. Formal: Formal: The loss is calculated as the difference between the actual label and the predicted one. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.417.json"
    },
    {
        "casual_text": "• BiLSTM-tok: This is a popular bidirectional LSTM model used for labeling sequences, like part-of-speech tagging (Plank et al., 2016) and named entity recognition (Panchendrarajan and Amaresan, 2018). We tweaked the settings based on the development set to get the best performance on each of the evaluation datasets.",
        "formal_text": "• BiLSTM-tok: Widely-used bidirectional LSTM architecture for sequence labeling, which has been applied to many tasks including part-of-speech tagging (Plank et al. , 2016) and named entity recognition (Panchendrarajan and Amaresan, 2018). We also tuned the hyperparameters based on the development set in order to achieve the best results on each of the evaluation datasets.",
        "GPT2_formal_text": "= [i_i, j_j] for i and j, respectively. This setup ensures the model can handle different types of text and not get confused by unimportant or irrelevant text. Formal: Basically, we take the sentence and make it sound like the original text by using a bunch of different tricks, like masking words, adding stuff that’s not in the sentence, and tweaking the sentence’s structure. Formal: We also use a method called Longformer (from Vaswani et al., 2017) to create the final speech. Formal: We’ve got two types of loss functions here: an internal loss and an external loss. For the internal loss, we use BERT’s masked language modeling (MLM) feature (Devlin et al., 2019). For the external loss, we use a tweaked version of the masked language modeling (MLM) (Mullenbach et al., 2018) to handle the external representation of the input. Formal: The total loss for the whole setup is L_N, where N is the total number of sentences. Formal: We use the same number of parameters as BERT to set up the parameters of the model. The model is trained using the Adam optimizer (Kingma and Ba, 2014) with a learning rate of 10^-8 and a dropout rate of 0.1. Formal: Formal: We check how well the model performs by calculating the mean and standard deviation. Formal: The model learns to create the speech using the final hidden representation, which we call the final_encoder. Formal: Formal: Formal: We also calculate the average perplexity (AP) for all the outputs, which we call the decoder_average_pars. Formal: Formal: Formal: Finally, we combine the decoder_average_pars and decoder_average_pars using a random matrix to get the final output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "coling",
        "filename": "2020.coling-main.335.json"
    },
    {
        "casual_text": "Web forums like Stack-Overflow, Quora, and Yahoo! Answers usually group their stuff into topic-based sections. In these sections, people ask questions, and other users jump in with comments trying to answer. Sometimes, the list of comments gets really long. A lot of these forums don't have moderators, so the content can get messy and repetitive.",
        "formal_text": "Question answering web forums such as Stack-Overflow, Quora, and Yahoo! Answers usually organize their content in topically-defined forums containing multiple question-comment threads, where a question posed by a user is often followed by a possibly very long list of comments by other users, supposedly intended to answer the question. Many forums are not moderated, which often results in noisy and redundant content.",
        "GPT2_formal_text": "ed is a vector of size d in R^d, and it's the length of the output sequence that's being masked. Formal: This part is about the masked language modeling thing. There are a bunch of ways people have tried to build this into a sentence model, like using fancy word embedding models (like those by Mikolov et al., 2013; Pennington et al., 2014; Lample et al., 2016; Peters et al., 2017; Schuster et al., 2018; Cai and Lampert, 2019; Luong et al., 2019), neural network models (like those by Chen et al., 2019; Cai et al., 2019; Luong et al., 2019a, b; Cheng et al., 2020; Wu et al., 2020), and language models (like those by Wang et al., 2018; Poria et al., 2020; Liu et al., 2021). The big challenge with all these approaches is figuring out how to get the masked words and their embeddings to align perfectly for a smooth and natural-sounding translation. Formal: To deal with this, we're introducing our own model called skipgram. Formal: From the start, skipgram has two main goals: first, we want to make sure the text is smooth and fluent. Second, we want to ensure the embeddings of the words don't get messed up by the masked tokens. Formal: In this paper, we’re introducing skipgram, which is a model that can figure out the hidden patterns in text and use that to create more natural-sounding sentences. Formal: We’ve built skipgram using a skipgram model that uses a block-diagonal attention mechanism (credit to Vaswani et al., 2017). Formal: In this paper, we’re also introducing skipgram, which is a model that uses a hierarchical attention mechanism to learn from the masked words and their embeddings. Formal: Here’s the link to skipgram: https://github.com/nltk/skipgram Formal: We’ve also done some experiments to test skipgram on different text generation tasks. Formal: Here are the links to skipgram: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D18",
        "filename": "D18-1452.json"
    },
    {
        "casual_text": "You can find the code and data here: https://github.com/gsarti/interpreting-complexity",
        "formal_text": "Code and data available at https: //github. com/ gsarti/interpreting-complexity",
        "GPT2_formal_text": "= {t j } n = 5: n is the number of views, P(z j = k | y i, y j | t j ) is the probability that the kth view in the k-th layer is chosen, and λ is the penalty term, which we'll talk about more in Section 4. Formal: We pick the k-th view based on the model's output. After that, we calculate the probability of each possible answer by multiplying it with the probability of the jth view chosen by the model. Formal: Let’s say we have M words in a document, and M + 1 is the set of all the words in the document. Then, we calculate the probability of picking any word in this set as P(z j = k | y i, y j | t j ). Formal: The model creates an image for each word, using a pre-trained model that we’ve trained. Formal: Following the approach in (Nguyen et al., 2017), the model’s output is a feature vector that includes a feature vector for the word we’re looking at. Formal: The model gets a score based on how well it handles the words it picks. Formal: The model then assigns a probability to each of these words. Formal: Lastly, we use a clustering method to group these words into clusters, which are similar to how humans group words into groups (Zhao et al., 2017). Formal: This clustering step ensures that words in the same cluster, even if they share the same meaning, end up in different clusters. Formal: We use a Gibbs sampler with a uniform random seed to group the words into clusters, creating a kind of informal sentence, like this: Formal: If the model’s output gives us an image for the word we’re looking at, then P(z j = k | y i, y j | t j ) equals 1, which means the word is selected for the k-th layer. Formal: After that, we calculate the probability of each answer using this: Formal: The model calculates a score for each possible answer, P(z j = k | y i, y j | t j ) using the model’s output. Formal: Formal: Finally, we assign a probability to each cluster based on the chosen answer. Formal: Formal: Finally, we use a clustering method",
        "directory": "cmcl",
        "filename": "2021.cmcl-1.5.json"
    },
    {
        "casual_text": "Okay, so the initial plan was to use the BLEU metric (Papineni et al., 2001) for evaluation since it's a pretty common way to evaluate machine translation. But, turns out, a lot of people think it’s not the best for languages that have a lot of inflections, like Slovenian and Serbian. Plus, the BLEU metric tends to give lower scores to rule-based machine translation (RBMT) systems, as mentioned by Callison-Burch et al. (2006) and Labaka et al. (2007). So, we decided not to use it.\n\nInstead, we looked into METEOR, which its creators say fixes many of the issues with BLEU and is more in line with how humans judge translations. The only problem? METEOR didn’t support our language pair, so we had to write some extra software to make it work.\n\nFor the actual evaluation, we used a bilingual parallel corpus (Dimitrova et al., 1998) to automatically check the translations.",
        "formal_text": "Preliminary evaluation plans included the evaluation using the BLEU (Papineni et al. , 2001 ) metric as it is one of the most used machine translation evaluation metrics. Many authors agree that Slovenian monolingual dictionary: <e lm=\"list\"><i>list</i><par n=\"žvenket/__n\"/></e> Slovenian -Serbian bilingual dictionary: <e><p><l>list<s n=\"n\"/><s n=\"m\"/></l> <r>list<s n=\"n\"/><s n=\"m\"/></r></p></e> Serbian monolingual dictionary: <e lm=\"list\"><i>list</i><par n=\"um/__n\"/></e> Figure 5: The extension of the bilingual corpus. First example shows a Slovenian lemma list, leaf, and the additional entries in the Serbian monolingual and SL-SR bilingual dictionaries Slovenian monolingual dictionary: <e lm=\"list\"><i>list</i><par n=\"žvenket/. . n\"/></e> SL-MK bilingual dictionary: <e><p><l>list<s n=\"n\"/></l><r> <s n=\"n\"/></r></p></e> Macedonian monolingual dictionary: <e lm=\" \"><i> </i><par n=\" /. . n\"></e> BLEU metric systematically penalises RBMT systems (Callison-Burch et al. , 2006; Labaka et al. , 2007) and it is not suited for highly inflective languages. The evaluation using the BLEU metric was not performed. Authors of METEOR state that their system fixes most of the problems encountered using the BLEU metric; they state that METEOR correlates highly with human judgement. Unfortunately METEOR did not support our language pair, additional software had to be written. The bilingual parallel corpus (Dimitrova et al. , 1998) was used in automatic evaluation of translations.",
        "GPT2_formal_text": "_len = arg max (arg max (|y|), arg min (|y|) ) # define the attention weights for the mask. mask_len = arg max (|y|) # the length of the mask. arg min (|y|) = arg max (|y|) # the length of the entity mask. mask_len_max = arg max (|y|) mask_len_min = arg max (|y|)\n\nWe use a text classification task to create the data for our multi-label entity masking task. In this task, we have three labels: (1) Person (PER), (2) Location (LOC), and (3) Organization (ORG). We train our model using the Google Prediction API.\n\nWe train our model using the validation set, which includes the top ten predictions. The goal is to maximize the F1 score for the prediction on the test set, which we calculate using the training set. To do this, we add a few extra features:\n\n1. The entity mask token.\n2. The entity span token.\n3. The entity type token.\n4. The entity type count token.\n5. The entity type position token.\n6. The entity type entity token.\n\nFor the final classification, we just pick the best prediction using a linear classifier. Formal: The mask token is the main entity token, the entity mask token is the mask token, the entity type token is the entity type token, the entity type count token is the entity type count token, and the entity type position token is the entity type position token. Formal: We evaluate the model using the F1 score for the correct prediction, which we get by using the validation set of the predictions. Formal: We use logarithm with the argmax function to figure out the masking probability. Formal: The mask token is the main entity token, the entity mask token is the mask token, the entity type token is the entity type token, the entity type count token is the entity type count token, and the entity type position token is the entity type position token. Formal: We evaluate the model using the F1 score for the correct prediction, which we get by using the validation set of the predictions. Formal: We use logarithm with the argmax function to figure out the masking probability. Formal:\n\nThis setup is similar to the entity",
        "directory": "eamt",
        "filename": "2010.eamt-1.24.json"
    },
    {
        "casual_text": "Alright, let's break down how we can capture the impact of this fancy control agreement principle (CAP). PSG uses Montagovian semantic type assignments to explain CAP, but since we haven't talked about semantics, we'll just assume the necessary type info is already in our feature structures. With that in mind, our CAP setup has three main parts: first, we need to define what a controller and a controllee (or target, as GPSQ calls it) are; second, we define what a control feature is; and third, we explain the instantiation principle. Let's go through each step.\n\nFirst, let's define controller and controllee: A category C is controlled by another category C~ in a constituent Co if, at a semantic level, one of these two things happens: either C is a functor that combines with C~ to make Co, or there's a control mediator C\" that joins C and C~ in that specific order to create Co.",
        "formal_text": "Finally, we sketch how the effect of the more elaborate control agreement principle (CAP) can be captured. ~PSG formulates CAP by making use of the Montagovian semantic type assignments. As we haven't discussed semantics, we're going to assume that the relevant type information is available inside our feature structures. With this assumed, our formulation of CAP falls into three steps: first, defining the notions of controller and controllee (or target in GPSQ terminology); second, defining the notion of a control feature; and third, defining the instantiation principle. We consider each in turn. Controller and controllee are defined as follows: 14 A category C is controlled by another category C ~ in a constituent Co if one of the following situations obtains at a semantic level: either C is a functor that applies to C ~ to yield a Co, or else there is a control mediator C\" which combines with C and C ~ in that order to yield a Co.",
        "GPT2_formal_text": "and maybe even the title and abstract. We’ll call this method `COPY_TRAIN`. Formal: The model, called `TRAIN`, is trained using a combo of methods called `PD` and `ER`. Formal: The training goals are set up to maximize the probability of generating the target sequence `t`. Formal: The model is trained using two main goals: `PRED` and `ER`. Formal: To boost performance, the model can be adjusted to predict both `t` and `t`. Formal: This adjustment is based on the results from the test set. Formal: The model is trained using a learning rate schedule called `learning` and a reward function `prod`. Formal: The model is trained using a reward function `prod` and a learning rate schedule `learning`. Formal: Each prediction is made up of a prediction probability and a probability distribution `p_t`. Formal: The loss function `L` is a standard model loss function. Formal: If the model is already trained, the loss function `L` is set to `L t` and the reward function `prod` is `prod t`. Formal: The learning rate schedule `learning` is also a standard learning rate schedule. Formal: The learning rate schedule `learning` is also a learning rate schedule. Formal: The parameter `|e, t|` is a function `|e, t|` that adjusts the parameters `|e, t|` during training. Formal: The loss function `L` is also a standard loss function. Formal: The reward function `Prod` is another standard reward function. Formal: The loss function `L` is a set value `|e, t|` that changes depending on the training data. Formal: The parameter `|e, t|` is a function `|e, t|` that adjusts the parameters `|e, t|` during training. Formal: The parameters `|e, t|` are set in a learning rate schedule `learning` that adjusts the parameters `|e, t|` as it goes through the training process. Formal: The loss function `L` is a set value `|e, t|` that changes depending on the training data. Formal: The parameters `|e, t|` are set in a learning rate schedule `learning` that adjusts the parameters",
        "directory": "E93",
        "filename": "E93-1004.json"
    },
    {
        "casual_text": "• and then figure out how to say it in the target language based on that interlingua stuff.",
        "formal_text": "• and to derive a surface structure of the target language from the interlingua expression.",
        "GPT2_formal_text": "ing.py :\n        Casual: Since word embeddings are a super useful tool in NLP, people have come up with ways to train models to work with them. Some researchers have looked into how to use the vector space model for non-linear stuff, like regression tasks (Chen et al., 2017; Lample et al., 2018b). But here's the thing: our approach is different because we're not focusing on learning a single, fixed vector space model. Instead, we're learning a type of vector space model that we’ll use to handle the problem of compressing sentences. Formal: We’ve created a new dataset with 3,715 sentences from TAC KBP, a dataset created by Pascanu et al. (2016). This dataset includes 6,998 sentences from Wikipedia articles and focuses on compressing sentences. It’s got 565 sentences in total, and each sentence has just one or two sentences. Formal: We’ve also started a bunch of experiments to see how well our method works. Formal: We’ve tested a bunch of NLP models and language models (like BERT, GPT-2, XLNet, and Longformer) on this dataset. Formal: We’re sharing results for these NLP models and models, along with a few experiments we did, for anyone to see. Formal: If you want more details about the dataset, including the algorithm we used, you can check out the original paper here. Formal: Formal: You can find the code for our experiments here. Formal: Table 1 shows the size of the dataset, and Table 2 gives some examples of sentences compressed by different models. Formal: We’ve shared all the datasets and code here. Formal: We’ve included the original paper with the code here. Formal: We’ve also shared results for these NLP models and models, along with a few experiments we did, for anyone to see. Formal: Formal: If you want more details about the dataset, including the algorithm we used, you can check out the original paper here. Formal: Formal: We’ve shared all the datasets and code here. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C90",
        "filename": "C90-2061.json"
    },
    {
        "casual_text": "The difference between the scores of the actual correct answer, which we'll call x_i, and a possible but not quite right answer, x_i,j, is what we're looking at here. The hinge loss is used to make sure that the correct answer's score is at least τ points higher than the score of the not-so-great option. We're calling the language model we train using this method the Large Margin Language Model (LMLM).",
        "formal_text": "is the margin between the scores of a ground-truth x i and a can-didate x i, j. The hinge loss on the margin encourages the log-likelihood of the ground-truth to be at least τ larger than that of the imperfect hypothesis. We call an LM trained by the above formulation as Large Margin Language Model (LMLM).",
        "GPT2_formal_text": "= t in t_i) is basically the probability p(t|s), based on the input sentence s. So, we can say: Formal: x_i = P(t|s), where P(t|s) is the probability of t happening given s. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D18",
        "filename": "D18-1150.json"
    },
    {
        "casual_text": "In SAMT, they use a bunch of CCG-like binary operators to boost the number of nonterminal labels. This is important because SAMT labels come from phrase structure grammar, which only has a limited set of labels that can't cover all the different syntactic structures of the phrases they extract. Almaghout et al. (2010) tried using single-category CCG labels as nonterminal labels. While CCG's flexible structures give better coverage than phrase structure grammar labels, using just single-category CCG labels still leaves about a third of the phrases unlabeled.\n\nTo fix this, we decided to expand the definition of nonterminal labels to include more than one CCG category. So, if there's no single CCG category at the top of the tree that can label a phrase, we look for the highest-scoring sequence of categories with the fewest CCG categories from the CCG trees that cover the phrase and use that as the label. Figure 2 shows some phrases from the sentence in Figure 1, along with their extended CCG labels.\n\nFor example, the phrase \"like cream\" has an extended CCG label made up of two categories: S[b]\\NP+conj. The CCG categories for the words \"like\" and \"cream\" are combined into S[b]\\NP. But this category can't be combined with the conj category.",
        "formal_text": "In SAMT, a set of CCG-like binary operators are used to increase the coverage of nonterminal labels. This is necessary as SAMT labels are extracted using phrase structure grammar, which has a small set of constituent labels that are insufficient to cover all the different syntactic structures of extracted phrases. Almaghout et al. (2010) use single-category CCG labels as nonterminal labels. Although CCG flexible structures allow a better label coverage than phrase structure grammar-based labels, using single-category CCG labels fails to label about one third of the total phrases. In or- der to increase label coverage, we extend the definition of the nonterminal label to be composed of more than one CCG category. Therefore, if there is no single CCG category at the root of the trees which cover a phrase, the highest-scoring sequence of categories with a minimum number of CCG categories is extracted from CCG trees covering the phrase and used as the phrase label. Figure 2 shows a set of phrases extracted from the sentence illustrated in Figure 1 along with their extended CCG labels. In this example, the phrase like cream and has an extended CCG label composed of two categories: S[b]\\NP+conj. The CCG categories of the words like and cream are combined into the CCG category S[b]\\NP. However, this category cannot be combined with the conj category.",
        "GPT2_formal_text": "ed = h_idf + l_idf; s = tf.select(s_idf, l_idf); for(i = 1; i < n; i++): if(s_idf == s_masked_m) { break; } else if(s_masked_m == s_idf + l_masked_m) { break; } else if(s_masked_m == s_masked + l_masked_m) { break; } else if(s_masked_m == s_masked + l_masked_m) { break; } else { return s; } }\n\nThis method creates a hierarchical attention network for figuring out the attention values. It does this by using a matrix called h_idf, which is a vector with dimensions n × l_masked_m. This matrix is created by combining two matrices: h_masked_m and h_masked. It also uses an l_masked_m × l_masked_m matrix to represent the attention values. Then, it adds up the results from these two matrices to get the final attention values.\n\nTo get the attention values for each word in the text, we use a weighted weighted average of these attention values. The weights for these weights are learned through a two-layer feedforward network. The learning rate for the parameters λ_d is also figured out through a two-layer feedforward network.\n\nThe model is trained using Adam (from Kingma and Ba, 2014) with a learning rate of 0.1, a regularization parameter called β, and a max number of epochs set to 5. As you can see in Figure 3 (b), the model is super efficient, making only about 14 × 10^-9 operations per second. Formal: In this part, we'll explain how to use the model, including the hyperparameters we used, along with the actual code. Formal: We used the AllenNLP toolkit (http://allenai.github.io/) and its official website (http://www.allenai.github.io/) to train the model. The training was done on a single Titan RTX 2080 Ti GPU. Formal: The model runs on a single Titan RTX 2080 Ti GPU. Formal: The model is built using a hierarchical attention network with a hidden layer dimension of",
        "directory": "eamt",
        "filename": "2012.eamt-1.44.json"
    },
    {
        "casual_text": "Plus, using a non-autoregressive model lets us create an encoder-only setup, which works better for summarization. This is because the input and output have a really close connection, and that's something encoder-decoder models, especially the autoregressive ones, can't fully handle.",
        "formal_text": "Moreover, a non-autoregressive model enables us to design an encoder-only architecture, which is more suited to the summarization task due to the strong correspondence between input and output, which cannot be fully utilized by encoder-decoder models, especially autoregressive ones.",
        "GPT2_formal_text": "= [h, t] for i, j in T):          Casual: So, we take a weighted average of the k most common words from the domain D_c (which is like a big collection of text data). This averaged word list is then fed into a multi-layer perceptron (MLP) to figure out the probability for the segmentation H(H(t)) based on the domain D_c. The model is trained with cross-entropy loss and a learning rate of 0.1. Formal: For each pair of entities, let's call them f_e and f_f, we give them a score called e_f(f_e) and e_f(f_f). Formal: Once we get that score for each pair of entities, we calculate the probability P(e_f(f_e) |e_f(f_f)) for the segmentation H(H(t)) based on the domain D_c. Formal: For the whole domain D_c, we use a standard logistic regression model to figure out the probability P(e_f(f_e) | e_f(f_f)) for the segmentation H(H(t)) in the context of the domain D_c. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.545.json"
    },
    {
        "casual_text": "In the MRC field, some earlier studies focus on general-purpose language modeling, but they require a lot of computational power to encode aspects (Zhang et al., 2020c). Others just combine texts from different MRC tasks to increase the size of the training dataset.",
        "formal_text": "In MRC field, several previous works perform general-purpose language modeling with considerable computing cost at encoding aspect Zhang et al. , 2020c), or splice texts among diverse MRC tasks simply to expand training dataset",
        "GPT2_formal_text": "ed_multi is like a tool that turns a sentence into a vector, where the number of \"disfluencies\" (like long pauses or typos) gets cut in half. This setup helps figure out the \"mention\" vector, which then helps with the prediction for the \"citation_attributed\" attribute. Formal: To get the prediction for a specific attribute, like \"mention_labeled_multi,\" we use this formula: Formal: To get the prediction for a specific attribute, like \"mention_labeled_multi,\" we use this formula: Formal: Formal: Here, 'r' stands for the recall, and 'c' is the precision. Formal: To get the prediction for a specific attribute, like \"mention_labeled_multi,\" we use this formula: Formal: Formal: For the first round, we focus on the most common citation phrases (like \"Most Recent\") and the top-1 citations from the abstracts of the papers we're looking at. Formal: We use the top-N citation phrases and the top-1 abstracts from the abstracts of the papers we're looking at. Formal: For the second round, we only look at the top-N citation phrases and the top-1 abstracts from the abstracts of the papers we're looking at. Formal: Formal: Formal: Here, 'r' is the recall, and 'c' is the precision. Formal: Formal: Formal: Then, we average these two numbers to get the prediction for the attribute. Formal: Formal: Formal: Formal: This gives us the prediction for \"mention_labeled_multi.\" Formal: Formal: Formal: Formal: Here, 'r' is the recall, and 'c' is the precision. Formal: Formal: Formal: Formal: Finally, we use this formula to get the prediction for the attribute. Formal: Formal: Formal: This gives us the prediction for \"mention_labeled_multi.\" Formal: Formal: Formal: Formal: We also figure out the citation length for the attribute by using this formula: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2022.acl-long.594.json"
    },
    {
        "casual_text": "Online social networks have gotten a lot of attention from researchers, especially when it comes to entity linking in these spaces. (Huang et al., 2014; Shen et al., 2013) came up with methods specifically made for linking named entities in tweets. The main challenge they tackle is that tweets are usually short and casual, but they also try to make use of any extra info that tweets might offer. For instance, (Shen et al., 2013) figured that each user's tweets reflect some kind of interest pattern and created a graph-based algorithm to rank entities based on this interest distribution. (Huang et al., 2014) also used a method with meta-paths on HIN in their entity linking approach, but they just used it to see if two mentions are connected. One thing all these studies have in common is that, even though they focus on tweets, they still link the entities to knowledge bases as the final goal.",
        "formal_text": "The prevalence of on-line social networks has also motivated researchers to study entity linking in such environments. (Huang et al. , 2014; Shen et al. , 2013; proposed methods that are specially designed for linking named entities in tweets. They mainly address the problem that tweets are usually short and informal, while taking advantage of some of the extra information that tweets may provide. For example, (Shen et al. , 2013) assumed that each user's tweets have an underlying interest distribution and proposed a graph based interest propagation algorithm to rank the entities. (Huang et al. , 2014) also used meta-path on HIN in their entity linking approach, but they only used it to get an indication of whether two mentions are related. Finally, although these studies focused on entity linking for tweets, they still use entities in knowledge bases as the target.",
        "GPT2_formal_text": "= {q i } n j=1\n         Casual: We've built a dataset of 200k dialogue acts for event extraction. Formal: We're training a basic model with a bunch of samples and then testing it on a real-world dataset. We're comparing it to a few other methods, and here's what we found: Formal: For the dev set, our model works better than the other ones, except when it comes to argument pairs. But when it comes to the test set, our model shines compared to the others, especially when it comes to argument pairs. Formal: When we’re looking at the dataset, we see that our model consistently outperforms the others, whether we’re training on real data or the dev set. This suggests that our model is doing a good job of capturing the event context, and using the reasoning skills it learns to make predictions. Formal: We’ve also included some examples of how our model did in different scenarios. Formal: The results we’re reporting show that our model is strong, particularly when it comes to argument pairs, even when we’re training on an unseen dataset. Formal: Formal: We’ve also compared our model’s performance to some other methods, and here are some of the results we got: Formal: We’ve got two datasets to work with—one with 400k dialogue acts and the other with 200k. Formal: The results we’re reporting are for the dev set. Formal: Formal: Formal: This means we’re testing our model on a real-world dataset, and our results are pretty good. Formal: Formal: Formal: When we’re testing on the dev set, the results are really good. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D18",
        "filename": "D18-1227.json"
    },
    {
        "casual_text": "Word-based methods have an edge because they can pick out important words in a sentence more easily than bunsetsu-based methods. This is because word-based methods don’t worry about bunsetsu and aren’t restricted by unit limitations.",
        "formal_text": "The advantage of word-based methods is that the methods can more freely select important words in a source sentence than bunsetsu-based methods. This is because word-based methods do not take account of bunsetsu in a source sentence and are not limited to unit constraints.",
        "GPT2_formal_text": "ed. pyc.stanford.edu/resource/en/\n          Casual: Using the attention mechanism for generating responses can make the model more accurate, but it also makes the responses more repetitive and choppy. For example, if the model uses attention to create a longer response, the model might end up repeating the same words over and over. Also, the model might forget important details from the input when generating a new response. This could mess up the context and make the generated response kind of generic or even a complete lie. To prevent this, we use a rule-based system to figure out the right structure for a response. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C12",
        "filename": "C12-1067.json"
    },
    {
        "casual_text": "Glow, introduced by Kingma and Dhariwal in 2018, is a type of normalizing flow that transforms data into a simple, known prior, like a spherical multivariate Gaussian distribution. It’s trained using the exact log-likelihood of the data and, as a generative model, it’s really good at capturing all the dependencies in high-dimensional data, which often comes in the form of a complex, multimodal probability distribution. To keep things fair when comparing it to other methods, we only use the encoder’s hidden states as the condition for each flow step and stick with a spherical Gaussian prior, just like in the original Glow.\n\nThrough some experiments, we noticed that a 2D-CNN-based network works better than a 1D-CNN, especially when it comes to reconstructing high-frequency details. As you can see in Figure 4d, our Glow-based decoder models the distribution of mel-spectrograms based on the encoder’s output hidden states x. This decoder is made up of k flow steps, labeled f1 to fk. Each step includes an affine coupling layer, an invertible 1x1 convolution, and an activation normalization layer. Essentially, the Glow-based decoder takes random variables from a spherical Gaussian distribution and turns them into mel-spectrograms.",
        "formal_text": "Glow (Kingma and Dhariwal, 2018) is a kind of normalizing flow, which maps data into a known and simple prior (e. g. , spherical multivariate Gaussian distribution). Glow is optimized with the exact log-likelihood of the data and as a generative model, it is very good at modeling all dependencies within very high-dimensional input data, which is usually specified in the form of a dependent and multimodal probability distribution. To make a fair comparison with other modeling methods, we only take the output hidden states of the encoder as the condition to each step of flows and use spherical 13 Through experiments, we find that the 2D-CNN-based network performs better than 1D-CNN in discriminator, especially for the reconstruction of high-frequency details. Gaussian as the prior distribution like in the vanilla Glow (Kingma and Dhariwal, 2018). As shown in Figure 4d, our Glow-based decoder models the distribution of mel-spectrograms conditioned on the encoder output hidden states x. The Glow-based decoder is composed of k flow steps f 1, . . . , f k, each of which consists of an affine coupling layer, an invertible 1x1 convolution, and an activation normalization layer. Glow-based decoder maps the spherical Gaussian random variables z to the mel-spectrograms as",
        "GPT2_formal_text": ", q_t_i and q_t_j, where i and j are the jth and 29th words in the sentence, respectively. The output vector is normalized by the logit to get the final logit value. Formal: If there are no spelling mistakes or word forms that don’t match the characters in the transcript, the model’s output looks like this: Formal: For the sake of comparison, we also made a test set where we skip words that don’t have any spelling errors or word forms. Formal: The generator’s output is written in the form of a string with the word and its position in the sentence. The system tries to guess the ending based on the length of the input sentence. Formal: Formal: To check how well the model handles missing words, we compare its output to the human-made predictions on the test set. Formal: Formal: We tested our model using the MSCOCO dataset (Lin et al., 2014) and compared it to human-made predictions in the same format (check out Section 6.1). The results are in Table 5. Formal: For the MSCOCO test set, the model’s performance is only marginally worse than the human’s. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.564.json"
    },
    {
        "casual_text": "(Self-disclosure) Yeah, I get it. I’d be super annoyed too if that happened to me.",
        "formal_text": "(Self-disclosure) I understand you. I would also have been really frustrated if that happened to me.",
        "GPT2_formal_text": "ing (H_mask) is a thing in the model. This is used to create a dictionary, called d_h, that keeps track of which tokens were masked in the original text. After that, we use the latent codes H_mask to make sure we don't mix up the original and modified versions of the input text. Formal: In this part, we'll take a closer look at how the model is set up. Formal: The model has an input layer with two attention layers, with a hidden size of 50 and a projection layer with 12. There's a hidden layer that helps the model understand and adapt to the context of the input text, and another hidden layer that focuses on adapting to the context of the masking tokens. Formal: This setup helps make the model better at handling the context of the masking tokens, which helps it pick up on and use the knowledge from the masked tokens. Formal: In the next sections, we'll take a closer look at the model and figure out what's going wrong. Formal: This issue was spotted by a method called probing, which is a way to figure out what's wrong with a model without needing a ton of human-labeled data. Formal: We found that if we tweak the model a bit to fix this problem, we can boost its performance on different datasets. Formal: The rest of this paper is organized like this: Section 2 talks about related work, Section 3 explains the model, Section 4 describes the probing method, Section 5 shows the results, and Section 6 wraps it all up. Formal: We also looked into how well the model can handle masked tokens by using a special masked token detector. Formal: We discovered that our approach can make the model more resilient to changes in the text. Formal: Formal: We also talked about how the model adapts to different contexts, and how we can use that to fix its errors. Formal: Formal: Formal: Lastly, we tested our model on a bunch of datasets and compared its performance on these tasks. Formal: Formal: Formal: The results? Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2021.acl-long.269.json"
    },
    {
        "casual_text": "Alright, so this Papillon module is all about creating user-friendly interfaces for looking at and editing dictionary entries. We’re building on some cool research by Thevenin and Coutaz from 1999, and also using a tool called ART-Studio, which was developed by Calvary and team in 2001. These guys came up with ways to semi-automatically design interfaces for different purposes. Here’s a quick rundown of the framework and models we’re using.",
        "formal_text": "This Papillon module has to generate graphic user interfaces for consulting and editing dictionary entries. We base our approach on the work done on Plasticity of User interfaces (Thevenin and Coutaz, 1999) and the tool ART-Studio (Calvary et al. , 2001). They propose frameworks and mechanisms to generate semiautomatically graphic user interfaces for different targets. Below we present the design framework and models used.",
        "GPT2_formal_text": "= (s_i, d_i) if not d_i: return (s_i, d_i) else: return (t, x)\n\nIn the first scenario, d_i is the token's representation, but in the second one, d_i is the variable for the i-th token. Both of these scenarios consider the values of the IOB2 token in the input. So, the multi-class output, t, is made up of x and x. Formal: We can combine the model's results with the existing token representations, T, to make the final prediction. Formal: First, we figure out the IOB2 value for each token, x. Then, we combine T and x to get the final output, t. Formal: The final input for the model is a sequence of tokens, x = (s_1, s_2, ..., s_n). Formal: Each token x i is represented as a matrix M_i, which is a matrix with dimensions n×d. Formal: We start with the token representation t(i) and use it to figure out the IOB2 values for the tokens. Formal: Lastly, we add the final state of the model to the sequence of tokens, x. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C04",
        "filename": "C04-1148.json"
    },
    {
        "casual_text": "Kind of like how educational testing researchers check if IRT models can accurately predict how people answer questions (American Educational Research Association, 2014), we’re checking how well DAD can predict if SQuAD models are getting questions right.",
        "formal_text": "Just as educational testing researchers validate IRT models by seeing if they predict subject responses correctly (American Educational Research Association, 2014), we validate how well DAD predicts whether SQuAD models get questions right.",
        "GPT2_formal_text": ". The last two rows of Table 4 show that the best results come from using the binary classifier, which works really well for predicting the grade level. The reason it works so well is that this binary classifier is really good at showing which class a student belongs to. The learning rate is set to 0.5 to make sure it doesn't get too influenced by the data. Formal: As mentioned in Section 3, different models can give different results for the binary classification task. The issue is that these models might be using different features or different ways of sampling data. So, it's crucial to pick the right model for the task. Formal: Basically, the batch size is the number of samples a model can produce. Formal: We used our model, which was trained on the NCIS dataset, to predict the grade level for students in different grades in the K-12 system. Formal: We also looked at the BERT model (Devlin et al., 2019) to predict the grade level for students in different grades in the K-12 system. Formal: To get the gradients, we run the model through a linear layer to create a vector for the predicted grade level. Formal: Formal: Using the validation set to train the model and the test set to test it, we can get the best results by combining both sets of data. Formal: Our experiments show that our method works really well. Formal: Our system can improve how well the system predicts the grade level and also predict the actual grade level. Formal: The results from our system suggest that using multiple labels or data from different sources helps improve accuracy. Formal: We also thought about using a transformer model for the task. But since it's super slow to train, we decided not to go that route. Formal: Formal: We tested our model on the NCIS dataset and got some pretty good results. Formal: We ran the model on the K-12 system and got similar results as the results from the NIS dataset. Formal: Formal: Formal: We tested our model on the NIS dataset and got similar results as the results from the NCIS dataset. Formal: Formal: Formal: We ran the model on the K-12 system and got similar results as the results from the NIS dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.346.json"
    },
    {
        "casual_text": "Tree kernels look at trees by focusing on smaller parts of them, called substructures or fragments. The kernel function checks if a part of a tree, which is shared by two trees, fits into the feature space we're trying to create. To do this, we need to describe the specific fragments we're interested in. We mainly look at two types: syntactic tree fragments (STF) and partial tree fragments (PTF).",
        "formal_text": "Tree kernels represent trees in terms of their substructures (fragments). The kernel function detects if a tree subpart (common to both trees) belongs to the feature space that we intend to generate. For such purpose, the desired fragments need to be described. We consider two important characterizations: the syntactic tree (STF) and the partial tree (PTF) fragments.",
        "GPT2_formal_text": "= {r k } n = (r k + 1) + 2 n t = (r k + 1) + 3 t n t s = (r k + 1) + 4 t n t s t j = (r k + 1) + 5 t s t j t k = (r k + 1) + 6 t n t s t k t l = (r k + 1) + 7 t s t l t s t j = (r k + 1) + 8 t l t s t k t h = (r k + 1) + 9 t n t s t k t l t h t = (r k + 1) + 10 t l t s t k t l t h t j = (r k + 1) + 11 t k t l t s t h t j t k t l = (r k + 1) + 12 t l t s t k t l t h t j t k t l t s t k t l t h t j t l t k t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h t j t l t h",
        "directory": "D09",
        "filename": "D09-1112.json"
    },
    {
        "casual_text": "Basically, if a summary doesn't match up with the original text when you ask a question and the answer changes depending on whether you're looking at the summary or the source, then it's considered inconsistent. So, we figure out how accurate the summary is by calculating something called the precision score.",
        "formal_text": "A summary is deemed inconsistent w. r. t. its source text if, given a question, the answer differs when conditioned on S or D. Therefore, we define the precision score for the evaluated summary as:",
        "GPT2_formal_text": "_1 = { 0.8, 0.9, 0.98, 0.99 } s = { { 0.9, 0.9, 0.98, 0.99 }, { 0.8, 0.8, 0.9, 0.99 }           Casual: We're using a GRU model with 5 hidden layers, and the activation function is set to sigmoid. We train our model using the TAC KBP dataset (Klein et al., 2008). We optimize everything using stochastic gradient descent, following the Adam optimizer. Formal: Since we're mainly looking at the average attention score for the feature set X, we treat the binary cross-entropy loss as a scalar, which is just a standard embedding loss. Formal: Let's break down the attention-based attention loss, which we call L_A. Formal: L_A is calculated as s = (s_1 * s_2 * ... * s_m * ... * s_N) for a given x_i, y_i, and a_i. Formal: The attention score s_i,j is the average attention score for feature y_i,j. Formal: For example, the attention scores for the positive aspect of \"e1\" and the negative aspect of \"e2\" in Figure 1 are calculated as s_1 = (s_1 * s_2 * ... * s_m * ... * s_N) and s_2 = (s_2 * ... * s_m * ... * s_N). Formal: Formal: Lastly, we calculate the final loss L_KP to show how well the model can correctly identify the positive and negative aspects of a pair of entities. Formal: The final loss L_KP is calculated as s = (s_1 * s_2 * ... * s_N) for a given x_i, y_i, and a_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.529.json"
    },
    {
        "casual_text": "Next, for each candidate Pj, we figure out the probability p(x, Pj) that the query x is referring to the same thing as Pj.",
        "formal_text": "Then, for all candidates P j we compute the probability p(x, P j ) that the query x corefers with as",
        "GPT2_formal_text": "', { '<textarea>', '<textarea-list>', '<textarea-title>', '<textarea-subtitle>', '<textarea>', '<textarea-item>', '<textarea>', '<textarea-meta>', '<textarea>', '<textarea-method>', '<textarea>', '<textarea>', '<textarea-value>', '<textarea>', '<textarea>', '<textarea-default>', '<textarea>', '<textarea>', '<textarea>', '<textarea-range>', '<textarea>', '<textarea>', '<textarea-number>', '<textarea>', '<textarea>', '<textarea-pre>', '<textarea>', '<textarea>', '<textarea>', '<textarea-post>', '<textarea>', '<textarea>', '<textarea-date>', '<textarea>', '<textarea>', '<textarea-date>', '<textarea-year>', '<textarea>', '<textarea-year>', '<textarea-month>', '<textarea>', '<textarea-month>', '<textarea-year>', '<textarea-month>', '<textarea-day>', '<textarea>', '<textarea-day>', '<textarea-date>', '<textarea-year>', '<textarea-date>', '<textarea-month>', '<textarea-year>', '<textarea-month>', '<textarea-date>', '<textarea-year>', '<textarea-month>', '<textarea-year>', '<textarea-year>', '<textarea-month>', '<textarea-year>', '<textarea-month>', '<textarea-year>', '<textarea-year>', '<textarea-month>', '<textarea-year>', '<textarea-month>', '<textarea-year>', '<textarea-year>', '<textarea-month>', '<textarea-year>",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.382.json"
    },
    {
        "casual_text": "We looked at the data from both English and Chinese search engines, focusing on a part of the AOL dataset. Table 2 shows the stats for different categories from both AOL and Sogou logs. Just to clarify, a \"click set\" is all the clicks tied to a specific query, not just one unique query. \n\nWhen it comes to using general user info versus individual user info, there’s no big difference in the number of records or users in both logs. But if you look at unique queries and distinct click sets, individual user info clearly wins, especially in the Sogou log. \n\nSo, even though using both common and individual user info works pretty well for half the users and half the search engine visits, the fact that there are way more unique queries and click sets shows how important individual user info is for personalizing search results. Methods that use individual user info are definitely more complex, even though some might say using common user info is easier to handle.",
        "formal_text": "The statistics of the two categories are conducted in the query log of both English and Chinese search engines. We use a subset of AOL Table 2 summarizes the statistics of different evidence categories over AOL and Sogou log. Note that click set refers to the set of clicks related to a query submission instead of a unique query. As for evidence for using common and individual user information, there is no clear distinction in terms of number of records, number of users in two logs. However, in terms of unique query and distinct click set, one can't fail to find that evidence for using individual user information clearly exceeds that for using common user information, especially in Sogou log. Therefore, though making use of common and individual user information can address equally well for half users and half visits to the search engine, the fact that much more unique queries and click sets actually claims the significance of needing individual user information to personalize web results. And methods exploiting individual user information provide a much more challenging task in terms of problem space, though one may argue utilizing common user information is much easier to attack.",
        "GPT2_formal_text": "ed. This method ensures that the output follows the semantic features for the query by using a discriminative attention layer. Since we are creating a generator using language models, we don't bother creating a discriminative attention layer for the encoder. Instead, we just use a linear layer to represent the input. Formal: We also throw in a word embedding layer for the encoder to act as the decoder's word embedding. Since the encoder is already trained, we don't need to retrain it. Formal: For the generator, we use word embeddings that were trained on the same document as the query to figure out the query's meaning. Formal: In this setup, we give the encoder a word embedding matrix and the decoder a hidden state vector. Formal: To make things simpler, we skip the word embedding layer and use a linear layer to represent the input. Formal: Basically, the encoder gets an input like this: [{w1, ..., wN}, where {w1, ..., wN} are the words in the input document, and w1 is the word embedding for the document. Then, the decoder uses the output from the linear layer to create a new hidden state vector for the input. Formal: In this setup, the decoder gets an input like this: [{w1, ..., wN}, where {w1, ..., wN} are the words in the input document, and wN is the word embedding for the document. Then, the encoder uses the output from the linear layer to create a new hidden state vector for the input. Formal: Basically, the encoder gets an input like this: [{w1, ..., wN}, where {w1, ..., wN} are the words in the input document, and wN is the word embedding for the document. Then, the decoder uses the output from the linear layer to create a new hidden state vector for the input. Formal: Here, wN is the word embedding for the document. Formal: Since the decoder is already trained, we don't need to retrain it. Formal: The document representation we get is called a query vector, and it's made up of words from the input document, like wi, ..., wi. Formal: We use the query vector as the generator's representation. Formal: For example,",
        "directory": "C10",
        "filename": "C10-2080.json"
    },
    {
        "casual_text": "Building dialogue systems that act like really good teachers is tricky, even though we've made some progress with end-to-end systems that can mimic human language pretty well. These systems work great because they have tons of training data and can handle a lot of information, but there's no way to make sure they always tell the truth. So, a simpler goal might be to create a system that sticks to the facts from one or more trusted sources. This kind of system could help teach people about specific topics through conversations or add extra info to a task-focused dialogue system.\n\nFor example, I go to animal shelters a lot. A \"no-kill\" shelter is one that doesn't put down healthy or treatable animals, even if the shelter is full. They only euthanize animals that are terminally ill.",
        "formal_text": "Dialogue systems that strive to be informative teachers are difficult to build, despite recent progress in training end-to-end systems that mimic human language at a linguistic level. These systems benefit from vast training data and great representational capacity; yet there are no controls (or training objectives) available that ensure they are truthful. A more limited goal for a system is to be faithful to one or more source documents that we implicitly trust. Such a system might help educate users about a particular topic through conversational interaction, or it might augment a task-oriented dialogue system by providing additional information about I visit animal shelters fairly often A \"no-kill\" shelter is an animal shelter that does not kill healthy or treatable animals even when the shelter is full, reserving euthanasia for terminally ill animals. . .",
        "GPT2_formal_text": "are used in the TICC for training its NLP system, as explained in section 4.2. The system's output is then passed through a linear layer and a max pooling layer to get a probability distribution. Formal: To figure out the part of the word, we use a bidirectional GRU to create an output. This output is a vector, which looks like (w_i, e_i, f_i) where e_i is the word's embedding, and w_i is the position embedding for the i-th word. Formal: A bidirectional GRU is a type of GRU that uses a bidirectional input and a linear output. Formal: The word embedding e_i is made up of three parts: the word's token embedding (e_i), position embedding (e_i+1), and a residual connection (e_i). Formal: To keep track of the word's position embedding (e_i+1), the GRU uses something called a linear projection. Formal: The final word representation for a word w_i is a vector with dimensions (w_i, e_i, f_i). Formal: We then combine these representations to create the final prediction for the word, like (w_i, e_i, f_i) = (w_i, e_i+1, f_i). Formal: We can use this combined representation to figure out how similar the words are. Formal: To figure out the part of the word, we use a bidirectional GRU to create an output. This output is a vector, which looks like (w_i, e_i, f_i) where e_i is the word's embedding, and w_i is the position embedding for the i-th word. Formal: Formal: Lastly, the system outputs a probability distribution, which is a vector with dimensions (p_i, w_i, e_i, f_i). Formal: This is what the word embedding e_i is based on the input word w_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2021.acl-long.58.json"
    },
    {
        "casual_text": "For our starting point, we're using both Logistic Regression (LR) and Random Forest (RF) algorithms with a simple set of features, like just the words themselves (lexical features).",
        "formal_text": "As baselines we consider both LR and RF algorithms with a set of basic features (i. e. , lexical).",
        "GPT2_formal_text": "ing, which is also how it's done with BERT. We tested it out using the same setup as in the original paper (Devlin et al., 2018). Just to be clear, the output tag is only generated if the input is unlabeled. Formal: We use the token sequence x = [x1, x2, ..., xn] to represent the input. Formal: We use the projection matrix P = [P1, P2, ..., Pn] to create the hidden vector h x i, which is basically the output of the attention. Formal: Once we get the token sequence h x i from the attention, we turn it into a vector x = [x1, ..., xn] using a projection matrix P. Formal: We use the query vector R = [R1, ..., Rn] to calculate the attention score for the query at position i. Formal: We calculate the attention score for the query query by using the output tag as input and the token sequence x as input. Formal: We also use the embedding matrix e e i to get the embedding h x i. Formal: Finally, we calculate the word embedding x y i using the output tag x and the hidden state h x i. Formal: Formal: Formal: For example, if we have a sentence like {The dog ate a pear}, the attention score is calculated as: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D17",
        "filename": "D17-1245.json"
    },
    {
        "casual_text": "We've conducted the first study to figure out how to automatically create Chinese character sets to make language learning easier. We tested different ways to represent characters and group them into families. The results showed that using averaged word vectors worked better than just using character vectors on their own. Plus, when we used K-means clustering, it helped create smaller groups of characters that had clear meanings. We hope these methods will make it easier to create more diverse and useful character sets.",
        "formal_text": "We have presented the first quantitative study on automatic construction of Chinese character sets to facilitate language learning. We have evaluated a number of methods for character representation and family clustering. Experimental results showed that averaged word vectors achieved statistically significant improvement over direct use of character vectors. Further, K-means clustering produced subfamilies that yielded character sets with distinctive meaning. It is hoped that these methods will help expand the variety and coverage of character",
        "GPT2_formal_text": "\" column in the query and the column \"best\" in the entity table. Formal: You can pick any of the values for \"best\" in the entity table, which are all in rows 2-4. Formal: For a given query \"x,\" the output from the softmax layer for the entity embedding is \"y.\" Formal: If you want to learn the entity embedding for query \"x,\" you can do the following: Formal: Entity embedding embedding(x, y): Here, x is the query, y is the entity embedding, and the hidden layer for the entity embedding is either the max or the tanh layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "bea",
        "filename": "2021.bea-1.6.json"
    },
    {
        "casual_text": "We only consider certain pairs (xi, Yi) for the number of sentences in a sentence bead. For the rest, we stick to simple combinations like 1-1, 1-2, 1-3, 1-4, and 2-2 for the number of sentences.",
        "formal_text": "We limit the pair (xi, Yi) of the numbers of sentences in a sentence bead to some probable ones. For the remnant, we allow only 1-1, 1-2, 1-3, 1-4, 2-2 as pMrs of the numbers of sentences:",
        "GPT2_formal_text": "_t (u, v) = conv_sigmoid (LSTM(u, v))) classifier = GPT-2T (u, v)\n          Casual: This classifier takes the logits from the two input spaces, u and v, and creates a binary classifier called GPT-2T that's based on the input features. The overall loss for classifier is calculated like this: Formal: Since GPT-2T is built using a Transformer architecture, it can't be directly tweaked like BERT. So, the loss is calculated based on the total loss for both the input spaces, x and y. Formal: The loss is calculated for the whole sentence, and the logits from the output spaces are combined into one. Formal: Formal: This is the top-level overview of the whole process. Formal: Formal: Formal: Here's the explanation of the classifier's operation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C94",
        "filename": "C94-2175.json"
    },
    {
        "casual_text": "The PosEdiOn program is typically a Python file, but you can also use a Windows executable if you prefer. If you want to send the zipped file via email, be aware that some email providers might block it because it contains an executable file. A better option could be sharing a link to the zipped file instead, which makes it easier to distribute the post-editing tasks.",
        "formal_text": "• The PosEdiOn program itself, usually as a Python file. Optionally, a Windows executable can be also used. In this case, sending the zipped file by e-mail can cause problems as some mail providers block attachments with executable files. Alternatively, a link to the zipped file can be used to distribute the post-editing tasks.",
        "GPT2_formal_text": "= [r i, c j ] for (i, j) in q(N i, N j ). Formal: In a natural language (NL), there are two types of words: basic words (unigrams) and full words (bigrams). These bigrams are broken down into smaller parts (nuclei) to create smaller parts of the text, called words. NL systems usually work with these bigram tokens by doing two things: breaking down the words into their smaller parts and applying a small-scale transformation. Formal: In this paper, we use the term \"word\" to mean any word in a sentence, no matter how it breaks down or what it means. The function f(w) = f(w 1, . . . , f(w N )) is a way to measure how different a word is from its bigram version, just like in other studies (like Montavon and Lavie, 1997; Schwenk, 1998; Tanenhaus et al., 1999; Auer and Schwenk, 1999).\n\nThere are also ways to use the distance function f(w) = f(w 1, . . . , f(w N )) to look at how similar different words are. But we’re focusing on the function f(w) = f(w 1, . . . , f(w N )) because that’s the one used in the development set. Formal: For each word in a sentence, we calculate the distance between its bigram version and its unigram version using this function: Formal: For the end-of-sentence marker, the distance is 0. Formal: For the word with the highest score, the distance is 0. Formal: Formal: The highest possible score is 0. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "eamt",
        "filename": "2020.eamt-1.43.json"
    },
    {
        "casual_text": "To do WSI on a larger scale, we stick with the main idea from Amrami and Goldberg (2019), which is to group together sparse vectors made from the lemmas of the top-k word replacements suggested by a masked language model. This helps us save a ton of storage space and makes the representations easier to understand. But to make it work for bigger datasets, we go through all the sentences in the corpus and grab the top-k substitutes for every word in a sentence at the same time, using just one call to BERT for the whole sentence. This means we can't use the dynamic-patterns part of their method, which needs BERT to run separately for each word in each sentence. Still, as we show in Section §5.1, we get pretty good WSI results even without that.",
        "formal_text": "In order to perform WSI at scale, we keep the main intuition from Amrami and Goldberg (2019), namely to cluster sparse vectors of lemmas of the top-k MLM-derived word substitutions. This results in vast storage saving, and also in a more interpretable representations. However, for scalability, we iterate over the corpus sentences and collect the top-k substitutes for all words in the sentence at once based on a single BERT call for that sentence. This precludes us from using the dynamic-patterns component of their method, which requires separately running BERT for each word in each sentence. However, as we show in Section §5. 1 we still obtain sufficiently high WSI results.",
        "GPT2_formal_text": "ed: it's like a special symbol that tells you which word is being messed with and how much. Formal: The input and output of the word embedding layer are basically two separate sets of values. The first part of the sequence is called \"word embedding,\" and the second part is called \"word embedding+mask.\" We use the words from the source language as \"source words,\" and the target words as \"target words.\" The words in the target language are called \"target words.\" To keep the translation from being too random, we add a special token called \"seed\" to the original source word embedding. The output of the word embedding layer is a binary matrix, which is a square matrix with dimensions n. Formal: This whole setup is kind of like the local transformer, but instead of the usual convolutional layers, we use a single multi-head attention layer. Formal: If you want to see how it all works in action, check out Figure 1. Formal: This is just one example to show you what we mean. Formal: Here, we have two input sequences. The first one is the source sentence x = x 1, . . . , x m−1, . . . , x n. The second one is the target sentence y = y 1, . . . , y m−1, . . . , y n. The input of the word embedding layer is the combination of the input and output vectors. The output is a binary matrix, and the dimensions of the matrix are d. Formal: Here's the equation that turns it into a formula: Formal: The model is trained to use the probabilities from this formula to figure out the probability of the target sentence y. Formal: The weights for the first part of the model are adjusted based on how likely the word w is to show up in the target sentence y. Formal: The weights for the second part are adjusted based on how likely the word w is to appear in the source sentence x. Formal: If the probability P(w | x, y) is higher than the probability P(y | x), the model tweaks the weights so that the probability P(w | y) equals P(y | x). Formal: Finally, we use a softmax function to get the word embedding vector P(w | x) and its normalized representation. Formal: To put it more formally, Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.325.json"
    },
    {
        "casual_text": "At this point, it should be pretty obvious why TFL (L) is a great fit for modern linguistics theories. First off, the LT part lets us talk about tree structures directly, which makes it perfect for setting rules about how words and phrases are grouped together. Plus, the LF part lets us describe more complex categories instead of just basic ones.",
        "formal_text": "At this stage, it should be intuitively clear why T F L (L) is well suited for modeling contemporary linguistic theories. On the one hand, the L T part of the language lets us talk directly about tree structure, thus clearly it is a suitable tool for imposing constraints on constituent structure. On the other hand, the L F part of the language permits the description of complex (rather than atomic) categories;",
        "GPT2_formal_text": ". It works with the system that figures out the categories for a query by looking at the sentence as a whole. Formal: So, we figure out the category of a query by looking at the sentence as a whole. Formal: To handle the issue of having too many categories, we use something called self-adversarial training (shoutout to Liu et al., 2016a). This involves training the model to make sure it can handle a bunch of different categories, not just the main one. Formal: We use a self-adversarial loss to tweak the model so it can learn to do a better job with multiple categories. Formal: The experiments for our classification task show that our method can really boost the performance of multi-domain text classification. Formal: For our next steps, we're thinking about applying it to other NLP tasks that deal with multiple domains. Formal: We also want to try it out on other specific NLP tasks. Formal: Lastly, we're thinking about expanding our method to tasks that involve figuring out relationships between things, like figuring out the relationship between the barcode and the food item. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E93",
        "filename": "E93-1004.json"
    },
    {
        "casual_text": "Sure! Here's a simpler way to say that:\n\n\"First, grab the top paragraph and make it the first one. Then, group the rest of the paragraphs together.\"",
        "formal_text": "4. Extract the top paragraph as the first paragraph. The others are classified into the rest.",
        "GPT2_formal_text": "_word (char_tokens): This is a self-attention thing that figures out the word type based on the characters in the word. It takes the character-level representations of the word and uses that to do its job.\n\nFor our model, we use the Transformer-based setup from Vaswani et al. (2017), which was introduced by Vaswani et al. (2017). It uses a recurrent neural network with a hidden layer size of 1024 and an output layer size of 256, which gives us a hidden dimension of 768. The model is trained using a batch size of 32 and a learning rate of 0.00001. Formal: We train our model using the train split of the Test set. For development, we use the dev split, which has the test set with some extra data added. Formal: For comparison, we also train two baselines: one with just a few-shot setup (called just one shot) and one with the full model (called full model). Formal: For all the experiments, we use the hyper-parameter α. Formal: For each setup, we create a new dataset by training the model with the full model on the test set. Formal: We test our model on the new development set and do a dropout rate of 0.1 to get the best results. Formal: For the cross-entropy loss, we use the Jensen-Shannon divergence, λ1, λ2, and λ3 from Eq. 3. Formal: We run our experiments 10 times and calculate the average accuracy and F1 score using the validation set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C98",
        "filename": "C98-2220.json"
    },
    {
        "casual_text": "W' is the count of words in different languages, so it doesn't include the ones we don't know. And max{w} is the highest number of words from the language that shows up the most in the sentence.",
        "formal_text": "where W' is the number of words in distinct languages, i. e. , the number of words except the undefined ones, max{w} is the maximum number of words belonging to the most frequent language in the sentence.",
        "GPT2_formal_text": ". The user's attention is turned into a fixed-length vector called h_t. This vector is set up like this: Formal: Basically, t_s = h_t * t_s. But this setup gets really expensive, so it's better to use something simpler, like h_t = [h_t1, ..., h_tJ, where each part is made up of multiple vectors, and J is the size of the attention matrix. Formal: In reality, we often just treat the values of the attention weights as vectors, so we can represent them as one. Formal: To figure out the attention weight for a given token i, you just multiply the word embedding vectors with it. Formal: In Figure 2, the horizontal axis shows the attention weights for the words in the answer sentence. Formal: If a question token gets split into multiple words, the attention weights for those words become the same as the attention weight for the word embedding vector w. Formal: Formal: For any question token, the attention weight is the sum of the attention weights for all the words in that question token. Formal: In the hyperplane of the hyperplane, the attention weight for a question token is the sum of the attention weights for all the words in that question token. Formal: Formal: In Figure 1, the word embedding vectors for the question and answer sentences are the same size, but they're not exactly the same. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "aacl",
        "filename": "2020.aacl-main.90.json"
    },
    {
        "casual_text": "We're using the definition and classification of spin from Boutron et al. in 2010, which breaks down spin into different types and subtypes.",
        "formal_text": "We adopt the definition and classification of spin introduced by Boutron et al. 2010and, who divided instances of spin into several types and subtypes.",
        "GPT2_formal_text": "ed/maxx))\n       Casual: Basically, this is saying that the attention weights for all the tokens in the input x, y, and w, w, and x, y, and w are combined and then passed through the decoder layer. Formal: We use a convolutional neural network to predict the attention weights for the tokens in the input x, y, and w. Formal: We also use a linear layer to combine the attention weights for all the tokens. Formal: We use a linear layer to combine the attention weights for all the tokens in the input x, y, and w. Formal: To get the attention weights for all the tokens in the input x, y, and w, w, and x, y, and w, and x, y, and w, and x, y, and w, and x, y, and w, we use a max pooling operation. Formal: Using max pooling, we get a single attention score for each token. Formal: We use a max pooling operation to get the attention weights for all the tokens in the input x, y, and w. Formal: Lastly, we use a convolutional neural network to predict the attention weights for the tokens in the input x, y, and w. Formal: We use a convolutional neural network to predict the attention weights for the tokens in the input x, y, and w. Formal: Using max pooling, we get a single attention score for each token. Formal: We use a convolutional neural network to predict the attention weights for the tokens in the input x, y, and w. Formal: We use a max pooling operation to get the attention weights for the tokens in the input x, y, and w. Formal: Lastly, we use a convolutional neural network to predict the attention weights for the tokens in the input x, y, and w. Formal: Formal: We use max pooling, max pooling, max pooling, max pooling, max pooling, max pooling, and max pooling to get the attention weights for the tokens in the input x, y, and w. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "bionlp",
        "filename": "2020.bionlp-1.5.json"
    },
    {
        "casual_text": "Basically, reordering makes a big difference when it comes to improving how well recognition and translation work, especially when you look at things like WER and BLEU scores. If you compare a cross-lingual language model without reordering to one with reordering, you can see some cool improvements. \n\nFor example, the model with reordering and local constraints gives a 0.70% drop in WER and a 3.06 point boost in BLEU. If you use IBM constraints for reordering, you get an even better result: a 0.85% drop in WER and a 3.58 point increase in BLEU. But the best results come from using ITG constraints for reordering—it gives the lowest WER, down 0.92%, and the highest BLEU, up 3.89 points. \n\nAll these WER improvements are super solid, with a 99% confidence level based on a two-proportional z-test. And the BLEU improvements are also strong, with a 95% confidence level from a paired student t-test using bootstrap resampling.\n\nOh, and we picked the segmentation order s = 3 because it works the best in our setup.",
        "formal_text": "In general, reordering has a significant effect on enhancing the performance of recognition and translation in the sense of WER reduction and BLEU improvement. Compared with the cross-lingual language model without reordering, the cross-lingual language model with reordering under local constraints gives 0. 70% absolute WER reduction and 3. 06 absolute BLEU improvement. The cross-lingual language model with reordering under IBM constraints gives 0. 85% absolute WER reduction and 3. 58 absolute BLEU improvement. The cross-lingual language model with reordering under ITG constraints yields the best performance, with 0. 92% absolute WER reduction and 3. 89 absolute BLEU improvement. All WER improvements pointed out here are statistically significant at 99% confidence according to a two-proportional z-test, and all BLEU improvements are statistically significant at 95% confidence according to a paired student t-test using bootstrap resampling. 8 We have chosen segmentation order s = 3 because it works the best in our system.",
        "GPT2_formal_text": ".\n      Casual: We also look at how well the model can tell apart different types of relationships between words. Let’s say W is the set of words that show up together a lot, and p_i is the category of words that can come from that context. We call this joint distribution p_ij. To see how well the model can do this, we calculate the normalized loss function, which we call L_ij, for each word w_i in the original sentence. Formal: We set the hidden vector h_ij to be a vector of size d_ij. Here, d_ij is the size of the vector in the embedding space, and we set the dimension of this vector to the sum of the original word vector's size and the embedding vector's size. Formal: We call the loss function L_ij for the pair of words w_i and w_j, where w_i and w_j are in the original sentence. Formal: Here, w_i is the word that appears most often in the context, and w_j is the word that appears least often. Formal: Finally, we average the loss L_ij for all words in the sentence, giving the total loss L_ij. Formal: Formal: To find the loss for all words in the sentence, we calculate the normalized loss, which we call L_ij, for each word w_i in the original sentence. Formal: Finally, we average the loss L_ij for all words in the sentence, giving the total loss L_ij. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D12",
        "filename": "D12-1070.json"
    },
    {
        "casual_text": "We tested these models by looking at how they handle entailment relations between simple intransitive sentences, verb phrases, and transitive sentences using the datasets from (Kartsaklis and Sadrzadeh, 2016). Turns out, the Frobenius models worked the best, especially when we added our sentence-level measures. Overall, our results back up what was found in earlier research (Kartsaklis and Sadrzadeh, 2016) and really show that compositional models using some kind of intersective feature selection—like point-wise vector multiplication or tensor-based models with element-wise mixing (like the Frobenius ones)—are better for entailment tasks in distributional settings.",
        "formal_text": "We experiment with these models and evaluate them on entailment relations between simple intransitive sentences, verb phrases, and transitive sentences on the datasets of (Kartsaklis and Sadrzadeh, 2016). Our findings suggest that the Frobenius models provide the highest performance, especially when combined with our sentence-level measures. On a more general note, the experimental results of this paper support that of previous work (Kartsaklis and Sadrzadeh, 2016) and strongly indicate that compositional models employing some form of intersective feature selection, i. e. point-wise vector multiplication or tensor-based models with an element of element-wise mixing (such as the Frobenius constructions), are more appropriate for entailment tasks in distributional settings.",
        "GPT2_formal_text": "layer, which is basically the same as the one in the input layer. This layer can handle some classification tasks by looking at the labels attached to the words in the input text. The cool thing about this setup is that it can easily be tweaked for any task you throw at it. Formal: To make things easier, we'll write the final result of the MRC like this: Formal: A) α = s1 + s2, where s1 is the first token, and s2 is the second one. Formal: B) α = s1 + s2, where s1 is the first token and s2 is the second one. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C16",
        "filename": "C16-1268.json"
    },
    {
        "casual_text": "We're using BBC-BJ for news and Desmond ODB (which is part of the Desmond dataset and focuses on religion) as our validation and test data. Here's why: (1) They're manually curated and really high-quality, (2) they're much smaller than the rest of our training data, so we're not losing too much that could have been used for training, and (3) they have similar sentence lengths to our training data. Unfortunately, there's no equivalent dataset for the conversation and general domains.\n\nFor the conversation domain, all the datasets are automatically aligned, which makes them pretty noisy. In the general domain, both Tatoeba and TALPCo are manually curated, but their sentences are way shorter compared to Wikimatrix, especially Tatoeba. So, for these two domains, we do a random split, using all the datasets in each domain for validation and testing. Each split has 2000 unique pairs that weren't in the training set.\n\nFor the general domain, we mix shorter sentences from TALPCo with longer ones from Wikimatrix for our validation and test data. We noticed that Tatoeba has high-quality sentences similar to TALPCo, just shorter. Since longer sentences make for a more challenging and meaningful evaluation, we decided to include TALPCo in our validation and test sets instead.",
        "formal_text": "• BBC-BJ for news, and • Desmond ODB (Our Daily Bread, the religion part of Desmond dataset) for religion, to be our validation and test data because (1) they are manually curated and of high-quality, (2) they are much smaller than the rest of training data and therefore do not sacrifice too much portion of data that could have been for training instead, and (3) they have similar sentence length compared to the training data. There is no such corpus for the conversation domain and the general domain. The datasets in the conversation domain are all automatically aligned and therefore are noisy. For the general domain, both Tatoeba and TALPCo are manually curated, but their sentences (especially Tatoeba) are very short compared to Wikimatrix. Therefore, for these two datasets, we do a random split involving all datasets in the domain for validation and testing, each having 2000 unique pairs not present in the training set. For the general domain, we mix shorter sentences from TALPCo and the longer ones from Wikimatrix as our validation and test data. We observe that Tatoeba has similar types of high-quality sentences like TALPCo has, albeit shorter. Therefore we choose TALPCo to be in the validation and test sets instead, because longer sentences mean more difficult and meaningful evaluation.",
        "GPT2_formal_text": "\" : [ ] } ] } : { [ [ \"user_agent\" ], [ \"text\" ], [ \"query\" ], [ \"type\" ], [ \"action\" ], [ \"text\" ], [ \"domain\" ], [ \"attributes\" ], [ \"lang\" ], [ \"attributes\" ], [ \"lang\" ], [ \"attributes\" ], [ \"lang\" ], ] }                 Casual: The two query questions here are asking about the user's past experiences, so they've got some language stuff mixed in. The query is also asking for more details about the product. The type is the usual type, like 'category', 'product', or 'service', but it could also be a more general category like 'content', 'elevation', or 'attributes'. Formal: Let's think about how different customer scenarios might lead to different queries. If a query is about a specific item, we think the customer might ask about it in a new way than they did before. So, the type might change based on that. Formal: Let’s also talk about how the type affects the actions the customer takes. If the type changes, we should also account for how the query changes. Formal: For example, if a query asks for the type of an item, it could involve making changes to their relationship with the item to fit the new query. Formal: A customer might want to compare different brands but not want to deal with confusing products. But in a similar situation, a customer might not want to answer a bunch of queries but still wants to use a specific service. So, the type changes based on that too. Formal: Another example is if a query is about the type of an item but not asking about its functions. Formal: Lastly, the query might involve combining different services to create a single overall service. Formal: Lastly, if a query is about the type of an item but not asking about its functions, it could involve making changes to their relationship with the item to fit the new query. Formal: Formal: Lastly, the query might involve combining different services to create a single overall service. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "bucc",
        "filename": "2020.bucc-1.6.json"
    },
    {
        "casual_text": "To figure out how well different ranking systems or models worked for comparing realizer systems, we looked at seven different \"systems\" from the whole dataset. These included five OpenCCG-based systems (three baseline models, plus the best and worst from the full perceptron model) and two XLE-based systems (the best and worst after ranking with an n-gram model). We compared the average adequacy and fluency scores of each of these seven systems to see how they stacked up against each other, which gave us 21 pairs to compare. Then, we used Tukey's HSD test to check which systems had significantly different scores. The test showed that five of the pairs had noticeably different results.",
        "formal_text": "In order to investigate the efficacy of the metrics in ranking different realizer systems, or competing realizations from the same system generated using different ranking models, we considered seven different \"systems\" from the whole dataset of realizations. These consisted of five OpenCCG-based realizations (the best realization from three baseline models, and the best and the worst realization from the full perceptron model), and two XLE-based systems (the best and the worst realization, after ranking the outputs of the XLE realizer with an n-gram model). The mean of the combined adequacy and fluency scores of each of these seven systems was compared with that of every other system, resulting in 21 pairwise comparisons. Then Tukey's HSD test was performed to determine the systems which differed significantly in terms of the average adequacy and fluency rating they received. 4 The test revealed five pairwise comparisons where the scores were significantly different.",
        "GPT2_formal_text": "_len, length) ) = arg max ( 0, 1)           Casual: The entropy of a word embedding d is calculated like this: Formal: We create embeddings for each word w_i using a BiLSTM. The hidden layer has a size of d_n. We use ReLU to normalize the embeddings to avoid overfitting. The number of dimensions in the embeddings is d_n. We feed these embeddings into a fully connected layer to get word representations. Formal: To make sure we keep learning the context, we adjust the embeddings so they work like a bag-of-words for the words in the sentence. Formal: We use GloVe (from Pennington et al., 2014) embeddings for the word embeddings, which have a dimension of d_n. Formal: We also add a ReLU activation to the word embeddings, which helps the model make better use of the context. Formal: We take the logits from the last two layers of the BiLSTM, average them, and add them to the context vectors before passing them through the fully connected layer. Formal: To find the hidden state in the fully connected layer, we average the logits from the last three layers of the BiLSTM, then average those again. Formal: After the fully connected layer, we calculate the entropy of the hidden state, and this is saved as h_d. Formal: We set the weight of the convolution layer in the fully connected layer to 1. Formal: Finally, we run the text through a transformer encoder to get the final word representations. Formal: Formal: We train the model using standard hyperparameters. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D10",
        "filename": "D10-1055.json"
    },
    {
        "casual_text": "The analysis we just talked about shows that doing the same search over and over isn't the main thing that matters. Instead, it seems that using info about individual users is just as important, if not more so, than using general info about all users. This part here is trying to dig deeper into this idea to help us figure out how to balance the importance of individual user data with common user data from a research perspective.",
        "formal_text": "The above analyses quantitative analyses have shown that the repetition of search is not the statistically dominant factor, with the impression that employing individual user information is equally, if not more, important than common user information. This section tries to further reveal this issue so as to balance the position of individual user information and common user information from a research point.",
        "GPT2_formal_text": ". For this example, we're using the example set from the ATIS corpus (Bates et al., 1992) to show the main ideas of the recipe. Formal: The recipe has a main dish and a condiment. The main dish has four ingredients, which are: meat, onion, garlic, and peas. The condiment has a couple of ingredients: tomato paste and seasoning. Formal: In the example, you can see the main dish, the condiment, and the ingredients for the main dish. Formal: The ingredients for the main dish are: meat, onion, garlic, peas, tomato paste, seasoning, and tomato. Formal: The ingredients for the condiment are: tomato paste, seasoning, and tomato. Formal: The main dish has four ingredients, which are: meat, onion, garlic, and peas. Formal: The condiment has a couple of ingredients: tomato paste and seasoning. Formal: The main dish has four ingredients, which are: meat, onion, garlic, and peas. Formal: The condiment has a couple of ingredients: tomato paste and seasoning. Formal: The main dish has four ingredients, which are: meat, onion, garlic, and peas. Formal: Formal: The main dish has four ingredients, which are: meat, onion, garlic, and peas. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C10",
        "filename": "C10-2080.json"
    },
    {
        "casual_text": "For the embedding layer, we went with the pre-trained GloVe 3 embeddings (Pennington et al., 2014) that were trained on 6 billion words from Wikipedia 2014 and Gigaword 5. These embeddings get fine-tuned during training. We limited the vocabulary to the 4,000 most common words, following Taghipour and Ng (2016), and treated any other words as unknown. We set the number of sentences per essay to the maximum for each prompt and capped the sentence length at 128. The models were trained in batches of 16 for 50 epochs. We tweaked the hyperparameters using optuna over 100 trials.",
        "formal_text": "In the embedding layer, we used the pre-trained word embedding GloVe 3 (Pennington et al. , 2014) trained on 6 billion words from Wikipedia 2014 and Gigaword 5. During the training process, word embeddings are fine-tuned. The vocabulary was set to the 4, 000 most frequent words by following Taghipour and Ng (2016) and treating other words as unknown words. We set the number of the essay sentences to the maximum for each essay prompts and the maximum sentence length to 128 and trained the models on batch size 16 for 50 epochs. The following hyperparameters were tuned by using optuna 4 in 100 trials.",
        "GPT2_formal_text": "ed is just the number of time and attention masking steps. The main idea behind this method is to guess the attention values for the whole dataset at the same time. The idea is that the model can use this info to make better predictions when it's fine-tuned on the training data. Formal: We're suggesting a new way to train the BERT model, specifically for dealing with social data. This method uses a model called BERT-EMD that's pre-trained on different kinds of social data—like posts from microblog, forum, and tweet data—to help it predict attention values for the whole dataset at the same time. Formal: We trained the BERT model to predict the attention values for both the source and target domains using different datasets, following the approach in (Zhou et al., 2020). We did this by combining the predictions from different datasets by averaging their attention vectors. The final results are shown in Figure 1. Formal: We picked the best model and added it to the BERT model in a way that focuses on predicting the attention values for the source domain, specifically for the source sentence S. Formal: We also trained the BERT model for the target domain using the same datasets as we did for the source domain. This setup is shown in Figure 1. Formal: For the BERT model, we used the model's predictions for the source domain S and the target domain T as the source and target domains, respectively. To train the BERT model for the target domain T, we added the predictions for the source domain T and the target domain S as the target domain and source domains. We trained the BERT model using the target domain T and the source domain S to predict the attention values. The results are shown in Figure 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "aacl",
        "filename": "2020.aacl-srw.17.json"
    },
    {
        "casual_text": "The context for the current character w_t comes from the characters around it within a window of size c, along with their segment labels (check out Figure 2). The characters w_i and their labels l_i in this context are turned into vectors, e_c(w_i) and e_c(l_i), both in R^d, where d is the size of the embedding.",
        "formal_text": "the pre-training context of current character w t is the around characters in the windows with size c, together with their corresponding segment labels ( Figure 2 ). Characters w i and labels l i in the context are represented by vectors e c w i ∈ R d and e c l i ∈ R d, respectively, where d is the embedding dimensionality.",
        "GPT2_formal_text": "(1) and (2) (Bagga and Baldwin, 2012). We calculate the cosine similarity between the candidate embedding (e_p) and the attention vector (e_t) using this formula. Formal: The cosine similarity between two attention vectors (e_p and e_t) is calculated as the geometric mean of their dot product, and then multiplied by the sum of two values: the cosine similarity between the attention vector and the embedding embedding. Formal: The attention score for a sentence is based on the cosine similarity between the embedding embedding and the attention vector. Formal: The model calculates the attention score by adding up the cosine similarity between the attention vector and the embedding embedding. Formal: The attention score is calculated by the attention score between the embedding and the attention vector. Formal: The model calculates the attention score by adding up the cosine similarity between the attention vector and the embedding embedding. Formal: The model calculates the attention score by the attention score between the embedding and the attention vector. Formal: Formal: The model calculates the attention score by adding up the cosine similarity between the attention vector and the embedding embedding. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D17",
        "filename": "D17-1079.json"
    },
    {
        "casual_text": "The model setups mentioned in Section 2.1 are different because they look at different \"window\" sizes on the output structure. Instead of changing the window size, we can get the same result by keeping the window size the same but tweaking the output values. For instance, imagine changing the output values by swapping single output tags with pairs of neighboring output tags.",
        "formal_text": "The common model structures described in Section 2. 1 differ from one another in that they examine varying sizes of \"windows\" on the output structure. Rather than varying the size of the window, we can achieve the same effect by fixing the window size, but transforming the output values. For example, consider the effects of transforming the output values by replacing individual output tags with pairs of adjacent output tags:",
        "GPT2_formal_text": "ing. This is useful for spotting news articles that mention a specific entity, but it can also be useful for classifying other articles if you train it properly. Formal: • A vector with 3d vectors showing the entity mention in each sentence, organized in a 3d space. Formal: • A vector with 3d vectors showing the entities mentioned in each sentence, organized in a 2d space. Formal: • A vector with 3d vectors showing the entity mention in each sentence, organized in a 1d space. Formal: • A vector with 3d vectors showing the entities mentioned in each sentence, organized in a 1d space. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D07",
        "filename": "D07-1084.json"
    },
    {
        "casual_text": "For a character-based Chinese NER model, the input sentence is treated as a sequence of characters.",
        "formal_text": "For a character-based Chinese NER model, the input sentence is seen as a character sequence",
        "GPT2_formal_text": "(x) = [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] + [x] +",
        "directory": "acl",
        "filename": "2020.acl-main.528.json"
    },
    {
        "casual_text": "Okay, let's break this down in a simpler way.\n\nWe have a bunch of pairs like (f_s, f_t) that can either be 0 or 1, and there are n of them. Now, we're defining something called W, which takes a feature f and gives us a real number. \n\nFor each feature f that looks like (f_s, f_t), W(f) is calculated like this: \n\nW(f) = (#(f_s, f_t) / #(f_s)) * (#(f_s, f_t) / #(f_t))\n\nBasically, it's the count of (f_s, f_t) divided by the count of f_s, multiplied by the count of (f_s, f_t) divided by the count of f_t.",
        "formal_text": "{(f s, f t ) 1, (f s, f t ) 2, . . . , (f s, f t ) n } ∈ {0, 1} n. Define W: f → R s. t. foreach feature f =(f s, f t ) W (f) = #(f s, f t ) #(fs) × #(f s, f t ) #(ft)",
        "GPT2_formal_text": "_tgt (s) = {s};\n           Casual: (1) Formal: The time it takes to train the model really depends on the dataset and how much computing power it uses. We noticed that during the validation phase, the model can sometimes take more than 1 hour to train on a dataset with 50 tweets. This is because the dataset has a lot of duplicate tweets. On the other hand, models trained on datasets with 200 tweets usually take just a few minutes to train, which is way faster than when we used the same dataset with just 100 tweets. Formal: The model can still be used to train a supervised SVM for Twitter sentiment classification even when we don’t have access to the full training data. Formal: In this study, we looked at how the model performs when we only have limited resources, even when we have the full training data available. Formal: We also noticed that if we train the model on a dataset with 1 tweet (let’s call it D) that has 1,000 tweets, the performance can go up to 3 tweets, which is pretty impressive. Formal: For a lot of datasets, we tested a model with a beam size of 8, which is just a linear combination of the number of training epochs and the size of the training set. Formal: In our next steps, we plan to look into more advanced methods, like the ones mentioned in Section 4, for improving the performance of extractive sentiment classification. Formal: Formal: Finally, we’ve included some results from the experimental dataset in Table 1. Formal: Lastly, we’ve included some results from the experimental dataset in Table 1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D08",
        "filename": "D08-1037.json"
    },
    {
        "casual_text": "• Transformer: We're using a basic Transformer model for training. Here are a couple of links for reference: 4) https://pypi.org/project/langdetect, and 5) http://lotus.kuee.kyoto-u.ac.jp/WAT/WAT2018/baseline/dataPreparationJE.html.",
        "formal_text": "• Transformer: We train a base Transformer 4 https: //pypi. org/project/langdetect. 5 http: //lotus. kuee. kyoto-u. ac. jp/WAT/ WAT2018/baseline/dataPreparationJE. html.",
        "GPT2_formal_text": ", and a translation model with a feature vector for each word, which are labeled as T (W i, W j, W i+1 ). Formal: The goal is to find the best T (W i, W j, W i+1 ). The best T (W i, W j ) is chosen based on a softmax function, which takes into account the translation probability p t (W i, W j ). Formal: The updated function f changes the probability distribution p t (W i, W j ) by adding a translation probability p t (W i, W j ). This process is repeated for every word in the sentence. Formal: The weights for the update function f are estimated using a minimum entropy method (Koehn et al., 2007). Formal: The updated function f also looks at the word vector φ i, j, which is a normalized version of the output probability vector p t (W i, W j ). Formal: Finally, we update the translation model using the updated weights. Formal: The new function f is calculated using the softmax function, which is normalized to the output probability p t (W i, W j ). Formal: Finally, the updated translation model is calculated using the updated weights. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2020.acl-main.147.json"
    },
    {
        "casual_text": "Just like the earlier phrase segmentation models (Lee et al., 2011; Xiong et al., 2011), we’ve added the phrase segmentation model as an extra feature function into the log-linear model.",
        "formal_text": "Like the previous works for phrase segmentation model (Lee et al. , 2011; Xiong et al. , 2011), we integrate the phrase segmentation model into the log-linear model as an additional feature function.",
        "GPT2_formal_text": "ed_vec * self_mask = self_mask + self_att.shape * m_att.shape + self_att.att.shape. So, it's like adding up all the attention vectors in the masked image, and then averaging those vectors together. Formal: At each step, the model generates a sequence of vectors. For the first one, it uses a linear transformation to get the x, y, and z vectors for the masked image. Then, it averages these vectors together for the second one, which is the normalized representation of the masked image. Formal: After processing the masked image, the final representation is created by combining the image's representation with the softmax layer on top of the input image data. Formal: Our model is built using the same decoding model as Barik et al. (2019), which is called RoBERTa-Large. Formal: We ran our experiments on the MNLI dataset with 15 different pre-trained models. Formal: To see how our model compares to others, we used the same dataset as Barik et al. (2019) and trained it for 30 epochs with an average of 3 × 4 representations per layer. Formal: We used the masked image from MNLI as our main input and evaluated our models using the IOB1 annotation scheme. Formal: We also compared our model to a few baselines and came up with three solutions: standard RoBERTa, LSTM-based models, and a mix of BiLSTM and LSTM-based models. Formal: In Appendix C, you can find the code and pre-trained models we used for our experiments. Formal: In Appendix D, you can find the nitty-gritty details about the pre-training process, including how many layers, hidden layers, and parameters we used. Formal: Formal: We also tried out some alternative methods, like the Convolutional Neural Network (CNN), LSTM-based model (BERT), and a mix of BiLSTM and LSTM-based models. Formal: For more details, check out Appendix G. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C12",
        "filename": "C12-2060.json"
    },
    {
        "casual_text": "To hit the second goal, we’re looking at two different ways to create questions from a passage. The first method uses a retrieval-based approach with a convolution neural network (CNN), and the second one is a generation-based method that uses a recurrent neural network (RNN). We’ll check how well these methods work by looking at the BLEU score (Papineni et al., 2002) and also getting feedback from people. In Section 9, we’ll talk about the good and bad sides of each method.",
        "formal_text": "To achieve the 2 nd goal, we explore two ways to generate questions for a given passage, one is a retrieval-based method using convolution neural network (CNN), the other is a generation-based method using recurrent neural network (RNN). We evaluate the generation quality by BLEU score (Papineni et al. , 2002) and human annotations, and discuss their pros and cons in Section 9.",
        "GPT2_formal_text": "= {q, a, q_t, q_i, a_t, q_j} | y = y_t, a_t, q_i, a_j ∈ Qp(a_t, q_i, q_t, q_i, a_j). Formal: The probability P_i(q) is calculated by multiplying the logit h(q) with the logit of the label y_t, a_t, q_i, a_j. Formal: The log probability P_i(a_t | q) is calculated by multiplying the logit h(a_t) with the logit of the label y_t, q_i, a_j. Formal: Formal: The log probability P_i(q) is also calculated by multiplying the logit h(q) with the logit of the label y_t, a_t, q_i, a_j. Formal: Formal: The log probability P_i(a_t | q) is also calculated by multiplying the logit h(a_t) with the logit of the label y_t, q_i, a_j. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D17",
        "filename": "D17-1090.json"
    },
    {
        "casual_text": "So, the idea here is to use some basic rules to change words into their closest Modern Standard Arabic (MSA) version, like what the MT system 1 already does with its training data. These rules can be made manually and then improved by using tools like GIZA++ (Al-Onaizan et al. 1999) to learn from bilingual or bi-dialectal data. There are also modules for phrasal extraction, as explained in (Och and Ney, 2004) and (Koehn et al., 2007). \n\nEach dialect has its own training process to create translation pairs between dialectal Arabic and MSA. This training uses a bi-dialectal corpus, but a small part of it is kept aside for tweaking parameters and testing. \n\nAfter that, all possible ways a word could be transliterated are considered, especially for Arabic written in Latin characters, like \"tawle\" for \"table\". Each option is weighted based on how much it changes from the original form.",
        "formal_text": "• by a character based dialect normalization, which utilizes simple rules to convert words into the most similar MSA word, as seen in the training data of the MT system 1. These simple rules can be hand-coded and enriched by rules which can be learned from bi-dialectal 2 with bilingual alignment modeling software like GIZA++ (Al-Onaizan et al. 1999), and phrasal extraction modules, as described in (Och and Ney, 2004) and (Koehn et al. , 2007). There is a separate training process to generate dialect Arabic/MSA word and phrase translation pairs for each dialect. This training is carried out on the bidialectal corpus, leaving out a small subset of sentences for parameter optimization and test. For further processing, all possible permutations and possibilities of transliteration of each word are taken into consideration, weighted by the grade of transformation from the original form. This step allows processing of Arabic which is encoded in latin characters, e. g. \"tawle\" for ‫, \"&%و#! \"‬ i. e. English \"table\".",
        "GPT2_formal_text": "ed. Instead of using the standard attention mechanism, we suggest using a CNN-based attention mechanism that is tailored to specific tasks. Formal: We tested the proposed framework on a bunch of different datasets using the Karpathy attention model, which is already included in the standard HVAE library. Our results show that our framework, when trained on datasets with lots of attention kernels, performs just as well as the standard HVAE. Formal: We also did a human evaluation and compared our method to the standard HVAE by fine-tuning on datasets with lots of attention kernels. Formal: We also created a new version of the HVAE that can handle long-term dependencies and is designed to handle different aspects of text. Formal: We tested our new HVAE on the CoNLL-2012 dataset, which has over 20,000 labeled documents. The results show that our framework works really well, outperforming the other methods. Formal: We also created a new version of the HVAE that can handle long-term dependencies and is designed to handle different aspects of text. Formal: Our framework outperforms the current top method for both long-term dependencies and multi-modality by fine-tuning on datasets with lots of attention kernels. Formal: Lastly, we included our framework in the HVAE library and shared the code publicly to help others build on it. Formal: For future work, we think we could explore how to incorporate other attention mechanisms, like linear, neural, and kernel attention, into the HVAE framework, to improve its overall performance. Formal: We also plan to expand the proposed framework to support other NLP tasks like sentiment analysis and question answering. Formal: The findings from our experiments show that our framework works well for both long-term dependencies and multi-modality, as well as for text processing tasks. Formal: The experiments also show that our framework can handle long-term dependencies and multi-modality. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "amta",
        "filename": "2010.amta-papers.5.json"
    },
    {
        "casual_text": "For spaCy, we stuck to the usual recommendations. All the other methods were done using TensorFlow. As for BERT models, we went with HuggingFace's implementation (shoutout to Wolf et al., 2020). We also used Adam (thanks, Kingma and Ba, 2015), Glorot initialization (props to Glorot and Bengio, 2010), and the categorical cross-entropy loss.",
        "formal_text": "For spaCy, we followed the recommended practices. 12 All other methods were implemented in tensorflow. 13 Concerning bert models, we used the implementation of huggingface (Wolf et al. , 2020). We also use Adam (Kingma and Ba, 2015), Glorot initialization (Glorot and Bengio, 2010), and the categorical cross-entropy loss.",
        "GPT2_formal_text": "= {k1, ..., kT} = {k1, ..., kT} N = {p1, ..., pT} = {1, ..., N} Formal: We're using the structured perceptron method to figure out the sentiment for each sentence in a document. Formal: We start with the LSTM's hidden state vectors h1 and h2, which were trained on a document-specific set of documents D D. Then, we add up all the features from all the sentences in the document. Finally, we take the average of these feature vectors to get the overall sentiment for the whole document. Formal: We train the LSTM model using a dataset D D to predict the sentiment for each sentence in the document. Formal: We also tweak the original LSTM model by replacing the source sequence h s i and the target sequence h t i with a mix of the hidden states for the source and target entities, following the method by (Xu et al., 2017). Formal: We calculate the loss for the LSTM model by minimizing the conditional cross-entropy loss. Formal: We also use a classifier called C to predict the sentiment for each sentence in the document. Formal: Finally, we combine the text representation h t i and the sentiment prediction model to get the contextualized sentiment embedding h c. Formal: The final score is calculated as (h c + h t i) + r. Formal: The loss is calculated using the formula: Formal: Finally, we calculate the loss for the LSTM model by minimizing the conditional cross-entropy loss. Formal: Formal: Formal: We also use a classifier called C to predict the sentiment for each sentence in the document. Formal: We calculate the loss for the LSTM model by minimizing the conditional cross-entropy loss. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "acl",
        "filename": "2022.acl-long.303.json"
    },
    {
        "casual_text": "So, for each utterance in the set {U1, U2, ..., UN}, we use max-pooling on the word embeddings within that utterance to get an individual embedding, which we'll call eui. This process gives us a collection of utterance embeddings like {eu1, eu2, ..., euN} for the entire dialogue.",
        "formal_text": ". . , U N }, we get the individual embedding e u i by max-pooling on the contextual word embeddings within the U i. In this way, we obtain a set of utterance embeddings {e u 1, e u 2, . . . , e u N } in one dialogue.",
        "GPT2_formal_text": "ing_1.0 model. This is a pre-trained model that was trained on a dataset that had different types of content. For our experiments, we used the same data as in (Wang et al., 2018). Formal: We take the sequence of position features and run it through an encoder-decoder setup with a dropout rate of 0.2. The input for the encoder is a sequence of actual words. The output for the decoder is a sequence of vector features. We feed these vector features into a fully-connected layer and then run the whole thing through a softmax layer to get the final prediction. Formal: We use the Adam optimizer (from Kingma and Ba, 2014) to optimize the network parameters. Formal: We ran the train/test split for 40 epochs. For each epoch, we tweaked the network setup using the Adam optimizer. Formal: We set the learning rate to 0.003 and kept the batch size to 50. Formal: We used the same dropout rate for the encoder-decoder and the dropout rate for the fully-connected layers as in the paper. Formal: We set the batch size to 50 and the width of the training set to 4. Formal: We ran the train/test split for 40 epochs. Formal: We used the same dropout rate for the encoder-decoder and the dropout rate for the fully-connected layers as in the paper. Formal: We set the batch size to 50 and the width of the training set to 4. Formal: We ran the train/test split for 40 epochs. Formal: We used the same dropout rate for the encoder-decoder and the dropout rate for the fully-connected layers as in the paper. Formal: We set the batch size to 50 and the width of the training set to 4. Formal: We ran the train/test split for 40 epochs. Formal: We used the same dropout rate for the encoder-decoder and the dropout rate for the fully-connected layers as in the paper. Formal: We set the batch size to 50 and the width of the training set to 4. Formal: We ran the train/test split for 40 epochs. Formal: We used the same dropout rate for the encoder-decoder and the dropout rate for the fully-connected layers as in the paper. Formal",
        "directory": "ccl",
        "filename": "2020.ccl-1.102.json"
    },
    {
        "casual_text": "Step 3 is basically the same as the word-level NLL process, but this time it's done on the freshly created data set. You can see this in Figure 1, right in the middle.",
        "formal_text": "Step (3) is identical to the word-level NLL process except now on the newly-generated data set. This is shown in Figure 1 (center).",
        "GPT2_formal_text": "= {h_t} n_t = argmax_t_l ∈ P_T |w_t ∈ R |p_t ∈ R |h_t ∈ P_R |h_t ∈ P_T |θ_t ∈ P_R |h_t ∈ P_R |h_t ∈ P_R |h_t ∈ P_R |h_t ∈ P_R |h_t ∈ P_R |θ_t ∈ P_R |p_t ∈ R |p_t ∈ R |p_t ∈ R |h_t ∈ P_R |h_t ∈ P_R |h_t ∈ P_R |θ_t ∈ P_R |\n\nwhere |h_t| is the size of the output tag. Formal: Once we have all the outputs from the source sentence, we can use a standard maximum entropy-based model to predict how likely a word is to be the head word in the next sentence. Formal: The model takes the sentence's current word (let's call it w_t) and tosses out any words that don't match the head word's length or any words that aren't in the target vocabulary. Formal: We then use a linear layer to figure out the probability p_t for the word before the head word. Formal: Next, the model uses a Maximum Entropy classifier to make a guess about the next word. Formal: Finally, we pick the word that has the highest probability and the one with the lowest probability for the head word. Formal: Here, |h_t| is the size of the output tag. Formal: Formal: Now that we have the output tags, we can use a simple linear classifier to predict how likely it is for the next word to be the head word. Formal: Lastly, we take the probabilities from the classifier's predictions and add them together to get the final probability p_t. Formal: We can see that this method works really well by using our experiments on the Gigawords corpus. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "D16",
        "filename": "D16-1139.json"
    },
    {
        "casual_text": "Let’s start by looking at a simpler version of the \"learning with hints\" problem. Imagine we’re only focused on learning f₂. We’ve got a little bit of data that’s already labeled with f₂ (we’ll call this D) and a whole bunch of data labeled with f₁ (we’ll call this D_unlab—the \"unlab\" part means it’s unlabeled as far as f₂ is concerned).",
        "formal_text": "We begin by considering a simplified version of the \"learning with hints\" problem. Suppose that all we care about is learning f 2. We have a small amount of data labeled by f 2 (call this D) and a large amount of data labeled by f 1 (call this D unlab -\"unlab\" because as far as f 2 is concerned, it is unlabeled).",
        "GPT2_formal_text": "ed = (0, 0, 1, 0, 1) # Attention mask (0, 1) = max_pooling(0, 0, 1)\n           Casual: The model looks at all the n words and picks the one with the highest attention score. If the word is inside quotation marks, it gets a 0. Formal: Formal: You can find more examples of paraphrasing in our datasets and in our models, all included in the supplementary material. Formal: We use this model to create questions based on the sentences we see. Formal: If the answer is in quotation marks, it's treated as an \"answer\" and gets a 0. Formal: We tested it on a really big test set with all kinds of question types and conditions. Formal: Since paraphrasing is a way to understand sentences, the model can pick up on relationships between words in the question, which helps it answer questions with more than one answer. Formal: We also noticed that the model can pick up on patterns in the question to answer more than one question. Formal: In the appendix, we share the results for paraphrasing on three different datasets, with each dataset containing around 5000 question-answer pairs. Formal: Our models work really well on datasets like QuAC, QuAC-Tran, and SQuAD, with even better performance on QuAC-KBP. Formal: For paraphrasing on QuAC, we used the same settings as the model we trained on QuAC. Formal: We also showed how our model can paraphrase on different datasets by using a simple yes/no question as input. Formal: Lastly, we tested it on a more complex dataset called QuAC-KBP with over 3,000 question-answer pairs, and the results are in Table 3. Formal: When we added a yes/no question, our model boosted the model's performance on QuAC-KBP. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D08",
        "filename": "D08-1071.json"
    },
    {
        "casual_text": "The results for French to English translation aren't as good: neither cont nor para on their own beat the baseline for any metrics. But when we combine them, they do better than the baseline across all metrics except BLEU, with a drop of -1.07 in TER. Looking closer at the CLE results, it seems like adjectives and nouns got the most benefit from using our extra models. Verbs, which improved a bit, are super inflected in French, so finding examples for a specific form is trickier than for less inflected word types, and finding paraphrases with the right inflection is also harder. Also, translating through English as a pivot can lead to paraphrases that aren't always great, especially when the source language is less inflected. Plus, our simple context modeling might not have been good at filtering out some bad examples. Overall, para worked better when English was the source language, improving over the baseline across all metrics.",
        "formal_text": "Results on French to English translation are less positive: neither cont nor para alone improve over the baseline with any metrics. However, their combination improves over the baseline with all metrics except BLEU, including a reduction of -1. 07 in TER. Detailed analysis of CLE results shows that the translation of adjectives and nouns benefited more from using our two additional models. Verbs, whose translation improved slightly, are strongly inflected in French, so finding examples for a given form is more difficult than for less inflected word categories, as is finding paraphrases with the appropriate inflection. Also, pivoting via English is one reason why paraphrases obtained via a low-inflected language can be of varying quality. Furthermore, the simplicity of our context modeling may have been ineffective in filtering out some bad examples. Overall, para was more effective with the low-inflected English as the source language, improving over the baseline with all metrics.",
        "GPT2_formal_text": "is the same for both the original and the transformed sentences. The dataset size is also the same as the token count. Formal: We also ran some tests on a smaller chunk of the document collection, specifically around the 300,000 mark, using the GLUE benchmark, as suggested by BERT-Base (Devlin et al., 2018). The results are in Table 4. Turns out, the performance of all models is pretty much the same—they're just a little faster, and the difference is statistically significant at p = 0.05. Formal: Lastly, we checked out the performance of models that are pretrained on datasets bigger than 300,000, which include 1.1 million and 7.3 million tokens. Formal: All the models showed similar results: the best model on the Test set got a precision of 0.94, while the best on the Enlarge set got 0.83. On average, the best model on the Test set is almost three times faster than the best model on the Enlarge set. Formal: The best model on the Test set is still the best on the Enlarge set. Formal: We also looked at models that are pretrained on datasets larger than 300,000. Formal: The best model on the Test set is still the best on the Enlarge set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "D10",
        "filename": "D10-1064.json"
    },
    {
        "casual_text": "We tested the three generators on a total of 300 sentences. For GPT-2, we used the first two words of a sentence as the starting point (prompt), made sure the generated text was about the same length as the original, and collected 10 different versions. With RoBERTa and T5, we messed with the original sentence three times, adding up to three [MASK] tokens each time, and asked the generator to create 5 different versions using beam search, just like Ribeiro et al. (2020) did. Polyjuice works similarly to RoBERTa and T5 when it comes to adding the [MASK] tokens, but we also tried out all the control codes. For each sentence, we randomly picked 5 versions from each generator to create the final set.",
        "formal_text": "We run the three generators on 300 sentences in total. In GPT-2, we take the first two words of an x as the input context (prompt), limit the length of the generation to be similar to x, and collect 10 counterfactuals. As for RoBERTa and T5, we repeatedly perturb x for three times, each time randomly placing up to three [MASK] tokens, and ask the generator to generate 5 counterfactuals through beam search, following Ribeiro et al. (2020). Polyjuice uses the same blank (mask) placement as in RoBERTa and T5, but we additionally enumerate through all control codes. For each x, we randomly sample 5 counterfactuals to formX per generator.",
        "GPT2_formal_text": "_dg2vec = self.attention_mask_dg2vec (D) self.attention_mask_r = self.attention_mask_r (D) self.attention_mask_vec = self.attention_mask_vec (D) end end function nh(attn, dg, attn, dg2vec) return self.attn, self.dg, attn, dg2vec end function L_attn(attn, dg) return self.attn, self.dg, attn, dg2vec end end function V_attn(attn, dg) return self.attn, self.dg, attn, dg2vec end function lh(attn, dg, attn, dg2vec) return self.attn, self.dg, attn, dg2vec end end function lh2(attn, dg, attn, dg2vec) return self.attn, self.dg, attn, dg2vec end end function lh2(attn, dg, attn, dg2vec) return self.attn, self.dg, attn, dg2vec end end function rh(attn, dg, attn, dg2vec) return self.attn, self.dg, attn, dg2vec end end function rh2(attn, dg, attn, dg2vec) return self.attn, self.dg, attn, dg2vec end end function W_attn(attn, dg) return self.attn, self.dg, attn, dg2vec end end function W_attn(attn, dg2vec) return self.attn, self.dg2vec end end function W_attn(attn, dg2vec2) return self.attn, self.dg2vec2 end end function G_attn(attn, dg) return self.attn, self.dg, attn, dg2vec2 end end function G_attn(attn, dg2vec2) return self.attn, self.dg2vec2 end end function G_att",
        "directory": "acl",
        "filename": "2021.acl-long.523.json"
    },
    {
        "casual_text": "So, we've been looking at recent studies on NER and trying to figure out how to improve the recognition of OOV (out-of-vocabulary) entities. In this project, we came up with a cool and flexible learning framework called MINER. It tackles the OOV recognition problem by looking at it from an information-theoretic angle. \n\nOn one hand, this method boosts the context information from the encoder's output. On the other hand, it helps get rid of unnecessary stuff that doesn't matter for the task and stops the model from just memorizing entities. \n\nThe approach has two main training goals based on mutual information: one is about maximizing useful information, and the other is about minimizing extra, unnecessary information. \n\nWhen we tested it on different datasets, MINER performed way better at predicting OOV entities compared to other methods.",
        "formal_text": "Based on the recent studies of NER, we analyze how to improve the OOV entity recognition. In this work, we propose a novel and flexible learning framework -MINER, to tackle OOV entities recognition issue from an information-theoretic perspective. On the one hand, this method can enhance the context information of the output of the encoder. On the other hand, it can safely eliminate task-irrelevant nuisances and prevents the model from rote memorizing the entities. Specifically, the proposed approach contains two mutual information based training objectives: generalizing information maximization, and superfluous information minimization. Experiments on various datasets demonstrate that MINER achieves much better performance in predicting out-of-vocabulary entities.",
        "GPT2_formal_text": "= { u i } | idx = { u i, . . . , u i+1 }. Formal: We start by using the logit loss to figure out the probability distribution. Then, we use something called a linear-chain CRF (LCF) to get the softmax output. Formal: To make sure the relevance of the retrieved info is properly taken into account, we add a linear distortion penalty to the logit loss. Formal: Alright, so we figure out the relevance score using this formula: Formal: The relevance score for a document D = (D_1, D_2, ..., D_{m-1}) is calculated by taking the logit loss and applying it to the logit of the retrieved info. Formal: Basically, the relevance score is calculated as the logit loss divided by the logit of the retrieved info, which gives us a relevance score. Formal: This is similar to the sentence-level relevance score but is specifically for documents. Formal: We also use a probabilistic model to figure out the relevance score for a single sentence. Formal: To make sure we're comparing the relevance scores for both sentence-level and document-level info, we use a logistic regression classifier to train the relevance model. Formal: Following what Li and his team did in 2015, we calculate the relevance score for a sentence by first taking the logit loss and applying it to the logit of the retrieved info. Formal: Finally, we sum up the relevance scores for all the sentences in the document D to get the overall relevance score. Formal: Formal: This whole process is laid out in Algorithm 1. Formal: You can find all the code and details here: https://github.com/FengShou1/Entropy-Guided-Semantic-Discourse-Augmentation. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "acl",
        "filename": "2022.acl-long.383.json"
    },
    {
        "casual_text": "We used a classifier based on a probabilistic version of the pair-wise algorithm (Soon et al., 2001). We focused only on resolving third person singular pronouns because that's where pronoun resolution is most common in psycholinguistics. Third person pronouns are different from first and second person ones since the latter are deictic, not anaphorical.",
        "formal_text": "We used a classifier that proceeds according to a probabilistic version of the pair-wise algorithm (Soon et al. , 2001 ). We only account for third person singular personal pronoun resolution in order to approach the psycholinguistic domain where pronoun resolution is most often restricted to these type of pronouns. The third person pronouns can be viewed as different from the first and the second as the latter are deictic rather than anaphorical.",
        "GPT2_formal_text": "ed_d = softmax(d) # Masking vector d for attention attention (K = 1) on the output. Formal: The model creates the response y i using the output from the softmax layer, which is also used to calculate the hidden states h i. This hidden state h i is used in a convolutional layer to create the response. Formal: The output of the convolutional layer is combined with the hidden states of the input, which are used to calculate the hidden states for the next convolutional layer. Formal: The output of this convolutional layer is turned into a softmax vector, which is then passed through a fully connected layer. Formal: The output of this fully connected layer is used to calculate the hidden states for the output. Formal: This output is passed through another fully connected layer with the same input, and the final output is passed through a fully connected layer again with the output. Formal: The output of this fully connected layer is also used to calculate the hidden states for the output. Formal: Finally, this output is passed through a fully connected layer with the output. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "E17",
        "filename": "E17-4006.json"
    },
    {
        "casual_text": "We found that using both label smoothing and vocabulary sharing in neural machine translation (NMT) can mess things up and lead to worse performance. To fix this, we came up with Masked Label Smoothing (MLS), which tweaks the smoothed probabilities based on the differences between languages. It’s straightforward but works really well. MLS not only improves translation quality but also helps the model make better predictions across a bunch of tasks compared to the original label smoothing.",
        "formal_text": "We reveal the conflict between label smoothing and vocabulary sharing techniques in NMT that jointly adopting the two techniques can lead to suboptimal performance. To address this issue, we introduce Masked Label Smoothing to eliminate the conflict by reallocating the smoothed probabilities according to the languages' differences. Simple yet effective, MLS shows improvement over original label smoothing from both translation quality and model's calibration on a wide range of tasks.",
        "GPT2_formal_text": "= c.items[i] for j in range(l): t = c.inputs[i] for j in range(l): t = c.outputs[j] return t, self, attn_ids, attn_values, attn_persons, attn_ids, attn_values, attn_persons_t, attn_persons_t, attn_persons_t, attn_persons_t, attn_persons_t, attn_persons_t, attn_persons_t, attn_rel_ids, attn_rel_values, attn_rel_persons, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel_persons_t, attn_rel",
        "directory": "acl",
        "filename": "2022.acl-short.74.json"
    },
    {
        "casual_text": "Alright, let's dive into some more examples of what START can do. We picked these conversations to show off how S-rules work and to give you a sense of what START can handle. Along the way, we'll also introduce some S-rules that deal with a few more tricky language patterns, like those indefinite objects and reflexive stuff we talked about earlier in the paper.",
        "formal_text": "In this section we present additional dialogues that START can handle. These interchanges are chosen to illustrate the use of S-rules as well as the range of coverage of the START system. In the process we introduce S-rules that handle several more semantic-syntactic interdependencies, including the indeflrfite object and reflexive alternations discussed in the introduction to the paper.",
        "GPT2_formal_text": "ing_tagger_tagger *, [mask] tuple, which is basically the type of info we need to figure out. Formal: In this paper, we’re using a latent variable approach to handle multi-label classification. The main idea behind this is that the latent variable is usually made up of words that show up in the data, and the vectors for that latent variable are based on how often those words appear. We treat the relation vector (which is just a way to describe the data) as a vector with two parts: one for the words in the dataset and another for the words around the relation.\n\nTo make sure the vectors for the two parts are similar, we use a softmax function on the vector for the dataset and the vector for the relation, like this: Formal: Each word in the dataset gets turned into a vector, but the vectors around the relation can be thought of as vectors. So, the probability of a word in the dataset and its vector are kind of like the same thing. Formal: After we have these vectors, we calculate the probability P(w i |h j |t), which is the chance that a word and its vector appear together. Formal: We’ll use the softmax function with these vectors to get the probabilities P(w i |h j |t). Formal: We’re not going to explain how to calculate these probabilities because we’re focusing on the individual words in the dataset instead of the whole dataset. Formal: We’re assuming that the vector w i has the same meaning as the word h j, so P(w i |h j |t) is the same as P(w i |h j |t). Formal: Using the example of the word w i in the dataset, the probability of the word w j in the relation is calculated as: Formal: Here, h j is just the latent variable for the word w j. Formal: The probability P(w i |h j |t) is calculated by multiplying the vectors P(w i |h j |t) with the probabilities P(w i |h j |t) multiplied by the softmax function. Formal: Lastly, the probability P(w i |h j |t) is the sum of all the probabilities P(w i |h j |t) for all the words in the dataset. Formal: We’re using some math stuff to",
        "directory": "C88",
        "filename": "C88-1065.json"
    },
    {
        "casual_text": "The link to summarization isn’t super clear, so we’re using a rough version of the model by Wang and Ling (2016) as a starting point in our experiments. Summarization research is a huge field, and we’re not diving into all of it here. If you want a deeper look, check out Gambhir and Gupta’s survey from 2017. Recently, we’ve been focusing on pulling out the main argument in just two sentences to use as a snippet for argument searches (Alshomary et al., 2020). Meanwhile, Egan et al. (2016) have been working on abstractive summaries that capture the key points of debates. We’re thinking there might be a connection between the conclusion’s stance and its premises. On a broad level, this is kind of similar to what Angelidis and Lapata (2018) did, where they modeled aspects and sentiments for summarizing opinions.",
        "formal_text": "The exact connection to summarization is unclear, which is why we include an approximation of the model of Wang and Ling (2016) as a baseline in our experiments. General research on summarization is manifold and beyond the scope of this work. For a survey, we refer the reader to Gambhir and Gupta (2017). In recent work, we summarize the core of an argument to be used as a snippet in the context of argument search by a two-sentence extract (Alshomary et al. , 2020) and Egan et al. (2016) create abstractive summaries of the main points in a debate. We hypothesize a dependency between the target and stance of a conclusion and those of the premises. At a high level, this resembles the work of Angelidis and Lapata (2018) where aspects and sentiments are modeled for the extractive summarization of opinions.",
        "GPT2_formal_text": ". The next part will explain how to find the best model for a specific task and then show how it can be used to solve a problem. Formal: Sure! Here's a more casual version:\n\nIf we have a source sequence S, we figure out a set of target sequences T, where each target sequence T i equals an embedding of the source sequence s i. Formal: The thing is, for a given source sequence s i, we only have a specific set of target sequences T i. Formal: We tweak the cross-entropy loss by adding a penalty to each target sequence T i. Formal: We do this by adding up the cross-entropy losses for each target sequence in T. Formal: The cross-entropy loss can be thought of like this: Formal: The cross-entropy loss is calculated based on the target sequence T i. Formal: In this case, T i = {s i, T i ∈ T }, where T i ∈ T. Formal: The cross-entropy loss for a specific target sequence t i is calculated based on the source sequence s i. Formal: To calculate the cross-entropy loss for a target sequence t i, we use the cross-entropy loss for the target sequence s i, which is calculated as the cross-entropy loss for T i. Formal: You can see the formula for cross-entropy loss here: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "acl",
        "filename": "2020.acl-main.399.json"
    },
    {
        "casual_text": "4. How well the scores and their meanings can be applied to different groups of people, situations, and tasks.",
        "formal_text": "4. Generalizability Extent to which score properties and interpretations can be generalized to and across population groups, settings, and tasks.",
        "GPT2_formal_text": "ed_tokens.txt\" [TODO] The context vector c_t is made up of three parts: c_0, c_1, and c_2. Formal: For each token w_i, the word embedding h_t is calculated using this formula: Formal: We can easily find the vector for the word w_i, which is in R^(n×d_i) as c_0. Formal: Following the approach of Xu et al. (2016a), we can use a feed-forward network (FFN) to get the word embedding h_t. Formal: Following the idea of He et al. (2016), we can use an attention mechanism to create a context-aware representation for the word w_i, which we call c_t. Formal: The attention mechanism is a bidirectional long short-term memory (BiLSTM) model that works on a multi-dimensional vector that's shaped like a square. We feed this encoded vector into an encoder block that's shaped like a square, and the final output is a non-linear function that decides the attention weight between the token and the word embedding. Formal: The attention mechanism works with an encoder block and an attention mechanism to create a context-aware representation of the token w_i, which we call c_t. Formal: Finally, the final representation for w_i is a biLSTM with an attention mechanism, which we call c_t. Formal: Formal: The weights for the words in the input context vector are figured out based on how much attention the token and word embedding pay to the tokens in the input. Formal: The context vector c_t for a token w_i is calculated by summing up the attention values of the token and word embedding. Formal: Finally, the final representation for w_i is a biLSTM with an attention mechanism, which we call c_t. Formal: Formal: The attention mechanism works with an encoder block and an attention mechanism to create a context-aware representation of the token w_i, which we call c_t. Formal: The attention mechanism is a bidirectional long short-term memory (BiLSTM) model that's shaped like a square. Formal: We feed the token and word embedding into an encoder block, and the output is a non",
        "directory": "eacl",
        "filename": "2021.eacl-main.137.json"
    },
    {
        "casual_text": "From what I can see, STATEMENT is a big reason for mix-ups in the data because it's the most common type. A lot of RE-QUEST and RESPONSE ACK examples got labeled as STATEMENT by mistake. We didn't use stuff like question marks in our features, but adding that could probably make things better.",
        "formal_text": "Looking over the data, STATEMENT is a common source of misclassification, as it is the majority class in the data. In particularly, a large number of RE-QUEST and RESPONSE ACK utterances were tagged as STATEMENT. We did not include punctuation such as question marks in our feature sets; including this would likely improve results further.",
        "GPT2_formal_text": ": the first 3 are the score vectors from the entity embedding layer, while the rest are the score vectors from the paragraph embedding layer. Formal: We use the cross-entropy loss to figure out how good the entity embedding vectors are for the paragraph embedding layer. Formal: To get a sense of how helpful the entity embedding vectors are for the entity embedding layer, we take the log loss of the entity embedding vector along with its context vectors as our evaluation score. We calculate this score using the cross-entropy loss, and then we normalize it to get the final score. Formal: We also measure how helpful the entity embedding vectors are for the paragraph embedding layer by adding up the log loss from both the entity and paragraph embedding layers. Formal: We take the log loss for the entity embedding vector along with its context vectors as our evaluation score for the entity embedding layer. Formal: Lastly, we normalize the loss scores to get the final score for the entity and paragraph embedding layers. Formal: The cross-entropy loss is calculated by integrating out the cross-entropy loss from the entity and paragraph embedding layers. Formal: The entity and paragraph embedding layers are both scaled by -1.5, and then we normalize their losses to get the final scaled loss for the entity and paragraph embedding layers. Formal: Figure 2 gives an example of how the entity and paragraph embedding layers work together. Formal: Formal: We train the entity and paragraph embedding layers separately using the cross-entropy loss. Then, we train them together using the cross-entropy loss. Formal: To predict the entity embedding vector using the context vector, we use the cross-entropy loss. Formal: To predict the entity embedding vector using the entity embedding vector, we use the cross-entropy loss. Formal: Formal: In this part, we’ll explain how to measure how helpful the entity and paragraph embedding layers are for each other. Formal: Let’s start by explaining the entity and paragraph embedding layers and the cross-entropy loss for the entity and paragraph embedding layers. Formal: The entity and paragraph embedding layers together are scaled by -1.5, and then we normalize their losses to get the final scaled loss for the entity and paragraph embedding layers. Formal: The entity and paragraph embedding layers together",
        "directory": "D10",
        "filename": "D10-1084.json"
    },
    {
        "casual_text": "Adverbs in English are super flexible. They can fit into just about any spot in a sentence, as long as they stay close to the verb. Fowler talks about this in his section on adverb placement and gives some wild examples, including cases where he suggests splitting infinitives.",
        "formal_text": "Adverbs in English are most accommodating. They permit themselves to be push into any convenient slot, so long as they do not get too far away from their verb. Fowler under Position of Adverbs has a marvelous chamber of horrors, and some examples where he recommends the split infinitive.",
        "GPT2_formal_text": "is the attention mask we use, as mentioned in Section 2.2. Formal: In this part, we're going to show you the main ideas behind the \"masked language model.\" Formal: But here's the thing: if we're talking about translating a sentence from a target language to a target language using a model that's been trained on something called a target-language data set, then the model should have access to that target-language data set. This is called cross-lingual transfer learning. Formal: But in this paper, we're more focused on translating from a source language to a target language. Formal: So, in this setup, we train the model to handle both the source and target languages in one go. Formal: The main idea behind our masked language model is to teach it a new way to handle words and phrases in the target language that aren't seen in the source language. Formal: To do this, we create a representation of the target language that's different from what we've seen before. Formal: To make this difference, we take out the words and phrases that aren't in the target language from the source sentence using a neural network. Formal: To make the model adapt to the new data, we use a cross-encoder setup. Formal: Finally, we plug this new representation into the model and keep going, making sure it learns the rest of the target language. Formal: Our approach for adapting a neural language model to a target language is similar to the word-level decoder and phrase-level decoder used in a lot of other work (like Zhao et al., 2016; Elbayad et al., 2017; Ran et al., 2018; Xie et al., 2018; Yang et al., 2019). Formal: In our experiments, we tested our approach with different neural network setups and different levels of multi-label classification. Formal: In Section 4.1, we compare our model with other methods for adapting neural models. Formal: We also looked at how well the adapted model did in both single-task and multi-task setups. Formal: We created two datasets to test our method: one is called the \"masked language model\" and the other is called the \"translated language model.\" Formal: We tested our method with two main datasets: the \"masked language model\" and the \"translated language model.\" Formal: We used a multilingual BERT-base model (Dev",
        "directory": "bcs",
        "filename": "1984.bcs-1.21.json"
    },
    {
        "casual_text": "Looking at this big collection of English texts, we've managed to figure out a pretty detailed grammar for this specific type of English (check out Harris 1968 and Kittredge and Lehrberger 1982 for more info).",
        "formal_text": "Examination of this extensive corpus of English has led to a fairly detailed grammar of this sublanguage (cf. Harris 1968, Kittredge and Lehrberger 1982).",
        "GPT2_formal_text": "= (q * t, g * t, h * t) (8) Formal: A user sends their query to the system, which checks it against a knowledge base. The system picks the best response by comparing it to a set of options that are ranked based on how relevant they are. Formal: Once the system decides on the best response, it sends back that same query and the system tries to match it with a knowledge base by searching for the best matching entity. Formal: The user sends a query, and the system checks it against a knowledge base. After that, the system uses a greedy algorithm to find the best match. Formal: The user sends a query, the system checks it against a knowledge base, and the best match is found using a greedy search algorithm. Formal: The user sends a query, the system checks it against a knowledge base, and the best match is found using a greedy search algorithm. Formal: The user sends a query, the system checks it against a knowledge base, and the best match is found using a greedy search algorithm. Formal: The user sends a query, the system checks it against a knowledge base, and the best match is found using a greedy search algorithm. Formal: The user sends a query, the system checks it against a knowledge base, and the best match is found using a greedy search algorithm. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and picks the best match from a set of options. Formal: The system checks the knowledge base and",
        "directory": "C86",
        "filename": "C86-1132.json"
    },
    {
        "casual_text": "• The directions are indicated by a term called \"d,\" which is set to -1. If we keep d at -1, the input will be processed in the forward direction. If we change it, the input will be processed in the backward direction.",
        "formal_text": "• The directions are marked by a direction term d = (−1). If we fixed d to −1, the input will be processed in forward direction, otherwise backward direction.",
        "GPT2_formal_text": "ing method, which basically turns each image into a special code. The codes for each image are grouped together and then sent to a teacher model to figure out how to combine them. The teacher then uses the combined image codes to teach the classifier how to predict the next image, like predicting the target class. The problem is, it's super hard to figure out the right sequence of codes for each image. So, we came up with a way to automatically create a code sequence for each image and label the classifier as the one that predicts the next image. For instance, let's say the input is the image x and the code sequence y = [x, y, y] = [1, 2, ..., n]. The classifier y that predicts the next image is labeled as the one that gets the highest probability p θ (y | x) = arg max p θ (y). Formal: We can write this whole process in a straightforward way by giving each image code a label. Formal: To make sure the code sequences aren't messed up, we add a special step after combining each image code. In this step, we remove any messed-up code sequences that aren't the right sequence for the image. Formal: We also add a limit to how much of the code sequence can be messed with. We set a limit to prevent the classifier from getting too confident. Formal: We use a special masked language model called GPT-2 (Radford et al., 2019) to handle the prediction. We train it using the code sequences we got from the previous step. Formal: You can check out the code sequences we trained in Fig. 2. Formal: We've included the dataset and the pre-trained GPT-2 model in our work. Formal: We trained the model using the masked language model (GPT-2) with some pretty good results on the code sequence labeling task. Formal: Formal: Lastly, we used the pretrained BERT model (Devlin et al., 2019) to predict the target class. Formal: Formal: Formal: To get a better understanding of how our model works, we did a classification test. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C18",
        "filename": "C18-1124.json"
    },
    {
        "casual_text": "We want our questions to focus on specific types of connections between ideas, called \"relation senses.\" To make sure we cover a wide range of these connections, we created a bunch of question templates based on the PDTB 3.0 (a big database of discourse relations, created by Webber et al. in 2019 and Prasad et al. in 2008). You can check out these templates in Table 3. Each question starts with a specific prefix that tells you what kind of relation sense it’s asking about. The placeholder X is filled in to point to the part of the text the question is about, like in Table 1.\n\nNot all PDTB senses are covered by our prefixes, though. For example, senses that have extra details about beliefs or speech acts (like \"Belief\" or \"SpeechAct\") were grouped under a more general sense. Also, we left out three \"Expansion\" senses because they don’t usually introduce new information that you could ask a question about. Instead, they focus on how the text is structured. One of these is \"Expansion. Conjunction,\" which is super common in the PDTB, especially in phrases where verbs are connected within a sentence. It makes up around 70% of the instances of this sense.\n\nFor example, in Ex. (2), there’s a discourse relation with two senses, one of which is \"Expansion. Conjunction.\" While it’s easy to think of a question about a causal sense, the conjunction part doesn’t really introduce any new information that you’d ask a question about.",
        "formal_text": "The Discourse Relation Sense We want our questions to denote relation senses. To define the set of discourse relations covered by our approach, we derived a set of question templates that cover most discourse relations in the PDTB 3. 0 (Webber et al. , 2019; Prasad et al. , 2008), as shown in Table 3. Each question template starts with a question prefix, which specifies the relation sense. The placeholder X is completed to capture the discourse unit referred to by the question, as in Table 1. Few PDTB senses are not covered by our question prefixes. First, senses with pragmatic specifications like Belief and SpeechAct were collapsed into their general sense. Second, three Expansion senses were not included because they usually do not assert a new \"informational\" proposition, about which a question could be asked, but rather capture structural properties of the text. One of those is Expansion. Conjunction, which is one of the most frequently occurring senses in the PDTB, especially in intra-sentential VP conjunctions, where it makes up about 70% of the sense instances. Ex. (2) displays a discourse relation with two senses, one of which Expansion. Conjunction. While it is natural to come up with a question targeting the causal sense, the conjunction relation does not seem to assert any proposition about which an informational question may be asked.",
        "GPT2_formal_text": "= (x_1, x_2, ..., x_n) # P_t = P_i, where i can be any number from 1 to T. Formal: In this paper, we use the WordPiece vocabulary, which is made up of characters from the ASCII set. You can check out more details about this vocabulary in Table 1. Formal: One cool thing about using the BUP-XG corpus for question answering is that it lets us grab phrase tables. Plus, the QA corpus is super useful for question-answer pair mining because it gives us detailed info about the topics people are asking about. Formal: In this part, we’ll quickly go over how we extract question-answer pairs using the BUP-XG corpus. Formal: We’ve got a table showing the language stuff we’re looking at in Table 2. Formal: We’re looking at three different setups to create question-answer pairs: a zero-shot setup, a few-shot setup, and a few-shot + translation setup. Formal: We’ve got two ways to measure how well we’re doing. Formal: The first method uses F1 score for question-answer pairs to see how well we’re extracting matching answer sentences. Formal: The second method is based on the BUP-XG corpus. It uses BUP-XG to calculate the question-answer pair loss. Formal: Since the BUP-XG corpus is all about answering questions, it’s naturally useful for question-answering. Formal: We’ve got a few examples of questions and their corresponding answers in Table 1. Formal: We’ll also look at three different ways to extract answer sentences from the BUP-XG corpus, and we’ll explain those in more detail in the next sections. Formal: The idea behind word embedding is that words are like points in a space that can represent different types of information. Formal: For each question, we’ll create a set of three embeddings. Formal: We’ve got a few examples of questions and their corresponding answers in Table 1. Formal: We’ll also look at three different ways to extract answer sentences from the BUP-XG corpus, and we’ll explain those in more detail in the next sections. Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.224.json"
    },
    {
        "casual_text": "With machine learning becoming a big deal in computational linguistics and doing really well in lots of areas, there's a growing need for lots of human-labeled data to train and test these algorithms. In the area of figuring out time-related stuff, this has resulted in the creation of TimeBank (Pustejovsky et al., 2003), which uses the TimeML language for labeling (Pustejovsky et al., 2005). TimeML is on its way to becoming an ISO standard for marking up events and time expressions (ISO/TC 37/SC 4/WG 2, 2007). TimeBank was also made available for TempEval-2007, the first competition focused on automatically figuring out the relationships and order between events and time expressions.",
        "formal_text": "As machine learning approaches are gaining dominance in computational linguistics and producing state-of-the-art results in many areas, they have in turn fueled the demand for large quantities of human-annotated data of various types that machine learning algorithms can be trained on and evaluated against. In the temporal inference domain, this has led to the creation of TimeBank (Pustejovsky et al. , 2003), which is annotated based on the TimeML language (Pustejovsky et al. , 2005). TimeML is becoming an ISO standard for annotating events and time expressions (ISO/TC 37/SC 4/WG 2, 2007). A version of the TimeBank has been provided as a shared public resource for TempEval-2007, the first temporal evaluation campaign aimed at automatically identifying temporal relations between events and time expressions as well the temporal ordering between events.",
        "GPT2_formal_text": "_v2' and 'attention_mask_v4' to get the final representation for the sentence. Formal: Let’s start by looking at the pre-trained Transformer from Vaswani et al. (2017), which is basically an upgraded version of BERT (Devlin et al., 2019). These pre-trained models can predict the next token by looking at the tokens around it. Formal: Here’s what we’ve done: Formal: To get the final hidden representation for the sentence, we’ve added some extra features (called attention weights) and a bias term (β) to the input sequence. Formal: We trained the model using the sentence from Table 1, which has two sections: the first and second parts. Then, we used this sentence to create the representation for the whole document. Formal: The main goal of the process is to estimate the next token in the context by using this representation. Formal: We’ve made sure that the input representation for the sentence is less than or equal to the hidden representation of the token. Formal: We’ve combined the hidden representation and the representation of the token into a single vector, using the results from the first and second parts. This vector has some weights, β, that helps balance how much the model learns from each part. Formal: To estimate the hidden representation for the next token, we use the knowledge from the first and second parts of the document, which we’ll call ℓ. Formal: We’ve tweaked the model by applying a softmax function to the output vector. Formal: To get the final representation for the sentence, we’ve added some extra features (called attention weights) and a bias term (β) to the input sequence. Formal: We trained the model using the sentence from Table 1, which has two sections: the first and second parts. Then, we used this sentence to create the representation for the whole document. Formal: The main goal of the process is to estimate the next token in the context by using this representation. Formal: To get the final representation for the sentence, we’ve added some extra features (called attention weights) and a bias term (β) to the input sequence. Formal: We tweaked the model by applying a softmax function to the output vector. Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C10",
        "filename": "C10-2156.json"
    },
    {
        "casual_text": "Cerco un posto dove mangiare hamburger vicino a \"Stagno Bosco\".",
        "formal_text": "Neural Machine Translation sto cercando un posto da \" hamburger \" vicino a \" stagno bosco \".",
        "GPT2_formal_text": "_string. t) = s_i - t_i: # Calculate the argument-role attention score for the s-th token x_s = arg_role(x_s, s_i) - arg_role_score(x_s, s_i) # Formal: The model is trained using a series of convolutional layers and a max pooling layer with max-pooling applied. The goal is to minimize the negative log-likelihood, which basically means finding the embedding vector that maximizes the log-likelihood. Formal: So, we can think of it like this: Formal: First, we get an input x_i with s elements. Then, we turn those elements into a hidden representation called h_i using a CNN. After that, we calculate the attention vector using the max pooling layer with max-pooling. Formal: Next, we use a max pooling layer with max-pooling applied to a hidden layer to get the output. Formal: Finally, we map this output to a vector using a fully connected layer. Formal: To get the attention vector for the x-th token, we do a max pooling with max-pooling followed by a fully connected layer. Formal: Here, S_i is the hidden state from the first convolutional layer of the CNN, and V_s is the hidden state of the first max pooling layer of the CNN. Formal: Finally, we calculate the attention vector using max-pooling with max-pooling, which gives us the attention vector for the x-th token. Formal: Formal: The model takes a hidden state h_i and outputs a vector h_i. Formal: Finally, we map this output to a vector using a fully connected layer. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.481.json"
    },
    {
        "casual_text": "Alright, so when it comes to pulling out information about cause and effect from text, there are two main types of tasks: causal phrase extraction and causal clause extraction. The first one is all about grabbing specific words or short phrases that show a cause-effect relationship.",
        "formal_text": "The tasks concerning causality extraction in text can be mainly divided into two categories, causal phrase extraction and causal clause extraction, where the former focuses on extracting word",
        "GPT2_formal_text": "ing_attn (s_i, t_i) = l_attn_attn(s_i, t_i) k = t_i + 1 c = 1 + e * tanh(attn_solve(s_i, c, t_i))            Casual: So, e is the element-wise multiplication, s_i is the hidden state vector, and c is the number of non-zero elements in the sequence. Formal: To get the attention vector for a specific token t_i in the sequence of tokens s, the attention masking attention is calculated like this: Formal: Each sentence s_i gets its own attention vector t_i, and the attention masking vector is calculated by combining the attention vectors for each sentence s_i in the sequence. Formal: Let’s say the attention vector for a token t_i in the sequence of tokens s is calculated like this: Formal: Let’s say the attention vector for a token t_i in the sequence of tokens s is calculated by combining the attention vectors for each sentence s_i in the sequence. Formal: The attention masking attention can also be written as a single scalar x. Formal: The final attention vector for the token t_i in the sequence of tokens s is what we call the attention masking attention vector. Formal: Since the attention vectors for different tokens in the sequence can vary a lot, we’ll call the final attention vector for the token t_i in the sequence of tokens s_i. Formal: This way of representing attention vectors is a bit different from the usual attention methods, which only look at the attention vectors for each token in the sequence and combine them into one representation. Formal: So, the final attention vector for the token t_i in the sequence of tokens s_i is what we call the attention masking attention vector. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.252.json"
    },
    {
        "casual_text": "This project is all about creating trees made up of topical segments. Each segment has a center that best represents its content. The main goal is to maximize net similarity, which is the total similarity between all centers and the data points they represent. The entire sequence of data points gets divided at each level of the tree, but with one rule: at each level (l), the centers must be part of the centers from the previous level (l-1). Figure 1a shows a part of the factor graph that explains how HAPS works for levels l and l-1.\n\nThe tree has L levels, starting from the top (l = L) down to the bottom (l = 1). The little numbers on the factor and variable nodes show which level they're on. At every level, there are N² variable nodes called cᵢⱼʲ and N variable nodes called eⱼ (N is the number of data points in the sequence). Each variable is either 0 or 1: cᵢⱼʲ = 1 means data point i at level l is part of the segment centered around data point j; eⱼ = 1 means there's a segment centered around j at level l.\n\nThere are four types of factor nodes in Figure 1a: I, E, C, and S. The I factors make sure each data point is in exactly one segment and that the centers at level l are part of those from level l-1. The E nodes ensure that segments are grouped neatly around their centers in solid blocks, not scattered all over. The values for I and E are 0 if everything's good and -∞ if something's wrong. The S factors measure how similar the data points are to each other.",
        "formal_text": "This work aims to build trees of topical segments. Each segment is characterized by a centre which best describes its content. The objective function is net similarity, the sum of similarities between all centres and the data points which they exemplify. The complete sequence of data points is to be segmented at each level of the tree, subject to the following constraint: centres at each level l, l > 1, must be a subset of the centres from the previous level l − 1. Figure 1a shows a fragment of the factor graph describing HAPS corresponding to levels l and l − 1. The tree has L levels, from the root (l = L) down to the leaves (l = 1). The superscripts of factor and variable nodes denote the level. At each level, there are N 2 variable nodes c l ij and N variable nodes e l j (N is the number of data points in the sequence to segment). A variable's value is 0 or 1: c l ij = 1 ⇔ the data point i at level l belongs to the segment centred around data point j; e l j = 1 ⇔ there is a segment centred around j at level l. Four types of factor nodes in Figure 1a are I, E, C and S. The I factors ensure that each data point is assigned to exactly one segment and that segment centres at level l are a subset of those from level l − 1. The E nodes ensure that segments are centred around the segment centres in solid blocks (rather than unordered clusters). The values of I and E are 0 for valid configurations and -∞ otherwise. The S factors capture similarities between data points.",
        "GPT2_formal_text": "= { u_i, v_i } t = input_tokens(s) 2 t = input_tokens(s) 3 t = input_tokens(s) 4 t = input_tokens(s) 5 Formal: Okay, so the final result is like a mix of the two, but it's not really what you'd call smooth. Formal: In this situation, the context vector u_i represents the unique word feature. The first token, s_1, is only learned during the first round of LSTM decoding, meaning it doesn't get used during the second round. The input word vectors, v_i, are then combined with the context vectors to figure out the overall context. Formal: The reason we're doing this is to see how much information we get from the input words by looking at how the input and context vectors interact. Formal: We can show how much more information is gained by tweaking the model by picking more tokens and tweaking the model with these hidden states. Formal: We ran a bunch of tests to figure out which layers help and which don't, to see which parts of the network are really important for understanding what the user is saying. Formal: We found that using recurrent layers worked the best for the concept identification task. Formal: We checked how the model performed with different attention patterns. Formal: We also checked how similar the outputs were, and you can find all that info in Table 3. Formal: We used BERTScore (from Devlin et al., 2019) to measure how similar the outputs were. Formal: Formal: Here, x_i stands for the input word vector, v_i is the context vector, and u_i is the hidden state vector from the last layer of the decoder. Formal: For the decoding phase, we changed the number of input tokens from 5 to 200. Formal: Formal: We used the same LSTM architecture for the word2vec model as what was used for the word2vec word embedding model. Formal: Formal: Formal: We tweaked the base model's parameters using Dropout (Srivastava et al., 2014) to make it more resistant to overfitting. Formal: Formal: We built a pretrained model using the Transformer architecture (Vaswani et al., 2017) and trained it for 1000 epochs. Formal: Formal:",
        "directory": "C14",
        "filename": "C14-1005.json"
    },
    {
        "casual_text": "In GA-soft, if the angle θ is bigger than 90 degrees, g P KD ends up going in the opposite direction of g CE. This could either slow down or even reverse the direction of the gradient descent.",
        "formal_text": "In GA-soft, when the angle θ is larger than 90 •, g P KD is in the opposite direction with g CE, thus might slow down or reverse this gradient descent direction.",
        "GPT2_formal_text": "_len: This is the length of the mask vector we get for the input to the attention network. In our tests, we set the output word embedding to 32. The output size is 32 because the attention mechanism is a binary CRF with a hidden dimension of 32. The variable y_i stands for the i-th word in the sentence. Formal: We take the output from the attention layer and feed it into the next output layer to get the predicted word embedding for the next word. Formal: Let's say \"y_i\" is the word embedding for the word i. Formal: The attention_mask_len is a positive number, meaning the output layer doesn't add any extra bias to the hidden representation. Formal: In the second layer, we add the current hidden state h_i to the output layer to get the hidden representation for the word \"y_i.\" Formal: The output layer looks like this: Formal: Finally, we pick the hidden vector h_i that gives us the correct prediction for the next word, y_i. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.242.json"
    },
    {
        "casual_text": "Doran and his colleagues (2001) found that the more times you try to start something, the less likely you are to succeed each time.",
        "formal_text": "Doran et al. (2001) demonstrate a negative relationship between number of initiative attempts and their success rate.",
        "GPT2_formal_text": "= [q_i, t_i, m_i, u_i, q_m, t_m]\n           Casual: The process is pretty much the same as what's done in sentence compression, except instead of dealing with phrases, it deals with pairs of phrases. Formal: In this project, we're looking at how to create sentences that are shorter than the original ones. To do this, we use something called \"sentence compression.\" The idea is to shrink the original sentence's length down to the same number of words, but keep the important stuff, like the important phrases and how they fit together. Formal: For each phrase pair, we run a bunch of sentences through a fancy neural network called a Recurrent Neural Network, which has a hidden layer with d dimensions. The goal is to maximize the log likelihood of the new phrase pairs. Formal: For each pair of phrases, we find the shortest possible sentence using a log-linear model. Then, we calculate the phrase pairs that are the most likely to have the shorter sentence. Formal: We used the length reduction method we talked about earlier (from He et al., 2015) to find these phrase pairs. We then split the original sentence into sentences with the shortest phrase pairs, and we use a neural network to make sure we get them right. Formal: We tested our model on a bunch of datasets, and the results are in Table 2. The results show that the model works pretty well. Formal: As we mentioned earlier, a lot of research has been done on sentence compression, and we're focusing on the problem we're working on now. Formal: We ran the model on two popular datasets, and the results are in Table 3. From what we can see, our model works really well—it's the best model we've seen so far. Formal: In this project, we're looking at how to create sentences that are shorter than the original ones. To do this, we use something called \"sentence compression.\" The idea is to shrink the original sentence's length down to the same number of words, but keep the important stuff, like the important phrases and how they fit together. Formal: For each phrase pair, we run a bunch of sentences through a fancy neural network called a Recurrent Neural Network, which has a hidden layer with d dimensions. The goal is to maximize the log likelihood of the new phrase pairs. Formal: We used",
        "directory": "E06",
        "filename": "E06-1024.json"
    },
    {
        "casual_text": "Usually, the numbers we predict form a smooth line that goes across time and space. To make things easier, we often just pick regular times (like every hour) and places (either on a grid or at weather stations) to give those numbers. Or, we might only focus on the big changes. MAR-WORDS is cool because it can handle both ways of describing the data. But honestly, figuring out exactly what kind of data we need is still a bit of a puzzle that we need to work on more.",
        "formal_text": "In the normal course of events, the predicted values make up a continuum in both time and space. For simplicity, values are often given at regular steps in time (e. g. , hourly) and space (either at grid points, or at weather observing sites). Alternatively, forecast parameters may be given in terms of significant changes only. MAR-WORDS is flexible enough to accept both types of data description. In fact, the structure and nature of the required data is a problem which needs more work to resolw~.",
        "GPT2_formal_text": "ing , which helps figure out which words are the important ones for a given event. When dealing with multi-label text, figuring out the best attention weights is super important. We call this whole process \"masking attention\" to show how important the words in the text are for the task we're working on. Formal: A model is considered good if it can correctly answer a question by giving just one answer, as long as all the correct answers are included. Formal: For the goal-based question-answering task (like SQuAD 1.1), we use this attention mechanism to pick out the important words for the question, which we call \"important words.\" Formal: This is what we call the \"important words\" feature. Formal: For the multi-choice question-answering task (SQuAD 2.0), the attention mechanism helps us identify the important questions and answer them, which we call \"important questions.\" Formal: We also have something called the \"important questions\" feature, which helps us pick out the important questions for the user-generated questions (UGC). Formal: There's another key thing called the \"important words\" feature, which is a part of the attention mechanism. Formal: We also have an \"important questions\" feature. Formal: Formal: In our experiments, we use a supervised method to train our model. Formal: We train the model using the same steps we use for the supervised version. Formal: Formal: Here, we're checking how well our model can answer the question by looking at the F1 scores from the four different models. Formal: In the results section, we show how well the model handles the multi-choice question-answering task in SQuAD. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C86",
        "filename": "C86-1132.json"
    },
    {
        "casual_text": "Sure, I can help with that! Here's the informal version:\n\nIt can be used as supervision for the conditional generation task. Once the autoencoder is pretrained, the encoder (enc) can be used to turn the training data into pairs of vectors.",
        "formal_text": "is available as supervision for the conditional generation task. After pretraining the autoencoder, enc is available to transform the training data into pairs of vectors",
        "GPT2_formal_text": ", which are like the label embeddings for the questions. For the first part, we use a linear layer with a ReLU activation to encode the question-answer pairs. But in the second part, we adjust it a bit by adding a softmax layer with a ReLU activation to the embeddings. Formal: The second part of the method is to figure out the probability distribution for the questions. Formal: The third part of the method is to estimate the weight vector for each question based on the embedding of the answer. Formal: After calculating these probabilities, we add a softmax layer to the embeddings, which gives us the actual probability distribution for the questions. Formal: We tested our method on two datasets: CNN/Daily Mail and Common Crawl. The results are shown in Figure 2. The horizontal axis shows the accuracy of classifying questions as positive or negative, while the vertical axis shows the accuracy of answering questions. The results show that the method works pretty well. Formal: For the third part, we predict the probability distribution for each question using a Bi-LSTM (Hochreiter and Schmidhuber, 1997) with a hidden layer size of 128. Formal: We looked at how well it works on the same two datasets, CNN/Daily Mail and Common Crawl, using the same task model. The results are shown in Figure 3. As you can see, our method does really well, beating all the other methods by a good margin. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.491.json"
    },
    {
        "casual_text": "To set up Gibbs sampling, we need to figure out the marginal distribution for the observations (words), the topic assignments, and the table indicators. We use the Dirichlet integral to handle the document topic distributions and the topic-word matrix, integrating them out. Then, we use the joint posterior distribution for a PDP to gradually get rid of the segment topic distributions. After all that, we end up with the marginal distribution:  \n\np(z 1:I,1:J, w 1:I,1:J, u 1:I,1:J | α, γ, a, b) = (5)",
        "formal_text": "To complete a formulation suitable for Gibbs sampling, we first compute the marginal distribution of the observations w 1: I, 1: J (words), the topic assignments z 1: I, 1: J and the table indicators u 1: I, 1: J. The Dirichlet integral is used to integrate out the document topic distributions µ 1: I and the topicby-words matrix Φ, and the joint posterior distribution computed for a PDP is used to recursively marginalise out the segment topic distributions ν 1: I, 1: J. With these variables marginalised out, we derive the following marginal distribution p( z 1: I, 1: J, w 1: I, 1: J, u 1: I, 1: J α, γ, a, b) = (5)",
        "GPT2_formal_text": "_8g, src = [i, j], where i and j are the positions of the first and last words of a pair, respectively. Then, the attention is passed on to the first and last words of the next pair, which are pairs that come after the current one. Formal: To make sure the attention isn't wasted, we use a graph attention mechanism. Specifically, the three-layer attention mechanism (Hochreiter and Schmidhuber, 1997) is used. Formal: We also use a discriminative attention mechanism to handle the text selection process. The objective function we use is log-likelihood (Hochreiter and Schmidhuber, 1997), with a decay factor of 2.5. Formal: When we combine the three-layer attention mechanism with the discriminative attention mechanism, we end up with a single-layer attention mechanism (Hochreiter and Schmidhuber, 1997). Formal: Here, the z represents the number of different input vectors, and the k-th one is the attention vector for the k-th word. Formal: We show how the attention mechanism, discriminative attention, and our new model work in Figure 3. Formal: For the graph attention mechanism, we use the M-path algorithm (Hochreiter and Schmidhuber, 1997). Formal: In the graph attention mechanism, the query vector is the output from the graph attention mechanism, and the key vector is the key that's most closely connected to the query vector. Formal: Formal: Formal: To get the attention vector for the k-th word, we use the k-th word embedding vector. Formal: Formal: The graph attention mechanism also works with a sub-graph attention mechanism. Formal: The sub-graph attention mechanism works by taking the sub-graph's sub-graph vector as the key. Formal: The sub-graph attention mechanism is similar to the graph attention mechanism but instead of the sub-graph's attention vector, it uses the key vector. Formal: Formal: Formal: The sub-graph attention mechanism can also be combined with the sub-graph attention mechanism. Formal: Formal: Formal: Formal: Formal: We also use the Inference Graph Attention Mechanism (Li and Bansal, 2017). Formal: Formal: Formal: The Inference Graph Attention Mechanism (Li and Bansal, 2017)",
        "directory": "D12",
        "filename": "D12-1049.json"
    },
    {
        "casual_text": "The NYTimes800k dataset is a bigger and more complete collection of New York Times articles, images, and captions. It's about 70% larger than the previous one. The dataset is split into 763K articles for training, 8K for validation, and 22K for testing. Table 5 shows a detailed comparison between GoodNews and NYTimes800k, focusing on the length of articles and captions, as well as the composition of the captions.",
        "formal_text": "NYTimes800k dataset is 70% larger and more complete dataset of New York Times articles, images, and captions. The number of train, validation and test sets are 763K, 8K and 22K respectively. Tab. 5 presents a detailed comparison between GoodNews and NYTimes800k in terms of articles and captions length, and captions composition.",
        "GPT2_formal_text": "= (q, s, t) for q, s, t in (q, s, t). Formal: So, we've got a bunch of tasks, like S1, S2, S3, and so on up to S_N. Formal: (1) We've got S1, S2, S3, and so on up to S_N. (2) We also have a set of responses, called G_s. Formal: In this paper, we talk about a method called residual connections (which are like residual connections in deep neural networks) that helps model the relationships between these tasks and their responses. After that, we suggest a way to summarize these relationships using something called a latent semantic vector. Formal: We made sure that G_s and G_s represent the relationships between tasks and their responses in a way that’s useful for summarization. Formal: We tested this idea by using the GQA dataset (from Radford et al., 2014) and another dataset called QQP (from Wiegreffe and Pinter, 2014) to see how well it works. Formal: We used the text-text and video-video datasets from Wiegreffe and Pinter (2014) to train our model on, and then we fine-tuned it on QQP and QScal to get the final results. Formal: Formal: Here’s the link to the full dataset: http://cdu.mitre.org/projects/qqp/ Formal: Formal: To see how well the model performs on different datasets, we tested it on the GQA dataset. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "emnlp",
        "filename": "2021.emnlp-main.419.json"
    },
    {
        "casual_text": "Some folks have been trying to connect social tagging systems with ontologies. An ontology is basically a way to describe how things are related. Peter Mika (2005) came up with a more detailed social tagging system that includes actors, concepts, and objects. He used tags that often appear together to create an ontology from these social tags. Wu et al. (2006a) took a different approach, using hierarchical clustering to build an ontology from tags that show similar-to relationships.\n\nLater on, some new ontology schemes were suggested that work better with social tagging systems, like the ones by Van Damme et al. (2007) and Echarte et al. (2007). These mainly focused on how tags, objects, and users are related, rather than just the tags themselves. Alexandre Passant (2007) manually matched tags to domain ontologies to make information retrieval on social media better.\n\nTo automate this process, Angeletou et al. (2007) used ontologies created by experts to find connections between tags, but they found that it didn’t cover much. Specia et al. (2007) suggested a framework to organize tags using existing ontologies, but they didn’t test it out. Finally, Kim et al. (2008) looked at the latest methods for modeling tags with semantic annotations.",
        "formal_text": "People have also worked on bridging social tagging systems and ontologies. An ontology defines relations between entities. Peter Mika (2005) proposed an extended scheme of social tagging that includes actors, concepts and objects, and used tag co-occurrences to construct an ontology from social tags. Wu et al. (2006a) used hierarchical clustering to build ontology from tags that also use similar-to relations. Later, ontology schemes that fits social tagging system were proposed, such as (Van Damme et al. , 2007) and (Echarte et al. , 2007), which mainly focused on the relation between tags, objects and users, rather than between tags themselves. Alexandre Passant (2007) mapped tags to domain ontologies manually to improve information retrieval in social media. To construct tag ontology automatically, Angeletou et al. (2007) used ontologies built by domain experts to find relations between tags, but observed a very low coverage. Specia et al. (2007) proposed an integrated framework for organizing tags by existing ontologies, but no experiment was performed. Kim et al. (2008) summarized the state-of-the-art methods to model tags with semantic annotations.",
        "GPT2_formal_text": ". Formal: So, we're basically saying that the output gets a bit more spread out compared to the original input. This way, the model can make more use of the context. \n\nNow, if we're dealing with input a = [a1, ..., an], let's say b is the set of non-zero elements from a to b, and c is the set of non-zero elements from c to c. \n\nThe output from the encoder in the encoder block, which we'll call f(a), will be a vector. The output from the decoder block, f(b), will be a vector. \n\nFor the masked token, the vector from f(a) is just the dot product of the embedding of the token and the vector from f(b) for a to a. Formal: The loss function L(a) is a weighted average of the embedding vectors for the tokens a to a, where the weight is the inverse of the dot product. Formal: We're saying that the token's embedding is the sum of the embedding of the masked token and the embedding of the original token. Formal: If we're dealing with the masked token embedding, L(a) will be the same as L(b). Formal: Formal: We're also saying that the token embedding is the sum of the embedding of the masked token and the embedding of the original token. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "C10",
        "filename": "C10-1114.json"
    },
    {
        "casual_text": "In Tables 4 and 5, we’re looking at the analysis for the Gadgets and Iphones datasets, which are kind of like the Politics dataset we saw in Table 3. Compared to Politics, we noticed that for Gadgets and Iphones, the CATD-FLOW models show some ups and downs in performance when we bump K from 5 to 20. This might be because LSTMs aren’t great at remembering long-term stuff. This problem seems to pop up more when there’s not a lot of training data. \n\nHere’s some quick numbers:  \n- N COMBINE (K=20, B=5): 750, 431, 444  \n- O COMBINE (K=20, B=10): 750, 434, 445",
        "formal_text": "In Table 4 and 5, we show the analysis for for Gadgets and Iphones datasets similar to Poli-tics dataset in Table 3. As compared to Politics, we observe that for Gadgets and Iphones, CATD-FLOW models have some fluctuations in performance when we increase K from 5 to 20, which may be due to the limited capability of LSTMs for memorizing long-term history. This issue is more prevalent when the training data size is small. . 750. 431. 444 N COMBINE (K=20, B=5). 750. 434. 445 O COMBINE (K=20, B=10). 750. 434 445",
        "GPT2_formal_text": ", and it turns out that this method works better than the basic one. Formal: Here’s how the paper is laid out. Section 2 gives a quick overview of the problem, explains the typical solutions, and talks about the tasks involved. Section 3 explains the relation extraction module and the sentence-level seq2seq model we’re using. Section 4 covers the sentence-level model, including its architecture and the parameters it learns. Finally, Section 5 wraps things up with the results of our tests. Formal: This part talks about the different parts of the system we’re using. Section 2 gives a quick overview of the problem, explains the usual solutions, and outlines the tasks. Section 3 explains the relation extraction module and the sentence-level seq2seq model we’re using. Section 4 covers the sentence-level model, including its architecture and the parameters it learns. Section 5 wraps things up with the results of our tests. Formal: The paper is organized into sections. In section 2, we’ll give a quick overview of the problem, explain the typical solutions, and talk about the tasks. Section 3 talks about the relation extraction module and the sentence-level model we’re using. Section 4 covers the sentence-level model, including its architecture and the parameters it learns. Finally, Section 5 wraps things up with the results of our tests. Formal: Section 2 is all about the relation extraction module, which is written in Python. Section 3 talks about the relation extraction model, which is written in Java and is shown in Figure 1. Formal: Section 4 focuses on the sentence-level seq2seq model, which is written in C++ and is shown in Figure 2. Formal: In section 5, we’ll talk about the results of our tests. Formal: Finally, we’ll wrap things up with the conclusions. Formal: Section 2 is all about the relation extraction module, which is written in Python. Section 3 explains the typical solutions, and Section 4 covers the tasks. Section 5 covers the relation extraction model, which is written in Java and is shown in Figure 2. Formal: Section 4 is all about the sentence-level seq2seq model, which is written in C++ and is shown in Figure 2. Formal: Section 5 is about the results of our tests. Formal: In section 5, we’ll wrap things up with the conclusions. Formal: Section 2 is all about the",
        "directory": "D19",
        "filename": "D19-1682.json"
    },
    {
        "casual_text": "If the word is a relative pronoun, chances are pretty good that the next word will be a noun.",
        "formal_text": "If word is relative pronoun then there is high probability that next word will be noun.",
        "GPT2_formal_text": "_src_len.txt, which is the masked text. This gives us the probability of the input sequence given the masked token sequence. Formal: The output from the attention mechanism is calculated as the logit activation, which is 1 divided by the size of the input sequence. Formal: For a masked token sequence s_i, we take the probability of the output from the attention mechanism h_t_i and multiply it by the input sequence length, which is the size of the input sequence. Formal: Instead of trying to directly match the output of the attention mechanism, we can use a basic sigmoid function to get the output instead. Formal: For a masked token sequence s_i, we calculate the logit activation using the embedding layer. Formal: The hidden state vector h_t_i, which is the output for the attention mechanism, is calculated like this: Formal: For a masked token sequence s_i, we take the probability of the output from the attention mechanism h_t_i multiplied by the embedding layer. Formal: We use a basic sigmoid function to get the output. Formal: The hidden state vector h_s_i, which is the output from the attention mechanism, is calculated like this: Formal: If the input sequence length is greater than or equal to the size of the hidden state vector h_t_i, the output is zero. Formal: If the input sequence length is less than or equal to the size of the hidden state vector h_s_i, the output is one. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C12",
        "filename": "C12-3021.json"
    },
    {
        "casual_text": "Preprocessing involves breaking down text into tokens, identifying things like names, URLs, and numbers.",
        "formal_text": "Preprocessing includes tokenization, recognition of named entity, urls and numbers.",
        "GPT2_formal_text": "= (h, t) = (r, e) for i from 1 to N, t, k in T: (h, t) = (r, e) + h * (e - t) for i from 1 to N + 1, t, k. Formal: To get the max probability P(r|e), you first need to find the best e(r|e) by minimizing the log probability P(r|e) for all possible e(r|e). Formal: To keep the max probability P(r|e) as low as possible, we use something called a Lagrangian relaxation method. Formal: You can find the Lagrangian relaxation for the whole process in a paper by Lafferty et al. (2001). Formal: In Section 5, we'll explain this relaxation process in more detail. Formal: So, we've got P(r|e), which is the log probability P(r|e), and P(e|r), which is the log probability P(e|r). Formal: For any r, P(r|e) and P(e|r) are independent, meaning they're not correlated. Formal: For all e, P(e|r) is the log probability P(e|r), and P(e|r) is the log probability P(e|r) for any e. Formal: So, the equation looks like this: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D16",
        "filename": "D16-1036.json"
    },
    {
        "casual_text": "Leaderboards are kind of the go-to way to track progress in question answering (Rajpurkar et al., 2016) and a bunch of other NLP tasks (Wang et al., 2019a). But this popularity has a downside: people get obsessed with chasing the \"state-of-the-art\" (SOTA) without really digging into the data or models (Linzen, 2020). For instance, those \"super-human\" models that dominate question answering leaderboards (Najberg, 2018) often flop in real-world scenarios (Feng et al., 2018; Wallace et al., 2019a) because they’ve just learned some tricks that don’t actually work in practice (McCoy et al., 2019; Niven and Kao, 2019). \n\nAnd here’s the kicker: focusing only on the numbers makes it seem like progress in one specific task is the same as progress in real-world NLP challenges (Bender and Koller, 2020). Basically, just looking at those shiny SOTA numbers doesn’t tell us much about how things actually work or where they fall apart (Lipton and Steinhardt, 2019). Leaderboards can give us a sense of how hard, clear, or doable certain examples are. If something has really low discriminability, it might mean there’s an annotation mistake. Like, there was a question that scored super low: \"Why did demand for rentals decrease?\" when the real answer should’ve been \"demand for higher quality housing increased.\"",
        "formal_text": "Leaderboard evaluations-for better or worse-are the de facto standard for measuring progress in question answering (Rajpurkar et al. , 2016) and in many NLP tasks (Wang et al. , 2019a ). An unfortunate side effect of leaderboard popularity is SOTA-chasing, often at the expense of carefully inspecting data and models (Linzen, 2020). For example, the same \"super-human\" models that top question answering leaderboards (Najberg, 2018) often fail spectacularly (Feng et al. , 2018; Wallace et al. , 2019a) by learning non-generalizable statistical patterns (McCoy et al. , 2019; Niven and Kao, 2019). Finally, focusing solely on metrics conflates progress on a specific task with progress on realworld NLP problems behind the task (Bender and Koller, 2020). Plainly, focusing on headline SOTA numbers \"provide(s) limited value for scientific progress absent insight into what drives them\" and where they fail (Lipton and Steinhardt, 2019 leaderboards infer the difficulty, discriminativeness, and feasibility of examples. Negative discriminability suggests an annotation error; for example, the question with most negative discriminability asks \"Why did demand for rentals decrease? \" when the answer is \"demand for higher quality housing increased. \"",
        "GPT2_formal_text": "ing_tokens = { 0.01, 0.02, 0.03, 0.04, 0.05, 0.06, 0.07, 0.08, 0.09, 0.10, 0.11, 0.12, 0.13, 0.14, 0.15, 0.16, 0.17, 0.18, 0.19, 0.20, 0.21, 0.22, 0.23, 0.24, 0.25, 0.26, 0.27, 0.28, 0.29, 0.30 }   \n\nSo, in simpler terms, this is like taking the sum of two sums, each with its own subscript. Formal: The model learns to deal with the different possible values of the token-to-word mapping and the attention mechanism. Formal: The model learns to learn to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns to learn. Formal: The model learns",
        "directory": "acl",
        "filename": "2021.acl-long.346.json"
    },
    {
        "casual_text": "We’ve come up with a bunch of syntactic transformation rules for translating Japanese into English, mainly because the two languages have different structures. These rules handle things like verb, noun, and clause reordering. Even though we’re mainly focusing on Japanese to English, a lot of these rules can also work for other languages that switch from SOV (Subject-Object-Verb) to SVO (Subject-Verb-Object) structures.",
        "formal_text": "We design a variety of syntactic transformation rules for Japanese-English translation motivated by their structural differences. Our rules cover verb, noun, and clause reordering. While we specifically focus on Japanese to English, many rules are broadly applicable to SOV to SVO languages.",
        "GPT2_formal_text": "= {k1, k2, ..., kn} where ki stands for the length of the next word in the text. Formal: After that, we combine the features from each of the frames into a single feature vector called v_f. Formal: The attention weight for this vector is calculated as the sum of the dot products of all the feature vectors we got from the two frames. Formal: For instance, take the vector v_f for the first word in the text, which is: Formal: Let’s say f_i = {v_1, v_2, ..., v_n} and h_i = {h_1, h_2, ..., h_n}. In this case, the attention weight is calculated as: Formal: Formal: We’re using the same setup as in section 3.2.1. Formal: Formal: Formal: Also, we have to note down the vector for the current word in the text. Formal: Formal: Formal: Using Equation (1), here’s how we calculate the attention weight for the first word in the text: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D15",
        "filename": "D15-1006.json"
    },
    {
        "casual_text": "Re-ordering can be seen as a mix of different problems that are connected and can be explained or solved using various linguistic ideas. First off, a lot of long-range re-ordering happens because of differences in how phrases are ordered. Syntax-based and hierarchical models, like the one by Chiang (2005), try to tackle this issue. On a smaller scale, things like word re-ordering within a phrase can usually be predicted based on the nature of the words and their context, with POS tags being a pretty clear clue.",
        "formal_text": "Re-ordering can also be viewed as composing of a number of related problems which can be explained or solved by a variety of linguistic phenomena. Firstly, differences between phrase ordering account for much of the long-range reordering. Syntax-based and hierarchical models such as (Chiang, 2005) attempts to address this problem. Shorter range re-ordering, such as intraphrasal word re-ordering, can often be predicted from the underlying property of the words and its context, the most obvious property being POS tags.",
        "GPT2_formal_text": "ed_max, but we can also do the same thing using the original input x_i and its representation h_i. The main difference is that we use an attention layer after all the non-linear stuff has already happened, which is different from other methods. Formal: A common approach in NLP is to use a bag-of-words (BOW) model, which is a popular neural network model. In this model, the word embeddings are based on the logits of the words around the word, which are then combined using a weighted average. The thing is, the BOW model often messes up when it comes to predicting the next word. So, to fix this, people have come up with a bunch of improvements. These include smoothing techniques, ways to break down the BOW model into different parts (Hinton et al., 2015), adding more hidden units (Raffel et al., 2017), and creating features that help the model better understand how words relate to each other (Pires et al., 2017). Formal: A key part of any word-based model is the attention layer. This layer helps the model understand the context in a document and the words around the word. But in this paper, we’re using a bidirectional GRU (BiGRU) model, which is a newer neural network approach for this kind of thing. Formal: To tackle this, we’re adding a word embedding layer that turns the text into a fixed-length vector and a sequence of output neurons. Formal: This layer can also be thought of as the final output of the BOW model, kind of like the output from a regular GRU. Formal: We’re using a pre-trained BERT model (Devlin et al., 2019) to handle the representations h_i, which we’ll call b_i. Formal: To train our model, we’ve come up with a few strategies, like stochastic gradient descent (SGD), dropout (S), and dropout rate (D), along with a non-linear function called tanh. Formal: The main goal is to maximize the cross-entropy loss, which is just a fancy way of saying we’re trying to minimize the cross-entropy (C_ent). Formal: We’re using the same sequence-to-sequence model as in the classic bidirectional GRU setup,",
        "directory": "E09",
        "filename": "E09-1043.json"
    },
    {
        "casual_text": "Figuring out which parts of the text are important is a big deal in NLP stuff (Reiter and Dale, 2000). Some folks in 2018 used a pointer network (Vinyals et al., 2015) to pick out key phrases for making questions, and Gehrmann et al. in 2018 used a content selector to control how often they copied stuff for summarizing. The goal of these methods is to make things more accurate, but our approach does that and also boosts diversity (check out our results for proof). Plus, our method helps models learn how to use the selected info, unlike Gehrmann et al. (2018) who manually restrict the copying for non-selected parts.",
        "formal_text": "Content Selection in NLP Selecting important parts of the context has been an important step in NLP applications (Reiter and Dale, 2000). 2018use a pointer network (Vinyals et al. , 2015) for extracting key phrases for question generation and Gehrmann et al. 2018use content selector to limit copying probability for abstractive summarization. The main purpose of these approaches is to enhance accuracy, while our method uses diverse content selection to enhance both accuracy and diversity (refer to our empirical results). Additionally, our method allows models to learn how to utilize information from the selected content, whereas Gehrmann et al. (2018) manually limit the copying mechanism on non-selected contents.",
        "GPT2_formal_text": "= { q, r } | t = {t 1, . . . , t m }. Formal: We'll also say q_t = q_t|t for any t, which means the t-th token in the input sequence. So, Formal: The input text can be thought of as a sequence of symbols and their corresponding indices, like this: Formal: (x 1, y 1, . . . , x m, y m ) = (x i, y i, . . . , y j ). Formal: Using the same setup as (Holtzman et al., 2015), we can also write the set of numbers {m i } as {m i, . . . , m j } for any i from 1 to m-1. Formal: Using the same setup as (Holtzman et al., 2015), we can also write the set of numbers {m i, . . . , m j } for any i from 1 to m-1. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "D19",
        "filename": "D19-1308.json"
    },
    {
        "casual_text": "Alright, let me break this down in a simpler way:\n\nSo, you have two taggers, A and B. They’re working through a sequence, and at each step, they’re dealing with weights (w) and tags (ta and tb). Here’s how it looks:\n\n- Tagger A goes through the sequence like this: w₂ ta₂ tb₂, then w₁ ta₁, tb₁, and so on, until it reaches wₙ₋₁ taₙ₋₁ tbₙ₋₁, and finally wₙ taₙ tbₙ.\n- Tagger B follows a similar pattern but skips the ta tags: w₂ tb₂, then w₁ tb₁, and continues until wₙ₋₁ tbₙ₋₁ wₙ tbₙ.\n\nBasically, both taggers are processing the same sequence, but Tagger A includes the ta tags, while Tagger B doesn’t.",
        "formal_text": "τ (⃗ y i, ⃗ y i−1 ). A tagger. . . B tagger. . . . . . w 2 ta 2 tb 2. . . w 1 ta 1. . . tb 1. . . . . . w n-1 ta n-1 tb n-1 w n. . . . . . ta n tb n A tagger. . . B tagger. . . . . . w 2 tb 2. . . w 1. . . tb 1. . . . . . w n-1 tb n-1 w n. . . . . . tb n. . . . . . . . . . . .",
        "GPT2_formal_text": "ing, as described by Ahmad et al. (2019) and Dong et al. (2019). The model is trained using a mix of a beam search and an MLP to predict token labels. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle tokens from both sides of an LSTM-based LSTM, which is used to grab the hidden states. Formal: The model can handle",
        "directory": "D16",
        "filename": "D16-1070.json"
    },
    {
        "casual_text": "The first dataset we're looking at comes from Yencken and Baldwin (2006), and it's based on human similarity judgments. The problem with this dataset is that it treats figuring out the difference between low and medium similarity pairs the same as distinguishing between medium and high similarity pairs. But, let's be real, for most practical uses, nailing the high similarity pairs with great accuracy is way more important. Still, this dataset is handy for checking how our methods stack up against those used in earlier studies.",
        "formal_text": "The first data set is the human similarity judgements from Yencken and Baldwin (2006). This data set is overly broad in that it weights the ability to distinguish low and medium similarity pairs equally with distinguishing medium and high similarity pairs. It is clear that for most applications, determining the high similarity pairs with high precision is most important. Nevertheless, this data set is useful for comparing our metrics with those proposed in previous research.",
        "GPT2_formal_text": ", which are the tags for each example in the test set. Formal: In this section, we'll explain how the test scores are defined and show the results. Formal: The first three rows show the scores for the tagger, and the last two rows show the results for the classifier. Formal: The scores for the tagger are calculated using the number of times the tag (x) is used. If there's a tie, we use the average score. Formal: The classifier uses the overall F1 score for the test set, but it also counts the scores for each example in the test set. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal",
        "directory": "C08",
        "filename": "C08-1131.json"
    },
    {
        "casual_text": "At every merge step in the CKY process, we add another subtree. This one doesn't just average the two attention values, a_cl and a_cr (like in eq. 1), but also includes the child polarity scores, p_cl and p_cr. This setup aligns with how the N-N nuclearity class is defined in RST, where all child nodes are considered equally important in multi-nucleus cases. The only way we can handle the extra complexity of doubling the number of trees in each cell is by using our heuristic method.",
        "formal_text": "an additional subtree at every merge in the CKY procedure, which averages not only the two attention values a c l, a cr (as shown in eq. 1) but also the child polarity scores p c l, p cr. This reflects the definition of the N-N nuclearity class according to RST, where an even importance for all child nodes is assumed in the multi-nucleus case. The additional complexity of doubling the number of trees in each cell is only manageable due to the use of our heuristic approach.",
        "GPT2_formal_text": "ed = { 0.3, 0.1, 0.1 } n = 3 # We'll use a regularizer here, but we'll tweak it a bit. It's like a starting point that can be adjusted as we train. Formal: The log-likelihood for a prediction is what we're talking about. It's a way to measure how well the model is doing in understanding the world. Formal: Since we don't have a perfect, exact way to measure things, we came up with something called the loss function, which we'll call L_i. Basically, L_i looks at how well the model fits with a specific example, f, to estimate the likelihood. Formal: If we're dealing with a model that can't pick a specific example, like w_i, it's usually a sign that we're dealing with some kind of uncertainty or uncertainty in the model. Formal: In our system, L_i is a matrix where the logits of a prediction, q_i, are scaled down to fit a specific example, f_i. Formal: If we have a single example, f_i, where the logits are positive, the loss function L_i(f_i) works like this: Formal: The main idea behind L_i is to pick a negative example, f_i, where the logits are negative. Formal: Here, f_i is a negative example and L_i(f_i) is the logits of that negative example, which we'll call L_i(f_i). Formal: In our model, we use a negative example, f_i, to figure out the probability P(f_i | f_i). Formal: After that, we use L_i(f_i) to estimate the likelihood P(w_i | f_i) of the correct prediction, which we'll call P(w_i | f_i). Formal: In our model, we take an example, f_i, and make the logits of its logits negative to see how uncertain the model is. Formal: The losses L_i(f_i) are calculated in a similar way, but they focus on how uncertain the model is. Formal: Formal: We can check out the full explanation for L_i in Algorithm 1. Formal: The loss function L_i(f_i) is",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.603.json"
    },
    {
        "casual_text": "For the next experiment, we’re using the method from Section 3 on an English dataset to create training data tailored to each language and train a separate semantic parser for each one. We had some of the validation set translated by humans and mixed it with the machine-translated validation data. From here on out, we’ll pick the model with the best exact match (em) accuracy on this combined set and test it on human-translated test data. As you can see in Table 4, this approach beats all the other methods we tried. We got improvements ranging from 33% to 50% compared to the previous best result, which was the Bootstrap approach. \n\nThe neural model we trained on this SPL data benefits from entity alignment between the sentence and the logical form, allowing it to copy entities directly. The exact match accuracy varies—it’s 53% for Chinese and 62% for Spanish when it comes to hotels, and 41% for Japanese and 68% for Spanish for restaurants. When you compare this to the English results, which were 65% for hotels and 69% for restaurants, you can see that performance drops for languages that are very different from English. However, languages closer to English, like Spanish, come pretty close to matching English’s performance.",
        "formal_text": "In the next experiment, we apply the methodology in Section 3 to the English dataset to create localized training data and train one semantic parser per language. We translate a portion of the validation set using human translators and combine it with the machine-translated validation data. For all the following experiments, the model with the highest em accuracy on this set is chosen and tested on human-translated test data. As shown in Table 4, the results obtained by this methodology outperforms all the baselines. Specifically, we achieve improvements between 33% to 50% over the previous state-of-the-art result, represented by the Bootstrap approach. The neural model trained on SPL data takes advantage of entity alignment in the utterance and logical form and can copy the entities directly. The exact match accuracy ranges from 53% in Chinese to 62% in Spanish for hotels, and from 41% in Japanese to 68% in Spanish for restaurants. Comparing to the accuracy of 65% and 69% for hotels and restaurants in English, respectively, we see a degradation in performance for languages that are very different from English. Languages close to English, such as Spanish, approach the performance of English.",
        "GPT2_formal_text": ", which are the unique tokens in the reference sentence. After that, we match the tokens in the summary to the predicted token sequence by running the FST. Finally, we get the probability of the actual label sequence yt. Formal: So, we've got a probability distribution p_t(i_t = yt) given a bunch of source tokens x_t. This probability is calculated by maximizing the log probability of yt given x_t, and the matrix P_s(t) shows the final probability distribution. Formal: The final probability p_t(y) is calculated by maximizing the log likelihood of y. Formal: To keep things simple, we use this probability for both the reference and the predicted token sequence. Formal: We also calculate the probability P_s(y) for the target sequence y. Formal: Formal: To figure out the new token sequence yt, we take the observed distribution P_s(y) and the predicted distribution P_t(y) and combine them using the forward and backward hidden states of the last hidden state of the sentence. Formal: Lastly, we use the softmax function to find the best alignment, which is the biggest possible sequence that gives us the highest probability P_t(y) for the target sequence y. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Form",
        "directory": "emnlp",
        "filename": "2020.emnlp-main.481.json"
    },
    {
        "casual_text": "Using two bytes to represent an integer gives you 2^16 - 1 = 65536 unique codes. But since there are way more words in English than we can fit into those codes, we need a way to handle words without assigning each one a specific code. By setting aside one bit for this purpose, we're left with 2^15 - 1 = 32767 possible combinations.",
        "formal_text": "In using two bytes to represent an integer it is possible to have 216 -1 = 65536 distinct codes. However, since it is impossible to have codes for all the words in the English Lan~lage, it is necessary to include a mechanism that allows for the representation of words without codes by their individual characters. Keeping one bit for that purpose ]. eaves 215 -i = 32767 possible number of combinations.",
        "GPT2_formal_text": "ing. The projection matrix is shaped as a square and has 3 rows, where each row represents the hidden state from the hidden layer of the network. This setup allows for a much wider range of values in the input space, making it easier to generate responses. The difference in how often the output shows up is calculated using this matrix. Formal: Formal: This means the probability of a response, given the input, is calculated based on the projected representation of the target word. Formal: We train the network using the target language's sentence embedding, which is defined as p(t_i) = [0.5 * p_i - p_1 + p_2]. Formal: Formal: In short, we train the model using the target language's sentence embedding. Formal: Formal: Let’s say W_t is the vocabulary we’re using. Formal: In Figure 2, you can see a quick example of how the latent representation for a sentence is projected on the target side. Formal: Formal: The word embedding vector for the target word w_t, given its hidden state h_t, is defined as p(w_t) = [0.5 * w_t - p_1 + p_2]. Formal: Formal: Here’s how the embedding vectors for the target and source words are projected on the target side. Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal: Formal:",
        "directory": "C86",
        "filename": "C86-1098.json"
    }
]