è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œå…±æœ‰ 32000 ä¸ªæ ·æœ¬ã€‚
éªŒè¯é›†åŠ è½½å®Œæˆï¼Œå…±æœ‰ 4000 ä¸ªæ ·æœ¬ã€‚
æ·»åŠ äº†å¡«å……æ ‡è®° '[PAD]' åˆ°åˆ†è¯å™¨ã€‚
æ•°æ®é¢„å¤„ç†å®Œæˆã€‚
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
æ¨¡å‹åŠ è½½å¹¶è°ƒæ•´è¯æ±‡è¡¨å®Œæˆã€‚
/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/mnt/file2/changye/NLPFINAL/fine-tuning/trainer.py:72: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning/main.py", line 192, in <module>
[rank0]:     main(args)
[rank0]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning/main.py", line 143, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2164, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2322, in _inner_training_loop
[rank0]:     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/accelerator.py", line 1339, in prepare
[rank0]:     result = tuple(
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/accelerator.py", line 1340, in <genexpr>
[rank0]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/accelerator.py", line 1215, in _prepare_one
[rank0]:     return self.prepare_model(obj, device_placement=device_placement)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/accelerator.py", line 1469, in prepare_model
[rank0]:     model = torch.nn.parallel.DistributedDataParallel(
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/distributed/utils.py", line 288, in _verify_param_shape_across_processes
[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank0]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:317, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
[rank0]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank0]: Last error:
[rank0]: Cuda failure 'out of memory'
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning/main.py", line 192, in <module>
[rank0]:     main(args)
[rank0]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning/main.py", line 143, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2164, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2322, in _inner_training_loop
[rank0]:     model, self.optimizer = self.accelerator.prepare(self.model, self.optimizer)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/accelerator.py", line 1339, in prepare
[rank0]:     result = tuple(
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/accelerator.py", line 1340, in <genexpr>
[rank0]:     self._prepare_one(obj, first_pass=True, device_placement=d) for obj, d in zip(args, device_placement)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/accelerator.py", line 1215, in _prepare_one
[rank0]:     return self.prepare_model(obj, device_placement=device_placement)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/accelerator.py", line 1469, in prepare_model
[rank0]:     model = torch.nn.parallel.DistributedDataParallel(
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/parallel/distributed.py", line 825, in __init__
[rank0]:     _verify_param_shape_across_processes(self.process_group, parameters)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/distributed/utils.py", line 288, in _verify_param_shape_across_processes
[rank0]:     return dist._verify_params_across_processes(process_group, tensors, logger)
[rank0]: torch.distributed.DistBackendError: NCCL error in: ../torch/csrc/distributed/c10d/NCCLUtils.hpp:317, unhandled cuda error (run with NCCL_DEBUG=INFO for details), NCCL version 2.21.5
[rank0]: ncclUnhandledCudaError: Call to CUDA function failed.
[rank0]: Last error:
[rank0]: Cuda failure 'out of memory'
