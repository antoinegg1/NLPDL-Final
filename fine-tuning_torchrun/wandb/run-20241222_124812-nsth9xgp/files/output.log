è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œå…±æœ‰ 32000 ä¸ªæ ·æœ¬ã€‚
éªŒè¯é›†åŠ è½½å®Œæˆï¼Œå…±æœ‰ 4000 ä¸ªæ ·æœ¬ã€‚
æ•°æ®é¢„å¤„ç†å®Œæˆã€‚
/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/mnt/file2/changye/NLPFINAL/fine-tuning/trainer.py:81: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|                                                                                                                               | 0/6000 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
  1%|â–ˆâ–‹                                                                                                                  | 88/6000 [05:04<5:37:06,  3.42s/it]
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 5e-05, 'epoch': 0.0}
