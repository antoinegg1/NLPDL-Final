ä½¿ç”¨è®¾å¤‡: cuda
è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œå…±æœ‰ 32000 ä¸ªæ ·æœ¬ã€‚
éªŒè¯é›†åŠ è½½å®Œæˆï¼Œå…±æœ‰ 4000 ä¸ªæ ·æœ¬ã€‚
æ·»åŠ äº†å¡«å……æ ‡è®° '[PAD]' åˆ°åˆ†è¯å™¨ã€‚
æ•°æ®é¢„å¤„ç†å®Œæˆã€‚
The new embeddings will be initialized from a multivariate normal distribution that has old embeddings' mean and covariance. As described in this article: https://nlp.stanford.edu/~johnhew/vocab-expansion.html. To disable this, use `mean_resizing=False`
æ¨¡å‹åŠ è½½å¹¶è°ƒæ•´è¯æ±‡è¡¨å®Œæˆã€‚
/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/mnt/file2/changye/NLPFINAL/fine-tuning/trainer.py:67: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
[34m[1mwandb[0m: [33mWARNING[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.
  0%|â–                                                                                                   | 10/6000 [00:40<6:07:06,  3.68s/it]
