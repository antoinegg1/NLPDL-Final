使用设备: cuda
训练集加载完成，共有 4000 个样本。
验证集加载完成，共有 32000 个样本。
添加了填充标记 '[PAD]' 到分词器。
数据预处理完成。
Traceback (most recent call last):
  File "/mnt/file2/changye/NLPFINAL/fine-tuning/main.py", line 175, in <module>
    main(args)
  File "/mnt/file2/changye/NLPFINAL/fine-tuning/main.py", line 102, in main
    model = GPT2LMHeadModel.from_pretrained('/mnt/file2/changye/model/fine_tuned_gpt2')
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4130, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1194, in __init__
    self.transformer = GPT2Model(config)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 904, in __init__
    self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 904, in <listcomp>
    self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 592, in __init__
    self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 208, in __init__
    self.reset_parameters()
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 212, in reset_parameters
    init.ones_(self.weight)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/init.py", line 255, in ones_
    return _no_grad_fill_(tensor, 1.0)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/init.py", line 64, in _no_grad_fill_
    return tensor.fill_(val)
KeyboardInterrupt
Traceback (most recent call last):
  File "/mnt/file2/changye/NLPFINAL/fine-tuning/main.py", line 175, in <module>
    main(args)
  File "/mnt/file2/changye/NLPFINAL/fine-tuning/main.py", line 102, in main
    model = GPT2LMHeadModel.from_pretrained('/mnt/file2/changye/model/fine_tuned_gpt2')
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/modeling_utils.py", line 4130, in from_pretrained
    model = cls(config, *model_args, **model_kwargs)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 1194, in __init__
    self.transformer = GPT2Model(config)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 904, in __init__
    self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 904, in <listcomp>
    self.h = nn.ModuleList([GPT2Block(config, layer_idx=i) for i in range(config.num_hidden_layers)])
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/models/gpt2/modeling_gpt2.py", line 592, in __init__
    self.ln_1 = nn.LayerNorm(hidden_size, eps=config.layer_norm_epsilon)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 208, in __init__
    self.reset_parameters()
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/modules/normalization.py", line 212, in reset_parameters
    init.ones_(self.weight)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/init.py", line 255, in ones_
    return _no_grad_fill_(tensor, 1.0)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/nn/init.py", line 64, in _no_grad_fill_
    return tensor.fill_(val)
KeyboardInterrupt
