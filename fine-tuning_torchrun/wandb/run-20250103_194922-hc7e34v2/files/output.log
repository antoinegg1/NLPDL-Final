ËÆ≠ÁªÉÈõÜÂä†ËΩΩÂÆåÊàêÔºåÂÖ±Êúâ 32000 ‰∏™Ê†∑Êú¨„ÄÇ
È™åËØÅÈõÜÂä†ËΩΩÂÆåÊàêÔºåÂÖ±Êúâ 4000 ‰∏™Ê†∑Êú¨„ÄÇ
Map: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 32000/32000 [00:38<00:00, 839.22 examples/s]
Êï∞ÊçÆÈ¢ÑÂ§ÑÁêÜÂÆåÊàê„ÄÇ
/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ü§ó Transformers. Use `eval_strategy` instead
  warnings.warn(
/mnt/file2/changye/NLPFINAL/fine-tuning_torchrun/trainer.py:74: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.
  trainer = Trainer(
  0%|                                                                                            | 5/8000 [00:40<16:32:41,  7.45s/it]Traceback (most recent call last):
{'loss': 2.9929, 'grad_norm': nan, 'learning_rate': 5e-05, 'epoch': 0.0}
  File "/mnt/file2/changye/NLPFINAL/fine-tuning_torchrun/main.py", line 168, in <module>
    main(args)
  File "/mnt/file2/changye/NLPFINAL/fine-tuning_torchrun/main.py", line 123, in main
    trainer.train()
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2164, in train
    return inner_training_loop(
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2575, in _inner_training_loop
    self.optimizer.step()
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
    self.scaler.step(self.optimizer, closure)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
    retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
    retval = optimizer.step(*args, **kwargs)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
    return method(*args, **kwargs)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
    return func.__get__(opt, opt.__class__)(*args, **kwargs)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
    out = func(*args, **kwargs)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
    ret = func(self, *args, **kwargs)
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/adamw.py", line 209, in step
    has_complex = self._init_group(
  File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/adamw.py", line 148, in _init_group
    state["exp_avg"] = torch.zeros_like(
torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 19.00 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
[rank0]: Traceback (most recent call last):
[rank0]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning_torchrun/main.py", line 168, in <module>
[rank0]:     main(args)
[rank0]:   File "/mnt/file2/changye/NLPFINAL/fine-tuning_torchrun/main.py", line 123, in main
[rank0]:     trainer.train()
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2164, in train
[rank0]:     return inner_training_loop(
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/trainer.py", line 2575, in _inner_training_loop
[rank0]:     self.optimizer.step()
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/optimizer.py", line 165, in step
[rank0]:     self.scaler.step(self.optimizer, closure)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 457, in step
[rank0]:     retval = self._maybe_opt_step(optimizer, optimizer_state, *args, **kwargs)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/amp/grad_scaler.py", line 352, in _maybe_opt_step
[rank0]:     retval = optimizer.step(*args, **kwargs)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/accelerate/optimizer.py", line 210, in patched_step
[rank0]:     return method(*args, **kwargs)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/lr_scheduler.py", line 137, in wrapper
[rank0]:     return func.__get__(opt, opt.__class__)(*args, **kwargs)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/optimizer.py", line 487, in wrapper
[rank0]:     out = func(*args, **kwargs)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/optimizer.py", line 91, in _use_grad
[rank0]:     ret = func(self, *args, **kwargs)
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/adamw.py", line 209, in step
[rank0]:     has_complex = self._init_group(
[rank0]:   File "/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/torch/optim/adamw.py", line 148, in _init_group
[rank0]:     state["exp_avg"] = torch.zeros_like(
[rank0]: torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 54.00 MiB. GPU 0 has a total capacity of 23.60 GiB of which 19.00 MiB is free. Including non-PyTorch memory, this process has 23.54 GiB memory in use. Of the allocated memory 22.06 GiB is allocated by PyTorch, and 1.06 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)
