è®­ç»ƒé›†åŠ è½½å®Œæˆï¼Œå…±æœ‰ 32000 ä¸ªæ ·æœ¬ã€‚
éªŒè¯é›†åŠ è½½å®Œæˆï¼Œå…±æœ‰ 4000 ä¸ªæ ·æœ¬ã€‚
æ•°æ®é¢„å¤„ç†å®Œæˆã€‚
/home/changye/miniconda3/envs/ML/lib/python3.10/site-packages/transformers/training_args.py:1575: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of ğŸ¤— Transformers. Use `eval_strategy` instead
  warnings.warn(
/mnt/file2/changye/NLPFINAL/fine-tuning/trainer.py:109: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.
  trainer = Seq2SeqTrainer(
  0%|                                                                                                                               | 0/6000 [00:00<?, ?it/s]Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.48.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.
  2%|â–ˆâ–‰                                                                                                                 | 100/6000 [06:10<5:33:40,  3.39s/it]
{'loss': 0.0, 'grad_norm': nan, 'learning_rate': 3e-05, 'epoch': 0.0}
{'loss': 0.0134, 'grad_norm': nan, 'learning_rate': 3e-05, 'epoch': 0.05}
                                                                                                                                                             
{'eval_loss': nan, 'eval_runtime': 26.8573, 'eval_samples_per_second': 148.936, 'eval_steps_per_second': 9.308, 'epoch': 0.05}
